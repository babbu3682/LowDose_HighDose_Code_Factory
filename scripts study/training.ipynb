{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Abdomen_CT/scripts_study/LowDose_HighDose_Code_Factory/scripts study\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [ 0/21]  eta: 0:03:30  lr: 0.000000  g_loss: 0.1078 (0.1078)  d_loss: 9.9838 (9.9838)  p_loss: 0.7817 (0.7817)  gp_loss: 9.9838 (9.9838)  time: 10.0110  data: 5.4722  max mem: 18583\n",
      "Train: [epoch:0]  [10/21]  eta: 0:00:53  lr: 0.000000  g_loss: 0.1082 (0.1084)  d_loss: 9.9838 (9.9838)  p_loss: 0.7849 (0.7868)  gp_loss: 9.9838 (9.9838)  time: 4.9050  data: 0.4976  max mem: 18664\n",
      "Train: [epoch:0]  [20/21]  eta: 0:00:04  lr: 0.000000  g_loss: 0.1082 (0.1081)  d_loss: 9.9837 (9.9837)  p_loss: 0.7849 (0.7841)  gp_loss: 9.9838 (9.9838)  time: 4.4119  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:0] Total time: 0:01:38 (4.6875 s / it)\n",
      "Averaged stats: lr: 0.000000  g_loss: 0.1082 (0.1081)  d_loss: 9.9837 (9.9837)  p_loss: 0.7849 (0.7841)  gp_loss: 9.9838 (9.9838)\n",
      "Valid: [epoch:0]  [  0/224]  eta: 0:04:22  L1_loss: 0.1297 (0.1297)  time: 1.1697  data: 1.1641  max mem: 18664\n",
      "Valid: [epoch:0]  [200/224]  eta: 0:00:00  L1_loss: 0.1315 (0.1266)  time: 0.0070  data: 0.0029  max mem: 18664\n",
      "Valid: [epoch:0]  [223/224]  eta: 0:00:00  L1_loss: 0.1328 (0.1268)  time: 0.0060  data: 0.0022  max mem: 18664\n",
      "Valid: [epoch:0] Total time: 0:00:02 (0.0123 s / it)\n",
      "Averaged stats: L1_loss: 0.1328 (0.1268)\n",
      "/workspace/Abdomen_CT/scripts_study/Predictions/Train/png/[Previous]WGAN_VGG/epoch_0_input_n_20.png\n",
      "Train: [epoch:1]  [ 0/21]  eta: 0:03:16  lr: 0.000000  g_loss: 0.1065 (0.1065)  d_loss: 9.9837 (9.9837)  p_loss: 0.7686 (0.7686)  gp_loss: 9.9838 (9.9838)  time: 9.3382  data: 4.8418  max mem: 18664\n",
      "Train: [epoch:1]  [10/21]  eta: 0:00:53  lr: 0.000000  g_loss: 0.1077 (0.1080)  d_loss: 9.9834 (9.9834)  p_loss: 0.7804 (0.7829)  gp_loss: 9.9836 (9.9835)  time: 4.8802  data: 0.4402  max mem: 18664\n",
      "Train: [epoch:1]  [20/21]  eta: 0:00:04  lr: 0.000000  g_loss: 0.1077 (0.1082)  d_loss: 9.9832 (9.9832)  p_loss: 0.7804 (0.7853)  gp_loss: 9.9833 (9.9834)  time: 4.4406  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:1] Total time: 0:01:38 (4.6823 s / it)\n",
      "Averaged stats: lr: 0.000000  g_loss: 0.1077 (0.1082)  d_loss: 9.9832 (9.9832)  p_loss: 0.7804 (0.7853)  gp_loss: 9.9833 (9.9834)\n",
      "Train: [epoch:2]  [ 0/21]  eta: 0:03:21  lr: 0.000000  g_loss: 0.1075 (0.1075)  d_loss: 9.9828 (9.9828)  p_loss: 0.7786 (0.7786)  gp_loss: 9.9829 (9.9829)  time: 9.6089  data: 5.1070  max mem: 18664\n",
      "Train: [epoch:2]  [10/21]  eta: 0:00:54  lr: 0.000000  g_loss: 0.1081 (0.1081)  d_loss: 9.9824 (9.9824)  p_loss: 0.7841 (0.7848)  gp_loss: 9.9826 (9.9826)  time: 4.9180  data: 0.4644  max mem: 18664\n",
      "Train: [epoch:2]  [20/21]  eta: 0:00:04  lr: 0.000000  g_loss: 0.1080 (0.1080)  d_loss: 9.9819 (9.9819)  p_loss: 0.7831 (0.7839)  gp_loss: 9.9821 (9.9821)  time: 4.4474  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:2] Total time: 0:01:38 (4.7023 s / it)\n",
      "Averaged stats: lr: 0.000000  g_loss: 0.1080 (0.1080)  d_loss: 9.9819 (9.9819)  p_loss: 0.7831 (0.7839)  gp_loss: 9.9821 (9.9821)\n",
      "Train: [epoch:3]  [ 0/21]  eta: 0:03:05  lr: 0.000000  g_loss: 0.1085 (0.1085)  d_loss: 9.9809 (9.9809)  p_loss: 0.7887 (0.7887)  gp_loss: 9.9811 (9.9811)  time: 8.8460  data: 4.3510  max mem: 18664\n",
      "Train: [epoch:3]  [10/21]  eta: 0:00:53  lr: 0.000000  g_loss: 0.1085 (0.1086)  d_loss: 9.9802 (9.9802)  p_loss: 0.7887 (0.7901)  gp_loss: 9.9805 (9.9805)  time: 4.8459  data: 0.3956  max mem: 18664\n",
      "Train: [epoch:3]  [20/21]  eta: 0:00:04  lr: 0.000000  g_loss: 0.1081 (0.1087)  d_loss: 9.9794 (9.9795)  p_loss: 0.7847 (0.7910)  gp_loss: 9.9797 (9.9798)  time: 4.4462  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:3] Total time: 0:01:37 (4.6650 s / it)\n",
      "Averaged stats: lr: 0.000000  g_loss: 0.1081 (0.1087)  d_loss: 9.9794 (9.9795)  p_loss: 0.7847 (0.7910)  gp_loss: 9.9797 (9.9798)\n",
      "Train: [epoch:4]  [ 0/21]  eta: 0:02:41  lr: 0.000000  g_loss: 0.1084 (0.1084)  d_loss: 9.9778 (9.9778)  p_loss: 0.7880 (0.7880)  gp_loss: 9.9781 (9.9781)  time: 7.6813  data: 3.1762  max mem: 18664\n",
      "Train: [epoch:4]  [10/21]  eta: 0:00:52  lr: 0.000000  g_loss: 0.1081 (0.1079)  d_loss: 9.9767 (9.9767)  p_loss: 0.7843 (0.7827)  gp_loss: 9.9770 (9.9771)  time: 4.7440  data: 0.2888  max mem: 18664\n",
      "Train: [epoch:4]  [20/21]  eta: 0:00:04  lr: 0.000000  g_loss: 0.1080 (0.1078)  d_loss: 9.9754 (9.9756)  p_loss: 0.7835 (0.7820)  gp_loss: 9.9758 (9.9760)  time: 4.4532  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:4] Total time: 0:01:36 (4.6168 s / it)\n",
      "Averaged stats: lr: 0.000000  g_loss: 0.1080 (0.1078)  d_loss: 9.9754 (9.9756)  p_loss: 0.7835 (0.7820)  gp_loss: 9.9758 (9.9760)\n",
      "Train: [epoch:5]  [ 0/21]  eta: 0:03:11  lr: 0.000000  g_loss: 0.1072 (0.1072)  d_loss: 9.9730 (9.9730)  p_loss: 0.7759 (0.7759)  gp_loss: 9.9736 (9.9736)  time: 9.0993  data: 4.5560  max mem: 18664\n",
      "Train: [epoch:5]  [10/21]  eta: 0:00:53  lr: 0.000000  g_loss: 0.1080 (0.1083)  d_loss: 9.9711 (9.9712)  p_loss: 0.7838 (0.7871)  gp_loss: 9.9718 (9.9718)  time: 4.8792  data: 0.4145  max mem: 18664\n",
      "Train: [epoch:5]  [20/21]  eta: 0:00:04  lr: 0.000000  g_loss: 0.1079 (0.1081)  d_loss: 9.9690 (9.9693)  p_loss: 0.7832 (0.7845)  gp_loss: 9.9697 (9.9700)  time: 4.4539  data: 0.0003  max mem: 18664\n",
      "Train: [epoch:5] Total time: 0:01:38 (4.6862 s / it)\n",
      "Averaged stats: lr: 0.000000  g_loss: 0.1079 (0.1081)  d_loss: 9.9690 (9.9693)  p_loss: 0.7832 (0.7845)  gp_loss: 9.9697 (9.9700)\n",
      "Train: [epoch:6]  [ 0/21]  eta: 0:03:20  lr: 0.000001  g_loss: 0.1060 (0.1060)  d_loss: 9.9649 (9.9649)  p_loss: 0.7643 (0.7643)  gp_loss: 9.9658 (9.9658)  time: 9.5483  data: 5.0409  max mem: 18664\n",
      "Train: [epoch:6]  [10/21]  eta: 0:00:54  lr: 0.000001  g_loss: 0.1076 (0.1075)  d_loss: 9.9623 (9.9623)  p_loss: 0.7800 (0.7794)  gp_loss: 9.9633 (9.9633)  time: 4.9162  data: 0.4584  max mem: 18664\n",
      "Train: [epoch:6]  [20/21]  eta: 0:00:04  lr: 0.000001  g_loss: 0.1078 (0.1078)  d_loss: 9.9589 (9.9593)  p_loss: 0.7823 (0.7828)  gp_loss: 9.9601 (9.9605)  time: 4.4523  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:6] Total time: 0:01:38 (4.7043 s / it)\n",
      "Averaged stats: lr: 0.000001  g_loss: 0.1078 (0.1078)  d_loss: 9.9589 (9.9593)  p_loss: 0.7823 (0.7828)  gp_loss: 9.9601 (9.9605)\n",
      "Train: [epoch:7]  [ 0/21]  eta: 0:03:03  lr: 0.000001  g_loss: 0.1074 (0.1074)  d_loss: 9.9522 (9.9522)  p_loss: 0.7782 (0.7782)  gp_loss: 9.9537 (9.9537)  time: 8.7345  data: 4.2383  max mem: 18664\n",
      "Train: [epoch:7]  [10/21]  eta: 0:00:53  lr: 0.000001  g_loss: 0.1074 (0.1081)  d_loss: 9.9480 (9.9479)  p_loss: 0.7785 (0.7861)  gp_loss: 9.9498 (9.9497)  time: 4.8370  data: 0.3854  max mem: 18664\n",
      "Train: [epoch:7]  [20/21]  eta: 0:00:04  lr: 0.000001  g_loss: 0.1076 (0.1079)  d_loss: 9.9421 (9.9429)  p_loss: 0.7815 (0.7838)  gp_loss: 9.9442 (9.9450)  time: 4.4469  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:7] Total time: 0:01:37 (4.6603 s / it)\n",
      "Averaged stats: lr: 0.000001  g_loss: 0.1076 (0.1079)  d_loss: 9.9421 (9.9429)  p_loss: 0.7815 (0.7838)  gp_loss: 9.9442 (9.9450)\n",
      "Train: [epoch:8]  [ 0/21]  eta: 0:03:04  lr: 0.000001  g_loss: 0.1077 (0.1077)  d_loss: 9.9310 (9.9310)  p_loss: 0.7828 (0.7828)  gp_loss: 9.9337 (9.9337)  time: 8.7823  data: 4.2308  max mem: 18664\n",
      "Train: [epoch:8]  [10/21]  eta: 0:00:53  lr: 0.000001  g_loss: 0.1082 (0.1081)  d_loss: 9.9232 (9.9232)  p_loss: 0.7882 (0.7870)  gp_loss: 9.9265 (9.9265)  time: 4.8415  data: 0.3847  max mem: 18664\n",
      "Train: [epoch:8]  [20/21]  eta: 0:00:04  lr: 0.000001  g_loss: 0.1076 (0.1078)  d_loss: 9.9129 (9.9138)  p_loss: 0.7821 (0.7833)  gp_loss: 9.9171 (9.9180)  time: 4.4462  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:8] Total time: 0:01:37 (4.6600 s / it)\n",
      "Averaged stats: lr: 0.000001  g_loss: 0.1076 (0.1078)  d_loss: 9.9129 (9.9138)  p_loss: 0.7821 (0.7833)  gp_loss: 9.9171 (9.9180)\n",
      "Train: [epoch:9]  [ 0/21]  eta: 0:03:04  lr: 0.000001  g_loss: 0.1067 (0.1067)  d_loss: 9.8901 (9.8901)  p_loss: 0.7708 (0.7708)  gp_loss: 9.8959 (9.8959)  time: 8.8065  data: 4.2076  max mem: 18664\n",
      "Train: [epoch:9]  [10/21]  eta: 0:00:53  lr: 0.000001  g_loss: 0.1073 (0.1078)  d_loss: 9.8759 (9.8753)  p_loss: 0.7708 (0.7783)  gp_loss: 9.8834 (9.8829)  time: 4.8444  data: 0.3826  max mem: 18664\n",
      "Train: [epoch:9]  [20/21]  eta: 0:00:04  lr: 0.000001  g_loss: 0.1094 (0.1088)  d_loss: 9.8548 (9.8561)  p_loss: 0.7809 (0.7829)  gp_loss: 9.8645 (9.8659)  time: 4.4467  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:9] Total time: 0:01:37 (4.6637 s / it)\n",
      "Averaged stats: lr: 0.000001  g_loss: 0.1094 (0.1088)  d_loss: 9.8548 (9.8561)  p_loss: 0.7809 (0.7829)  gp_loss: 9.8645 (9.8659)\n",
      "Train: [epoch:10]  [ 0/21]  eta: 0:03:15  lr: 0.000005  g_loss: 0.1126 (0.1126)  d_loss: 9.7906 (9.7906)  p_loss: 0.7916 (0.7916)  gp_loss: 9.8086 (9.8086)  time: 9.3099  data: 4.8084  max mem: 18664\n",
      "Train: [epoch:10]  [10/21]  eta: 0:00:53  lr: 0.000005  g_loss: 0.1346 (0.1397)  d_loss: 9.4289 (9.2215)  p_loss: 0.7916 (0.7894)  gp_loss: 9.5040 (9.3383)  time: 4.8898  data: 0.4372  max mem: 18664\n",
      "Train: [epoch:10]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 0.1826 (0.1953)  d_loss: 7.3631 (6.1490)  p_loss: 0.7830 (0.7859)  gp_loss: 7.8175 (7.0596)  time: 4.4464  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:10] Total time: 0:01:38 (4.6867 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 0.1826 (0.1953)  d_loss: 7.3631 (6.1490)  p_loss: 0.7830 (0.7859)  gp_loss: 7.8175 (7.0596)\n",
      "Train: [epoch:11]  [ 0/21]  eta: 0:03:00  lr: 0.000005  g_loss: 0.5760 (0.5760)  d_loss: -2.9600 (-2.9600)  p_loss: 0.7872 (0.7872)  gp_loss: 1.8145 (1.8145)  time: 8.6044  data: 4.1029  max mem: 18664\n",
      "Train: [epoch:11]  [10/21]  eta: 0:00:53  lr: 0.000005  g_loss: 0.7861 (0.8237)  d_loss: -3.8017 (-3.6538)  p_loss: 0.7872 (0.7922)  gp_loss: 2.4023 (2.6052)  time: 4.8229  data: 0.3731  max mem: 18664\n",
      "Train: [epoch:11]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 1.1807 (1.4004)  d_loss: -4.6917 (-4.4650)  p_loss: 0.7829 (0.7845)  gp_loss: 2.1924 (2.4051)  time: 4.4445  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:11] Total time: 0:01:37 (4.6525 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 1.1807 (1.4004)  d_loss: -4.6917 (-4.4650)  p_loss: 0.7829 (0.7845)  gp_loss: 2.1924 (2.4051)\n",
      "Train: [epoch:12]  [ 0/21]  eta: 0:03:01  lr: 0.000005  g_loss: 3.2245 (3.2245)  d_loss: -5.0826 (-5.0826)  p_loss: 0.7800 (0.7800)  gp_loss: 2.4705 (2.4705)  time: 8.6639  data: 4.1647  max mem: 18664\n",
      "Train: [epoch:12]  [10/21]  eta: 0:00:53  lr: 0.000005  g_loss: 3.4678 (3.5406)  d_loss: -6.1359 (-5.9548)  p_loss: 0.7854 (0.7876)  gp_loss: 2.2737 (2.3169)  time: 4.8295  data: 0.3787  max mem: 18664\n",
      "Train: [epoch:12]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 3.8956 (3.8180)  d_loss: -6.3109 (-6.1914)  p_loss: 0.7854 (0.7846)  gp_loss: 2.4568 (2.4453)  time: 4.4537  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:12] Total time: 0:01:37 (4.6633 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 3.8956 (3.8180)  d_loss: -6.3109 (-6.1914)  p_loss: 0.7854 (0.7846)  gp_loss: 2.4568 (2.4453)\n",
      "Train: [epoch:13]  [ 0/21]  eta: 0:03:17  lr: 0.000005  g_loss: 4.1324 (4.1324)  d_loss: -6.3015 (-6.3015)  p_loss: 0.8005 (0.8005)  gp_loss: 2.6801 (2.6801)  time: 9.3969  data: 4.8912  max mem: 18664\n",
      "Train: [epoch:13]  [10/21]  eta: 0:00:53  lr: 0.000005  g_loss: 4.4968 (4.4277)  d_loss: -6.5813 (-6.5766)  p_loss: 0.7793 (0.7847)  gp_loss: 2.7110 (2.6903)  time: 4.8962  data: 0.4447  max mem: 18664\n",
      "Train: [epoch:13]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 4.5911 (4.6209)  d_loss: -6.6798 (-6.6229)  p_loss: 0.7773 (0.7808)  gp_loss: 2.6621 (2.6727)  time: 4.4457  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:13] Total time: 0:01:38 (4.6919 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 4.5911 (4.6209)  d_loss: -6.6798 (-6.6229)  p_loss: 0.7773 (0.7808)  gp_loss: 2.6621 (2.6727)\n",
      "Train: [epoch:14]  [ 0/21]  eta: 0:03:31  lr: 0.000005  g_loss: 5.3633 (5.3633)  d_loss: -6.5737 (-6.5737)  p_loss: 0.7618 (0.7618)  gp_loss: 2.6370 (2.6370)  time: 10.0733  data: 5.5756  max mem: 18664\n",
      "Train: [epoch:14]  [10/21]  eta: 0:00:54  lr: 0.000005  g_loss: 5.4243 (5.4144)  d_loss: -6.6522 (-6.6686)  p_loss: 0.7784 (0.7797)  gp_loss: 2.6874 (2.6709)  time: 4.9575  data: 0.5070  max mem: 18664\n",
      "Train: [epoch:14]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 5.6674 (5.7803)  d_loss: -6.6990 (-6.7141)  p_loss: 0.7787 (0.7810)  gp_loss: 2.6079 (2.6196)  time: 4.4470  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:14] Total time: 0:01:39 (4.7265 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 5.6674 (5.7803)  d_loss: -6.6990 (-6.7141)  p_loss: 0.7787 (0.7810)  gp_loss: 2.6079 (2.6196)\n",
      "Train: [epoch:15]  [ 0/21]  eta: 0:03:29  lr: 0.000005  g_loss: 6.9440 (6.9440)  d_loss: -6.5054 (-6.5054)  p_loss: 0.7843 (0.7843)  gp_loss: 2.5210 (2.5210)  time: 9.9648  data: 5.4367  max mem: 18664\n",
      "Train: [epoch:15]  [10/21]  eta: 0:00:54  lr: 0.000005  g_loss: 7.2919 (7.3083)  d_loss: -6.6914 (-6.7548)  p_loss: 0.7867 (0.7806)  gp_loss: 2.5210 (2.4740)  time: 4.9470  data: 0.4944  max mem: 18664\n",
      "Train: [epoch:15]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 7.8493 (7.9722)  d_loss: -6.9553 (-6.8631)  p_loss: 0.7836 (0.7799)  gp_loss: 2.3773 (2.3788)  time: 4.4458  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:15] Total time: 0:01:39 (4.7205 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 7.8493 (7.9722)  d_loss: -6.9553 (-6.8631)  p_loss: 0.7836 (0.7799)  gp_loss: 2.3773 (2.3788)\n",
      "Train: [epoch:16]  [ 0/21]  eta: 0:03:12  lr: 0.000005  g_loss: 9.2682 (9.2682)  d_loss: -7.2093 (-7.2093)  p_loss: 0.7938 (0.7938)  gp_loss: 2.2833 (2.2833)  time: 9.1492  data: 4.6164  max mem: 18664\n",
      "Train: [epoch:16]  [10/21]  eta: 0:00:53  lr: 0.000005  g_loss: 10.1808 (10.3817)  d_loss: -7.2213 (-7.1785)  p_loss: 0.7804 (0.7790)  gp_loss: 2.0453 (2.0774)  time: 4.8823  data: 0.4198  max mem: 18664\n",
      "Train: [epoch:16]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 11.1998 (11.3740)  d_loss: -7.2285 (-7.2162)  p_loss: 0.7804 (0.7791)  gp_loss: 1.9284 (1.9769)  time: 4.4528  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:16] Total time: 0:01:38 (4.6866 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 11.1998 (11.3740)  d_loss: -7.2285 (-7.2162)  p_loss: 0.7804 (0.7791)  gp_loss: 1.9284 (1.9769)\n",
      "Train: [epoch:17]  [ 0/21]  eta: 0:03:12  lr: 0.000005  g_loss: 13.7329 (13.7329)  d_loss: -7.3027 (-7.3027)  p_loss: 0.7792 (0.7792)  gp_loss: 1.6251 (1.6251)  time: 9.1821  data: 4.6597  max mem: 18664\n",
      "Train: [epoch:17]  [10/21]  eta: 0:00:53  lr: 0.000005  g_loss: 14.6817 (14.5037)  d_loss: -7.4511 (-7.4443)  p_loss: 0.7838 (0.7818)  gp_loss: 1.6372 (1.6398)  time: 4.8833  data: 0.4237  max mem: 18664\n",
      "Train: [epoch:17]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 15.6553 (15.4158)  d_loss: -7.5981 (-7.5867)  p_loss: 0.7795 (0.7792)  gp_loss: 1.5579 (1.5852)  time: 4.4488  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:17] Total time: 0:01:38 (4.6856 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 15.6553 (15.4158)  d_loss: -7.5981 (-7.5867)  p_loss: 0.7795 (0.7792)  gp_loss: 1.5579 (1.5852)\n",
      "Train: [epoch:18]  [ 0/21]  eta: 0:02:58  lr: 0.000005  g_loss: 17.1182 (17.1182)  d_loss: -7.9119 (-7.9119)  p_loss: 0.8042 (0.8042)  gp_loss: 1.5106 (1.5106)  time: 8.4972  data: 3.9955  max mem: 18664\n",
      "Train: [epoch:18]  [10/21]  eta: 0:00:52  lr: 0.000005  g_loss: 17.5538 (17.5389)  d_loss: -7.8858 (-7.8279)  p_loss: 0.7891 (0.7864)  gp_loss: 1.5106 (1.4836)  time: 4.8146  data: 0.3633  max mem: 18664\n",
      "Train: [epoch:18]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 17.6813 (17.7777)  d_loss: -7.9133 (-7.8912)  p_loss: 0.7802 (0.7844)  gp_loss: 1.4184 (1.4954)  time: 4.4460  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:18] Total time: 0:01:37 (4.6487 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 17.6813 (17.7777)  d_loss: -7.9133 (-7.8912)  p_loss: 0.7802 (0.7844)  gp_loss: 1.4184 (1.4954)\n",
      "Train: [epoch:19]  [ 0/21]  eta: 0:02:46  lr: 0.000005  g_loss: 18.3932 (18.3932)  d_loss: -7.9766 (-7.9766)  p_loss: 0.7825 (0.7825)  gp_loss: 0.9749 (0.9749)  time: 7.9352  data: 3.3517  max mem: 18664\n",
      "Train: [epoch:19]  [10/21]  eta: 0:00:52  lr: 0.000005  g_loss: 17.8599 (17.9400)  d_loss: -7.8991 (-7.9329)  p_loss: 0.7825 (0.7867)  gp_loss: 1.5237 (1.4472)  time: 4.7641  data: 0.3048  max mem: 18664\n",
      "Train: [epoch:19]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 17.6739 (17.8105)  d_loss: -7.9759 (-7.9828)  p_loss: 0.7783 (0.7798)  gp_loss: 1.5198 (1.4817)  time: 4.4464  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:19] Total time: 0:01:37 (4.6249 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 17.6739 (17.8105)  d_loss: -7.9759 (-7.9828)  p_loss: 0.7783 (0.7798)  gp_loss: 1.5198 (1.4817)\n",
      "Train: [epoch:20]  [ 0/21]  eta: 0:03:23  lr: 0.000005  g_loss: 18.0151 (18.0151)  d_loss: -7.9416 (-7.9416)  p_loss: 0.7846 (0.7846)  gp_loss: 0.9636 (0.9636)  time: 9.6828  data: 5.1900  max mem: 18664\n",
      "Train: [epoch:20]  [10/21]  eta: 0:00:54  lr: 0.000005  g_loss: 17.6650 (17.5624)  d_loss: -8.0190 (-8.0083)  p_loss: 0.7920 (0.7889)  gp_loss: 1.4156 (1.4876)  time: 4.9220  data: 0.4719  max mem: 18664\n",
      "Train: [epoch:20]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 17.5316 (17.5008)  d_loss: -8.0450 (-8.0199)  p_loss: 0.7835 (0.7866)  gp_loss: 1.4222 (1.4748)  time: 4.4456  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:20] Total time: 0:01:38 (4.7049 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 17.5316 (17.5008)  d_loss: -8.0450 (-8.0199)  p_loss: 0.7835 (0.7866)  gp_loss: 1.4222 (1.4748)\n",
      "Train: [epoch:21]  [ 0/21]  eta: 0:03:15  lr: 0.000005  g_loss: 17.5112 (17.5112)  d_loss: -8.0554 (-8.0554)  p_loss: 0.7850 (0.7850)  gp_loss: 1.3628 (1.3628)  time: 9.3215  data: 4.8292  max mem: 18664\n",
      "Train: [epoch:21]  [10/21]  eta: 0:00:53  lr: 0.000005  g_loss: 17.3253 (17.3028)  d_loss: -8.0401 (-8.0177)  p_loss: 0.7867 (0.7877)  gp_loss: 1.3628 (1.4650)  time: 4.8873  data: 0.4391  max mem: 18664\n",
      "Train: [epoch:21]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 17.1924 (17.2438)  d_loss: -8.0240 (-7.9863)  p_loss: 0.7867 (0.7859)  gp_loss: 1.3443 (1.4530)  time: 4.4459  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:21] Total time: 0:01:38 (4.6876 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 17.1924 (17.2438)  d_loss: -8.0240 (-7.9863)  p_loss: 0.7867 (0.7859)  gp_loss: 1.3443 (1.4530)\n",
      "Train: [epoch:22]  [ 0/21]  eta: 0:03:20  lr: 0.000005  g_loss: 16.6551 (16.6551)  d_loss: -7.7759 (-7.7759)  p_loss: 0.7986 (0.7986)  gp_loss: 1.6958 (1.6958)  time: 9.5690  data: 5.0648  max mem: 18664\n",
      "Train: [epoch:22]  [10/21]  eta: 0:00:54  lr: 0.000005  g_loss: 17.1861 (17.0824)  d_loss: -7.8368 (-7.8947)  p_loss: 0.7828 (0.7843)  gp_loss: 1.5609 (1.4501)  time: 4.9110  data: 0.4605  max mem: 18664\n",
      "Train: [epoch:22]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 17.2017 (17.1115)  d_loss: -7.8746 (-7.8813)  p_loss: 0.7894 (0.7888)  gp_loss: 1.3815 (1.4352)  time: 4.4467  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:22] Total time: 0:01:38 (4.7032 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 17.2017 (17.1115)  d_loss: -7.8746 (-7.8813)  p_loss: 0.7894 (0.7888)  gp_loss: 1.3815 (1.4352)\n",
      "Train: [epoch:23]  [ 0/21]  eta: 0:02:38  lr: 0.000005  g_loss: 16.7748 (16.7748)  d_loss: -7.8320 (-7.8320)  p_loss: 0.7993 (0.7993)  gp_loss: 1.4695 (1.4695)  time: 7.5669  data: 2.9449  max mem: 18664\n",
      "Train: [epoch:23]  [10/21]  eta: 0:00:52  lr: 0.000005  g_loss: 17.1785 (17.1158)  d_loss: -7.8320 (-7.8308)  p_loss: 0.8001 (0.8020)  gp_loss: 1.3839 (1.3694)  time: 4.7461  data: 0.2678  max mem: 18664\n",
      "Train: [epoch:23]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 17.1844 (17.1219)  d_loss: -7.7997 (-7.7919)  p_loss: 0.7954 (0.7991)  gp_loss: 1.2855 (1.3509)  time: 4.4587  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:23] Total time: 0:01:36 (4.6159 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 17.1844 (17.1219)  d_loss: -7.7997 (-7.7919)  p_loss: 0.7954 (0.7991)  gp_loss: 1.2855 (1.3509)\n",
      "Train: [epoch:24]  [ 0/21]  eta: 0:02:54  lr: 0.000005  g_loss: 17.1572 (17.1572)  d_loss: -7.8725 (-7.8725)  p_loss: 0.8116 (0.8116)  gp_loss: 1.2083 (1.2083)  time: 8.3125  data: 3.7996  max mem: 18664\n",
      "Train: [epoch:24]  [10/21]  eta: 0:00:52  lr: 0.000005  g_loss: 17.1572 (17.1304)  d_loss: -7.6958 (-7.6947)  p_loss: 0.8098 (0.8074)  gp_loss: 1.2947 (1.3038)  time: 4.8054  data: 0.3455  max mem: 18664\n",
      "Train: [epoch:24]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 17.1533 (17.1446)  d_loss: -7.6645 (-7.6607)  p_loss: 0.8098 (0.8085)  gp_loss: 1.2847 (1.2949)  time: 4.4516  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:24] Total time: 0:01:37 (4.6456 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 17.1533 (17.1446)  d_loss: -7.6645 (-7.6607)  p_loss: 0.8098 (0.8085)  gp_loss: 1.2847 (1.2949)\n",
      "Train: [epoch:25]  [ 0/21]  eta: 0:02:51  lr: 0.000005  g_loss: 17.3015 (17.3015)  d_loss: -7.6387 (-7.6387)  p_loss: 0.8273 (0.8273)  gp_loss: 1.0646 (1.0646)  time: 8.1443  data: 3.5807  max mem: 18664\n",
      "Train: [epoch:25]  [10/21]  eta: 0:00:52  lr: 0.000005  g_loss: 17.2198 (17.2105)  d_loss: -7.5637 (-7.5936)  p_loss: 0.8190 (0.8209)  gp_loss: 1.2104 (1.2269)  time: 4.7924  data: 0.3256  max mem: 18664\n",
      "Train: [epoch:25]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 17.2198 (17.2446)  d_loss: -7.5637 (-7.5717)  p_loss: 0.8190 (0.8220)  gp_loss: 1.2104 (1.2242)  time: 4.4542  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:25] Total time: 0:01:37 (4.6408 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 17.2198 (17.2446)  d_loss: -7.5637 (-7.5717)  p_loss: 0.8190 (0.8220)  gp_loss: 1.2104 (1.2242)\n",
      "Valid: [epoch:25]  [  0/224]  eta: 0:05:08  L1_loss: 0.1215 (0.1215)  time: 1.3791  data: 1.3736  max mem: 18664\n",
      "Valid: [epoch:25]  [200/224]  eta: 0:00:00  L1_loss: 0.1172 (0.1179)  time: 0.0063  data: 0.0025  max mem: 18664\n",
      "Valid: [epoch:25]  [223/224]  eta: 0:00:00  L1_loss: 0.1214 (0.1180)  time: 0.0060  data: 0.0022  max mem: 18664\n",
      "Valid: [epoch:25] Total time: 0:00:03 (0.0138 s / it)\n",
      "Averaged stats: L1_loss: 0.1214 (0.1180)\n",
      "/workspace/Abdomen_CT/scripts_study/Predictions/Train/png/[Previous]WGAN_VGG/epoch_25_input_n_20.png\n",
      "Train: [epoch:26]  [ 0/21]  eta: 0:02:56  lr: 0.000005  g_loss: 17.5048 (17.5048)  d_loss: -7.2026 (-7.2026)  p_loss: 0.8635 (0.8635)  gp_loss: 0.9593 (0.9593)  time: 8.4242  data: 3.8937  max mem: 18664\n",
      "Train: [epoch:26]  [10/21]  eta: 0:00:53  lr: 0.000005  g_loss: 17.4778 (17.4417)  d_loss: -7.4477 (-7.4323)  p_loss: 0.8391 (0.8411)  gp_loss: 1.1289 (1.1656)  time: 4.8197  data: 0.3541  max mem: 18664\n",
      "Train: [epoch:26]  [20/21]  eta: 0:00:04  lr: 0.000005  g_loss: 17.5454 (17.5286)  d_loss: -7.4260 (-7.4035)  p_loss: 0.8377 (0.8401)  gp_loss: 1.1606 (1.1543)  time: 4.4576  data: 0.0001  max mem: 18664\n",
      "Train: [epoch:26] Total time: 0:01:37 (4.6577 s / it)\n",
      "Averaged stats: lr: 0.000005  g_loss: 17.5454 (17.5286)  d_loss: -7.4260 (-7.4035)  p_loss: 0.8377 (0.8401)  gp_loss: 1.1606 (1.1543)\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--batch-size 100 \\\n",
    "--epochs 1000 \\\n",
    "--lr 1e-6 \\\n",
    "--lr_scheduler \"lambda\" \\\n",
    "--data-set 'Mayo_DCM' \\\n",
    "--model-name 'WGAN_VGG' \\\n",
    "--criterion 'L1 Loss' \\\n",
    "--output_dir '/workspace/Abdomen_CT/scripts_study/model/[Previous]WGAN_VGG/'\\\n",
    "--save_dir '/workspace/Abdomen_CT/scripts_study/Predictions/Train/png/[Previous]WGAN_VGG/'\\\n",
    "--validate-every 1 \\\n",
    "--num_workers 16 \\\n",
    "--criterion_mode 'single label' \\\n",
    "--multiple_GT \"False\" \\\n",
    "--patch_training \"True\" \\\n",
    "--multi-gpu-mode 'Single'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
