# MTD-GAN - Official Pytorch Implementation

It's scheduled to be uploaded soon. We are modifying the code for easier use.
We proposed a supervised multi-task aiding representation transfer learning network called <b>MTD-Net</b>.


## üí° Highlights
<!--
‚Ä¢ Develop a discriminator utilizing multi-task learning (MTL), which leverages three simultaneous tasks‚Äîrestoration, image-level decision, and pixel-level decision‚Äîto transfer contextual, global, and local feedback between the real normal-dose and synthesized images to the generator.
-->
<!--
‚Ä¢ Propose two regulations to improve the representation capabilities of the discriminator: restoration consistency (RC), which compares the discriminator's outputs from the input data with the corresponding restoration data generated by our MTL discriminator for consistency, and non-difference suppression (NDS), which excludes areas that cause confusion in discriminator decisions.
-->
<!--
‚Ä¢ Design a novel generator that consists of residual fast Fourier transform with convolution (Res-FFT-Conv) blocks [13] that fuse frequency-spatial dual-domain representations. The proposed generator effectively captures rich information by simultaneously utilizing spatial (or local), spectral (or global), and residual connections. To the best of our knowledge, this represents an inaugural effort in employing the Res-FFT-Conv block within the generator for LDCT denoising, which demonstrates the versatility of the block.
-->
<!--
‚Ä¢ Evaluate our network with extensive experiments, including an ablation study and visual scoring using two distinct datasets of brain and abdominal CT images. Six metrics based on pixel- and feature-spaces were used, and the results indicated superior performances in both quantitative and qualitative measures compared to those of state-of-the-art denoising techniques.
-->



<!-- <p align="center"><img width="100%" src="figures/Graphical_Abstract.png" /></p> -->


## Paper
This repository provides the official implementation of training SMART-Net as well as the usage of the pre-trained MTD-Net in the following paper:

Sunggu Kyung, Jongjun Won, Seongyong Pak, Sunwoo Kim, Sangyoon Lee, Kanggil Park, Gil-Sun Hong, and Namkug Kim

<!-- <b>Generative Adversarial Network with Robust Discriminator Through Multi-Task Learning for Low-Dose CT Denoising</b> <br/>
[Sunggu Kyung](https://github.com/babbu3682)<sup>1</sup>, Jongjun Won, Seongyong Pak, Sunwoo Kim, Sangyoon Lee, Kanggil Park, Gil-Sun Hong, and Namkug Kim <br/>
[MI2RL LAB](https://www.mi2rl.co/) <br/>
<b>(Under revision...)</b> IEEE Transactions on Medical Imaging (TMI) <br/>



## Requirements
+ Linux
+ CUDA 11.6
+ Python 3.8.5
+ Pytorch 1.13.1

## üì¶ MTD-GAN Framework
### 1. Clone the repository and install dependencies
```bash
$ git clone https://github.com/babbu3682/MTD-GAN.git
$ cd MTD-GAN/
$ pip install -r requirements.txt
```

### 2. Preparing data
#### For your convenience, we have provided few 3D nii samples from [Physionet publish dataset](https://physionet.org/content/ct-ich/1.3.1/) as well as their mask labels. 
#### Note: We do not use this data as a train, it is just for code publishing examples.

<!-- Download the data from [this repository](https://zenodo.org/record/4625321/files/TransVW_data.zip?download=1).  -->

- The processed dataset directory structure
```
datasets/MAYO
    train
        |--  full_3mm
        |--  quarter_3mm
            |--  L067
            |--  L096
            |--  L109
            |--  L143
                    .
                    .
                    .
    valid
        |--  full_3mm
        |--  quarter_3mm
            |--  L333
                    .
                    .
                    .
    test
        |--  full_3mm
        |--  quarter_3mm
            |--  L506
                    .
                    .
                    .
```

### 3. Upstream

#### üìã To Do List
- [x] publish previous works' weights
- [x] publish our works' weights


**+ train**:
```bash
CUDA_VISIBLE_DEVICES=2 python -W ignore train.py \
--dataset 'mayo' \
--dataset-type-train 'window_patch' \
--dataset-type-valid 'window' \
--batch-size 20 \
--train-num-workers 16 \
--valid-num-workers 16 \
--model 'MTD_GAN_Method' \
--loss 'L1 Loss' \
--method 'pcgrad' \
--optimizer 'adamw' \
--scheduler 'poly_lr' \
--epochs 500 \
--warmup-epochs 10 \
--lr 1e-4 \
--min-lr 1e-6 \
--multi-gpu-mode 'Single' \
--device 'cuda' \
--print-freq 10 \
--save-checkpoint-every 1 \
--checkpoint-dir '/workspace/sunggu/4.Dose_img2img/MTD_GAN/checkpoints/abdomen/MTD_GAN' \
--save-dir '/workspace/sunggu/4.Dose_img2img/MTD_GAN/predictions/train/abdomen/MTD_GAN' \
--memo 'abdomen, 500 epoch, node 14'
```

<!-- **+ test**:
```bash
# CUDA_VISIBLE_DEVICES=2 python -W ignore test.py \
# --dataset 'mayo_test' \
# --dataset-type-test 'window' \
# --test-batch-size 1 \
# --test-num-workers 16 \
# --model 'MTD_GAN' \
# --loss 'L1 Loss' \
# --multi-gpu-mode 'Single' \
# --device 'cuda' \
# --print-freq 10 \
# --checkpoint-dir '/workspace/sunggu/4.Dose_img2img/MTD_GAN/checkpoints/abdomen/MTD_GAN' \
# --save-dir '/workspace/sunggu/4.Dose_img2img/MTD_GAN/predictions/test/abdomen/MTD_GAN' \
# --resume "/workspace/sunggu/4.Dose_img2img/MTD_GAN/checkpoints/abdomen/MTD_GAN/epoch_77777_checkpoint.pth" \
# --memo 'abdomen, node 14' \
# --epoch 77777
``` -->


## Excuse
For personal information security reasons of medical data in Korea, our data cannot be disclosed.


## üìù Citation
If you use this code for your research, please cite our papers:
```
‚è≥ It's scheduled to be uploaded soon.
```

## ü§ù Acknowledgement
We build MTD-GAN framework by referring to the released code at [qubvel/segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) and [Project-MONAI/MONAI](https://github.com/Project-MONAI/MONAI). 
This is a patent-pending technology.

We acknowledge the open-source libraries, including the [Diffuser](https://github.com/huggingface/diffusers) and [MONAI Generative Models](https://github.com/Project-MONAI/GenerativeModels), which enabled valuable comparisons in this study, and we extend our thanks to the pioneering authors (e.g., [RED-CNN](https://github.com/SSinyu/RED-CNN), [EDCNN](https://github.com/workingcoder/EDCNN), [CTformer](https://github.com/wdayang/CTformer), [Restormer](https://github.com/swz30/Restormer), [WGAN_VGG](https://github.com/hyeongyuy/CT-WGAN_VGG_tensorflow), [MAP-NN](https://github.com/hmshan/MAP-NN), [DUGAN](https://github.com/Hzzone/DU-GAN)).

### üõ°Ô∏è License <a name="license"></a>
Project is distributed under [MIT License](https://github.com/babbu3682/MTD-GAN/blob/main/LICENSE)
