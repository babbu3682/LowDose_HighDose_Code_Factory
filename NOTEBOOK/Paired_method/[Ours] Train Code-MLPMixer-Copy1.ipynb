{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pretrainedmodels==0.7.4\n",
    "# !pip install efficientnet-pytorch==0.6.3\n",
    "# !pip install timm==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CUDA 11.1\n",
    "# !pip install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE -> MAE Loss 꿀팁!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 11 08:30:11 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "| 39%   64C    P2   252W / 260W |  22198MiB / 48601MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     Off  | 00000000:1C:00.0 Off |                  Off |\n",
      "| 32%   35C    P0    61W / 260W |      0MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 8000     Off  | 00000000:1D:00.0 Off |                  Off |\n",
      "| 31%   33C    P0    69W / 260W |      0MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     Off  | 00000000:1E:00.0 Off |                  Off |\n",
      "| 32%   32C    P0    60W / 260W |      0MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Quadro RTX 8000     Off  | 00000000:3D:00.0 Off |                  Off |\n",
      "| 31%   30C    P0    59W / 260W |      0MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Quadro RTX 8000     Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "| 30%   31C    P0    61W / 260W |      0MiB / 48601MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Quadro RTX 8000     Off  | 00000000:40:00.0 Off |                  Off |\n",
      "| 36%   61C    P2   263W / 260W |  48064MiB / 48601MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Quadro RTX 8000     Off  | 00000000:41:00.0 Off |                  Off |\n",
      "| 38%   64C    P2   275W / 260W |  48066MiB / 48601MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sunggu/4.Dose_img2img/scripts study\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/sunggu/4.Dose_img2img/scripts study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 갯수 =  64\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(\"CPU 갯수 = \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc as container_abcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPMixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "***********************************************\n",
      "Dataset Name:  Sinogram_DCM\n",
      "---------- Model ----------\n",
      "Resume From:  /workspace/sunggu/4.Dose_img2img/model/[Ours]MLPMixer_L1_P1/epoch_2_checkpoint.pth\n",
      "Output To:  /workspace/sunggu/4.Dose_img2img/model/[Ours]MLPMixer_L1_P1\n",
      "Save   To:  /workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0001\n",
      "Batchsize:  2\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  6899\n",
      "Valid [Total]  number =  14\n",
      "Creating criterion: Change L2 L1 Loss\n",
      "Creating model: MLPMixer\n",
      "Load feature extractor...!\n",
      "Number of Learnable Params: 258110209\n",
      "MLPMixer(\n",
      "  (denoiser): Revised_UNet(\n",
      "    (enc1_1): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (enc1_2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (pool1): DownsampleBlock(\n",
      "      (downsample): Sequential(\n",
      "        (0): PixelUnshuffle(downscale_factor=2)\n",
      "        (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (enc2_1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (enc2_2): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (pool2): DownsampleBlock(\n",
      "      (downsample): Sequential(\n",
      "        (0): PixelUnshuffle(downscale_factor=2)\n",
      "        (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (enc3_1): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (enc3_2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (pool3): DownsampleBlock(\n",
      "      (downsample): Sequential(\n",
      "        (0): PixelUnshuffle(downscale_factor=2)\n",
      "        (1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (enc4_1): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (enc4_2): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (pool4): DownsampleBlock(\n",
      "      (downsample): Sequential(\n",
      "        (0): PixelUnshuffle(downscale_factor=2)\n",
      "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (enc5_1): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (dec5_1): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (unpool4): UpsampleBlock(\n",
      "      (upsample): Sequential(\n",
      "        (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PixelShuffle(upscale_factor=2)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (dec4_2): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (dec4_1): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (unpool3): UpsampleBlock(\n",
      "      (upsample): Sequential(\n",
      "        (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PixelShuffle(upscale_factor=2)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (dec3_2): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (dec3_1): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (unpool2): UpsampleBlock(\n",
      "      (upsample): Sequential(\n",
      "        (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PixelShuffle(upscale_factor=2)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (dec2_2): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (dec2_1): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (unpool1): UpsampleBlock(\n",
      "      (upsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): PixelShuffle(upscale_factor=2)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (dec1_2): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (dec1_1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (fc): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (patch_embed): Sequential(\n",
      "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=1, p2=1)\n",
      "    (1): Linear(in_features=1, out_features=768, bias=True)\n",
      "  )\n",
      "  (mixer_blocks): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): PreNormResidual(\n",
      "        (fn): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=1, bias=True)\n",
      "    (2): Rearrange('b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h=64, p1=1, p2=1)\n",
      "  )\n",
      ")\n",
      "Loading pre-trained Weight...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 1000 epochs\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [   0/3449]  eta: 11:04:01  lr: 0.000010  loss: 0.0623 (0.0623)  time: 11.5515  data: 3.9054  max mem: 33983\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [  10/3449]  eta: 4:10:26  lr: 0.000010  loss: 0.0552 (0.0556)  time: 4.3695  data: 0.3552  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [  20/3449]  eta: 3:54:39  lr: 0.000010  loss: 0.0552 (0.0566)  time: 3.7337  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [  30/3449]  eta: 3:46:22  lr: 0.000010  loss: 0.0561 (0.0543)  time: 3.7546  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [  40/3449]  eta: 3:42:35  lr: 0.000010  loss: 0.0550 (0.0517)  time: 3.7201  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [  50/3449]  eta: 3:40:13  lr: 0.000010  loss: 0.0455 (0.0501)  time: 3.7549  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [  60/3449]  eta: 3:38:58  lr: 0.000010  loss: 0.0494 (0.0499)  time: 3.7929  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [  70/3449]  eta: 3:37:56  lr: 0.000010  loss: 0.0499 (0.0494)  time: 3.8255  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [  80/3449]  eta: 3:36:35  lr: 0.000010  loss: 0.0384 (0.0479)  time: 3.7987  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [  90/3449]  eta: 3:34:54  lr: 0.000010  loss: 0.0443 (0.0490)  time: 3.7285  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 100/3449]  eta: 3:33:34  lr: 0.000010  loss: 0.0619 (0.0502)  time: 3.6997  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 110/3449]  eta: 3:31:57  lr: 0.000010  loss: 0.0483 (0.0492)  time: 3.6719  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 120/3449]  eta: 3:30:56  lr: 0.000010  loss: 0.0436 (0.0489)  time: 3.6783  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 130/3449]  eta: 3:29:54  lr: 0.000010  loss: 0.0497 (0.0491)  time: 3.7167  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 140/3449]  eta: 3:29:06  lr: 0.000010  loss: 0.0473 (0.0487)  time: 3.7297  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 150/3449]  eta: 3:28:49  lr: 0.000010  loss: 0.0504 (0.0490)  time: 3.8188  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 160/3449]  eta: 3:28:13  lr: 0.000010  loss: 0.0504 (0.0488)  time: 3.8475  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 170/3449]  eta: 3:27:25  lr: 0.000010  loss: 0.0438 (0.0486)  time: 3.7782  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 180/3449]  eta: 3:26:36  lr: 0.000010  loss: 0.0390 (0.0479)  time: 3.7420  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 190/3449]  eta: 3:26:21  lr: 0.000010  loss: 0.0378 (0.0477)  time: 3.8300  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 200/3449]  eta: 3:25:58  lr: 0.000010  loss: 0.0552 (0.0484)  time: 3.9092  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 210/3449]  eta: 3:25:18  lr: 0.000010  loss: 0.0579 (0.0486)  time: 3.8419  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 220/3449]  eta: 3:24:32  lr: 0.000010  loss: 0.0556 (0.0488)  time: 3.7670  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 230/3449]  eta: 3:24:01  lr: 0.000010  loss: 0.0474 (0.0484)  time: 3.8007  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 240/3449]  eta: 3:23:20  lr: 0.000010  loss: 0.0435 (0.0486)  time: 3.8176  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 250/3449]  eta: 3:22:39  lr: 0.000010  loss: 0.0553 (0.0487)  time: 3.7772  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 260/3449]  eta: 3:22:01  lr: 0.000010  loss: 0.0495 (0.0484)  time: 3.7912  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 270/3449]  eta: 3:21:22  lr: 0.000010  loss: 0.0495 (0.0486)  time: 3.7982  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 280/3449]  eta: 3:20:49  lr: 0.000010  loss: 0.0523 (0.0483)  time: 3.8164  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 290/3449]  eta: 3:20:15  lr: 0.000010  loss: 0.0522 (0.0485)  time: 3.8433  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 300/3449]  eta: 3:19:29  lr: 0.000010  loss: 0.0485 (0.0486)  time: 3.7856  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 310/3449]  eta: 3:18:58  lr: 0.000010  loss: 0.0489 (0.0486)  time: 3.7956  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 320/3449]  eta: 3:18:25  lr: 0.000010  loss: 0.0514 (0.0487)  time: 3.8608  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 330/3449]  eta: 3:17:45  lr: 0.000010  loss: 0.0606 (0.0489)  time: 3.8232  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 340/3449]  eta: 3:17:00  lr: 0.000010  loss: 0.0545 (0.0489)  time: 3.7575  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 350/3449]  eta: 3:16:21  lr: 0.000010  loss: 0.0545 (0.0489)  time: 3.7568  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 360/3449]  eta: 3:15:38  lr: 0.000010  loss: 0.0480 (0.0489)  time: 3.7644  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 370/3449]  eta: 3:15:07  lr: 0.000010  loss: 0.0472 (0.0488)  time: 3.8173  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 380/3449]  eta: 3:14:26  lr: 0.000010  loss: 0.0447 (0.0488)  time: 3.8238  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 390/3449]  eta: 3:13:48  lr: 0.000010  loss: 0.0396 (0.0485)  time: 3.7815  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 400/3449]  eta: 3:13:18  lr: 0.000010  loss: 0.0419 (0.0484)  time: 3.8556  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 410/3449]  eta: 3:12:39  lr: 0.000010  loss: 0.0470 (0.0484)  time: 3.8452  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 420/3449]  eta: 3:12:03  lr: 0.000010  loss: 0.0611 (0.0488)  time: 3.8142  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 430/3449]  eta: 3:11:21  lr: 0.000010  loss: 0.0566 (0.0487)  time: 3.7927  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 440/3449]  eta: 3:10:47  lr: 0.000010  loss: 0.0545 (0.0490)  time: 3.8051  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 450/3449]  eta: 3:10:04  lr: 0.000010  loss: 0.0600 (0.0491)  time: 3.7986  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 460/3449]  eta: 3:09:21  lr: 0.000010  loss: 0.0547 (0.0493)  time: 3.7239  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 470/3449]  eta: 3:08:55  lr: 0.000010  loss: 0.0521 (0.0493)  time: 3.8543  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 480/3449]  eta: 3:08:21  lr: 0.000010  loss: 0.0528 (0.0493)  time: 3.9309  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 490/3449]  eta: 3:07:39  lr: 0.000010  loss: 0.0570 (0.0494)  time: 3.8115  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 500/3449]  eta: 3:06:59  lr: 0.000010  loss: 0.0485 (0.0494)  time: 3.7582  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 510/3449]  eta: 3:06:18  lr: 0.000010  loss: 0.0456 (0.0494)  time: 3.7605  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 520/3449]  eta: 3:05:38  lr: 0.000010  loss: 0.0453 (0.0494)  time: 3.7665  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 530/3449]  eta: 3:05:04  lr: 0.000010  loss: 0.0485 (0.0494)  time: 3.8215  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 540/3449]  eta: 3:04:21  lr: 0.000010  loss: 0.0544 (0.0495)  time: 3.7899  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 550/3449]  eta: 3:03:45  lr: 0.000010  loss: 0.0492 (0.0495)  time: 3.7729  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 560/3449]  eta: 3:03:08  lr: 0.000010  loss: 0.0526 (0.0496)  time: 3.8329  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 570/3449]  eta: 3:02:30  lr: 0.000010  loss: 0.0542 (0.0497)  time: 3.8136  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 580/3449]  eta: 3:01:52  lr: 0.000010  loss: 0.0517 (0.0496)  time: 3.8014  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 590/3449]  eta: 3:01:21  lr: 0.000010  loss: 0.0518 (0.0497)  time: 3.8769  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 600/3449]  eta: 3:00:42  lr: 0.000010  loss: 0.0570 (0.0498)  time: 3.8683  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 610/3449]  eta: 3:00:03  lr: 0.000010  loss: 0.0490 (0.0497)  time: 3.7851  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 620/3449]  eta: 2:59:27  lr: 0.000010  loss: 0.0485 (0.0496)  time: 3.8225  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 630/3449]  eta: 2:58:47  lr: 0.000010  loss: 0.0371 (0.0495)  time: 3.8058  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 640/3449]  eta: 2:58:10  lr: 0.000010  loss: 0.0467 (0.0495)  time: 3.7901  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 650/3449]  eta: 2:57:33  lr: 0.000010  loss: 0.0474 (0.0495)  time: 3.8336  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 660/3449]  eta: 2:56:52  lr: 0.000010  loss: 0.0430 (0.0494)  time: 3.7880  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 670/3449]  eta: 2:56:16  lr: 0.000010  loss: 0.0587 (0.0496)  time: 3.7995  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 680/3449]  eta: 2:55:30  lr: 0.000010  loss: 0.0602 (0.0497)  time: 3.7345  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 690/3449]  eta: 2:54:52  lr: 0.000010  loss: 0.0552 (0.0497)  time: 3.6984  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 700/3449]  eta: 2:54:16  lr: 0.000010  loss: 0.0541 (0.0498)  time: 3.8219  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 710/3449]  eta: 2:53:34  lr: 0.000010  loss: 0.0554 (0.0498)  time: 3.7879  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 720/3449]  eta: 2:53:00  lr: 0.000010  loss: 0.0534 (0.0499)  time: 3.8082  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 730/3449]  eta: 2:52:19  lr: 0.000010  loss: 0.0521 (0.0499)  time: 3.8142  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 740/3449]  eta: 2:51:42  lr: 0.000010  loss: 0.0494 (0.0498)  time: 3.7752  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 750/3449]  eta: 2:51:03  lr: 0.000010  loss: 0.0512 (0.0499)  time: 3.8029  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 760/3449]  eta: 2:50:21  lr: 0.000010  loss: 0.0558 (0.0499)  time: 3.7387  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 770/3449]  eta: 2:49:40  lr: 0.000010  loss: 0.0535 (0.0499)  time: 3.7078  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 780/3449]  eta: 2:49:03  lr: 0.000010  loss: 0.0512 (0.0499)  time: 3.7689  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 790/3449]  eta: 2:48:25  lr: 0.000010  loss: 0.0467 (0.0498)  time: 3.8018  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 800/3449]  eta: 2:47:42  lr: 0.000010  loss: 0.0429 (0.0498)  time: 3.7221  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 810/3449]  eta: 2:47:03  lr: 0.000010  loss: 0.0371 (0.0497)  time: 3.7090  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 820/3449]  eta: 2:46:23  lr: 0.000010  loss: 0.0480 (0.0497)  time: 3.7571  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 830/3449]  eta: 2:45:45  lr: 0.000010  loss: 0.0605 (0.0498)  time: 3.7666  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 840/3449]  eta: 2:45:06  lr: 0.000010  loss: 0.0516 (0.0497)  time: 3.7704  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 850/3449]  eta: 2:44:26  lr: 0.000010  loss: 0.0489 (0.0498)  time: 3.7583  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 860/3449]  eta: 2:43:46  lr: 0.000010  loss: 0.0581 (0.0498)  time: 3.7376  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 870/3449]  eta: 2:43:08  lr: 0.000010  loss: 0.0580 (0.0498)  time: 3.7604  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 880/3449]  eta: 2:42:34  lr: 0.000010  loss: 0.0580 (0.0499)  time: 3.8545  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 890/3449]  eta: 2:41:53  lr: 0.000010  loss: 0.0547 (0.0499)  time: 3.8013  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 900/3449]  eta: 2:41:17  lr: 0.000010  loss: 0.0555 (0.0500)  time: 3.7883  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 910/3449]  eta: 2:40:40  lr: 0.000010  loss: 0.0564 (0.0500)  time: 3.8514  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 920/3449]  eta: 2:40:01  lr: 0.000010  loss: 0.0532 (0.0500)  time: 3.7898  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 930/3449]  eta: 2:39:20  lr: 0.000010  loss: 0.0472 (0.0499)  time: 3.7259  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 940/3449]  eta: 2:38:39  lr: 0.000010  loss: 0.0375 (0.0499)  time: 3.6812  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 950/3449]  eta: 2:38:02  lr: 0.000010  loss: 0.0439 (0.0499)  time: 3.7529  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 960/3449]  eta: 2:37:22  lr: 0.000010  loss: 0.0576 (0.0499)  time: 3.7711  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 970/3449]  eta: 2:36:43  lr: 0.000010  loss: 0.0585 (0.0501)  time: 3.7392  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 980/3449]  eta: 2:36:04  lr: 0.000010  loss: 0.0580 (0.0501)  time: 3.7596  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 990/3449]  eta: 2:35:24  lr: 0.000010  loss: 0.0472 (0.0500)  time: 3.7303  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1000/3449]  eta: 2:34:48  lr: 0.000010  loss: 0.0486 (0.0500)  time: 3.7787  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1010/3449]  eta: 2:34:08  lr: 0.000010  loss: 0.0476 (0.0500)  time: 3.7915  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1020/3449]  eta: 2:33:31  lr: 0.000010  loss: 0.0448 (0.0499)  time: 3.7773  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1030/3449]  eta: 2:32:52  lr: 0.000010  loss: 0.0471 (0.0499)  time: 3.7778  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1040/3449]  eta: 2:32:14  lr: 0.000010  loss: 0.0465 (0.0499)  time: 3.7662  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1050/3449]  eta: 2:31:37  lr: 0.000010  loss: 0.0465 (0.0499)  time: 3.8091  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1060/3449]  eta: 2:30:59  lr: 0.000010  loss: 0.0532 (0.0500)  time: 3.8217  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1070/3449]  eta: 2:30:22  lr: 0.000010  loss: 0.0546 (0.0500)  time: 3.8217  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1080/3449]  eta: 2:29:46  lr: 0.000010  loss: 0.0503 (0.0499)  time: 3.8377  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1090/3449]  eta: 2:29:07  lr: 0.000010  loss: 0.0508 (0.0500)  time: 3.8163  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1100/3449]  eta: 2:28:29  lr: 0.000010  loss: 0.0508 (0.0499)  time: 3.7752  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1110/3449]  eta: 2:27:51  lr: 0.000010  loss: 0.0419 (0.0499)  time: 3.7693  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1120/3449]  eta: 2:27:12  lr: 0.000010  loss: 0.0465 (0.0499)  time: 3.7608  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1130/3449]  eta: 2:26:34  lr: 0.000010  loss: 0.0505 (0.0499)  time: 3.7707  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1140/3449]  eta: 2:25:56  lr: 0.000010  loss: 0.0586 (0.0500)  time: 3.8058  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1150/3449]  eta: 2:25:19  lr: 0.000010  loss: 0.0481 (0.0499)  time: 3.8292  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1160/3449]  eta: 2:24:41  lr: 0.000010  loss: 0.0511 (0.0500)  time: 3.8138  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1170/3449]  eta: 2:24:01  lr: 0.000010  loss: 0.0537 (0.0500)  time: 3.7342  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1180/3449]  eta: 2:23:24  lr: 0.000010  loss: 0.0553 (0.0501)  time: 3.7451  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1190/3449]  eta: 2:22:47  lr: 0.000010  loss: 0.0481 (0.0501)  time: 3.8321  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1200/3449]  eta: 2:22:09  lr: 0.000010  loss: 0.0413 (0.0500)  time: 3.8091  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1210/3449]  eta: 2:21:32  lr: 0.000010  loss: 0.0483 (0.0500)  time: 3.8223  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1220/3449]  eta: 2:20:52  lr: 0.000010  loss: 0.0507 (0.0500)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1230/3449]  eta: 2:20:13  lr: 0.000010  loss: 0.0513 (0.0500)  time: 3.6928  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1240/3449]  eta: 2:19:36  lr: 0.000010  loss: 0.0508 (0.0500)  time: 3.7969  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1250/3449]  eta: 2:18:59  lr: 0.000010  loss: 0.0547 (0.0501)  time: 3.8458  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1260/3449]  eta: 2:18:23  lr: 0.000010  loss: 0.0559 (0.0501)  time: 3.8593  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1270/3449]  eta: 2:17:44  lr: 0.000010  loss: 0.0548 (0.0501)  time: 3.8318  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1280/3449]  eta: 2:17:06  lr: 0.000010  loss: 0.0423 (0.0500)  time: 3.7790  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1290/3449]  eta: 2:16:29  lr: 0.000010  loss: 0.0445 (0.0500)  time: 3.8177  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1300/3449]  eta: 2:15:51  lr: 0.000010  loss: 0.0508 (0.0500)  time: 3.7958  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1310/3449]  eta: 2:15:13  lr: 0.000010  loss: 0.0504 (0.0500)  time: 3.7709  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1320/3449]  eta: 2:14:34  lr: 0.000010  loss: 0.0444 (0.0499)  time: 3.7563  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1330/3449]  eta: 2:13:54  lr: 0.000010  loss: 0.0523 (0.0500)  time: 3.7144  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1340/3449]  eta: 2:13:18  lr: 0.000010  loss: 0.0450 (0.0499)  time: 3.8050  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1350/3449]  eta: 2:12:37  lr: 0.000010  loss: 0.0372 (0.0499)  time: 3.7451  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1360/3449]  eta: 2:11:58  lr: 0.000010  loss: 0.0533 (0.0499)  time: 3.6684  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1370/3449]  eta: 2:11:24  lr: 0.000010  loss: 0.0648 (0.0500)  time: 3.8974  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1380/3449]  eta: 2:10:46  lr: 0.000010  loss: 0.0570 (0.0500)  time: 3.8868  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1390/3449]  eta: 2:10:09  lr: 0.000010  loss: 0.0525 (0.0500)  time: 3.7911  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1400/3449]  eta: 2:09:31  lr: 0.000010  loss: 0.0505 (0.0501)  time: 3.8243  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1410/3449]  eta: 2:08:52  lr: 0.000010  loss: 0.0552 (0.0501)  time: 3.7836  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1420/3449]  eta: 2:08:16  lr: 0.000010  loss: 0.0546 (0.0502)  time: 3.8225  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1430/3449]  eta: 2:07:40  lr: 0.000010  loss: 0.0510 (0.0501)  time: 3.8983  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1440/3449]  eta: 2:07:01  lr: 0.000010  loss: 0.0446 (0.0501)  time: 3.8245  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1450/3449]  eta: 2:06:24  lr: 0.000010  loss: 0.0465 (0.0501)  time: 3.8007  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1460/3449]  eta: 2:05:46  lr: 0.000010  loss: 0.0470 (0.0501)  time: 3.8526  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1470/3449]  eta: 2:05:08  lr: 0.000010  loss: 0.0490 (0.0501)  time: 3.7988  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1480/3449]  eta: 2:04:30  lr: 0.000010  loss: 0.0556 (0.0501)  time: 3.7868  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1490/3449]  eta: 2:03:52  lr: 0.000010  loss: 0.0480 (0.0500)  time: 3.7936  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1500/3449]  eta: 2:03:16  lr: 0.000010  loss: 0.0394 (0.0500)  time: 3.8494  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1510/3449]  eta: 2:02:37  lr: 0.000010  loss: 0.0569 (0.0500)  time: 3.8161  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1520/3449]  eta: 2:01:58  lr: 0.000010  loss: 0.0575 (0.0501)  time: 3.7299  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1530/3449]  eta: 2:01:19  lr: 0.000010  loss: 0.0552 (0.0501)  time: 3.7000  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1540/3449]  eta: 2:00:41  lr: 0.000010  loss: 0.0552 (0.0500)  time: 3.7296  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1550/3449]  eta: 2:00:03  lr: 0.000010  loss: 0.0542 (0.0500)  time: 3.7997  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1560/3449]  eta: 1:59:25  lr: 0.000010  loss: 0.0533 (0.0500)  time: 3.8061  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1570/3449]  eta: 1:58:47  lr: 0.000010  loss: 0.0522 (0.0500)  time: 3.7805  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1580/3449]  eta: 1:58:11  lr: 0.000010  loss: 0.0506 (0.0500)  time: 3.8452  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1590/3449]  eta: 1:57:32  lr: 0.000010  loss: 0.0520 (0.0500)  time: 3.8511  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1600/3449]  eta: 1:56:53  lr: 0.000010  loss: 0.0579 (0.0501)  time: 3.7372  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1610/3449]  eta: 1:56:15  lr: 0.000010  loss: 0.0582 (0.0501)  time: 3.7122  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1620/3449]  eta: 1:55:36  lr: 0.000010  loss: 0.0533 (0.0501)  time: 3.7178  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1630/3449]  eta: 1:54:57  lr: 0.000010  loss: 0.0457 (0.0501)  time: 3.7303  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1640/3449]  eta: 1:54:19  lr: 0.000010  loss: 0.0552 (0.0502)  time: 3.7517  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1650/3449]  eta: 1:53:41  lr: 0.000010  loss: 0.0550 (0.0502)  time: 3.7808  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1660/3449]  eta: 1:53:05  lr: 0.000010  loss: 0.0512 (0.0502)  time: 3.8839  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1670/3449]  eta: 1:52:27  lr: 0.000010  loss: 0.0561 (0.0502)  time: 3.8678  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1680/3449]  eta: 1:51:49  lr: 0.000010  loss: 0.0544 (0.0501)  time: 3.7957  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1690/3449]  eta: 1:51:11  lr: 0.000010  loss: 0.0472 (0.0501)  time: 3.7698  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1700/3449]  eta: 1:50:32  lr: 0.000010  loss: 0.0502 (0.0501)  time: 3.6987  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1710/3449]  eta: 1:49:53  lr: 0.000010  loss: 0.0499 (0.0501)  time: 3.7224  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1720/3449]  eta: 1:49:15  lr: 0.000010  loss: 0.0454 (0.0501)  time: 3.7524  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1730/3449]  eta: 1:48:37  lr: 0.000010  loss: 0.0474 (0.0500)  time: 3.7867  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1740/3449]  eta: 1:48:00  lr: 0.000010  loss: 0.0529 (0.0501)  time: 3.8268  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1750/3449]  eta: 1:47:23  lr: 0.000010  loss: 0.0538 (0.0501)  time: 3.8543  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1760/3449]  eta: 1:46:45  lr: 0.000010  loss: 0.0522 (0.0501)  time: 3.8519  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1770/3449]  eta: 1:46:07  lr: 0.000010  loss: 0.0451 (0.0500)  time: 3.8021  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1780/3449]  eta: 1:45:29  lr: 0.000010  loss: 0.0364 (0.0500)  time: 3.7809  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1790/3449]  eta: 1:44:50  lr: 0.000010  loss: 0.0543 (0.0500)  time: 3.7418  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1800/3449]  eta: 1:44:11  lr: 0.000010  loss: 0.0550 (0.0500)  time: 3.6869  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1810/3449]  eta: 1:43:33  lr: 0.000010  loss: 0.0482 (0.0500)  time: 3.6876  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1820/3449]  eta: 1:42:55  lr: 0.000010  loss: 0.0437 (0.0500)  time: 3.7535  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1830/3449]  eta: 1:42:17  lr: 0.000010  loss: 0.0550 (0.0500)  time: 3.8214  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1840/3449]  eta: 1:41:40  lr: 0.000010  loss: 0.0558 (0.0500)  time: 3.8556  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1850/3449]  eta: 1:41:03  lr: 0.000010  loss: 0.0525 (0.0501)  time: 3.8902  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1860/3449]  eta: 1:40:25  lr: 0.000010  loss: 0.0505 (0.0501)  time: 3.8254  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1870/3449]  eta: 1:39:47  lr: 0.000010  loss: 0.0463 (0.0501)  time: 3.7522  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1880/3449]  eta: 1:39:08  lr: 0.000010  loss: 0.0463 (0.0501)  time: 3.7311  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1890/3449]  eta: 1:38:31  lr: 0.000010  loss: 0.0520 (0.0501)  time: 3.7835  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1900/3449]  eta: 1:37:53  lr: 0.000010  loss: 0.0522 (0.0501)  time: 3.8411  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1910/3449]  eta: 1:37:15  lr: 0.000010  loss: 0.0574 (0.0501)  time: 3.7684  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1920/3449]  eta: 1:36:36  lr: 0.000010  loss: 0.0536 (0.0501)  time: 3.7342  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1930/3449]  eta: 1:35:58  lr: 0.000010  loss: 0.0441 (0.0501)  time: 3.7466  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1940/3449]  eta: 1:35:21  lr: 0.000010  loss: 0.0478 (0.0501)  time: 3.8163  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1950/3449]  eta: 1:34:43  lr: 0.000010  loss: 0.0432 (0.0501)  time: 3.8600  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1960/3449]  eta: 1:34:05  lr: 0.000010  loss: 0.0534 (0.0501)  time: 3.7685  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1970/3449]  eta: 1:33:27  lr: 0.000010  loss: 0.0585 (0.0501)  time: 3.7344  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1980/3449]  eta: 1:32:48  lr: 0.000010  loss: 0.0495 (0.0501)  time: 3.7293  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [1990/3449]  eta: 1:32:09  lr: 0.000010  loss: 0.0592 (0.0502)  time: 3.6923  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2000/3449]  eta: 1:31:32  lr: 0.000010  loss: 0.0547 (0.0501)  time: 3.8173  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2010/3449]  eta: 1:30:56  lr: 0.000010  loss: 0.0464 (0.0501)  time: 3.9316  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2020/3449]  eta: 1:30:18  lr: 0.000010  loss: 0.0459 (0.0500)  time: 3.8808  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2030/3449]  eta: 1:29:40  lr: 0.000010  loss: 0.0424 (0.0500)  time: 3.8249  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2040/3449]  eta: 1:29:02  lr: 0.000010  loss: 0.0487 (0.0500)  time: 3.7942  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2050/3449]  eta: 1:28:24  lr: 0.000010  loss: 0.0454 (0.0500)  time: 3.7872  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2060/3449]  eta: 1:27:47  lr: 0.000010  loss: 0.0450 (0.0500)  time: 3.8248  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2070/3449]  eta: 1:27:10  lr: 0.000010  loss: 0.0450 (0.0500)  time: 3.8762  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2080/3449]  eta: 1:26:32  lr: 0.000010  loss: 0.0402 (0.0500)  time: 3.8405  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2090/3449]  eta: 1:25:54  lr: 0.000010  loss: 0.0402 (0.0499)  time: 3.7881  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2100/3449]  eta: 1:25:16  lr: 0.000010  loss: 0.0432 (0.0499)  time: 3.8007  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2110/3449]  eta: 1:24:38  lr: 0.000010  loss: 0.0412 (0.0498)  time: 3.8234  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2120/3449]  eta: 1:24:00  lr: 0.000010  loss: 0.0423 (0.0498)  time: 3.8147  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2130/3449]  eta: 1:23:22  lr: 0.000010  loss: 0.0519 (0.0498)  time: 3.7603  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2140/3449]  eta: 1:22:44  lr: 0.000010  loss: 0.0495 (0.0498)  time: 3.7766  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2150/3449]  eta: 1:22:06  lr: 0.000010  loss: 0.0543 (0.0499)  time: 3.8259  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2160/3449]  eta: 1:21:29  lr: 0.000010  loss: 0.0554 (0.0499)  time: 3.8455  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2170/3449]  eta: 1:20:51  lr: 0.000010  loss: 0.0481 (0.0498)  time: 3.8214  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2180/3449]  eta: 1:20:13  lr: 0.000010  loss: 0.0428 (0.0498)  time: 3.8044  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2190/3449]  eta: 1:19:35  lr: 0.000010  loss: 0.0501 (0.0499)  time: 3.7967  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2200/3449]  eta: 1:18:58  lr: 0.000010  loss: 0.0541 (0.0499)  time: 3.8254  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2210/3449]  eta: 1:18:20  lr: 0.000010  loss: 0.0523 (0.0499)  time: 3.9042  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2220/3449]  eta: 1:17:42  lr: 0.000010  loss: 0.0465 (0.0499)  time: 3.7857  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2230/3449]  eta: 1:17:04  lr: 0.000010  loss: 0.0545 (0.0499)  time: 3.7147  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2240/3449]  eta: 1:16:25  lr: 0.000010  loss: 0.0537 (0.0499)  time: 3.7344  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2250/3449]  eta: 1:15:47  lr: 0.000010  loss: 0.0494 (0.0499)  time: 3.7564  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2260/3449]  eta: 1:15:09  lr: 0.000010  loss: 0.0518 (0.0499)  time: 3.8003  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2270/3449]  eta: 1:14:31  lr: 0.000010  loss: 0.0402 (0.0499)  time: 3.7340  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2280/3449]  eta: 1:13:53  lr: 0.000010  loss: 0.0383 (0.0498)  time: 3.7258  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2290/3449]  eta: 1:13:15  lr: 0.000010  loss: 0.0444 (0.0498)  time: 3.7415  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2300/3449]  eta: 1:12:37  lr: 0.000010  loss: 0.0554 (0.0499)  time: 3.8285  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2310/3449]  eta: 1:11:59  lr: 0.000010  loss: 0.0459 (0.0498)  time: 3.7900  data: 0.0003  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2320/3449]  eta: 1:11:21  lr: 0.000010  loss: 0.0452 (0.0498)  time: 3.7437  data: 0.0003  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2330/3449]  eta: 1:10:43  lr: 0.000010  loss: 0.0488 (0.0498)  time: 3.8404  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2340/3449]  eta: 1:10:06  lr: 0.000010  loss: 0.0512 (0.0498)  time: 3.8739  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2350/3449]  eta: 1:09:28  lr: 0.000010  loss: 0.0516 (0.0498)  time: 3.8557  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2360/3449]  eta: 1:08:50  lr: 0.000010  loss: 0.0497 (0.0498)  time: 3.8253  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2370/3449]  eta: 1:08:13  lr: 0.000010  loss: 0.0526 (0.0499)  time: 3.8565  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2380/3449]  eta: 1:07:35  lr: 0.000010  loss: 0.0580 (0.0499)  time: 3.8536  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2390/3449]  eta: 1:06:57  lr: 0.000010  loss: 0.0504 (0.0498)  time: 3.7948  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2400/3449]  eta: 1:06:19  lr: 0.000010  loss: 0.0458 (0.0498)  time: 3.7729  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2410/3449]  eta: 1:05:41  lr: 0.000010  loss: 0.0486 (0.0499)  time: 3.8127  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2420/3449]  eta: 1:05:04  lr: 0.000010  loss: 0.0633 (0.0499)  time: 3.8317  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2430/3449]  eta: 1:04:25  lr: 0.000010  loss: 0.0582 (0.0499)  time: 3.7588  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2440/3449]  eta: 1:03:47  lr: 0.000010  loss: 0.0522 (0.0499)  time: 3.7657  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2450/3449]  eta: 1:03:09  lr: 0.000010  loss: 0.0503 (0.0499)  time: 3.7889  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2460/3449]  eta: 1:02:31  lr: 0.000010  loss: 0.0499 (0.0499)  time: 3.7791  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2470/3449]  eta: 1:01:54  lr: 0.000010  loss: 0.0524 (0.0499)  time: 3.8361  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2480/3449]  eta: 1:01:16  lr: 0.000010  loss: 0.0549 (0.0499)  time: 3.7984  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2490/3449]  eta: 1:00:38  lr: 0.000010  loss: 0.0518 (0.0499)  time: 3.7779  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2500/3449]  eta: 1:00:00  lr: 0.000010  loss: 0.0509 (0.0499)  time: 3.7881  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2510/3449]  eta: 0:59:21  lr: 0.000010  loss: 0.0499 (0.0499)  time: 3.7269  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2520/3449]  eta: 0:58:44  lr: 0.000010  loss: 0.0487 (0.0499)  time: 3.7571  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2530/3449]  eta: 0:58:05  lr: 0.000010  loss: 0.0489 (0.0499)  time: 3.7551  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2540/3449]  eta: 0:57:27  lr: 0.000010  loss: 0.0413 (0.0498)  time: 3.6808  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2550/3449]  eta: 0:56:49  lr: 0.000010  loss: 0.0356 (0.0498)  time: 3.6990  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2560/3449]  eta: 0:56:10  lr: 0.000010  loss: 0.0470 (0.0498)  time: 3.7030  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2570/3449]  eta: 0:55:32  lr: 0.000010  loss: 0.0528 (0.0498)  time: 3.7218  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2580/3449]  eta: 0:54:54  lr: 0.000010  loss: 0.0456 (0.0498)  time: 3.6916  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2590/3449]  eta: 0:54:16  lr: 0.000010  loss: 0.0502 (0.0498)  time: 3.6655  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2600/3449]  eta: 0:53:38  lr: 0.000010  loss: 0.0497 (0.0498)  time: 3.7584  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2610/3449]  eta: 0:52:59  lr: 0.000010  loss: 0.0451 (0.0498)  time: 3.7312  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2620/3449]  eta: 0:52:21  lr: 0.000010  loss: 0.0443 (0.0498)  time: 3.6946  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2630/3449]  eta: 0:51:43  lr: 0.000010  loss: 0.0434 (0.0497)  time: 3.7594  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2640/3449]  eta: 0:51:06  lr: 0.000010  loss: 0.0446 (0.0497)  time: 3.7862  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2650/3449]  eta: 0:50:28  lr: 0.000010  loss: 0.0446 (0.0497)  time: 3.7820  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2660/3449]  eta: 0:49:50  lr: 0.000010  loss: 0.0579 (0.0498)  time: 3.7644  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2670/3449]  eta: 0:49:12  lr: 0.000010  loss: 0.0565 (0.0498)  time: 3.7859  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2680/3449]  eta: 0:48:34  lr: 0.000010  loss: 0.0499 (0.0498)  time: 3.8286  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2690/3449]  eta: 0:47:56  lr: 0.000010  loss: 0.0562 (0.0498)  time: 3.8055  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2700/3449]  eta: 0:47:18  lr: 0.000010  loss: 0.0562 (0.0498)  time: 3.8032  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2710/3449]  eta: 0:46:40  lr: 0.000010  loss: 0.0555 (0.0498)  time: 3.8105  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2720/3449]  eta: 0:46:02  lr: 0.000010  loss: 0.0523 (0.0498)  time: 3.7620  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2730/3449]  eta: 0:45:24  lr: 0.000010  loss: 0.0458 (0.0498)  time: 3.7493  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2740/3449]  eta: 0:44:46  lr: 0.000010  loss: 0.0489 (0.0498)  time: 3.7263  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2750/3449]  eta: 0:44:08  lr: 0.000010  loss: 0.0513 (0.0498)  time: 3.7367  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2760/3449]  eta: 0:43:31  lr: 0.000010  loss: 0.0377 (0.0497)  time: 3.8077  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2770/3449]  eta: 0:42:53  lr: 0.000010  loss: 0.0438 (0.0497)  time: 3.7929  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2780/3449]  eta: 0:42:14  lr: 0.000010  loss: 0.0507 (0.0497)  time: 3.7250  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2790/3449]  eta: 0:41:36  lr: 0.000010  loss: 0.0431 (0.0497)  time: 3.7355  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2800/3449]  eta: 0:40:59  lr: 0.000010  loss: 0.0424 (0.0497)  time: 3.7830  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2810/3449]  eta: 0:40:21  lr: 0.000010  loss: 0.0484 (0.0497)  time: 3.8430  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2820/3449]  eta: 0:39:43  lr: 0.000010  loss: 0.0540 (0.0497)  time: 3.8504  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2830/3449]  eta: 0:39:05  lr: 0.000010  loss: 0.0568 (0.0497)  time: 3.8654  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2840/3449]  eta: 0:38:27  lr: 0.000010  loss: 0.0433 (0.0496)  time: 3.8161  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2850/3449]  eta: 0:37:50  lr: 0.000010  loss: 0.0445 (0.0497)  time: 3.7586  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2860/3449]  eta: 0:37:12  lr: 0.000010  loss: 0.0532 (0.0497)  time: 3.7618  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2870/3449]  eta: 0:36:34  lr: 0.000010  loss: 0.0417 (0.0496)  time: 3.7719  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2880/3449]  eta: 0:35:56  lr: 0.000010  loss: 0.0380 (0.0496)  time: 3.7831  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2890/3449]  eta: 0:35:18  lr: 0.000010  loss: 0.0403 (0.0496)  time: 3.7402  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2900/3449]  eta: 0:34:40  lr: 0.000010  loss: 0.0420 (0.0495)  time: 3.7308  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2910/3449]  eta: 0:34:02  lr: 0.000010  loss: 0.0477 (0.0495)  time: 3.7224  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2920/3449]  eta: 0:33:24  lr: 0.000010  loss: 0.0477 (0.0495)  time: 3.7189  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2930/3449]  eta: 0:32:46  lr: 0.000010  loss: 0.0474 (0.0495)  time: 3.7697  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2940/3449]  eta: 0:32:08  lr: 0.000010  loss: 0.0509 (0.0495)  time: 3.8021  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2950/3449]  eta: 0:31:30  lr: 0.000010  loss: 0.0488 (0.0495)  time: 3.7683  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2960/3449]  eta: 0:30:52  lr: 0.000010  loss: 0.0520 (0.0495)  time: 3.7775  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2970/3449]  eta: 0:30:14  lr: 0.000010  loss: 0.0553 (0.0496)  time: 3.8164  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2980/3449]  eta: 0:29:36  lr: 0.000010  loss: 0.0474 (0.0495)  time: 3.7839  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [2990/3449]  eta: 0:28:58  lr: 0.000010  loss: 0.0447 (0.0495)  time: 3.7388  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3000/3449]  eta: 0:28:20  lr: 0.000010  loss: 0.0447 (0.0495)  time: 3.7277  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3010/3449]  eta: 0:27:42  lr: 0.000010  loss: 0.0443 (0.0495)  time: 3.7357  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3020/3449]  eta: 0:27:05  lr: 0.000010  loss: 0.0466 (0.0495)  time: 3.7961  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3030/3449]  eta: 0:26:27  lr: 0.000010  loss: 0.0495 (0.0495)  time: 3.7933  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3040/3449]  eta: 0:25:49  lr: 0.000010  loss: 0.0617 (0.0496)  time: 3.7526  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3050/3449]  eta: 0:25:11  lr: 0.000010  loss: 0.0591 (0.0496)  time: 3.8189  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3060/3449]  eta: 0:24:33  lr: 0.000010  loss: 0.0544 (0.0496)  time: 3.8418  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3070/3449]  eta: 0:23:55  lr: 0.000010  loss: 0.0515 (0.0496)  time: 3.8089  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3080/3449]  eta: 0:23:17  lr: 0.000010  loss: 0.0525 (0.0496)  time: 3.7973  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3090/3449]  eta: 0:22:40  lr: 0.000010  loss: 0.0542 (0.0496)  time: 3.8400  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3100/3449]  eta: 0:22:02  lr: 0.000010  loss: 0.0503 (0.0496)  time: 3.8933  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3110/3449]  eta: 0:21:24  lr: 0.000010  loss: 0.0521 (0.0496)  time: 3.8191  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3120/3449]  eta: 0:20:46  lr: 0.000010  loss: 0.0497 (0.0496)  time: 3.7930  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3130/3449]  eta: 0:20:08  lr: 0.000010  loss: 0.0497 (0.0496)  time: 3.7817  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3140/3449]  eta: 0:19:30  lr: 0.000010  loss: 0.0558 (0.0496)  time: 3.7011  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3150/3449]  eta: 0:18:52  lr: 0.000010  loss: 0.0533 (0.0496)  time: 3.6821  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3160/3449]  eta: 0:18:14  lr: 0.000010  loss: 0.0517 (0.0496)  time: 3.7022  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3170/3449]  eta: 0:17:36  lr: 0.000010  loss: 0.0580 (0.0496)  time: 3.7636  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3180/3449]  eta: 0:16:59  lr: 0.000010  loss: 0.0586 (0.0496)  time: 3.8289  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3190/3449]  eta: 0:16:21  lr: 0.000010  loss: 0.0534 (0.0497)  time: 3.7808  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3200/3449]  eta: 0:15:43  lr: 0.000010  loss: 0.0543 (0.0497)  time: 3.7657  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3210/3449]  eta: 0:15:05  lr: 0.000010  loss: 0.0502 (0.0497)  time: 3.8050  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3220/3449]  eta: 0:14:27  lr: 0.000010  loss: 0.0480 (0.0497)  time: 3.8175  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3230/3449]  eta: 0:13:49  lr: 0.000010  loss: 0.0458 (0.0497)  time: 3.8240  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3240/3449]  eta: 0:13:11  lr: 0.000010  loss: 0.0563 (0.0497)  time: 3.8757  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3250/3449]  eta: 0:12:33  lr: 0.000010  loss: 0.0549 (0.0497)  time: 3.8806  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3260/3449]  eta: 0:11:56  lr: 0.000010  loss: 0.0472 (0.0497)  time: 3.8003  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3270/3449]  eta: 0:11:18  lr: 0.000010  loss: 0.0446 (0.0497)  time: 3.7843  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3280/3449]  eta: 0:10:40  lr: 0.000010  loss: 0.0499 (0.0497)  time: 3.8198  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3290/3449]  eta: 0:10:02  lr: 0.000010  loss: 0.0432 (0.0496)  time: 3.8082  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3300/3449]  eta: 0:09:24  lr: 0.000010  loss: 0.0455 (0.0496)  time: 3.7863  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3310/3449]  eta: 0:08:46  lr: 0.000010  loss: 0.0502 (0.0496)  time: 3.8151  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3320/3449]  eta: 0:08:08  lr: 0.000010  loss: 0.0541 (0.0497)  time: 3.7930  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3330/3449]  eta: 0:07:30  lr: 0.000010  loss: 0.0506 (0.0496)  time: 3.8351  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3340/3449]  eta: 0:06:53  lr: 0.000010  loss: 0.0501 (0.0496)  time: 3.8730  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3350/3449]  eta: 0:06:15  lr: 0.000010  loss: 0.0532 (0.0496)  time: 3.7762  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3360/3449]  eta: 0:05:37  lr: 0.000010  loss: 0.0523 (0.0496)  time: 3.7258  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3370/3449]  eta: 0:04:59  lr: 0.000010  loss: 0.0525 (0.0496)  time: 3.7721  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3380/3449]  eta: 0:04:21  lr: 0.000010  loss: 0.0533 (0.0496)  time: 3.8166  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3390/3449]  eta: 0:03:43  lr: 0.000010  loss: 0.0475 (0.0496)  time: 3.7450  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3400/3449]  eta: 0:03:05  lr: 0.000010  loss: 0.0475 (0.0496)  time: 3.6750  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3410/3449]  eta: 0:02:27  lr: 0.000010  loss: 0.0559 (0.0496)  time: 3.8403  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3420/3449]  eta: 0:01:49  lr: 0.000010  loss: 0.0567 (0.0496)  time: 3.8761  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3430/3449]  eta: 0:01:11  lr: 0.000010  loss: 0.0588 (0.0497)  time: 3.7352  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3440/3449]  eta: 0:00:34  lr: 0.000010  loss: 0.0588 (0.0497)  time: 3.6752  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [3448/3449]  eta: 0:00:03  lr: 0.000010  loss: 0.0525 (0.0497)  time: 3.7525  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:3] Total time: 3:37:47 (3.7888 s / it)\n",
      "Averaged stats: lr: 0.000010  loss: 0.0525 (0.0497)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:3]  [ 0/14]  eta: 0:04:57  loss: 0.0451 (0.0451)  time: 21.2160  data: 1.4937  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:3]  [13/14]  eta: 0:00:18  loss: 0.0378 (0.0392)  time: 18.4461  data: 0.1069  max mem: 34968\n",
      "Valid: [epoch:3] Total time: 0:04:18 (18.4547 s / it)\n",
      "Averaged stats: loss: 0.0378 (0.0392)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_3_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.039%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "/home/sunggu/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [   0/3449]  eta: 5:10:55  lr: 0.000030  loss: 0.0675 (0.0675)  time: 5.4091  data: 1.5823  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [  10/3449]  eta: 3:47:32  lr: 0.000030  loss: 0.0675 (0.0656)  time: 3.9698  data: 0.1440  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [  20/3449]  eta: 3:42:10  lr: 0.000030  loss: 0.0642 (0.1159)  time: 3.8115  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [  30/3449]  eta: 3:39:52  lr: 0.000030  loss: 0.0563 (0.0949)  time: 3.7972  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [  40/3449]  eta: 3:36:28  lr: 0.000030  loss: 0.0484 (0.0828)  time: 3.7284  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [  50/3449]  eta: 3:36:06  lr: 0.000030  loss: 0.0496 (0.0761)  time: 3.7469  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [  60/3449]  eta: 3:34:19  lr: 0.000030  loss: 0.0485 (0.0715)  time: 3.7624  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [  70/3449]  eta: 3:33:31  lr: 0.000030  loss: 0.0476 (0.0686)  time: 3.7322  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [  80/3449]  eta: 3:32:32  lr: 0.000030  loss: 0.0412 (0.0644)  time: 3.7577  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [  90/3449]  eta: 3:31:56  lr: 0.000030  loss: 0.0412 (0.0633)  time: 3.7656  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 100/3449]  eta: 3:30:26  lr: 0.000030  loss: 0.0601 (0.0620)  time: 3.7087  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 110/3449]  eta: 3:28:59  lr: 0.000030  loss: 0.0455 (0.0606)  time: 3.6181  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 120/3449]  eta: 3:28:05  lr: 0.000030  loss: 0.0493 (0.0597)  time: 3.6508  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 130/3449]  eta: 3:26:58  lr: 0.000030  loss: 0.0440 (0.0580)  time: 3.6645  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 140/3449]  eta: 3:26:23  lr: 0.000030  loss: 0.0492 (0.0576)  time: 3.6927  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 150/3449]  eta: 3:25:44  lr: 0.000030  loss: 0.0524 (0.0569)  time: 3.7442  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 160/3449]  eta: 3:25:00  lr: 0.000030  loss: 0.0525 (0.0567)  time: 3.7225  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 170/3449]  eta: 3:24:16  lr: 0.000030  loss: 0.0525 (0.0564)  time: 3.7064  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 180/3449]  eta: 3:23:44  lr: 0.000030  loss: 0.0433 (0.0552)  time: 3.7386  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 190/3449]  eta: 3:23:02  lr: 0.000030  loss: 0.0438 (0.0554)  time: 3.7409  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 200/3449]  eta: 3:22:32  lr: 0.000030  loss: 0.0595 (0.0555)  time: 3.7455  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 210/3449]  eta: 3:22:10  lr: 0.000030  loss: 0.0595 (0.0557)  time: 3.8123  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 220/3449]  eta: 3:21:19  lr: 0.000030  loss: 0.0578 (0.0556)  time: 3.7465  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 230/3449]  eta: 3:20:38  lr: 0.000030  loss: 0.0430 (0.0549)  time: 3.6829  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 240/3449]  eta: 3:19:47  lr: 0.000030  loss: 0.0404 (0.0546)  time: 3.6779  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 250/3449]  eta: 3:18:55  lr: 0.000030  loss: 0.0431 (0.0542)  time: 3.6298  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 260/3449]  eta: 3:18:09  lr: 0.000030  loss: 0.0477 (0.0539)  time: 3.6411  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 270/3449]  eta: 3:17:33  lr: 0.000030  loss: 0.0458 (0.0534)  time: 3.7006  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 280/3449]  eta: 3:16:55  lr: 0.000030  loss: 0.0409 (0.0532)  time: 3.7312  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 290/3449]  eta: 3:16:27  lr: 0.000030  loss: 0.0411 (0.0527)  time: 3.7665  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 300/3449]  eta: 3:15:57  lr: 0.000030  loss: 0.0449 (0.0526)  time: 3.8043  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 310/3449]  eta: 3:15:31  lr: 0.000030  loss: 0.0485 (0.0524)  time: 3.8233  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 320/3449]  eta: 3:15:03  lr: 0.000030  loss: 0.0497 (0.0525)  time: 3.8427  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 330/3449]  eta: 3:14:22  lr: 0.000030  loss: 0.0543 (0.0525)  time: 3.7704  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 340/3449]  eta: 3:13:58  lr: 0.000030  loss: 0.0460 (0.0522)  time: 3.7943  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 350/3449]  eta: 3:13:12  lr: 0.000030  loss: 0.0483 (0.0522)  time: 3.7641  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 360/3449]  eta: 3:12:36  lr: 0.000030  loss: 0.0583 (0.0524)  time: 3.7021  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 370/3449]  eta: 3:11:58  lr: 0.000030  loss: 0.0583 (0.0525)  time: 3.7445  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 380/3449]  eta: 3:11:26  lr: 0.000030  loss: 0.0518 (0.0524)  time: 3.7727  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 390/3449]  eta: 3:10:56  lr: 0.000030  loss: 0.0466 (0.0525)  time: 3.8237  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 400/3449]  eta: 3:10:14  lr: 0.000030  loss: 0.0547 (0.0524)  time: 3.7611  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 410/3449]  eta: 3:09:35  lr: 0.000030  loss: 0.0536 (0.0523)  time: 3.7070  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 420/3449]  eta: 3:09:03  lr: 0.000030  loss: 0.0556 (0.0525)  time: 3.7683  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 430/3449]  eta: 3:08:25  lr: 0.000030  loss: 0.0495 (0.0524)  time: 3.7736  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 440/3449]  eta: 3:07:46  lr: 0.000030  loss: 0.0495 (0.0525)  time: 3.7297  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 450/3449]  eta: 3:07:08  lr: 0.000030  loss: 0.0573 (0.0525)  time: 3.7336  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 460/3449]  eta: 3:06:37  lr: 0.000030  loss: 0.0558 (0.0527)  time: 3.7889  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 470/3449]  eta: 3:06:00  lr: 0.000030  loss: 0.0545 (0.0526)  time: 3.7942  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 480/3449]  eta: 3:05:19  lr: 0.000030  loss: 0.0571 (0.0527)  time: 3.7228  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 490/3449]  eta: 3:04:38  lr: 0.000030  loss: 0.0492 (0.0526)  time: 3.6928  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 500/3449]  eta: 3:04:04  lr: 0.000030  loss: 0.0492 (0.0525)  time: 3.7455  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 510/3449]  eta: 3:03:32  lr: 0.000030  loss: 0.0522 (0.0526)  time: 3.8169  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 520/3449]  eta: 3:02:57  lr: 0.000030  loss: 0.0522 (0.0524)  time: 3.8101  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 530/3449]  eta: 3:02:15  lr: 0.000030  loss: 0.0491 (0.0524)  time: 3.7258  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 540/3449]  eta: 3:01:44  lr: 0.000030  loss: 0.0478 (0.0524)  time: 3.7726  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 550/3449]  eta: 3:01:07  lr: 0.000030  loss: 0.0547 (0.0524)  time: 3.8190  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 560/3449]  eta: 3:00:28  lr: 0.000030  loss: 0.0545 (0.0524)  time: 3.7346  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 570/3449]  eta: 2:59:54  lr: 0.000030  loss: 0.0545 (0.0525)  time: 3.7603  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 580/3449]  eta: 2:59:14  lr: 0.000030  loss: 0.0550 (0.0526)  time: 3.7562  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 590/3449]  eta: 2:58:39  lr: 0.000030  loss: 0.0617 (0.0527)  time: 3.7571  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 600/3449]  eta: 2:58:09  lr: 0.000030  loss: 0.0580 (0.0526)  time: 3.8551  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 610/3449]  eta: 2:57:30  lr: 0.000030  loss: 0.0540 (0.0527)  time: 3.8095  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 620/3449]  eta: 2:56:51  lr: 0.000030  loss: 0.0544 (0.0526)  time: 3.7204  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 630/3449]  eta: 2:56:14  lr: 0.000030  loss: 0.0457 (0.0525)  time: 3.7422  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 640/3449]  eta: 2:55:36  lr: 0.000030  loss: 0.0425 (0.0524)  time: 3.7541  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 650/3449]  eta: 2:54:58  lr: 0.000030  loss: 0.0460 (0.0522)  time: 3.7398  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 660/3449]  eta: 2:54:24  lr: 0.000030  loss: 0.0528 (0.0523)  time: 3.7830  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 670/3449]  eta: 2:53:49  lr: 0.000030  loss: 0.0575 (0.0524)  time: 3.8240  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 680/3449]  eta: 2:53:18  lr: 0.000030  loss: 0.0596 (0.0524)  time: 3.8629  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 690/3449]  eta: 2:52:43  lr: 0.000030  loss: 0.0574 (0.0524)  time: 3.8713  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 700/3449]  eta: 2:52:04  lr: 0.000030  loss: 0.0470 (0.0523)  time: 3.7707  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 710/3449]  eta: 2:51:26  lr: 0.000030  loss: 0.0567 (0.0524)  time: 3.7215  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 720/3449]  eta: 2:50:46  lr: 0.000030  loss: 0.0586 (0.0525)  time: 3.7210  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 730/3449]  eta: 2:50:09  lr: 0.000030  loss: 0.0544 (0.0525)  time: 3.7357  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 740/3449]  eta: 2:49:34  lr: 0.000030  loss: 0.0481 (0.0524)  time: 3.7895  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 750/3449]  eta: 2:49:01  lr: 0.000030  loss: 0.0479 (0.0524)  time: 3.8514  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 760/3449]  eta: 2:48:28  lr: 0.000030  loss: 0.0562 (0.0525)  time: 3.8903  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 770/3449]  eta: 2:47:54  lr: 0.000030  loss: 0.0598 (0.0525)  time: 3.8781  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 780/3449]  eta: 2:47:17  lr: 0.000030  loss: 0.0536 (0.0525)  time: 3.8236  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 790/3449]  eta: 2:46:43  lr: 0.000030  loss: 0.0506 (0.0524)  time: 3.8150  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 800/3449]  eta: 2:46:06  lr: 0.000030  loss: 0.0485 (0.0524)  time: 3.8135  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 810/3449]  eta: 2:45:27  lr: 0.000030  loss: 0.0392 (0.0523)  time: 3.7616  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 820/3449]  eta: 2:44:48  lr: 0.000030  loss: 0.0368 (0.0522)  time: 3.7303  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 830/3449]  eta: 2:44:13  lr: 0.000030  loss: 0.0452 (0.0522)  time: 3.7730  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 840/3449]  eta: 2:43:37  lr: 0.000030  loss: 0.0525 (0.0522)  time: 3.8328  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 850/3449]  eta: 2:43:06  lr: 0.000030  loss: 0.0525 (0.0522)  time: 3.9055  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 860/3449]  eta: 2:42:25  lr: 0.000030  loss: 0.0558 (0.0522)  time: 3.8135  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 870/3449]  eta: 2:41:48  lr: 0.000030  loss: 0.0564 (0.0523)  time: 3.7236  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 880/3449]  eta: 2:41:11  lr: 0.000030  loss: 0.0550 (0.0522)  time: 3.7856  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 890/3449]  eta: 2:40:34  lr: 0.000030  loss: 0.0556 (0.0523)  time: 3.7701  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 900/3449]  eta: 2:39:55  lr: 0.000030  loss: 0.0585 (0.0524)  time: 3.7434  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 910/3449]  eta: 2:39:17  lr: 0.000030  loss: 0.0560 (0.0523)  time: 3.7460  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 920/3449]  eta: 2:38:38  lr: 0.000030  loss: 0.0530 (0.0524)  time: 3.7361  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 930/3449]  eta: 2:37:57  lr: 0.000030  loss: 0.0558 (0.0523)  time: 3.6811  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 940/3449]  eta: 2:37:20  lr: 0.000030  loss: 0.0549 (0.0522)  time: 3.7148  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 950/3449]  eta: 2:36:42  lr: 0.000030  loss: 0.0597 (0.0523)  time: 3.7678  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 960/3449]  eta: 2:36:08  lr: 0.000030  loss: 0.0622 (0.0523)  time: 3.8253  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 970/3449]  eta: 2:35:33  lr: 0.000030  loss: 0.0628 (0.0524)  time: 3.8832  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 980/3449]  eta: 2:34:55  lr: 0.000030  loss: 0.0540 (0.0523)  time: 3.8158  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 990/3449]  eta: 2:34:17  lr: 0.000030  loss: 0.0447 (0.0522)  time: 3.7549  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1000/3449]  eta: 2:33:40  lr: 0.000030  loss: 0.0467 (0.0522)  time: 3.7690  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1010/3449]  eta: 2:33:04  lr: 0.000030  loss: 0.0443 (0.0520)  time: 3.8008  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1020/3449]  eta: 2:32:27  lr: 0.000030  loss: 0.0408 (0.0520)  time: 3.8112  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1030/3449]  eta: 2:31:47  lr: 0.000030  loss: 0.0493 (0.0519)  time: 3.7402  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1040/3449]  eta: 2:31:09  lr: 0.000030  loss: 0.0474 (0.0519)  time: 3.7049  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1050/3449]  eta: 2:30:32  lr: 0.000030  loss: 0.0474 (0.0519)  time: 3.7713  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1060/3449]  eta: 2:29:53  lr: 0.000030  loss: 0.0572 (0.0519)  time: 3.7544  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1070/3449]  eta: 2:29:17  lr: 0.000030  loss: 0.0572 (0.0520)  time: 3.7606  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1080/3449]  eta: 2:28:41  lr: 0.000030  loss: 0.0541 (0.0520)  time: 3.8288  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1090/3449]  eta: 2:28:08  lr: 0.000030  loss: 0.0545 (0.0520)  time: 3.9038  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1100/3449]  eta: 2:27:29  lr: 0.000030  loss: 0.0558 (0.0520)  time: 3.8588  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1110/3449]  eta: 2:26:51  lr: 0.000030  loss: 0.0567 (0.0521)  time: 3.7451  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1120/3449]  eta: 2:26:14  lr: 0.000030  loss: 0.0532 (0.0521)  time: 3.7704  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1130/3449]  eta: 2:25:39  lr: 0.000030  loss: 0.0612 (0.0522)  time: 3.8401  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1140/3449]  eta: 2:25:00  lr: 0.000030  loss: 0.0594 (0.0522)  time: 3.8061  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1150/3449]  eta: 2:24:23  lr: 0.000030  loss: 0.0468 (0.0520)  time: 3.7532  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1160/3449]  eta: 2:23:44  lr: 0.000030  loss: 0.0468 (0.0520)  time: 3.7559  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1170/3449]  eta: 2:23:08  lr: 0.000030  loss: 0.0534 (0.0521)  time: 3.7827  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1180/3449]  eta: 2:22:30  lr: 0.000030  loss: 0.0546 (0.0521)  time: 3.7861  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1190/3449]  eta: 2:21:52  lr: 0.000030  loss: 0.0546 (0.0521)  time: 3.7472  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1200/3449]  eta: 2:21:15  lr: 0.000030  loss: 0.0461 (0.0520)  time: 3.7838  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1210/3449]  eta: 2:20:39  lr: 0.000030  loss: 0.0456 (0.0520)  time: 3.8299  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1220/3449]  eta: 2:20:03  lr: 0.000030  loss: 0.0460 (0.0519)  time: 3.8542  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1230/3449]  eta: 2:19:26  lr: 0.000030  loss: 0.0497 (0.0519)  time: 3.8416  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1240/3449]  eta: 2:18:51  lr: 0.000030  loss: 0.0497 (0.0519)  time: 3.8668  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1250/3449]  eta: 2:18:15  lr: 0.000030  loss: 0.0491 (0.0519)  time: 3.8821  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1260/3449]  eta: 2:17:37  lr: 0.000030  loss: 0.0514 (0.0519)  time: 3.8265  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1270/3449]  eta: 2:16:59  lr: 0.000030  loss: 0.0592 (0.0520)  time: 3.7575  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1280/3449]  eta: 2:16:21  lr: 0.000030  loss: 0.0498 (0.0519)  time: 3.7338  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1290/3449]  eta: 2:15:42  lr: 0.000030  loss: 0.0449 (0.0518)  time: 3.7440  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1300/3449]  eta: 2:15:04  lr: 0.000030  loss: 0.0522 (0.0519)  time: 3.7350  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1310/3449]  eta: 2:14:25  lr: 0.000030  loss: 0.0521 (0.0518)  time: 3.7208  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1320/3449]  eta: 2:13:49  lr: 0.000030  loss: 0.0386 (0.0517)  time: 3.7844  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1330/3449]  eta: 2:13:10  lr: 0.000030  loss: 0.0454 (0.0517)  time: 3.7787  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1340/3449]  eta: 2:12:33  lr: 0.000030  loss: 0.0502 (0.0517)  time: 3.7497  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1350/3449]  eta: 2:11:55  lr: 0.000030  loss: 0.0502 (0.0517)  time: 3.7994  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1360/3449]  eta: 2:11:18  lr: 0.000030  loss: 0.0524 (0.0518)  time: 3.7903  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1370/3449]  eta: 2:10:41  lr: 0.000030  loss: 0.0620 (0.0518)  time: 3.8167  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1380/3449]  eta: 2:10:04  lr: 0.000030  loss: 0.0555 (0.0518)  time: 3.8110  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1390/3449]  eta: 2:09:26  lr: 0.000030  loss: 0.0490 (0.0518)  time: 3.7637  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1400/3449]  eta: 2:08:47  lr: 0.000030  loss: 0.0531 (0.0518)  time: 3.7405  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1410/3449]  eta: 2:08:09  lr: 0.000030  loss: 0.0552 (0.0519)  time: 3.7375  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1420/3449]  eta: 2:07:32  lr: 0.000030  loss: 0.0515 (0.0518)  time: 3.7570  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1430/3449]  eta: 2:06:54  lr: 0.000030  loss: 0.0486 (0.0518)  time: 3.7589  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1440/3449]  eta: 2:06:16  lr: 0.000030  loss: 0.0486 (0.0518)  time: 3.7644  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1450/3449]  eta: 2:05:38  lr: 0.000030  loss: 0.0487 (0.0518)  time: 3.7763  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1460/3449]  eta: 2:05:00  lr: 0.000030  loss: 0.0474 (0.0517)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1470/3449]  eta: 2:04:23  lr: 0.000030  loss: 0.0427 (0.0517)  time: 3.7692  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1480/3449]  eta: 2:03:44  lr: 0.000030  loss: 0.0386 (0.0516)  time: 3.7419  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1490/3449]  eta: 2:03:06  lr: 0.000030  loss: 0.0359 (0.0516)  time: 3.7284  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1500/3449]  eta: 2:02:28  lr: 0.000030  loss: 0.0591 (0.0516)  time: 3.7451  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1510/3449]  eta: 2:01:51  lr: 0.000030  loss: 0.0633 (0.0516)  time: 3.7972  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1520/3449]  eta: 2:01:14  lr: 0.000030  loss: 0.0522 (0.0516)  time: 3.8359  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1530/3449]  eta: 2:00:38  lr: 0.000030  loss: 0.0500 (0.0517)  time: 3.8349  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1540/3449]  eta: 2:00:01  lr: 0.000030  loss: 0.0549 (0.0517)  time: 3.8494  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1550/3449]  eta: 1:59:23  lr: 0.000030  loss: 0.0604 (0.0517)  time: 3.8227  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1560/3449]  eta: 1:58:46  lr: 0.000030  loss: 0.0545 (0.0517)  time: 3.8045  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1570/3449]  eta: 1:58:08  lr: 0.000030  loss: 0.0478 (0.0516)  time: 3.7841  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1580/3449]  eta: 1:57:30  lr: 0.000030  loss: 0.0436 (0.0516)  time: 3.7497  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1590/3449]  eta: 1:56:54  lr: 0.000030  loss: 0.0544 (0.0516)  time: 3.8017  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1600/3449]  eta: 1:56:15  lr: 0.000030  loss: 0.0527 (0.0515)  time: 3.7695  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1610/3449]  eta: 1:55:38  lr: 0.000030  loss: 0.0516 (0.0515)  time: 3.7636  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1620/3449]  eta: 1:55:01  lr: 0.000030  loss: 0.0516 (0.0516)  time: 3.8697  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1630/3449]  eta: 1:54:23  lr: 0.000030  loss: 0.0502 (0.0515)  time: 3.7925  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1640/3449]  eta: 1:53:46  lr: 0.000030  loss: 0.0502 (0.0515)  time: 3.7819  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1650/3449]  eta: 1:53:08  lr: 0.000030  loss: 0.0578 (0.0516)  time: 3.8190  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1660/3449]  eta: 1:52:31  lr: 0.000030  loss: 0.0508 (0.0515)  time: 3.7640  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1670/3449]  eta: 1:51:53  lr: 0.000030  loss: 0.0500 (0.0515)  time: 3.7968  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1680/3449]  eta: 1:51:16  lr: 0.000030  loss: 0.0523 (0.0515)  time: 3.8268  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1690/3449]  eta: 1:50:38  lr: 0.000030  loss: 0.0510 (0.0515)  time: 3.7862  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1700/3449]  eta: 1:50:00  lr: 0.000030  loss: 0.0527 (0.0515)  time: 3.7371  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1710/3449]  eta: 1:49:24  lr: 0.000030  loss: 0.0507 (0.0515)  time: 3.8223  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1720/3449]  eta: 1:48:47  lr: 0.000030  loss: 0.0553 (0.0515)  time: 3.9048  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1730/3449]  eta: 1:48:10  lr: 0.000030  loss: 0.0597 (0.0515)  time: 3.8539  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1740/3449]  eta: 1:47:33  lr: 0.000030  loss: 0.0581 (0.0516)  time: 3.8646  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1750/3449]  eta: 1:46:56  lr: 0.000030  loss: 0.0560 (0.0516)  time: 3.8810  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1760/3449]  eta: 1:46:18  lr: 0.000030  loss: 0.0549 (0.0516)  time: 3.8009  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1770/3449]  eta: 1:45:41  lr: 0.000030  loss: 0.0507 (0.0516)  time: 3.8116  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1780/3449]  eta: 1:45:04  lr: 0.000030  loss: 0.0558 (0.0516)  time: 3.8703  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1790/3449]  eta: 1:44:27  lr: 0.000030  loss: 0.0564 (0.0516)  time: 3.8420  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1800/3449]  eta: 1:43:49  lr: 0.000030  loss: 0.0539 (0.0517)  time: 3.7947  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1810/3449]  eta: 1:43:12  lr: 0.000030  loss: 0.0513 (0.0516)  time: 3.8286  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1820/3449]  eta: 1:42:35  lr: 0.000030  loss: 0.0431 (0.0516)  time: 3.8397  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1830/3449]  eta: 1:41:57  lr: 0.000030  loss: 0.0490 (0.0516)  time: 3.7806  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1840/3449]  eta: 1:41:19  lr: 0.000030  loss: 0.0496 (0.0516)  time: 3.7788  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1850/3449]  eta: 1:40:41  lr: 0.000030  loss: 0.0486 (0.0516)  time: 3.7636  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1860/3449]  eta: 1:40:03  lr: 0.000030  loss: 0.0472 (0.0515)  time: 3.7299  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1870/3449]  eta: 1:39:25  lr: 0.000030  loss: 0.0450 (0.0515)  time: 3.7475  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1880/3449]  eta: 1:38:47  lr: 0.000030  loss: 0.0432 (0.0514)  time: 3.7692  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1890/3449]  eta: 1:38:09  lr: 0.000030  loss: 0.0486 (0.0514)  time: 3.7312  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1900/3449]  eta: 1:37:30  lr: 0.000030  loss: 0.0529 (0.0514)  time: 3.6809  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1910/3449]  eta: 1:36:53  lr: 0.000030  loss: 0.0575 (0.0515)  time: 3.7509  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1920/3449]  eta: 1:36:15  lr: 0.000030  loss: 0.0545 (0.0514)  time: 3.8273  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1930/3449]  eta: 1:35:39  lr: 0.000030  loss: 0.0423 (0.0514)  time: 3.8739  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1940/3449]  eta: 1:35:01  lr: 0.000030  loss: 0.0423 (0.0513)  time: 3.8737  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1950/3449]  eta: 1:34:23  lr: 0.000030  loss: 0.0452 (0.0513)  time: 3.8045  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1960/3449]  eta: 1:33:45  lr: 0.000030  loss: 0.0538 (0.0514)  time: 3.7371  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1970/3449]  eta: 1:33:08  lr: 0.000030  loss: 0.0559 (0.0514)  time: 3.7822  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1980/3449]  eta: 1:32:30  lr: 0.000030  loss: 0.0529 (0.0514)  time: 3.8465  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [1990/3449]  eta: 1:31:53  lr: 0.000030  loss: 0.0529 (0.0514)  time: 3.8326  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2000/3449]  eta: 1:31:15  lr: 0.000030  loss: 0.0484 (0.0513)  time: 3.7827  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2010/3449]  eta: 1:30:36  lr: 0.000030  loss: 0.0507 (0.0514)  time: 3.6863  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2020/3449]  eta: 1:29:59  lr: 0.000030  loss: 0.0492 (0.0513)  time: 3.7295  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2030/3449]  eta: 1:29:22  lr: 0.000030  loss: 0.0492 (0.0513)  time: 3.8838  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2040/3449]  eta: 1:28:44  lr: 0.000030  loss: 0.0541 (0.0513)  time: 3.8644  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2050/3449]  eta: 1:28:06  lr: 0.000030  loss: 0.0511 (0.0513)  time: 3.7587  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2060/3449]  eta: 1:27:29  lr: 0.000030  loss: 0.0503 (0.0513)  time: 3.8087  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2070/3449]  eta: 1:26:51  lr: 0.000030  loss: 0.0554 (0.0513)  time: 3.7995  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2080/3449]  eta: 1:26:14  lr: 0.000030  loss: 0.0554 (0.0513)  time: 3.8028  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2090/3449]  eta: 1:25:36  lr: 0.000030  loss: 0.0528 (0.0513)  time: 3.7878  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2100/3449]  eta: 1:24:57  lr: 0.000030  loss: 0.0566 (0.0513)  time: 3.7227  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2110/3449]  eta: 1:24:21  lr: 0.000030  loss: 0.0492 (0.0513)  time: 3.8767  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2120/3449]  eta: 1:23:43  lr: 0.000030  loss: 0.0464 (0.0513)  time: 3.8580  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2130/3449]  eta: 1:23:05  lr: 0.000030  loss: 0.0470 (0.0513)  time: 3.7577  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2140/3449]  eta: 1:22:28  lr: 0.000030  loss: 0.0485 (0.0513)  time: 3.8414  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2150/3449]  eta: 1:21:50  lr: 0.000030  loss: 0.0489 (0.0513)  time: 3.7919  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2160/3449]  eta: 1:21:12  lr: 0.000030  loss: 0.0503 (0.0513)  time: 3.7361  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2170/3449]  eta: 1:20:34  lr: 0.000030  loss: 0.0478 (0.0513)  time: 3.7954  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2180/3449]  eta: 1:19:57  lr: 0.000030  loss: 0.0477 (0.0513)  time: 3.8019  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2190/3449]  eta: 1:19:19  lr: 0.000030  loss: 0.0483 (0.0512)  time: 3.7730  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2200/3449]  eta: 1:18:41  lr: 0.000030  loss: 0.0508 (0.0513)  time: 3.8014  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2210/3449]  eta: 1:18:03  lr: 0.000030  loss: 0.0527 (0.0513)  time: 3.8062  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2220/3449]  eta: 1:17:26  lr: 0.000030  loss: 0.0455 (0.0512)  time: 3.8097  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2230/3449]  eta: 1:16:48  lr: 0.000030  loss: 0.0456 (0.0513)  time: 3.8419  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2240/3449]  eta: 1:16:11  lr: 0.000030  loss: 0.0585 (0.0513)  time: 3.8432  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2250/3449]  eta: 1:15:33  lr: 0.000030  loss: 0.0535 (0.0513)  time: 3.8362  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2260/3449]  eta: 1:14:56  lr: 0.000030  loss: 0.0443 (0.0513)  time: 3.8466  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2270/3449]  eta: 1:14:18  lr: 0.000030  loss: 0.0470 (0.0513)  time: 3.8154  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2280/3449]  eta: 1:13:40  lr: 0.000030  loss: 0.0483 (0.0512)  time: 3.7767  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2290/3449]  eta: 1:13:02  lr: 0.000030  loss: 0.0433 (0.0512)  time: 3.7639  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2300/3449]  eta: 1:12:25  lr: 0.000030  loss: 0.0434 (0.0512)  time: 3.7744  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2310/3449]  eta: 1:11:47  lr: 0.000030  loss: 0.0422 (0.0511)  time: 3.8134  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2320/3449]  eta: 1:11:10  lr: 0.000030  loss: 0.0454 (0.0512)  time: 3.8514  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2330/3449]  eta: 1:10:32  lr: 0.000030  loss: 0.0557 (0.0512)  time: 3.8369  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2340/3449]  eta: 1:09:54  lr: 0.000030  loss: 0.0544 (0.0512)  time: 3.8274  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2350/3449]  eta: 1:09:17  lr: 0.000030  loss: 0.0548 (0.0512)  time: 3.8151  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2360/3449]  eta: 1:08:39  lr: 0.000030  loss: 0.0566 (0.0512)  time: 3.7525  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2370/3449]  eta: 1:08:01  lr: 0.000030  loss: 0.0578 (0.0512)  time: 3.7424  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2380/3449]  eta: 1:07:23  lr: 0.000030  loss: 0.0557 (0.0513)  time: 3.8180  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2390/3449]  eta: 1:06:45  lr: 0.000030  loss: 0.0533 (0.0513)  time: 3.8006  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2400/3449]  eta: 1:06:07  lr: 0.000030  loss: 0.0421 (0.0512)  time: 3.7738  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2410/3449]  eta: 1:05:30  lr: 0.000030  loss: 0.0472 (0.0512)  time: 3.8241  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2420/3449]  eta: 1:04:52  lr: 0.000030  loss: 0.0539 (0.0513)  time: 3.7574  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2430/3449]  eta: 1:04:14  lr: 0.000030  loss: 0.0581 (0.0513)  time: 3.7379  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2440/3449]  eta: 1:03:36  lr: 0.000030  loss: 0.0542 (0.0513)  time: 3.8295  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2450/3449]  eta: 1:02:59  lr: 0.000030  loss: 0.0488 (0.0513)  time: 3.9189  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2460/3449]  eta: 1:02:21  lr: 0.000030  loss: 0.0537 (0.0513)  time: 3.8809  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2470/3449]  eta: 1:01:44  lr: 0.000030  loss: 0.0560 (0.0513)  time: 3.8182  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2480/3449]  eta: 1:01:06  lr: 0.000030  loss: 0.0554 (0.0513)  time: 3.7931  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2490/3449]  eta: 1:00:28  lr: 0.000030  loss: 0.0554 (0.0513)  time: 3.7779  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2500/3449]  eta: 0:59:51  lr: 0.000030  loss: 0.0507 (0.0512)  time: 3.8595  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2510/3449]  eta: 0:59:14  lr: 0.000030  loss: 0.0447 (0.0512)  time: 3.9334  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2520/3449]  eta: 0:58:36  lr: 0.000030  loss: 0.0381 (0.0512)  time: 3.9400  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2530/3449]  eta: 0:57:59  lr: 0.000030  loss: 0.0384 (0.0511)  time: 3.8958  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2540/3449]  eta: 0:57:21  lr: 0.000030  loss: 0.0444 (0.0511)  time: 3.9093  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2550/3449]  eta: 0:56:44  lr: 0.000030  loss: 0.0486 (0.0511)  time: 3.8817  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2560/3449]  eta: 0:56:06  lr: 0.000030  loss: 0.0527 (0.0511)  time: 3.7937  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2570/3449]  eta: 0:55:28  lr: 0.000030  loss: 0.0562 (0.0511)  time: 3.7951  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2580/3449]  eta: 0:54:50  lr: 0.000030  loss: 0.0495 (0.0511)  time: 3.8121  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2590/3449]  eta: 0:54:12  lr: 0.000030  loss: 0.0491 (0.0511)  time: 3.7908  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2600/3449]  eta: 0:53:34  lr: 0.000030  loss: 0.0429 (0.0511)  time: 3.7866  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2610/3449]  eta: 0:52:56  lr: 0.000030  loss: 0.0388 (0.0510)  time: 3.7391  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2620/3449]  eta: 0:52:18  lr: 0.000030  loss: 0.0371 (0.0510)  time: 3.7354  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2630/3449]  eta: 0:51:40  lr: 0.000030  loss: 0.0490 (0.0510)  time: 3.7862  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2640/3449]  eta: 0:51:03  lr: 0.000030  loss: 0.0464 (0.0510)  time: 3.7979  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2650/3449]  eta: 0:50:25  lr: 0.000030  loss: 0.0460 (0.0510)  time: 3.8319  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2660/3449]  eta: 0:49:47  lr: 0.000030  loss: 0.0524 (0.0510)  time: 3.8263  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2670/3449]  eta: 0:49:10  lr: 0.000030  loss: 0.0535 (0.0510)  time: 3.8381  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2680/3449]  eta: 0:48:31  lr: 0.000030  loss: 0.0553 (0.0510)  time: 3.7648  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2690/3449]  eta: 0:47:54  lr: 0.000030  loss: 0.0510 (0.0510)  time: 3.7390  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2700/3449]  eta: 0:47:16  lr: 0.000030  loss: 0.0505 (0.0511)  time: 3.8178  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2710/3449]  eta: 0:46:38  lr: 0.000030  loss: 0.0556 (0.0511)  time: 3.8507  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2720/3449]  eta: 0:46:01  lr: 0.000030  loss: 0.0503 (0.0511)  time: 3.8750  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2730/3449]  eta: 0:45:23  lr: 0.000030  loss: 0.0503 (0.0510)  time: 3.8303  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2740/3449]  eta: 0:44:45  lr: 0.000030  loss: 0.0446 (0.0510)  time: 3.8604  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2750/3449]  eta: 0:44:07  lr: 0.000030  loss: 0.0463 (0.0510)  time: 3.8758  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2760/3449]  eta: 0:43:30  lr: 0.000030  loss: 0.0465 (0.0510)  time: 3.8480  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2770/3449]  eta: 0:42:52  lr: 0.000030  loss: 0.0482 (0.0510)  time: 3.7977  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2780/3449]  eta: 0:42:14  lr: 0.000030  loss: 0.0482 (0.0510)  time: 3.7608  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2790/3449]  eta: 0:41:36  lr: 0.000030  loss: 0.0392 (0.0509)  time: 3.8254  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2800/3449]  eta: 0:40:58  lr: 0.000030  loss: 0.0392 (0.0509)  time: 3.8243  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2810/3449]  eta: 0:40:20  lr: 0.000030  loss: 0.0455 (0.0509)  time: 3.8032  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2820/3449]  eta: 0:39:42  lr: 0.000030  loss: 0.0455 (0.0509)  time: 3.7570  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2830/3449]  eta: 0:39:04  lr: 0.000030  loss: 0.0528 (0.0509)  time: 3.7576  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2840/3449]  eta: 0:38:26  lr: 0.000030  loss: 0.0472 (0.0509)  time: 3.7737  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2850/3449]  eta: 0:37:49  lr: 0.000030  loss: 0.0471 (0.0509)  time: 3.7828  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2860/3449]  eta: 0:37:11  lr: 0.000030  loss: 0.0482 (0.0509)  time: 3.8392  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2870/3449]  eta: 0:36:33  lr: 0.000030  loss: 0.0502 (0.0509)  time: 3.8657  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2880/3449]  eta: 0:35:55  lr: 0.000030  loss: 0.0472 (0.0509)  time: 3.8316  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2890/3449]  eta: 0:35:17  lr: 0.000030  loss: 0.0412 (0.0508)  time: 3.7639  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2900/3449]  eta: 0:34:40  lr: 0.000030  loss: 0.0492 (0.0508)  time: 3.7982  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2910/3449]  eta: 0:34:02  lr: 0.000030  loss: 0.0506 (0.0508)  time: 3.8376  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2920/3449]  eta: 0:33:24  lr: 0.000030  loss: 0.0504 (0.0508)  time: 3.8147  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2930/3449]  eta: 0:32:46  lr: 0.000030  loss: 0.0519 (0.0508)  time: 3.8185  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2940/3449]  eta: 0:32:08  lr: 0.000030  loss: 0.0498 (0.0508)  time: 3.8442  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2950/3449]  eta: 0:31:30  lr: 0.000030  loss: 0.0478 (0.0508)  time: 3.7455  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2960/3449]  eta: 0:30:52  lr: 0.000030  loss: 0.0479 (0.0508)  time: 3.6481  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2970/3449]  eta: 0:30:14  lr: 0.000030  loss: 0.0518 (0.0508)  time: 3.7422  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2980/3449]  eta: 0:29:36  lr: 0.000030  loss: 0.0516 (0.0508)  time: 3.8338  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [2990/3449]  eta: 0:28:59  lr: 0.000030  loss: 0.0492 (0.0508)  time: 3.8202  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3000/3449]  eta: 0:28:21  lr: 0.000030  loss: 0.0505 (0.0508)  time: 3.8833  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3010/3449]  eta: 0:27:43  lr: 0.000030  loss: 0.0549 (0.0508)  time: 3.9110  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3020/3449]  eta: 0:27:05  lr: 0.000030  loss: 0.0504 (0.0508)  time: 3.8970  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3030/3449]  eta: 0:26:28  lr: 0.000030  loss: 0.0492 (0.0508)  time: 3.9006  data: 0.0003  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3040/3449]  eta: 0:25:50  lr: 0.000030  loss: 0.0545 (0.0508)  time: 3.8325  data: 0.0003  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3050/3449]  eta: 0:25:12  lr: 0.000030  loss: 0.0539 (0.0508)  time: 3.8384  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3060/3449]  eta: 0:24:34  lr: 0.000030  loss: 0.0515 (0.0508)  time: 3.9185  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3070/3449]  eta: 0:23:56  lr: 0.000030  loss: 0.0581 (0.0508)  time: 3.8126  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3080/3449]  eta: 0:23:18  lr: 0.000030  loss: 0.0588 (0.0508)  time: 3.7268  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3090/3449]  eta: 0:22:40  lr: 0.000030  loss: 0.0553 (0.0508)  time: 3.7854  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3100/3449]  eta: 0:22:03  lr: 0.000030  loss: 0.0516 (0.0508)  time: 3.8062  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3110/3449]  eta: 0:21:25  lr: 0.000030  loss: 0.0473 (0.0508)  time: 3.8051  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3120/3449]  eta: 0:20:47  lr: 0.000030  loss: 0.0509 (0.0508)  time: 3.7403  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3130/3449]  eta: 0:20:09  lr: 0.000030  loss: 0.0509 (0.0508)  time: 3.7509  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3140/3449]  eta: 0:19:31  lr: 0.000030  loss: 0.0513 (0.0508)  time: 3.7781  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3150/3449]  eta: 0:18:53  lr: 0.000030  loss: 0.0532 (0.0508)  time: 3.7634  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3160/3449]  eta: 0:18:15  lr: 0.000030  loss: 0.0522 (0.0508)  time: 3.8262  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3170/3449]  eta: 0:17:37  lr: 0.000030  loss: 0.0547 (0.0508)  time: 3.8802  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3180/3449]  eta: 0:16:59  lr: 0.000030  loss: 0.0523 (0.0508)  time: 3.8386  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3190/3449]  eta: 0:16:21  lr: 0.000030  loss: 0.0503 (0.0508)  time: 3.7900  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3200/3449]  eta: 0:15:44  lr: 0.000030  loss: 0.0506 (0.0508)  time: 3.8156  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3210/3449]  eta: 0:15:06  lr: 0.000030  loss: 0.0554 (0.0508)  time: 3.8149  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3220/3449]  eta: 0:14:28  lr: 0.000030  loss: 0.0553 (0.0509)  time: 3.7320  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3230/3449]  eta: 0:13:50  lr: 0.000030  loss: 0.0534 (0.0508)  time: 3.7522  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3240/3449]  eta: 0:13:12  lr: 0.000030  loss: 0.0561 (0.0509)  time: 3.7714  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3250/3449]  eta: 0:12:34  lr: 0.000030  loss: 0.0535 (0.0508)  time: 3.7797  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3260/3449]  eta: 0:11:56  lr: 0.000030  loss: 0.0560 (0.0509)  time: 3.8414  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3270/3449]  eta: 0:11:18  lr: 0.000030  loss: 0.0543 (0.0508)  time: 3.7757  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3280/3449]  eta: 0:10:40  lr: 0.000030  loss: 0.0444 (0.0508)  time: 3.6957  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3290/3449]  eta: 0:10:02  lr: 0.000030  loss: 0.0446 (0.0508)  time: 3.7006  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3300/3449]  eta: 0:09:24  lr: 0.000030  loss: 0.0512 (0.0508)  time: 3.7031  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3310/3449]  eta: 0:08:46  lr: 0.000030  loss: 0.0567 (0.0508)  time: 3.7779  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3320/3449]  eta: 0:08:08  lr: 0.000030  loss: 0.0598 (0.0508)  time: 3.8365  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3330/3449]  eta: 0:07:31  lr: 0.000030  loss: 0.0572 (0.0508)  time: 3.8114  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3340/3449]  eta: 0:06:53  lr: 0.000030  loss: 0.0550 (0.0509)  time: 3.7767  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3350/3449]  eta: 0:06:15  lr: 0.000030  loss: 0.0604 (0.0509)  time: 3.7706  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3360/3449]  eta: 0:05:37  lr: 0.000030  loss: 0.0547 (0.0509)  time: 3.8107  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3370/3449]  eta: 0:04:59  lr: 0.000030  loss: 0.0534 (0.0509)  time: 3.7926  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3380/3449]  eta: 0:04:21  lr: 0.000030  loss: 0.0521 (0.0509)  time: 3.7636  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3390/3449]  eta: 0:03:43  lr: 0.000030  loss: 0.0481 (0.0509)  time: 3.8130  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3400/3449]  eta: 0:03:05  lr: 0.000030  loss: 0.0563 (0.0509)  time: 3.7911  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3410/3449]  eta: 0:02:27  lr: 0.000030  loss: 0.0596 (0.0509)  time: 3.8104  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3420/3449]  eta: 0:01:49  lr: 0.000030  loss: 0.0511 (0.0509)  time: 3.7736  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3430/3449]  eta: 0:01:12  lr: 0.000030  loss: 0.0504 (0.0509)  time: 3.7347  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3440/3449]  eta: 0:00:34  lr: 0.000030  loss: 0.0443 (0.0509)  time: 3.7628  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [3448/3449]  eta: 0:00:03  lr: 0.000030  loss: 0.0587 (0.0509)  time: 3.7943  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:4] Total time: 3:37:51 (3.7901 s / it)\n",
      "Averaged stats: lr: 0.000030  loss: 0.0587 (0.0509)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:4]  [ 0/14]  eta: 0:04:21  loss: 0.0460 (0.0460)  time: 18.6877  data: 0.4456  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:4]  [13/14]  eta: 0:00:18  loss: 0.0392 (0.0407)  time: 18.2698  data: 0.0320  max mem: 34968\n",
      "Valid: [epoch:4] Total time: 0:04:15 (18.2772 s / it)\n",
      "Averaged stats: loss: 0.0392 (0.0407)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_4_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.041%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [   0/3449]  eta: 5:16:02  lr: 0.000040  loss: 0.0843 (0.0843)  time: 5.4979  data: 1.5388  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [  10/3449]  eta: 3:41:13  lr: 0.000040  loss: 0.0567 (0.0611)  time: 3.8596  data: 0.1400  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [  20/3449]  eta: 3:40:50  lr: 0.000040  loss: 0.0508 (0.0535)  time: 3.7825  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [  30/3449]  eta: 3:39:00  lr: 0.000040  loss: 0.0514 (0.0526)  time: 3.8343  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [  40/3449]  eta: 3:38:07  lr: 0.000040  loss: 0.0484 (0.0496)  time: 3.8124  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [  50/3449]  eta: 3:37:00  lr: 0.000040  loss: 0.0436 (0.0493)  time: 3.8115  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [  60/3449]  eta: 3:35:51  lr: 0.000040  loss: 0.0519 (0.0503)  time: 3.7860  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [  70/3449]  eta: 3:35:40  lr: 0.000040  loss: 0.0500 (0.0505)  time: 3.8264  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [  80/3449]  eta: 3:34:33  lr: 0.000040  loss: 0.0463 (0.0486)  time: 3.8204  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [  90/3449]  eta: 3:33:59  lr: 0.000040  loss: 0.0404 (0.0498)  time: 3.7966  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 100/3449]  eta: 3:33:10  lr: 0.000040  loss: 0.0484 (0.0494)  time: 3.8107  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 110/3449]  eta: 3:31:52  lr: 0.000040  loss: 0.0431 (0.0486)  time: 3.7395  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 120/3449]  eta: 3:30:55  lr: 0.000040  loss: 0.0431 (0.0488)  time: 3.7135  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 130/3449]  eta: 3:30:15  lr: 0.000040  loss: 0.0430 (0.0482)  time: 3.7650  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 140/3449]  eta: 3:30:12  lr: 0.000040  loss: 0.0479 (0.0484)  time: 3.8704  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 150/3449]  eta: 3:29:51  lr: 0.000040  loss: 0.0546 (0.0487)  time: 3.9216  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 160/3449]  eta: 3:28:51  lr: 0.000040  loss: 0.0536 (0.0487)  time: 3.8010  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 170/3449]  eta: 3:28:09  lr: 0.000040  loss: 0.0493 (0.0487)  time: 3.7481  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 180/3449]  eta: 3:27:32  lr: 0.000040  loss: 0.0462 (0.0481)  time: 3.8028  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 190/3449]  eta: 3:26:53  lr: 0.000040  loss: 0.0462 (0.0485)  time: 3.8093  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 200/3449]  eta: 3:26:15  lr: 0.000040  loss: 0.0627 (0.0488)  time: 3.8073  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 210/3449]  eta: 3:25:54  lr: 0.000040  loss: 0.0630 (0.0493)  time: 3.8666  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 220/3449]  eta: 3:25:12  lr: 0.000040  loss: 0.0627 (0.0495)  time: 3.8543  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 230/3449]  eta: 3:24:33  lr: 0.000040  loss: 0.0530 (0.0498)  time: 3.7977  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 240/3449]  eta: 3:23:58  lr: 0.000040  loss: 0.0562 (0.0500)  time: 3.8186  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 250/3449]  eta: 3:23:13  lr: 0.000040  loss: 0.0600 (0.0504)  time: 3.7969  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 260/3449]  eta: 3:23:01  lr: 0.000040  loss: 0.0570 (0.0503)  time: 3.8951  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 270/3449]  eta: 3:22:13  lr: 0.000040  loss: 0.0467 (0.0501)  time: 3.8790  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 280/3449]  eta: 3:21:30  lr: 0.000040  loss: 0.0465 (0.0498)  time: 3.7547  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 290/3449]  eta: 3:20:47  lr: 0.000040  loss: 0.0465 (0.0496)  time: 3.7727  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 300/3449]  eta: 3:20:11  lr: 0.000040  loss: 0.0480 (0.0498)  time: 3.8008  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 310/3449]  eta: 3:19:32  lr: 0.000040  loss: 0.0457 (0.0495)  time: 3.8234  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 320/3449]  eta: 3:18:56  lr: 0.000040  loss: 0.0461 (0.0496)  time: 3.8245  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 330/3449]  eta: 3:18:18  lr: 0.000040  loss: 0.0535 (0.0497)  time: 3.8221  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 340/3449]  eta: 3:17:43  lr: 0.000040  loss: 0.0477 (0.0496)  time: 3.8324  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 350/3449]  eta: 3:16:53  lr: 0.000040  loss: 0.0432 (0.0494)  time: 3.7671  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 360/3449]  eta: 3:16:05  lr: 0.000040  loss: 0.0525 (0.0497)  time: 3.6851  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 370/3449]  eta: 3:15:20  lr: 0.000040  loss: 0.0531 (0.0497)  time: 3.7107  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 380/3449]  eta: 3:14:46  lr: 0.000040  loss: 0.0414 (0.0495)  time: 3.7919  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 390/3449]  eta: 3:14:06  lr: 0.000040  loss: 0.0466 (0.0495)  time: 3.8241  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 400/3449]  eta: 3:13:28  lr: 0.000040  loss: 0.0517 (0.0495)  time: 3.8002  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 410/3449]  eta: 3:12:51  lr: 0.000040  loss: 0.0545 (0.0496)  time: 3.8105  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 420/3449]  eta: 3:12:13  lr: 0.000040  loss: 0.0572 (0.0498)  time: 3.8109  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 430/3449]  eta: 3:11:40  lr: 0.000040  loss: 0.0548 (0.0496)  time: 3.8479  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 440/3449]  eta: 3:11:07  lr: 0.000040  loss: 0.0539 (0.0498)  time: 3.8802  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 450/3449]  eta: 3:10:23  lr: 0.000040  loss: 0.0582 (0.0500)  time: 3.7989  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 460/3449]  eta: 3:09:43  lr: 0.000040  loss: 0.0558 (0.0501)  time: 3.7561  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 470/3449]  eta: 3:09:13  lr: 0.000040  loss: 0.0539 (0.0501)  time: 3.8570  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 480/3449]  eta: 3:08:29  lr: 0.000040  loss: 0.0554 (0.0502)  time: 3.8254  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 490/3449]  eta: 3:07:47  lr: 0.000040  loss: 0.0528 (0.0502)  time: 3.7297  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 500/3449]  eta: 3:07:10  lr: 0.000040  loss: 0.0500 (0.0501)  time: 3.7880  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 510/3449]  eta: 3:06:33  lr: 0.000040  loss: 0.0383 (0.0499)  time: 3.8289  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 520/3449]  eta: 3:05:54  lr: 0.000040  loss: 0.0448 (0.0499)  time: 3.8097  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 530/3449]  eta: 3:05:16  lr: 0.000040  loss: 0.0503 (0.0499)  time: 3.8067  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 540/3449]  eta: 3:04:36  lr: 0.000040  loss: 0.0514 (0.0499)  time: 3.7834  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 550/3449]  eta: 3:03:56  lr: 0.000040  loss: 0.0533 (0.0500)  time: 3.7723  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 560/3449]  eta: 3:03:12  lr: 0.000040  loss: 0.0534 (0.0499)  time: 3.7328  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 570/3449]  eta: 3:02:31  lr: 0.000040  loss: 0.0513 (0.0500)  time: 3.7152  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 580/3449]  eta: 3:01:50  lr: 0.000040  loss: 0.0544 (0.0500)  time: 3.7457  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 590/3449]  eta: 3:01:13  lr: 0.000040  loss: 0.0544 (0.0500)  time: 3.7862  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 600/3449]  eta: 3:00:32  lr: 0.000040  loss: 0.0501 (0.0500)  time: 3.7810  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 610/3449]  eta: 2:59:52  lr: 0.000040  loss: 0.0528 (0.0500)  time: 3.7485  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 620/3449]  eta: 2:59:11  lr: 0.000040  loss: 0.0533 (0.0500)  time: 3.7553  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 630/3449]  eta: 2:58:31  lr: 0.000040  loss: 0.0519 (0.0499)  time: 3.7422  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 640/3449]  eta: 2:57:53  lr: 0.000040  loss: 0.0515 (0.0500)  time: 3.7709  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 650/3449]  eta: 2:57:16  lr: 0.000040  loss: 0.0582 (0.0501)  time: 3.8139  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 660/3449]  eta: 2:56:38  lr: 0.000040  loss: 0.0438 (0.0499)  time: 3.8105  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 670/3449]  eta: 2:56:01  lr: 0.000040  loss: 0.0518 (0.0500)  time: 3.8177  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 680/3449]  eta: 2:55:25  lr: 0.000040  loss: 0.0567 (0.0500)  time: 3.8398  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 690/3449]  eta: 2:54:48  lr: 0.000040  loss: 0.0470 (0.0500)  time: 3.8386  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 700/3449]  eta: 2:54:11  lr: 0.000040  loss: 0.0481 (0.0500)  time: 3.8346  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 710/3449]  eta: 2:53:36  lr: 0.000040  loss: 0.0499 (0.0500)  time: 3.8503  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 720/3449]  eta: 2:52:57  lr: 0.000040  loss: 0.0462 (0.0499)  time: 3.8173  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 730/3449]  eta: 2:52:18  lr: 0.000040  loss: 0.0483 (0.0499)  time: 3.7738  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 740/3449]  eta: 2:51:37  lr: 0.000040  loss: 0.0518 (0.0500)  time: 3.7578  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 750/3449]  eta: 2:50:58  lr: 0.000040  loss: 0.0527 (0.0500)  time: 3.7516  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 760/3449]  eta: 2:50:21  lr: 0.000040  loss: 0.0547 (0.0500)  time: 3.7968  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 770/3449]  eta: 2:49:42  lr: 0.000040  loss: 0.0547 (0.0501)  time: 3.8018  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 780/3449]  eta: 2:49:04  lr: 0.000040  loss: 0.0541 (0.0502)  time: 3.7881  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 790/3449]  eta: 2:48:27  lr: 0.000040  loss: 0.0538 (0.0501)  time: 3.8082  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 800/3449]  eta: 2:47:46  lr: 0.000040  loss: 0.0375 (0.0501)  time: 3.7680  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 810/3449]  eta: 2:47:09  lr: 0.000040  loss: 0.0359 (0.0500)  time: 3.7782  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 820/3449]  eta: 2:46:33  lr: 0.000040  loss: 0.0484 (0.0500)  time: 3.8446  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 830/3449]  eta: 2:45:53  lr: 0.000040  loss: 0.0519 (0.0501)  time: 3.8042  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 840/3449]  eta: 2:45:16  lr: 0.000040  loss: 0.0497 (0.0501)  time: 3.7954  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 850/3449]  eta: 2:44:43  lr: 0.000040  loss: 0.0514 (0.0501)  time: 3.8925  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 860/3449]  eta: 2:44:02  lr: 0.000040  loss: 0.0534 (0.0501)  time: 3.8279  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 870/3449]  eta: 2:43:20  lr: 0.000040  loss: 0.0630 (0.0503)  time: 3.6892  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 880/3449]  eta: 2:42:43  lr: 0.000040  loss: 0.0575 (0.0502)  time: 3.7509  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 890/3449]  eta: 2:42:04  lr: 0.000040  loss: 0.0481 (0.0502)  time: 3.7989  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 900/3449]  eta: 2:41:26  lr: 0.000040  loss: 0.0448 (0.0502)  time: 3.7775  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 910/3449]  eta: 2:40:48  lr: 0.000040  loss: 0.0566 (0.0503)  time: 3.7943  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 920/3449]  eta: 2:40:10  lr: 0.000040  loss: 0.0583 (0.0503)  time: 3.8074  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 930/3449]  eta: 2:39:35  lr: 0.000040  loss: 0.0580 (0.0503)  time: 3.8633  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 940/3449]  eta: 2:38:55  lr: 0.000040  loss: 0.0513 (0.0503)  time: 3.8211  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 950/3449]  eta: 2:38:21  lr: 0.000040  loss: 0.0603 (0.0504)  time: 3.8274  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 960/3449]  eta: 2:37:43  lr: 0.000040  loss: 0.0634 (0.0505)  time: 3.8742  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 970/3449]  eta: 2:37:07  lr: 0.000040  loss: 0.0614 (0.0506)  time: 3.8537  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 980/3449]  eta: 2:36:28  lr: 0.000040  loss: 0.0541 (0.0505)  time: 3.8280  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 990/3449]  eta: 2:35:53  lr: 0.000040  loss: 0.0481 (0.0504)  time: 3.8355  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1000/3449]  eta: 2:35:15  lr: 0.000040  loss: 0.0406 (0.0504)  time: 3.8691  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1010/3449]  eta: 2:34:37  lr: 0.000040  loss: 0.0391 (0.0502)  time: 3.8097  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1020/3449]  eta: 2:33:58  lr: 0.000040  loss: 0.0417 (0.0502)  time: 3.7738  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1030/3449]  eta: 2:33:20  lr: 0.000040  loss: 0.0602 (0.0503)  time: 3.7829  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1040/3449]  eta: 2:32:42  lr: 0.000040  loss: 0.0510 (0.0503)  time: 3.8124  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1050/3449]  eta: 2:32:05  lr: 0.000040  loss: 0.0458 (0.0502)  time: 3.8352  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1060/3449]  eta: 2:31:27  lr: 0.000040  loss: 0.0458 (0.0502)  time: 3.8271  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1070/3449]  eta: 2:30:51  lr: 0.000040  loss: 0.0491 (0.0502)  time: 3.8400  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1080/3449]  eta: 2:30:12  lr: 0.000040  loss: 0.0489 (0.0502)  time: 3.8162  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1090/3449]  eta: 2:29:36  lr: 0.000040  loss: 0.0473 (0.0502)  time: 3.8293  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1100/3449]  eta: 2:28:59  lr: 0.000040  loss: 0.0470 (0.0502)  time: 3.8812  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1110/3449]  eta: 2:28:21  lr: 0.000040  loss: 0.0470 (0.0502)  time: 3.8361  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1120/3449]  eta: 2:27:45  lr: 0.000040  loss: 0.0452 (0.0502)  time: 3.8615  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1130/3449]  eta: 2:27:06  lr: 0.000040  loss: 0.0586 (0.0502)  time: 3.8219  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1140/3449]  eta: 2:26:27  lr: 0.000040  loss: 0.0619 (0.0503)  time: 3.7571  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1150/3449]  eta: 2:25:49  lr: 0.000040  loss: 0.0527 (0.0503)  time: 3.7909  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1160/3449]  eta: 2:25:11  lr: 0.000040  loss: 0.0537 (0.0503)  time: 3.8047  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1170/3449]  eta: 2:24:32  lr: 0.000040  loss: 0.0556 (0.0503)  time: 3.7805  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1180/3449]  eta: 2:23:52  lr: 0.000040  loss: 0.0550 (0.0504)  time: 3.7297  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1190/3449]  eta: 2:23:13  lr: 0.000040  loss: 0.0535 (0.0504)  time: 3.7449  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1200/3449]  eta: 2:22:36  lr: 0.000040  loss: 0.0408 (0.0503)  time: 3.8218  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1210/3449]  eta: 2:21:56  lr: 0.000040  loss: 0.0450 (0.0503)  time: 3.7698  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1220/3449]  eta: 2:21:19  lr: 0.000040  loss: 0.0489 (0.0503)  time: 3.7714  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1230/3449]  eta: 2:20:42  lr: 0.000040  loss: 0.0457 (0.0502)  time: 3.8671  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1240/3449]  eta: 2:20:03  lr: 0.000040  loss: 0.0474 (0.0502)  time: 3.8174  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1250/3449]  eta: 2:19:22  lr: 0.000040  loss: 0.0558 (0.0503)  time: 3.6941  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1260/3449]  eta: 2:18:45  lr: 0.000040  loss: 0.0573 (0.0503)  time: 3.7498  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1270/3449]  eta: 2:18:08  lr: 0.000040  loss: 0.0548 (0.0503)  time: 3.8565  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1280/3449]  eta: 2:17:29  lr: 0.000040  loss: 0.0527 (0.0502)  time: 3.7823  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1290/3449]  eta: 2:16:51  lr: 0.000040  loss: 0.0459 (0.0502)  time: 3.7662  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1300/3449]  eta: 2:16:12  lr: 0.000040  loss: 0.0505 (0.0503)  time: 3.7928  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1310/3449]  eta: 2:15:35  lr: 0.000040  loss: 0.0505 (0.0503)  time: 3.8312  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1320/3449]  eta: 2:14:58  lr: 0.000040  loss: 0.0479 (0.0503)  time: 3.8592  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1330/3449]  eta: 2:14:21  lr: 0.000040  loss: 0.0514 (0.0503)  time: 3.8429  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1340/3449]  eta: 2:13:42  lr: 0.000040  loss: 0.0520 (0.0503)  time: 3.8079  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1350/3449]  eta: 2:13:04  lr: 0.000040  loss: 0.0393 (0.0502)  time: 3.7975  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1360/3449]  eta: 2:12:28  lr: 0.000040  loss: 0.0594 (0.0503)  time: 3.8700  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1370/3449]  eta: 2:11:49  lr: 0.000040  loss: 0.0633 (0.0503)  time: 3.8172  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1380/3449]  eta: 2:11:11  lr: 0.000040  loss: 0.0531 (0.0503)  time: 3.7791  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1390/3449]  eta: 2:10:31  lr: 0.000040  loss: 0.0473 (0.0502)  time: 3.7654  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1400/3449]  eta: 2:09:54  lr: 0.000040  loss: 0.0493 (0.0502)  time: 3.7844  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1410/3449]  eta: 2:09:15  lr: 0.000040  loss: 0.0559 (0.0502)  time: 3.7840  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1420/3449]  eta: 2:08:35  lr: 0.000040  loss: 0.0549 (0.0502)  time: 3.7006  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1430/3449]  eta: 2:07:59  lr: 0.000040  loss: 0.0532 (0.0502)  time: 3.8025  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1440/3449]  eta: 2:07:21  lr: 0.000040  loss: 0.0460 (0.0501)  time: 3.8732  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1450/3449]  eta: 2:06:45  lr: 0.000040  loss: 0.0465 (0.0502)  time: 3.9016  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1460/3449]  eta: 2:06:06  lr: 0.000040  loss: 0.0510 (0.0502)  time: 3.8245  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1470/3449]  eta: 2:05:29  lr: 0.000040  loss: 0.0474 (0.0501)  time: 3.7859  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1480/3449]  eta: 2:04:50  lr: 0.000040  loss: 0.0500 (0.0501)  time: 3.8322  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1490/3449]  eta: 2:04:13  lr: 0.000040  loss: 0.0493 (0.0501)  time: 3.8273  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1500/3449]  eta: 2:03:35  lr: 0.000040  loss: 0.0568 (0.0501)  time: 3.8334  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1510/3449]  eta: 2:02:58  lr: 0.000040  loss: 0.0656 (0.0502)  time: 3.8334  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1520/3449]  eta: 2:02:19  lr: 0.000040  loss: 0.0661 (0.0503)  time: 3.7989  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1530/3449]  eta: 2:01:40  lr: 0.000040  loss: 0.0559 (0.0503)  time: 3.7340  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1540/3449]  eta: 2:01:03  lr: 0.000040  loss: 0.0511 (0.0503)  time: 3.8041  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1550/3449]  eta: 2:00:24  lr: 0.000040  loss: 0.0511 (0.0503)  time: 3.7886  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1560/3449]  eta: 1:59:44  lr: 0.000040  loss: 0.0478 (0.0502)  time: 3.6824  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1570/3449]  eta: 1:59:06  lr: 0.000040  loss: 0.0475 (0.0502)  time: 3.7241  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1580/3449]  eta: 1:58:28  lr: 0.000040  loss: 0.0479 (0.0503)  time: 3.8241  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1590/3449]  eta: 1:57:50  lr: 0.000040  loss: 0.0569 (0.0503)  time: 3.8111  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1600/3449]  eta: 1:57:11  lr: 0.000040  loss: 0.0569 (0.0503)  time: 3.7658  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1610/3449]  eta: 1:56:32  lr: 0.000040  loss: 0.0579 (0.0504)  time: 3.7335  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1620/3449]  eta: 1:55:55  lr: 0.000040  loss: 0.0552 (0.0504)  time: 3.7803  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1630/3449]  eta: 1:55:15  lr: 0.000040  loss: 0.0505 (0.0504)  time: 3.7736  data: 0.0003  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1640/3449]  eta: 1:54:38  lr: 0.000040  loss: 0.0523 (0.0504)  time: 3.7655  data: 0.0003  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1650/3449]  eta: 1:53:59  lr: 0.000040  loss: 0.0542 (0.0505)  time: 3.8023  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1660/3449]  eta: 1:53:21  lr: 0.000040  loss: 0.0495 (0.0504)  time: 3.7782  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1670/3449]  eta: 1:52:43  lr: 0.000040  loss: 0.0474 (0.0504)  time: 3.7925  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1680/3449]  eta: 1:52:04  lr: 0.000040  loss: 0.0512 (0.0505)  time: 3.7537  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1690/3449]  eta: 1:51:26  lr: 0.000040  loss: 0.0512 (0.0504)  time: 3.7648  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1700/3449]  eta: 1:50:47  lr: 0.000040  loss: 0.0531 (0.0504)  time: 3.7331  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1710/3449]  eta: 1:50:08  lr: 0.000040  loss: 0.0490 (0.0504)  time: 3.7105  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1720/3449]  eta: 1:49:30  lr: 0.000040  loss: 0.0475 (0.0504)  time: 3.7830  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1730/3449]  eta: 1:48:52  lr: 0.000040  loss: 0.0562 (0.0504)  time: 3.7885  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1740/3449]  eta: 1:48:13  lr: 0.000040  loss: 0.0561 (0.0504)  time: 3.7239  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1750/3449]  eta: 1:47:35  lr: 0.000040  loss: 0.0492 (0.0504)  time: 3.7546  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1760/3449]  eta: 1:46:58  lr: 0.000040  loss: 0.0492 (0.0504)  time: 3.8399  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1770/3449]  eta: 1:46:20  lr: 0.000040  loss: 0.0470 (0.0503)  time: 3.8431  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1780/3449]  eta: 1:45:42  lr: 0.000040  loss: 0.0523 (0.0504)  time: 3.8288  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1790/3449]  eta: 1:45:05  lr: 0.000040  loss: 0.0523 (0.0504)  time: 3.8607  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1800/3449]  eta: 1:44:27  lr: 0.000040  loss: 0.0520 (0.0504)  time: 3.8667  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1810/3449]  eta: 1:43:51  lr: 0.000040  loss: 0.0545 (0.0505)  time: 3.9106  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1820/3449]  eta: 1:43:13  lr: 0.000040  loss: 0.0519 (0.0504)  time: 3.8765  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1830/3449]  eta: 1:42:35  lr: 0.000040  loss: 0.0519 (0.0505)  time: 3.7848  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1840/3449]  eta: 1:41:57  lr: 0.000040  loss: 0.0519 (0.0505)  time: 3.8332  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1850/3449]  eta: 1:41:20  lr: 0.000040  loss: 0.0563 (0.0505)  time: 3.8574  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1860/3449]  eta: 1:40:42  lr: 0.000040  loss: 0.0504 (0.0504)  time: 3.8498  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1870/3449]  eta: 1:40:04  lr: 0.000040  loss: 0.0430 (0.0504)  time: 3.8199  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1880/3449]  eta: 1:39:26  lr: 0.000040  loss: 0.0440 (0.0504)  time: 3.8213  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1890/3449]  eta: 1:38:48  lr: 0.000040  loss: 0.0486 (0.0503)  time: 3.8221  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1900/3449]  eta: 1:38:11  lr: 0.000040  loss: 0.0521 (0.0504)  time: 3.8528  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1910/3449]  eta: 1:37:32  lr: 0.000040  loss: 0.0609 (0.0504)  time: 3.8219  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1920/3449]  eta: 1:36:54  lr: 0.000040  loss: 0.0575 (0.0504)  time: 3.7249  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1930/3449]  eta: 1:36:15  lr: 0.000040  loss: 0.0502 (0.0504)  time: 3.7123  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1940/3449]  eta: 1:35:37  lr: 0.000040  loss: 0.0517 (0.0504)  time: 3.7573  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1950/3449]  eta: 1:34:59  lr: 0.000040  loss: 0.0541 (0.0505)  time: 3.8061  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1960/3449]  eta: 1:34:21  lr: 0.000040  loss: 0.0496 (0.0504)  time: 3.8131  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1970/3449]  eta: 1:33:42  lr: 0.000040  loss: 0.0491 (0.0504)  time: 3.7698  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1980/3449]  eta: 1:33:05  lr: 0.000040  loss: 0.0539 (0.0505)  time: 3.7745  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [1990/3449]  eta: 1:32:26  lr: 0.000040  loss: 0.0533 (0.0505)  time: 3.7692  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2000/3449]  eta: 1:31:48  lr: 0.000040  loss: 0.0411 (0.0504)  time: 3.7336  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2010/3449]  eta: 1:31:09  lr: 0.000040  loss: 0.0437 (0.0504)  time: 3.7648  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2020/3449]  eta: 1:30:32  lr: 0.000040  loss: 0.0458 (0.0504)  time: 3.8060  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2030/3449]  eta: 1:29:54  lr: 0.000040  loss: 0.0591 (0.0505)  time: 3.8178  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2040/3449]  eta: 1:29:15  lr: 0.000040  loss: 0.0632 (0.0505)  time: 3.7256  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2050/3449]  eta: 1:28:36  lr: 0.000040  loss: 0.0514 (0.0505)  time: 3.7036  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2060/3449]  eta: 1:27:58  lr: 0.000040  loss: 0.0467 (0.0505)  time: 3.7530  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2070/3449]  eta: 1:27:19  lr: 0.000040  loss: 0.0476 (0.0505)  time: 3.7399  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2080/3449]  eta: 1:26:41  lr: 0.000040  loss: 0.0554 (0.0505)  time: 3.6997  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2090/3449]  eta: 1:26:02  lr: 0.000040  loss: 0.0554 (0.0505)  time: 3.6685  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2100/3449]  eta: 1:25:24  lr: 0.000040  loss: 0.0460 (0.0504)  time: 3.7439  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2110/3449]  eta: 1:24:46  lr: 0.000040  loss: 0.0414 (0.0504)  time: 3.7937  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2120/3449]  eta: 1:24:08  lr: 0.000040  loss: 0.0450 (0.0504)  time: 3.8379  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2130/3449]  eta: 1:23:30  lr: 0.000040  loss: 0.0498 (0.0504)  time: 3.8624  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2140/3449]  eta: 1:22:53  lr: 0.000040  loss: 0.0520 (0.0504)  time: 3.8143  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2150/3449]  eta: 1:22:14  lr: 0.000040  loss: 0.0566 (0.0504)  time: 3.7823  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2160/3449]  eta: 1:21:36  lr: 0.000040  loss: 0.0501 (0.0504)  time: 3.7339  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2170/3449]  eta: 1:20:57  lr: 0.000040  loss: 0.0476 (0.0504)  time: 3.7119  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2180/3449]  eta: 1:20:19  lr: 0.000040  loss: 0.0476 (0.0504)  time: 3.7204  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2190/3449]  eta: 1:19:41  lr: 0.000040  loss: 0.0499 (0.0504)  time: 3.7899  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2200/3449]  eta: 1:19:03  lr: 0.000040  loss: 0.0534 (0.0504)  time: 3.7968  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2210/3449]  eta: 1:18:25  lr: 0.000040  loss: 0.0439 (0.0504)  time: 3.7606  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2220/3449]  eta: 1:17:47  lr: 0.000040  loss: 0.0462 (0.0504)  time: 3.7726  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2230/3449]  eta: 1:17:09  lr: 0.000040  loss: 0.0537 (0.0504)  time: 3.8021  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2240/3449]  eta: 1:16:31  lr: 0.000040  loss: 0.0567 (0.0504)  time: 3.8302  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2250/3449]  eta: 1:15:52  lr: 0.000040  loss: 0.0579 (0.0504)  time: 3.7292  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2260/3449]  eta: 1:15:14  lr: 0.000040  loss: 0.0610 (0.0504)  time: 3.6457  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2270/3449]  eta: 1:14:35  lr: 0.000040  loss: 0.0435 (0.0504)  time: 3.6808  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2280/3449]  eta: 1:13:57  lr: 0.000040  loss: 0.0413 (0.0503)  time: 3.7076  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2290/3449]  eta: 1:13:18  lr: 0.000040  loss: 0.0481 (0.0504)  time: 3.6974  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2300/3449]  eta: 1:12:40  lr: 0.000040  loss: 0.0552 (0.0504)  time: 3.7578  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2310/3449]  eta: 1:12:02  lr: 0.000040  loss: 0.0431 (0.0503)  time: 3.7820  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2320/3449]  eta: 1:11:24  lr: 0.000040  loss: 0.0457 (0.0503)  time: 3.7561  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2330/3449]  eta: 1:10:47  lr: 0.000040  loss: 0.0506 (0.0503)  time: 3.8412  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2340/3449]  eta: 1:10:08  lr: 0.000040  loss: 0.0477 (0.0503)  time: 3.7635  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2350/3449]  eta: 1:09:30  lr: 0.000040  loss: 0.0547 (0.0504)  time: 3.7400  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2360/3449]  eta: 1:08:52  lr: 0.000040  loss: 0.0556 (0.0504)  time: 3.8399  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2370/3449]  eta: 1:08:15  lr: 0.000040  loss: 0.0524 (0.0504)  time: 3.8446  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2380/3449]  eta: 1:07:37  lr: 0.000040  loss: 0.0579 (0.0504)  time: 3.8397  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2390/3449]  eta: 1:06:59  lr: 0.000040  loss: 0.0507 (0.0504)  time: 3.8384  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2400/3449]  eta: 1:06:22  lr: 0.000040  loss: 0.0482 (0.0504)  time: 3.8483  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2410/3449]  eta: 1:05:44  lr: 0.000040  loss: 0.0487 (0.0504)  time: 3.8358  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2420/3449]  eta: 1:05:05  lr: 0.000040  loss: 0.0492 (0.0504)  time: 3.7728  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2430/3449]  eta: 1:04:27  lr: 0.000040  loss: 0.0541 (0.0504)  time: 3.7226  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2440/3449]  eta: 1:03:49  lr: 0.000040  loss: 0.0541 (0.0504)  time: 3.7748  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2450/3449]  eta: 1:03:12  lr: 0.000040  loss: 0.0509 (0.0504)  time: 3.8330  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2460/3449]  eta: 1:02:34  lr: 0.000040  loss: 0.0509 (0.0504)  time: 3.8264  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2470/3449]  eta: 1:01:56  lr: 0.000040  loss: 0.0500 (0.0504)  time: 3.8002  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2480/3449]  eta: 1:01:18  lr: 0.000040  loss: 0.0475 (0.0504)  time: 3.8210  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2490/3449]  eta: 1:00:40  lr: 0.000040  loss: 0.0508 (0.0504)  time: 3.7792  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2500/3449]  eta: 1:00:02  lr: 0.000040  loss: 0.0509 (0.0504)  time: 3.7732  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2510/3449]  eta: 0:59:24  lr: 0.000040  loss: 0.0509 (0.0504)  time: 3.7904  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2520/3449]  eta: 0:58:46  lr: 0.000040  loss: 0.0434 (0.0503)  time: 3.8014  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2530/3449]  eta: 0:58:08  lr: 0.000040  loss: 0.0431 (0.0503)  time: 3.8129  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2540/3449]  eta: 0:57:30  lr: 0.000040  loss: 0.0445 (0.0503)  time: 3.8002  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2550/3449]  eta: 0:56:52  lr: 0.000040  loss: 0.0490 (0.0503)  time: 3.7979  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2560/3449]  eta: 0:56:14  lr: 0.000040  loss: 0.0524 (0.0503)  time: 3.7353  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2570/3449]  eta: 0:55:36  lr: 0.000040  loss: 0.0539 (0.0503)  time: 3.8045  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2580/3449]  eta: 0:54:58  lr: 0.000040  loss: 0.0495 (0.0503)  time: 3.8650  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2590/3449]  eta: 0:54:20  lr: 0.000040  loss: 0.0544 (0.0503)  time: 3.7819  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2600/3449]  eta: 0:53:42  lr: 0.000040  loss: 0.0544 (0.0503)  time: 3.7565  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2610/3449]  eta: 0:53:04  lr: 0.000040  loss: 0.0456 (0.0503)  time: 3.7388  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2620/3449]  eta: 0:52:26  lr: 0.000040  loss: 0.0516 (0.0503)  time: 3.7549  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2630/3449]  eta: 0:51:48  lr: 0.000040  loss: 0.0418 (0.0503)  time: 3.7909  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2640/3449]  eta: 0:51:10  lr: 0.000040  loss: 0.0417 (0.0502)  time: 3.7375  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2650/3449]  eta: 0:50:32  lr: 0.000040  loss: 0.0435 (0.0502)  time: 3.7309  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2660/3449]  eta: 0:49:54  lr: 0.000040  loss: 0.0526 (0.0503)  time: 3.7602  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2670/3449]  eta: 0:49:16  lr: 0.000040  loss: 0.0494 (0.0502)  time: 3.8226  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2680/3449]  eta: 0:48:38  lr: 0.000040  loss: 0.0527 (0.0503)  time: 3.8913  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2690/3449]  eta: 0:48:00  lr: 0.000040  loss: 0.0527 (0.0503)  time: 3.8697  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2700/3449]  eta: 0:47:22  lr: 0.000040  loss: 0.0550 (0.0503)  time: 3.8104  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2710/3449]  eta: 0:46:44  lr: 0.000040  loss: 0.0559 (0.0503)  time: 3.6928  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2720/3449]  eta: 0:46:06  lr: 0.000040  loss: 0.0550 (0.0503)  time: 3.7379  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2730/3449]  eta: 0:45:28  lr: 0.000040  loss: 0.0508 (0.0503)  time: 3.8025  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2740/3449]  eta: 0:44:50  lr: 0.000040  loss: 0.0489 (0.0503)  time: 3.7445  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2750/3449]  eta: 0:44:12  lr: 0.000040  loss: 0.0511 (0.0503)  time: 3.7729  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2760/3449]  eta: 0:43:34  lr: 0.000040  loss: 0.0477 (0.0502)  time: 3.7664  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2770/3449]  eta: 0:42:56  lr: 0.000040  loss: 0.0499 (0.0503)  time: 3.7450  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2780/3449]  eta: 0:42:18  lr: 0.000040  loss: 0.0504 (0.0502)  time: 3.7417  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2790/3449]  eta: 0:41:40  lr: 0.000040  loss: 0.0478 (0.0502)  time: 3.7060  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2800/3449]  eta: 0:41:02  lr: 0.000040  loss: 0.0499 (0.0502)  time: 3.7579  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2810/3449]  eta: 0:40:24  lr: 0.000040  loss: 0.0455 (0.0502)  time: 3.8444  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2820/3449]  eta: 0:39:47  lr: 0.000040  loss: 0.0515 (0.0502)  time: 3.9396  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2830/3449]  eta: 0:39:08  lr: 0.000040  loss: 0.0555 (0.0502)  time: 3.8824  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2840/3449]  eta: 0:38:31  lr: 0.000040  loss: 0.0449 (0.0502)  time: 3.8004  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2850/3449]  eta: 0:37:53  lr: 0.000040  loss: 0.0444 (0.0502)  time: 3.7981  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2860/3449]  eta: 0:37:15  lr: 0.000040  loss: 0.0488 (0.0502)  time: 3.7478  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2870/3449]  eta: 0:36:37  lr: 0.000040  loss: 0.0414 (0.0502)  time: 3.8035  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2880/3449]  eta: 0:35:59  lr: 0.000040  loss: 0.0467 (0.0502)  time: 3.7850  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2890/3449]  eta: 0:35:21  lr: 0.000040  loss: 0.0474 (0.0501)  time: 3.7786  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2900/3449]  eta: 0:34:43  lr: 0.000040  loss: 0.0429 (0.0501)  time: 3.8415  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2910/3449]  eta: 0:34:05  lr: 0.000040  loss: 0.0581 (0.0502)  time: 3.8081  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2920/3449]  eta: 0:33:27  lr: 0.000040  loss: 0.0596 (0.0502)  time: 3.7619  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2930/3449]  eta: 0:32:49  lr: 0.000040  loss: 0.0580 (0.0502)  time: 3.7356  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2940/3449]  eta: 0:32:11  lr: 0.000040  loss: 0.0407 (0.0502)  time: 3.7853  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2950/3449]  eta: 0:31:33  lr: 0.000040  loss: 0.0437 (0.0501)  time: 3.8087  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2960/3449]  eta: 0:30:55  lr: 0.000040  loss: 0.0483 (0.0501)  time: 3.8080  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2970/3449]  eta: 0:30:17  lr: 0.000040  loss: 0.0545 (0.0502)  time: 3.8195  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2980/3449]  eta: 0:29:39  lr: 0.000040  loss: 0.0533 (0.0501)  time: 3.7766  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [2990/3449]  eta: 0:29:01  lr: 0.000040  loss: 0.0526 (0.0502)  time: 3.7779  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3000/3449]  eta: 0:28:23  lr: 0.000040  loss: 0.0526 (0.0501)  time: 3.8087  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3010/3449]  eta: 0:27:45  lr: 0.000040  loss: 0.0537 (0.0502)  time: 3.8427  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3020/3449]  eta: 0:27:08  lr: 0.000040  loss: 0.0534 (0.0502)  time: 3.9219  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3030/3449]  eta: 0:26:30  lr: 0.000040  loss: 0.0514 (0.0502)  time: 3.8650  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3040/3449]  eta: 0:25:52  lr: 0.000040  loss: 0.0552 (0.0502)  time: 3.7692  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3050/3449]  eta: 0:25:14  lr: 0.000040  loss: 0.0552 (0.0502)  time: 3.7883  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3060/3449]  eta: 0:24:36  lr: 0.000040  loss: 0.0556 (0.0502)  time: 3.7666  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3070/3449]  eta: 0:23:58  lr: 0.000040  loss: 0.0544 (0.0502)  time: 3.7710  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3080/3449]  eta: 0:23:20  lr: 0.000040  loss: 0.0508 (0.0502)  time: 3.8316  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3090/3449]  eta: 0:22:42  lr: 0.000040  loss: 0.0508 (0.0502)  time: 3.8208  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3100/3449]  eta: 0:22:04  lr: 0.000040  loss: 0.0518 (0.0502)  time: 3.7774  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3110/3449]  eta: 0:21:26  lr: 0.000040  loss: 0.0661 (0.0503)  time: 3.7492  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3120/3449]  eta: 0:20:48  lr: 0.000040  loss: 0.0651 (0.0503)  time: 3.7081  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3130/3449]  eta: 0:20:10  lr: 0.000040  loss: 0.0473 (0.0503)  time: 3.7422  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3140/3449]  eta: 0:19:32  lr: 0.000040  loss: 0.0492 (0.0503)  time: 3.7973  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3150/3449]  eta: 0:18:54  lr: 0.000040  loss: 0.0450 (0.0503)  time: 3.7666  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3160/3449]  eta: 0:18:16  lr: 0.000040  loss: 0.0447 (0.0503)  time: 3.7775  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3170/3449]  eta: 0:17:38  lr: 0.000040  loss: 0.0611 (0.0503)  time: 3.8338  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3180/3449]  eta: 0:17:00  lr: 0.000040  loss: 0.0590 (0.0503)  time: 3.8188  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3190/3449]  eta: 0:16:22  lr: 0.000040  loss: 0.0527 (0.0503)  time: 3.7536  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3200/3449]  eta: 0:15:44  lr: 0.000040  loss: 0.0464 (0.0503)  time: 3.7208  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3210/3449]  eta: 0:15:06  lr: 0.000040  loss: 0.0514 (0.0503)  time: 3.7585  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3220/3449]  eta: 0:14:28  lr: 0.000040  loss: 0.0514 (0.0503)  time: 3.7846  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3230/3449]  eta: 0:13:50  lr: 0.000040  loss: 0.0522 (0.0503)  time: 3.8086  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3240/3449]  eta: 0:13:13  lr: 0.000040  loss: 0.0591 (0.0504)  time: 3.8443  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3250/3449]  eta: 0:12:35  lr: 0.000040  loss: 0.0606 (0.0504)  time: 3.7697  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3260/3449]  eta: 0:11:57  lr: 0.000040  loss: 0.0539 (0.0504)  time: 3.7226  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3270/3449]  eta: 0:11:19  lr: 0.000040  loss: 0.0507 (0.0504)  time: 3.7451  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3280/3449]  eta: 0:10:41  lr: 0.000040  loss: 0.0488 (0.0504)  time: 3.7180  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3290/3449]  eta: 0:10:03  lr: 0.000040  loss: 0.0479 (0.0504)  time: 3.7353  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3300/3449]  eta: 0:09:25  lr: 0.000040  loss: 0.0475 (0.0504)  time: 3.7973  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3310/3449]  eta: 0:08:47  lr: 0.000040  loss: 0.0544 (0.0504)  time: 3.7577  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3320/3449]  eta: 0:08:09  lr: 0.000040  loss: 0.0512 (0.0504)  time: 3.7894  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3330/3449]  eta: 0:07:31  lr: 0.000040  loss: 0.0537 (0.0504)  time: 3.8682  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3340/3449]  eta: 0:06:53  lr: 0.000040  loss: 0.0549 (0.0504)  time: 3.8168  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3350/3449]  eta: 0:06:15  lr: 0.000040  loss: 0.0549 (0.0504)  time: 3.7847  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3360/3449]  eta: 0:05:37  lr: 0.000040  loss: 0.0536 (0.0504)  time: 3.7948  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3370/3449]  eta: 0:04:59  lr: 0.000040  loss: 0.0536 (0.0504)  time: 3.8065  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3380/3449]  eta: 0:04:21  lr: 0.000040  loss: 0.0494 (0.0504)  time: 3.8198  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3390/3449]  eta: 0:03:43  lr: 0.000040  loss: 0.0492 (0.0504)  time: 3.8692  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3400/3449]  eta: 0:03:05  lr: 0.000040  loss: 0.0497 (0.0504)  time: 3.8440  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3410/3449]  eta: 0:02:27  lr: 0.000040  loss: 0.0589 (0.0504)  time: 3.7542  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3420/3449]  eta: 0:01:50  lr: 0.000040  loss: 0.0564 (0.0504)  time: 3.7780  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3430/3449]  eta: 0:01:12  lr: 0.000040  loss: 0.0532 (0.0504)  time: 3.7767  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3440/3449]  eta: 0:00:34  lr: 0.000040  loss: 0.0540 (0.0504)  time: 3.7322  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [3448/3449]  eta: 0:00:03  lr: 0.000040  loss: 0.0546 (0.0504)  time: 3.7471  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:5] Total time: 3:38:04 (3.7938 s / it)\n",
      "Averaged stats: lr: 0.000040  loss: 0.0546 (0.0504)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:5]  [ 0/14]  eta: 0:04:21  loss: 0.0460 (0.0460)  time: 18.6432  data: 0.4290  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:5]  [13/14]  eta: 0:00:18  loss: 0.0392 (0.0407)  time: 18.2674  data: 0.0308  max mem: 34968\n",
      "Valid: [epoch:5] Total time: 0:04:15 (18.2767 s / it)\n",
      "Averaged stats: loss: 0.0392 (0.0407)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_5_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.041%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [   0/3449]  eta: 5:00:52  lr: 0.000050  loss: 0.0810 (0.0810)  time: 5.2342  data: 1.5278  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [  10/3449]  eta: 3:43:00  lr: 0.000050  loss: 0.0610 (0.0555)  time: 3.8910  data: 0.1390  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [  20/3449]  eta: 3:39:57  lr: 0.000050  loss: 0.0518 (0.0527)  time: 3.7796  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [  30/3449]  eta: 3:37:28  lr: 0.000050  loss: 0.0496 (0.0520)  time: 3.7753  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [  40/3449]  eta: 3:37:24  lr: 0.000050  loss: 0.0499 (0.0506)  time: 3.8029  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [  50/3449]  eta: 3:35:12  lr: 0.000050  loss: 0.0499 (0.0497)  time: 3.7720  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [  60/3449]  eta: 3:34:37  lr: 0.000050  loss: 0.0482 (0.0500)  time: 3.7456  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [  70/3449]  eta: 3:33:22  lr: 0.000050  loss: 0.0516 (0.0510)  time: 3.7626  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [  80/3449]  eta: 3:32:48  lr: 0.000050  loss: 0.0480 (0.0499)  time: 3.7596  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [  90/3449]  eta: 3:32:19  lr: 0.000050  loss: 0.0427 (0.0498)  time: 3.8060  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 100/3449]  eta: 3:31:02  lr: 0.000050  loss: 0.0427 (0.0493)  time: 3.7451  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 110/3449]  eta: 3:30:50  lr: 0.000050  loss: 0.0404 (0.0480)  time: 3.7713  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 120/3449]  eta: 3:30:00  lr: 0.000050  loss: 0.0489 (0.0484)  time: 3.8046  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 130/3449]  eta: 3:29:33  lr: 0.000050  loss: 0.0497 (0.0481)  time: 3.7872  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 140/3449]  eta: 3:28:39  lr: 0.000050  loss: 0.0491 (0.0482)  time: 3.7745  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 150/3449]  eta: 3:27:51  lr: 0.000050  loss: 0.0540 (0.0485)  time: 3.7280  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 160/3449]  eta: 3:27:06  lr: 0.000050  loss: 0.0397 (0.0479)  time: 3.7410  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 170/3449]  eta: 3:26:13  lr: 0.000050  loss: 0.0397 (0.0478)  time: 3.7208  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 180/3449]  eta: 3:25:28  lr: 0.000050  loss: 0.0407 (0.0467)  time: 3.7149  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 190/3449]  eta: 3:25:01  lr: 0.000050  loss: 0.0394 (0.0470)  time: 3.7858  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 200/3449]  eta: 3:24:32  lr: 0.000050  loss: 0.0547 (0.0475)  time: 3.8338  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 210/3449]  eta: 3:23:59  lr: 0.000050  loss: 0.0567 (0.0477)  time: 3.8177  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 220/3449]  eta: 3:23:16  lr: 0.000050  loss: 0.0543 (0.0479)  time: 3.7758  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 230/3449]  eta: 3:22:33  lr: 0.000050  loss: 0.0514 (0.0479)  time: 3.7426  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 240/3449]  eta: 3:21:56  lr: 0.000050  loss: 0.0425 (0.0478)  time: 3.7602  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 250/3449]  eta: 3:21:20  lr: 0.000050  loss: 0.0531 (0.0481)  time: 3.7824  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 260/3449]  eta: 3:20:38  lr: 0.000050  loss: 0.0575 (0.0482)  time: 3.7645  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 270/3449]  eta: 3:20:10  lr: 0.000050  loss: 0.0535 (0.0484)  time: 3.7994  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 280/3449]  eta: 3:19:37  lr: 0.000050  loss: 0.0516 (0.0481)  time: 3.8376  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 290/3449]  eta: 3:18:54  lr: 0.000050  loss: 0.0470 (0.0481)  time: 3.7755  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 300/3449]  eta: 3:18:19  lr: 0.000050  loss: 0.0511 (0.0480)  time: 3.7709  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 310/3449]  eta: 3:17:46  lr: 0.000050  loss: 0.0368 (0.0476)  time: 3.8187  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 320/3449]  eta: 3:17:15  lr: 0.000050  loss: 0.0410 (0.0477)  time: 3.8354  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 330/3449]  eta: 3:16:32  lr: 0.000050  loss: 0.0545 (0.0480)  time: 3.7883  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 340/3449]  eta: 3:16:05  lr: 0.000050  loss: 0.0496 (0.0480)  time: 3.8171  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 350/3449]  eta: 3:15:26  lr: 0.000050  loss: 0.0455 (0.0481)  time: 3.8337  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 360/3449]  eta: 3:14:46  lr: 0.000050  loss: 0.0452 (0.0479)  time: 3.7647  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 370/3449]  eta: 3:14:07  lr: 0.000050  loss: 0.0483 (0.0479)  time: 3.7659  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 380/3449]  eta: 3:13:30  lr: 0.000050  loss: 0.0483 (0.0480)  time: 3.7787  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 390/3449]  eta: 3:12:48  lr: 0.000050  loss: 0.0430 (0.0478)  time: 3.7583  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 400/3449]  eta: 3:12:15  lr: 0.000050  loss: 0.0428 (0.0478)  time: 3.7908  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 410/3449]  eta: 3:11:42  lr: 0.000050  loss: 0.0569 (0.0480)  time: 3.8490  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 420/3449]  eta: 3:11:07  lr: 0.000050  loss: 0.0625 (0.0482)  time: 3.8339  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 430/3449]  eta: 3:10:27  lr: 0.000050  loss: 0.0543 (0.0481)  time: 3.7877  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 440/3449]  eta: 3:09:54  lr: 0.000050  loss: 0.0543 (0.0485)  time: 3.8039  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 450/3449]  eta: 3:09:11  lr: 0.000050  loss: 0.0613 (0.0487)  time: 3.7843  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 460/3449]  eta: 3:08:33  lr: 0.000050  loss: 0.0543 (0.0486)  time: 3.7449  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 470/3449]  eta: 3:07:59  lr: 0.000050  loss: 0.0534 (0.0487)  time: 3.8169  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 480/3449]  eta: 3:07:24  lr: 0.000050  loss: 0.0547 (0.0488)  time: 3.8405  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 490/3449]  eta: 3:06:43  lr: 0.000050  loss: 0.0534 (0.0489)  time: 3.7793  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 500/3449]  eta: 3:06:11  lr: 0.000050  loss: 0.0541 (0.0491)  time: 3.8155  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 510/3449]  eta: 3:05:34  lr: 0.000050  loss: 0.0569 (0.0491)  time: 3.8518  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 520/3449]  eta: 3:04:56  lr: 0.000050  loss: 0.0539 (0.0491)  time: 3.7967  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 530/3449]  eta: 3:04:15  lr: 0.000050  loss: 0.0557 (0.0492)  time: 3.7563  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 540/3449]  eta: 3:03:36  lr: 0.000050  loss: 0.0503 (0.0491)  time: 3.7436  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 550/3449]  eta: 3:02:56  lr: 0.000050  loss: 0.0432 (0.0490)  time: 3.7568  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 560/3449]  eta: 3:02:21  lr: 0.000050  loss: 0.0478 (0.0490)  time: 3.7909  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 570/3449]  eta: 3:01:39  lr: 0.000050  loss: 0.0530 (0.0491)  time: 3.7766  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 580/3449]  eta: 3:01:08  lr: 0.000050  loss: 0.0525 (0.0492)  time: 3.8201  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 590/3449]  eta: 3:00:28  lr: 0.000050  loss: 0.0542 (0.0493)  time: 3.8268  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 600/3449]  eta: 2:59:48  lr: 0.000050  loss: 0.0590 (0.0494)  time: 3.7425  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 610/3449]  eta: 2:59:06  lr: 0.000050  loss: 0.0559 (0.0494)  time: 3.7243  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 620/3449]  eta: 2:58:26  lr: 0.000050  loss: 0.0551 (0.0495)  time: 3.7221  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 630/3449]  eta: 2:57:50  lr: 0.000050  loss: 0.0516 (0.0494)  time: 3.7786  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 640/3449]  eta: 2:57:18  lr: 0.000050  loss: 0.0503 (0.0494)  time: 3.8638  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 650/3449]  eta: 2:56:38  lr: 0.000050  loss: 0.0576 (0.0496)  time: 3.8278  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 660/3449]  eta: 2:55:59  lr: 0.000050  loss: 0.0571 (0.0496)  time: 3.7493  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 670/3449]  eta: 2:55:19  lr: 0.000050  loss: 0.0552 (0.0498)  time: 3.7562  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 680/3449]  eta: 2:54:42  lr: 0.000050  loss: 0.0634 (0.0499)  time: 3.7716  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 690/3449]  eta: 2:54:03  lr: 0.000050  loss: 0.0519 (0.0498)  time: 3.7713  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 700/3449]  eta: 2:53:24  lr: 0.000050  loss: 0.0475 (0.0497)  time: 3.7618  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 710/3449]  eta: 2:52:44  lr: 0.000050  loss: 0.0515 (0.0498)  time: 3.7461  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 720/3449]  eta: 2:52:04  lr: 0.000050  loss: 0.0600 (0.0499)  time: 3.7254  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 730/3449]  eta: 2:51:27  lr: 0.000050  loss: 0.0519 (0.0499)  time: 3.7724  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 740/3449]  eta: 2:50:51  lr: 0.000050  loss: 0.0503 (0.0499)  time: 3.8170  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 750/3449]  eta: 2:50:12  lr: 0.000050  loss: 0.0524 (0.0499)  time: 3.7871  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 760/3449]  eta: 2:49:31  lr: 0.000050  loss: 0.0562 (0.0500)  time: 3.7240  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 770/3449]  eta: 2:48:53  lr: 0.000050  loss: 0.0586 (0.0501)  time: 3.7411  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 780/3449]  eta: 2:48:20  lr: 0.000050  loss: 0.0559 (0.0501)  time: 3.8517  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 790/3449]  eta: 2:47:41  lr: 0.000050  loss: 0.0434 (0.0500)  time: 3.8294  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 800/3449]  eta: 2:47:00  lr: 0.000050  loss: 0.0368 (0.0499)  time: 3.7227  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 810/3449]  eta: 2:46:20  lr: 0.000050  loss: 0.0397 (0.0499)  time: 3.7128  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 820/3449]  eta: 2:45:42  lr: 0.000050  loss: 0.0476 (0.0499)  time: 3.7408  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 830/3449]  eta: 2:45:05  lr: 0.000050  loss: 0.0491 (0.0500)  time: 3.7855  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 840/3449]  eta: 2:44:28  lr: 0.000050  loss: 0.0525 (0.0499)  time: 3.8101  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 850/3449]  eta: 2:43:52  lr: 0.000050  loss: 0.0534 (0.0501)  time: 3.8232  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 860/3449]  eta: 2:43:16  lr: 0.000050  loss: 0.0534 (0.0500)  time: 3.8465  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 870/3449]  eta: 2:42:40  lr: 0.000050  loss: 0.0473 (0.0500)  time: 3.8524  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 880/3449]  eta: 2:42:01  lr: 0.000050  loss: 0.0473 (0.0500)  time: 3.7849  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 890/3449]  eta: 2:41:24  lr: 0.000050  loss: 0.0511 (0.0501)  time: 3.7695  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 900/3449]  eta: 2:40:49  lr: 0.000050  loss: 0.0511 (0.0501)  time: 3.8621  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 910/3449]  eta: 2:40:09  lr: 0.000050  loss: 0.0524 (0.0501)  time: 3.7975  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 920/3449]  eta: 2:39:29  lr: 0.000050  loss: 0.0586 (0.0502)  time: 3.7062  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 930/3449]  eta: 2:38:50  lr: 0.000050  loss: 0.0564 (0.0501)  time: 3.7312  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 940/3449]  eta: 2:38:17  lr: 0.000050  loss: 0.0451 (0.0501)  time: 3.8410  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 950/3449]  eta: 2:37:39  lr: 0.000050  loss: 0.0558 (0.0502)  time: 3.8731  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 960/3449]  eta: 2:37:02  lr: 0.000050  loss: 0.0579 (0.0502)  time: 3.8100  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 970/3449]  eta: 2:36:23  lr: 0.000050  loss: 0.0482 (0.0502)  time: 3.7791  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 980/3449]  eta: 2:35:44  lr: 0.000050  loss: 0.0492 (0.0503)  time: 3.7327  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 990/3449]  eta: 2:35:03  lr: 0.000050  loss: 0.0544 (0.0503)  time: 3.6950  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1000/3449]  eta: 2:34:24  lr: 0.000050  loss: 0.0514 (0.0502)  time: 3.7052  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1010/3449]  eta: 2:33:47  lr: 0.000050  loss: 0.0486 (0.0501)  time: 3.7780  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1020/3449]  eta: 2:33:11  lr: 0.000050  loss: 0.0505 (0.0501)  time: 3.8285  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1030/3449]  eta: 2:32:31  lr: 0.000050  loss: 0.0513 (0.0501)  time: 3.7634  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1040/3449]  eta: 2:31:52  lr: 0.000050  loss: 0.0494 (0.0501)  time: 3.7117  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1050/3449]  eta: 2:31:13  lr: 0.000050  loss: 0.0494 (0.0501)  time: 3.7419  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1060/3449]  eta: 2:30:36  lr: 0.000050  loss: 0.0561 (0.0501)  time: 3.7808  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1070/3449]  eta: 2:30:00  lr: 0.000050  loss: 0.0603 (0.0501)  time: 3.8503  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1080/3449]  eta: 2:29:23  lr: 0.000050  loss: 0.0506 (0.0501)  time: 3.8384  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1090/3449]  eta: 2:28:42  lr: 0.000050  loss: 0.0538 (0.0502)  time: 3.7211  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1100/3449]  eta: 2:28:08  lr: 0.000050  loss: 0.0538 (0.0501)  time: 3.8068  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1110/3449]  eta: 2:27:28  lr: 0.000050  loss: 0.0548 (0.0501)  time: 3.8312  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1120/3449]  eta: 2:26:52  lr: 0.000050  loss: 0.0555 (0.0501)  time: 3.7830  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1130/3449]  eta: 2:26:14  lr: 0.000050  loss: 0.0544 (0.0502)  time: 3.8200  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1140/3449]  eta: 2:25:35  lr: 0.000050  loss: 0.0585 (0.0503)  time: 3.7412  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1150/3449]  eta: 2:24:57  lr: 0.000050  loss: 0.0541 (0.0502)  time: 3.7432  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1160/3449]  eta: 2:24:20  lr: 0.000050  loss: 0.0494 (0.0502)  time: 3.7994  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1170/3449]  eta: 2:23:41  lr: 0.000050  loss: 0.0475 (0.0502)  time: 3.7844  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1180/3449]  eta: 2:23:04  lr: 0.000050  loss: 0.0497 (0.0503)  time: 3.7790  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1190/3449]  eta: 2:22:29  lr: 0.000050  loss: 0.0545 (0.0503)  time: 3.8593  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1200/3449]  eta: 2:21:52  lr: 0.000050  loss: 0.0427 (0.0503)  time: 3.8850  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1210/3449]  eta: 2:21:13  lr: 0.000050  loss: 0.0415 (0.0502)  time: 3.8010  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1220/3449]  eta: 2:20:35  lr: 0.000050  loss: 0.0485 (0.0502)  time: 3.7336  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1230/3449]  eta: 2:19:58  lr: 0.000050  loss: 0.0528 (0.0502)  time: 3.7961  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1240/3449]  eta: 2:19:20  lr: 0.000050  loss: 0.0566 (0.0503)  time: 3.8290  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1250/3449]  eta: 2:18:41  lr: 0.000050  loss: 0.0511 (0.0502)  time: 3.7332  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1260/3449]  eta: 2:18:06  lr: 0.000050  loss: 0.0511 (0.0503)  time: 3.8185  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1270/3449]  eta: 2:17:32  lr: 0.000050  loss: 0.0559 (0.0503)  time: 3.9829  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1280/3449]  eta: 2:16:53  lr: 0.000050  loss: 0.0473 (0.0502)  time: 3.8748  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1290/3449]  eta: 2:16:14  lr: 0.000050  loss: 0.0406 (0.0502)  time: 3.7345  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1300/3449]  eta: 2:15:37  lr: 0.000050  loss: 0.0482 (0.0502)  time: 3.7739  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1310/3449]  eta: 2:15:00  lr: 0.000050  loss: 0.0450 (0.0501)  time: 3.8215  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1320/3449]  eta: 2:14:24  lr: 0.000050  loss: 0.0476 (0.0501)  time: 3.8765  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1330/3449]  eta: 2:13:47  lr: 0.000050  loss: 0.0552 (0.0502)  time: 3.8920  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1340/3449]  eta: 2:13:09  lr: 0.000050  loss: 0.0537 (0.0502)  time: 3.7977  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1350/3449]  eta: 2:12:30  lr: 0.000050  loss: 0.0451 (0.0501)  time: 3.7340  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1360/3449]  eta: 2:11:53  lr: 0.000050  loss: 0.0459 (0.0501)  time: 3.7814  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1370/3449]  eta: 2:11:14  lr: 0.000050  loss: 0.0603 (0.0502)  time: 3.7756  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1380/3449]  eta: 2:10:35  lr: 0.000050  loss: 0.0603 (0.0502)  time: 3.7042  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1390/3449]  eta: 2:09:56  lr: 0.000050  loss: 0.0576 (0.0502)  time: 3.7139  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1400/3449]  eta: 2:09:19  lr: 0.000050  loss: 0.0576 (0.0503)  time: 3.7955  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1410/3449]  eta: 2:08:42  lr: 0.000050  loss: 0.0518 (0.0503)  time: 3.8412  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1420/3449]  eta: 2:08:04  lr: 0.000050  loss: 0.0549 (0.0503)  time: 3.7957  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1430/3449]  eta: 2:07:28  lr: 0.000050  loss: 0.0550 (0.0503)  time: 3.8501  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1440/3449]  eta: 2:06:49  lr: 0.000050  loss: 0.0379 (0.0502)  time: 3.8172  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1450/3449]  eta: 2:06:10  lr: 0.000050  loss: 0.0488 (0.0503)  time: 3.7273  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1460/3449]  eta: 2:05:32  lr: 0.000050  loss: 0.0514 (0.0502)  time: 3.7693  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1470/3449]  eta: 2:04:55  lr: 0.000050  loss: 0.0535 (0.0503)  time: 3.8156  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1480/3449]  eta: 2:04:17  lr: 0.000050  loss: 0.0518 (0.0503)  time: 3.7839  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1490/3449]  eta: 2:03:38  lr: 0.000050  loss: 0.0452 (0.0502)  time: 3.7226  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1500/3449]  eta: 2:03:00  lr: 0.000050  loss: 0.0436 (0.0502)  time: 3.7360  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1510/3449]  eta: 2:02:24  lr: 0.000050  loss: 0.0548 (0.0502)  time: 3.8567  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1520/3449]  eta: 2:01:46  lr: 0.000050  loss: 0.0482 (0.0502)  time: 3.8667  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1530/3449]  eta: 2:01:08  lr: 0.000050  loss: 0.0517 (0.0502)  time: 3.7822  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1540/3449]  eta: 2:00:32  lr: 0.000050  loss: 0.0517 (0.0501)  time: 3.8657  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1550/3449]  eta: 1:59:54  lr: 0.000050  loss: 0.0525 (0.0501)  time: 3.8421  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1560/3449]  eta: 1:59:16  lr: 0.000050  loss: 0.0454 (0.0501)  time: 3.7708  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1570/3449]  eta: 1:58:39  lr: 0.000050  loss: 0.0454 (0.0501)  time: 3.8567  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1580/3449]  eta: 1:58:01  lr: 0.000050  loss: 0.0493 (0.0501)  time: 3.8190  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1590/3449]  eta: 1:57:24  lr: 0.000050  loss: 0.0464 (0.0501)  time: 3.8015  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1600/3449]  eta: 1:56:46  lr: 0.000050  loss: 0.0488 (0.0501)  time: 3.8459  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1610/3449]  eta: 1:56:09  lr: 0.000050  loss: 0.0556 (0.0501)  time: 3.8145  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1620/3449]  eta: 1:55:32  lr: 0.000050  loss: 0.0556 (0.0502)  time: 3.8357  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1630/3449]  eta: 1:54:53  lr: 0.000050  loss: 0.0528 (0.0502)  time: 3.7892  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1640/3449]  eta: 1:54:15  lr: 0.000050  loss: 0.0528 (0.0502)  time: 3.7736  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1650/3449]  eta: 1:53:36  lr: 0.000050  loss: 0.0612 (0.0503)  time: 3.7494  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1660/3449]  eta: 1:52:58  lr: 0.000050  loss: 0.0462 (0.0502)  time: 3.6964  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1670/3449]  eta: 1:52:20  lr: 0.000050  loss: 0.0423 (0.0502)  time: 3.7720  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1680/3449]  eta: 1:51:44  lr: 0.000050  loss: 0.0499 (0.0502)  time: 3.8935  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1690/3449]  eta: 1:51:06  lr: 0.000050  loss: 0.0497 (0.0502)  time: 3.8633  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1700/3449]  eta: 1:50:30  lr: 0.000050  loss: 0.0497 (0.0502)  time: 3.8911  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1710/3449]  eta: 1:49:53  lr: 0.000050  loss: 0.0481 (0.0502)  time: 3.9324  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1720/3449]  eta: 1:49:14  lr: 0.000050  loss: 0.0530 (0.0502)  time: 3.7827  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1730/3449]  eta: 1:48:36  lr: 0.000050  loss: 0.0592 (0.0503)  time: 3.7398  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1740/3449]  eta: 1:47:58  lr: 0.000050  loss: 0.0579 (0.0503)  time: 3.7667  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1750/3449]  eta: 1:47:20  lr: 0.000050  loss: 0.0561 (0.0503)  time: 3.7628  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1760/3449]  eta: 1:46:42  lr: 0.000050  loss: 0.0538 (0.0503)  time: 3.7616  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1770/3449]  eta: 1:46:04  lr: 0.000050  loss: 0.0455 (0.0502)  time: 3.8056  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1780/3449]  eta: 1:45:27  lr: 0.000050  loss: 0.0513 (0.0503)  time: 3.8530  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1790/3449]  eta: 1:44:48  lr: 0.000050  loss: 0.0596 (0.0503)  time: 3.7569  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1800/3449]  eta: 1:44:10  lr: 0.000050  loss: 0.0545 (0.0503)  time: 3.7444  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1810/3449]  eta: 1:43:32  lr: 0.000050  loss: 0.0505 (0.0503)  time: 3.8148  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1820/3449]  eta: 1:42:55  lr: 0.000050  loss: 0.0537 (0.0503)  time: 3.8150  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1830/3449]  eta: 1:42:16  lr: 0.000050  loss: 0.0585 (0.0504)  time: 3.7353  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1840/3449]  eta: 1:41:38  lr: 0.000050  loss: 0.0551 (0.0504)  time: 3.7162  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1850/3449]  eta: 1:41:01  lr: 0.000050  loss: 0.0532 (0.0504)  time: 3.8478  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1860/3449]  eta: 1:40:23  lr: 0.000050  loss: 0.0512 (0.0504)  time: 3.8474  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1870/3449]  eta: 1:39:45  lr: 0.000050  loss: 0.0447 (0.0503)  time: 3.7957  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1880/3449]  eta: 1:39:07  lr: 0.000050  loss: 0.0439 (0.0503)  time: 3.7921  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1890/3449]  eta: 1:38:30  lr: 0.000050  loss: 0.0490 (0.0503)  time: 3.8134  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1900/3449]  eta: 1:37:52  lr: 0.000050  loss: 0.0549 (0.0504)  time: 3.8051  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1910/3449]  eta: 1:37:13  lr: 0.000050  loss: 0.0592 (0.0504)  time: 3.7423  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1920/3449]  eta: 1:36:35  lr: 0.000050  loss: 0.0559 (0.0504)  time: 3.7324  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1930/3449]  eta: 1:35:57  lr: 0.000050  loss: 0.0397 (0.0503)  time: 3.7705  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1940/3449]  eta: 1:35:20  lr: 0.000050  loss: 0.0444 (0.0503)  time: 3.8286  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1950/3449]  eta: 1:34:42  lr: 0.000050  loss: 0.0465 (0.0504)  time: 3.8701  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1960/3449]  eta: 1:34:05  lr: 0.000050  loss: 0.0561 (0.0504)  time: 3.8474  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1970/3449]  eta: 1:33:26  lr: 0.000050  loss: 0.0559 (0.0504)  time: 3.7891  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1980/3449]  eta: 1:32:48  lr: 0.000050  loss: 0.0559 (0.0504)  time: 3.7395  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [1990/3449]  eta: 1:32:10  lr: 0.000050  loss: 0.0582 (0.0504)  time: 3.7462  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2000/3449]  eta: 1:31:32  lr: 0.000050  loss: 0.0485 (0.0504)  time: 3.7799  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2010/3449]  eta: 1:30:54  lr: 0.000050  loss: 0.0446 (0.0504)  time: 3.7554  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2020/3449]  eta: 1:30:15  lr: 0.000050  loss: 0.0504 (0.0503)  time: 3.6956  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2030/3449]  eta: 1:29:37  lr: 0.000050  loss: 0.0537 (0.0504)  time: 3.7262  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2040/3449]  eta: 1:29:00  lr: 0.000050  loss: 0.0537 (0.0504)  time: 3.8602  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2050/3449]  eta: 1:28:23  lr: 0.000050  loss: 0.0451 (0.0503)  time: 3.9132  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2060/3449]  eta: 1:27:45  lr: 0.000050  loss: 0.0481 (0.0504)  time: 3.8260  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2070/3449]  eta: 1:27:07  lr: 0.000050  loss: 0.0517 (0.0504)  time: 3.7853  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2080/3449]  eta: 1:26:29  lr: 0.000050  loss: 0.0517 (0.0504)  time: 3.8241  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2090/3449]  eta: 1:25:52  lr: 0.000050  loss: 0.0494 (0.0504)  time: 3.8520  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2100/3449]  eta: 1:25:14  lr: 0.000050  loss: 0.0432 (0.0504)  time: 3.8176  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2110/3449]  eta: 1:24:37  lr: 0.000050  loss: 0.0421 (0.0503)  time: 3.8316  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2120/3449]  eta: 1:23:59  lr: 0.000050  loss: 0.0551 (0.0504)  time: 3.8641  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2130/3449]  eta: 1:23:22  lr: 0.000050  loss: 0.0559 (0.0504)  time: 3.8711  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2140/3449]  eta: 1:22:43  lr: 0.000050  loss: 0.0497 (0.0504)  time: 3.7888  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2150/3449]  eta: 1:22:06  lr: 0.000050  loss: 0.0475 (0.0504)  time: 3.7575  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2160/3449]  eta: 1:21:27  lr: 0.000050  loss: 0.0456 (0.0504)  time: 3.7776  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2170/3449]  eta: 1:20:49  lr: 0.000050  loss: 0.0525 (0.0504)  time: 3.7532  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2180/3449]  eta: 1:20:11  lr: 0.000050  loss: 0.0496 (0.0503)  time: 3.7772  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2190/3449]  eta: 1:19:33  lr: 0.000050  loss: 0.0488 (0.0503)  time: 3.7868  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2200/3449]  eta: 1:18:56  lr: 0.000050  loss: 0.0530 (0.0504)  time: 3.8549  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2210/3449]  eta: 1:18:18  lr: 0.000050  loss: 0.0517 (0.0504)  time: 3.8684  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2220/3449]  eta: 1:17:41  lr: 0.000050  loss: 0.0421 (0.0503)  time: 3.8357  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2230/3449]  eta: 1:17:03  lr: 0.000050  loss: 0.0471 (0.0504)  time: 3.8036  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2240/3449]  eta: 1:16:25  lr: 0.000050  loss: 0.0466 (0.0503)  time: 3.7915  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2250/3449]  eta: 1:15:47  lr: 0.000050  loss: 0.0466 (0.0503)  time: 3.8080  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2260/3449]  eta: 1:15:09  lr: 0.000050  loss: 0.0561 (0.0504)  time: 3.7623  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2270/3449]  eta: 1:14:30  lr: 0.000050  loss: 0.0455 (0.0503)  time: 3.7047  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2280/3449]  eta: 1:13:52  lr: 0.000050  loss: 0.0455 (0.0503)  time: 3.7226  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2290/3449]  eta: 1:13:14  lr: 0.000050  loss: 0.0469 (0.0503)  time: 3.7494  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2300/3449]  eta: 1:12:36  lr: 0.000050  loss: 0.0580 (0.0503)  time: 3.7693  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2310/3449]  eta: 1:11:58  lr: 0.000050  loss: 0.0521 (0.0503)  time: 3.7484  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2320/3449]  eta: 1:11:20  lr: 0.000050  loss: 0.0530 (0.0503)  time: 3.7332  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2330/3449]  eta: 1:10:42  lr: 0.000050  loss: 0.0534 (0.0503)  time: 3.8453  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2340/3449]  eta: 1:10:04  lr: 0.000050  loss: 0.0529 (0.0503)  time: 3.8288  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2350/3449]  eta: 1:09:27  lr: 0.000050  loss: 0.0546 (0.0503)  time: 3.7918  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2360/3449]  eta: 1:08:49  lr: 0.000050  loss: 0.0420 (0.0503)  time: 3.8148  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2370/3449]  eta: 1:08:11  lr: 0.000050  loss: 0.0539 (0.0503)  time: 3.7917  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2380/3449]  eta: 1:07:32  lr: 0.000050  loss: 0.0556 (0.0503)  time: 3.7559  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2390/3449]  eta: 1:06:54  lr: 0.000050  loss: 0.0481 (0.0503)  time: 3.7136  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2400/3449]  eta: 1:06:16  lr: 0.000050  loss: 0.0420 (0.0503)  time: 3.7743  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2410/3449]  eta: 1:05:38  lr: 0.000050  loss: 0.0388 (0.0503)  time: 3.7658  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2420/3449]  eta: 1:05:00  lr: 0.000050  loss: 0.0520 (0.0503)  time: 3.7598  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2430/3449]  eta: 1:04:23  lr: 0.000050  loss: 0.0546 (0.0503)  time: 3.8685  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2440/3449]  eta: 1:03:45  lr: 0.000050  loss: 0.0515 (0.0503)  time: 3.8820  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2450/3449]  eta: 1:03:07  lr: 0.000050  loss: 0.0472 (0.0503)  time: 3.8355  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2460/3449]  eta: 1:02:30  lr: 0.000050  loss: 0.0536 (0.0502)  time: 3.8650  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2470/3449]  eta: 1:01:52  lr: 0.000050  loss: 0.0538 (0.0503)  time: 3.8210  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2480/3449]  eta: 1:01:14  lr: 0.000050  loss: 0.0543 (0.0503)  time: 3.7287  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2490/3449]  eta: 1:00:35  lr: 0.000050  loss: 0.0492 (0.0503)  time: 3.7001  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2500/3449]  eta: 0:59:58  lr: 0.000050  loss: 0.0503 (0.0503)  time: 3.7595  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2510/3449]  eta: 0:59:20  lr: 0.000050  loss: 0.0561 (0.0503)  time: 3.8263  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2520/3449]  eta: 0:58:42  lr: 0.000050  loss: 0.0473 (0.0502)  time: 3.8317  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2530/3449]  eta: 0:58:04  lr: 0.000050  loss: 0.0461 (0.0502)  time: 3.7975  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2540/3449]  eta: 0:57:26  lr: 0.000050  loss: 0.0448 (0.0502)  time: 3.7552  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2550/3449]  eta: 0:56:48  lr: 0.000050  loss: 0.0474 (0.0502)  time: 3.7418  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2560/3449]  eta: 0:56:10  lr: 0.000050  loss: 0.0548 (0.0502)  time: 3.8296  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2570/3449]  eta: 0:55:32  lr: 0.000050  loss: 0.0522 (0.0502)  time: 3.8541  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2580/3449]  eta: 0:54:54  lr: 0.000050  loss: 0.0485 (0.0502)  time: 3.7225  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2590/3449]  eta: 0:54:16  lr: 0.000050  loss: 0.0483 (0.0502)  time: 3.6865  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2600/3449]  eta: 0:53:38  lr: 0.000050  loss: 0.0438 (0.0502)  time: 3.7434  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2610/3449]  eta: 0:53:00  lr: 0.000050  loss: 0.0438 (0.0502)  time: 3.7754  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2620/3449]  eta: 0:52:22  lr: 0.000050  loss: 0.0478 (0.0502)  time: 3.8044  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2630/3449]  eta: 0:51:44  lr: 0.000050  loss: 0.0478 (0.0502)  time: 3.7905  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2640/3449]  eta: 0:51:06  lr: 0.000050  loss: 0.0464 (0.0501)  time: 3.8051  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2650/3449]  eta: 0:50:29  lr: 0.000050  loss: 0.0522 (0.0501)  time: 3.8459  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2660/3449]  eta: 0:49:50  lr: 0.000050  loss: 0.0581 (0.0502)  time: 3.7351  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2670/3449]  eta: 0:49:12  lr: 0.000050  loss: 0.0567 (0.0502)  time: 3.7081  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2680/3449]  eta: 0:48:34  lr: 0.000050  loss: 0.0581 (0.0502)  time: 3.7609  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2690/3449]  eta: 0:47:57  lr: 0.000050  loss: 0.0607 (0.0502)  time: 3.7927  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2700/3449]  eta: 0:47:19  lr: 0.000050  loss: 0.0601 (0.0503)  time: 3.8226  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2710/3449]  eta: 0:46:41  lr: 0.000050  loss: 0.0601 (0.0503)  time: 3.7725  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2720/3449]  eta: 0:46:03  lr: 0.000050  loss: 0.0559 (0.0503)  time: 3.7919  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2730/3449]  eta: 0:45:25  lr: 0.000050  loss: 0.0487 (0.0502)  time: 3.8449  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2740/3449]  eta: 0:44:47  lr: 0.000050  loss: 0.0451 (0.0502)  time: 3.8392  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2750/3449]  eta: 0:44:10  lr: 0.000050  loss: 0.0484 (0.0502)  time: 3.8412  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2760/3449]  eta: 0:43:31  lr: 0.000050  loss: 0.0507 (0.0502)  time: 3.7813  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2770/3449]  eta: 0:42:54  lr: 0.000050  loss: 0.0487 (0.0502)  time: 3.7918  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2780/3449]  eta: 0:42:16  lr: 0.000050  loss: 0.0478 (0.0502)  time: 3.8250  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2790/3449]  eta: 0:41:38  lr: 0.000050  loss: 0.0494 (0.0502)  time: 3.7221  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2800/3449]  eta: 0:41:00  lr: 0.000050  loss: 0.0494 (0.0502)  time: 3.7679  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2810/3449]  eta: 0:40:22  lr: 0.000050  loss: 0.0485 (0.0502)  time: 3.8196  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2820/3449]  eta: 0:39:44  lr: 0.000050  loss: 0.0513 (0.0502)  time: 3.7174  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2830/3449]  eta: 0:39:06  lr: 0.000050  loss: 0.0681 (0.0503)  time: 3.7315  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2840/3449]  eta: 0:38:28  lr: 0.000050  loss: 0.0448 (0.0502)  time: 3.8560  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2850/3449]  eta: 0:37:50  lr: 0.000050  loss: 0.0490 (0.0502)  time: 3.8920  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2860/3449]  eta: 0:37:13  lr: 0.000050  loss: 0.0573 (0.0502)  time: 3.8791  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2870/3449]  eta: 0:36:35  lr: 0.000050  loss: 0.0475 (0.0502)  time: 3.8405  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2880/3449]  eta: 0:35:57  lr: 0.000050  loss: 0.0475 (0.0502)  time: 3.7765  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2890/3449]  eta: 0:35:19  lr: 0.000050  loss: 0.0448 (0.0502)  time: 3.7934  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2900/3449]  eta: 0:34:41  lr: 0.000050  loss: 0.0414 (0.0501)  time: 3.7878  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2910/3449]  eta: 0:34:03  lr: 0.000050  loss: 0.0479 (0.0501)  time: 3.7757  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2920/3449]  eta: 0:33:25  lr: 0.000050  loss: 0.0560 (0.0502)  time: 3.7612  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2930/3449]  eta: 0:32:47  lr: 0.000050  loss: 0.0559 (0.0502)  time: 3.7921  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2940/3449]  eta: 0:32:10  lr: 0.000050  loss: 0.0522 (0.0501)  time: 3.8950  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2950/3449]  eta: 0:31:32  lr: 0.000050  loss: 0.0518 (0.0501)  time: 3.8448  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2960/3449]  eta: 0:30:53  lr: 0.000050  loss: 0.0531 (0.0502)  time: 3.6805  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2970/3449]  eta: 0:30:16  lr: 0.000050  loss: 0.0590 (0.0502)  time: 3.7140  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2980/3449]  eta: 0:29:38  lr: 0.000050  loss: 0.0550 (0.0502)  time: 3.8375  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [2990/3449]  eta: 0:29:00  lr: 0.000050  loss: 0.0550 (0.0502)  time: 3.8385  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3000/3449]  eta: 0:28:22  lr: 0.000050  loss: 0.0577 (0.0502)  time: 3.8442  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3010/3449]  eta: 0:27:44  lr: 0.000050  loss: 0.0500 (0.0502)  time: 3.8347  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3020/3449]  eta: 0:27:06  lr: 0.000050  loss: 0.0470 (0.0502)  time: 3.8312  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3030/3449]  eta: 0:26:28  lr: 0.000050  loss: 0.0556 (0.0503)  time: 3.8004  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3040/3449]  eta: 0:25:50  lr: 0.000050  loss: 0.0556 (0.0502)  time: 3.7649  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3050/3449]  eta: 0:25:12  lr: 0.000050  loss: 0.0470 (0.0502)  time: 3.7816  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3060/3449]  eta: 0:24:35  lr: 0.000050  loss: 0.0470 (0.0503)  time: 3.7875  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3070/3449]  eta: 0:23:57  lr: 0.000050  loss: 0.0602 (0.0503)  time: 3.7968  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3080/3449]  eta: 0:23:19  lr: 0.000050  loss: 0.0546 (0.0503)  time: 3.7199  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3090/3449]  eta: 0:22:41  lr: 0.000050  loss: 0.0542 (0.0503)  time: 3.7600  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3100/3449]  eta: 0:22:03  lr: 0.000050  loss: 0.0491 (0.0503)  time: 3.8759  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3110/3449]  eta: 0:21:25  lr: 0.000050  loss: 0.0534 (0.0503)  time: 3.8418  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3120/3449]  eta: 0:20:47  lr: 0.000050  loss: 0.0529 (0.0503)  time: 3.8624  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3130/3449]  eta: 0:20:09  lr: 0.000050  loss: 0.0450 (0.0503)  time: 3.8233  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3140/3449]  eta: 0:19:31  lr: 0.000050  loss: 0.0475 (0.0503)  time: 3.7686  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3150/3449]  eta: 0:18:53  lr: 0.000050  loss: 0.0476 (0.0503)  time: 3.8335  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3160/3449]  eta: 0:18:15  lr: 0.000050  loss: 0.0524 (0.0503)  time: 3.7919  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3170/3449]  eta: 0:17:38  lr: 0.000050  loss: 0.0618 (0.0503)  time: 3.7654  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3180/3449]  eta: 0:17:00  lr: 0.000050  loss: 0.0570 (0.0503)  time: 3.7380  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3190/3449]  eta: 0:16:22  lr: 0.000050  loss: 0.0525 (0.0503)  time: 3.7812  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3200/3449]  eta: 0:15:44  lr: 0.000050  loss: 0.0523 (0.0503)  time: 3.8317  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3210/3449]  eta: 0:15:06  lr: 0.000050  loss: 0.0515 (0.0503)  time: 3.8252  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3220/3449]  eta: 0:14:28  lr: 0.000050  loss: 0.0515 (0.0503)  time: 3.8762  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3230/3449]  eta: 0:13:50  lr: 0.000050  loss: 0.0532 (0.0503)  time: 3.8675  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3240/3449]  eta: 0:13:12  lr: 0.000050  loss: 0.0566 (0.0504)  time: 3.7892  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3250/3449]  eta: 0:12:34  lr: 0.000050  loss: 0.0592 (0.0504)  time: 3.7322  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3260/3449]  eta: 0:11:56  lr: 0.000050  loss: 0.0490 (0.0504)  time: 3.7872  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3270/3449]  eta: 0:11:18  lr: 0.000050  loss: 0.0469 (0.0504)  time: 3.8133  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3280/3449]  eta: 0:10:40  lr: 0.000050  loss: 0.0503 (0.0504)  time: 3.7207  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3290/3449]  eta: 0:10:02  lr: 0.000050  loss: 0.0483 (0.0504)  time: 3.7034  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3300/3449]  eta: 0:09:25  lr: 0.000050  loss: 0.0479 (0.0504)  time: 3.7537  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3310/3449]  eta: 0:08:47  lr: 0.000050  loss: 0.0585 (0.0504)  time: 3.7239  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3320/3449]  eta: 0:08:09  lr: 0.000050  loss: 0.0617 (0.0504)  time: 3.6946  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3330/3449]  eta: 0:07:31  lr: 0.000050  loss: 0.0533 (0.0504)  time: 3.7589  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3340/3449]  eta: 0:06:53  lr: 0.000050  loss: 0.0533 (0.0504)  time: 3.7945  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3350/3449]  eta: 0:06:15  lr: 0.000050  loss: 0.0499 (0.0504)  time: 3.7991  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3360/3449]  eta: 0:05:37  lr: 0.000050  loss: 0.0519 (0.0504)  time: 3.8608  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3370/3449]  eta: 0:04:59  lr: 0.000050  loss: 0.0519 (0.0504)  time: 3.7969  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3380/3449]  eta: 0:04:21  lr: 0.000050  loss: 0.0440 (0.0504)  time: 3.7531  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3390/3449]  eta: 0:03:43  lr: 0.000050  loss: 0.0481 (0.0504)  time: 3.7448  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3400/3449]  eta: 0:03:05  lr: 0.000050  loss: 0.0509 (0.0504)  time: 3.7617  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3410/3449]  eta: 0:02:27  lr: 0.000050  loss: 0.0564 (0.0504)  time: 3.8100  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3420/3449]  eta: 0:01:49  lr: 0.000050  loss: 0.0533 (0.0505)  time: 3.8109  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3430/3449]  eta: 0:01:12  lr: 0.000050  loss: 0.0579 (0.0505)  time: 3.8464  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3440/3449]  eta: 0:00:34  lr: 0.000050  loss: 0.0621 (0.0505)  time: 3.8554  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [3448/3449]  eta: 0:00:03  lr: 0.000050  loss: 0.0577 (0.0505)  time: 3.8527  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:6] Total time: 3:37:59 (3.7922 s / it)\n",
      "Averaged stats: lr: 0.000050  loss: 0.0577 (0.0505)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:6]  [ 0/14]  eta: 0:04:21  loss: 0.0459 (0.0459)  time: 18.7090  data: 0.4961  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:6]  [13/14]  eta: 0:00:18  loss: 0.0392 (0.0407)  time: 18.2675  data: 0.0356  max mem: 34968\n",
      "Valid: [epoch:6] Total time: 0:04:15 (18.2797 s / it)\n",
      "Averaged stats: loss: 0.0392 (0.0407)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_6_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.041%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [   0/3449]  eta: 5:02:09  lr: 0.000060  loss: 0.0374 (0.0374)  time: 5.2563  data: 1.8266  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [  10/3449]  eta: 3:45:21  lr: 0.000060  loss: 0.0605 (0.0578)  time: 3.9318  data: 0.1663  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [  20/3449]  eta: 3:41:31  lr: 0.000060  loss: 0.0576 (0.0557)  time: 3.8072  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [  30/3449]  eta: 3:39:47  lr: 0.000060  loss: 0.0476 (0.0517)  time: 3.8161  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [  40/3449]  eta: 3:36:40  lr: 0.000060  loss: 0.0461 (0.0506)  time: 3.7481  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [  50/3449]  eta: 3:35:12  lr: 0.000060  loss: 0.0446 (0.0483)  time: 3.7086  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [  60/3449]  eta: 3:34:28  lr: 0.000060  loss: 0.0443 (0.0494)  time: 3.7632  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [  70/3449]  eta: 3:34:00  lr: 0.000060  loss: 0.0530 (0.0510)  time: 3.8036  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [  80/3449]  eta: 3:33:09  lr: 0.000060  loss: 0.0477 (0.0491)  time: 3.7936  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [  90/3449]  eta: 3:32:26  lr: 0.000060  loss: 0.0345 (0.0483)  time: 3.7751  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 100/3449]  eta: 3:31:34  lr: 0.000060  loss: 0.0411 (0.0486)  time: 3.7677  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 110/3449]  eta: 3:30:50  lr: 0.000060  loss: 0.0457 (0.0479)  time: 3.7606  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 120/3449]  eta: 3:29:54  lr: 0.000060  loss: 0.0477 (0.0484)  time: 3.7471  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 130/3449]  eta: 3:28:48  lr: 0.000060  loss: 0.0494 (0.0480)  time: 3.6979  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 140/3449]  eta: 3:28:08  lr: 0.000060  loss: 0.0494 (0.0478)  time: 3.7186  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 150/3449]  eta: 3:27:50  lr: 0.000060  loss: 0.0546 (0.0480)  time: 3.8151  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 160/3449]  eta: 3:27:12  lr: 0.000060  loss: 0.0498 (0.0476)  time: 3.8201  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 170/3449]  eta: 3:26:58  lr: 0.000060  loss: 0.0490 (0.0478)  time: 3.8424  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 180/3449]  eta: 3:26:14  lr: 0.000060  loss: 0.0431 (0.0474)  time: 3.8297  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 190/3449]  eta: 3:25:56  lr: 0.000060  loss: 0.0383 (0.0470)  time: 3.8269  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 200/3449]  eta: 3:25:13  lr: 0.000060  loss: 0.0519 (0.0474)  time: 3.8316  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 210/3449]  eta: 3:24:39  lr: 0.000060  loss: 0.0531 (0.0476)  time: 3.7882  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 220/3449]  eta: 3:23:50  lr: 0.000060  loss: 0.0531 (0.0478)  time: 3.7641  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 230/3449]  eta: 3:23:20  lr: 0.000060  loss: 0.0558 (0.0482)  time: 3.7795  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 240/3449]  eta: 3:22:58  lr: 0.000060  loss: 0.0558 (0.0484)  time: 3.8754  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 250/3449]  eta: 3:22:05  lr: 0.000060  loss: 0.0554 (0.0487)  time: 3.7915  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 260/3449]  eta: 3:21:23  lr: 0.000060  loss: 0.0525 (0.0487)  time: 3.7188  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 270/3449]  eta: 3:20:53  lr: 0.000060  loss: 0.0483 (0.0487)  time: 3.8089  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 280/3449]  eta: 3:20:24  lr: 0.000060  loss: 0.0540 (0.0487)  time: 3.8655  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 290/3449]  eta: 3:19:43  lr: 0.000060  loss: 0.0473 (0.0487)  time: 3.8176  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 300/3449]  eta: 3:19:00  lr: 0.000060  loss: 0.0476 (0.0490)  time: 3.7550  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 310/3449]  eta: 3:18:22  lr: 0.000060  loss: 0.0452 (0.0488)  time: 3.7695  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 320/3449]  eta: 3:17:44  lr: 0.000060  loss: 0.0444 (0.0489)  time: 3.7913  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 330/3449]  eta: 3:17:06  lr: 0.000060  loss: 0.0507 (0.0490)  time: 3.7900  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 340/3449]  eta: 3:16:28  lr: 0.000060  loss: 0.0463 (0.0488)  time: 3.7874  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 350/3449]  eta: 3:15:42  lr: 0.000060  loss: 0.0463 (0.0487)  time: 3.7471  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 360/3449]  eta: 3:15:02  lr: 0.000060  loss: 0.0485 (0.0490)  time: 3.7367  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 370/3449]  eta: 3:14:22  lr: 0.000060  loss: 0.0575 (0.0490)  time: 3.7631  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 380/3449]  eta: 3:13:46  lr: 0.000060  loss: 0.0537 (0.0490)  time: 3.7864  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 390/3449]  eta: 3:13:11  lr: 0.000060  loss: 0.0455 (0.0490)  time: 3.8181  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 400/3449]  eta: 3:12:39  lr: 0.000060  loss: 0.0455 (0.0489)  time: 3.8438  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 410/3449]  eta: 3:12:03  lr: 0.000060  loss: 0.0486 (0.0489)  time: 3.8399  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 420/3449]  eta: 3:11:25  lr: 0.000060  loss: 0.0550 (0.0491)  time: 3.8031  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 430/3449]  eta: 3:10:42  lr: 0.000060  loss: 0.0570 (0.0490)  time: 3.7570  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 440/3449]  eta: 3:10:20  lr: 0.000060  loss: 0.0600 (0.0493)  time: 3.8743  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 450/3449]  eta: 3:09:40  lr: 0.000060  loss: 0.0610 (0.0496)  time: 3.8959  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 460/3449]  eta: 3:08:55  lr: 0.000060  loss: 0.0533 (0.0495)  time: 3.7269  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 470/3449]  eta: 3:08:24  lr: 0.000060  loss: 0.0556 (0.0497)  time: 3.7882  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 480/3449]  eta: 3:07:42  lr: 0.000060  loss: 0.0639 (0.0500)  time: 3.8086  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 490/3449]  eta: 3:07:02  lr: 0.000060  loss: 0.0463 (0.0498)  time: 3.7494  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 500/3449]  eta: 3:06:22  lr: 0.000060  loss: 0.0426 (0.0497)  time: 3.7627  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 510/3449]  eta: 3:05:48  lr: 0.000060  loss: 0.0425 (0.0495)  time: 3.8040  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 520/3449]  eta: 3:05:03  lr: 0.000060  loss: 0.0508 (0.0496)  time: 3.7659  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 530/3449]  eta: 3:04:25  lr: 0.000060  loss: 0.0576 (0.0497)  time: 3.7299  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 540/3449]  eta: 3:03:47  lr: 0.000060  loss: 0.0553 (0.0497)  time: 3.7827  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 550/3449]  eta: 3:03:12  lr: 0.000060  loss: 0.0510 (0.0498)  time: 3.8159  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 560/3449]  eta: 3:02:34  lr: 0.000060  loss: 0.0500 (0.0497)  time: 3.8190  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 570/3449]  eta: 3:01:58  lr: 0.000060  loss: 0.0560 (0.0499)  time: 3.8074  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 580/3449]  eta: 3:01:27  lr: 0.000060  loss: 0.0560 (0.0499)  time: 3.8870  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 590/3449]  eta: 3:00:50  lr: 0.000060  loss: 0.0499 (0.0500)  time: 3.8746  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 600/3449]  eta: 3:00:09  lr: 0.000060  loss: 0.0541 (0.0500)  time: 3.7730  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 610/3449]  eta: 2:59:32  lr: 0.000060  loss: 0.0522 (0.0501)  time: 3.7728  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 620/3449]  eta: 2:58:50  lr: 0.000060  loss: 0.0522 (0.0501)  time: 3.7578  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 630/3449]  eta: 2:58:14  lr: 0.000060  loss: 0.0493 (0.0500)  time: 3.7778  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 640/3449]  eta: 2:57:35  lr: 0.000060  loss: 0.0449 (0.0500)  time: 3.8070  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 650/3449]  eta: 2:56:58  lr: 0.000060  loss: 0.0534 (0.0501)  time: 3.7867  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 660/3449]  eta: 2:56:16  lr: 0.000060  loss: 0.0534 (0.0501)  time: 3.7534  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 670/3449]  eta: 2:55:40  lr: 0.000060  loss: 0.0607 (0.0503)  time: 3.7670  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 680/3449]  eta: 2:55:02  lr: 0.000060  loss: 0.0612 (0.0502)  time: 3.8086  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 690/3449]  eta: 2:54:26  lr: 0.000060  loss: 0.0515 (0.0502)  time: 3.8221  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 700/3449]  eta: 2:53:44  lr: 0.000060  loss: 0.0515 (0.0503)  time: 3.7666  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 710/3449]  eta: 2:53:07  lr: 0.000060  loss: 0.0568 (0.0505)  time: 3.7450  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 720/3449]  eta: 2:52:36  lr: 0.000060  loss: 0.0568 (0.0505)  time: 3.8973  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 730/3449]  eta: 2:51:55  lr: 0.000060  loss: 0.0532 (0.0505)  time: 3.8555  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 740/3449]  eta: 2:51:18  lr: 0.000060  loss: 0.0466 (0.0504)  time: 3.7605  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 750/3449]  eta: 2:50:38  lr: 0.000060  loss: 0.0465 (0.0504)  time: 3.7760  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 760/3449]  eta: 2:50:05  lr: 0.000060  loss: 0.0525 (0.0505)  time: 3.8467  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 770/3449]  eta: 2:49:26  lr: 0.000060  loss: 0.0501 (0.0505)  time: 3.8427  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 780/3449]  eta: 2:48:45  lr: 0.000060  loss: 0.0501 (0.0505)  time: 3.7371  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 790/3449]  eta: 2:48:03  lr: 0.000060  loss: 0.0523 (0.0504)  time: 3.6989  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 800/3449]  eta: 2:47:27  lr: 0.000060  loss: 0.0439 (0.0503)  time: 3.7496  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 810/3449]  eta: 2:46:47  lr: 0.000060  loss: 0.0391 (0.0502)  time: 3.7867  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 820/3449]  eta: 2:46:07  lr: 0.000060  loss: 0.0394 (0.0501)  time: 3.7343  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 830/3449]  eta: 2:45:29  lr: 0.000060  loss: 0.0472 (0.0501)  time: 3.7520  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 840/3449]  eta: 2:44:48  lr: 0.000060  loss: 0.0472 (0.0501)  time: 3.7417  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 850/3449]  eta: 2:44:09  lr: 0.000060  loss: 0.0495 (0.0502)  time: 3.7171  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 860/3449]  eta: 2:43:30  lr: 0.000060  loss: 0.0493 (0.0502)  time: 3.7514  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 870/3449]  eta: 2:42:54  lr: 0.000060  loss: 0.0437 (0.0501)  time: 3.8152  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 880/3449]  eta: 2:42:11  lr: 0.000060  loss: 0.0466 (0.0501)  time: 3.7352  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 890/3449]  eta: 2:41:29  lr: 0.000060  loss: 0.0505 (0.0501)  time: 3.6336  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 900/3449]  eta: 2:40:50  lr: 0.000060  loss: 0.0577 (0.0502)  time: 3.6895  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 910/3449]  eta: 2:40:11  lr: 0.000060  loss: 0.0613 (0.0503)  time: 3.7304  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 920/3449]  eta: 2:39:32  lr: 0.000060  loss: 0.0579 (0.0503)  time: 3.7407  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 930/3449]  eta: 2:38:57  lr: 0.000060  loss: 0.0549 (0.0503)  time: 3.8324  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 940/3449]  eta: 2:38:22  lr: 0.000060  loss: 0.0478 (0.0503)  time: 3.9007  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 950/3449]  eta: 2:37:44  lr: 0.000060  loss: 0.0486 (0.0503)  time: 3.8322  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 960/3449]  eta: 2:37:06  lr: 0.000060  loss: 0.0566 (0.0503)  time: 3.7829  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 970/3449]  eta: 2:36:26  lr: 0.000060  loss: 0.0565 (0.0504)  time: 3.7461  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 980/3449]  eta: 2:35:46  lr: 0.000060  loss: 0.0546 (0.0504)  time: 3.7070  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 990/3449]  eta: 2:35:08  lr: 0.000060  loss: 0.0541 (0.0503)  time: 3.7442  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1000/3449]  eta: 2:34:30  lr: 0.000060  loss: 0.0525 (0.0502)  time: 3.7737  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1010/3449]  eta: 2:33:53  lr: 0.000060  loss: 0.0426 (0.0501)  time: 3.7919  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1020/3449]  eta: 2:33:15  lr: 0.000060  loss: 0.0429 (0.0501)  time: 3.7930  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1030/3449]  eta: 2:32:38  lr: 0.000060  loss: 0.0512 (0.0501)  time: 3.8024  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1040/3449]  eta: 2:32:00  lr: 0.000060  loss: 0.0514 (0.0501)  time: 3.8089  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1050/3449]  eta: 2:31:23  lr: 0.000060  loss: 0.0546 (0.0502)  time: 3.8023  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1060/3449]  eta: 2:30:44  lr: 0.000060  loss: 0.0514 (0.0502)  time: 3.7857  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1070/3449]  eta: 2:30:05  lr: 0.000060  loss: 0.0536 (0.0502)  time: 3.7324  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1080/3449]  eta: 2:29:24  lr: 0.000060  loss: 0.0510 (0.0502)  time: 3.6850  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1090/3449]  eta: 2:28:44  lr: 0.000060  loss: 0.0510 (0.0502)  time: 3.6679  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1100/3449]  eta: 2:28:07  lr: 0.000060  loss: 0.0567 (0.0502)  time: 3.7477  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1110/3449]  eta: 2:27:29  lr: 0.000060  loss: 0.0549 (0.0502)  time: 3.7917  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1120/3449]  eta: 2:26:52  lr: 0.000060  loss: 0.0493 (0.0502)  time: 3.8090  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1130/3449]  eta: 2:26:15  lr: 0.000060  loss: 0.0590 (0.0503)  time: 3.8244  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1140/3449]  eta: 2:25:38  lr: 0.000060  loss: 0.0596 (0.0504)  time: 3.8207  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1150/3449]  eta: 2:25:01  lr: 0.000060  loss: 0.0546 (0.0504)  time: 3.8388  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1160/3449]  eta: 2:24:23  lr: 0.000060  loss: 0.0511 (0.0504)  time: 3.8095  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1170/3449]  eta: 2:23:45  lr: 0.000060  loss: 0.0495 (0.0504)  time: 3.7683  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1180/3449]  eta: 2:23:08  lr: 0.000060  loss: 0.0520 (0.0504)  time: 3.7981  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1190/3449]  eta: 2:22:30  lr: 0.000060  loss: 0.0546 (0.0504)  time: 3.8076  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1200/3449]  eta: 2:21:52  lr: 0.000060  loss: 0.0506 (0.0504)  time: 3.7609  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1210/3449]  eta: 2:21:15  lr: 0.000060  loss: 0.0506 (0.0504)  time: 3.8034  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1220/3449]  eta: 2:20:40  lr: 0.000060  loss: 0.0495 (0.0504)  time: 3.8940  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1230/3449]  eta: 2:20:01  lr: 0.000060  loss: 0.0495 (0.0504)  time: 3.8259  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1240/3449]  eta: 2:19:22  lr: 0.000060  loss: 0.0564 (0.0504)  time: 3.7211  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1250/3449]  eta: 2:18:43  lr: 0.000060  loss: 0.0515 (0.0504)  time: 3.7240  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1260/3449]  eta: 2:18:06  lr: 0.000060  loss: 0.0510 (0.0504)  time: 3.7709  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1270/3449]  eta: 2:17:28  lr: 0.000060  loss: 0.0521 (0.0504)  time: 3.7955  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1280/3449]  eta: 2:16:50  lr: 0.000060  loss: 0.0505 (0.0504)  time: 3.8030  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1290/3449]  eta: 2:16:12  lr: 0.000060  loss: 0.0501 (0.0504)  time: 3.7878  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1300/3449]  eta: 2:15:35  lr: 0.000060  loss: 0.0558 (0.0505)  time: 3.8002  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1310/3449]  eta: 2:14:58  lr: 0.000060  loss: 0.0531 (0.0504)  time: 3.8498  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1320/3449]  eta: 2:14:19  lr: 0.000060  loss: 0.0504 (0.0504)  time: 3.7754  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1330/3449]  eta: 2:13:41  lr: 0.000060  loss: 0.0557 (0.0504)  time: 3.7362  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1340/3449]  eta: 2:13:05  lr: 0.000060  loss: 0.0489 (0.0504)  time: 3.8493  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1350/3449]  eta: 2:12:29  lr: 0.000060  loss: 0.0470 (0.0504)  time: 3.8993  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1360/3449]  eta: 2:11:52  lr: 0.000060  loss: 0.0611 (0.0505)  time: 3.8671  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1370/3449]  eta: 2:11:17  lr: 0.000060  loss: 0.0637 (0.0506)  time: 3.9127  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1380/3449]  eta: 2:10:38  lr: 0.000060  loss: 0.0552 (0.0506)  time: 3.8558  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1390/3449]  eta: 2:09:59  lr: 0.000060  loss: 0.0579 (0.0507)  time: 3.7270  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1400/3449]  eta: 2:09:22  lr: 0.000060  loss: 0.0610 (0.0507)  time: 3.7765  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1410/3449]  eta: 2:08:45  lr: 0.000060  loss: 0.0590 (0.0508)  time: 3.8245  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1420/3449]  eta: 2:08:07  lr: 0.000060  loss: 0.0552 (0.0508)  time: 3.7962  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1430/3449]  eta: 2:07:28  lr: 0.000060  loss: 0.0471 (0.0507)  time: 3.7724  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1440/3449]  eta: 2:06:51  lr: 0.000060  loss: 0.0373 (0.0507)  time: 3.7766  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1450/3449]  eta: 2:06:13  lr: 0.000060  loss: 0.0500 (0.0507)  time: 3.8155  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1460/3449]  eta: 2:05:37  lr: 0.000060  loss: 0.0500 (0.0507)  time: 3.8495  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1470/3449]  eta: 2:04:58  lr: 0.000060  loss: 0.0464 (0.0507)  time: 3.7918  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1480/3449]  eta: 2:04:18  lr: 0.000060  loss: 0.0464 (0.0506)  time: 3.7006  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1490/3449]  eta: 2:03:42  lr: 0.000060  loss: 0.0390 (0.0505)  time: 3.7788  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1500/3449]  eta: 2:03:04  lr: 0.000060  loss: 0.0531 (0.0506)  time: 3.8491  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1510/3449]  eta: 2:02:28  lr: 0.000060  loss: 0.0608 (0.0507)  time: 3.8833  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1520/3449]  eta: 2:01:49  lr: 0.000060  loss: 0.0611 (0.0507)  time: 3.8038  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1530/3449]  eta: 2:01:11  lr: 0.000060  loss: 0.0581 (0.0507)  time: 3.7260  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1540/3449]  eta: 2:00:33  lr: 0.000060  loss: 0.0583 (0.0507)  time: 3.8053  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1550/3449]  eta: 1:59:57  lr: 0.000060  loss: 0.0547 (0.0507)  time: 3.8545  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1560/3449]  eta: 1:59:18  lr: 0.000060  loss: 0.0547 (0.0507)  time: 3.8106  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1570/3449]  eta: 1:58:39  lr: 0.000060  loss: 0.0522 (0.0507)  time: 3.7272  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1580/3449]  eta: 1:58:00  lr: 0.000060  loss: 0.0355 (0.0506)  time: 3.6850  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1590/3449]  eta: 1:57:22  lr: 0.000060  loss: 0.0415 (0.0506)  time: 3.7240  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1600/3449]  eta: 1:56:44  lr: 0.000060  loss: 0.0510 (0.0506)  time: 3.8002  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1610/3449]  eta: 1:56:07  lr: 0.000060  loss: 0.0492 (0.0506)  time: 3.8173  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1620/3449]  eta: 1:55:30  lr: 0.000060  loss: 0.0486 (0.0506)  time: 3.8279  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1630/3449]  eta: 1:54:52  lr: 0.000060  loss: 0.0474 (0.0506)  time: 3.8109  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1640/3449]  eta: 1:54:13  lr: 0.000060  loss: 0.0557 (0.0507)  time: 3.7378  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1650/3449]  eta: 1:53:35  lr: 0.000060  loss: 0.0636 (0.0507)  time: 3.7637  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1660/3449]  eta: 1:52:58  lr: 0.000060  loss: 0.0570 (0.0507)  time: 3.8245  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1670/3449]  eta: 1:52:19  lr: 0.000060  loss: 0.0487 (0.0507)  time: 3.7764  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1680/3449]  eta: 1:51:42  lr: 0.000060  loss: 0.0489 (0.0506)  time: 3.7801  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1690/3449]  eta: 1:51:05  lr: 0.000060  loss: 0.0461 (0.0506)  time: 3.8527  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1700/3449]  eta: 1:50:27  lr: 0.000060  loss: 0.0482 (0.0506)  time: 3.8622  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1710/3449]  eta: 1:49:50  lr: 0.000060  loss: 0.0470 (0.0506)  time: 3.8238  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1720/3449]  eta: 1:49:10  lr: 0.000060  loss: 0.0405 (0.0506)  time: 3.7133  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1730/3449]  eta: 1:48:32  lr: 0.000060  loss: 0.0554 (0.0506)  time: 3.7082  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1740/3449]  eta: 1:47:54  lr: 0.000060  loss: 0.0595 (0.0506)  time: 3.7762  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1750/3449]  eta: 1:47:16  lr: 0.000060  loss: 0.0573 (0.0506)  time: 3.7310  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1760/3449]  eta: 1:46:37  lr: 0.000060  loss: 0.0463 (0.0506)  time: 3.7200  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1770/3449]  eta: 1:45:59  lr: 0.000060  loss: 0.0452 (0.0505)  time: 3.7705  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1780/3449]  eta: 1:45:21  lr: 0.000060  loss: 0.0486 (0.0505)  time: 3.7904  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1790/3449]  eta: 1:44:44  lr: 0.000060  loss: 0.0533 (0.0505)  time: 3.7796  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1800/3449]  eta: 1:44:06  lr: 0.000060  loss: 0.0494 (0.0505)  time: 3.8124  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1810/3449]  eta: 1:43:29  lr: 0.000060  loss: 0.0495 (0.0505)  time: 3.8354  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1820/3449]  eta: 1:42:51  lr: 0.000060  loss: 0.0508 (0.0505)  time: 3.8327  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1830/3449]  eta: 1:42:13  lr: 0.000060  loss: 0.0527 (0.0505)  time: 3.7795  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1840/3449]  eta: 1:41:34  lr: 0.000060  loss: 0.0540 (0.0505)  time: 3.7228  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1850/3449]  eta: 1:40:55  lr: 0.000060  loss: 0.0540 (0.0505)  time: 3.6774  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1860/3449]  eta: 1:40:17  lr: 0.000060  loss: 0.0517 (0.0505)  time: 3.7226  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1870/3449]  eta: 1:39:40  lr: 0.000060  loss: 0.0432 (0.0505)  time: 3.8421  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1880/3449]  eta: 1:39:02  lr: 0.000060  loss: 0.0386 (0.0504)  time: 3.8054  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1890/3449]  eta: 1:38:24  lr: 0.000060  loss: 0.0461 (0.0503)  time: 3.7474  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1900/3449]  eta: 1:37:45  lr: 0.000060  loss: 0.0498 (0.0504)  time: 3.7144  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1910/3449]  eta: 1:37:08  lr: 0.000060  loss: 0.0582 (0.0504)  time: 3.7641  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1920/3449]  eta: 1:36:30  lr: 0.000060  loss: 0.0553 (0.0504)  time: 3.8687  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1930/3449]  eta: 1:35:52  lr: 0.000060  loss: 0.0481 (0.0504)  time: 3.8104  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1940/3449]  eta: 1:35:13  lr: 0.000060  loss: 0.0481 (0.0503)  time: 3.7058  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1950/3449]  eta: 1:34:35  lr: 0.000060  loss: 0.0470 (0.0503)  time: 3.6747  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1960/3449]  eta: 1:33:57  lr: 0.000060  loss: 0.0534 (0.0504)  time: 3.7565  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1970/3449]  eta: 1:33:18  lr: 0.000060  loss: 0.0571 (0.0504)  time: 3.7322  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1980/3449]  eta: 1:32:40  lr: 0.000060  loss: 0.0579 (0.0504)  time: 3.6844  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [1990/3449]  eta: 1:32:03  lr: 0.000060  loss: 0.0552 (0.0504)  time: 3.7981  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2000/3449]  eta: 1:31:25  lr: 0.000060  loss: 0.0520 (0.0504)  time: 3.8448  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2010/3449]  eta: 1:30:47  lr: 0.000060  loss: 0.0497 (0.0504)  time: 3.7853  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2020/3449]  eta: 1:30:09  lr: 0.000060  loss: 0.0525 (0.0504)  time: 3.7734  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2030/3449]  eta: 1:29:31  lr: 0.000060  loss: 0.0547 (0.0504)  time: 3.7976  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2040/3449]  eta: 1:28:54  lr: 0.000060  loss: 0.0580 (0.0505)  time: 3.8439  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2050/3449]  eta: 1:28:17  lr: 0.000060  loss: 0.0557 (0.0504)  time: 3.8702  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2060/3449]  eta: 1:27:39  lr: 0.000060  loss: 0.0478 (0.0504)  time: 3.7961  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2070/3449]  eta: 1:27:01  lr: 0.000060  loss: 0.0528 (0.0505)  time: 3.7395  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2080/3449]  eta: 1:26:22  lr: 0.000060  loss: 0.0592 (0.0505)  time: 3.7467  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2090/3449]  eta: 1:25:45  lr: 0.000060  loss: 0.0448 (0.0504)  time: 3.8064  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2100/3449]  eta: 1:25:07  lr: 0.000060  loss: 0.0446 (0.0504)  time: 3.8094  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2110/3449]  eta: 1:24:29  lr: 0.000060  loss: 0.0488 (0.0504)  time: 3.7791  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2120/3449]  eta: 1:23:51  lr: 0.000060  loss: 0.0422 (0.0504)  time: 3.7905  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2130/3449]  eta: 1:23:13  lr: 0.000060  loss: 0.0535 (0.0504)  time: 3.7454  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2140/3449]  eta: 1:22:35  lr: 0.000060  loss: 0.0547 (0.0504)  time: 3.7065  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2150/3449]  eta: 1:21:57  lr: 0.000060  loss: 0.0516 (0.0504)  time: 3.7606  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2160/3449]  eta: 1:21:19  lr: 0.000060  loss: 0.0476 (0.0504)  time: 3.8242  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2170/3449]  eta: 1:20:42  lr: 0.000060  loss: 0.0478 (0.0504)  time: 3.8096  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2180/3449]  eta: 1:20:04  lr: 0.000060  loss: 0.0494 (0.0504)  time: 3.7786  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2190/3449]  eta: 1:19:25  lr: 0.000060  loss: 0.0558 (0.0504)  time: 3.7284  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2200/3449]  eta: 1:18:48  lr: 0.000060  loss: 0.0558 (0.0504)  time: 3.7552  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2210/3449]  eta: 1:18:10  lr: 0.000060  loss: 0.0419 (0.0504)  time: 3.8202  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2220/3449]  eta: 1:17:32  lr: 0.000060  loss: 0.0438 (0.0504)  time: 3.7831  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2230/3449]  eta: 1:16:53  lr: 0.000060  loss: 0.0542 (0.0504)  time: 3.7124  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2240/3449]  eta: 1:16:15  lr: 0.000060  loss: 0.0509 (0.0504)  time: 3.7156  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2250/3449]  eta: 1:15:37  lr: 0.000060  loss: 0.0513 (0.0504)  time: 3.7574  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2260/3449]  eta: 1:15:00  lr: 0.000060  loss: 0.0527 (0.0504)  time: 3.7778  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2270/3449]  eta: 1:14:23  lr: 0.000060  loss: 0.0413 (0.0503)  time: 3.9148  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2280/3449]  eta: 1:13:45  lr: 0.000060  loss: 0.0498 (0.0503)  time: 3.9043  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2290/3449]  eta: 1:13:08  lr: 0.000060  loss: 0.0499 (0.0503)  time: 3.8031  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2300/3449]  eta: 1:12:30  lr: 0.000060  loss: 0.0504 (0.0504)  time: 3.8111  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2310/3449]  eta: 1:11:51  lr: 0.000060  loss: 0.0509 (0.0503)  time: 3.7540  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2320/3449]  eta: 1:11:13  lr: 0.000060  loss: 0.0566 (0.0504)  time: 3.7302  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2330/3449]  eta: 1:10:35  lr: 0.000060  loss: 0.0554 (0.0503)  time: 3.7319  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2340/3449]  eta: 1:09:57  lr: 0.000060  loss: 0.0496 (0.0503)  time: 3.7180  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2350/3449]  eta: 1:09:19  lr: 0.000060  loss: 0.0536 (0.0504)  time: 3.7228  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2360/3449]  eta: 1:08:41  lr: 0.000060  loss: 0.0518 (0.0504)  time: 3.7751  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2370/3449]  eta: 1:08:03  lr: 0.000060  loss: 0.0464 (0.0504)  time: 3.7534  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2380/3449]  eta: 1:07:25  lr: 0.000060  loss: 0.0554 (0.0504)  time: 3.7255  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2390/3449]  eta: 1:06:47  lr: 0.000060  loss: 0.0481 (0.0504)  time: 3.8025  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2400/3449]  eta: 1:06:09  lr: 0.000060  loss: 0.0354 (0.0503)  time: 3.7803  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2410/3449]  eta: 1:05:31  lr: 0.000060  loss: 0.0500 (0.0503)  time: 3.7463  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2420/3449]  eta: 1:04:54  lr: 0.000060  loss: 0.0593 (0.0504)  time: 3.8044  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2430/3449]  eta: 1:04:16  lr: 0.000060  loss: 0.0593 (0.0504)  time: 3.8337  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2440/3449]  eta: 1:03:38  lr: 0.000060  loss: 0.0572 (0.0504)  time: 3.7733  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2450/3449]  eta: 1:03:00  lr: 0.000060  loss: 0.0546 (0.0504)  time: 3.7801  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2460/3449]  eta: 1:02:23  lr: 0.000060  loss: 0.0537 (0.0504)  time: 3.8059  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2470/3449]  eta: 1:01:45  lr: 0.000060  loss: 0.0540 (0.0505)  time: 3.8172  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2480/3449]  eta: 1:01:07  lr: 0.000060  loss: 0.0610 (0.0505)  time: 3.8229  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2490/3449]  eta: 1:00:29  lr: 0.000060  loss: 0.0622 (0.0505)  time: 3.7912  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2500/3449]  eta: 0:59:51  lr: 0.000060  loss: 0.0534 (0.0505)  time: 3.7641  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2510/3449]  eta: 0:59:14  lr: 0.000060  loss: 0.0460 (0.0505)  time: 3.7885  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2520/3449]  eta: 0:58:36  lr: 0.000060  loss: 0.0460 (0.0505)  time: 3.8520  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2530/3449]  eta: 0:57:58  lr: 0.000060  loss: 0.0494 (0.0505)  time: 3.7728  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2540/3449]  eta: 0:57:20  lr: 0.000060  loss: 0.0510 (0.0505)  time: 3.7644  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2550/3449]  eta: 0:56:43  lr: 0.000060  loss: 0.0531 (0.0505)  time: 3.8365  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2560/3449]  eta: 0:56:05  lr: 0.000060  loss: 0.0506 (0.0505)  time: 3.8382  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2570/3449]  eta: 0:55:27  lr: 0.000060  loss: 0.0518 (0.0505)  time: 3.8227  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2580/3449]  eta: 0:54:49  lr: 0.000060  loss: 0.0510 (0.0505)  time: 3.7414  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2590/3449]  eta: 0:54:11  lr: 0.000060  loss: 0.0550 (0.0505)  time: 3.7341  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2600/3449]  eta: 0:53:33  lr: 0.000060  loss: 0.0532 (0.0505)  time: 3.8077  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2610/3449]  eta: 0:52:55  lr: 0.000060  loss: 0.0476 (0.0506)  time: 3.7896  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2620/3449]  eta: 0:52:18  lr: 0.000060  loss: 0.0578 (0.0506)  time: 3.7789  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2630/3449]  eta: 0:51:40  lr: 0.000060  loss: 0.0545 (0.0506)  time: 3.8386  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2640/3449]  eta: 0:51:02  lr: 0.000060  loss: 0.0454 (0.0505)  time: 3.7894  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2650/3449]  eta: 0:50:24  lr: 0.000060  loss: 0.0530 (0.0505)  time: 3.7711  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2660/3449]  eta: 0:49:46  lr: 0.000060  loss: 0.0526 (0.0505)  time: 3.8234  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2670/3449]  eta: 0:49:09  lr: 0.000060  loss: 0.0413 (0.0505)  time: 3.8194  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2680/3449]  eta: 0:48:31  lr: 0.000060  loss: 0.0551 (0.0505)  time: 3.7920  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2690/3449]  eta: 0:47:53  lr: 0.000060  loss: 0.0523 (0.0505)  time: 3.7478  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2700/3449]  eta: 0:47:15  lr: 0.000060  loss: 0.0500 (0.0505)  time: 3.8139  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2710/3449]  eta: 0:46:38  lr: 0.000060  loss: 0.0500 (0.0505)  time: 3.8802  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2720/3449]  eta: 0:46:00  lr: 0.000060  loss: 0.0512 (0.0505)  time: 3.8542  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2730/3449]  eta: 0:45:22  lr: 0.000060  loss: 0.0488 (0.0505)  time: 3.8249  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2740/3449]  eta: 0:44:44  lr: 0.000060  loss: 0.0458 (0.0505)  time: 3.8263  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2750/3449]  eta: 0:44:06  lr: 0.000060  loss: 0.0477 (0.0505)  time: 3.8215  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2760/3449]  eta: 0:43:29  lr: 0.000060  loss: 0.0482 (0.0505)  time: 3.8595  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2770/3449]  eta: 0:42:51  lr: 0.000060  loss: 0.0501 (0.0505)  time: 3.8357  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2780/3449]  eta: 0:42:13  lr: 0.000060  loss: 0.0522 (0.0505)  time: 3.8157  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2790/3449]  eta: 0:41:36  lr: 0.000060  loss: 0.0372 (0.0505)  time: 3.8486  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2800/3449]  eta: 0:40:58  lr: 0.000060  loss: 0.0376 (0.0504)  time: 3.8344  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2810/3449]  eta: 0:40:20  lr: 0.000060  loss: 0.0442 (0.0504)  time: 3.7611  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2820/3449]  eta: 0:39:42  lr: 0.000060  loss: 0.0538 (0.0504)  time: 3.7327  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2830/3449]  eta: 0:39:04  lr: 0.000060  loss: 0.0582 (0.0505)  time: 3.7774  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2840/3449]  eta: 0:38:26  lr: 0.000060  loss: 0.0489 (0.0504)  time: 3.7764  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2850/3449]  eta: 0:37:48  lr: 0.000060  loss: 0.0432 (0.0505)  time: 3.8007  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2860/3449]  eta: 0:37:10  lr: 0.000060  loss: 0.0528 (0.0505)  time: 3.7442  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2870/3449]  eta: 0:36:32  lr: 0.000060  loss: 0.0404 (0.0504)  time: 3.6926  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2880/3449]  eta: 0:35:54  lr: 0.000060  loss: 0.0359 (0.0504)  time: 3.7444  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2890/3449]  eta: 0:35:16  lr: 0.000060  loss: 0.0486 (0.0504)  time: 3.7628  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2900/3449]  eta: 0:34:38  lr: 0.000060  loss: 0.0499 (0.0504)  time: 3.7316  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2910/3449]  eta: 0:34:00  lr: 0.000060  loss: 0.0530 (0.0504)  time: 3.7541  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2920/3449]  eta: 0:33:23  lr: 0.000060  loss: 0.0533 (0.0504)  time: 3.8330  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2930/3449]  eta: 0:32:45  lr: 0.000060  loss: 0.0509 (0.0504)  time: 3.7859  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2940/3449]  eta: 0:32:07  lr: 0.000060  loss: 0.0509 (0.0504)  time: 3.7571  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2950/3449]  eta: 0:31:29  lr: 0.000060  loss: 0.0470 (0.0504)  time: 3.7703  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2960/3449]  eta: 0:30:51  lr: 0.000060  loss: 0.0492 (0.0504)  time: 3.7053  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2970/3449]  eta: 0:30:13  lr: 0.000060  loss: 0.0540 (0.0504)  time: 3.7147  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2980/3449]  eta: 0:29:35  lr: 0.000060  loss: 0.0478 (0.0504)  time: 3.7555  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [2990/3449]  eta: 0:28:57  lr: 0.000060  loss: 0.0478 (0.0504)  time: 3.7809  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3000/3449]  eta: 0:28:19  lr: 0.000060  loss: 0.0466 (0.0504)  time: 3.8200  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3010/3449]  eta: 0:27:42  lr: 0.000060  loss: 0.0435 (0.0504)  time: 3.8787  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3020/3449]  eta: 0:27:04  lr: 0.000060  loss: 0.0437 (0.0504)  time: 3.8191  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3030/3449]  eta: 0:26:26  lr: 0.000060  loss: 0.0459 (0.0504)  time: 3.7823  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3040/3449]  eta: 0:25:48  lr: 0.000060  loss: 0.0463 (0.0504)  time: 3.8149  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3050/3449]  eta: 0:25:10  lr: 0.000060  loss: 0.0470 (0.0504)  time: 3.7574  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3060/3449]  eta: 0:24:32  lr: 0.000060  loss: 0.0550 (0.0504)  time: 3.7780  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3070/3449]  eta: 0:23:55  lr: 0.000060  loss: 0.0511 (0.0504)  time: 3.8248  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3080/3449]  eta: 0:23:17  lr: 0.000060  loss: 0.0474 (0.0504)  time: 3.7796  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3090/3449]  eta: 0:22:39  lr: 0.000060  loss: 0.0505 (0.0504)  time: 3.8355  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3100/3449]  eta: 0:22:01  lr: 0.000060  loss: 0.0466 (0.0504)  time: 3.8591  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3110/3449]  eta: 0:21:23  lr: 0.000060  loss: 0.0578 (0.0504)  time: 3.7595  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3120/3449]  eta: 0:20:45  lr: 0.000060  loss: 0.0531 (0.0504)  time: 3.7751  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3130/3449]  eta: 0:20:07  lr: 0.000060  loss: 0.0454 (0.0504)  time: 3.7608  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3140/3449]  eta: 0:19:29  lr: 0.000060  loss: 0.0512 (0.0504)  time: 3.7235  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3150/3449]  eta: 0:18:52  lr: 0.000060  loss: 0.0495 (0.0503)  time: 3.7452  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3160/3449]  eta: 0:18:14  lr: 0.000060  loss: 0.0495 (0.0504)  time: 3.8016  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3170/3449]  eta: 0:17:36  lr: 0.000060  loss: 0.0588 (0.0504)  time: 3.8609  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3180/3449]  eta: 0:16:58  lr: 0.000060  loss: 0.0566 (0.0504)  time: 3.9008  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3190/3449]  eta: 0:16:20  lr: 0.000060  loss: 0.0467 (0.0504)  time: 3.8559  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3200/3449]  eta: 0:15:42  lr: 0.000060  loss: 0.0432 (0.0504)  time: 3.7603  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3210/3449]  eta: 0:15:05  lr: 0.000060  loss: 0.0424 (0.0503)  time: 3.7532  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3220/3449]  eta: 0:14:27  lr: 0.000060  loss: 0.0497 (0.0503)  time: 3.7341  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3230/3449]  eta: 0:13:49  lr: 0.000060  loss: 0.0564 (0.0504)  time: 3.7286  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3240/3449]  eta: 0:13:11  lr: 0.000060  loss: 0.0564 (0.0504)  time: 3.7502  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3250/3449]  eta: 0:12:33  lr: 0.000060  loss: 0.0529 (0.0504)  time: 3.8060  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3260/3449]  eta: 0:11:55  lr: 0.000060  loss: 0.0529 (0.0504)  time: 3.8025  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3270/3449]  eta: 0:11:17  lr: 0.000060  loss: 0.0507 (0.0504)  time: 3.7837  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3280/3449]  eta: 0:10:39  lr: 0.000060  loss: 0.0505 (0.0504)  time: 3.7996  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3290/3449]  eta: 0:10:02  lr: 0.000060  loss: 0.0466 (0.0503)  time: 3.8176  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3300/3449]  eta: 0:09:24  lr: 0.000060  loss: 0.0513 (0.0504)  time: 3.8563  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3310/3449]  eta: 0:08:46  lr: 0.000060  loss: 0.0569 (0.0504)  time: 3.8087  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3320/3449]  eta: 0:08:08  lr: 0.000060  loss: 0.0581 (0.0504)  time: 3.7573  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3330/3449]  eta: 0:07:30  lr: 0.000060  loss: 0.0583 (0.0504)  time: 3.7583  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3340/3449]  eta: 0:06:52  lr: 0.000060  loss: 0.0589 (0.0504)  time: 3.7830  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3350/3449]  eta: 0:06:14  lr: 0.000060  loss: 0.0589 (0.0504)  time: 3.8147  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3360/3449]  eta: 0:05:37  lr: 0.000060  loss: 0.0447 (0.0504)  time: 3.8153  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3370/3449]  eta: 0:04:59  lr: 0.000060  loss: 0.0490 (0.0504)  time: 3.8227  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3380/3449]  eta: 0:04:21  lr: 0.000060  loss: 0.0549 (0.0504)  time: 3.8189  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3390/3449]  eta: 0:03:43  lr: 0.000060  loss: 0.0553 (0.0504)  time: 3.7911  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3400/3449]  eta: 0:03:05  lr: 0.000060  loss: 0.0588 (0.0505)  time: 3.8179  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3410/3449]  eta: 0:02:27  lr: 0.000060  loss: 0.0541 (0.0504)  time: 3.8604  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3420/3449]  eta: 0:01:49  lr: 0.000060  loss: 0.0501 (0.0504)  time: 3.8421  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3430/3449]  eta: 0:01:11  lr: 0.000060  loss: 0.0521 (0.0505)  time: 3.7549  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3440/3449]  eta: 0:00:34  lr: 0.000060  loss: 0.0610 (0.0505)  time: 3.6848  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [3448/3449]  eta: 0:00:03  lr: 0.000060  loss: 0.0539 (0.0505)  time: 3.7141  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:7] Total time: 3:37:41 (3.7871 s / it)\n",
      "Averaged stats: lr: 0.000060  loss: 0.0539 (0.0505)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:7]  [ 0/14]  eta: 0:04:21  loss: 0.0339 (0.0339)  time: 18.6720  data: 0.4480  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:7]  [13/14]  eta: 0:00:18  loss: 0.0392 (0.0407)  time: 18.2707  data: 0.0322  max mem: 34968\n",
      "Valid: [epoch:7] Total time: 0:04:15 (18.2847 s / it)\n",
      "Averaged stats: loss: 0.0392 (0.0407)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_7_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.041%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [   0/3449]  eta: 4:54:32  lr: 0.000070  loss: 0.0559 (0.0559)  time: 5.1240  data: 1.2398  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [  10/3449]  eta: 3:41:12  lr: 0.000070  loss: 0.0559 (0.0577)  time: 3.8594  data: 0.1128  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [  20/3449]  eta: 3:37:11  lr: 0.000070  loss: 0.0555 (0.0558)  time: 3.7343  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [  30/3449]  eta: 3:36:46  lr: 0.000070  loss: 0.0503 (0.0504)  time: 3.7739  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [  40/3449]  eta: 3:34:36  lr: 0.000070  loss: 0.0431 (0.0504)  time: 3.7529  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [  50/3449]  eta: 3:33:10  lr: 0.000070  loss: 0.0481 (0.0487)  time: 3.6988  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [  60/3449]  eta: 3:32:13  lr: 0.000070  loss: 0.0454 (0.0491)  time: 3.7162  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [  70/3449]  eta: 3:31:19  lr: 0.000070  loss: 0.0460 (0.0480)  time: 3.7258  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [  80/3449]  eta: 3:30:46  lr: 0.000070  loss: 0.0460 (0.0478)  time: 3.7428  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [  90/3449]  eta: 3:30:32  lr: 0.000070  loss: 0.0565 (0.0488)  time: 3.7904  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 100/3449]  eta: 3:30:07  lr: 0.000070  loss: 0.0550 (0.0484)  time: 3.8080  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 110/3449]  eta: 3:29:47  lr: 0.000070  loss: 0.0408 (0.0476)  time: 3.8110  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 120/3449]  eta: 3:29:01  lr: 0.000070  loss: 0.0408 (0.0475)  time: 3.7823  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 130/3449]  eta: 3:28:41  lr: 0.000070  loss: 0.0415 (0.0471)  time: 3.7875  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 140/3449]  eta: 3:28:49  lr: 0.000070  loss: 0.0488 (0.0476)  time: 3.9017  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 150/3449]  eta: 3:28:33  lr: 0.000070  loss: 0.0582 (0.0481)  time: 3.9280  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 160/3449]  eta: 3:27:52  lr: 0.000070  loss: 0.0552 (0.0483)  time: 3.8328  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 170/3449]  eta: 3:27:08  lr: 0.000070  loss: 0.0489 (0.0481)  time: 3.7693  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 180/3449]  eta: 3:26:10  lr: 0.000070  loss: 0.0381 (0.0473)  time: 3.7210  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 190/3449]  eta: 3:25:44  lr: 0.000070  loss: 0.0308 (0.0471)  time: 3.7678  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 200/3449]  eta: 3:25:09  lr: 0.000070  loss: 0.0552 (0.0478)  time: 3.8274  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 210/3449]  eta: 3:24:18  lr: 0.000070  loss: 0.0557 (0.0476)  time: 3.7517  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 220/3449]  eta: 3:23:35  lr: 0.000070  loss: 0.0482 (0.0478)  time: 3.7280  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 230/3449]  eta: 3:23:20  lr: 0.000070  loss: 0.0476 (0.0479)  time: 3.8497  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 240/3449]  eta: 3:22:32  lr: 0.000070  loss: 0.0461 (0.0478)  time: 3.8295  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 250/3449]  eta: 3:21:44  lr: 0.000070  loss: 0.0506 (0.0482)  time: 3.7102  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 260/3449]  eta: 3:21:04  lr: 0.000070  loss: 0.0581 (0.0486)  time: 3.7369  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 270/3449]  eta: 3:20:15  lr: 0.000070  loss: 0.0564 (0.0487)  time: 3.7263  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 280/3449]  eta: 3:19:36  lr: 0.000070  loss: 0.0533 (0.0487)  time: 3.7267  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 290/3449]  eta: 3:18:48  lr: 0.000070  loss: 0.0524 (0.0486)  time: 3.7278  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 300/3449]  eta: 3:18:12  lr: 0.000070  loss: 0.0532 (0.0487)  time: 3.7431  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 310/3449]  eta: 3:17:31  lr: 0.000070  loss: 0.0528 (0.0488)  time: 3.7699  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 320/3449]  eta: 3:17:05  lr: 0.000070  loss: 0.0547 (0.0493)  time: 3.8203  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 330/3449]  eta: 3:16:37  lr: 0.000070  loss: 0.0556 (0.0495)  time: 3.8915  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 340/3449]  eta: 3:16:05  lr: 0.000070  loss: 0.0493 (0.0494)  time: 3.8610  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 350/3449]  eta: 3:15:15  lr: 0.000070  loss: 0.0477 (0.0493)  time: 3.7448  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 360/3449]  eta: 3:14:39  lr: 0.000070  loss: 0.0458 (0.0493)  time: 3.7275  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 370/3449]  eta: 3:14:10  lr: 0.000070  loss: 0.0496 (0.0492)  time: 3.8436  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 380/3449]  eta: 3:13:26  lr: 0.000070  loss: 0.0496 (0.0491)  time: 3.7971  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 390/3449]  eta: 3:12:59  lr: 0.000070  loss: 0.0460 (0.0491)  time: 3.8124  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 400/3449]  eta: 3:12:22  lr: 0.000070  loss: 0.0424 (0.0488)  time: 3.8538  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 410/3449]  eta: 3:11:49  lr: 0.000070  loss: 0.0411 (0.0487)  time: 3.8244  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 420/3449]  eta: 3:11:03  lr: 0.000070  loss: 0.0478 (0.0488)  time: 3.7690  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 430/3449]  eta: 3:10:25  lr: 0.000070  loss: 0.0493 (0.0486)  time: 3.7294  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 440/3449]  eta: 3:09:49  lr: 0.000070  loss: 0.0530 (0.0489)  time: 3.7968  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 450/3449]  eta: 3:09:10  lr: 0.000070  loss: 0.0635 (0.0493)  time: 3.7901  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 460/3449]  eta: 3:08:31  lr: 0.000070  loss: 0.0559 (0.0493)  time: 3.7610  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 470/3449]  eta: 3:07:54  lr: 0.000070  loss: 0.0546 (0.0495)  time: 3.7811  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 480/3449]  eta: 3:07:26  lr: 0.000070  loss: 0.0548 (0.0495)  time: 3.8724  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 490/3449]  eta: 3:06:45  lr: 0.000070  loss: 0.0475 (0.0494)  time: 3.8381  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 500/3449]  eta: 3:06:03  lr: 0.000070  loss: 0.0465 (0.0492)  time: 3.7285  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 510/3449]  eta: 3:05:32  lr: 0.000070  loss: 0.0465 (0.0493)  time: 3.8173  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 520/3449]  eta: 3:04:58  lr: 0.000070  loss: 0.0567 (0.0494)  time: 3.8833  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 530/3449]  eta: 3:04:23  lr: 0.000070  loss: 0.0506 (0.0493)  time: 3.8465  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 540/3449]  eta: 3:03:49  lr: 0.000070  loss: 0.0472 (0.0494)  time: 3.8510  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 550/3449]  eta: 3:03:10  lr: 0.000070  loss: 0.0451 (0.0494)  time: 3.8136  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 560/3449]  eta: 3:02:31  lr: 0.000070  loss: 0.0503 (0.0494)  time: 3.7713  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 570/3449]  eta: 3:01:55  lr: 0.000070  loss: 0.0503 (0.0493)  time: 3.8049  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 580/3449]  eta: 3:01:15  lr: 0.000070  loss: 0.0514 (0.0494)  time: 3.7880  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 590/3449]  eta: 3:00:40  lr: 0.000070  loss: 0.0573 (0.0496)  time: 3.7989  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 600/3449]  eta: 2:59:59  lr: 0.000070  loss: 0.0560 (0.0495)  time: 3.7835  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 610/3449]  eta: 2:59:17  lr: 0.000070  loss: 0.0499 (0.0494)  time: 3.7145  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 620/3449]  eta: 2:58:42  lr: 0.000070  loss: 0.0506 (0.0494)  time: 3.7871  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 630/3449]  eta: 2:58:09  lr: 0.000070  loss: 0.0387 (0.0494)  time: 3.8751  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 640/3449]  eta: 2:57:31  lr: 0.000070  loss: 0.0538 (0.0494)  time: 3.8379  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 650/3449]  eta: 2:56:48  lr: 0.000070  loss: 0.0579 (0.0494)  time: 3.7347  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 660/3449]  eta: 2:56:13  lr: 0.000070  loss: 0.0507 (0.0495)  time: 3.7741  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 670/3449]  eta: 2:55:38  lr: 0.000070  loss: 0.0585 (0.0497)  time: 3.8547  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 680/3449]  eta: 2:55:00  lr: 0.000070  loss: 0.0532 (0.0496)  time: 3.8217  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 690/3449]  eta: 2:54:25  lr: 0.000070  loss: 0.0459 (0.0495)  time: 3.8282  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 700/3449]  eta: 2:53:43  lr: 0.000070  loss: 0.0459 (0.0495)  time: 3.7705  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 710/3449]  eta: 2:53:05  lr: 0.000070  loss: 0.0514 (0.0496)  time: 3.7439  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 720/3449]  eta: 2:52:24  lr: 0.000070  loss: 0.0547 (0.0496)  time: 3.7491  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 730/3449]  eta: 2:51:45  lr: 0.000070  loss: 0.0503 (0.0496)  time: 3.7378  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 740/3449]  eta: 2:51:06  lr: 0.000070  loss: 0.0467 (0.0496)  time: 3.7613  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 750/3449]  eta: 2:50:27  lr: 0.000070  loss: 0.0415 (0.0495)  time: 3.7574  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 760/3449]  eta: 2:49:53  lr: 0.000070  loss: 0.0581 (0.0496)  time: 3.8307  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 770/3449]  eta: 2:49:10  lr: 0.000070  loss: 0.0566 (0.0496)  time: 3.7731  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 780/3449]  eta: 2:48:34  lr: 0.000070  loss: 0.0523 (0.0495)  time: 3.7472  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 790/3449]  eta: 2:47:51  lr: 0.000070  loss: 0.0415 (0.0494)  time: 3.7427  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 800/3449]  eta: 2:47:12  lr: 0.000070  loss: 0.0380 (0.0493)  time: 3.6981  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 810/3449]  eta: 2:46:36  lr: 0.000070  loss: 0.0494 (0.0494)  time: 3.7886  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 820/3449]  eta: 2:45:59  lr: 0.000070  loss: 0.0519 (0.0494)  time: 3.8249  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 830/3449]  eta: 2:45:23  lr: 0.000070  loss: 0.0576 (0.0496)  time: 3.8301  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 840/3449]  eta: 2:44:46  lr: 0.000070  loss: 0.0538 (0.0496)  time: 3.8351  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 850/3449]  eta: 2:44:08  lr: 0.000070  loss: 0.0503 (0.0496)  time: 3.8134  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 860/3449]  eta: 2:43:33  lr: 0.000070  loss: 0.0545 (0.0496)  time: 3.8374  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 870/3449]  eta: 2:42:54  lr: 0.000070  loss: 0.0553 (0.0497)  time: 3.8173  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 880/3449]  eta: 2:42:19  lr: 0.000070  loss: 0.0524 (0.0497)  time: 3.8189  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 890/3449]  eta: 2:41:40  lr: 0.000070  loss: 0.0546 (0.0498)  time: 3.8145  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 900/3449]  eta: 2:41:03  lr: 0.000070  loss: 0.0571 (0.0498)  time: 3.7961  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 910/3449]  eta: 2:40:25  lr: 0.000070  loss: 0.0578 (0.0499)  time: 3.8008  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 920/3449]  eta: 2:39:44  lr: 0.000070  loss: 0.0570 (0.0500)  time: 3.7261  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 930/3449]  eta: 2:39:03  lr: 0.000070  loss: 0.0533 (0.0500)  time: 3.6838  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 940/3449]  eta: 2:38:22  lr: 0.000070  loss: 0.0495 (0.0500)  time: 3.6834  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 950/3449]  eta: 2:37:45  lr: 0.000070  loss: 0.0531 (0.0501)  time: 3.7428  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 960/3449]  eta: 2:37:09  lr: 0.000070  loss: 0.0627 (0.0502)  time: 3.8330  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 970/3449]  eta: 2:36:32  lr: 0.000070  loss: 0.0643 (0.0504)  time: 3.8472  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 980/3449]  eta: 2:35:57  lr: 0.000070  loss: 0.0592 (0.0504)  time: 3.8564  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 990/3449]  eta: 2:35:20  lr: 0.000070  loss: 0.0537 (0.0504)  time: 3.8580  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1000/3449]  eta: 2:34:41  lr: 0.000070  loss: 0.0530 (0.0504)  time: 3.7866  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1010/3449]  eta: 2:34:02  lr: 0.000070  loss: 0.0463 (0.0503)  time: 3.7563  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1020/3449]  eta: 2:33:25  lr: 0.000070  loss: 0.0480 (0.0504)  time: 3.7915  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1030/3449]  eta: 2:32:47  lr: 0.000070  loss: 0.0512 (0.0503)  time: 3.7950  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1040/3449]  eta: 2:32:07  lr: 0.000070  loss: 0.0494 (0.0504)  time: 3.7460  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1050/3449]  eta: 2:31:28  lr: 0.000070  loss: 0.0473 (0.0503)  time: 3.7250  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1060/3449]  eta: 2:30:54  lr: 0.000070  loss: 0.0604 (0.0505)  time: 3.8552  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1070/3449]  eta: 2:30:17  lr: 0.000070  loss: 0.0631 (0.0505)  time: 3.8862  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1080/3449]  eta: 2:29:40  lr: 0.000070  loss: 0.0535 (0.0505)  time: 3.8282  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1090/3449]  eta: 2:29:01  lr: 0.000070  loss: 0.0463 (0.0505)  time: 3.8062  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1100/3449]  eta: 2:28:23  lr: 0.000070  loss: 0.0543 (0.0505)  time: 3.7564  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1110/3449]  eta: 2:27:44  lr: 0.000070  loss: 0.0569 (0.0505)  time: 3.7438  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1120/3449]  eta: 2:27:05  lr: 0.000070  loss: 0.0490 (0.0505)  time: 3.7354  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1130/3449]  eta: 2:26:24  lr: 0.000070  loss: 0.0552 (0.0506)  time: 3.6945  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1140/3449]  eta: 2:25:46  lr: 0.000070  loss: 0.0580 (0.0506)  time: 3.7095  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1150/3449]  eta: 2:25:08  lr: 0.000070  loss: 0.0531 (0.0506)  time: 3.7805  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1160/3449]  eta: 2:24:28  lr: 0.000070  loss: 0.0531 (0.0506)  time: 3.7435  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1170/3449]  eta: 2:23:48  lr: 0.000070  loss: 0.0598 (0.0507)  time: 3.6843  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1180/3449]  eta: 2:23:09  lr: 0.000070  loss: 0.0570 (0.0508)  time: 3.6884  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1190/3449]  eta: 2:22:31  lr: 0.000070  loss: 0.0480 (0.0508)  time: 3.7539  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1200/3449]  eta: 2:21:54  lr: 0.000070  loss: 0.0433 (0.0507)  time: 3.8112  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1210/3449]  eta: 2:21:17  lr: 0.000070  loss: 0.0393 (0.0506)  time: 3.8363  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1220/3449]  eta: 2:20:40  lr: 0.000070  loss: 0.0436 (0.0506)  time: 3.8191  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1230/3449]  eta: 2:20:03  lr: 0.000070  loss: 0.0478 (0.0506)  time: 3.8222  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1240/3449]  eta: 2:19:24  lr: 0.000070  loss: 0.0558 (0.0507)  time: 3.8036  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1250/3449]  eta: 2:18:47  lr: 0.000070  loss: 0.0562 (0.0507)  time: 3.7865  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1260/3449]  eta: 2:18:10  lr: 0.000070  loss: 0.0562 (0.0507)  time: 3.8365  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1270/3449]  eta: 2:17:32  lr: 0.000070  loss: 0.0573 (0.0508)  time: 3.8108  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1280/3449]  eta: 2:16:53  lr: 0.000070  loss: 0.0462 (0.0507)  time: 3.7502  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1290/3449]  eta: 2:16:15  lr: 0.000070  loss: 0.0417 (0.0506)  time: 3.7318  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1300/3449]  eta: 2:15:38  lr: 0.000070  loss: 0.0516 (0.0507)  time: 3.7873  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1310/3449]  eta: 2:14:59  lr: 0.000070  loss: 0.0516 (0.0506)  time: 3.8056  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1320/3449]  eta: 2:14:21  lr: 0.000070  loss: 0.0456 (0.0506)  time: 3.7431  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1330/3449]  eta: 2:13:44  lr: 0.000070  loss: 0.0563 (0.0507)  time: 3.7806  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1340/3449]  eta: 2:13:06  lr: 0.000070  loss: 0.0555 (0.0507)  time: 3.8249  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1350/3449]  eta: 2:12:29  lr: 0.000070  loss: 0.0482 (0.0506)  time: 3.8119  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1360/3449]  eta: 2:11:52  lr: 0.000070  loss: 0.0547 (0.0507)  time: 3.8306  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1370/3449]  eta: 2:11:15  lr: 0.000070  loss: 0.0547 (0.0507)  time: 3.8652  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1380/3449]  eta: 2:10:35  lr: 0.000070  loss: 0.0565 (0.0507)  time: 3.7649  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1390/3449]  eta: 2:09:58  lr: 0.000070  loss: 0.0565 (0.0507)  time: 3.7536  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1400/3449]  eta: 2:09:21  lr: 0.000070  loss: 0.0498 (0.0507)  time: 3.8552  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1410/3449]  eta: 2:08:43  lr: 0.000070  loss: 0.0508 (0.0507)  time: 3.8126  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1420/3449]  eta: 2:08:07  lr: 0.000070  loss: 0.0583 (0.0508)  time: 3.8344  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1430/3449]  eta: 2:07:29  lr: 0.000070  loss: 0.0536 (0.0508)  time: 3.8256  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1440/3449]  eta: 2:06:51  lr: 0.000070  loss: 0.0447 (0.0507)  time: 3.7857  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1450/3449]  eta: 2:06:14  lr: 0.000070  loss: 0.0513 (0.0507)  time: 3.8303  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1460/3449]  eta: 2:05:39  lr: 0.000070  loss: 0.0513 (0.0507)  time: 3.9179  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1470/3449]  eta: 2:05:01  lr: 0.000070  loss: 0.0532 (0.0508)  time: 3.8822  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1480/3449]  eta: 2:04:23  lr: 0.000070  loss: 0.0512 (0.0507)  time: 3.8055  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1490/3449]  eta: 2:03:46  lr: 0.000070  loss: 0.0509 (0.0507)  time: 3.8275  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1500/3449]  eta: 2:03:07  lr: 0.000070  loss: 0.0525 (0.0507)  time: 3.7616  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1510/3449]  eta: 2:02:29  lr: 0.000070  loss: 0.0525 (0.0508)  time: 3.7344  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1520/3449]  eta: 2:01:51  lr: 0.000070  loss: 0.0492 (0.0508)  time: 3.7817  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1530/3449]  eta: 2:01:11  lr: 0.000070  loss: 0.0458 (0.0507)  time: 3.7304  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1540/3449]  eta: 2:00:33  lr: 0.000070  loss: 0.0414 (0.0507)  time: 3.7030  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1550/3449]  eta: 1:59:57  lr: 0.000070  loss: 0.0416 (0.0507)  time: 3.8540  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1560/3449]  eta: 1:59:20  lr: 0.000070  loss: 0.0507 (0.0506)  time: 3.9070  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1570/3449]  eta: 1:58:43  lr: 0.000070  loss: 0.0488 (0.0506)  time: 3.8515  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1580/3449]  eta: 1:58:06  lr: 0.000070  loss: 0.0488 (0.0506)  time: 3.8515  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1590/3449]  eta: 1:57:28  lr: 0.000070  loss: 0.0540 (0.0506)  time: 3.8535  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1600/3449]  eta: 1:56:51  lr: 0.000070  loss: 0.0540 (0.0506)  time: 3.8246  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1610/3449]  eta: 1:56:13  lr: 0.000070  loss: 0.0517 (0.0506)  time: 3.7883  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1620/3449]  eta: 1:55:35  lr: 0.000070  loss: 0.0500 (0.0506)  time: 3.7938  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1630/3449]  eta: 1:54:57  lr: 0.000070  loss: 0.0509 (0.0506)  time: 3.8128  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1640/3449]  eta: 1:54:19  lr: 0.000070  loss: 0.0512 (0.0506)  time: 3.8003  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1650/3449]  eta: 1:53:41  lr: 0.000070  loss: 0.0521 (0.0506)  time: 3.7975  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1660/3449]  eta: 1:53:04  lr: 0.000070  loss: 0.0515 (0.0506)  time: 3.8137  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1670/3449]  eta: 1:52:26  lr: 0.000070  loss: 0.0522 (0.0507)  time: 3.8032  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1680/3449]  eta: 1:51:49  lr: 0.000070  loss: 0.0522 (0.0507)  time: 3.8571  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1690/3449]  eta: 1:51:11  lr: 0.000070  loss: 0.0513 (0.0507)  time: 3.8376  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1700/3449]  eta: 1:50:34  lr: 0.000070  loss: 0.0529 (0.0507)  time: 3.8008  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1710/3449]  eta: 1:49:56  lr: 0.000070  loss: 0.0499 (0.0507)  time: 3.8415  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1720/3449]  eta: 1:49:19  lr: 0.000070  loss: 0.0499 (0.0507)  time: 3.8531  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1730/3449]  eta: 1:48:41  lr: 0.000070  loss: 0.0528 (0.0507)  time: 3.8423  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1740/3449]  eta: 1:48:03  lr: 0.000070  loss: 0.0473 (0.0507)  time: 3.7794  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1750/3449]  eta: 1:47:24  lr: 0.000070  loss: 0.0432 (0.0506)  time: 3.7462  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1760/3449]  eta: 1:46:46  lr: 0.000070  loss: 0.0384 (0.0506)  time: 3.7673  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1770/3449]  eta: 1:46:08  lr: 0.000070  loss: 0.0370 (0.0505)  time: 3.7449  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1780/3449]  eta: 1:45:29  lr: 0.000070  loss: 0.0412 (0.0505)  time: 3.7335  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1790/3449]  eta: 1:44:50  lr: 0.000070  loss: 0.0469 (0.0505)  time: 3.6987  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1800/3449]  eta: 1:44:12  lr: 0.000070  loss: 0.0486 (0.0505)  time: 3.7208  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1810/3449]  eta: 1:43:34  lr: 0.000070  loss: 0.0492 (0.0505)  time: 3.7750  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1820/3449]  eta: 1:42:56  lr: 0.000070  loss: 0.0473 (0.0504)  time: 3.7600  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1830/3449]  eta: 1:42:18  lr: 0.000070  loss: 0.0559 (0.0505)  time: 3.8046  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1840/3449]  eta: 1:41:40  lr: 0.000070  loss: 0.0621 (0.0505)  time: 3.8155  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1850/3449]  eta: 1:41:03  lr: 0.000070  loss: 0.0448 (0.0505)  time: 3.7978  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1860/3449]  eta: 1:40:25  lr: 0.000070  loss: 0.0444 (0.0505)  time: 3.7973  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1870/3449]  eta: 1:39:46  lr: 0.000070  loss: 0.0442 (0.0504)  time: 3.7740  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1880/3449]  eta: 1:39:09  lr: 0.000070  loss: 0.0421 (0.0504)  time: 3.7940  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1890/3449]  eta: 1:38:31  lr: 0.000070  loss: 0.0415 (0.0503)  time: 3.7992  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1900/3449]  eta: 1:37:53  lr: 0.000070  loss: 0.0415 (0.0503)  time: 3.7982  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1910/3449]  eta: 1:37:16  lr: 0.000070  loss: 0.0566 (0.0504)  time: 3.8524  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1920/3449]  eta: 1:36:37  lr: 0.000070  loss: 0.0562 (0.0503)  time: 3.7725  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1930/3449]  eta: 1:35:58  lr: 0.000070  loss: 0.0550 (0.0504)  time: 3.6760  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1940/3449]  eta: 1:35:20  lr: 0.000070  loss: 0.0550 (0.0503)  time: 3.6963  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1950/3449]  eta: 1:34:42  lr: 0.000070  loss: 0.0446 (0.0503)  time: 3.7521  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1960/3449]  eta: 1:34:04  lr: 0.000070  loss: 0.0526 (0.0503)  time: 3.7947  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1970/3449]  eta: 1:33:27  lr: 0.000070  loss: 0.0548 (0.0504)  time: 3.8734  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1980/3449]  eta: 1:32:50  lr: 0.000070  loss: 0.0616 (0.0504)  time: 3.9401  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [1990/3449]  eta: 1:32:12  lr: 0.000070  loss: 0.0597 (0.0504)  time: 3.8681  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2000/3449]  eta: 1:31:35  lr: 0.000070  loss: 0.0470 (0.0504)  time: 3.8442  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2010/3449]  eta: 1:30:57  lr: 0.000070  loss: 0.0409 (0.0503)  time: 3.8354  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2020/3449]  eta: 1:30:20  lr: 0.000070  loss: 0.0425 (0.0503)  time: 3.8115  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2030/3449]  eta: 1:29:42  lr: 0.000070  loss: 0.0485 (0.0503)  time: 3.8461  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2040/3449]  eta: 1:29:04  lr: 0.000070  loss: 0.0511 (0.0504)  time: 3.7880  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2050/3449]  eta: 1:28:26  lr: 0.000070  loss: 0.0459 (0.0503)  time: 3.7808  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2060/3449]  eta: 1:27:49  lr: 0.000070  loss: 0.0424 (0.0503)  time: 3.9149  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2070/3449]  eta: 1:27:11  lr: 0.000070  loss: 0.0480 (0.0503)  time: 3.8861  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2080/3449]  eta: 1:26:34  lr: 0.000070  loss: 0.0544 (0.0504)  time: 3.8096  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2090/3449]  eta: 1:25:56  lr: 0.000070  loss: 0.0579 (0.0503)  time: 3.7893  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2100/3449]  eta: 1:25:16  lr: 0.000070  loss: 0.0482 (0.0503)  time: 3.6737  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2110/3449]  eta: 1:24:38  lr: 0.000070  loss: 0.0424 (0.0503)  time: 3.6979  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2120/3449]  eta: 1:24:01  lr: 0.000070  loss: 0.0430 (0.0503)  time: 3.8197  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2130/3449]  eta: 1:23:24  lr: 0.000070  loss: 0.0551 (0.0503)  time: 3.8879  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2140/3449]  eta: 1:22:46  lr: 0.000070  loss: 0.0531 (0.0503)  time: 3.8602  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2150/3449]  eta: 1:22:08  lr: 0.000070  loss: 0.0530 (0.0503)  time: 3.8048  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2160/3449]  eta: 1:21:30  lr: 0.000070  loss: 0.0540 (0.0503)  time: 3.8161  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2170/3449]  eta: 1:20:52  lr: 0.000070  loss: 0.0484 (0.0503)  time: 3.7595  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2180/3449]  eta: 1:20:13  lr: 0.000070  loss: 0.0413 (0.0503)  time: 3.7035  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2190/3449]  eta: 1:19:35  lr: 0.000070  loss: 0.0499 (0.0502)  time: 3.7174  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2200/3449]  eta: 1:18:57  lr: 0.000070  loss: 0.0499 (0.0503)  time: 3.7551  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2210/3449]  eta: 1:18:18  lr: 0.000070  loss: 0.0480 (0.0503)  time: 3.7347  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2220/3449]  eta: 1:17:41  lr: 0.000070  loss: 0.0445 (0.0502)  time: 3.7589  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2230/3449]  eta: 1:17:03  lr: 0.000070  loss: 0.0445 (0.0502)  time: 3.7879  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2240/3449]  eta: 1:16:24  lr: 0.000070  loss: 0.0388 (0.0502)  time: 3.7476  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2250/3449]  eta: 1:15:46  lr: 0.000070  loss: 0.0520 (0.0502)  time: 3.7746  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2260/3449]  eta: 1:15:09  lr: 0.000070  loss: 0.0579 (0.0502)  time: 3.8324  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2270/3449]  eta: 1:14:31  lr: 0.000070  loss: 0.0428 (0.0502)  time: 3.8530  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2280/3449]  eta: 1:13:53  lr: 0.000070  loss: 0.0413 (0.0501)  time: 3.8038  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2290/3449]  eta: 1:13:16  lr: 0.000070  loss: 0.0453 (0.0502)  time: 3.8013  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2300/3449]  eta: 1:12:38  lr: 0.000070  loss: 0.0556 (0.0502)  time: 3.8429  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2310/3449]  eta: 1:12:00  lr: 0.000070  loss: 0.0456 (0.0501)  time: 3.8583  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2320/3449]  eta: 1:11:23  lr: 0.000070  loss: 0.0456 (0.0501)  time: 3.8580  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2330/3449]  eta: 1:10:44  lr: 0.000070  loss: 0.0506 (0.0501)  time: 3.7716  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2340/3449]  eta: 1:10:07  lr: 0.000070  loss: 0.0534 (0.0501)  time: 3.7854  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2350/3449]  eta: 1:09:28  lr: 0.000070  loss: 0.0552 (0.0502)  time: 3.7801  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2360/3449]  eta: 1:08:51  lr: 0.000070  loss: 0.0552 (0.0502)  time: 3.7883  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2370/3449]  eta: 1:08:13  lr: 0.000070  loss: 0.0563 (0.0502)  time: 3.8342  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2380/3449]  eta: 1:07:35  lr: 0.000070  loss: 0.0570 (0.0502)  time: 3.7913  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2390/3449]  eta: 1:06:57  lr: 0.000070  loss: 0.0525 (0.0502)  time: 3.7847  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2400/3449]  eta: 1:06:19  lr: 0.000070  loss: 0.0444 (0.0502)  time: 3.7881  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2410/3449]  eta: 1:05:41  lr: 0.000070  loss: 0.0493 (0.0502)  time: 3.7820  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2420/3449]  eta: 1:05:03  lr: 0.000070  loss: 0.0514 (0.0502)  time: 3.8005  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2430/3449]  eta: 1:04:25  lr: 0.000070  loss: 0.0524 (0.0502)  time: 3.8293  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2440/3449]  eta: 1:03:47  lr: 0.000070  loss: 0.0555 (0.0502)  time: 3.7821  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2450/3449]  eta: 1:03:09  lr: 0.000070  loss: 0.0504 (0.0502)  time: 3.8005  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2460/3449]  eta: 1:02:32  lr: 0.000070  loss: 0.0525 (0.0502)  time: 3.8310  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2470/3449]  eta: 1:01:54  lr: 0.000070  loss: 0.0534 (0.0503)  time: 3.7900  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2480/3449]  eta: 1:01:16  lr: 0.000070  loss: 0.0583 (0.0503)  time: 3.7933  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2490/3449]  eta: 1:00:38  lr: 0.000070  loss: 0.0539 (0.0503)  time: 3.8801  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2500/3449]  eta: 1:00:00  lr: 0.000070  loss: 0.0530 (0.0503)  time: 3.8249  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2510/3449]  eta: 0:59:22  lr: 0.000070  loss: 0.0499 (0.0503)  time: 3.6885  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2520/3449]  eta: 0:58:44  lr: 0.000070  loss: 0.0403 (0.0503)  time: 3.7130  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2530/3449]  eta: 0:58:06  lr: 0.000070  loss: 0.0489 (0.0503)  time: 3.7799  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2540/3449]  eta: 0:57:27  lr: 0.000070  loss: 0.0523 (0.0503)  time: 3.7460  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2550/3449]  eta: 0:56:50  lr: 0.000070  loss: 0.0518 (0.0503)  time: 3.7537  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2560/3449]  eta: 0:56:12  lr: 0.000070  loss: 0.0537 (0.0503)  time: 3.8241  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2570/3449]  eta: 0:55:34  lr: 0.000070  loss: 0.0546 (0.0503)  time: 3.8061  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2580/3449]  eta: 0:54:56  lr: 0.000070  loss: 0.0488 (0.0502)  time: 3.8081  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2590/3449]  eta: 0:54:18  lr: 0.000070  loss: 0.0464 (0.0503)  time: 3.7769  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2600/3449]  eta: 0:53:40  lr: 0.000070  loss: 0.0465 (0.0502)  time: 3.7846  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2610/3449]  eta: 0:53:02  lr: 0.000070  loss: 0.0525 (0.0503)  time: 3.8673  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2620/3449]  eta: 0:52:24  lr: 0.000070  loss: 0.0547 (0.0503)  time: 3.8174  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2630/3449]  eta: 0:51:47  lr: 0.000070  loss: 0.0505 (0.0502)  time: 3.8030  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2640/3449]  eta: 0:51:09  lr: 0.000070  loss: 0.0474 (0.0502)  time: 3.8299  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2650/3449]  eta: 0:50:31  lr: 0.000070  loss: 0.0488 (0.0502)  time: 3.7789  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2660/3449]  eta: 0:49:52  lr: 0.000070  loss: 0.0525 (0.0502)  time: 3.7168  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2670/3449]  eta: 0:49:15  lr: 0.000070  loss: 0.0469 (0.0502)  time: 3.7842  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2680/3449]  eta: 0:48:36  lr: 0.000070  loss: 0.0535 (0.0502)  time: 3.7765  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2690/3449]  eta: 0:47:59  lr: 0.000070  loss: 0.0565 (0.0502)  time: 3.7468  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2700/3449]  eta: 0:47:21  lr: 0.000070  loss: 0.0529 (0.0503)  time: 3.7965  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2710/3449]  eta: 0:46:43  lr: 0.000070  loss: 0.0577 (0.0503)  time: 3.7624  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2720/3449]  eta: 0:46:05  lr: 0.000070  loss: 0.0577 (0.0503)  time: 3.7802  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2730/3449]  eta: 0:45:27  lr: 0.000070  loss: 0.0552 (0.0503)  time: 3.7644  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2740/3449]  eta: 0:44:48  lr: 0.000070  loss: 0.0521 (0.0503)  time: 3.7108  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2750/3449]  eta: 0:44:10  lr: 0.000070  loss: 0.0490 (0.0503)  time: 3.7010  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2760/3449]  eta: 0:43:32  lr: 0.000070  loss: 0.0455 (0.0503)  time: 3.7134  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2770/3449]  eta: 0:42:54  lr: 0.000070  loss: 0.0503 (0.0503)  time: 3.7234  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2780/3449]  eta: 0:42:16  lr: 0.000070  loss: 0.0517 (0.0503)  time: 3.7631  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2790/3449]  eta: 0:41:38  lr: 0.000070  loss: 0.0469 (0.0503)  time: 3.7982  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2800/3449]  eta: 0:41:01  lr: 0.000070  loss: 0.0469 (0.0502)  time: 3.8641  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2810/3449]  eta: 0:40:23  lr: 0.000070  loss: 0.0472 (0.0502)  time: 3.8738  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2820/3449]  eta: 0:39:45  lr: 0.000070  loss: 0.0580 (0.0503)  time: 3.7616  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2830/3449]  eta: 0:39:07  lr: 0.000070  loss: 0.0608 (0.0503)  time: 3.7159  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2840/3449]  eta: 0:38:28  lr: 0.000070  loss: 0.0471 (0.0503)  time: 3.7235  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2850/3449]  eta: 0:37:51  lr: 0.000070  loss: 0.0474 (0.0503)  time: 3.7758  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2860/3449]  eta: 0:37:13  lr: 0.000070  loss: 0.0538 (0.0503)  time: 3.8168  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2870/3449]  eta: 0:36:35  lr: 0.000070  loss: 0.0518 (0.0503)  time: 3.7591  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2880/3449]  eta: 0:35:57  lr: 0.000070  loss: 0.0488 (0.0503)  time: 3.7083  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2890/3449]  eta: 0:35:19  lr: 0.000070  loss: 0.0472 (0.0502)  time: 3.6979  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2900/3449]  eta: 0:34:41  lr: 0.000070  loss: 0.0477 (0.0502)  time: 3.7683  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2910/3449]  eta: 0:34:03  lr: 0.000070  loss: 0.0518 (0.0502)  time: 3.7855  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2920/3449]  eta: 0:33:25  lr: 0.000070  loss: 0.0529 (0.0502)  time: 3.7503  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2930/3449]  eta: 0:32:47  lr: 0.000070  loss: 0.0575 (0.0503)  time: 3.7857  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2940/3449]  eta: 0:32:09  lr: 0.000070  loss: 0.0519 (0.0502)  time: 3.7236  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2950/3449]  eta: 0:31:31  lr: 0.000070  loss: 0.0481 (0.0502)  time: 3.8019  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2960/3449]  eta: 0:30:53  lr: 0.000070  loss: 0.0521 (0.0503)  time: 3.8432  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2970/3449]  eta: 0:30:15  lr: 0.000070  loss: 0.0593 (0.0503)  time: 3.7942  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2980/3449]  eta: 0:29:37  lr: 0.000070  loss: 0.0526 (0.0503)  time: 3.8232  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [2990/3449]  eta: 0:29:00  lr: 0.000070  loss: 0.0480 (0.0503)  time: 3.8142  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3000/3449]  eta: 0:28:22  lr: 0.000070  loss: 0.0495 (0.0503)  time: 3.8365  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3010/3449]  eta: 0:27:44  lr: 0.000070  loss: 0.0437 (0.0503)  time: 3.7841  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3020/3449]  eta: 0:27:06  lr: 0.000070  loss: 0.0499 (0.0503)  time: 3.7586  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3030/3449]  eta: 0:26:28  lr: 0.000070  loss: 0.0527 (0.0503)  time: 3.7595  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3040/3449]  eta: 0:25:50  lr: 0.000070  loss: 0.0528 (0.0503)  time: 3.7244  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3050/3449]  eta: 0:25:12  lr: 0.000070  loss: 0.0487 (0.0503)  time: 3.7584  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3060/3449]  eta: 0:24:34  lr: 0.000070  loss: 0.0523 (0.0503)  time: 3.8068  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3070/3449]  eta: 0:23:56  lr: 0.000070  loss: 0.0545 (0.0503)  time: 3.7550  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3080/3449]  eta: 0:23:18  lr: 0.000070  loss: 0.0569 (0.0503)  time: 3.6907  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3090/3449]  eta: 0:22:40  lr: 0.000070  loss: 0.0550 (0.0503)  time: 3.6898  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3100/3449]  eta: 0:22:02  lr: 0.000070  loss: 0.0531 (0.0504)  time: 3.7977  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3110/3449]  eta: 0:21:24  lr: 0.000070  loss: 0.0560 (0.0504)  time: 3.9046  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3120/3449]  eta: 0:20:46  lr: 0.000070  loss: 0.0533 (0.0504)  time: 3.8370  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3130/3449]  eta: 0:20:08  lr: 0.000070  loss: 0.0511 (0.0504)  time: 3.7298  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3140/3449]  eta: 0:19:31  lr: 0.000070  loss: 0.0511 (0.0504)  time: 3.7839  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3150/3449]  eta: 0:18:53  lr: 0.000070  loss: 0.0455 (0.0503)  time: 3.8077  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3160/3449]  eta: 0:18:15  lr: 0.000070  loss: 0.0455 (0.0503)  time: 3.7743  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3170/3449]  eta: 0:17:37  lr: 0.000070  loss: 0.0581 (0.0504)  time: 3.7850  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3180/3449]  eta: 0:16:59  lr: 0.000070  loss: 0.0581 (0.0504)  time: 3.7827  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3190/3449]  eta: 0:16:21  lr: 0.000070  loss: 0.0499 (0.0504)  time: 3.7893  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3200/3449]  eta: 0:15:43  lr: 0.000070  loss: 0.0466 (0.0503)  time: 3.7782  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3210/3449]  eta: 0:15:05  lr: 0.000070  loss: 0.0466 (0.0504)  time: 3.7507  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3220/3449]  eta: 0:14:27  lr: 0.000070  loss: 0.0535 (0.0504)  time: 3.7852  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3230/3449]  eta: 0:13:49  lr: 0.000070  loss: 0.0565 (0.0504)  time: 3.7823  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3240/3449]  eta: 0:13:12  lr: 0.000070  loss: 0.0600 (0.0504)  time: 3.7631  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3250/3449]  eta: 0:12:34  lr: 0.000070  loss: 0.0643 (0.0504)  time: 3.7706  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3260/3449]  eta: 0:11:56  lr: 0.000070  loss: 0.0551 (0.0504)  time: 3.7074  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3270/3449]  eta: 0:11:18  lr: 0.000070  loss: 0.0456 (0.0504)  time: 3.7598  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3280/3449]  eta: 0:10:40  lr: 0.000070  loss: 0.0451 (0.0504)  time: 3.8425  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3290/3449]  eta: 0:10:02  lr: 0.000070  loss: 0.0447 (0.0503)  time: 3.7996  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3300/3449]  eta: 0:09:24  lr: 0.000070  loss: 0.0464 (0.0503)  time: 3.8348  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3310/3449]  eta: 0:08:46  lr: 0.000070  loss: 0.0557 (0.0504)  time: 3.8482  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3320/3449]  eta: 0:08:08  lr: 0.000070  loss: 0.0600 (0.0504)  time: 3.8115  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3330/3449]  eta: 0:07:31  lr: 0.000070  loss: 0.0592 (0.0504)  time: 3.8485  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3340/3449]  eta: 0:06:53  lr: 0.000070  loss: 0.0487 (0.0504)  time: 3.8521  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3350/3449]  eta: 0:06:15  lr: 0.000070  loss: 0.0464 (0.0504)  time: 3.8479  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3360/3449]  eta: 0:05:37  lr: 0.000070  loss: 0.0464 (0.0504)  time: 3.8385  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3370/3449]  eta: 0:04:59  lr: 0.000070  loss: 0.0501 (0.0504)  time: 3.8405  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3380/3449]  eta: 0:04:21  lr: 0.000070  loss: 0.0552 (0.0504)  time: 3.7969  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3390/3449]  eta: 0:03:43  lr: 0.000070  loss: 0.0552 (0.0504)  time: 3.7228  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3400/3449]  eta: 0:03:05  lr: 0.000070  loss: 0.0573 (0.0504)  time: 3.7723  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3410/3449]  eta: 0:02:27  lr: 0.000070  loss: 0.0573 (0.0504)  time: 3.7977  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3420/3449]  eta: 0:01:49  lr: 0.000070  loss: 0.0537 (0.0504)  time: 3.7654  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3430/3449]  eta: 0:01:12  lr: 0.000070  loss: 0.0599 (0.0504)  time: 3.7844  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3440/3449]  eta: 0:00:34  lr: 0.000070  loss: 0.0606 (0.0504)  time: 3.8116  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [3448/3449]  eta: 0:00:03  lr: 0.000070  loss: 0.0589 (0.0504)  time: 3.7964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:8] Total time: 3:37:53 (3.7905 s / it)\n",
      "Averaged stats: lr: 0.000070  loss: 0.0589 (0.0504)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:8]  [ 0/14]  eta: 0:04:21  loss: 0.0354 (0.0354)  time: 18.6526  data: 0.4276  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:8]  [13/14]  eta: 0:00:18  loss: 0.0392 (0.0407)  time: 18.2667  data: 0.0307  max mem: 34968\n",
      "Valid: [epoch:8] Total time: 0:04:15 (18.2782 s / it)\n",
      "Averaged stats: loss: 0.0392 (0.0407)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_8_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.041%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [   0/3449]  eta: 4:48:19  lr: 0.000080  loss: 0.0867 (0.0867)  time: 5.0159  data: 1.5872  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [  10/3449]  eta: 3:39:11  lr: 0.000080  loss: 0.0662 (0.0687)  time: 3.8242  data: 0.1444  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [  20/3449]  eta: 3:36:33  lr: 0.000080  loss: 0.0594 (0.0632)  time: 3.7278  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [  30/3449]  eta: 3:38:39  lr: 0.000080  loss: 0.0438 (0.0557)  time: 3.8444  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [  40/3449]  eta: 3:37:39  lr: 0.000080  loss: 0.0403 (0.0539)  time: 3.8749  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [  50/3449]  eta: 3:37:04  lr: 0.000080  loss: 0.0459 (0.0516)  time: 3.8237  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [  60/3449]  eta: 3:35:50  lr: 0.000080  loss: 0.0492 (0.0521)  time: 3.8019  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [  70/3449]  eta: 3:35:03  lr: 0.000080  loss: 0.0521 (0.0519)  time: 3.7850  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [  80/3449]  eta: 3:33:08  lr: 0.000080  loss: 0.0477 (0.0505)  time: 3.7183  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [  90/3449]  eta: 3:32:21  lr: 0.000080  loss: 0.0472 (0.0509)  time: 3.7024  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 100/3449]  eta: 3:31:31  lr: 0.000080  loss: 0.0437 (0.0503)  time: 3.7645  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 110/3449]  eta: 3:30:19  lr: 0.000080  loss: 0.0433 (0.0499)  time: 3.7172  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 120/3449]  eta: 3:29:37  lr: 0.000080  loss: 0.0460 (0.0500)  time: 3.7195  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 130/3449]  eta: 3:28:52  lr: 0.000080  loss: 0.0503 (0.0500)  time: 3.7566  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 140/3449]  eta: 3:28:22  lr: 0.000080  loss: 0.0507 (0.0500)  time: 3.7790  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 150/3449]  eta: 3:27:50  lr: 0.000080  loss: 0.0534 (0.0503)  time: 3.8077  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 160/3449]  eta: 3:27:08  lr: 0.000080  loss: 0.0526 (0.0504)  time: 3.7838  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 170/3449]  eta: 3:26:24  lr: 0.000080  loss: 0.0468 (0.0500)  time: 3.7525  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 180/3449]  eta: 3:25:29  lr: 0.000080  loss: 0.0397 (0.0495)  time: 3.7140  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 190/3449]  eta: 3:25:06  lr: 0.000080  loss: 0.0377 (0.0489)  time: 3.7695  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 200/3449]  eta: 3:24:25  lr: 0.000080  loss: 0.0466 (0.0491)  time: 3.8045  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 210/3449]  eta: 3:23:38  lr: 0.000080  loss: 0.0525 (0.0493)  time: 3.7361  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 220/3449]  eta: 3:23:09  lr: 0.000080  loss: 0.0552 (0.0498)  time: 3.7752  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 230/3449]  eta: 3:22:10  lr: 0.000080  loss: 0.0610 (0.0502)  time: 3.7284  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 240/3449]  eta: 3:21:45  lr: 0.000080  loss: 0.0571 (0.0502)  time: 3.7443  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 250/3449]  eta: 3:21:22  lr: 0.000080  loss: 0.0594 (0.0507)  time: 3.8738  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 260/3449]  eta: 3:20:33  lr: 0.000080  loss: 0.0594 (0.0507)  time: 3.7848  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 270/3449]  eta: 3:19:47  lr: 0.000080  loss: 0.0509 (0.0509)  time: 3.6938  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 280/3449]  eta: 3:19:21  lr: 0.000080  loss: 0.0608 (0.0508)  time: 3.7878  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 290/3449]  eta: 3:18:42  lr: 0.000080  loss: 0.0529 (0.0509)  time: 3.8177  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 300/3449]  eta: 3:18:02  lr: 0.000080  loss: 0.0531 (0.0509)  time: 3.7568  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 310/3449]  eta: 3:17:16  lr: 0.000080  loss: 0.0481 (0.0507)  time: 3.7238  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 320/3449]  eta: 3:16:41  lr: 0.000080  loss: 0.0481 (0.0504)  time: 3.7485  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 330/3449]  eta: 3:16:02  lr: 0.000080  loss: 0.0547 (0.0506)  time: 3.7777  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 340/3449]  eta: 3:15:27  lr: 0.000080  loss: 0.0510 (0.0507)  time: 3.7789  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 350/3449]  eta: 3:14:53  lr: 0.000080  loss: 0.0495 (0.0505)  time: 3.8067  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 360/3449]  eta: 3:14:13  lr: 0.000080  loss: 0.0417 (0.0504)  time: 3.7824  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 370/3449]  eta: 3:13:32  lr: 0.000080  loss: 0.0457 (0.0503)  time: 3.7411  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 380/3449]  eta: 3:12:55  lr: 0.000080  loss: 0.0451 (0.0502)  time: 3.7518  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 390/3449]  eta: 3:12:16  lr: 0.000080  loss: 0.0478 (0.0503)  time: 3.7687  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 400/3449]  eta: 3:11:38  lr: 0.000080  loss: 0.0519 (0.0504)  time: 3.7656  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 410/3449]  eta: 3:11:17  lr: 0.000080  loss: 0.0519 (0.0505)  time: 3.8786  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 420/3449]  eta: 3:10:38  lr: 0.000080  loss: 0.0523 (0.0506)  time: 3.8783  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 430/3449]  eta: 3:10:04  lr: 0.000080  loss: 0.0474 (0.0503)  time: 3.7949  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 440/3449]  eta: 3:09:30  lr: 0.000080  loss: 0.0474 (0.0505)  time: 3.8299  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 450/3449]  eta: 3:08:50  lr: 0.000080  loss: 0.0607 (0.0508)  time: 3.7894  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 460/3449]  eta: 3:08:16  lr: 0.000080  loss: 0.0551 (0.0507)  time: 3.7919  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 470/3449]  eta: 3:07:42  lr: 0.000080  loss: 0.0568 (0.0509)  time: 3.8404  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 480/3449]  eta: 3:07:11  lr: 0.000080  loss: 0.0599 (0.0511)  time: 3.8622  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 490/3449]  eta: 3:06:39  lr: 0.000080  loss: 0.0609 (0.0512)  time: 3.8824  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 500/3449]  eta: 3:06:01  lr: 0.000080  loss: 0.0518 (0.0510)  time: 3.8346  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 510/3449]  eta: 3:05:21  lr: 0.000080  loss: 0.0497 (0.0511)  time: 3.7707  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 520/3449]  eta: 3:04:49  lr: 0.000080  loss: 0.0513 (0.0511)  time: 3.8173  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 530/3449]  eta: 3:04:12  lr: 0.000080  loss: 0.0495 (0.0511)  time: 3.8391  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 540/3449]  eta: 3:03:31  lr: 0.000080  loss: 0.0524 (0.0512)  time: 3.7676  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 550/3449]  eta: 3:02:53  lr: 0.000080  loss: 0.0554 (0.0512)  time: 3.7545  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 560/3449]  eta: 3:02:17  lr: 0.000080  loss: 0.0545 (0.0512)  time: 3.7967  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 570/3449]  eta: 3:01:43  lr: 0.000080  loss: 0.0530 (0.0512)  time: 3.8412  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 580/3449]  eta: 3:01:05  lr: 0.000080  loss: 0.0522 (0.0512)  time: 3.8214  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 590/3449]  eta: 3:00:25  lr: 0.000080  loss: 0.0510 (0.0512)  time: 3.7719  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 600/3449]  eta: 2:59:45  lr: 0.000080  loss: 0.0511 (0.0512)  time: 3.7457  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 610/3449]  eta: 2:59:08  lr: 0.000080  loss: 0.0513 (0.0511)  time: 3.7676  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 620/3449]  eta: 2:58:23  lr: 0.000080  loss: 0.0528 (0.0515)  time: 3.7195  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 630/3449]  eta: 2:57:39  lr: 0.000080  loss: 0.0528 (0.0515)  time: 3.6434  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 640/3449]  eta: 2:56:52  lr: 0.000080  loss: 0.0480 (0.0515)  time: 3.6106  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 650/3449]  eta: 2:56:07  lr: 0.000080  loss: 0.0480 (0.0514)  time: 3.5821  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 660/3449]  eta: 2:55:27  lr: 0.000080  loss: 0.0496 (0.0514)  time: 3.6563  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 670/3449]  eta: 2:54:46  lr: 0.000080  loss: 0.0580 (0.0516)  time: 3.7150  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 680/3449]  eta: 2:54:09  lr: 0.000080  loss: 0.0565 (0.0515)  time: 3.7434  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 690/3449]  eta: 2:53:29  lr: 0.000080  loss: 0.0461 (0.0514)  time: 3.7531  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 700/3449]  eta: 2:52:45  lr: 0.000080  loss: 0.0502 (0.0514)  time: 3.6721  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 710/3449]  eta: 2:52:07  lr: 0.000080  loss: 0.0563 (0.0514)  time: 3.6817  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 720/3449]  eta: 2:51:26  lr: 0.000080  loss: 0.0571 (0.0515)  time: 3.7156  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 730/3449]  eta: 2:50:43  lr: 0.000080  loss: 0.0531 (0.0514)  time: 3.6626  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 740/3449]  eta: 2:50:03  lr: 0.000080  loss: 0.0426 (0.0514)  time: 3.6661  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 750/3449]  eta: 2:49:24  lr: 0.000080  loss: 0.0426 (0.0513)  time: 3.7171  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 760/3449]  eta: 2:48:47  lr: 0.000080  loss: 0.0550 (0.0513)  time: 3.7652  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 770/3449]  eta: 2:48:10  lr: 0.000080  loss: 0.0584 (0.0515)  time: 3.7786  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 780/3449]  eta: 2:47:29  lr: 0.000080  loss: 0.0511 (0.0514)  time: 3.7265  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 790/3449]  eta: 2:46:52  lr: 0.000080  loss: 0.0511 (0.0514)  time: 3.7336  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 800/3449]  eta: 2:46:12  lr: 0.000080  loss: 0.0441 (0.0513)  time: 3.7404  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 810/3449]  eta: 2:45:36  lr: 0.000080  loss: 0.0412 (0.0513)  time: 3.7580  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 820/3449]  eta: 2:44:57  lr: 0.000080  loss: 0.0473 (0.0513)  time: 3.7726  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 830/3449]  eta: 2:44:20  lr: 0.000080  loss: 0.0586 (0.0513)  time: 3.7415  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 840/3449]  eta: 2:43:35  lr: 0.000080  loss: 0.0574 (0.0513)  time: 3.6549  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 850/3449]  eta: 2:42:53  lr: 0.000080  loss: 0.0491 (0.0512)  time: 3.5761  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 860/3449]  eta: 2:42:15  lr: 0.000080  loss: 0.0494 (0.0512)  time: 3.6730  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 870/3449]  eta: 2:41:34  lr: 0.000080  loss: 0.0524 (0.0512)  time: 3.6978  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 880/3449]  eta: 2:40:52  lr: 0.000080  loss: 0.0524 (0.0511)  time: 3.6251  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 890/3449]  eta: 2:40:12  lr: 0.000080  loss: 0.0417 (0.0510)  time: 3.6513  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 900/3449]  eta: 2:39:35  lr: 0.000080  loss: 0.0423 (0.0510)  time: 3.7251  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 910/3449]  eta: 2:38:57  lr: 0.000080  loss: 0.0555 (0.0510)  time: 3.7549  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 920/3449]  eta: 2:38:20  lr: 0.000080  loss: 0.0488 (0.0510)  time: 3.7574  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 930/3449]  eta: 2:37:42  lr: 0.000080  loss: 0.0547 (0.0511)  time: 3.7636  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 940/3449]  eta: 2:37:03  lr: 0.000080  loss: 0.0561 (0.0511)  time: 3.7220  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 950/3449]  eta: 2:36:25  lr: 0.000080  loss: 0.0552 (0.0511)  time: 3.7090  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 960/3449]  eta: 2:35:48  lr: 0.000080  loss: 0.0543 (0.0511)  time: 3.7588  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 970/3449]  eta: 2:35:09  lr: 0.000080  loss: 0.0543 (0.0512)  time: 3.7459  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 980/3449]  eta: 2:34:30  lr: 0.000080  loss: 0.0538 (0.0512)  time: 3.7053  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 990/3449]  eta: 2:33:51  lr: 0.000080  loss: 0.0519 (0.0512)  time: 3.6992  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1000/3449]  eta: 2:33:09  lr: 0.000080  loss: 0.0512 (0.0511)  time: 3.6287  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1010/3449]  eta: 2:32:30  lr: 0.000080  loss: 0.0427 (0.0510)  time: 3.6323  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1020/3449]  eta: 2:31:52  lr: 0.000080  loss: 0.0406 (0.0510)  time: 3.7215  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1030/3449]  eta: 2:31:14  lr: 0.000080  loss: 0.0524 (0.0510)  time: 3.7193  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1040/3449]  eta: 2:30:35  lr: 0.000080  loss: 0.0505 (0.0509)  time: 3.7144  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1050/3449]  eta: 2:29:57  lr: 0.000080  loss: 0.0460 (0.0509)  time: 3.7133  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1060/3449]  eta: 2:29:19  lr: 0.000080  loss: 0.0561 (0.0510)  time: 3.7096  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1070/3449]  eta: 2:28:43  lr: 0.000080  loss: 0.0588 (0.0510)  time: 3.7656  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1080/3449]  eta: 2:28:04  lr: 0.000080  loss: 0.0530 (0.0510)  time: 3.7668  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1090/3449]  eta: 2:27:28  lr: 0.000080  loss: 0.0515 (0.0510)  time: 3.7662  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1100/3449]  eta: 2:26:51  lr: 0.000080  loss: 0.0515 (0.0510)  time: 3.7873  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1110/3449]  eta: 2:26:11  lr: 0.000080  loss: 0.0524 (0.0510)  time: 3.6988  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1120/3449]  eta: 2:25:33  lr: 0.000080  loss: 0.0524 (0.0510)  time: 3.6714  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1130/3449]  eta: 2:24:55  lr: 0.000080  loss: 0.0552 (0.0511)  time: 3.7290  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1140/3449]  eta: 2:24:17  lr: 0.000080  loss: 0.0618 (0.0512)  time: 3.7350  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1150/3449]  eta: 2:23:38  lr: 0.000080  loss: 0.0505 (0.0511)  time: 3.6958  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1160/3449]  eta: 2:22:59  lr: 0.000080  loss: 0.0397 (0.0511)  time: 3.6820  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1170/3449]  eta: 2:22:21  lr: 0.000080  loss: 0.0529 (0.0511)  time: 3.7039  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1180/3449]  eta: 2:21:45  lr: 0.000080  loss: 0.0518 (0.0511)  time: 3.7567  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1190/3449]  eta: 2:21:05  lr: 0.000080  loss: 0.0537 (0.0512)  time: 3.7011  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1200/3449]  eta: 2:20:22  lr: 0.000080  loss: 0.0419 (0.0511)  time: 3.5528  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1210/3449]  eta: 2:19:43  lr: 0.000080  loss: 0.0373 (0.0510)  time: 3.5749  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1220/3449]  eta: 2:19:05  lr: 0.000080  loss: 0.0424 (0.0509)  time: 3.6759  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1230/3449]  eta: 2:18:28  lr: 0.000080  loss: 0.0510 (0.0509)  time: 3.7321  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1240/3449]  eta: 2:17:49  lr: 0.000080  loss: 0.0557 (0.0509)  time: 3.7236  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1250/3449]  eta: 2:17:08  lr: 0.000080  loss: 0.0509 (0.0509)  time: 3.5899  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1260/3449]  eta: 2:16:31  lr: 0.000080  loss: 0.0540 (0.0509)  time: 3.6461  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1270/3449]  eta: 2:15:52  lr: 0.000080  loss: 0.0521 (0.0509)  time: 3.7211  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1280/3449]  eta: 2:15:15  lr: 0.000080  loss: 0.0458 (0.0508)  time: 3.7206  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1290/3449]  eta: 2:14:37  lr: 0.000080  loss: 0.0473 (0.0508)  time: 3.7346  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1300/3449]  eta: 2:14:01  lr: 0.000080  loss: 0.0517 (0.0508)  time: 3.7573  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1310/3449]  eta: 2:13:24  lr: 0.000080  loss: 0.0517 (0.0508)  time: 3.7942  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1320/3449]  eta: 2:12:46  lr: 0.000080  loss: 0.0464 (0.0507)  time: 3.7386  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1330/3449]  eta: 2:12:09  lr: 0.000080  loss: 0.0466 (0.0507)  time: 3.7346  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1340/3449]  eta: 2:11:30  lr: 0.000080  loss: 0.0502 (0.0507)  time: 3.7092  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1350/3449]  eta: 2:10:52  lr: 0.000080  loss: 0.0479 (0.0507)  time: 3.6805  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1360/3449]  eta: 2:10:16  lr: 0.000080  loss: 0.0539 (0.0507)  time: 3.7574  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1370/3449]  eta: 2:09:39  lr: 0.000080  loss: 0.0589 (0.0507)  time: 3.7834  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1380/3449]  eta: 2:09:00  lr: 0.000080  loss: 0.0556 (0.0507)  time: 3.7026  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1390/3449]  eta: 2:08:24  lr: 0.000080  loss: 0.0526 (0.0507)  time: 3.7667  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1400/3449]  eta: 2:07:45  lr: 0.000080  loss: 0.0526 (0.0507)  time: 3.7316  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1410/3449]  eta: 2:07:07  lr: 0.000080  loss: 0.0574 (0.0508)  time: 3.6518  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1420/3449]  eta: 2:06:31  lr: 0.000080  loss: 0.0604 (0.0508)  time: 3.7721  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1430/3449]  eta: 2:05:52  lr: 0.000080  loss: 0.0540 (0.0508)  time: 3.7372  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1440/3449]  eta: 2:05:15  lr: 0.000080  loss: 0.0482 (0.0508)  time: 3.7148  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1450/3449]  eta: 2:04:37  lr: 0.000080  loss: 0.0522 (0.0508)  time: 3.7238  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1460/3449]  eta: 2:03:58  lr: 0.000080  loss: 0.0508 (0.0508)  time: 3.6585  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1470/3449]  eta: 2:03:22  lr: 0.000080  loss: 0.0445 (0.0508)  time: 3.7229  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1480/3449]  eta: 2:02:44  lr: 0.000080  loss: 0.0486 (0.0508)  time: 3.7494  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1490/3449]  eta: 2:02:06  lr: 0.000080  loss: 0.0541 (0.0508)  time: 3.6935  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1500/3449]  eta: 2:01:27  lr: 0.000080  loss: 0.0599 (0.0508)  time: 3.6605  data: 0.0004  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1510/3449]  eta: 2:00:48  lr: 0.000080  loss: 0.0635 (0.0509)  time: 3.6243  data: 0.0004  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1520/3449]  eta: 2:00:10  lr: 0.000080  loss: 0.0632 (0.0509)  time: 3.6562  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1530/3449]  eta: 1:59:31  lr: 0.000080  loss: 0.0582 (0.0510)  time: 3.6739  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1540/3449]  eta: 1:58:54  lr: 0.000080  loss: 0.0614 (0.0510)  time: 3.6976  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1550/3449]  eta: 1:58:17  lr: 0.000080  loss: 0.0618 (0.0510)  time: 3.7397  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1560/3449]  eta: 1:57:38  lr: 0.000080  loss: 0.0512 (0.0510)  time: 3.6631  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1570/3449]  eta: 1:56:59  lr: 0.000080  loss: 0.0498 (0.0510)  time: 3.6203  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1580/3449]  eta: 1:56:21  lr: 0.000080  loss: 0.0459 (0.0509)  time: 3.6515  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1590/3449]  eta: 1:55:44  lr: 0.000080  loss: 0.0503 (0.0510)  time: 3.7267  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1600/3449]  eta: 1:55:06  lr: 0.000080  loss: 0.0503 (0.0510)  time: 3.7232  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1610/3449]  eta: 1:54:29  lr: 0.000080  loss: 0.0424 (0.0510)  time: 3.7157  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1620/3449]  eta: 1:53:52  lr: 0.000080  loss: 0.0438 (0.0510)  time: 3.7721  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1630/3449]  eta: 1:53:15  lr: 0.000080  loss: 0.0485 (0.0510)  time: 3.7499  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1640/3449]  eta: 1:52:38  lr: 0.000080  loss: 0.0485 (0.0510)  time: 3.7855  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1650/3449]  eta: 1:52:01  lr: 0.000080  loss: 0.0493 (0.0510)  time: 3.7726  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1660/3449]  eta: 1:51:24  lr: 0.000080  loss: 0.0523 (0.0510)  time: 3.7463  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1670/3449]  eta: 1:50:46  lr: 0.000080  loss: 0.0530 (0.0510)  time: 3.7335  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1680/3449]  eta: 1:50:10  lr: 0.000080  loss: 0.0530 (0.0510)  time: 3.7585  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1690/3449]  eta: 1:49:33  lr: 0.000080  loss: 0.0476 (0.0510)  time: 3.8053  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1700/3449]  eta: 1:48:56  lr: 0.000080  loss: 0.0469 (0.0510)  time: 3.7661  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1710/3449]  eta: 1:48:19  lr: 0.000080  loss: 0.0481 (0.0509)  time: 3.7715  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1720/3449]  eta: 1:47:41  lr: 0.000080  loss: 0.0483 (0.0509)  time: 3.7241  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1730/3449]  eta: 1:47:04  lr: 0.000080  loss: 0.0572 (0.0509)  time: 3.7331  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1740/3449]  eta: 1:46:27  lr: 0.000080  loss: 0.0579 (0.0509)  time: 3.8032  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1750/3449]  eta: 1:45:50  lr: 0.000080  loss: 0.0579 (0.0509)  time: 3.7874  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1760/3449]  eta: 1:45:14  lr: 0.000080  loss: 0.0522 (0.0509)  time: 3.8169  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1770/3449]  eta: 1:44:36  lr: 0.000080  loss: 0.0370 (0.0508)  time: 3.7802  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1780/3449]  eta: 1:43:58  lr: 0.000080  loss: 0.0407 (0.0508)  time: 3.6686  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1790/3449]  eta: 1:43:20  lr: 0.000080  loss: 0.0508 (0.0508)  time: 3.6791  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1800/3449]  eta: 1:42:43  lr: 0.000080  loss: 0.0544 (0.0509)  time: 3.7119  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1810/3449]  eta: 1:42:05  lr: 0.000080  loss: 0.0524 (0.0508)  time: 3.6976  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1820/3449]  eta: 1:41:29  lr: 0.000080  loss: 0.0449 (0.0508)  time: 3.7851  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1830/3449]  eta: 1:40:52  lr: 0.000080  loss: 0.0569 (0.0509)  time: 3.8512  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1840/3449]  eta: 1:40:14  lr: 0.000080  loss: 0.0627 (0.0509)  time: 3.7655  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1850/3449]  eta: 1:39:37  lr: 0.000080  loss: 0.0503 (0.0509)  time: 3.7124  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1860/3449]  eta: 1:39:00  lr: 0.000080  loss: 0.0386 (0.0508)  time: 3.7444  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1870/3449]  eta: 1:38:21  lr: 0.000080  loss: 0.0400 (0.0508)  time: 3.7007  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1880/3449]  eta: 1:37:43  lr: 0.000080  loss: 0.0491 (0.0507)  time: 3.6204  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1890/3449]  eta: 1:37:06  lr: 0.000080  loss: 0.0540 (0.0508)  time: 3.6819  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1900/3449]  eta: 1:36:28  lr: 0.000080  loss: 0.0563 (0.0508)  time: 3.7107  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1910/3449]  eta: 1:35:50  lr: 0.000080  loss: 0.0573 (0.0509)  time: 3.6637  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1920/3449]  eta: 1:35:13  lr: 0.000080  loss: 0.0502 (0.0508)  time: 3.7147  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1930/3449]  eta: 1:34:34  lr: 0.000080  loss: 0.0403 (0.0508)  time: 3.6956  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1940/3449]  eta: 1:33:56  lr: 0.000080  loss: 0.0418 (0.0507)  time: 3.6245  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1950/3449]  eta: 1:33:19  lr: 0.000080  loss: 0.0418 (0.0507)  time: 3.7005  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1960/3449]  eta: 1:32:42  lr: 0.000080  loss: 0.0549 (0.0507)  time: 3.7851  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1970/3449]  eta: 1:32:05  lr: 0.000080  loss: 0.0510 (0.0507)  time: 3.7845  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1980/3449]  eta: 1:31:27  lr: 0.000080  loss: 0.0491 (0.0507)  time: 3.7180  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [1990/3449]  eta: 1:30:50  lr: 0.000080  loss: 0.0523 (0.0507)  time: 3.7097  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2000/3449]  eta: 1:30:13  lr: 0.000080  loss: 0.0447 (0.0507)  time: 3.7427  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2010/3449]  eta: 1:29:35  lr: 0.000080  loss: 0.0350 (0.0506)  time: 3.6921  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2020/3449]  eta: 1:28:57  lr: 0.000080  loss: 0.0356 (0.0506)  time: 3.6723  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2030/3449]  eta: 1:28:20  lr: 0.000080  loss: 0.0530 (0.0506)  time: 3.7210  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2040/3449]  eta: 1:27:43  lr: 0.000080  loss: 0.0548 (0.0506)  time: 3.7761  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2050/3449]  eta: 1:27:06  lr: 0.000080  loss: 0.0492 (0.0506)  time: 3.7706  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2060/3449]  eta: 1:26:29  lr: 0.000080  loss: 0.0533 (0.0507)  time: 3.7552  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2070/3449]  eta: 1:25:50  lr: 0.000080  loss: 0.0589 (0.0507)  time: 3.6992  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2080/3449]  eta: 1:25:13  lr: 0.000080  loss: 0.0556 (0.0507)  time: 3.6860  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2090/3449]  eta: 1:24:36  lr: 0.000080  loss: 0.0556 (0.0507)  time: 3.7398  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2100/3449]  eta: 1:23:58  lr: 0.000080  loss: 0.0459 (0.0506)  time: 3.7214  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2110/3449]  eta: 1:23:20  lr: 0.000080  loss: 0.0458 (0.0506)  time: 3.6244  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2120/3449]  eta: 1:22:42  lr: 0.000080  loss: 0.0411 (0.0506)  time: 3.6434  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2130/3449]  eta: 1:22:05  lr: 0.000080  loss: 0.0525 (0.0506)  time: 3.7422  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2140/3449]  eta: 1:21:28  lr: 0.000080  loss: 0.0533 (0.0506)  time: 3.7283  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2150/3449]  eta: 1:20:50  lr: 0.000080  loss: 0.0569 (0.0506)  time: 3.7423  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2160/3449]  eta: 1:20:14  lr: 0.000080  loss: 0.0545 (0.0506)  time: 3.8312  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2170/3449]  eta: 1:19:36  lr: 0.000080  loss: 0.0483 (0.0506)  time: 3.7476  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2180/3449]  eta: 1:18:58  lr: 0.000080  loss: 0.0515 (0.0506)  time: 3.6542  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2190/3449]  eta: 1:18:21  lr: 0.000080  loss: 0.0550 (0.0506)  time: 3.7111  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2200/3449]  eta: 1:17:44  lr: 0.000080  loss: 0.0585 (0.0507)  time: 3.7166  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2210/3449]  eta: 1:17:06  lr: 0.000080  loss: 0.0557 (0.0506)  time: 3.7149  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2220/3449]  eta: 1:16:29  lr: 0.000080  loss: 0.0427 (0.0506)  time: 3.7302  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2230/3449]  eta: 1:15:51  lr: 0.000080  loss: 0.0532 (0.0507)  time: 3.7312  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2240/3449]  eta: 1:15:14  lr: 0.000080  loss: 0.0532 (0.0506)  time: 3.7289  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2250/3449]  eta: 1:14:36  lr: 0.000080  loss: 0.0502 (0.0507)  time: 3.6982  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2260/3449]  eta: 1:13:59  lr: 0.000080  loss: 0.0502 (0.0507)  time: 3.7122  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2270/3449]  eta: 1:13:21  lr: 0.000080  loss: 0.0430 (0.0506)  time: 3.7138  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2280/3449]  eta: 1:12:44  lr: 0.000080  loss: 0.0352 (0.0505)  time: 3.6702  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2290/3449]  eta: 1:12:07  lr: 0.000080  loss: 0.0419 (0.0505)  time: 3.7265  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2300/3449]  eta: 1:11:30  lr: 0.000080  loss: 0.0528 (0.0505)  time: 3.7855  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2310/3449]  eta: 1:10:53  lr: 0.000080  loss: 0.0429 (0.0505)  time: 3.7993  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2320/3449]  eta: 1:10:16  lr: 0.000080  loss: 0.0478 (0.0505)  time: 3.7825  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2330/3449]  eta: 1:09:38  lr: 0.000080  loss: 0.0548 (0.0505)  time: 3.7044  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2340/3449]  eta: 1:09:01  lr: 0.000080  loss: 0.0558 (0.0505)  time: 3.7408  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2350/3449]  eta: 1:08:24  lr: 0.000080  loss: 0.0557 (0.0505)  time: 3.8061  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2360/3449]  eta: 1:07:47  lr: 0.000080  loss: 0.0497 (0.0505)  time: 3.8061  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2370/3449]  eta: 1:07:10  lr: 0.000080  loss: 0.0497 (0.0505)  time: 3.8048  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2380/3449]  eta: 1:06:33  lr: 0.000080  loss: 0.0514 (0.0505)  time: 3.7943  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2390/3449]  eta: 1:05:55  lr: 0.000080  loss: 0.0494 (0.0505)  time: 3.7843  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2400/3449]  eta: 1:05:18  lr: 0.000080  loss: 0.0461 (0.0505)  time: 3.7407  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2410/3449]  eta: 1:04:41  lr: 0.000080  loss: 0.0543 (0.0505)  time: 3.7852  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2420/3449]  eta: 1:04:04  lr: 0.000080  loss: 0.0536 (0.0505)  time: 3.8240  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2430/3449]  eta: 1:03:27  lr: 0.000080  loss: 0.0536 (0.0505)  time: 3.8386  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2440/3449]  eta: 1:02:50  lr: 0.000080  loss: 0.0464 (0.0505)  time: 3.7946  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2450/3449]  eta: 1:02:13  lr: 0.000080  loss: 0.0492 (0.0505)  time: 3.7667  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2460/3449]  eta: 1:01:35  lr: 0.000080  loss: 0.0556 (0.0506)  time: 3.7784  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2470/3449]  eta: 1:00:58  lr: 0.000080  loss: 0.0583 (0.0506)  time: 3.7606  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2480/3449]  eta: 1:00:21  lr: 0.000080  loss: 0.0596 (0.0506)  time: 3.7538  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2490/3449]  eta: 0:59:44  lr: 0.000080  loss: 0.0532 (0.0506)  time: 3.8204  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2500/3449]  eta: 0:59:07  lr: 0.000080  loss: 0.0501 (0.0506)  time: 3.8419  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2510/3449]  eta: 0:58:29  lr: 0.000080  loss: 0.0525 (0.0506)  time: 3.7585  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2520/3449]  eta: 0:57:53  lr: 0.000080  loss: 0.0494 (0.0506)  time: 3.8004  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2530/3449]  eta: 0:57:15  lr: 0.000080  loss: 0.0445 (0.0506)  time: 3.8399  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2540/3449]  eta: 0:56:38  lr: 0.000080  loss: 0.0444 (0.0506)  time: 3.8182  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2550/3449]  eta: 0:56:01  lr: 0.000080  loss: 0.0462 (0.0506)  time: 3.7317  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2560/3449]  eta: 0:55:23  lr: 0.000080  loss: 0.0465 (0.0505)  time: 3.6765  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2570/3449]  eta: 0:54:46  lr: 0.000080  loss: 0.0487 (0.0506)  time: 3.7616  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2580/3449]  eta: 0:54:09  lr: 0.000080  loss: 0.0531 (0.0506)  time: 3.8433  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2590/3449]  eta: 0:53:32  lr: 0.000080  loss: 0.0482 (0.0506)  time: 3.8085  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2600/3449]  eta: 0:52:54  lr: 0.000080  loss: 0.0440 (0.0505)  time: 3.7268  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2610/3449]  eta: 0:52:17  lr: 0.000080  loss: 0.0500 (0.0505)  time: 3.7484  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2620/3449]  eta: 0:51:39  lr: 0.000080  loss: 0.0553 (0.0506)  time: 3.7292  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2630/3449]  eta: 0:51:02  lr: 0.000080  loss: 0.0504 (0.0506)  time: 3.7626  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2640/3449]  eta: 0:50:25  lr: 0.000080  loss: 0.0407 (0.0505)  time: 3.7965  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2650/3449]  eta: 0:49:48  lr: 0.000080  loss: 0.0421 (0.0505)  time: 3.7787  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2660/3449]  eta: 0:49:11  lr: 0.000080  loss: 0.0554 (0.0505)  time: 3.8029  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2670/3449]  eta: 0:48:33  lr: 0.000080  loss: 0.0601 (0.0506)  time: 3.7928  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2680/3449]  eta: 0:47:56  lr: 0.000080  loss: 0.0581 (0.0506)  time: 3.7283  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2690/3449]  eta: 0:47:18  lr: 0.000080  loss: 0.0536 (0.0506)  time: 3.6906  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2700/3449]  eta: 0:46:41  lr: 0.000080  loss: 0.0523 (0.0506)  time: 3.7217  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2710/3449]  eta: 0:46:03  lr: 0.000080  loss: 0.0519 (0.0506)  time: 3.7158  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2720/3449]  eta: 0:45:26  lr: 0.000080  loss: 0.0524 (0.0506)  time: 3.7174  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2730/3449]  eta: 0:44:48  lr: 0.000080  loss: 0.0524 (0.0506)  time: 3.7269  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2740/3449]  eta: 0:44:11  lr: 0.000080  loss: 0.0553 (0.0507)  time: 3.7642  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2750/3449]  eta: 0:43:34  lr: 0.000080  loss: 0.0539 (0.0507)  time: 3.8287  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2760/3449]  eta: 0:42:57  lr: 0.000080  loss: 0.0524 (0.0507)  time: 3.8638  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2770/3449]  eta: 0:42:19  lr: 0.000080  loss: 0.0462 (0.0507)  time: 3.7772  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2780/3449]  eta: 0:41:42  lr: 0.000080  loss: 0.0457 (0.0507)  time: 3.7290  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2790/3449]  eta: 0:41:05  lr: 0.000080  loss: 0.0424 (0.0506)  time: 3.7689  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2800/3449]  eta: 0:40:28  lr: 0.000080  loss: 0.0421 (0.0506)  time: 3.7974  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2810/3449]  eta: 0:39:50  lr: 0.000080  loss: 0.0499 (0.0506)  time: 3.8137  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2820/3449]  eta: 0:39:13  lr: 0.000080  loss: 0.0570 (0.0506)  time: 3.7667  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2830/3449]  eta: 0:38:35  lr: 0.000080  loss: 0.0571 (0.0506)  time: 3.7471  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2840/3449]  eta: 0:37:58  lr: 0.000080  loss: 0.0435 (0.0506)  time: 3.7765  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2850/3449]  eta: 0:37:21  lr: 0.000080  loss: 0.0464 (0.0506)  time: 3.7871  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2860/3449]  eta: 0:36:44  lr: 0.000080  loss: 0.0603 (0.0506)  time: 3.8192  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2870/3449]  eta: 0:36:06  lr: 0.000080  loss: 0.0501 (0.0506)  time: 3.8079  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2880/3449]  eta: 0:35:29  lr: 0.000080  loss: 0.0503 (0.0506)  time: 3.8628  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2890/3449]  eta: 0:34:52  lr: 0.000080  loss: 0.0503 (0.0506)  time: 3.8867  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2900/3449]  eta: 0:34:14  lr: 0.000080  loss: 0.0401 (0.0505)  time: 3.7591  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2910/3449]  eta: 0:33:37  lr: 0.000080  loss: 0.0479 (0.0505)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2920/3449]  eta: 0:33:00  lr: 0.000080  loss: 0.0521 (0.0505)  time: 3.8165  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2930/3449]  eta: 0:32:22  lr: 0.000080  loss: 0.0505 (0.0505)  time: 3.7408  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2940/3449]  eta: 0:31:45  lr: 0.000080  loss: 0.0575 (0.0506)  time: 3.7557  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2950/3449]  eta: 0:31:08  lr: 0.000080  loss: 0.0484 (0.0505)  time: 3.8342  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2960/3449]  eta: 0:30:30  lr: 0.000080  loss: 0.0502 (0.0505)  time: 3.7749  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2970/3449]  eta: 0:29:53  lr: 0.000080  loss: 0.0573 (0.0506)  time: 3.7821  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2980/3449]  eta: 0:29:16  lr: 0.000080  loss: 0.0548 (0.0506)  time: 3.8463  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [2990/3449]  eta: 0:28:38  lr: 0.000080  loss: 0.0591 (0.0506)  time: 3.8404  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3000/3449]  eta: 0:28:01  lr: 0.000080  loss: 0.0596 (0.0506)  time: 3.7999  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3010/3449]  eta: 0:27:24  lr: 0.000080  loss: 0.0484 (0.0506)  time: 3.7777  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3020/3449]  eta: 0:26:46  lr: 0.000080  loss: 0.0468 (0.0506)  time: 3.8062  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3030/3449]  eta: 0:26:09  lr: 0.000080  loss: 0.0468 (0.0506)  time: 3.7791  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3040/3449]  eta: 0:25:31  lr: 0.000080  loss: 0.0557 (0.0506)  time: 3.7044  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3050/3449]  eta: 0:24:54  lr: 0.000080  loss: 0.0559 (0.0506)  time: 3.7540  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3060/3449]  eta: 0:24:17  lr: 0.000080  loss: 0.0559 (0.0506)  time: 3.8218  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3070/3449]  eta: 0:23:39  lr: 0.000080  loss: 0.0484 (0.0506)  time: 3.7707  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3080/3449]  eta: 0:23:02  lr: 0.000080  loss: 0.0481 (0.0506)  time: 3.7739  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3090/3449]  eta: 0:22:24  lr: 0.000080  loss: 0.0472 (0.0506)  time: 3.8535  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3100/3449]  eta: 0:21:47  lr: 0.000080  loss: 0.0493 (0.0506)  time: 3.8722  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3110/3449]  eta: 0:21:10  lr: 0.000080  loss: 0.0600 (0.0506)  time: 3.7940  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3120/3449]  eta: 0:20:32  lr: 0.000080  loss: 0.0547 (0.0506)  time: 3.7652  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3130/3449]  eta: 0:19:55  lr: 0.000080  loss: 0.0508 (0.0506)  time: 3.7552  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3140/3449]  eta: 0:19:17  lr: 0.000080  loss: 0.0464 (0.0506)  time: 3.7536  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3150/3449]  eta: 0:18:40  lr: 0.000080  loss: 0.0401 (0.0506)  time: 3.8195  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3160/3449]  eta: 0:18:02  lr: 0.000080  loss: 0.0433 (0.0506)  time: 3.8139  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3170/3449]  eta: 0:17:25  lr: 0.000080  loss: 0.0520 (0.0506)  time: 3.7844  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3180/3449]  eta: 0:16:48  lr: 0.000080  loss: 0.0513 (0.0506)  time: 3.7750  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3190/3449]  eta: 0:16:10  lr: 0.000080  loss: 0.0566 (0.0506)  time: 3.7890  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3200/3449]  eta: 0:15:33  lr: 0.000080  loss: 0.0566 (0.0506)  time: 3.8621  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3210/3449]  eta: 0:14:55  lr: 0.000080  loss: 0.0499 (0.0506)  time: 3.8261  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3220/3449]  eta: 0:14:18  lr: 0.000080  loss: 0.0493 (0.0506)  time: 3.7968  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3230/3449]  eta: 0:13:40  lr: 0.000080  loss: 0.0582 (0.0506)  time: 3.8235  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3240/3449]  eta: 0:13:03  lr: 0.000080  loss: 0.0599 (0.0506)  time: 3.7889  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3250/3449]  eta: 0:12:26  lr: 0.000080  loss: 0.0540 (0.0506)  time: 3.8050  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3260/3449]  eta: 0:11:48  lr: 0.000080  loss: 0.0412 (0.0506)  time: 3.8358  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3270/3449]  eta: 0:11:11  lr: 0.000080  loss: 0.0457 (0.0506)  time: 3.7808  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3280/3449]  eta: 0:10:33  lr: 0.000080  loss: 0.0472 (0.0506)  time: 3.7384  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3290/3449]  eta: 0:09:56  lr: 0.000080  loss: 0.0476 (0.0506)  time: 3.7508  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3300/3449]  eta: 0:09:18  lr: 0.000080  loss: 0.0517 (0.0506)  time: 3.7290  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3310/3449]  eta: 0:08:41  lr: 0.000080  loss: 0.0605 (0.0506)  time: 3.6800  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3320/3449]  eta: 0:08:03  lr: 0.000080  loss: 0.0645 (0.0506)  time: 3.7234  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3330/3449]  eta: 0:07:26  lr: 0.000080  loss: 0.0539 (0.0506)  time: 3.8268  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3340/3449]  eta: 0:06:48  lr: 0.000080  loss: 0.0433 (0.0506)  time: 3.8153  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3350/3449]  eta: 0:06:11  lr: 0.000080  loss: 0.0563 (0.0506)  time: 3.7948  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3360/3449]  eta: 0:05:33  lr: 0.000080  loss: 0.0541 (0.0506)  time: 3.8367  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3370/3449]  eta: 0:04:56  lr: 0.000080  loss: 0.0506 (0.0506)  time: 3.8245  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3380/3449]  eta: 0:04:18  lr: 0.000080  loss: 0.0492 (0.0506)  time: 3.7601  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3390/3449]  eta: 0:03:41  lr: 0.000080  loss: 0.0470 (0.0506)  time: 3.7300  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3400/3449]  eta: 0:03:03  lr: 0.000080  loss: 0.0514 (0.0506)  time: 3.7959  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3410/3449]  eta: 0:02:26  lr: 0.000080  loss: 0.0613 (0.0506)  time: 3.8212  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3420/3449]  eta: 0:01:48  lr: 0.000080  loss: 0.0610 (0.0506)  time: 3.7420  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3430/3449]  eta: 0:01:11  lr: 0.000080  loss: 0.0624 (0.0507)  time: 3.7643  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3440/3449]  eta: 0:00:33  lr: 0.000080  loss: 0.0630 (0.0507)  time: 3.8214  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [3448/3449]  eta: 0:00:03  lr: 0.000080  loss: 0.0531 (0.0507)  time: 3.7909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:9] Total time: 3:35:35 (3.7505 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.0531 (0.0507)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:9]  [ 0/14]  eta: 0:04:22  loss: 0.0368 (0.0368)  time: 18.7310  data: 0.5136  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:9]  [13/14]  eta: 0:00:18  loss: 0.0393 (0.0407)  time: 18.2741  data: 0.0369  max mem: 34968\n",
      "Valid: [epoch:9] Total time: 0:04:15 (18.2819 s / it)\n",
      "Averaged stats: loss: 0.0393 (0.0407)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_9_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.041%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [   0/3449]  eta: 5:01:05  lr: 0.000090  loss: 0.0938 (0.0938)  time: 5.2380  data: 1.4924  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [  10/3449]  eta: 3:45:31  lr: 0.000090  loss: 0.0656 (0.0629)  time: 3.9346  data: 0.1358  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [  20/3449]  eta: 3:40:27  lr: 0.000090  loss: 0.0570 (0.0591)  time: 3.7884  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [  30/3449]  eta: 3:40:28  lr: 0.000090  loss: 0.0534 (0.0546)  time: 3.8331  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [  40/3449]  eta: 3:38:38  lr: 0.000090  loss: 0.0475 (0.0521)  time: 3.8386  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [  50/3449]  eta: 3:37:37  lr: 0.000090  loss: 0.0492 (0.0510)  time: 3.7988  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [  60/3449]  eta: 3:36:51  lr: 0.000090  loss: 0.0487 (0.0506)  time: 3.8214  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [  70/3449]  eta: 3:36:05  lr: 0.000090  loss: 0.0522 (0.0503)  time: 3.8259  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [  80/3449]  eta: 3:35:42  lr: 0.000090  loss: 0.0526 (0.0508)  time: 3.8478  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [  90/3449]  eta: 3:35:03  lr: 0.000090  loss: 0.0607 (0.0516)  time: 3.8564  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 100/3449]  eta: 3:33:44  lr: 0.000090  loss: 0.0540 (0.0511)  time: 3.7808  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 110/3449]  eta: 3:32:45  lr: 0.000090  loss: 0.0412 (0.0500)  time: 3.7403  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 120/3449]  eta: 3:31:54  lr: 0.000090  loss: 0.0464 (0.0501)  time: 3.7684  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 130/3449]  eta: 3:30:59  lr: 0.000090  loss: 0.0475 (0.0490)  time: 3.7641  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 140/3449]  eta: 3:29:54  lr: 0.000090  loss: 0.0323 (0.0489)  time: 3.7267  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 150/3449]  eta: 3:28:53  lr: 0.000090  loss: 0.0581 (0.0492)  time: 3.7014  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 160/3449]  eta: 3:28:13  lr: 0.000090  loss: 0.0502 (0.0493)  time: 3.7447  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 170/3449]  eta: 3:27:48  lr: 0.000090  loss: 0.0489 (0.0493)  time: 3.8271  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 180/3449]  eta: 3:26:46  lr: 0.000090  loss: 0.0419 (0.0486)  time: 3.7691  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 190/3449]  eta: 3:26:10  lr: 0.000090  loss: 0.0340 (0.0483)  time: 3.7392  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 200/3449]  eta: 3:25:38  lr: 0.000090  loss: 0.0412 (0.0482)  time: 3.8180  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 210/3449]  eta: 3:24:42  lr: 0.000090  loss: 0.0558 (0.0485)  time: 3.7568  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 220/3449]  eta: 3:24:19  lr: 0.000090  loss: 0.0562 (0.0487)  time: 3.7875  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 230/3449]  eta: 3:23:28  lr: 0.000090  loss: 0.0548 (0.0488)  time: 3.7993  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 240/3449]  eta: 3:22:42  lr: 0.000090  loss: 0.0466 (0.0486)  time: 3.7186  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 250/3449]  eta: 3:22:08  lr: 0.000090  loss: 0.0445 (0.0488)  time: 3.7738  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 260/3449]  eta: 3:21:27  lr: 0.000090  loss: 0.0447 (0.0488)  time: 3.7932  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 270/3449]  eta: 3:20:36  lr: 0.000090  loss: 0.0503 (0.0490)  time: 3.7254  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 280/3449]  eta: 3:19:52  lr: 0.000090  loss: 0.0527 (0.0491)  time: 3.7044  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 290/3449]  eta: 3:19:20  lr: 0.000090  loss: 0.0464 (0.0489)  time: 3.7838  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 300/3449]  eta: 3:18:42  lr: 0.000090  loss: 0.0501 (0.0493)  time: 3.8095  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 310/3449]  eta: 3:18:09  lr: 0.000090  loss: 0.0462 (0.0490)  time: 3.8105  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 320/3449]  eta: 3:17:32  lr: 0.000090  loss: 0.0462 (0.0490)  time: 3.8198  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 330/3449]  eta: 3:16:55  lr: 0.000090  loss: 0.0601 (0.0492)  time: 3.7971  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 340/3449]  eta: 3:16:09  lr: 0.000090  loss: 0.0469 (0.0491)  time: 3.7469  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 350/3449]  eta: 3:15:34  lr: 0.000090  loss: 0.0478 (0.0492)  time: 3.7577  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 360/3449]  eta: 3:14:51  lr: 0.000090  loss: 0.0546 (0.0494)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 370/3449]  eta: 3:14:28  lr: 0.000090  loss: 0.0548 (0.0495)  time: 3.8469  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 380/3449]  eta: 3:13:52  lr: 0.000090  loss: 0.0524 (0.0496)  time: 3.8876  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 390/3449]  eta: 3:13:20  lr: 0.000090  loss: 0.0523 (0.0498)  time: 3.8390  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 400/3449]  eta: 3:12:46  lr: 0.000090  loss: 0.0523 (0.0499)  time: 3.8547  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 410/3449]  eta: 3:12:08  lr: 0.000090  loss: 0.0433 (0.0497)  time: 3.8171  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 420/3449]  eta: 3:11:28  lr: 0.000090  loss: 0.0513 (0.0499)  time: 3.7797  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 430/3449]  eta: 3:10:47  lr: 0.000090  loss: 0.0544 (0.0499)  time: 3.7591  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 440/3449]  eta: 3:10:11  lr: 0.000090  loss: 0.0594 (0.0501)  time: 3.7869  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 450/3449]  eta: 3:09:35  lr: 0.000090  loss: 0.0633 (0.0502)  time: 3.8138  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 460/3449]  eta: 3:08:56  lr: 0.000090  loss: 0.0514 (0.0501)  time: 3.8009  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 470/3449]  eta: 3:08:19  lr: 0.000090  loss: 0.0514 (0.0502)  time: 3.7946  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 480/3449]  eta: 3:07:34  lr: 0.000090  loss: 0.0554 (0.0503)  time: 3.7408  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 490/3449]  eta: 3:06:58  lr: 0.000090  loss: 0.0532 (0.0502)  time: 3.7481  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 500/3449]  eta: 3:06:19  lr: 0.000090  loss: 0.0537 (0.0503)  time: 3.7956  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 510/3449]  eta: 3:05:36  lr: 0.000090  loss: 0.0496 (0.0502)  time: 3.7415  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 520/3449]  eta: 3:05:00  lr: 0.000090  loss: 0.0447 (0.0502)  time: 3.7671  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 530/3449]  eta: 3:04:24  lr: 0.000090  loss: 0.0488 (0.0502)  time: 3.8246  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 540/3449]  eta: 3:03:43  lr: 0.000090  loss: 0.0528 (0.0503)  time: 3.7726  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 550/3449]  eta: 3:03:06  lr: 0.000090  loss: 0.0537 (0.0503)  time: 3.7670  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 560/3449]  eta: 3:02:25  lr: 0.000090  loss: 0.0498 (0.0502)  time: 3.7712  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 570/3449]  eta: 3:01:46  lr: 0.000090  loss: 0.0498 (0.0503)  time: 3.7452  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 580/3449]  eta: 3:01:11  lr: 0.000090  loss: 0.0519 (0.0503)  time: 3.8081  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 590/3449]  eta: 3:00:38  lr: 0.000090  loss: 0.0477 (0.0503)  time: 3.8681  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 600/3449]  eta: 3:00:03  lr: 0.000090  loss: 0.0528 (0.0504)  time: 3.8669  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 610/3449]  eta: 2:59:23  lr: 0.000090  loss: 0.0556 (0.0504)  time: 3.8066  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 620/3449]  eta: 2:58:41  lr: 0.000090  loss: 0.0483 (0.0504)  time: 3.7313  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 630/3449]  eta: 2:58:01  lr: 0.000090  loss: 0.0475 (0.0503)  time: 3.7234  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 640/3449]  eta: 2:57:27  lr: 0.000090  loss: 0.0475 (0.0503)  time: 3.8009  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 650/3449]  eta: 2:56:52  lr: 0.000090  loss: 0.0496 (0.0503)  time: 3.8625  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 660/3449]  eta: 2:56:14  lr: 0.000090  loss: 0.0527 (0.0504)  time: 3.8253  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 670/3449]  eta: 2:55:40  lr: 0.000090  loss: 0.0557 (0.0505)  time: 3.8349  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 680/3449]  eta: 2:55:02  lr: 0.000090  loss: 0.0587 (0.0506)  time: 3.8455  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 690/3449]  eta: 2:54:27  lr: 0.000090  loss: 0.0487 (0.0505)  time: 3.8300  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 700/3449]  eta: 2:53:48  lr: 0.000090  loss: 0.0471 (0.0505)  time: 3.8104  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 710/3449]  eta: 2:53:09  lr: 0.000090  loss: 0.0536 (0.0505)  time: 3.7691  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 720/3449]  eta: 2:52:33  lr: 0.000090  loss: 0.0472 (0.0504)  time: 3.8027  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 730/3449]  eta: 2:51:55  lr: 0.000090  loss: 0.0429 (0.0504)  time: 3.8126  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 740/3449]  eta: 2:51:14  lr: 0.000090  loss: 0.0459 (0.0504)  time: 3.7497  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 750/3449]  eta: 2:50:37  lr: 0.000090  loss: 0.0550 (0.0505)  time: 3.7655  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 760/3449]  eta: 2:49:57  lr: 0.000090  loss: 0.0549 (0.0505)  time: 3.7884  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 770/3449]  eta: 2:49:19  lr: 0.000090  loss: 0.0493 (0.0504)  time: 3.7623  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 780/3449]  eta: 2:48:38  lr: 0.000090  loss: 0.0493 (0.0504)  time: 3.7429  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 790/3449]  eta: 2:48:01  lr: 0.000090  loss: 0.0544 (0.0505)  time: 3.7724  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 800/3449]  eta: 2:47:22  lr: 0.000090  loss: 0.0523 (0.0504)  time: 3.7923  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 810/3449]  eta: 2:46:42  lr: 0.000090  loss: 0.0523 (0.0504)  time: 3.7350  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 820/3449]  eta: 2:46:07  lr: 0.000090  loss: 0.0532 (0.0505)  time: 3.8046  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 830/3449]  eta: 2:45:31  lr: 0.000090  loss: 0.0609 (0.0506)  time: 3.8612  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 840/3449]  eta: 2:44:55  lr: 0.000090  loss: 0.0510 (0.0505)  time: 3.8403  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 850/3449]  eta: 2:44:19  lr: 0.000090  loss: 0.0556 (0.0507)  time: 3.8562  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 860/3449]  eta: 2:43:41  lr: 0.000090  loss: 0.0556 (0.0507)  time: 3.8273  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 870/3449]  eta: 2:43:02  lr: 0.000090  loss: 0.0432 (0.0506)  time: 3.7718  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 880/3449]  eta: 2:42:23  lr: 0.000090  loss: 0.0497 (0.0506)  time: 3.7581  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 890/3449]  eta: 2:41:45  lr: 0.000090  loss: 0.0441 (0.0505)  time: 3.7815  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 900/3449]  eta: 2:41:07  lr: 0.000090  loss: 0.0578 (0.0506)  time: 3.7850  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 910/3449]  eta: 2:40:30  lr: 0.000090  loss: 0.0607 (0.0507)  time: 3.8069  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 920/3449]  eta: 2:39:54  lr: 0.000090  loss: 0.0578 (0.0507)  time: 3.8585  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 930/3449]  eta: 2:39:18  lr: 0.000090  loss: 0.0461 (0.0506)  time: 3.8610  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 940/3449]  eta: 2:38:40  lr: 0.000090  loss: 0.0427 (0.0505)  time: 3.8149  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 950/3449]  eta: 2:38:00  lr: 0.000090  loss: 0.0530 (0.0506)  time: 3.7603  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 960/3449]  eta: 2:37:27  lr: 0.000090  loss: 0.0665 (0.0507)  time: 3.8549  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 970/3449]  eta: 2:36:46  lr: 0.000090  loss: 0.0578 (0.0507)  time: 3.8324  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 980/3449]  eta: 2:36:08  lr: 0.000090  loss: 0.0479 (0.0508)  time: 3.7444  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 990/3449]  eta: 2:35:30  lr: 0.000090  loss: 0.0521 (0.0507)  time: 3.7872  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1000/3449]  eta: 2:34:53  lr: 0.000090  loss: 0.0489 (0.0507)  time: 3.8077  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1010/3449]  eta: 2:34:15  lr: 0.000090  loss: 0.0482 (0.0507)  time: 3.8127  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1020/3449]  eta: 2:33:38  lr: 0.000090  loss: 0.0475 (0.0506)  time: 3.8196  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1030/3449]  eta: 2:33:01  lr: 0.000090  loss: 0.0505 (0.0506)  time: 3.8400  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1040/3449]  eta: 2:32:20  lr: 0.000090  loss: 0.0463 (0.0505)  time: 3.7351  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1050/3449]  eta: 2:31:43  lr: 0.000090  loss: 0.0502 (0.0506)  time: 3.7568  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1060/3449]  eta: 2:31:07  lr: 0.000090  loss: 0.0559 (0.0506)  time: 3.8657  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1070/3449]  eta: 2:30:30  lr: 0.000090  loss: 0.0598 (0.0507)  time: 3.8493  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1080/3449]  eta: 2:29:49  lr: 0.000090  loss: 0.0535 (0.0507)  time: 3.7449  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1090/3449]  eta: 2:29:11  lr: 0.000090  loss: 0.0541 (0.0507)  time: 3.7216  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1100/3449]  eta: 2:28:31  lr: 0.000090  loss: 0.0608 (0.0508)  time: 3.7511  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1110/3449]  eta: 2:27:55  lr: 0.000090  loss: 0.0535 (0.0507)  time: 3.7906  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1120/3449]  eta: 2:27:18  lr: 0.000090  loss: 0.0400 (0.0507)  time: 3.8545  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1130/3449]  eta: 2:26:38  lr: 0.000090  loss: 0.0449 (0.0507)  time: 3.7754  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1140/3449]  eta: 2:25:59  lr: 0.000090  loss: 0.0552 (0.0507)  time: 3.7294  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1150/3449]  eta: 2:25:23  lr: 0.000090  loss: 0.0533 (0.0507)  time: 3.8129  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1160/3449]  eta: 2:24:48  lr: 0.000090  loss: 0.0522 (0.0507)  time: 3.9063  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1170/3449]  eta: 2:24:08  lr: 0.000090  loss: 0.0483 (0.0506)  time: 3.8257  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1180/3449]  eta: 2:23:29  lr: 0.000090  loss: 0.0488 (0.0506)  time: 3.7176  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1190/3449]  eta: 2:22:51  lr: 0.000090  loss: 0.0538 (0.0507)  time: 3.7689  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1200/3449]  eta: 2:22:12  lr: 0.000090  loss: 0.0435 (0.0506)  time: 3.7786  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1210/3449]  eta: 2:21:36  lr: 0.000090  loss: 0.0402 (0.0505)  time: 3.8048  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1220/3449]  eta: 2:21:00  lr: 0.000090  loss: 0.0431 (0.0505)  time: 3.8845  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1230/3449]  eta: 2:20:20  lr: 0.000090  loss: 0.0541 (0.0505)  time: 3.7828  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1240/3449]  eta: 2:19:42  lr: 0.000090  loss: 0.0605 (0.0506)  time: 3.7326  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1250/3449]  eta: 2:19:04  lr: 0.000090  loss: 0.0598 (0.0507)  time: 3.8009  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1260/3449]  eta: 2:18:27  lr: 0.000090  loss: 0.0577 (0.0508)  time: 3.8419  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1270/3449]  eta: 2:17:51  lr: 0.000090  loss: 0.0573 (0.0508)  time: 3.8696  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1280/3449]  eta: 2:17:13  lr: 0.000090  loss: 0.0466 (0.0507)  time: 3.8464  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1290/3449]  eta: 2:16:34  lr: 0.000090  loss: 0.0466 (0.0506)  time: 3.7769  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1300/3449]  eta: 2:15:57  lr: 0.000090  loss: 0.0513 (0.0507)  time: 3.7711  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1310/3449]  eta: 2:15:18  lr: 0.000090  loss: 0.0533 (0.0507)  time: 3.7925  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1320/3449]  eta: 2:14:41  lr: 0.000090  loss: 0.0400 (0.0505)  time: 3.8187  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1330/3449]  eta: 2:14:03  lr: 0.000090  loss: 0.0399 (0.0505)  time: 3.8255  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1340/3449]  eta: 2:13:24  lr: 0.000090  loss: 0.0495 (0.0505)  time: 3.7656  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1350/3449]  eta: 2:12:46  lr: 0.000090  loss: 0.0495 (0.0505)  time: 3.7548  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1360/3449]  eta: 2:12:10  lr: 0.000090  loss: 0.0457 (0.0505)  time: 3.8265  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1370/3449]  eta: 2:11:31  lr: 0.000090  loss: 0.0483 (0.0505)  time: 3.8338  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1380/3449]  eta: 2:10:54  lr: 0.000090  loss: 0.0511 (0.0505)  time: 3.7944  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1390/3449]  eta: 2:10:16  lr: 0.000090  loss: 0.0565 (0.0506)  time: 3.8226  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1400/3449]  eta: 2:09:37  lr: 0.000090  loss: 0.0588 (0.0506)  time: 3.7658  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1410/3449]  eta: 2:08:59  lr: 0.000090  loss: 0.0576 (0.0507)  time: 3.7494  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1420/3449]  eta: 2:08:19  lr: 0.000090  loss: 0.0576 (0.0508)  time: 3.7390  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1430/3449]  eta: 2:07:42  lr: 0.000090  loss: 0.0556 (0.0508)  time: 3.7540  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1440/3449]  eta: 2:07:03  lr: 0.000090  loss: 0.0489 (0.0507)  time: 3.7906  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1450/3449]  eta: 2:06:25  lr: 0.000090  loss: 0.0472 (0.0507)  time: 3.7835  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1460/3449]  eta: 2:05:47  lr: 0.000090  loss: 0.0472 (0.0507)  time: 3.7763  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1470/3449]  eta: 2:05:09  lr: 0.000090  loss: 0.0549 (0.0507)  time: 3.7884  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1480/3449]  eta: 2:04:32  lr: 0.000090  loss: 0.0525 (0.0507)  time: 3.8355  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1490/3449]  eta: 2:03:54  lr: 0.000090  loss: 0.0407 (0.0506)  time: 3.7962  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1500/3449]  eta: 2:03:14  lr: 0.000090  loss: 0.0407 (0.0506)  time: 3.7159  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1510/3449]  eta: 2:02:36  lr: 0.000090  loss: 0.0619 (0.0507)  time: 3.7249  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1520/3449]  eta: 2:01:57  lr: 0.000090  loss: 0.0667 (0.0508)  time: 3.7575  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1530/3449]  eta: 2:01:19  lr: 0.000090  loss: 0.0539 (0.0507)  time: 3.7715  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1540/3449]  eta: 2:00:42  lr: 0.000090  loss: 0.0508 (0.0507)  time: 3.8251  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1550/3449]  eta: 2:00:05  lr: 0.000090  loss: 0.0557 (0.0507)  time: 3.8357  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1560/3449]  eta: 1:59:27  lr: 0.000090  loss: 0.0542 (0.0507)  time: 3.8291  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1570/3449]  eta: 1:58:49  lr: 0.000090  loss: 0.0412 (0.0507)  time: 3.8146  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1580/3449]  eta: 1:58:11  lr: 0.000090  loss: 0.0459 (0.0507)  time: 3.7690  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1590/3449]  eta: 1:57:32  lr: 0.000090  loss: 0.0459 (0.0507)  time: 3.7216  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1600/3449]  eta: 1:56:55  lr: 0.000090  loss: 0.0494 (0.0507)  time: 3.8002  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1610/3449]  eta: 1:56:18  lr: 0.000090  loss: 0.0633 (0.0508)  time: 3.8972  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1620/3449]  eta: 1:55:40  lr: 0.000090  loss: 0.0609 (0.0508)  time: 3.8341  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1630/3449]  eta: 1:55:00  lr: 0.000090  loss: 0.0588 (0.0509)  time: 3.6872  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1640/3449]  eta: 1:54:23  lr: 0.000090  loss: 0.0548 (0.0509)  time: 3.7278  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1650/3449]  eta: 1:53:45  lr: 0.000090  loss: 0.0504 (0.0509)  time: 3.8424  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1660/3449]  eta: 1:53:07  lr: 0.000090  loss: 0.0466 (0.0509)  time: 3.8000  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1670/3449]  eta: 1:52:29  lr: 0.000090  loss: 0.0491 (0.0508)  time: 3.7755  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1680/3449]  eta: 1:51:51  lr: 0.000090  loss: 0.0492 (0.0509)  time: 3.8206  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1690/3449]  eta: 1:51:14  lr: 0.000090  loss: 0.0492 (0.0508)  time: 3.8519  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1700/3449]  eta: 1:50:36  lr: 0.000090  loss: 0.0466 (0.0508)  time: 3.8259  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1710/3449]  eta: 1:49:58  lr: 0.000090  loss: 0.0420 (0.0508)  time: 3.7778  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1720/3449]  eta: 1:49:21  lr: 0.000090  loss: 0.0420 (0.0508)  time: 3.8442  data: 0.0004  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1730/3449]  eta: 1:48:43  lr: 0.000090  loss: 0.0564 (0.0508)  time: 3.8352  data: 0.0004  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1740/3449]  eta: 1:48:05  lr: 0.000090  loss: 0.0535 (0.0508)  time: 3.7561  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1750/3449]  eta: 1:47:28  lr: 0.000090  loss: 0.0589 (0.0508)  time: 3.8304  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1760/3449]  eta: 1:46:50  lr: 0.000090  loss: 0.0567 (0.0508)  time: 3.8671  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1770/3449]  eta: 1:46:13  lr: 0.000090  loss: 0.0473 (0.0508)  time: 3.8782  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1780/3449]  eta: 1:45:36  lr: 0.000090  loss: 0.0514 (0.0508)  time: 3.8666  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1790/3449]  eta: 1:44:58  lr: 0.000090  loss: 0.0557 (0.0509)  time: 3.8499  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1800/3449]  eta: 1:44:20  lr: 0.000090  loss: 0.0537 (0.0509)  time: 3.8188  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1810/3449]  eta: 1:43:43  lr: 0.000090  loss: 0.0528 (0.0508)  time: 3.8203  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1820/3449]  eta: 1:43:05  lr: 0.000090  loss: 0.0503 (0.0508)  time: 3.8584  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1830/3449]  eta: 1:42:27  lr: 0.000090  loss: 0.0551 (0.0508)  time: 3.8335  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1840/3449]  eta: 1:41:49  lr: 0.000090  loss: 0.0547 (0.0508)  time: 3.7643  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1850/3449]  eta: 1:41:09  lr: 0.000090  loss: 0.0534 (0.0509)  time: 3.6426  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1860/3449]  eta: 1:40:30  lr: 0.000090  loss: 0.0526 (0.0509)  time: 3.6609  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1870/3449]  eta: 1:39:52  lr: 0.000090  loss: 0.0485 (0.0508)  time: 3.7245  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1880/3449]  eta: 1:39:14  lr: 0.000090  loss: 0.0504 (0.0508)  time: 3.7751  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1890/3449]  eta: 1:38:36  lr: 0.000090  loss: 0.0537 (0.0508)  time: 3.7785  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1900/3449]  eta: 1:37:58  lr: 0.000090  loss: 0.0524 (0.0509)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1910/3449]  eta: 1:37:20  lr: 0.000090  loss: 0.0591 (0.0509)  time: 3.8130  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1920/3449]  eta: 1:36:42  lr: 0.000090  loss: 0.0486 (0.0509)  time: 3.7775  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1930/3449]  eta: 1:36:03  lr: 0.000090  loss: 0.0467 (0.0509)  time: 3.7381  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1940/3449]  eta: 1:35:25  lr: 0.000090  loss: 0.0438 (0.0508)  time: 3.7502  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1950/3449]  eta: 1:34:48  lr: 0.000090  loss: 0.0438 (0.0508)  time: 3.8372  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1960/3449]  eta: 1:34:10  lr: 0.000090  loss: 0.0528 (0.0508)  time: 3.8467  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1970/3449]  eta: 1:33:33  lr: 0.000090  loss: 0.0537 (0.0509)  time: 3.8376  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1980/3449]  eta: 1:32:54  lr: 0.000090  loss: 0.0552 (0.0509)  time: 3.8132  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [1990/3449]  eta: 1:32:16  lr: 0.000090  loss: 0.0422 (0.0508)  time: 3.7528  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2000/3449]  eta: 1:31:39  lr: 0.000090  loss: 0.0358 (0.0508)  time: 3.8033  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2010/3449]  eta: 1:31:01  lr: 0.000090  loss: 0.0496 (0.0508)  time: 3.8132  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2020/3449]  eta: 1:30:22  lr: 0.000090  loss: 0.0452 (0.0507)  time: 3.7564  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2030/3449]  eta: 1:29:44  lr: 0.000090  loss: 0.0475 (0.0507)  time: 3.7412  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2040/3449]  eta: 1:29:05  lr: 0.000090  loss: 0.0589 (0.0508)  time: 3.7331  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2050/3449]  eta: 1:28:27  lr: 0.000090  loss: 0.0508 (0.0507)  time: 3.7037  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2060/3449]  eta: 1:27:48  lr: 0.000090  loss: 0.0432 (0.0507)  time: 3.6774  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2070/3449]  eta: 1:27:10  lr: 0.000090  loss: 0.0543 (0.0507)  time: 3.7642  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2080/3449]  eta: 1:26:32  lr: 0.000090  loss: 0.0498 (0.0507)  time: 3.7955  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2090/3449]  eta: 1:25:54  lr: 0.000090  loss: 0.0516 (0.0507)  time: 3.7620  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2100/3449]  eta: 1:25:17  lr: 0.000090  loss: 0.0426 (0.0507)  time: 3.8258  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2110/3449]  eta: 1:24:38  lr: 0.000090  loss: 0.0426 (0.0506)  time: 3.7714  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2120/3449]  eta: 1:24:00  lr: 0.000090  loss: 0.0484 (0.0507)  time: 3.7409  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2130/3449]  eta: 1:23:22  lr: 0.000090  loss: 0.0553 (0.0506)  time: 3.7762  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2140/3449]  eta: 1:22:45  lr: 0.000090  loss: 0.0487 (0.0506)  time: 3.8460  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2150/3449]  eta: 1:22:07  lr: 0.000090  loss: 0.0540 (0.0507)  time: 3.8780  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2160/3449]  eta: 1:21:29  lr: 0.000090  loss: 0.0546 (0.0507)  time: 3.8126  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2170/3449]  eta: 1:20:51  lr: 0.000090  loss: 0.0490 (0.0506)  time: 3.8059  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2180/3449]  eta: 1:20:13  lr: 0.000090  loss: 0.0484 (0.0506)  time: 3.7346  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2190/3449]  eta: 1:19:35  lr: 0.000090  loss: 0.0492 (0.0506)  time: 3.6996  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2200/3449]  eta: 1:18:56  lr: 0.000090  loss: 0.0578 (0.0507)  time: 3.7503  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2210/3449]  eta: 1:18:19  lr: 0.000090  loss: 0.0436 (0.0506)  time: 3.7946  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2220/3449]  eta: 1:17:41  lr: 0.000090  loss: 0.0433 (0.0506)  time: 3.8140  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2230/3449]  eta: 1:17:03  lr: 0.000090  loss: 0.0538 (0.0506)  time: 3.8143  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2240/3449]  eta: 1:16:25  lr: 0.000090  loss: 0.0551 (0.0506)  time: 3.7943  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2250/3449]  eta: 1:15:47  lr: 0.000090  loss: 0.0551 (0.0507)  time: 3.7388  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2260/3449]  eta: 1:15:09  lr: 0.000090  loss: 0.0641 (0.0507)  time: 3.7740  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2270/3449]  eta: 1:14:31  lr: 0.000090  loss: 0.0470 (0.0506)  time: 3.8172  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2280/3449]  eta: 1:13:53  lr: 0.000090  loss: 0.0393 (0.0506)  time: 3.8270  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2290/3449]  eta: 1:13:16  lr: 0.000090  loss: 0.0426 (0.0506)  time: 3.8329  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2300/3449]  eta: 1:12:38  lr: 0.000090  loss: 0.0525 (0.0506)  time: 3.8188  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2310/3449]  eta: 1:12:00  lr: 0.000090  loss: 0.0478 (0.0505)  time: 3.7894  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2320/3449]  eta: 1:11:21  lr: 0.000090  loss: 0.0465 (0.0506)  time: 3.7057  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2330/3449]  eta: 1:10:43  lr: 0.000090  loss: 0.0536 (0.0506)  time: 3.7075  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2340/3449]  eta: 1:10:05  lr: 0.000090  loss: 0.0539 (0.0506)  time: 3.7285  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2350/3449]  eta: 1:09:27  lr: 0.000090  loss: 0.0556 (0.0506)  time: 3.7672  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2360/3449]  eta: 1:08:49  lr: 0.000090  loss: 0.0499 (0.0506)  time: 3.7695  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2370/3449]  eta: 1:08:11  lr: 0.000090  loss: 0.0468 (0.0506)  time: 3.7643  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2380/3449]  eta: 1:07:33  lr: 0.000090  loss: 0.0468 (0.0506)  time: 3.8153  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2390/3449]  eta: 1:06:56  lr: 0.000090  loss: 0.0416 (0.0505)  time: 3.8807  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2400/3449]  eta: 1:06:18  lr: 0.000090  loss: 0.0466 (0.0505)  time: 3.8714  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2410/3449]  eta: 1:05:40  lr: 0.000090  loss: 0.0520 (0.0505)  time: 3.8084  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2420/3449]  eta: 1:05:02  lr: 0.000090  loss: 0.0562 (0.0506)  time: 3.7619  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2430/3449]  eta: 1:04:23  lr: 0.000090  loss: 0.0545 (0.0506)  time: 3.7065  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2440/3449]  eta: 1:03:46  lr: 0.000090  loss: 0.0525 (0.0506)  time: 3.7558  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2450/3449]  eta: 1:03:08  lr: 0.000090  loss: 0.0517 (0.0506)  time: 3.8324  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2460/3449]  eta: 1:02:31  lr: 0.000090  loss: 0.0548 (0.0506)  time: 3.8979  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2470/3449]  eta: 1:01:53  lr: 0.000090  loss: 0.0548 (0.0506)  time: 3.8429  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2480/3449]  eta: 1:01:14  lr: 0.000090  loss: 0.0559 (0.0506)  time: 3.7241  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2490/3449]  eta: 1:00:36  lr: 0.000090  loss: 0.0517 (0.0506)  time: 3.7178  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2500/3449]  eta: 0:59:58  lr: 0.000090  loss: 0.0510 (0.0506)  time: 3.7439  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2510/3449]  eta: 0:59:20  lr: 0.000090  loss: 0.0542 (0.0506)  time: 3.8092  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2520/3449]  eta: 0:58:43  lr: 0.000090  loss: 0.0475 (0.0506)  time: 3.8532  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2530/3449]  eta: 0:58:05  lr: 0.000090  loss: 0.0469 (0.0506)  time: 3.8673  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2540/3449]  eta: 0:57:27  lr: 0.000090  loss: 0.0502 (0.0506)  time: 3.8472  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2550/3449]  eta: 0:56:49  lr: 0.000090  loss: 0.0499 (0.0506)  time: 3.7650  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2560/3449]  eta: 0:56:11  lr: 0.000090  loss: 0.0446 (0.0506)  time: 3.6864  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2570/3449]  eta: 0:55:32  lr: 0.000090  loss: 0.0446 (0.0505)  time: 3.6697  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2580/3449]  eta: 0:54:55  lr: 0.000090  loss: 0.0446 (0.0505)  time: 3.7777  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2590/3449]  eta: 0:54:17  lr: 0.000090  loss: 0.0501 (0.0505)  time: 3.8253  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2600/3449]  eta: 0:53:39  lr: 0.000090  loss: 0.0501 (0.0505)  time: 3.8221  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2610/3449]  eta: 0:53:01  lr: 0.000090  loss: 0.0493 (0.0506)  time: 3.8108  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2620/3449]  eta: 0:52:23  lr: 0.000090  loss: 0.0529 (0.0506)  time: 3.7628  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2630/3449]  eta: 0:51:45  lr: 0.000090  loss: 0.0458 (0.0505)  time: 3.7682  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2640/3449]  eta: 0:51:07  lr: 0.000090  loss: 0.0456 (0.0505)  time: 3.8025  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2650/3449]  eta: 0:50:29  lr: 0.000090  loss: 0.0454 (0.0505)  time: 3.8087  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2660/3449]  eta: 0:49:51  lr: 0.000090  loss: 0.0485 (0.0505)  time: 3.8049  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2670/3449]  eta: 0:49:14  lr: 0.000090  loss: 0.0542 (0.0505)  time: 3.8943  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2680/3449]  eta: 0:48:36  lr: 0.000090  loss: 0.0543 (0.0506)  time: 3.8127  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2690/3449]  eta: 0:47:58  lr: 0.000090  loss: 0.0572 (0.0506)  time: 3.7660  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2700/3449]  eta: 0:47:20  lr: 0.000090  loss: 0.0581 (0.0506)  time: 3.7867  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2710/3449]  eta: 0:46:42  lr: 0.000090  loss: 0.0561 (0.0506)  time: 3.7443  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2720/3449]  eta: 0:46:04  lr: 0.000090  loss: 0.0553 (0.0506)  time: 3.7334  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2730/3449]  eta: 0:45:26  lr: 0.000090  loss: 0.0502 (0.0506)  time: 3.7715  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2740/3449]  eta: 0:44:48  lr: 0.000090  loss: 0.0479 (0.0506)  time: 3.8426  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2750/3449]  eta: 0:44:10  lr: 0.000090  loss: 0.0503 (0.0506)  time: 3.7921  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2760/3449]  eta: 0:43:32  lr: 0.000090  loss: 0.0497 (0.0506)  time: 3.7614  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2770/3449]  eta: 0:42:54  lr: 0.000090  loss: 0.0591 (0.0506)  time: 3.7333  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2780/3449]  eta: 0:42:16  lr: 0.000090  loss: 0.0489 (0.0506)  time: 3.7467  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2790/3449]  eta: 0:41:38  lr: 0.000090  loss: 0.0425 (0.0506)  time: 3.8136  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2800/3449]  eta: 0:41:01  lr: 0.000090  loss: 0.0441 (0.0506)  time: 3.8686  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2810/3449]  eta: 0:40:23  lr: 0.000090  loss: 0.0455 (0.0506)  time: 3.8603  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2820/3449]  eta: 0:39:45  lr: 0.000090  loss: 0.0521 (0.0506)  time: 3.8071  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2830/3449]  eta: 0:39:07  lr: 0.000090  loss: 0.0650 (0.0506)  time: 3.7382  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2840/3449]  eta: 0:38:29  lr: 0.000090  loss: 0.0481 (0.0506)  time: 3.7290  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2850/3449]  eta: 0:37:51  lr: 0.000090  loss: 0.0464 (0.0506)  time: 3.7833  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2860/3449]  eta: 0:37:13  lr: 0.000090  loss: 0.0520 (0.0506)  time: 3.7644  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2870/3449]  eta: 0:36:35  lr: 0.000090  loss: 0.0463 (0.0506)  time: 3.7410  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2880/3449]  eta: 0:35:57  lr: 0.000090  loss: 0.0517 (0.0506)  time: 3.8090  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2890/3449]  eta: 0:35:19  lr: 0.000090  loss: 0.0522 (0.0506)  time: 3.8296  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2900/3449]  eta: 0:34:41  lr: 0.000090  loss: 0.0449 (0.0506)  time: 3.8047  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2910/3449]  eta: 0:34:03  lr: 0.000090  loss: 0.0466 (0.0506)  time: 3.8050  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2920/3449]  eta: 0:33:26  lr: 0.000090  loss: 0.0521 (0.0506)  time: 3.8508  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2930/3449]  eta: 0:32:48  lr: 0.000090  loss: 0.0518 (0.0506)  time: 3.8805  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2940/3449]  eta: 0:32:10  lr: 0.000090  loss: 0.0524 (0.0506)  time: 3.7846  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2950/3449]  eta: 0:31:32  lr: 0.000090  loss: 0.0524 (0.0506)  time: 3.7802  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2960/3449]  eta: 0:30:54  lr: 0.000090  loss: 0.0456 (0.0506)  time: 3.8258  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2970/3449]  eta: 0:30:16  lr: 0.000090  loss: 0.0503 (0.0506)  time: 3.7679  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2980/3449]  eta: 0:29:38  lr: 0.000090  loss: 0.0576 (0.0506)  time: 3.7883  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [2990/3449]  eta: 0:29:00  lr: 0.000090  loss: 0.0502 (0.0506)  time: 3.8187  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3000/3449]  eta: 0:28:22  lr: 0.000090  loss: 0.0492 (0.0506)  time: 3.8019  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3010/3449]  eta: 0:27:44  lr: 0.000090  loss: 0.0401 (0.0506)  time: 3.8172  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3020/3449]  eta: 0:27:06  lr: 0.000090  loss: 0.0409 (0.0506)  time: 3.7459  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3030/3449]  eta: 0:26:28  lr: 0.000090  loss: 0.0464 (0.0506)  time: 3.7416  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3040/3449]  eta: 0:25:50  lr: 0.000090  loss: 0.0550 (0.0506)  time: 3.7560  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3050/3449]  eta: 0:25:12  lr: 0.000090  loss: 0.0523 (0.0506)  time: 3.7134  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3060/3449]  eta: 0:24:34  lr: 0.000090  loss: 0.0532 (0.0506)  time: 3.7233  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3070/3449]  eta: 0:23:56  lr: 0.000090  loss: 0.0630 (0.0506)  time: 3.7642  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3080/3449]  eta: 0:23:19  lr: 0.000090  loss: 0.0598 (0.0507)  time: 3.8303  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3090/3449]  eta: 0:22:41  lr: 0.000090  loss: 0.0459 (0.0506)  time: 3.8507  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3100/3449]  eta: 0:22:03  lr: 0.000090  loss: 0.0447 (0.0506)  time: 3.8086  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3110/3449]  eta: 0:21:25  lr: 0.000090  loss: 0.0637 (0.0507)  time: 3.7200  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3120/3449]  eta: 0:20:47  lr: 0.000090  loss: 0.0585 (0.0507)  time: 3.7835  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3130/3449]  eta: 0:20:09  lr: 0.000090  loss: 0.0506 (0.0507)  time: 3.7789  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3140/3449]  eta: 0:19:31  lr: 0.000090  loss: 0.0506 (0.0507)  time: 3.8549  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3150/3449]  eta: 0:18:53  lr: 0.000090  loss: 0.0468 (0.0506)  time: 3.9068  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3160/3449]  eta: 0:18:15  lr: 0.000090  loss: 0.0430 (0.0506)  time: 3.7978  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3170/3449]  eta: 0:17:38  lr: 0.000090  loss: 0.0583 (0.0506)  time: 3.8594  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3180/3449]  eta: 0:17:00  lr: 0.000090  loss: 0.0599 (0.0507)  time: 3.8457  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3190/3449]  eta: 0:16:22  lr: 0.000090  loss: 0.0531 (0.0507)  time: 3.8140  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3200/3449]  eta: 0:15:44  lr: 0.000090  loss: 0.0463 (0.0506)  time: 3.8307  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3210/3449]  eta: 0:15:06  lr: 0.000090  loss: 0.0463 (0.0506)  time: 3.8230  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3220/3449]  eta: 0:14:28  lr: 0.000090  loss: 0.0474 (0.0506)  time: 3.7741  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3230/3449]  eta: 0:13:50  lr: 0.000090  loss: 0.0474 (0.0506)  time: 3.7524  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3240/3449]  eta: 0:13:12  lr: 0.000090  loss: 0.0559 (0.0506)  time: 3.8196  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3250/3449]  eta: 0:12:34  lr: 0.000090  loss: 0.0562 (0.0507)  time: 3.7965  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3260/3449]  eta: 0:11:56  lr: 0.000090  loss: 0.0430 (0.0506)  time: 3.7505  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3270/3449]  eta: 0:11:18  lr: 0.000090  loss: 0.0390 (0.0506)  time: 3.7900  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3280/3449]  eta: 0:10:40  lr: 0.000090  loss: 0.0441 (0.0506)  time: 3.7488  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3290/3449]  eta: 0:10:03  lr: 0.000090  loss: 0.0397 (0.0505)  time: 3.7737  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3300/3449]  eta: 0:09:25  lr: 0.000090  loss: 0.0463 (0.0506)  time: 3.8049  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3310/3449]  eta: 0:08:47  lr: 0.000090  loss: 0.0570 (0.0506)  time: 3.8372  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3320/3449]  eta: 0:08:09  lr: 0.000090  loss: 0.0614 (0.0506)  time: 3.8332  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3330/3449]  eta: 0:07:31  lr: 0.000090  loss: 0.0551 (0.0506)  time: 3.7880  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3340/3449]  eta: 0:06:53  lr: 0.000090  loss: 0.0524 (0.0506)  time: 3.8270  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3350/3449]  eta: 0:06:15  lr: 0.000090  loss: 0.0554 (0.0506)  time: 3.8407  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3360/3449]  eta: 0:05:37  lr: 0.000090  loss: 0.0516 (0.0506)  time: 3.8506  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3370/3449]  eta: 0:04:59  lr: 0.000090  loss: 0.0526 (0.0506)  time: 3.8369  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3380/3449]  eta: 0:04:21  lr: 0.000090  loss: 0.0534 (0.0506)  time: 3.8369  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3390/3449]  eta: 0:03:43  lr: 0.000090  loss: 0.0510 (0.0506)  time: 3.9018  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3400/3449]  eta: 0:03:05  lr: 0.000090  loss: 0.0544 (0.0506)  time: 3.8082  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3410/3449]  eta: 0:02:27  lr: 0.000090  loss: 0.0548 (0.0506)  time: 3.7568  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3420/3449]  eta: 0:01:50  lr: 0.000090  loss: 0.0547 (0.0506)  time: 3.8224  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3430/3449]  eta: 0:01:12  lr: 0.000090  loss: 0.0547 (0.0506)  time: 3.7862  data: 0.0002  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3440/3449]  eta: 0:00:34  lr: 0.000090  loss: 0.0549 (0.0506)  time: 3.7641  data: 0.0001  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [3448/3449]  eta: 0:00:03  lr: 0.000090  loss: 0.0549 (0.0506)  time: 3.7379  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:10] Total time: 3:38:03 (3.7933 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.0549 (0.0506)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:10]  [ 0/14]  eta: 0:04:22  loss: 0.0306 (0.0306)  time: 18.7191  data: 0.4747  max mem: 34968\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:10]  [13/14]  eta: 0:00:18  loss: 0.0393 (0.0407)  time: 18.2847  data: 0.0340  max mem: 34968\n",
      "Valid: [epoch:10] Total time: 0:04:16 (18.2935 s / it)\n",
      "Averaged stats: loss: 0.0393 (0.0407)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_10_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.041%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "Train: [epoch:11]  [   0/3449]  eta: 4:36:59  lr: 0.000100  loss: 0.7058 (0.7058)  time: 4.8187  data: 1.3839  max mem: 34968\n",
      "Train: [epoch:11]  [  10/3449]  eta: 3:37:31  lr: 0.000100  loss: 1.8336 (1.9516)  time: 3.7951  data: 0.1259  max mem: 34968\n",
      "Train: [epoch:11]  [  20/3449]  eta: 3:36:01  lr: 0.000100  loss: 2.0879 (1.9489)  time: 3.7280  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [  30/3449]  eta: 3:37:28  lr: 0.000100  loss: 1.9543 (1.9145)  time: 3.8280  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [  40/3449]  eta: 3:36:04  lr: 0.000100  loss: 1.7521 (1.8394)  time: 3.8272  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [  50/3449]  eta: 3:36:02  lr: 0.000100  loss: 1.6568 (1.8052)  time: 3.8096  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [  60/3449]  eta: 3:34:56  lr: 0.000100  loss: 1.6942 (1.8197)  time: 3.8103  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [  70/3449]  eta: 3:33:55  lr: 0.000100  loss: 1.8158 (1.8264)  time: 3.7597  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [  80/3449]  eta: 3:32:52  lr: 0.000100  loss: 1.8969 (1.8013)  time: 3.7473  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [  90/3449]  eta: 3:31:50  lr: 0.000100  loss: 1.9698 (1.8203)  time: 3.7321  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 100/3449]  eta: 3:30:39  lr: 0.000100  loss: 1.9698 (1.8439)  time: 3.7058  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 110/3449]  eta: 3:29:55  lr: 0.000100  loss: 1.6873 (1.8116)  time: 3.7188  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 120/3449]  eta: 3:29:19  lr: 0.000100  loss: 1.6897 (1.8093)  time: 3.7661  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 130/3449]  eta: 3:29:03  lr: 0.000100  loss: 1.6581 (1.7790)  time: 3.8192  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 140/3449]  eta: 3:28:25  lr: 0.000100  loss: 1.2624 (1.7598)  time: 3.8188  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 150/3449]  eta: 3:27:37  lr: 0.000100  loss: 1.9620 (1.7866)  time: 3.7546  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 160/3449]  eta: 3:27:02  lr: 0.000100  loss: 1.8519 (1.7786)  time: 3.7591  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 170/3449]  eta: 3:26:37  lr: 0.000100  loss: 1.5111 (1.7678)  time: 3.8181  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 180/3449]  eta: 3:25:38  lr: 0.000100  loss: 1.6540 (1.7493)  time: 3.7545  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 190/3449]  eta: 3:24:57  lr: 0.000100  loss: 1.4228 (1.7299)  time: 3.7078  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 200/3449]  eta: 3:24:17  lr: 0.000100  loss: 1.6774 (1.7451)  time: 3.7569  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 210/3449]  eta: 3:23:57  lr: 0.000100  loss: 1.9823 (1.7601)  time: 3.8255  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 220/3449]  eta: 3:23:02  lr: 0.000100  loss: 2.1869 (1.7733)  time: 3.7749  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 230/3449]  eta: 3:22:28  lr: 0.000100  loss: 2.1812 (1.7837)  time: 3.7304  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 240/3449]  eta: 3:21:54  lr: 0.000100  loss: 1.8935 (1.7815)  time: 3.8019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 250/3449]  eta: 3:21:33  lr: 0.000100  loss: 2.0069 (1.7949)  time: 3.8531  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 260/3449]  eta: 3:20:52  lr: 0.000100  loss: 2.0617 (1.8022)  time: 3.8286  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 270/3449]  eta: 3:20:03  lr: 0.000100  loss: 2.0617 (1.8140)  time: 3.7172  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 280/3449]  eta: 3:19:26  lr: 0.000100  loss: 2.0123 (1.8069)  time: 3.7353  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 290/3449]  eta: 3:18:46  lr: 0.000100  loss: 1.7128 (1.8027)  time: 3.7725  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 300/3449]  eta: 3:18:03  lr: 0.000100  loss: 1.7752 (1.8044)  time: 3.7406  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 310/3449]  eta: 3:17:25  lr: 0.000100  loss: 1.7184 (1.7968)  time: 3.7487  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 320/3449]  eta: 3:16:40  lr: 0.000100  loss: 1.7535 (1.7965)  time: 3.7316  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 330/3449]  eta: 3:16:14  lr: 0.000100  loss: 1.9364 (1.7972)  time: 3.7958  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 340/3449]  eta: 3:15:35  lr: 0.000100  loss: 1.6872 (1.7878)  time: 3.8290  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 350/3449]  eta: 3:14:57  lr: 0.000100  loss: 1.6077 (1.7769)  time: 3.7672  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 360/3449]  eta: 3:14:13  lr: 0.000100  loss: 1.7245 (1.7884)  time: 3.7381  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 370/3449]  eta: 3:13:42  lr: 0.000100  loss: 2.1414 (1.7905)  time: 3.7745  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 380/3449]  eta: 3:13:03  lr: 0.000100  loss: 1.7880 (1.7844)  time: 3.8048  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 390/3449]  eta: 3:12:30  lr: 0.000100  loss: 1.6265 (1.7848)  time: 3.7962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 400/3449]  eta: 3:11:47  lr: 0.000100  loss: 1.6702 (1.7819)  time: 3.7756  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 410/3449]  eta: 3:11:07  lr: 0.000100  loss: 1.6799 (1.7829)  time: 3.7294  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 420/3449]  eta: 3:10:34  lr: 0.000100  loss: 1.9027 (1.7807)  time: 3.7875  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 430/3449]  eta: 3:09:48  lr: 0.000100  loss: 1.5701 (1.7786)  time: 3.7481  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 440/3449]  eta: 3:09:14  lr: 0.000100  loss: 1.7503 (1.7873)  time: 3.7448  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 450/3449]  eta: 3:08:34  lr: 0.000100  loss: 2.2333 (1.7895)  time: 3.7794  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 460/3449]  eta: 3:07:56  lr: 0.000100  loss: 1.6926 (1.7831)  time: 3.7493  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 470/3449]  eta: 3:07:13  lr: 0.000100  loss: 1.6926 (1.7869)  time: 3.7276  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 480/3449]  eta: 3:06:35  lr: 0.000100  loss: 1.7970 (1.7892)  time: 3.7309  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 490/3449]  eta: 3:05:57  lr: 0.000100  loss: 1.8224 (1.7921)  time: 3.7702  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 500/3449]  eta: 3:05:09  lr: 0.000100  loss: 1.7108 (1.7887)  time: 3.6764  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 510/3449]  eta: 3:04:28  lr: 0.000100  loss: 1.6945 (1.7929)  time: 3.6544  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [ 520/3449]  eta: 3:03:51  lr: 0.000100  loss: 1.8668 (1.7935)  time: 3.7503  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 530/3449]  eta: 3:03:15  lr: 0.000100  loss: 1.8502 (1.7933)  time: 3.7830  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 540/3449]  eta: 3:02:49  lr: 0.000100  loss: 1.9613 (1.7982)  time: 3.8853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 550/3449]  eta: 3:02:13  lr: 0.000100  loss: 1.9963 (1.8006)  time: 3.8992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 560/3449]  eta: 3:01:35  lr: 0.000100  loss: 1.9819 (1.8005)  time: 3.7869  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 570/3449]  eta: 3:00:55  lr: 0.000100  loss: 1.9819 (1.8012)  time: 3.7456  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 580/3449]  eta: 3:00:09  lr: 0.000100  loss: 1.9971 (1.8046)  time: 3.6618  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 590/3449]  eta: 2:59:33  lr: 0.000100  loss: 1.9971 (1.8068)  time: 3.6974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 600/3449]  eta: 2:58:56  lr: 0.000100  loss: 2.0077 (1.8091)  time: 3.7918  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 610/3449]  eta: 2:58:20  lr: 0.000100  loss: 2.0020 (1.8110)  time: 3.7944  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 620/3449]  eta: 2:57:41  lr: 0.000100  loss: 1.9767 (1.8110)  time: 3.7735  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 630/3449]  eta: 2:57:03  lr: 0.000100  loss: 1.8884 (1.8121)  time: 3.7518  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 640/3449]  eta: 2:56:26  lr: 0.000100  loss: 1.7307 (1.8115)  time: 3.7760  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 650/3449]  eta: 2:55:53  lr: 0.000100  loss: 2.0374 (1.8169)  time: 3.8364  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 660/3449]  eta: 2:55:12  lr: 0.000100  loss: 2.0374 (1.8158)  time: 3.7788  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 670/3449]  eta: 2:54:41  lr: 0.000100  loss: 1.9945 (1.8221)  time: 3.8048  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 680/3449]  eta: 2:54:03  lr: 0.000100  loss: 2.0367 (1.8222)  time: 3.8534  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 690/3449]  eta: 2:53:31  lr: 0.000100  loss: 1.5192 (1.8168)  time: 3.8479  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 700/3449]  eta: 2:52:55  lr: 0.000100  loss: 1.7389 (1.8187)  time: 3.8665  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 710/3449]  eta: 2:52:13  lr: 0.000100  loss: 1.9509 (1.8208)  time: 3.7409  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 720/3449]  eta: 2:51:35  lr: 0.000100  loss: 2.0100 (1.8212)  time: 3.7093  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 730/3449]  eta: 2:50:57  lr: 0.000100  loss: 1.8386 (1.8183)  time: 3.7592  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 740/3449]  eta: 2:50:15  lr: 0.000100  loss: 1.6915 (1.8154)  time: 3.7170  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 750/3449]  eta: 2:49:33  lr: 0.000100  loss: 1.8073 (1.8180)  time: 3.6565  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 760/3449]  eta: 2:48:56  lr: 0.000100  loss: 1.8137 (1.8174)  time: 3.7216  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 770/3449]  eta: 2:48:19  lr: 0.000100  loss: 2.0094 (1.8221)  time: 3.7792  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 780/3449]  eta: 2:47:40  lr: 0.000100  loss: 2.0094 (1.8195)  time: 3.7587  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 790/3449]  eta: 2:47:04  lr: 0.000100  loss: 1.8047 (1.8206)  time: 3.7851  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 800/3449]  eta: 2:46:29  lr: 0.000100  loss: 1.5919 (1.8147)  time: 3.8337  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [ 810/3449]  eta: 2:45:50  lr: 0.000100  loss: 1.5080 (1.8122)  time: 3.7891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 820/3449]  eta: 2:45:11  lr: 0.000100  loss: 1.8233 (1.8133)  time: 3.7365  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 830/3449]  eta: 2:44:32  lr: 0.000100  loss: 2.0383 (1.8175)  time: 3.7326  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 840/3449]  eta: 2:43:55  lr: 0.000100  loss: 1.9174 (1.8151)  time: 3.7633  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 850/3449]  eta: 2:43:17  lr: 0.000100  loss: 1.9330 (1.8182)  time: 3.7800  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 860/3449]  eta: 2:42:35  lr: 0.000100  loss: 2.0228 (1.8162)  time: 3.6818  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 870/3449]  eta: 2:41:57  lr: 0.000100  loss: 1.7376 (1.8165)  time: 3.6770  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 880/3449]  eta: 2:41:21  lr: 0.000100  loss: 1.6801 (1.8167)  time: 3.7972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 890/3449]  eta: 2:40:42  lr: 0.000100  loss: 2.0044 (1.8192)  time: 3.7822  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 900/3449]  eta: 2:40:07  lr: 0.000100  loss: 1.9750 (1.8190)  time: 3.7854  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 910/3449]  eta: 2:39:35  lr: 0.000100  loss: 1.9750 (1.8204)  time: 3.9095  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 920/3449]  eta: 2:38:58  lr: 0.000100  loss: 2.0719 (1.8197)  time: 3.8945  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 930/3449]  eta: 2:38:23  lr: 0.000100  loss: 1.6778 (1.8174)  time: 3.8394  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 940/3449]  eta: 2:37:46  lr: 0.000100  loss: 1.5793 (1.8133)  time: 3.8320  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 950/3449]  eta: 2:37:08  lr: 0.000100  loss: 1.5958 (1.8144)  time: 3.7814  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 960/3449]  eta: 2:36:28  lr: 0.000100  loss: 2.0533 (1.8151)  time: 3.7254  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 970/3449]  eta: 2:35:50  lr: 0.000100  loss: 2.0533 (1.8190)  time: 3.7183  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 980/3449]  eta: 2:35:09  lr: 0.000100  loss: 2.2168 (1.8199)  time: 3.6996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [ 990/3449]  eta: 2:34:29  lr: 0.000100  loss: 1.9133 (1.8196)  time: 3.6765  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1000/3449]  eta: 2:33:50  lr: 0.000100  loss: 1.8488 (1.8185)  time: 3.6995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1010/3449]  eta: 2:33:12  lr: 0.000100  loss: 1.5074 (1.8151)  time: 3.7205  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1020/3449]  eta: 2:32:34  lr: 0.000100  loss: 1.6165 (1.8162)  time: 3.7521  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1030/3449]  eta: 2:31:54  lr: 0.000100  loss: 1.9371 (1.8157)  time: 3.7113  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1040/3449]  eta: 2:31:14  lr: 0.000100  loss: 1.6732 (1.8127)  time: 3.6727  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1050/3449]  eta: 2:30:35  lr: 0.000100  loss: 1.6732 (1.8129)  time: 3.6930  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1060/3449]  eta: 2:29:58  lr: 0.000100  loss: 2.0239 (1.8150)  time: 3.7531  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1070/3449]  eta: 2:29:19  lr: 0.000100  loss: 2.0486 (1.8153)  time: 3.7512  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1080/3449]  eta: 2:28:40  lr: 0.000100  loss: 1.7417 (1.8153)  time: 3.7084  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1090/3449]  eta: 2:28:03  lr: 0.000100  loss: 1.9369 (1.8176)  time: 3.7643  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1100/3449]  eta: 2:27:25  lr: 0.000100  loss: 2.0728 (1.8180)  time: 3.7792  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1110/3449]  eta: 2:26:48  lr: 0.000100  loss: 2.0349 (1.8185)  time: 3.7748  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1120/3449]  eta: 2:26:09  lr: 0.000100  loss: 1.8944 (1.8186)  time: 3.7426  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1130/3449]  eta: 2:25:32  lr: 0.000100  loss: 1.9158 (1.8208)  time: 3.7312  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1140/3449]  eta: 2:24:55  lr: 0.000100  loss: 2.2811 (1.8258)  time: 3.7814  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1150/3449]  eta: 2:24:17  lr: 0.000100  loss: 2.1313 (1.8243)  time: 3.7926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1160/3449]  eta: 2:23:42  lr: 0.000100  loss: 1.8643 (1.8254)  time: 3.8318  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1170/3449]  eta: 2:23:04  lr: 0.000100  loss: 1.9665 (1.8268)  time: 3.8155  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [1180/3449]  eta: 2:22:24  lr: 0.000100  loss: 2.0891 (1.8275)  time: 3.7077  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1190/3449]  eta: 2:21:48  lr: 0.000100  loss: 1.9795 (1.8280)  time: 3.7355  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1200/3449]  eta: 2:21:09  lr: 0.000100  loss: 1.6431 (1.8258)  time: 3.7684  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1210/3449]  eta: 2:20:32  lr: 0.000100  loss: 1.4491 (1.8230)  time: 3.7519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1220/3449]  eta: 2:19:56  lr: 0.000100  loss: 1.4553 (1.8204)  time: 3.8221  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1230/3449]  eta: 2:19:21  lr: 0.000100  loss: 1.4969 (1.8180)  time: 3.8816  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1240/3449]  eta: 2:18:44  lr: 0.000100  loss: 1.8071 (1.8191)  time: 3.8545  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1250/3449]  eta: 2:18:07  lr: 0.000100  loss: 1.9818 (1.8190)  time: 3.8165  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1260/3449]  eta: 2:17:27  lr: 0.000100  loss: 2.1087 (1.8225)  time: 3.7279  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1270/3449]  eta: 2:16:49  lr: 0.000100  loss: 2.1534 (1.8236)  time: 3.6894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1280/3449]  eta: 2:16:13  lr: 0.000100  loss: 1.6573 (1.8218)  time: 3.8281  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1290/3449]  eta: 2:15:37  lr: 0.000100  loss: 1.5720 (1.8201)  time: 3.8856  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1300/3449]  eta: 2:14:59  lr: 0.000100  loss: 2.0710 (1.8229)  time: 3.8073  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1310/3449]  eta: 2:14:20  lr: 0.000100  loss: 2.0710 (1.8219)  time: 3.7014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1320/3449]  eta: 2:13:42  lr: 0.000100  loss: 1.7508 (1.8221)  time: 3.7029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1330/3449]  eta: 2:13:05  lr: 0.000100  loss: 1.9465 (1.8243)  time: 3.7861  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1340/3449]  eta: 2:12:27  lr: 0.000100  loss: 1.9460 (1.8235)  time: 3.7934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1350/3449]  eta: 2:11:50  lr: 0.000100  loss: 1.8582 (1.8234)  time: 3.7739  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1360/3449]  eta: 2:11:12  lr: 0.000100  loss: 2.0625 (1.8255)  time: 3.7825  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1370/3449]  eta: 2:10:38  lr: 0.000100  loss: 2.0859 (1.8278)  time: 3.8749  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1380/3449]  eta: 2:09:59  lr: 0.000100  loss: 1.9859 (1.8279)  time: 3.8375  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1390/3449]  eta: 2:09:22  lr: 0.000100  loss: 1.9320 (1.8291)  time: 3.7765  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1400/3449]  eta: 2:08:42  lr: 0.000100  loss: 1.9698 (1.8312)  time: 3.7315  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1410/3449]  eta: 2:08:04  lr: 0.000100  loss: 2.0362 (1.8317)  time: 3.6510  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1420/3449]  eta: 2:07:26  lr: 0.000100  loss: 2.0362 (1.8340)  time: 3.7467  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1430/3449]  eta: 2:06:50  lr: 0.000100  loss: 1.9536 (1.8329)  time: 3.8217  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1440/3449]  eta: 2:06:12  lr: 0.000100  loss: 1.6785 (1.8317)  time: 3.7908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1450/3449]  eta: 2:05:33  lr: 0.000100  loss: 1.9585 (1.8332)  time: 3.7235  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1460/3449]  eta: 2:04:55  lr: 0.000100  loss: 1.8371 (1.8328)  time: 3.7177  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1470/3449]  eta: 2:04:18  lr: 0.000100  loss: 1.7576 (1.8335)  time: 3.7661  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1480/3449]  eta: 2:03:39  lr: 0.000100  loss: 1.8602 (1.8329)  time: 3.7564  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1490/3449]  eta: 2:03:02  lr: 0.000100  loss: 1.8358 (1.8312)  time: 3.7441  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1500/3449]  eta: 2:02:24  lr: 0.000100  loss: 1.8572 (1.8323)  time: 3.7649  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1510/3449]  eta: 2:01:46  lr: 0.000100  loss: 2.2667 (1.8353)  time: 3.7492  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1520/3449]  eta: 2:01:08  lr: 0.000100  loss: 2.2049 (1.8364)  time: 3.7691  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1530/3449]  eta: 2:00:31  lr: 0.000100  loss: 1.8781 (1.8352)  time: 3.8037  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1540/3449]  eta: 1:59:54  lr: 0.000100  loss: 1.4899 (1.8342)  time: 3.8070  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1550/3449]  eta: 1:59:17  lr: 0.000100  loss: 1.7685 (1.8346)  time: 3.8013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1560/3449]  eta: 1:58:38  lr: 0.000100  loss: 1.9110 (1.8360)  time: 3.7522  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1570/3449]  eta: 1:58:02  lr: 0.000100  loss: 1.9633 (1.8349)  time: 3.7845  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1580/3449]  eta: 1:57:26  lr: 0.000100  loss: 1.9633 (1.8357)  time: 3.8909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1590/3449]  eta: 1:56:47  lr: 0.000100  loss: 2.0042 (1.8352)  time: 3.8159  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1600/3449]  eta: 1:56:10  lr: 0.000100  loss: 1.9255 (1.8346)  time: 3.7401  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1610/3449]  eta: 1:55:32  lr: 0.000100  loss: 1.8962 (1.8344)  time: 3.7565  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1620/3449]  eta: 1:54:54  lr: 0.000100  loss: 1.7461 (1.8339)  time: 3.7512  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1630/3449]  eta: 1:54:16  lr: 0.000100  loss: 1.7409 (1.8332)  time: 3.7493  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1640/3449]  eta: 1:53:38  lr: 0.000100  loss: 2.0154 (1.8348)  time: 3.7518  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1650/3449]  eta: 1:53:00  lr: 0.000100  loss: 2.0938 (1.8352)  time: 3.7687  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1660/3449]  eta: 1:52:23  lr: 0.000100  loss: 1.7132 (1.8336)  time: 3.7992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1670/3449]  eta: 1:51:46  lr: 0.000100  loss: 1.4604 (1.8332)  time: 3.7991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1680/3449]  eta: 1:51:10  lr: 0.000100  loss: 1.9458 (1.8351)  time: 3.8460  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1690/3449]  eta: 1:50:32  lr: 0.000100  loss: 2.0940 (1.8353)  time: 3.8310  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1700/3449]  eta: 1:49:52  lr: 0.000100  loss: 1.8209 (1.8351)  time: 3.6822  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1710/3449]  eta: 1:49:15  lr: 0.000100  loss: 1.8209 (1.8348)  time: 3.7275  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1720/3449]  eta: 1:48:39  lr: 0.000100  loss: 2.0082 (1.8367)  time: 3.8735  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1730/3449]  eta: 1:48:01  lr: 0.000100  loss: 2.0657 (1.8370)  time: 3.8223  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1740/3449]  eta: 1:47:24  lr: 0.000100  loss: 2.0554 (1.8375)  time: 3.7857  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1750/3449]  eta: 1:46:45  lr: 0.000100  loss: 2.1795 (1.8377)  time: 3.7587  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1760/3449]  eta: 1:46:08  lr: 0.000100  loss: 1.7452 (1.8370)  time: 3.7598  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1770/3449]  eta: 1:45:30  lr: 0.000100  loss: 1.4824 (1.8350)  time: 3.7510  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1780/3449]  eta: 1:44:53  lr: 0.000100  loss: 1.5048 (1.8352)  time: 3.7528  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1790/3449]  eta: 1:44:14  lr: 0.000100  loss: 1.8561 (1.8340)  time: 3.7661  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1800/3449]  eta: 1:43:36  lr: 0.000100  loss: 1.9778 (1.8357)  time: 3.7182  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1810/3449]  eta: 1:42:59  lr: 0.000100  loss: 1.9778 (1.8352)  time: 3.7607  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1820/3449]  eta: 1:42:21  lr: 0.000100  loss: 1.8924 (1.8351)  time: 3.7863  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [1830/3449]  eta: 1:41:42  lr: 0.000100  loss: 1.8542 (1.8361)  time: 3.7345  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [1840/3449]  eta: 1:41:05  lr: 0.000100  loss: 1.8033 (1.8349)  time: 3.7350  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1850/3449]  eta: 1:40:27  lr: 0.000100  loss: 1.7627 (1.8349)  time: 3.7587  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1860/3449]  eta: 1:39:49  lr: 0.000100  loss: 1.9713 (1.8348)  time: 3.7161  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1870/3449]  eta: 1:39:10  lr: 0.000100  loss: 1.6667 (1.8332)  time: 3.6521  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1880/3449]  eta: 1:38:31  lr: 0.000100  loss: 1.5545 (1.8324)  time: 3.6273  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1890/3449]  eta: 1:37:53  lr: 0.000100  loss: 1.6490 (1.8312)  time: 3.6879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1900/3449]  eta: 1:37:16  lr: 0.000100  loss: 2.0373 (1.8317)  time: 3.7695  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1910/3449]  eta: 1:36:38  lr: 0.000100  loss: 2.1038 (1.8320)  time: 3.8171  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1920/3449]  eta: 1:36:00  lr: 0.000100  loss: 1.8341 (1.8317)  time: 3.7698  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1930/3449]  eta: 1:35:22  lr: 0.000100  loss: 1.8475 (1.8321)  time: 3.7337  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1940/3449]  eta: 1:34:44  lr: 0.000100  loss: 1.5839 (1.8292)  time: 3.7315  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1950/3449]  eta: 1:34:07  lr: 0.000100  loss: 1.3875 (1.8288)  time: 3.7696  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1960/3449]  eta: 1:33:29  lr: 0.000100  loss: 2.0159 (1.8296)  time: 3.7672  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1970/3449]  eta: 1:32:52  lr: 0.000100  loss: 2.0159 (1.8294)  time: 3.7551  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1980/3449]  eta: 1:32:14  lr: 0.000100  loss: 2.0574 (1.8299)  time: 3.8095  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [1990/3449]  eta: 1:31:37  lr: 0.000100  loss: 2.1612 (1.8301)  time: 3.8016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2000/3449]  eta: 1:30:59  lr: 0.000100  loss: 1.7769 (1.8290)  time: 3.7730  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2010/3449]  eta: 1:30:22  lr: 0.000100  loss: 1.6211 (1.8276)  time: 3.7994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2020/3449]  eta: 1:29:44  lr: 0.000100  loss: 1.6108 (1.8271)  time: 3.7613  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2030/3449]  eta: 1:29:06  lr: 0.000100  loss: 1.6618 (1.8266)  time: 3.7114  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2040/3449]  eta: 1:28:27  lr: 0.000100  loss: 1.7803 (1.8270)  time: 3.7029  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2050/3449]  eta: 1:27:49  lr: 0.000100  loss: 1.5906 (1.8259)  time: 3.6719  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2060/3449]  eta: 1:27:11  lr: 0.000100  loss: 1.9437 (1.8277)  time: 3.6886  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2070/3449]  eta: 1:26:33  lr: 0.000100  loss: 1.9629 (1.8280)  time: 3.7525  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2080/3449]  eta: 1:25:56  lr: 0.000100  loss: 2.2480 (1.8303)  time: 3.7646  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2090/3449]  eta: 1:25:19  lr: 0.000100  loss: 1.9359 (1.8287)  time: 3.8266  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2100/3449]  eta: 1:24:40  lr: 0.000100  loss: 1.7701 (1.8278)  time: 3.7722  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2110/3449]  eta: 1:24:03  lr: 0.000100  loss: 1.7701 (1.8269)  time: 3.6946  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2120/3449]  eta: 1:23:25  lr: 0.000100  loss: 1.6981 (1.8259)  time: 3.7645  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2130/3449]  eta: 1:22:48  lr: 0.000100  loss: 1.7069 (1.8262)  time: 3.7902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2140/3449]  eta: 1:22:11  lr: 0.000100  loss: 1.7969 (1.8253)  time: 3.8372  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2150/3449]  eta: 1:21:33  lr: 0.000100  loss: 1.6549 (1.8251)  time: 3.8484  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2160/3449]  eta: 1:20:55  lr: 0.000100  loss: 1.8862 (1.8256)  time: 3.7619  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2170/3449]  eta: 1:20:18  lr: 0.000100  loss: 1.6885 (1.8238)  time: 3.7535  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2180/3449]  eta: 1:19:40  lr: 0.000100  loss: 1.5035 (1.8237)  time: 3.7747  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2190/3449]  eta: 1:19:02  lr: 0.000100  loss: 1.8237 (1.8241)  time: 3.7541  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2200/3449]  eta: 1:18:25  lr: 0.000100  loss: 1.9250 (1.8257)  time: 3.7635  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2210/3449]  eta: 1:17:47  lr: 0.000100  loss: 1.9250 (1.8256)  time: 3.7433  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2220/3449]  eta: 1:17:09  lr: 0.000100  loss: 1.6365 (1.8254)  time: 3.7466  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2230/3449]  eta: 1:16:31  lr: 0.000100  loss: 2.0735 (1.8263)  time: 3.7502  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2240/3449]  eta: 1:15:53  lr: 0.000100  loss: 2.0000 (1.8259)  time: 3.7307  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2250/3449]  eta: 1:15:16  lr: 0.000100  loss: 1.8807 (1.8269)  time: 3.7632  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2260/3449]  eta: 1:14:38  lr: 0.000100  loss: 1.7657 (1.8267)  time: 3.7808  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2270/3449]  eta: 1:14:00  lr: 0.000100  loss: 1.8096 (1.8266)  time: 3.7697  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2280/3449]  eta: 1:13:23  lr: 0.000100  loss: 1.6752 (1.8240)  time: 3.7596  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2290/3449]  eta: 1:12:46  lr: 0.000100  loss: 1.5114 (1.8240)  time: 3.8115  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2300/3449]  eta: 1:12:08  lr: 0.000100  loss: 1.9173 (1.8239)  time: 3.8707  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2310/3449]  eta: 1:11:31  lr: 0.000100  loss: 1.7531 (1.8229)  time: 3.8227  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2320/3449]  eta: 1:10:53  lr: 0.000100  loss: 1.7482 (1.8231)  time: 3.8040  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2330/3449]  eta: 1:10:16  lr: 0.000100  loss: 1.9544 (1.8236)  time: 3.7920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2340/3449]  eta: 1:09:38  lr: 0.000100  loss: 1.8817 (1.8235)  time: 3.7644  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2350/3449]  eta: 1:09:00  lr: 0.000100  loss: 2.0083 (1.8245)  time: 3.7452  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2360/3449]  eta: 1:08:22  lr: 0.000100  loss: 2.0569 (1.8253)  time: 3.7408  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2370/3449]  eta: 1:07:45  lr: 0.000100  loss: 1.9396 (1.8259)  time: 3.8370  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2380/3449]  eta: 1:07:07  lr: 0.000100  loss: 1.8903 (1.8258)  time: 3.8109  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2390/3449]  eta: 1:06:30  lr: 0.000100  loss: 1.6887 (1.8241)  time: 3.7432  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2400/3449]  eta: 1:05:52  lr: 0.000100  loss: 1.4830 (1.8236)  time: 3.7083  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2410/3449]  eta: 1:05:14  lr: 0.000100  loss: 1.5846 (1.8238)  time: 3.7443  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2420/3449]  eta: 1:04:36  lr: 0.000100  loss: 1.6240 (1.8234)  time: 3.7658  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2430/3449]  eta: 1:03:59  lr: 0.000100  loss: 2.0014 (1.8236)  time: 3.7243  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2440/3449]  eta: 1:03:21  lr: 0.000100  loss: 1.7853 (1.8230)  time: 3.8477  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2450/3449]  eta: 1:02:44  lr: 0.000100  loss: 1.7333 (1.8233)  time: 3.8693  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2460/3449]  eta: 1:02:06  lr: 0.000100  loss: 1.9867 (1.8234)  time: 3.7771  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2470/3449]  eta: 1:01:29  lr: 0.000100  loss: 1.7328 (1.8231)  time: 3.7579  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2480/3449]  eta: 1:00:51  lr: 0.000100  loss: 1.9417 (1.8246)  time: 3.7839  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2490/3449]  eta: 1:00:13  lr: 0.000100  loss: 1.9794 (1.8243)  time: 3.6973  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [2500/3449]  eta: 0:59:35  lr: 0.000100  loss: 1.8969 (1.8241)  time: 3.6654  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2510/3449]  eta: 0:58:57  lr: 0.000100  loss: 1.8969 (1.8243)  time: 3.7884  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2520/3449]  eta: 0:58:20  lr: 0.000100  loss: 1.6916 (1.8229)  time: 3.7923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2530/3449]  eta: 0:57:42  lr: 0.000100  loss: 1.8079 (1.8236)  time: 3.7879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2540/3449]  eta: 0:57:04  lr: 0.000100  loss: 1.9056 (1.8236)  time: 3.7921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2550/3449]  eta: 0:56:27  lr: 0.000100  loss: 1.9056 (1.8238)  time: 3.7478  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2560/3449]  eta: 0:55:49  lr: 0.000100  loss: 2.0460 (1.8238)  time: 3.7480  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2570/3449]  eta: 0:55:11  lr: 0.000100  loss: 1.7208 (1.8239)  time: 3.7523  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2580/3449]  eta: 0:54:33  lr: 0.000100  loss: 1.7208 (1.8230)  time: 3.6997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2590/3449]  eta: 0:53:55  lr: 0.000100  loss: 1.8772 (1.8238)  time: 3.6547  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2600/3449]  eta: 0:53:17  lr: 0.000100  loss: 1.9623 (1.8228)  time: 3.7093  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2610/3449]  eta: 0:52:40  lr: 0.000100  loss: 1.9743 (1.8233)  time: 3.7255  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2620/3449]  eta: 0:52:02  lr: 0.000100  loss: 1.8322 (1.8229)  time: 3.7498  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2630/3449]  eta: 0:51:25  lr: 0.000100  loss: 1.7180 (1.8226)  time: 3.8367  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2640/3449]  eta: 0:50:47  lr: 0.000100  loss: 1.7180 (1.8219)  time: 3.8449  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2650/3449]  eta: 0:50:10  lr: 0.000100  loss: 1.8308 (1.8218)  time: 3.8208  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2660/3449]  eta: 0:49:32  lr: 0.000100  loss: 2.0096 (1.8234)  time: 3.7555  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2670/3449]  eta: 0:48:54  lr: 0.000100  loss: 2.0096 (1.8234)  time: 3.6944  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2680/3449]  eta: 0:48:16  lr: 0.000100  loss: 1.8952 (1.8244)  time: 3.7357  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2690/3449]  eta: 0:47:39  lr: 0.000100  loss: 2.1287 (1.8247)  time: 3.7754  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2700/3449]  eta: 0:47:01  lr: 0.000100  loss: 2.2715 (1.8267)  time: 3.8029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2710/3449]  eta: 0:46:24  lr: 0.000100  loss: 2.1044 (1.8262)  time: 3.8345  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2720/3449]  eta: 0:45:46  lr: 0.000100  loss: 1.9400 (1.8257)  time: 3.8418  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2730/3449]  eta: 0:45:08  lr: 0.000100  loss: 1.5146 (1.8242)  time: 3.7901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2740/3449]  eta: 0:44:31  lr: 0.000100  loss: 1.5536 (1.8247)  time: 3.7459  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2750/3449]  eta: 0:43:53  lr: 0.000100  loss: 1.9746 (1.8246)  time: 3.7673  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2760/3449]  eta: 0:43:16  lr: 0.000100  loss: 1.7688 (1.8236)  time: 3.8211  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2770/3449]  eta: 0:42:38  lr: 0.000100  loss: 1.3077 (1.8233)  time: 3.8122  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2780/3449]  eta: 0:42:00  lr: 0.000100  loss: 1.8089 (1.8235)  time: 3.6999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2790/3449]  eta: 0:41:22  lr: 0.000100  loss: 1.8089 (1.8235)  time: 3.6671  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2800/3449]  eta: 0:40:44  lr: 0.000100  loss: 1.5738 (1.8225)  time: 3.7337  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2810/3449]  eta: 0:40:07  lr: 0.000100  loss: 1.7720 (1.8224)  time: 3.7519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2820/3449]  eta: 0:39:29  lr: 0.000100  loss: 1.8726 (1.8222)  time: 3.7312  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2830/3449]  eta: 0:38:51  lr: 0.000100  loss: 1.8605 (1.8219)  time: 3.7593  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2840/3449]  eta: 0:38:14  lr: 0.000100  loss: 1.7299 (1.8213)  time: 3.8058  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2850/3449]  eta: 0:37:36  lr: 0.000100  loss: 1.6297 (1.8217)  time: 3.7829  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2860/3449]  eta: 0:36:58  lr: 0.000100  loss: 1.8681 (1.8216)  time: 3.7861  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2870/3449]  eta: 0:36:21  lr: 0.000100  loss: 1.8247 (1.8206)  time: 3.8616  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2880/3449]  eta: 0:35:43  lr: 0.000100  loss: 1.9729 (1.8214)  time: 3.8467  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2890/3449]  eta: 0:35:06  lr: 0.000100  loss: 1.7419 (1.8196)  time: 3.8330  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [2900/3449]  eta: 0:34:28  lr: 0.000100  loss: 1.6491 (1.8194)  time: 3.8286  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2910/3449]  eta: 0:33:51  lr: 0.000100  loss: 1.7480 (1.8186)  time: 3.8027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2920/3449]  eta: 0:33:13  lr: 0.000100  loss: 1.8277 (1.8190)  time: 3.7718  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2930/3449]  eta: 0:32:35  lr: 0.000100  loss: 2.0852 (1.8193)  time: 3.7219  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2940/3449]  eta: 0:31:58  lr: 0.000100  loss: 2.0095 (1.8197)  time: 3.7723  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2950/3449]  eta: 0:31:20  lr: 0.000100  loss: 1.9349 (1.8196)  time: 3.8371  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2960/3449]  eta: 0:30:42  lr: 0.000100  loss: 1.9651 (1.8198)  time: 3.8252  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2970/3449]  eta: 0:30:05  lr: 0.000100  loss: 1.9924 (1.8197)  time: 3.7653  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2980/3449]  eta: 0:29:27  lr: 0.000100  loss: 1.8924 (1.8198)  time: 3.7165  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [2990/3449]  eta: 0:28:49  lr: 0.000100  loss: 1.8892 (1.8200)  time: 3.7823  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3000/3449]  eta: 0:28:12  lr: 0.000100  loss: 1.8892 (1.8204)  time: 3.8186  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3010/3449]  eta: 0:27:34  lr: 0.000100  loss: 1.8441 (1.8202)  time: 3.7222  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3020/3449]  eta: 0:26:56  lr: 0.000100  loss: 1.8441 (1.8201)  time: 3.7641  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [3030/3449]  eta: 0:26:19  lr: 0.000100  loss: 2.0242 (1.8202)  time: 3.8215  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [3040/3449]  eta: 0:25:41  lr: 0.000100  loss: 1.9858 (1.8209)  time: 3.7819  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3050/3449]  eta: 0:25:03  lr: 0.000100  loss: 1.9372 (1.8206)  time: 3.7779  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3060/3449]  eta: 0:24:25  lr: 0.000100  loss: 1.7780 (1.8208)  time: 3.7289  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3070/3449]  eta: 0:23:48  lr: 0.000100  loss: 1.9545 (1.8206)  time: 3.7992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3080/3449]  eta: 0:23:10  lr: 0.000100  loss: 1.9810 (1.8213)  time: 3.8422  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3090/3449]  eta: 0:22:33  lr: 0.000100  loss: 1.9031 (1.8209)  time: 3.8128  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3100/3449]  eta: 0:21:55  lr: 0.000100  loss: 1.8035 (1.8211)  time: 3.8293  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3110/3449]  eta: 0:21:17  lr: 0.000100  loss: 2.1832 (1.8228)  time: 3.7781  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3120/3449]  eta: 0:20:40  lr: 0.000100  loss: 2.1769 (1.8230)  time: 3.7598  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3130/3449]  eta: 0:20:02  lr: 0.000100  loss: 1.8193 (1.8223)  time: 3.7494  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3140/3449]  eta: 0:19:24  lr: 0.000100  loss: 1.7248 (1.8218)  time: 3.7664  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3150/3449]  eta: 0:18:47  lr: 0.000100  loss: 1.7839 (1.8216)  time: 3.7726  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [3160/3449]  eta: 0:18:09  lr: 0.000100  loss: 1.4882 (1.8206)  time: 3.7809  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3170/3449]  eta: 0:17:31  lr: 0.000100  loss: 1.3946 (1.8198)  time: 3.7547  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [3180/3449]  eta: 0:16:53  lr: 0.000100  loss: 1.5854 (1.8196)  time: 3.7867  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [3190/3449]  eta: 0:16:16  lr: 0.000100  loss: 1.7615 (1.8201)  time: 3.8678  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3200/3449]  eta: 0:15:38  lr: 0.000100  loss: 2.0560 (1.8202)  time: 3.7812  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3210/3449]  eta: 0:15:00  lr: 0.000100  loss: 2.0098 (1.8207)  time: 3.6803  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3220/3449]  eta: 0:14:23  lr: 0.000100  loss: 1.8933 (1.8206)  time: 3.7095  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3230/3449]  eta: 0:13:45  lr: 0.000100  loss: 1.7897 (1.8213)  time: 3.7519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3240/3449]  eta: 0:13:07  lr: 0.000100  loss: 1.9398 (1.8220)  time: 3.7826  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [3250/3449]  eta: 0:12:30  lr: 0.000100  loss: 1.9398 (1.8230)  time: 3.8266  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3260/3449]  eta: 0:11:52  lr: 0.000100  loss: 1.8110 (1.8219)  time: 3.8410  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3270/3449]  eta: 0:11:14  lr: 0.000100  loss: 1.6319 (1.8215)  time: 3.8239  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3280/3449]  eta: 0:10:37  lr: 0.000100  loss: 1.8900 (1.8218)  time: 3.8170  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3290/3449]  eta: 0:09:59  lr: 0.000100  loss: 1.9443 (1.8211)  time: 3.8892  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3300/3449]  eta: 0:09:21  lr: 0.000100  loss: 1.4050 (1.8211)  time: 3.9078  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3310/3449]  eta: 0:08:44  lr: 0.000100  loss: 1.9870 (1.8216)  time: 3.8523  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3320/3449]  eta: 0:08:06  lr: 0.000100  loss: 2.0366 (1.8219)  time: 3.7925  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3330/3449]  eta: 0:07:28  lr: 0.000100  loss: 1.9540 (1.8220)  time: 3.7490  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3340/3449]  eta: 0:06:51  lr: 0.000100  loss: 2.1214 (1.8231)  time: 3.7330  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3350/3449]  eta: 0:06:13  lr: 0.000100  loss: 2.0169 (1.8227)  time: 3.7595  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3360/3449]  eta: 0:05:35  lr: 0.000100  loss: 1.4592 (1.8212)  time: 3.7937  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3370/3449]  eta: 0:04:57  lr: 0.000100  loss: 1.1763 (1.8200)  time: 3.7888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [3380/3449]  eta: 0:04:20  lr: 0.000100  loss: 1.3585 (1.8195)  time: 3.7697  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3390/3449]  eta: 0:03:42  lr: 0.000100  loss: 1.8469 (1.8196)  time: 3.7701  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3400/3449]  eta: 0:03:04  lr: 0.000100  loss: 2.0063 (1.8196)  time: 3.8128  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:11]  [3410/3449]  eta: 0:02:27  lr: 0.000100  loss: 1.7435 (1.8193)  time: 3.8220  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3420/3449]  eta: 0:01:49  lr: 0.000100  loss: 1.7435 (1.8188)  time: 3.7908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3430/3449]  eta: 0:01:11  lr: 0.000100  loss: 2.1130 (1.8199)  time: 3.7935  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3440/3449]  eta: 0:00:33  lr: 0.000100  loss: 2.1435 (1.8198)  time: 3.7764  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 1.7575 (1.8196)  time: 3.7039  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:11] Total time: 3:36:46 (3.7712 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 1.7575 (1.8196)\n",
      "Valid: [epoch:11]  [ 0/14]  eta: 0:04:21  loss: 1.2524 (1.2524)  time: 18.6998  data: 0.4573  max mem: 34968\n",
      "Valid: [epoch:11]  [13/14]  eta: 0:00:18  loss: 1.4597 (1.5345)  time: 18.2846  data: 0.0328  max mem: 34968\n",
      "Valid: [epoch:11] Total time: 0:04:16 (18.2930 s / it)\n",
      "Averaged stats: loss: 1.4597 (1.5345)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_11_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.534%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "Train: [epoch:12]  [   0/3449]  eta: 4:44:52  lr: 0.000100  loss: 3.0278 (3.0278)  time: 4.9557  data: 1.1240  max mem: 34968\n",
      "Train: [epoch:12]  [  10/3449]  eta: 3:45:53  lr: 0.000100  loss: 2.5233 (2.4647)  time: 3.9411  data: 0.1023  max mem: 34968\n",
      "Train: [epoch:12]  [  20/3449]  eta: 3:36:20  lr: 0.000100  loss: 2.3051 (2.3024)  time: 3.7270  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [  30/3449]  eta: 3:36:22  lr: 0.000100  loss: 2.1801 (2.2226)  time: 3.7180  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [  40/3449]  eta: 3:36:14  lr: 0.000100  loss: 2.1512 (2.1701)  time: 3.8274  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [  50/3449]  eta: 3:35:08  lr: 0.000100  loss: 2.0684 (2.1251)  time: 3.7983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [  60/3449]  eta: 3:34:03  lr: 0.000100  loss: 2.1754 (2.1383)  time: 3.7563  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [  70/3449]  eta: 3:32:41  lr: 0.000100  loss: 2.0597 (2.1224)  time: 3.7232  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [  80/3449]  eta: 3:32:15  lr: 0.000100  loss: 1.9563 (2.0720)  time: 3.7519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [  90/3449]  eta: 3:31:37  lr: 0.000100  loss: 2.1923 (2.1035)  time: 3.7931  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 100/3449]  eta: 3:31:05  lr: 0.000100  loss: 2.3390 (2.1157)  time: 3.7889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 110/3449]  eta: 3:30:11  lr: 0.000100  loss: 1.7488 (2.0698)  time: 3.7627  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 120/3449]  eta: 3:29:13  lr: 0.000100  loss: 1.7245 (2.0406)  time: 3.7141  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 130/3449]  eta: 3:28:41  lr: 0.000100  loss: 1.9502 (2.0339)  time: 3.7488  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 140/3449]  eta: 3:27:40  lr: 0.000100  loss: 1.9777 (2.0273)  time: 3.7339  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 150/3449]  eta: 3:27:02  lr: 0.000100  loss: 1.9777 (2.0218)  time: 3.7187  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 160/3449]  eta: 3:26:00  lr: 0.000100  loss: 1.7468 (2.0064)  time: 3.7060  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 170/3449]  eta: 3:25:22  lr: 0.000100  loss: 1.8853 (2.0002)  time: 3.7014  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 180/3449]  eta: 3:24:42  lr: 0.000100  loss: 1.8331 (1.9726)  time: 3.7487  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 190/3449]  eta: 3:24:01  lr: 0.000100  loss: 1.7184 (1.9663)  time: 3.7388  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 200/3449]  eta: 3:23:32  lr: 0.000100  loss: 2.4436 (1.9812)  time: 3.7753  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 210/3449]  eta: 3:22:44  lr: 0.000100  loss: 2.3052 (1.9833)  time: 3.7500  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 220/3449]  eta: 3:22:17  lr: 0.000100  loss: 2.1334 (1.9920)  time: 3.7570  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 230/3449]  eta: 3:21:40  lr: 0.000100  loss: 2.1342 (2.0007)  time: 3.7972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 240/3449]  eta: 3:21:06  lr: 0.000100  loss: 2.0184 (1.9979)  time: 3.7748  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 250/3449]  eta: 3:20:34  lr: 0.000100  loss: 2.2382 (2.0072)  time: 3.7949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 260/3449]  eta: 3:19:58  lr: 0.000100  loss: 2.2748 (2.0056)  time: 3.7923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 270/3449]  eta: 3:19:18  lr: 0.000100  loss: 2.0460 (2.0042)  time: 3.7599  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 280/3449]  eta: 3:18:42  lr: 0.000100  loss: 2.1342 (2.0046)  time: 3.7579  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 290/3449]  eta: 3:18:03  lr: 0.000100  loss: 2.1180 (2.0035)  time: 3.7610  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 300/3449]  eta: 3:17:14  lr: 0.000100  loss: 1.9769 (2.0003)  time: 3.7004  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [ 310/3449]  eta: 3:16:39  lr: 0.000100  loss: 1.9032 (1.9892)  time: 3.7208  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 320/3449]  eta: 3:15:51  lr: 0.000100  loss: 1.9357 (1.9943)  time: 3.7198  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 330/3449]  eta: 3:15:13  lr: 0.000100  loss: 2.0601 (1.9930)  time: 3.6977  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 340/3449]  eta: 3:14:35  lr: 0.000100  loss: 1.9656 (1.9926)  time: 3.7504  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 350/3449]  eta: 3:14:00  lr: 0.000100  loss: 1.8626 (1.9903)  time: 3.7714  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 360/3449]  eta: 3:13:33  lr: 0.000100  loss: 2.2970 (2.0002)  time: 3.8294  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 370/3449]  eta: 3:13:06  lr: 0.000100  loss: 2.3193 (1.9980)  time: 3.8834  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 380/3449]  eta: 3:12:39  lr: 0.000100  loss: 2.0848 (2.0003)  time: 3.8944  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 390/3449]  eta: 3:11:52  lr: 0.000100  loss: 1.9601 (1.9960)  time: 3.7662  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 400/3449]  eta: 3:11:18  lr: 0.000100  loss: 1.8414 (1.9996)  time: 3.7265  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 410/3449]  eta: 3:10:39  lr: 0.000100  loss: 1.8662 (1.9852)  time: 3.7851  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 420/3449]  eta: 3:10:00  lr: 0.000100  loss: 2.0745 (1.9886)  time: 3.7452  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 430/3449]  eta: 3:09:25  lr: 0.000100  loss: 2.0139 (1.9867)  time: 3.7710  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 440/3449]  eta: 3:08:38  lr: 0.000100  loss: 2.3850 (1.9994)  time: 3.7194  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 450/3449]  eta: 3:07:56  lr: 0.000100  loss: 2.3850 (1.9982)  time: 3.6612  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 460/3449]  eta: 3:07:16  lr: 0.000100  loss: 1.9883 (1.9981)  time: 3.7040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 470/3449]  eta: 3:06:26  lr: 0.000100  loss: 2.0641 (2.0003)  time: 3.6453  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 480/3449]  eta: 3:05:42  lr: 0.000100  loss: 2.3465 (2.0033)  time: 3.6123  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 490/3449]  eta: 3:05:07  lr: 0.000100  loss: 1.9160 (1.9977)  time: 3.7218  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 500/3449]  eta: 3:04:30  lr: 0.000100  loss: 1.9046 (1.9922)  time: 3.7766  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 510/3449]  eta: 3:03:59  lr: 0.000100  loss: 2.0456 (1.9903)  time: 3.8171  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 520/3449]  eta: 3:03:24  lr: 0.000100  loss: 1.7978 (1.9840)  time: 3.8373  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 530/3449]  eta: 3:02:47  lr: 0.000100  loss: 1.9304 (1.9883)  time: 3.7852  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 540/3449]  eta: 3:02:13  lr: 0.000100  loss: 2.0678 (1.9802)  time: 3.7939  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 550/3449]  eta: 3:01:36  lr: 0.000100  loss: 1.9023 (1.9801)  time: 3.7904  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 560/3449]  eta: 3:00:56  lr: 0.000100  loss: 1.9499 (1.9776)  time: 3.7357  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 570/3449]  eta: 3:00:26  lr: 0.000100  loss: 1.8513 (1.9745)  time: 3.8157  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 580/3449]  eta: 2:59:42  lr: 0.000100  loss: 1.7472 (1.9704)  time: 3.7787  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 590/3449]  eta: 2:59:02  lr: 0.000100  loss: 1.7464 (1.9683)  time: 3.6676  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 600/3449]  eta: 2:58:27  lr: 0.000100  loss: 1.5381 (1.9562)  time: 3.7528  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 610/3449]  eta: 2:57:52  lr: 0.000100  loss: 1.5954 (1.9535)  time: 3.8090  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 620/3449]  eta: 2:57:11  lr: 0.000100  loss: 1.9224 (1.9461)  time: 3.7506  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 630/3449]  eta: 2:56:25  lr: 0.000100  loss: 1.3517 (1.9399)  time: 3.6332  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 640/3449]  eta: 2:55:42  lr: 0.000100  loss: 1.6501 (1.9357)  time: 3.6037  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 650/3449]  eta: 2:55:14  lr: 0.000100  loss: 2.0030 (1.9379)  time: 3.7945  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 660/3449]  eta: 2:54:36  lr: 0.000100  loss: 2.0030 (1.9364)  time: 3.8628  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 670/3449]  eta: 2:54:01  lr: 0.000100  loss: 1.9031 (1.9384)  time: 3.7928  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 680/3449]  eta: 2:53:27  lr: 0.000100  loss: 1.8534 (1.9352)  time: 3.8212  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 690/3449]  eta: 2:52:47  lr: 0.000100  loss: 1.7769 (1.9319)  time: 3.7601  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 700/3449]  eta: 2:52:10  lr: 0.000100  loss: 1.8492 (1.9297)  time: 3.7435  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 710/3449]  eta: 2:51:37  lr: 0.000100  loss: 1.8821 (1.9278)  time: 3.8346  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 720/3449]  eta: 2:50:57  lr: 0.000100  loss: 1.8022 (1.9271)  time: 3.7825  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 730/3449]  eta: 2:50:21  lr: 0.000100  loss: 1.8082 (1.9241)  time: 3.7412  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 740/3449]  eta: 2:49:45  lr: 0.000100  loss: 1.8082 (1.9215)  time: 3.7996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 750/3449]  eta: 2:49:05  lr: 0.000100  loss: 1.8092 (1.9193)  time: 3.7485  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 760/3449]  eta: 2:48:27  lr: 0.000100  loss: 1.8275 (1.9167)  time: 3.7238  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 770/3449]  eta: 2:47:54  lr: 0.000100  loss: 1.7828 (1.9119)  time: 3.8237  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 780/3449]  eta: 2:47:13  lr: 0.000100  loss: 1.8597 (1.9108)  time: 3.7786  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 790/3449]  eta: 2:46:36  lr: 0.000100  loss: 1.8051 (1.9078)  time: 3.7236  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 800/3449]  eta: 2:45:56  lr: 0.000100  loss: 1.5081 (1.9009)  time: 3.7275  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 810/3449]  eta: 2:45:18  lr: 0.000100  loss: 1.3034 (1.8962)  time: 3.7128  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 820/3449]  eta: 2:44:42  lr: 0.000100  loss: 1.6942 (1.8955)  time: 3.7853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 830/3449]  eta: 2:44:07  lr: 0.000100  loss: 2.0174 (1.8975)  time: 3.8191  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 840/3449]  eta: 2:43:29  lr: 0.000100  loss: 1.8801 (1.8937)  time: 3.7825  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 850/3449]  eta: 2:42:55  lr: 0.000100  loss: 1.6546 (1.8933)  time: 3.8139  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 860/3449]  eta: 2:42:20  lr: 0.000100  loss: 1.7360 (1.8904)  time: 3.8660  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 870/3449]  eta: 2:41:43  lr: 0.000100  loss: 1.5885 (1.8871)  time: 3.8085  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 880/3449]  eta: 2:41:06  lr: 0.000100  loss: 1.8256 (1.8865)  time: 3.7806  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 890/3449]  eta: 2:40:30  lr: 0.000100  loss: 1.8659 (1.8846)  time: 3.8015  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 900/3449]  eta: 2:39:53  lr: 0.000100  loss: 2.0834 (1.8872)  time: 3.8055  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 910/3449]  eta: 2:39:13  lr: 0.000100  loss: 2.0834 (1.8872)  time: 3.7467  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 920/3449]  eta: 2:38:36  lr: 0.000100  loss: 1.8816 (1.8870)  time: 3.7305  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 930/3449]  eta: 2:37:56  lr: 0.000100  loss: 1.6451 (1.8827)  time: 3.7278  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 940/3449]  eta: 2:37:20  lr: 0.000100  loss: 1.4017 (1.8787)  time: 3.7586  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 950/3449]  eta: 2:36:43  lr: 0.000100  loss: 1.4694 (1.8760)  time: 3.7955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [ 960/3449]  eta: 2:36:05  lr: 0.000100  loss: 2.0604 (1.8781)  time: 3.7643  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [ 970/3449]  eta: 2:35:29  lr: 0.000100  loss: 2.2110 (1.8807)  time: 3.7941  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 980/3449]  eta: 2:34:51  lr: 0.000100  loss: 1.9200 (1.8780)  time: 3.7759  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [ 990/3449]  eta: 2:34:11  lr: 0.000100  loss: 1.6316 (1.8749)  time: 3.7147  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1000/3449]  eta: 2:33:32  lr: 0.000100  loss: 1.6737 (1.8744)  time: 3.6891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1010/3449]  eta: 2:32:53  lr: 0.000100  loss: 1.5535 (1.8683)  time: 3.7031  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1020/3449]  eta: 2:32:17  lr: 0.000100  loss: 1.5693 (1.8678)  time: 3.7658  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1030/3449]  eta: 2:31:39  lr: 0.000100  loss: 1.8482 (1.8652)  time: 3.7913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1040/3449]  eta: 2:31:02  lr: 0.000100  loss: 1.6975 (1.8621)  time: 3.7751  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1050/3449]  eta: 2:30:25  lr: 0.000100  loss: 1.7471 (1.8612)  time: 3.7864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1060/3449]  eta: 2:29:49  lr: 0.000100  loss: 1.7930 (1.8583)  time: 3.8077  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1070/3449]  eta: 2:29:16  lr: 0.000100  loss: 1.7947 (1.8587)  time: 3.9054  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1080/3449]  eta: 2:28:40  lr: 0.000100  loss: 1.7947 (1.8560)  time: 3.9201  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1090/3449]  eta: 2:28:01  lr: 0.000100  loss: 1.7892 (1.8555)  time: 3.7586  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1100/3449]  eta: 2:27:24  lr: 0.000100  loss: 1.8370 (1.8543)  time: 3.7402  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1110/3449]  eta: 2:26:49  lr: 0.000100  loss: 1.7533 (1.8531)  time: 3.8415  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1120/3449]  eta: 2:26:09  lr: 0.000100  loss: 1.5961 (1.8498)  time: 3.7806  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1130/3449]  eta: 2:25:30  lr: 0.000100  loss: 1.5961 (1.8492)  time: 3.6913  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1140/3449]  eta: 2:24:54  lr: 0.000100  loss: 1.9793 (1.8512)  time: 3.7565  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1150/3449]  eta: 2:24:15  lr: 0.000100  loss: 1.7662 (1.8484)  time: 3.7660  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1160/3449]  eta: 2:23:37  lr: 0.000100  loss: 1.6492 (1.8467)  time: 3.7357  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1170/3449]  eta: 2:23:02  lr: 0.000100  loss: 1.9135 (1.8477)  time: 3.8165  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1180/3449]  eta: 2:22:23  lr: 0.000100  loss: 1.9208 (1.8462)  time: 3.8026  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1190/3449]  eta: 2:21:46  lr: 0.000100  loss: 1.6532 (1.8444)  time: 3.7528  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1200/3449]  eta: 2:21:11  lr: 0.000100  loss: 1.5499 (1.8417)  time: 3.8423  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1210/3449]  eta: 2:20:34  lr: 0.000100  loss: 1.5952 (1.8417)  time: 3.8432  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1220/3449]  eta: 2:19:54  lr: 0.000100  loss: 1.8754 (1.8404)  time: 3.7115  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1230/3449]  eta: 2:19:18  lr: 0.000100  loss: 1.8777 (1.8392)  time: 3.7516  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1240/3449]  eta: 2:18:40  lr: 0.000100  loss: 1.8280 (1.8375)  time: 3.8075  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1250/3449]  eta: 2:18:03  lr: 0.000100  loss: 1.7117 (1.8373)  time: 3.7815  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1260/3449]  eta: 2:17:25  lr: 0.000100  loss: 2.0849 (1.8385)  time: 3.7730  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1270/3449]  eta: 2:16:47  lr: 0.000100  loss: 1.9607 (1.8388)  time: 3.7505  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1280/3449]  eta: 2:16:10  lr: 0.000100  loss: 1.6753 (1.8353)  time: 3.7987  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1290/3449]  eta: 2:15:33  lr: 0.000100  loss: 1.5030 (1.8336)  time: 3.8187  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1300/3449]  eta: 2:14:57  lr: 0.000100  loss: 1.9596 (1.8318)  time: 3.8316  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1310/3449]  eta: 2:14:19  lr: 0.000100  loss: 1.8121 (1.8307)  time: 3.8102  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1320/3449]  eta: 2:13:43  lr: 0.000100  loss: 1.7208 (1.8286)  time: 3.8135  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1330/3449]  eta: 2:13:06  lr: 0.000100  loss: 1.7533 (1.8282)  time: 3.8426  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1340/3449]  eta: 2:12:29  lr: 0.000100  loss: 1.7768 (1.8260)  time: 3.7985  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1350/3449]  eta: 2:11:50  lr: 0.000100  loss: 1.4484 (1.8234)  time: 3.7512  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1360/3449]  eta: 2:11:12  lr: 0.000100  loss: 1.6914 (1.8242)  time: 3.7205  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1370/3449]  eta: 2:10:34  lr: 0.000100  loss: 1.8864 (1.8232)  time: 3.7183  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1380/3449]  eta: 2:09:55  lr: 0.000100  loss: 1.8852 (1.8218)  time: 3.7220  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1390/3449]  eta: 2:09:19  lr: 0.000100  loss: 1.8929 (1.8221)  time: 3.7881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1400/3449]  eta: 2:08:40  lr: 0.000100  loss: 1.9549 (1.8224)  time: 3.7810  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1410/3449]  eta: 2:08:03  lr: 0.000100  loss: 2.0348 (1.8236)  time: 3.7566  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1420/3449]  eta: 2:07:25  lr: 0.000100  loss: 2.0348 (1.8249)  time: 3.7763  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1430/3449]  eta: 2:06:47  lr: 0.000100  loss: 1.8855 (1.8232)  time: 3.7449  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1440/3449]  eta: 2:06:10  lr: 0.000100  loss: 1.8157 (1.8241)  time: 3.7668  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1450/3449]  eta: 2:05:32  lr: 0.000100  loss: 1.8819 (1.8242)  time: 3.7799  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1460/3449]  eta: 2:04:56  lr: 0.000100  loss: 1.6066 (1.8226)  time: 3.8449  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1470/3449]  eta: 2:04:20  lr: 0.000100  loss: 1.6265 (1.8222)  time: 3.8867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1480/3449]  eta: 2:03:42  lr: 0.000100  loss: 1.6954 (1.8206)  time: 3.7881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1490/3449]  eta: 2:03:03  lr: 0.000100  loss: 1.3910 (1.8166)  time: 3.7104  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1500/3449]  eta: 2:02:25  lr: 0.000100  loss: 1.4943 (1.8159)  time: 3.7195  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1510/3449]  eta: 2:01:47  lr: 0.000100  loss: 1.8882 (1.8158)  time: 3.7424  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1520/3449]  eta: 2:01:08  lr: 0.000100  loss: 1.8840 (1.8159)  time: 3.6975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1530/3449]  eta: 2:00:31  lr: 0.000100  loss: 1.8373 (1.8148)  time: 3.7420  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1540/3449]  eta: 1:59:53  lr: 0.000100  loss: 1.7608 (1.8141)  time: 3.8116  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1550/3449]  eta: 1:59:16  lr: 0.000100  loss: 1.7608 (1.8136)  time: 3.7929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1560/3449]  eta: 1:58:39  lr: 0.000100  loss: 1.8340 (1.8128)  time: 3.7884  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1570/3449]  eta: 1:58:01  lr: 0.000100  loss: 1.6970 (1.8108)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1580/3449]  eta: 1:57:24  lr: 0.000100  loss: 1.8050 (1.8112)  time: 3.8052  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1590/3449]  eta: 1:56:48  lr: 0.000100  loss: 1.8050 (1.8110)  time: 3.8793  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1600/3449]  eta: 1:56:11  lr: 0.000100  loss: 1.5508 (1.8089)  time: 3.8522  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1610/3449]  eta: 1:55:32  lr: 0.000100  loss: 1.4315 (1.8083)  time: 3.7464  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1620/3449]  eta: 1:54:55  lr: 0.000100  loss: 1.3662 (1.8047)  time: 3.7414  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [1630/3449]  eta: 1:54:18  lr: 0.000100  loss: 1.4611 (1.8045)  time: 3.8100  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1640/3449]  eta: 1:53:39  lr: 0.000100  loss: 1.7342 (1.8040)  time: 3.7756  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1650/3449]  eta: 1:53:01  lr: 0.000100  loss: 1.8039 (1.8043)  time: 3.7175  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1660/3449]  eta: 1:52:23  lr: 0.000100  loss: 1.7532 (1.8034)  time: 3.7485  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1670/3449]  eta: 1:51:46  lr: 0.000100  loss: 1.8794 (1.8040)  time: 3.7914  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1680/3449]  eta: 1:51:10  lr: 0.000100  loss: 2.0436 (1.8054)  time: 3.8420  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1690/3449]  eta: 1:50:31  lr: 0.000100  loss: 2.0436 (1.8064)  time: 3.7938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1700/3449]  eta: 1:49:54  lr: 0.000100  loss: 1.7520 (1.8038)  time: 3.7623  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1710/3449]  eta: 1:49:15  lr: 0.000100  loss: 1.2648 (1.8012)  time: 3.7537  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1720/3449]  eta: 1:48:38  lr: 0.000100  loss: 1.3807 (1.7998)  time: 3.7465  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1730/3449]  eta: 1:48:02  lr: 0.000100  loss: 1.8852 (1.8013)  time: 3.8498  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1740/3449]  eta: 1:47:23  lr: 0.000100  loss: 1.9209 (1.8006)  time: 3.8004  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1750/3449]  eta: 1:46:46  lr: 0.000100  loss: 1.4548 (1.7983)  time: 3.7393  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1760/3449]  eta: 1:46:08  lr: 0.000100  loss: 1.3319 (1.7972)  time: 3.7643  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1770/3449]  eta: 1:45:31  lr: 0.000100  loss: 1.5592 (1.7945)  time: 3.8206  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1780/3449]  eta: 1:44:53  lr: 0.000100  loss: 1.5763 (1.7947)  time: 3.8303  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1790/3449]  eta: 1:44:17  lr: 0.000100  loss: 1.6775 (1.7934)  time: 3.8271  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1800/3449]  eta: 1:43:40  lr: 0.000100  loss: 1.7573 (1.7941)  time: 3.8616  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1810/3449]  eta: 1:43:02  lr: 0.000100  loss: 1.8487 (1.7932)  time: 3.7842  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1820/3449]  eta: 1:42:24  lr: 0.000100  loss: 1.5698 (1.7925)  time: 3.7347  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1830/3449]  eta: 1:41:46  lr: 0.000100  loss: 1.9071 (1.7942)  time: 3.7652  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1840/3449]  eta: 1:41:09  lr: 0.000100  loss: 1.8292 (1.7933)  time: 3.8062  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1850/3449]  eta: 1:40:31  lr: 0.000100  loss: 1.7934 (1.7934)  time: 3.8050  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1860/3449]  eta: 1:39:53  lr: 0.000100  loss: 1.7478 (1.7909)  time: 3.7795  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1870/3449]  eta: 1:39:17  lr: 0.000100  loss: 1.3374 (1.7880)  time: 3.8250  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1880/3449]  eta: 1:38:38  lr: 0.000100  loss: 1.4751 (1.7868)  time: 3.7971  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1890/3449]  eta: 1:38:01  lr: 0.000100  loss: 1.6201 (1.7857)  time: 3.7621  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1900/3449]  eta: 1:37:24  lr: 0.000100  loss: 1.7501 (1.7855)  time: 3.8222  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1910/3449]  eta: 1:36:46  lr: 0.000100  loss: 1.7985 (1.7858)  time: 3.7970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1920/3449]  eta: 1:36:08  lr: 0.000100  loss: 1.5071 (1.7830)  time: 3.7856  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1930/3449]  eta: 1:35:30  lr: 0.000100  loss: 1.2279 (1.7807)  time: 3.7483  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1940/3449]  eta: 1:34:51  lr: 0.000100  loss: 1.3578 (1.7788)  time: 3.6722  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1950/3449]  eta: 1:34:14  lr: 0.000100  loss: 1.5212 (1.7776)  time: 3.7045  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1960/3449]  eta: 1:33:35  lr: 0.000100  loss: 1.5709 (1.7771)  time: 3.7010  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1970/3449]  eta: 1:32:56  lr: 0.000100  loss: 1.6912 (1.7769)  time: 3.6362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [1980/3449]  eta: 1:32:19  lr: 0.000100  loss: 1.7154 (1.7773)  time: 3.7380  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [1990/3449]  eta: 1:31:40  lr: 0.000100  loss: 1.9780 (1.7779)  time: 3.7537  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2000/3449]  eta: 1:31:04  lr: 0.000100  loss: 1.7898 (1.7760)  time: 3.7676  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2010/3449]  eta: 1:30:27  lr: 0.000100  loss: 1.6311 (1.7751)  time: 3.8836  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2020/3449]  eta: 1:29:50  lr: 0.000100  loss: 1.2780 (1.7731)  time: 3.8747  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2030/3449]  eta: 1:29:12  lr: 0.000100  loss: 1.1779 (1.7719)  time: 3.8509  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2040/3449]  eta: 1:28:35  lr: 0.000100  loss: 1.6334 (1.7719)  time: 3.8293  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2050/3449]  eta: 1:27:57  lr: 0.000100  loss: 1.5550 (1.7712)  time: 3.7933  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2060/3449]  eta: 1:27:20  lr: 0.000100  loss: 1.6418 (1.7711)  time: 3.7823  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2070/3449]  eta: 1:26:42  lr: 0.000100  loss: 1.6792 (1.7710)  time: 3.8157  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2080/3449]  eta: 1:26:05  lr: 0.000100  loss: 1.8505 (1.7721)  time: 3.8176  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2090/3449]  eta: 1:25:27  lr: 0.000100  loss: 1.8802 (1.7710)  time: 3.7993  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2100/3449]  eta: 1:24:50  lr: 0.000100  loss: 1.3879 (1.7689)  time: 3.8141  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2110/3449]  eta: 1:24:12  lr: 0.000100  loss: 1.3862 (1.7676)  time: 3.8046  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2120/3449]  eta: 1:23:34  lr: 0.000100  loss: 1.6985 (1.7687)  time: 3.7320  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2130/3449]  eta: 1:22:56  lr: 0.000100  loss: 1.7535 (1.7671)  time: 3.7290  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2140/3449]  eta: 1:22:19  lr: 0.000100  loss: 1.4891 (1.7661)  time: 3.7986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2150/3449]  eta: 1:21:42  lr: 0.000100  loss: 1.6412 (1.7665)  time: 3.8611  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2160/3449]  eta: 1:21:04  lr: 0.000100  loss: 1.8054 (1.7661)  time: 3.8641  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2170/3449]  eta: 1:20:26  lr: 0.000100  loss: 1.5183 (1.7648)  time: 3.7975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2180/3449]  eta: 1:19:49  lr: 0.000100  loss: 1.7297 (1.7651)  time: 3.7482  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2190/3449]  eta: 1:19:12  lr: 0.000100  loss: 1.8668 (1.7650)  time: 3.8288  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2200/3449]  eta: 1:18:35  lr: 0.000100  loss: 1.9263 (1.7665)  time: 3.9224  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2210/3449]  eta: 1:17:56  lr: 0.000100  loss: 1.9647 (1.7658)  time: 3.8025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2220/3449]  eta: 1:17:18  lr: 0.000100  loss: 1.6032 (1.7648)  time: 3.6489  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2230/3449]  eta: 1:16:40  lr: 0.000100  loss: 1.7928 (1.7644)  time: 3.6878  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2240/3449]  eta: 1:16:03  lr: 0.000100  loss: 1.7928 (1.7638)  time: 3.8332  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2250/3449]  eta: 1:15:25  lr: 0.000100  loss: 1.4522 (1.7642)  time: 3.8331  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2260/3449]  eta: 1:14:47  lr: 0.000100  loss: 1.7746 (1.7637)  time: 3.7693  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2270/3449]  eta: 1:14:10  lr: 0.000100  loss: 1.5728 (1.7622)  time: 3.8085  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2280/3449]  eta: 1:13:32  lr: 0.000100  loss: 1.4717 (1.7607)  time: 3.7793  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [2290/3449]  eta: 1:12:54  lr: 0.000100  loss: 1.4717 (1.7597)  time: 3.7401  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2300/3449]  eta: 1:12:17  lr: 0.000100  loss: 1.8513 (1.7601)  time: 3.8290  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2310/3449]  eta: 1:11:39  lr: 0.000100  loss: 1.6885 (1.7591)  time: 3.8538  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2320/3449]  eta: 1:11:01  lr: 0.000100  loss: 1.5903 (1.7591)  time: 3.7437  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2330/3449]  eta: 1:10:24  lr: 0.000100  loss: 1.6579 (1.7577)  time: 3.7668  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2340/3449]  eta: 1:09:46  lr: 0.000100  loss: 1.6835 (1.7576)  time: 3.8063  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2350/3449]  eta: 1:09:08  lr: 0.000100  loss: 1.7268 (1.7575)  time: 3.7713  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2360/3449]  eta: 1:08:30  lr: 0.000100  loss: 1.8006 (1.7572)  time: 3.7199  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2370/3449]  eta: 1:07:52  lr: 0.000100  loss: 1.6685 (1.7573)  time: 3.7205  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2380/3449]  eta: 1:07:14  lr: 0.000100  loss: 1.7190 (1.7571)  time: 3.7434  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2390/3449]  eta: 1:06:36  lr: 0.000100  loss: 1.6855 (1.7557)  time: 3.7323  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2400/3449]  eta: 1:05:59  lr: 0.000100  loss: 1.3102 (1.7551)  time: 3.8213  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2410/3449]  eta: 1:05:22  lr: 0.000100  loss: 1.4692 (1.7541)  time: 3.8800  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2420/3449]  eta: 1:04:44  lr: 0.000100  loss: 1.4865 (1.7536)  time: 3.8278  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2430/3449]  eta: 1:04:06  lr: 0.000100  loss: 1.8311 (1.7535)  time: 3.7696  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2440/3449]  eta: 1:03:29  lr: 0.000100  loss: 1.9157 (1.7528)  time: 3.7727  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2450/3449]  eta: 1:02:51  lr: 0.000100  loss: 1.6039 (1.7516)  time: 3.8326  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2460/3449]  eta: 1:02:14  lr: 0.000100  loss: 1.5290 (1.7510)  time: 3.9089  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2470/3449]  eta: 1:01:36  lr: 0.000100  loss: 1.6227 (1.7505)  time: 3.8390  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2480/3449]  eta: 1:00:58  lr: 0.000100  loss: 1.7461 (1.7507)  time: 3.7124  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2490/3449]  eta: 1:00:20  lr: 0.000100  loss: 1.8230 (1.7493)  time: 3.7101  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2500/3449]  eta: 0:59:43  lr: 0.000100  loss: 1.7455 (1.7493)  time: 3.8050  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2510/3449]  eta: 0:59:05  lr: 0.000100  loss: 1.7218 (1.7485)  time: 3.8411  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2520/3449]  eta: 0:58:28  lr: 0.000100  loss: 1.5396 (1.7470)  time: 3.8232  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2530/3449]  eta: 0:57:50  lr: 0.000100  loss: 1.5810 (1.7458)  time: 3.8093  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2540/3449]  eta: 0:57:12  lr: 0.000100  loss: 1.5529 (1.7444)  time: 3.7862  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2550/3449]  eta: 0:56:35  lr: 0.000100  loss: 1.5529 (1.7446)  time: 3.7964  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2560/3449]  eta: 0:55:58  lr: 0.000100  loss: 1.7257 (1.7441)  time: 3.8891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2570/3449]  eta: 0:55:20  lr: 0.000100  loss: 1.7329 (1.7438)  time: 3.9121  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2580/3449]  eta: 0:54:42  lr: 0.000100  loss: 1.7329 (1.7429)  time: 3.8020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2590/3449]  eta: 0:54:04  lr: 0.000100  loss: 1.4695 (1.7417)  time: 3.7659  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2600/3449]  eta: 0:53:27  lr: 0.000100  loss: 1.5459 (1.7405)  time: 3.8150  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2610/3449]  eta: 0:52:49  lr: 0.000100  loss: 1.3361 (1.7384)  time: 3.7998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2620/3449]  eta: 0:52:11  lr: 0.000100  loss: 1.6531 (1.7384)  time: 3.7913  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2630/3449]  eta: 0:51:34  lr: 0.000100  loss: 1.6669 (1.7378)  time: 3.8085  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2640/3449]  eta: 0:50:56  lr: 0.000100  loss: 1.5329 (1.7363)  time: 3.7470  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2650/3449]  eta: 0:50:18  lr: 0.000100  loss: 1.5424 (1.7361)  time: 3.7420  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2660/3449]  eta: 0:49:40  lr: 0.000100  loss: 1.7389 (1.7369)  time: 3.7567  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2670/3449]  eta: 0:49:02  lr: 0.000100  loss: 1.6463 (1.7369)  time: 3.7724  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2680/3449]  eta: 0:48:25  lr: 0.000100  loss: 1.5730 (1.7374)  time: 3.8134  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2690/3449]  eta: 0:47:47  lr: 0.000100  loss: 1.9457 (1.7373)  time: 3.7700  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2700/3449]  eta: 0:47:09  lr: 0.000100  loss: 1.9510 (1.7386)  time: 3.7390  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2710/3449]  eta: 0:46:31  lr: 0.000100  loss: 1.8409 (1.7381)  time: 3.7176  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2720/3449]  eta: 0:45:53  lr: 0.000100  loss: 1.6068 (1.7380)  time: 3.7194  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2730/3449]  eta: 0:45:15  lr: 0.000100  loss: 1.5721 (1.7371)  time: 3.7237  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [2740/3449]  eta: 0:44:37  lr: 0.000100  loss: 1.4451 (1.7370)  time: 3.7293  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2750/3449]  eta: 0:43:59  lr: 0.000100  loss: 1.8129 (1.7371)  time: 3.7833  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2760/3449]  eta: 0:43:22  lr: 0.000100  loss: 1.7320 (1.7369)  time: 3.8644  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2770/3449]  eta: 0:42:44  lr: 0.000100  loss: 1.5980 (1.7365)  time: 3.8285  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2780/3449]  eta: 0:42:06  lr: 0.000100  loss: 1.7525 (1.7363)  time: 3.7111  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2790/3449]  eta: 0:41:28  lr: 0.000100  loss: 1.4824 (1.7349)  time: 3.7106  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2800/3449]  eta: 0:40:51  lr: 0.000100  loss: 1.6599 (1.7344)  time: 3.8447  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2810/3449]  eta: 0:40:13  lr: 0.000100  loss: 1.7005 (1.7334)  time: 3.8732  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2820/3449]  eta: 0:39:35  lr: 0.000100  loss: 1.8296 (1.7345)  time: 3.7421  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2830/3449]  eta: 0:38:57  lr: 0.000100  loss: 1.8697 (1.7344)  time: 3.7070  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2840/3449]  eta: 0:38:20  lr: 0.000100  loss: 1.4873 (1.7335)  time: 3.7608  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2850/3449]  eta: 0:37:42  lr: 0.000100  loss: 1.6921 (1.7342)  time: 3.7738  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2860/3449]  eta: 0:37:04  lr: 0.000100  loss: 1.6735 (1.7329)  time: 3.7798  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2870/3449]  eta: 0:36:27  lr: 0.000100  loss: 1.3127 (1.7318)  time: 3.8699  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2880/3449]  eta: 0:35:49  lr: 0.000100  loss: 1.7749 (1.7319)  time: 3.8439  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2890/3449]  eta: 0:35:11  lr: 0.000100  loss: 1.7266 (1.7308)  time: 3.8216  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2900/3449]  eta: 0:34:33  lr: 0.000100  loss: 1.6926 (1.7304)  time: 3.8006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2910/3449]  eta: 0:33:55  lr: 0.000100  loss: 1.6745 (1.7296)  time: 3.7106  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2920/3449]  eta: 0:33:18  lr: 0.000100  loss: 1.6618 (1.7301)  time: 3.7716  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2930/3449]  eta: 0:32:40  lr: 0.000100  loss: 1.6716 (1.7300)  time: 3.8348  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2940/3449]  eta: 0:32:02  lr: 0.000100  loss: 1.6593 (1.7297)  time: 3.8107  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [2950/3449]  eta: 0:31:25  lr: 0.000100  loss: 1.5679 (1.7294)  time: 3.7841  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2960/3449]  eta: 0:30:47  lr: 0.000100  loss: 1.8379 (1.7300)  time: 3.7085  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2970/3449]  eta: 0:30:09  lr: 0.000100  loss: 1.7905 (1.7288)  time: 3.6956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2980/3449]  eta: 0:29:31  lr: 0.000100  loss: 1.6315 (1.7288)  time: 3.6571  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [2990/3449]  eta: 0:28:53  lr: 0.000100  loss: 1.7457 (1.7289)  time: 3.6638  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3000/3449]  eta: 0:28:15  lr: 0.000100  loss: 1.8081 (1.7292)  time: 3.7765  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3010/3449]  eta: 0:27:37  lr: 0.000100  loss: 1.6136 (1.7286)  time: 3.8200  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3020/3449]  eta: 0:27:00  lr: 0.000100  loss: 1.5477 (1.7284)  time: 3.8027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3030/3449]  eta: 0:26:22  lr: 0.000100  loss: 1.6607 (1.7289)  time: 3.7142  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3040/3449]  eta: 0:25:44  lr: 0.000100  loss: 1.9707 (1.7294)  time: 3.6639  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3050/3449]  eta: 0:25:06  lr: 0.000100  loss: 1.9469 (1.7285)  time: 3.7130  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3060/3449]  eta: 0:24:28  lr: 0.000100  loss: 1.6347 (1.7286)  time: 3.6646  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3070/3449]  eta: 0:23:50  lr: 0.000100  loss: 1.7586 (1.7286)  time: 3.5824  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3080/3449]  eta: 0:23:12  lr: 0.000100  loss: 1.7809 (1.7285)  time: 3.6489  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3090/3449]  eta: 0:22:34  lr: 0.000100  loss: 1.7609 (1.7281)  time: 3.6908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3100/3449]  eta: 0:21:57  lr: 0.000100  loss: 1.7609 (1.7283)  time: 3.7360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3110/3449]  eta: 0:21:19  lr: 0.000100  loss: 1.9284 (1.7286)  time: 3.6958  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3120/3449]  eta: 0:20:41  lr: 0.000100  loss: 1.7551 (1.7281)  time: 3.6696  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3130/3449]  eta: 0:20:03  lr: 0.000100  loss: 1.7551 (1.7278)  time: 3.7843  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3140/3449]  eta: 0:19:25  lr: 0.000100  loss: 1.7044 (1.7278)  time: 3.7565  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3150/3449]  eta: 0:18:48  lr: 0.000100  loss: 1.6206 (1.7267)  time: 3.7791  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3160/3449]  eta: 0:18:10  lr: 0.000100  loss: 1.7256 (1.7269)  time: 3.7347  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3170/3449]  eta: 0:17:32  lr: 0.000100  loss: 1.8008 (1.7270)  time: 3.7705  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3180/3449]  eta: 0:16:55  lr: 0.000100  loss: 1.7899 (1.7270)  time: 3.8254  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3190/3449]  eta: 0:16:17  lr: 0.000100  loss: 1.8762 (1.7274)  time: 3.6955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3200/3449]  eta: 0:15:39  lr: 0.000100  loss: 1.6293 (1.7264)  time: 3.7347  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3210/3449]  eta: 0:15:01  lr: 0.000100  loss: 1.6293 (1.7263)  time: 3.7897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3220/3449]  eta: 0:14:24  lr: 0.000100  loss: 1.7122 (1.7259)  time: 3.7899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3230/3449]  eta: 0:13:46  lr: 0.000100  loss: 1.7969 (1.7266)  time: 3.7775  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [3240/3449]  eta: 0:13:08  lr: 0.000100  loss: 1.8339 (1.7268)  time: 3.8038  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [3250/3449]  eta: 0:12:30  lr: 0.000100  loss: 1.4919 (1.7266)  time: 3.8232  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [3260/3449]  eta: 0:11:53  lr: 0.000100  loss: 1.3578 (1.7249)  time: 3.8250  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3270/3449]  eta: 0:11:15  lr: 0.000100  loss: 1.5011 (1.7246)  time: 3.8360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3280/3449]  eta: 0:10:37  lr: 0.000100  loss: 1.6003 (1.7240)  time: 3.8432  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3290/3449]  eta: 0:10:00  lr: 0.000100  loss: 1.5975 (1.7236)  time: 3.8262  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3300/3449]  eta: 0:09:22  lr: 0.000100  loss: 1.7209 (1.7237)  time: 3.7960  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3310/3449]  eta: 0:08:44  lr: 0.000100  loss: 1.9177 (1.7243)  time: 3.7741  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3320/3449]  eta: 0:08:06  lr: 0.000100  loss: 1.9358 (1.7247)  time: 3.7725  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3330/3449]  eta: 0:07:29  lr: 0.000100  loss: 1.7311 (1.7246)  time: 3.7727  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3340/3449]  eta: 0:06:51  lr: 0.000100  loss: 1.8861 (1.7254)  time: 3.7331  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3350/3449]  eta: 0:06:13  lr: 0.000100  loss: 1.8861 (1.7251)  time: 3.8435  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3360/3449]  eta: 0:05:35  lr: 0.000100  loss: 1.7053 (1.7246)  time: 3.8713  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3370/3449]  eta: 0:04:58  lr: 0.000100  loss: 1.2082 (1.7232)  time: 3.7909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3380/3449]  eta: 0:04:20  lr: 0.000100  loss: 1.3439 (1.7226)  time: 3.7761  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3390/3449]  eta: 0:03:42  lr: 0.000100  loss: 1.6187 (1.7225)  time: 3.7341  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3400/3449]  eta: 0:03:04  lr: 0.000100  loss: 1.7255 (1.7226)  time: 3.7631  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [3410/3449]  eta: 0:02:27  lr: 0.000100  loss: 1.7714 (1.7224)  time: 3.7797  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3420/3449]  eta: 0:01:49  lr: 0.000100  loss: 1.6743 (1.7221)  time: 3.7513  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3430/3449]  eta: 0:01:11  lr: 0.000100  loss: 1.8229 (1.7225)  time: 3.7221  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12]  [3440/3449]  eta: 0:00:33  lr: 0.000100  loss: 1.9693 (1.7231)  time: 3.7905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:12]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 1.8289 (1.7229)  time: 3.8705  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:12] Total time: 3:36:59 (3.7750 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 1.8289 (1.7229)\n",
      "Valid: [epoch:12]  [ 0/14]  eta: 0:04:22  loss: 1.5842 (1.5842)  time: 18.7615  data: 0.5569  max mem: 34968\n",
      "Valid: [epoch:12]  [13/14]  eta: 0:00:18  loss: 1.3059 (1.3730)  time: 18.2723  data: 0.0400  max mem: 34968\n",
      "Valid: [epoch:12] Total time: 0:04:15 (18.2854 s / it)\n",
      "Averaged stats: loss: 1.3059 (1.3730)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_12_input_n_20.png\n",
      "loss of the network on the 14 valid images: 1.373%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "Train: [epoch:13]  [   0/3449]  eta: 5:17:38  lr: 0.000100  loss: 2.5289 (2.5289)  time: 5.5258  data: 1.6738  max mem: 34968\n",
      "Train: [epoch:13]  [  10/3449]  eta: 3:45:03  lr: 0.000100  loss: 2.2675 (2.1234)  time: 3.9267  data: 0.1523  max mem: 34968\n",
      "Train: [epoch:13]  [  20/3449]  eta: 3:36:58  lr: 0.000100  loss: 2.0906 (2.0588)  time: 3.7102  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [  30/3449]  eta: 3:36:57  lr: 0.000100  loss: 1.8358 (1.9099)  time: 3.7417  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [  40/3449]  eta: 3:36:53  lr: 0.000100  loss: 1.8214 (1.8290)  time: 3.8393  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [  50/3449]  eta: 3:36:25  lr: 0.000100  loss: 1.7853 (1.8069)  time: 3.8403  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [  60/3449]  eta: 3:36:58  lr: 0.000100  loss: 1.8975 (1.8368)  time: 3.8902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [  70/3449]  eta: 3:37:29  lr: 0.000100  loss: 1.8425 (1.7857)  time: 3.9679  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [  80/3449]  eta: 3:35:33  lr: 0.000100  loss: 1.5469 (1.7384)  time: 3.8313  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [  90/3449]  eta: 3:34:03  lr: 0.000100  loss: 2.1094 (1.7907)  time: 3.6872  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [ 100/3449]  eta: 3:33:28  lr: 0.000100  loss: 2.1126 (1.7850)  time: 3.7670  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 110/3449]  eta: 3:33:03  lr: 0.000100  loss: 1.3787 (1.7312)  time: 3.8509  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 120/3449]  eta: 3:33:01  lr: 0.000100  loss: 1.4736 (1.7306)  time: 3.9136  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 130/3449]  eta: 3:32:18  lr: 0.000100  loss: 1.4736 (1.7104)  time: 3.8905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 140/3449]  eta: 3:31:58  lr: 0.000100  loss: 1.4498 (1.7104)  time: 3.8690  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 150/3449]  eta: 3:31:19  lr: 0.000100  loss: 1.6457 (1.7089)  time: 3.8794  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 160/3449]  eta: 3:30:39  lr: 0.000100  loss: 1.6401 (1.7058)  time: 3.8377  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 170/3449]  eta: 3:29:51  lr: 0.000100  loss: 1.7458 (1.7113)  time: 3.8159  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 180/3449]  eta: 3:28:41  lr: 0.000100  loss: 1.6177 (1.6955)  time: 3.7290  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 190/3449]  eta: 3:27:35  lr: 0.000100  loss: 1.6394 (1.7005)  time: 3.6640  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 200/3449]  eta: 3:26:46  lr: 0.000100  loss: 2.1043 (1.7257)  time: 3.7138  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 210/3449]  eta: 3:26:13  lr: 0.000100  loss: 2.2090 (1.7310)  time: 3.8054  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 220/3449]  eta: 3:25:37  lr: 0.000100  loss: 2.0705 (1.7465)  time: 3.8427  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 230/3449]  eta: 3:24:59  lr: 0.000100  loss: 2.0231 (1.7569)  time: 3.8276  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 240/3449]  eta: 3:24:20  lr: 0.000100  loss: 1.9366 (1.7499)  time: 3.8181  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 250/3449]  eta: 3:23:29  lr: 0.000100  loss: 2.1480 (1.7695)  time: 3.7683  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 260/3449]  eta: 3:22:38  lr: 0.000100  loss: 2.1480 (1.7684)  time: 3.7158  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 270/3449]  eta: 3:21:39  lr: 0.000100  loss: 1.8743 (1.7710)  time: 3.6745  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 280/3449]  eta: 3:21:01  lr: 0.000100  loss: 1.7823 (1.7668)  time: 3.7236  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 290/3449]  eta: 3:20:22  lr: 0.000100  loss: 1.7823 (1.7638)  time: 3.8000  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 300/3449]  eta: 3:19:35  lr: 0.000100  loss: 1.8129 (1.7640)  time: 3.7555  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 310/3449]  eta: 3:18:53  lr: 0.000100  loss: 1.7205 (1.7590)  time: 3.7432  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 320/3449]  eta: 3:18:14  lr: 0.000100  loss: 1.7895 (1.7611)  time: 3.7773  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 330/3449]  eta: 3:17:38  lr: 0.000100  loss: 1.8239 (1.7633)  time: 3.8069  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 340/3449]  eta: 3:16:50  lr: 0.000100  loss: 1.7744 (1.7622)  time: 3.7618  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 350/3449]  eta: 3:16:18  lr: 0.000100  loss: 1.6390 (1.7578)  time: 3.7814  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 360/3449]  eta: 3:15:44  lr: 0.000100  loss: 1.8373 (1.7646)  time: 3.8553  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 370/3449]  eta: 3:15:11  lr: 0.000100  loss: 1.9901 (1.7664)  time: 3.8525  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 380/3449]  eta: 3:14:23  lr: 0.000100  loss: 1.8198 (1.7665)  time: 3.7732  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 390/3449]  eta: 3:13:43  lr: 0.000100  loss: 1.8141 (1.7683)  time: 3.7289  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 400/3449]  eta: 3:13:16  lr: 0.000100  loss: 1.8958 (1.7693)  time: 3.8549  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 410/3449]  eta: 3:12:38  lr: 0.000100  loss: 1.9475 (1.7724)  time: 3.8709  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 420/3449]  eta: 3:11:58  lr: 0.000100  loss: 2.1148 (1.7733)  time: 3.7919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 430/3449]  eta: 3:11:17  lr: 0.000100  loss: 1.6780 (1.7686)  time: 3.7684  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 440/3449]  eta: 3:10:44  lr: 0.000100  loss: 1.8719 (1.7777)  time: 3.8175  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 450/3449]  eta: 3:10:00  lr: 0.000100  loss: 1.8719 (1.7716)  time: 3.7948  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 460/3449]  eta: 3:09:26  lr: 0.000100  loss: 1.6230 (1.7699)  time: 3.7864  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 470/3449]  eta: 3:08:50  lr: 0.000100  loss: 1.6348 (1.7615)  time: 3.8500  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 480/3449]  eta: 3:08:15  lr: 0.000100  loss: 0.6113 (1.7336)  time: 3.8431  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 490/3449]  eta: 3:07:37  lr: 0.000100  loss: 0.3459 (1.7054)  time: 3.8284  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 500/3449]  eta: 3:06:52  lr: 0.000100  loss: 0.3301 (1.6787)  time: 3.7489  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 510/3449]  eta: 3:06:15  lr: 0.000100  loss: 0.3728 (1.6544)  time: 3.7570  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 520/3449]  eta: 3:05:42  lr: 0.000100  loss: 0.2622 (1.6269)  time: 3.8541  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 530/3449]  eta: 3:05:04  lr: 0.000100  loss: 0.1950 (1.6007)  time: 3.8504  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 540/3449]  eta: 3:04:31  lr: 0.000100  loss: 0.2104 (1.5751)  time: 3.8519  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 550/3449]  eta: 3:03:50  lr: 0.000100  loss: 0.1971 (1.5501)  time: 3.8154  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 560/3449]  eta: 3:03:14  lr: 0.000100  loss: 0.1825 (1.5261)  time: 3.7969  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 570/3449]  eta: 3:02:34  lr: 0.000100  loss: 0.2061 (1.5037)  time: 3.8150  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 580/3449]  eta: 3:02:00  lr: 0.000100  loss: 0.2325 (1.4820)  time: 3.8287  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 590/3449]  eta: 3:01:18  lr: 0.000100  loss: 0.2490 (1.4616)  time: 3.7981  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 600/3449]  eta: 3:00:35  lr: 0.000100  loss: 0.1731 (1.4398)  time: 3.7154  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 610/3449]  eta: 2:59:58  lr: 0.000100  loss: 0.1004 (1.4178)  time: 3.7685  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 620/3449]  eta: 2:59:17  lr: 0.000100  loss: 0.0958 (1.3975)  time: 3.7822  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 630/3449]  eta: 2:58:40  lr: 0.000100  loss: 0.1476 (1.3781)  time: 3.7792  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 640/3449]  eta: 2:58:09  lr: 0.000100  loss: 0.1733 (1.3603)  time: 3.8974  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 650/3449]  eta: 2:57:35  lr: 0.000100  loss: 0.2040 (1.3426)  time: 3.9321  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 660/3449]  eta: 2:56:56  lr: 0.000100  loss: 0.1927 (1.3253)  time: 3.8379  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 670/3449]  eta: 2:56:18  lr: 0.000100  loss: 0.1457 (1.3076)  time: 3.7970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 680/3449]  eta: 2:55:45  lr: 0.000100  loss: 0.0844 (1.2895)  time: 3.8739  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 690/3449]  eta: 2:55:08  lr: 0.000100  loss: 0.0936 (1.2740)  time: 3.8824  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 700/3449]  eta: 2:54:31  lr: 0.000100  loss: 0.1835 (1.2588)  time: 3.8266  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 710/3449]  eta: 2:53:53  lr: 0.000100  loss: 0.1231 (1.2425)  time: 3.8222  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 720/3449]  eta: 2:53:14  lr: 0.000100  loss: 0.1117 (1.2269)  time: 3.8036  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 730/3449]  eta: 2:52:41  lr: 0.000100  loss: 0.1172 (1.2118)  time: 3.8649  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 740/3449]  eta: 2:52:04  lr: 0.000100  loss: 0.1122 (1.1968)  time: 3.8943  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 750/3449]  eta: 2:51:27  lr: 0.000100  loss: 0.0936 (1.1820)  time: 3.8422  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [ 760/3449]  eta: 2:50:47  lr: 0.000100  loss: 0.0936 (1.1681)  time: 3.7957  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 770/3449]  eta: 2:50:07  lr: 0.000100  loss: 0.0938 (1.1541)  time: 3.7582  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 780/3449]  eta: 2:49:31  lr: 0.000100  loss: 0.0838 (1.1403)  time: 3.8110  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 790/3449]  eta: 2:48:52  lr: 0.000100  loss: 0.0679 (1.1271)  time: 3.8269  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 800/3449]  eta: 2:48:15  lr: 0.000100  loss: 0.1062 (1.1148)  time: 3.8208  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 810/3449]  eta: 2:47:41  lr: 0.000100  loss: 0.1062 (1.1023)  time: 3.8982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 820/3449]  eta: 2:47:00  lr: 0.000100  loss: 0.1141 (1.0912)  time: 3.8357  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 830/3449]  eta: 2:46:16  lr: 0.000100  loss: 0.1496 (1.0801)  time: 3.6733  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 840/3449]  eta: 2:45:37  lr: 0.000100  loss: 0.1427 (1.0686)  time: 3.6961  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 850/3449]  eta: 2:45:00  lr: 0.000100  loss: 0.1373 (1.0580)  time: 3.8095  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 860/3449]  eta: 2:44:22  lr: 0.000100  loss: 0.1445 (1.0474)  time: 3.8317  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 870/3449]  eta: 2:43:45  lr: 0.000100  loss: 0.1328 (1.0368)  time: 3.8237  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 880/3449]  eta: 2:43:06  lr: 0.000100  loss: 0.1162 (1.0262)  time: 3.8048  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 890/3449]  eta: 2:42:26  lr: 0.000100  loss: 0.1479 (1.0172)  time: 3.7733  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 900/3449]  eta: 2:41:48  lr: 0.000100  loss: 0.1746 (1.0076)  time: 3.7847  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 910/3449]  eta: 2:41:13  lr: 0.000100  loss: 0.0855 (0.9972)  time: 3.8591  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 920/3449]  eta: 2:40:38  lr: 0.000100  loss: 0.0582 (0.9871)  time: 3.9221  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 930/3449]  eta: 2:39:59  lr: 0.000100  loss: 0.0895 (0.9783)  time: 3.8437  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 940/3449]  eta: 2:39:22  lr: 0.000100  loss: 0.1577 (0.9700)  time: 3.8095  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 950/3449]  eta: 2:38:45  lr: 0.000100  loss: 0.1893 (0.9622)  time: 3.8567  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 960/3449]  eta: 2:38:08  lr: 0.000100  loss: 0.1893 (0.9545)  time: 3.8480  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [ 970/3449]  eta: 2:37:28  lr: 0.000100  loss: 0.1767 (0.9466)  time: 3.7933  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 980/3449]  eta: 2:36:49  lr: 0.000100  loss: 0.1571 (0.9384)  time: 3.7635  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [ 990/3449]  eta: 2:36:09  lr: 0.000100  loss: 0.0961 (0.9300)  time: 3.7624  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1000/3449]  eta: 2:35:32  lr: 0.000100  loss: 0.0919 (0.9218)  time: 3.8040  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1010/3449]  eta: 2:34:53  lr: 0.000100  loss: 0.1107 (0.9140)  time: 3.8131  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1020/3449]  eta: 2:34:18  lr: 0.000100  loss: 0.1207 (0.9062)  time: 3.8490  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1030/3449]  eta: 2:33:41  lr: 0.000100  loss: 0.0949 (0.8983)  time: 3.9009  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1040/3449]  eta: 2:33:02  lr: 0.000100  loss: 0.0935 (0.8910)  time: 3.8164  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1050/3449]  eta: 2:32:23  lr: 0.000100  loss: 0.2171 (0.8854)  time: 3.7713  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1060/3449]  eta: 2:31:44  lr: 0.000100  loss: 0.1838 (0.8787)  time: 3.7855  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1070/3449]  eta: 2:31:06  lr: 0.000100  loss: 0.1279 (0.8714)  time: 3.8063  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1080/3449]  eta: 2:30:27  lr: 0.000100  loss: 0.0701 (0.8639)  time: 3.7883  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1090/3449]  eta: 2:29:47  lr: 0.000100  loss: 0.0600 (0.8565)  time: 3.7404  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1100/3449]  eta: 2:29:10  lr: 0.000100  loss: 0.0671 (0.8495)  time: 3.7804  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1110/3449]  eta: 2:28:31  lr: 0.000100  loss: 0.0855 (0.8427)  time: 3.8075  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1120/3449]  eta: 2:27:55  lr: 0.000100  loss: 0.1064 (0.8362)  time: 3.8443  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1130/3449]  eta: 2:27:15  lr: 0.000100  loss: 0.0634 (0.8293)  time: 3.8106  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1140/3449]  eta: 2:26:40  lr: 0.000100  loss: 0.0426 (0.8224)  time: 3.8382  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1150/3449]  eta: 2:26:03  lr: 0.000100  loss: 0.0422 (0.8157)  time: 3.9138  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1160/3449]  eta: 2:25:25  lr: 0.000100  loss: 0.0403 (0.8090)  time: 3.8552  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1170/3449]  eta: 2:24:46  lr: 0.000100  loss: 0.0489 (0.8028)  time: 3.8095  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1180/3449]  eta: 2:24:07  lr: 0.000100  loss: 0.0703 (0.7967)  time: 3.7692  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1190/3449]  eta: 2:23:30  lr: 0.000100  loss: 0.0661 (0.7905)  time: 3.7956  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1200/3449]  eta: 2:22:53  lr: 0.000100  loss: 0.0653 (0.7847)  time: 3.8715  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1210/3449]  eta: 2:22:15  lr: 0.000100  loss: 0.1117 (0.7794)  time: 3.8474  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1220/3449]  eta: 2:21:36  lr: 0.000100  loss: 0.1425 (0.7748)  time: 3.7752  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1230/3449]  eta: 2:20:56  lr: 0.000100  loss: 0.1475 (0.7696)  time: 3.7497  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1240/3449]  eta: 2:20:18  lr: 0.000100  loss: 0.0979 (0.7640)  time: 3.7525  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1250/3449]  eta: 2:19:40  lr: 0.000100  loss: 0.0624 (0.7587)  time: 3.8082  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1260/3449]  eta: 2:19:01  lr: 0.000100  loss: 0.1250 (0.7542)  time: 3.7958  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1270/3449]  eta: 2:18:21  lr: 0.000100  loss: 0.1287 (0.7492)  time: 3.7422  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1280/3449]  eta: 2:17:43  lr: 0.000100  loss: 0.1035 (0.7443)  time: 3.7668  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1290/3449]  eta: 2:17:05  lr: 0.000100  loss: 0.0769 (0.7390)  time: 3.8011  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1300/3449]  eta: 2:16:26  lr: 0.000100  loss: 0.0640 (0.7339)  time: 3.7915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1310/3449]  eta: 2:15:49  lr: 0.000100  loss: 0.1088 (0.7299)  time: 3.8171  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1320/3449]  eta: 2:15:13  lr: 0.000100  loss: 0.1448 (0.7253)  time: 3.8938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1330/3449]  eta: 2:14:34  lr: 0.000100  loss: 0.1075 (0.7207)  time: 3.8464  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1340/3449]  eta: 2:13:53  lr: 0.000100  loss: 0.0887 (0.7159)  time: 3.7096  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1350/3449]  eta: 2:13:15  lr: 0.000100  loss: 0.0887 (0.7114)  time: 3.7301  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1360/3449]  eta: 2:12:37  lr: 0.000100  loss: 0.1023 (0.7069)  time: 3.8010  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1370/3449]  eta: 2:11:59  lr: 0.000100  loss: 0.0829 (0.7023)  time: 3.8265  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1380/3449]  eta: 2:11:23  lr: 0.000100  loss: 0.0807 (0.6978)  time: 3.8767  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1390/3449]  eta: 2:10:45  lr: 0.000100  loss: 0.0665 (0.6933)  time: 3.8759  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1400/3449]  eta: 2:10:08  lr: 0.000100  loss: 0.0651 (0.6889)  time: 3.8715  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1410/3449]  eta: 2:09:29  lr: 0.000100  loss: 0.0875 (0.6848)  time: 3.8277  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [1420/3449]  eta: 2:08:50  lr: 0.000100  loss: 0.1114 (0.6807)  time: 3.7443  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1430/3449]  eta: 2:08:11  lr: 0.000100  loss: 0.0808 (0.6764)  time: 3.7457  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1440/3449]  eta: 2:07:32  lr: 0.000100  loss: 0.0526 (0.6720)  time: 3.7342  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1450/3449]  eta: 2:06:53  lr: 0.000100  loss: 0.0706 (0.6684)  time: 3.7465  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1460/3449]  eta: 2:06:15  lr: 0.000100  loss: 0.0928 (0.6646)  time: 3.7901  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1470/3449]  eta: 2:05:36  lr: 0.000100  loss: 0.0897 (0.6607)  time: 3.7428  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1480/3449]  eta: 2:04:59  lr: 0.000100  loss: 0.0870 (0.6569)  time: 3.7971  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1490/3449]  eta: 2:04:20  lr: 0.000100  loss: 0.0871 (0.6533)  time: 3.8479  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1500/3449]  eta: 2:03:43  lr: 0.000100  loss: 0.0569 (0.6493)  time: 3.8152  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1510/3449]  eta: 2:03:05  lr: 0.000100  loss: 0.0404 (0.6453)  time: 3.8230  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1520/3449]  eta: 2:02:27  lr: 0.000100  loss: 0.0738 (0.6420)  time: 3.8103  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1530/3449]  eta: 2:01:48  lr: 0.000100  loss: 0.0835 (0.6383)  time: 3.7920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1540/3449]  eta: 2:01:10  lr: 0.000100  loss: 0.0532 (0.6345)  time: 3.7912  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1550/3449]  eta: 2:00:31  lr: 0.000100  loss: 0.0332 (0.6306)  time: 3.7921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1560/3449]  eta: 1:59:53  lr: 0.000100  loss: 0.0426 (0.6271)  time: 3.7702  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1570/3449]  eta: 1:59:16  lr: 0.000100  loss: 0.0816 (0.6236)  time: 3.8216  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1580/3449]  eta: 1:58:38  lr: 0.000100  loss: 0.0942 (0.6207)  time: 3.8439  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1590/3449]  eta: 1:57:59  lr: 0.000100  loss: 0.1274 (0.6175)  time: 3.7810  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1600/3449]  eta: 1:57:22  lr: 0.000100  loss: 0.1005 (0.6143)  time: 3.8291  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1610/3449]  eta: 1:56:43  lr: 0.000100  loss: 0.0819 (0.6109)  time: 3.8246  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1620/3449]  eta: 1:56:05  lr: 0.000100  loss: 0.0566 (0.6074)  time: 3.7833  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1630/3449]  eta: 1:55:27  lr: 0.000100  loss: 0.0607 (0.6042)  time: 3.8323  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1640/3449]  eta: 1:54:49  lr: 0.000100  loss: 0.0772 (0.6011)  time: 3.8129  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1650/3449]  eta: 1:54:10  lr: 0.000100  loss: 0.0711 (0.5979)  time: 3.7596  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1660/3449]  eta: 1:53:33  lr: 0.000100  loss: 0.0656 (0.5947)  time: 3.8035  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1670/3449]  eta: 1:52:54  lr: 0.000100  loss: 0.0721 (0.5916)  time: 3.8004  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1680/3449]  eta: 1:52:15  lr: 0.000100  loss: 0.0966 (0.5890)  time: 3.7340  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1690/3449]  eta: 1:51:37  lr: 0.000100  loss: 0.1296 (0.5862)  time: 3.7728  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1700/3449]  eta: 1:50:58  lr: 0.000100  loss: 0.0579 (0.5831)  time: 3.7834  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1710/3449]  eta: 1:50:20  lr: 0.000100  loss: 0.0460 (0.5799)  time: 3.7730  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1720/3449]  eta: 1:49:41  lr: 0.000100  loss: 0.0460 (0.5770)  time: 3.7648  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1730/3449]  eta: 1:49:04  lr: 0.000100  loss: 0.0871 (0.5741)  time: 3.7867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1740/3449]  eta: 1:48:25  lr: 0.000100  loss: 0.0907 (0.5713)  time: 3.7781  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1750/3449]  eta: 1:47:47  lr: 0.000100  loss: 0.0769 (0.5685)  time: 3.7705  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1760/3449]  eta: 1:47:08  lr: 0.000100  loss: 0.0765 (0.5657)  time: 3.7775  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1770/3449]  eta: 1:46:30  lr: 0.000100  loss: 0.0795 (0.5630)  time: 3.7518  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1780/3449]  eta: 1:45:51  lr: 0.000100  loss: 0.0527 (0.5601)  time: 3.7515  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1790/3449]  eta: 1:45:13  lr: 0.000100  loss: 0.0348 (0.5572)  time: 3.7793  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1800/3449]  eta: 1:44:34  lr: 0.000100  loss: 0.0487 (0.5544)  time: 3.7858  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1810/3449]  eta: 1:43:58  lr: 0.000100  loss: 0.0487 (0.5516)  time: 3.8474  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1820/3449]  eta: 1:43:20  lr: 0.000100  loss: 0.0360 (0.5487)  time: 3.9253  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1830/3449]  eta: 1:42:40  lr: 0.000100  loss: 0.0367 (0.5464)  time: 3.7498  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1840/3449]  eta: 1:42:03  lr: 0.000100  loss: 0.1112 (0.5440)  time: 3.7317  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1850/3449]  eta: 1:41:23  lr: 0.000100  loss: 0.0868 (0.5415)  time: 3.7505  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1860/3449]  eta: 1:40:45  lr: 0.000100  loss: 0.0744 (0.5390)  time: 3.7142  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1870/3449]  eta: 1:40:09  lr: 0.000100  loss: 0.0926 (0.5367)  time: 3.8782  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1880/3449]  eta: 1:39:30  lr: 0.000100  loss: 0.0679 (0.5342)  time: 3.8428  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1890/3449]  eta: 1:38:52  lr: 0.000100  loss: 0.0429 (0.5316)  time: 3.7828  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1900/3449]  eta: 1:38:14  lr: 0.000100  loss: 0.0429 (0.5292)  time: 3.8234  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1910/3449]  eta: 1:37:35  lr: 0.000100  loss: 0.0654 (0.5267)  time: 3.7715  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1920/3449]  eta: 1:36:57  lr: 0.000100  loss: 0.0425 (0.5241)  time: 3.7383  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1930/3449]  eta: 1:36:19  lr: 0.000100  loss: 0.0319 (0.5216)  time: 3.7915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1940/3449]  eta: 1:35:41  lr: 0.000100  loss: 0.0342 (0.5192)  time: 3.8046  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1950/3449]  eta: 1:35:02  lr: 0.000100  loss: 0.0342 (0.5168)  time: 3.7601  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [1960/3449]  eta: 1:34:24  lr: 0.000100  loss: 0.0457 (0.5146)  time: 3.7872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1970/3449]  eta: 1:33:46  lr: 0.000100  loss: 0.0612 (0.5123)  time: 3.7736  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1980/3449]  eta: 1:33:08  lr: 0.000100  loss: 0.0674 (0.5100)  time: 3.7760  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [1990/3449]  eta: 1:32:30  lr: 0.000100  loss: 0.0711 (0.5079)  time: 3.7992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2000/3449]  eta: 1:31:52  lr: 0.000100  loss: 0.1222 (0.5062)  time: 3.8159  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2010/3449]  eta: 1:31:14  lr: 0.000100  loss: 0.1599 (0.5043)  time: 3.8414  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2020/3449]  eta: 1:30:36  lr: 0.000100  loss: 0.1009 (0.5023)  time: 3.8321  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2030/3449]  eta: 1:29:58  lr: 0.000100  loss: 0.0894 (0.5002)  time: 3.7981  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2040/3449]  eta: 1:29:20  lr: 0.000100  loss: 0.0687 (0.4982)  time: 3.7607  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2050/3449]  eta: 1:28:42  lr: 0.000100  loss: 0.0664 (0.4961)  time: 3.8410  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2060/3449]  eta: 1:28:04  lr: 0.000100  loss: 0.0816 (0.4943)  time: 3.8298  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2070/3449]  eta: 1:27:25  lr: 0.000100  loss: 0.0965 (0.4923)  time: 3.7473  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [2080/3449]  eta: 1:26:47  lr: 0.000100  loss: 0.0685 (0.4902)  time: 3.7814  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2090/3449]  eta: 1:26:08  lr: 0.000100  loss: 0.0605 (0.4882)  time: 3.7358  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2100/3449]  eta: 1:25:31  lr: 0.000100  loss: 0.0625 (0.4862)  time: 3.7603  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2110/3449]  eta: 1:24:52  lr: 0.000100  loss: 0.0620 (0.4842)  time: 3.8085  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2120/3449]  eta: 1:24:14  lr: 0.000100  loss: 0.0539 (0.4821)  time: 3.7639  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2130/3449]  eta: 1:23:36  lr: 0.000100  loss: 0.0462 (0.4802)  time: 3.7967  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2140/3449]  eta: 1:22:58  lr: 0.000100  loss: 0.0625 (0.4783)  time: 3.7610  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2150/3449]  eta: 1:22:19  lr: 0.000100  loss: 0.0945 (0.4767)  time: 3.7308  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2160/3449]  eta: 1:21:41  lr: 0.000100  loss: 0.0781 (0.4748)  time: 3.7440  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2170/3449]  eta: 1:21:03  lr: 0.000100  loss: 0.0512 (0.4729)  time: 3.7652  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2180/3449]  eta: 1:20:24  lr: 0.000100  loss: 0.0756 (0.4714)  time: 3.7465  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2190/3449]  eta: 1:19:46  lr: 0.000100  loss: 0.1174 (0.4697)  time: 3.6918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2200/3449]  eta: 1:19:07  lr: 0.000100  loss: 0.0477 (0.4677)  time: 3.7338  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2210/3449]  eta: 1:18:29  lr: 0.000100  loss: 0.0286 (0.4658)  time: 3.7772  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2220/3449]  eta: 1:17:51  lr: 0.000100  loss: 0.0292 (0.4638)  time: 3.7437  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2230/3449]  eta: 1:17:13  lr: 0.000100  loss: 0.0380 (0.4620)  time: 3.7458  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2240/3449]  eta: 1:16:34  lr: 0.000100  loss: 0.0739 (0.4604)  time: 3.7554  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2250/3449]  eta: 1:15:56  lr: 0.000100  loss: 0.1138 (0.4588)  time: 3.7541  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2260/3449]  eta: 1:15:18  lr: 0.000100  loss: 0.0884 (0.4572)  time: 3.7484  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2270/3449]  eta: 1:14:40  lr: 0.000100  loss: 0.0822 (0.4555)  time: 3.8035  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2280/3449]  eta: 1:14:02  lr: 0.000100  loss: 0.0648 (0.4538)  time: 3.8268  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2290/3449]  eta: 1:13:24  lr: 0.000100  loss: 0.0672 (0.4522)  time: 3.7596  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2300/3449]  eta: 1:12:46  lr: 0.000100  loss: 0.0980 (0.4507)  time: 3.8359  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2310/3449]  eta: 1:12:08  lr: 0.000100  loss: 0.1211 (0.4493)  time: 3.8068  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2320/3449]  eta: 1:11:30  lr: 0.000100  loss: 0.1192 (0.4478)  time: 3.7422  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2330/3449]  eta: 1:10:51  lr: 0.000100  loss: 0.1154 (0.4463)  time: 3.7295  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2340/3449]  eta: 1:10:13  lr: 0.000100  loss: 0.0905 (0.4448)  time: 3.7182  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2350/3449]  eta: 1:09:35  lr: 0.000100  loss: 0.0399 (0.4430)  time: 3.8218  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2360/3449]  eta: 1:08:58  lr: 0.000100  loss: 0.0382 (0.4414)  time: 3.8731  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2370/3449]  eta: 1:08:19  lr: 0.000100  loss: 0.0394 (0.4397)  time: 3.7771  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2380/3449]  eta: 1:07:41  lr: 0.000100  loss: 0.0368 (0.4381)  time: 3.7608  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2390/3449]  eta: 1:07:03  lr: 0.000100  loss: 0.0433 (0.4365)  time: 3.7733  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2400/3449]  eta: 1:06:25  lr: 0.000100  loss: 0.0686 (0.4350)  time: 3.7435  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2410/3449]  eta: 1:05:47  lr: 0.000100  loss: 0.0675 (0.4335)  time: 3.7698  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2420/3449]  eta: 1:05:09  lr: 0.000100  loss: 0.0600 (0.4320)  time: 3.7762  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2430/3449]  eta: 1:04:31  lr: 0.000100  loss: 0.0580 (0.4305)  time: 3.8080  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2440/3449]  eta: 1:03:53  lr: 0.000100  loss: 0.0655 (0.4291)  time: 3.7844  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2450/3449]  eta: 1:03:15  lr: 0.000100  loss: 0.0730 (0.4277)  time: 3.8592  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2460/3449]  eta: 1:02:38  lr: 0.000100  loss: 0.0422 (0.4261)  time: 3.9248  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2470/3449]  eta: 1:01:59  lr: 0.000100  loss: 0.0254 (0.4245)  time: 3.8313  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2480/3449]  eta: 1:01:22  lr: 0.000100  loss: 0.0281 (0.4230)  time: 3.8203  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2490/3449]  eta: 1:00:44  lr: 0.000100  loss: 0.0609 (0.4216)  time: 3.8354  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2500/3449]  eta: 1:00:06  lr: 0.000100  loss: 0.0629 (0.4202)  time: 3.7728  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2510/3449]  eta: 0:59:28  lr: 0.000100  loss: 0.0448 (0.4187)  time: 3.8399  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2520/3449]  eta: 0:58:50  lr: 0.000100  loss: 0.0300 (0.4172)  time: 3.8397  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2530/3449]  eta: 0:58:12  lr: 0.000100  loss: 0.0511 (0.4159)  time: 3.8021  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2540/3449]  eta: 0:57:34  lr: 0.000100  loss: 0.0631 (0.4145)  time: 3.7696  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2550/3449]  eta: 0:56:56  lr: 0.000100  loss: 0.0616 (0.4131)  time: 3.7285  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2560/3449]  eta: 0:56:18  lr: 0.000100  loss: 0.0588 (0.4118)  time: 3.8344  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2570/3449]  eta: 0:55:40  lr: 0.000100  loss: 0.0682 (0.4104)  time: 3.8309  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2580/3449]  eta: 0:55:02  lr: 0.000100  loss: 0.0602 (0.4091)  time: 3.7887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2590/3449]  eta: 0:54:24  lr: 0.000100  loss: 0.0449 (0.4076)  time: 3.7893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2600/3449]  eta: 0:53:46  lr: 0.000100  loss: 0.0311 (0.4062)  time: 3.8191  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2610/3449]  eta: 0:53:08  lr: 0.000100  loss: 0.0303 (0.4048)  time: 3.8033  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2620/3449]  eta: 0:52:29  lr: 0.000100  loss: 0.0390 (0.4035)  time: 3.7343  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2630/3449]  eta: 0:51:51  lr: 0.000100  loss: 0.0570 (0.4022)  time: 3.7549  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2640/3449]  eta: 0:51:14  lr: 0.000100  loss: 0.0427 (0.4008)  time: 3.8487  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2650/3449]  eta: 0:50:36  lr: 0.000100  loss: 0.0497 (0.3996)  time: 3.8227  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2660/3449]  eta: 0:49:57  lr: 0.000100  loss: 0.0573 (0.3983)  time: 3.7166  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2670/3449]  eta: 0:49:19  lr: 0.000100  loss: 0.0559 (0.3970)  time: 3.7293  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2680/3449]  eta: 0:48:41  lr: 0.000100  loss: 0.0457 (0.3958)  time: 3.7969  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2690/3449]  eta: 0:48:03  lr: 0.000100  loss: 0.0705 (0.3946)  time: 3.8378  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2700/3449]  eta: 0:47:25  lr: 0.000100  loss: 0.0911 (0.3934)  time: 3.8218  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2710/3449]  eta: 0:46:47  lr: 0.000100  loss: 0.0730 (0.3922)  time: 3.7916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2720/3449]  eta: 0:46:09  lr: 0.000100  loss: 0.0685 (0.3910)  time: 3.8062  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2730/3449]  eta: 0:45:32  lr: 0.000100  loss: 0.0711 (0.3900)  time: 3.8691  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [2740/3449]  eta: 0:44:54  lr: 0.000100  loss: 0.0948 (0.3888)  time: 3.8287  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2750/3449]  eta: 0:44:15  lr: 0.000100  loss: 0.0948 (0.3878)  time: 3.7250  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2760/3449]  eta: 0:43:37  lr: 0.000100  loss: 0.0670 (0.3866)  time: 3.7473  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2770/3449]  eta: 0:42:59  lr: 0.000100  loss: 0.0539 (0.3854)  time: 3.7272  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2780/3449]  eta: 0:42:21  lr: 0.000100  loss: 0.0521 (0.3843)  time: 3.7773  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2790/3449]  eta: 0:41:43  lr: 0.000100  loss: 0.0665 (0.3831)  time: 3.8331  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2800/3449]  eta: 0:41:05  lr: 0.000100  loss: 0.0509 (0.3819)  time: 3.8111  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2810/3449]  eta: 0:40:27  lr: 0.000100  loss: 0.0360 (0.3807)  time: 3.8038  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2820/3449]  eta: 0:39:49  lr: 0.000100  loss: 0.0336 (0.3794)  time: 3.7814  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2830/3449]  eta: 0:39:11  lr: 0.000100  loss: 0.0268 (0.3782)  time: 3.8102  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2840/3449]  eta: 0:38:33  lr: 0.000100  loss: 0.0290 (0.3770)  time: 3.7949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2850/3449]  eta: 0:37:55  lr: 0.000100  loss: 0.0402 (0.3758)  time: 3.7631  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2860/3449]  eta: 0:37:17  lr: 0.000100  loss: 0.0451 (0.3747)  time: 3.7503  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2870/3449]  eta: 0:36:39  lr: 0.000100  loss: 0.0637 (0.3736)  time: 3.7384  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2880/3449]  eta: 0:36:01  lr: 0.000100  loss: 0.0511 (0.3725)  time: 3.7008  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2890/3449]  eta: 0:35:23  lr: 0.000100  loss: 0.0624 (0.3716)  time: 3.7167  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2900/3449]  eta: 0:34:45  lr: 0.000100  loss: 0.0991 (0.3706)  time: 3.8136  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2910/3449]  eta: 0:34:07  lr: 0.000100  loss: 0.0679 (0.3695)  time: 3.8420  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2920/3449]  eta: 0:33:29  lr: 0.000100  loss: 0.0325 (0.3684)  time: 3.8542  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2930/3449]  eta: 0:32:51  lr: 0.000100  loss: 0.0288 (0.3672)  time: 3.9322  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2940/3449]  eta: 0:32:13  lr: 0.000100  loss: 0.0303 (0.3661)  time: 3.8774  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2950/3449]  eta: 0:31:35  lr: 0.000100  loss: 0.0309 (0.3650)  time: 3.7729  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2960/3449]  eta: 0:30:57  lr: 0.000100  loss: 0.0275 (0.3638)  time: 3.7757  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2970/3449]  eta: 0:30:19  lr: 0.000100  loss: 0.0384 (0.3628)  time: 3.7642  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [2980/3449]  eta: 0:29:41  lr: 0.000100  loss: 0.0727 (0.3619)  time: 3.7346  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [2990/3449]  eta: 0:29:03  lr: 0.000100  loss: 0.0546 (0.3609)  time: 3.7911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3000/3449]  eta: 0:28:25  lr: 0.000100  loss: 0.0546 (0.3600)  time: 3.8374  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3010/3449]  eta: 0:27:47  lr: 0.000100  loss: 0.0948 (0.3592)  time: 3.8226  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3020/3449]  eta: 0:27:09  lr: 0.000100  loss: 0.1280 (0.3584)  time: 3.7885  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3030/3449]  eta: 0:26:31  lr: 0.000100  loss: 0.0995 (0.3576)  time: 3.7157  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3040/3449]  eta: 0:25:53  lr: 0.000100  loss: 0.1189 (0.3569)  time: 3.7224  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3050/3449]  eta: 0:25:15  lr: 0.000100  loss: 0.1090 (0.3560)  time: 3.7356  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3060/3449]  eta: 0:24:37  lr: 0.000100  loss: 0.0572 (0.3550)  time: 3.7174  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3070/3449]  eta: 0:23:59  lr: 0.000100  loss: 0.0572 (0.3541)  time: 3.8010  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [3080/3449]  eta: 0:23:21  lr: 0.000100  loss: 0.0893 (0.3532)  time: 3.8209  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3090/3449]  eta: 0:22:43  lr: 0.000100  loss: 0.0897 (0.3524)  time: 3.7587  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3100/3449]  eta: 0:22:05  lr: 0.000100  loss: 0.1007 (0.3516)  time: 3.8104  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3110/3449]  eta: 0:21:27  lr: 0.000100  loss: 0.0416 (0.3505)  time: 3.8653  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3120/3449]  eta: 0:20:49  lr: 0.000100  loss: 0.0285 (0.3495)  time: 3.8204  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3130/3449]  eta: 0:20:11  lr: 0.000100  loss: 0.0285 (0.3485)  time: 3.7713  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3140/3449]  eta: 0:19:33  lr: 0.000100  loss: 0.0360 (0.3476)  time: 3.7681  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3150/3449]  eta: 0:18:55  lr: 0.000100  loss: 0.0506 (0.3466)  time: 3.8368  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3160/3449]  eta: 0:18:17  lr: 0.000100  loss: 0.0548 (0.3457)  time: 3.8590  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3170/3449]  eta: 0:17:39  lr: 0.000100  loss: 0.0548 (0.3448)  time: 3.8356  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3180/3449]  eta: 0:17:01  lr: 0.000100  loss: 0.0343 (0.3438)  time: 3.8247  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3190/3449]  eta: 0:16:23  lr: 0.000100  loss: 0.0289 (0.3430)  time: 3.7713  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3200/3449]  eta: 0:15:45  lr: 0.000100  loss: 0.0791 (0.3422)  time: 3.7702  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3210/3449]  eta: 0:15:07  lr: 0.000100  loss: 0.1028 (0.3414)  time: 3.8171  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [3220/3449]  eta: 0:14:29  lr: 0.000100  loss: 0.0605 (0.3405)  time: 3.7858  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [3230/3449]  eta: 0:13:51  lr: 0.000100  loss: 0.0438 (0.3396)  time: 3.7732  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3240/3449]  eta: 0:13:13  lr: 0.000100  loss: 0.0291 (0.3387)  time: 3.8908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3250/3449]  eta: 0:12:35  lr: 0.000100  loss: 0.0381 (0.3378)  time: 3.8996  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3260/3449]  eta: 0:11:58  lr: 0.000100  loss: 0.0412 (0.3370)  time: 3.8251  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3270/3449]  eta: 0:11:20  lr: 0.000100  loss: 0.0732 (0.3361)  time: 3.8154  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3280/3449]  eta: 0:10:42  lr: 0.000100  loss: 0.0534 (0.3352)  time: 3.8295  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3290/3449]  eta: 0:10:04  lr: 0.000100  loss: 0.0332 (0.3343)  time: 3.7776  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3300/3449]  eta: 0:09:26  lr: 0.000100  loss: 0.0373 (0.3335)  time: 3.8003  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3310/3449]  eta: 0:08:48  lr: 0.000100  loss: 0.0857 (0.3328)  time: 3.7969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [3320/3449]  eta: 0:08:10  lr: 0.000100  loss: 0.0857 (0.3320)  time: 3.7607  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [3330/3449]  eta: 0:07:32  lr: 0.000100  loss: 0.0753 (0.3312)  time: 3.8231  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [3340/3449]  eta: 0:06:54  lr: 0.000100  loss: 0.0840 (0.3305)  time: 3.8150  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3350/3449]  eta: 0:06:16  lr: 0.000100  loss: 0.0695 (0.3297)  time: 3.8583  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3360/3449]  eta: 0:05:38  lr: 0.000100  loss: 0.0595 (0.3289)  time: 3.9152  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [3370/3449]  eta: 0:05:00  lr: 0.000100  loss: 0.0595 (0.3281)  time: 3.8650  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [3380/3449]  eta: 0:04:22  lr: 0.000100  loss: 0.0515 (0.3274)  time: 3.7864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:13]  [3390/3449]  eta: 0:03:44  lr: 0.000100  loss: 0.0634 (0.3266)  time: 3.8040  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [3400/3449]  eta: 0:03:06  lr: 0.000100  loss: 0.0634 (0.3258)  time: 3.8010  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3410/3449]  eta: 0:02:28  lr: 0.000100  loss: 0.0985 (0.3252)  time: 3.8583  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3420/3449]  eta: 0:01:50  lr: 0.000100  loss: 0.0567 (0.3244)  time: 3.8644  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3430/3449]  eta: 0:01:12  lr: 0.000100  loss: 0.0567 (0.3236)  time: 3.7580  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3440/3449]  eta: 0:00:34  lr: 0.000100  loss: 0.0618 (0.3229)  time: 3.7759  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0625 (0.3223)  time: 3.7810  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:13] Total time: 3:38:26 (3.8002 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0625 (0.3223)\n",
      "Valid: [epoch:13]  [ 0/14]  eta: 0:04:22  loss: 0.0415 (0.0415)  time: 18.7430  data: 0.5512  max mem: 34968\n",
      "Valid: [epoch:13]  [13/14]  eta: 0:00:18  loss: 0.0416 (0.0465)  time: 18.2503  data: 0.0396  max mem: 34968\n",
      "Valid: [epoch:13] Total time: 0:04:15 (18.2631 s / it)\n",
      "Averaged stats: loss: 0.0416 (0.0465)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_13_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.046%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "Train: [epoch:14]  [   0/3449]  eta: 4:56:46  lr: 0.000100  loss: 0.0661 (0.0661)  time: 5.1627  data: 1.7061  max mem: 34968\n",
      "Train: [epoch:14]  [  10/3449]  eta: 3:38:25  lr: 0.000100  loss: 0.0593 (0.0704)  time: 3.8109  data: 0.1552  max mem: 34968\n",
      "Train: [epoch:14]  [  20/3449]  eta: 3:40:25  lr: 0.000100  loss: 0.0593 (0.0794)  time: 3.7917  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [  30/3449]  eta: 3:38:36  lr: 0.000100  loss: 0.1101 (0.0940)  time: 3.8505  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [  40/3449]  eta: 3:36:30  lr: 0.000100  loss: 0.1200 (0.0997)  time: 3.7621  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [  50/3449]  eta: 3:35:45  lr: 0.000100  loss: 0.1179 (0.1083)  time: 3.7655  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [  60/3449]  eta: 3:35:20  lr: 0.000100  loss: 0.1106 (0.1072)  time: 3.8161  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [  70/3449]  eta: 3:34:39  lr: 0.000100  loss: 0.1006 (0.1088)  time: 3.8193  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [  80/3449]  eta: 3:33:47  lr: 0.000100  loss: 0.0997 (0.1067)  time: 3.7922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [  90/3449]  eta: 3:32:49  lr: 0.000100  loss: 0.0823 (0.1018)  time: 3.7665  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 100/3449]  eta: 3:32:16  lr: 0.000100  loss: 0.0624 (0.0989)  time: 3.7852  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 110/3449]  eta: 3:31:39  lr: 0.000100  loss: 0.0585 (0.0948)  time: 3.8105  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 120/3449]  eta: 3:30:40  lr: 0.000100  loss: 0.0580 (0.0923)  time: 3.7667  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 130/3449]  eta: 3:30:05  lr: 0.000100  loss: 0.0615 (0.0900)  time: 3.7680  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 140/3449]  eta: 3:29:31  lr: 0.000100  loss: 0.0653 (0.0916)  time: 3.8123  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 150/3449]  eta: 3:28:52  lr: 0.000100  loss: 0.0807 (0.0904)  time: 3.8050  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 160/3449]  eta: 3:27:35  lr: 0.000100  loss: 0.0641 (0.0887)  time: 3.7017  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 170/3449]  eta: 3:26:57  lr: 0.000100  loss: 0.0773 (0.0904)  time: 3.6967  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 180/3449]  eta: 3:26:26  lr: 0.000100  loss: 0.1031 (0.0909)  time: 3.8040  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 190/3449]  eta: 3:26:02  lr: 0.000100  loss: 0.0738 (0.0891)  time: 3.8491  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 200/3449]  eta: 3:25:20  lr: 0.000100  loss: 0.0624 (0.0884)  time: 3.8204  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 210/3449]  eta: 3:24:54  lr: 0.000100  loss: 0.0727 (0.0872)  time: 3.8169  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 220/3449]  eta: 3:24:08  lr: 0.000100  loss: 0.0675 (0.0863)  time: 3.8044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 230/3449]  eta: 3:23:13  lr: 0.000100  loss: 0.0805 (0.0869)  time: 3.7082  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 240/3449]  eta: 3:22:39  lr: 0.000100  loss: 0.0928 (0.0875)  time: 3.7432  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 250/3449]  eta: 3:21:52  lr: 0.000100  loss: 0.0826 (0.0869)  time: 3.7670  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 260/3449]  eta: 3:21:27  lr: 0.000100  loss: 0.0596 (0.0857)  time: 3.8062  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 270/3449]  eta: 3:20:55  lr: 0.000100  loss: 0.0595 (0.0846)  time: 3.8657  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 280/3449]  eta: 3:20:10  lr: 0.000100  loss: 0.0460 (0.0830)  time: 3.7843  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 290/3449]  eta: 3:19:24  lr: 0.000100  loss: 0.0311 (0.0815)  time: 3.7235  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 300/3449]  eta: 3:19:01  lr: 0.000100  loss: 0.0532 (0.0828)  time: 3.8249  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 310/3449]  eta: 3:18:19  lr: 0.000100  loss: 0.1085 (0.0830)  time: 3.8413  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 320/3449]  eta: 3:17:37  lr: 0.000100  loss: 0.0681 (0.0823)  time: 3.7492  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 330/3449]  eta: 3:16:55  lr: 0.000100  loss: 0.0558 (0.0817)  time: 3.7450  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 340/3449]  eta: 3:16:26  lr: 0.000100  loss: 0.0559 (0.0809)  time: 3.8174  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 350/3449]  eta: 3:15:43  lr: 0.000100  loss: 0.0523 (0.0801)  time: 3.8076  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 360/3449]  eta: 3:15:02  lr: 0.000100  loss: 0.0376 (0.0787)  time: 3.7417  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 370/3449]  eta: 3:14:28  lr: 0.000100  loss: 0.0293 (0.0777)  time: 3.7993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 380/3449]  eta: 3:13:55  lr: 0.000100  loss: 0.0618 (0.0787)  time: 3.8443  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 390/3449]  eta: 3:13:16  lr: 0.000100  loss: 0.0618 (0.0780)  time: 3.8087  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 400/3449]  eta: 3:12:31  lr: 0.000100  loss: 0.0328 (0.0767)  time: 3.7385  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 410/3449]  eta: 3:11:49  lr: 0.000100  loss: 0.0272 (0.0758)  time: 3.7165  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 420/3449]  eta: 3:11:10  lr: 0.000100  loss: 0.0422 (0.0754)  time: 3.7525  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 430/3449]  eta: 3:10:32  lr: 0.000100  loss: 0.0651 (0.0752)  time: 3.7823  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 440/3449]  eta: 3:09:54  lr: 0.000100  loss: 0.0592 (0.0747)  time: 3.7827  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 450/3449]  eta: 3:09:13  lr: 0.000100  loss: 0.0470 (0.0740)  time: 3.7617  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 460/3449]  eta: 3:08:41  lr: 0.000100  loss: 0.0458 (0.0735)  time: 3.8100  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 470/3449]  eta: 3:08:01  lr: 0.000100  loss: 0.0355 (0.0727)  time: 3.8147  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 480/3449]  eta: 3:07:24  lr: 0.000100  loss: 0.0324 (0.0719)  time: 3.7726  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 490/3449]  eta: 3:06:51  lr: 0.000100  loss: 0.0306 (0.0713)  time: 3.8322  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 500/3449]  eta: 3:06:10  lr: 0.000100  loss: 0.0500 (0.0710)  time: 3.8031  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 510/3449]  eta: 3:05:26  lr: 0.000100  loss: 0.0572 (0.0708)  time: 3.7065  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 520/3449]  eta: 3:04:46  lr: 0.000100  loss: 0.0566 (0.0707)  time: 3.7114  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 530/3449]  eta: 3:04:10  lr: 0.000100  loss: 0.0482 (0.0702)  time: 3.7902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 540/3449]  eta: 3:03:37  lr: 0.000100  loss: 0.0593 (0.0701)  time: 3.8518  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [ 550/3449]  eta: 3:03:04  lr: 0.000100  loss: 0.0628 (0.0699)  time: 3.8707  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 560/3449]  eta: 3:02:23  lr: 0.000100  loss: 0.0628 (0.0698)  time: 3.8084  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 570/3449]  eta: 3:01:48  lr: 0.000100  loss: 0.0594 (0.0696)  time: 3.7921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 580/3449]  eta: 3:01:10  lr: 0.000100  loss: 0.0605 (0.0695)  time: 3.8071  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 590/3449]  eta: 3:00:33  lr: 0.000100  loss: 0.0614 (0.0695)  time: 3.7969  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 600/3449]  eta: 3:00:00  lr: 0.000100  loss: 0.0377 (0.0689)  time: 3.8499  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 610/3449]  eta: 2:59:14  lr: 0.000100  loss: 0.0332 (0.0685)  time: 3.7573  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 620/3449]  eta: 2:58:46  lr: 0.000100  loss: 0.0569 (0.0692)  time: 3.8144  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 630/3449]  eta: 2:58:06  lr: 0.000100  loss: 0.1085 (0.0696)  time: 3.8698  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 640/3449]  eta: 2:57:35  lr: 0.000100  loss: 0.0820 (0.0698)  time: 3.8519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 650/3449]  eta: 2:56:52  lr: 0.000100  loss: 0.0849 (0.0702)  time: 3.8127  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 660/3449]  eta: 2:56:13  lr: 0.000100  loss: 0.0969 (0.0706)  time: 3.7110  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 670/3449]  eta: 2:55:37  lr: 0.000100  loss: 0.0950 (0.0710)  time: 3.7988  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 680/3449]  eta: 2:54:58  lr: 0.000100  loss: 0.0396 (0.0705)  time: 3.8096  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 690/3449]  eta: 2:54:21  lr: 0.000100  loss: 0.0319 (0.0702)  time: 3.7970  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 700/3449]  eta: 2:53:46  lr: 0.000100  loss: 0.0494 (0.0700)  time: 3.8430  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 710/3449]  eta: 2:53:14  lr: 0.000100  loss: 0.0494 (0.0697)  time: 3.8984  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 720/3449]  eta: 2:52:36  lr: 0.000100  loss: 0.0461 (0.0695)  time: 3.8695  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 730/3449]  eta: 2:51:58  lr: 0.000100  loss: 0.0553 (0.0694)  time: 3.7962  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 740/3449]  eta: 2:51:24  lr: 0.000100  loss: 0.0612 (0.0692)  time: 3.8460  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 750/3449]  eta: 2:50:41  lr: 0.000100  loss: 0.0594 (0.0691)  time: 3.7879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 760/3449]  eta: 2:50:05  lr: 0.000100  loss: 0.0373 (0.0686)  time: 3.7452  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 770/3449]  eta: 2:49:26  lr: 0.000100  loss: 0.0301 (0.0681)  time: 3.7951  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 780/3449]  eta: 2:48:51  lr: 0.000100  loss: 0.0312 (0.0676)  time: 3.8245  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 790/3449]  eta: 2:48:12  lr: 0.000100  loss: 0.0343 (0.0674)  time: 3.8267  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 800/3449]  eta: 2:47:34  lr: 0.000100  loss: 0.0599 (0.0674)  time: 3.7842  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 810/3449]  eta: 2:46:53  lr: 0.000100  loss: 0.0749 (0.0680)  time: 3.7564  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 820/3449]  eta: 2:46:12  lr: 0.000100  loss: 0.0889 (0.0682)  time: 3.7025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 830/3449]  eta: 2:45:35  lr: 0.000100  loss: 0.0805 (0.0683)  time: 3.7597  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 840/3449]  eta: 2:44:55  lr: 0.000100  loss: 0.0578 (0.0681)  time: 3.7698  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 850/3449]  eta: 2:44:19  lr: 0.000100  loss: 0.0572 (0.0681)  time: 3.7864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 860/3449]  eta: 2:43:45  lr: 0.000100  loss: 0.0689 (0.0681)  time: 3.8889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 870/3449]  eta: 2:43:06  lr: 0.000100  loss: 0.0386 (0.0677)  time: 3.8390  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 880/3449]  eta: 2:42:30  lr: 0.000100  loss: 0.0386 (0.0675)  time: 3.8153  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 890/3449]  eta: 2:41:52  lr: 0.000100  loss: 0.0367 (0.0671)  time: 3.8231  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 900/3449]  eta: 2:41:13  lr: 0.000100  loss: 0.0302 (0.0667)  time: 3.7687  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 910/3449]  eta: 2:40:30  lr: 0.000100  loss: 0.0269 (0.0662)  time: 3.6907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 920/3449]  eta: 2:39:54  lr: 0.000100  loss: 0.0292 (0.0658)  time: 3.7439  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 930/3449]  eta: 2:39:14  lr: 0.000100  loss: 0.0244 (0.0654)  time: 3.7945  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 940/3449]  eta: 2:38:38  lr: 0.000100  loss: 0.0332 (0.0653)  time: 3.7858  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 950/3449]  eta: 2:37:59  lr: 0.000100  loss: 0.0493 (0.0653)  time: 3.8105  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 960/3449]  eta: 2:37:20  lr: 0.000100  loss: 0.0493 (0.0651)  time: 3.7696  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 970/3449]  eta: 2:36:42  lr: 0.000100  loss: 0.0577 (0.0655)  time: 3.7693  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [ 980/3449]  eta: 2:36:06  lr: 0.000100  loss: 0.0884 (0.0657)  time: 3.8274  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [ 990/3449]  eta: 2:35:26  lr: 0.000100  loss: 0.0746 (0.0657)  time: 3.7960  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1000/3449]  eta: 2:34:49  lr: 0.000100  loss: 0.0490 (0.0655)  time: 3.7558  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1010/3449]  eta: 2:34:14  lr: 0.000100  loss: 0.0512 (0.0654)  time: 3.8654  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1020/3449]  eta: 2:33:36  lr: 0.000100  loss: 0.0566 (0.0653)  time: 3.8531  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1030/3449]  eta: 2:32:56  lr: 0.000100  loss: 0.0378 (0.0650)  time: 3.7559  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1040/3449]  eta: 2:32:18  lr: 0.000100  loss: 0.0355 (0.0646)  time: 3.7638  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1050/3449]  eta: 2:31:40  lr: 0.000100  loss: 0.0329 (0.0645)  time: 3.7874  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1060/3449]  eta: 2:31:04  lr: 0.000100  loss: 0.0503 (0.0644)  time: 3.8309  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1070/3449]  eta: 2:30:27  lr: 0.000100  loss: 0.0535 (0.0643)  time: 3.8645  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1080/3449]  eta: 2:29:48  lr: 0.000100  loss: 0.0438 (0.0642)  time: 3.7930  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1090/3449]  eta: 2:29:11  lr: 0.000100  loss: 0.0530 (0.0642)  time: 3.7824  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1100/3449]  eta: 2:28:32  lr: 0.000100  loss: 0.0538 (0.0642)  time: 3.7772  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1110/3449]  eta: 2:27:53  lr: 0.000100  loss: 0.0492 (0.0640)  time: 3.7488  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1120/3449]  eta: 2:27:18  lr: 0.000100  loss: 0.0333 (0.0638)  time: 3.8512  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1130/3449]  eta: 2:26:42  lr: 0.000100  loss: 0.0387 (0.0637)  time: 3.9022  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1140/3449]  eta: 2:26:05  lr: 0.000100  loss: 0.0555 (0.0636)  time: 3.8623  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1150/3449]  eta: 2:25:26  lr: 0.000100  loss: 0.0532 (0.0636)  time: 3.8015  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1160/3449]  eta: 2:24:49  lr: 0.000100  loss: 0.0558 (0.0636)  time: 3.7956  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1170/3449]  eta: 2:24:10  lr: 0.000100  loss: 0.0490 (0.0635)  time: 3.7969  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1180/3449]  eta: 2:23:33  lr: 0.000100  loss: 0.0334 (0.0632)  time: 3.8030  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1190/3449]  eta: 2:22:56  lr: 0.000100  loss: 0.0459 (0.0636)  time: 3.8550  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1200/3449]  eta: 2:22:16  lr: 0.000100  loss: 0.0899 (0.0638)  time: 3.7548  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [1210/3449]  eta: 2:21:37  lr: 0.000100  loss: 0.0508 (0.0636)  time: 3.6937  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1220/3449]  eta: 2:20:58  lr: 0.000100  loss: 0.0349 (0.0634)  time: 3.7491  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1230/3449]  eta: 2:20:21  lr: 0.000100  loss: 0.0349 (0.0635)  time: 3.8062  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1240/3449]  eta: 2:19:45  lr: 0.000100  loss: 0.0989 (0.0641)  time: 3.8726  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1250/3449]  eta: 2:19:07  lr: 0.000100  loss: 0.1565 (0.0649)  time: 3.8444  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1260/3449]  eta: 2:18:29  lr: 0.000100  loss: 0.1770 (0.0656)  time: 3.7863  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1270/3449]  eta: 2:17:50  lr: 0.000100  loss: 0.1514 (0.0662)  time: 3.7763  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1280/3449]  eta: 2:17:13  lr: 0.000100  loss: 0.1282 (0.0666)  time: 3.7857  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1290/3449]  eta: 2:16:35  lr: 0.000100  loss: 0.0959 (0.0667)  time: 3.8197  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1300/3449]  eta: 2:15:58  lr: 0.000100  loss: 0.0593 (0.0666)  time: 3.8378  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1310/3449]  eta: 2:15:21  lr: 0.000100  loss: 0.0465 (0.0664)  time: 3.8523  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1320/3449]  eta: 2:14:42  lr: 0.000100  loss: 0.0455 (0.0663)  time: 3.7971  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1330/3449]  eta: 2:14:03  lr: 0.000100  loss: 0.0482 (0.0662)  time: 3.7350  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1340/3449]  eta: 2:13:25  lr: 0.000100  loss: 0.0494 (0.0661)  time: 3.7625  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1350/3449]  eta: 2:12:46  lr: 0.000100  loss: 0.0757 (0.0665)  time: 3.7610  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1360/3449]  eta: 2:12:09  lr: 0.000100  loss: 0.1149 (0.0669)  time: 3.8056  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1370/3449]  eta: 2:11:30  lr: 0.000100  loss: 0.0971 (0.0671)  time: 3.7922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1380/3449]  eta: 2:10:51  lr: 0.000100  loss: 0.0561 (0.0669)  time: 3.7142  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1390/3449]  eta: 2:10:12  lr: 0.000100  loss: 0.0305 (0.0666)  time: 3.7040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1400/3449]  eta: 2:09:34  lr: 0.000100  loss: 0.0256 (0.0663)  time: 3.7647  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1410/3449]  eta: 2:08:57  lr: 0.000100  loss: 0.0225 (0.0660)  time: 3.8196  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1420/3449]  eta: 2:08:19  lr: 0.000100  loss: 0.0262 (0.0659)  time: 3.8049  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1430/3449]  eta: 2:07:41  lr: 0.000100  loss: 0.0673 (0.0661)  time: 3.8231  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1440/3449]  eta: 2:07:03  lr: 0.000100  loss: 0.0794 (0.0661)  time: 3.8149  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1450/3449]  eta: 2:06:26  lr: 0.000100  loss: 0.0561 (0.0661)  time: 3.8079  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1460/3449]  eta: 2:05:49  lr: 0.000100  loss: 0.0601 (0.0662)  time: 3.8384  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1470/3449]  eta: 2:05:09  lr: 0.000100  loss: 0.0992 (0.0664)  time: 3.7594  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1480/3449]  eta: 2:04:30  lr: 0.000100  loss: 0.1080 (0.0668)  time: 3.7064  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1490/3449]  eta: 2:03:52  lr: 0.000100  loss: 0.1279 (0.0672)  time: 3.7587  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1500/3449]  eta: 2:03:13  lr: 0.000100  loss: 0.0980 (0.0673)  time: 3.7500  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1510/3449]  eta: 2:02:36  lr: 0.000100  loss: 0.0794 (0.0673)  time: 3.7777  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1520/3449]  eta: 2:01:59  lr: 0.000100  loss: 0.0552 (0.0672)  time: 3.8673  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1530/3449]  eta: 2:01:22  lr: 0.000100  loss: 0.0602 (0.0672)  time: 3.8558  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1540/3449]  eta: 2:00:45  lr: 0.000100  loss: 0.0609 (0.0671)  time: 3.8478  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1550/3449]  eta: 2:00:09  lr: 0.000100  loss: 0.0593 (0.0670)  time: 3.9115  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1560/3449]  eta: 1:59:32  lr: 0.000100  loss: 0.0394 (0.0668)  time: 3.9184  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1570/3449]  eta: 1:58:53  lr: 0.000100  loss: 0.0304 (0.0667)  time: 3.8296  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1580/3449]  eta: 1:58:15  lr: 0.000100  loss: 0.0466 (0.0666)  time: 3.7520  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1590/3449]  eta: 1:57:37  lr: 0.000100  loss: 0.0687 (0.0668)  time: 3.7953  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1600/3449]  eta: 1:57:01  lr: 0.000100  loss: 0.0961 (0.0670)  time: 3.8646  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1610/3449]  eta: 1:56:23  lr: 0.000100  loss: 0.0777 (0.0670)  time: 3.8419  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1620/3449]  eta: 1:55:45  lr: 0.000100  loss: 0.0507 (0.0668)  time: 3.8281  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1630/3449]  eta: 1:55:08  lr: 0.000100  loss: 0.0324 (0.0667)  time: 3.8457  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1640/3449]  eta: 1:54:29  lr: 0.000100  loss: 0.0540 (0.0666)  time: 3.7804  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1650/3449]  eta: 1:53:49  lr: 0.000100  loss: 0.0413 (0.0664)  time: 3.6925  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1660/3449]  eta: 1:53:11  lr: 0.000100  loss: 0.0376 (0.0664)  time: 3.7055  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1670/3449]  eta: 1:52:32  lr: 0.000100  loss: 0.0449 (0.0663)  time: 3.7352  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1680/3449]  eta: 1:51:55  lr: 0.000100  loss: 0.0550 (0.0663)  time: 3.7686  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1690/3449]  eta: 1:51:16  lr: 0.000100  loss: 0.0407 (0.0661)  time: 3.7984  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1700/3449]  eta: 1:50:39  lr: 0.000100  loss: 0.0400 (0.0661)  time: 3.8276  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1710/3449]  eta: 1:50:02  lr: 0.000100  loss: 0.0569 (0.0660)  time: 3.8669  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1720/3449]  eta: 1:49:25  lr: 0.000100  loss: 0.0562 (0.0660)  time: 3.8694  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1730/3449]  eta: 1:48:47  lr: 0.000100  loss: 0.0574 (0.0660)  time: 3.8665  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1740/3449]  eta: 1:48:08  lr: 0.000100  loss: 0.0543 (0.0659)  time: 3.7365  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1750/3449]  eta: 1:47:29  lr: 0.000100  loss: 0.0543 (0.0659)  time: 3.6768  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1760/3449]  eta: 1:46:52  lr: 0.000100  loss: 0.0697 (0.0660)  time: 3.8334  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1770/3449]  eta: 1:46:13  lr: 0.000100  loss: 0.0697 (0.0659)  time: 3.8016  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1780/3449]  eta: 1:45:35  lr: 0.000100  loss: 0.0813 (0.0662)  time: 3.7057  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1790/3449]  eta: 1:44:57  lr: 0.000100  loss: 0.1003 (0.0665)  time: 3.7686  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1800/3449]  eta: 1:44:19  lr: 0.000100  loss: 0.0698 (0.0664)  time: 3.8175  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1810/3449]  eta: 1:43:41  lr: 0.000100  loss: 0.0377 (0.0662)  time: 3.8086  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1820/3449]  eta: 1:43:03  lr: 0.000100  loss: 0.0318 (0.0661)  time: 3.7696  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1830/3449]  eta: 1:42:25  lr: 0.000100  loss: 0.0388 (0.0661)  time: 3.8077  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1840/3449]  eta: 1:41:47  lr: 0.000100  loss: 0.0683 (0.0662)  time: 3.8098  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1850/3449]  eta: 1:41:10  lr: 0.000100  loss: 0.0602 (0.0661)  time: 3.8210  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1860/3449]  eta: 1:40:33  lr: 0.000100  loss: 0.0576 (0.0661)  time: 3.8729  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [1870/3449]  eta: 1:39:54  lr: 0.000100  loss: 0.0573 (0.0660)  time: 3.8038  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1880/3449]  eta: 1:39:16  lr: 0.000100  loss: 0.0517 (0.0660)  time: 3.7373  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1890/3449]  eta: 1:38:38  lr: 0.000100  loss: 0.0639 (0.0660)  time: 3.7727  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1900/3449]  eta: 1:38:01  lr: 0.000100  loss: 0.0698 (0.0660)  time: 3.8781  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1910/3449]  eta: 1:37:23  lr: 0.000100  loss: 0.0627 (0.0660)  time: 3.8532  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1920/3449]  eta: 1:36:44  lr: 0.000100  loss: 0.0601 (0.0660)  time: 3.7091  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1930/3449]  eta: 1:36:07  lr: 0.000100  loss: 0.0822 (0.0662)  time: 3.7940  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1940/3449]  eta: 1:35:27  lr: 0.000100  loss: 0.0742 (0.0661)  time: 3.7626  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1950/3449]  eta: 1:34:50  lr: 0.000100  loss: 0.0326 (0.0659)  time: 3.7402  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [1960/3449]  eta: 1:34:13  lr: 0.000100  loss: 0.0307 (0.0658)  time: 3.8789  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1970/3449]  eta: 1:33:35  lr: 0.000100  loss: 0.0286 (0.0656)  time: 3.8522  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1980/3449]  eta: 1:32:57  lr: 0.000100  loss: 0.0245 (0.0654)  time: 3.8220  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [1990/3449]  eta: 1:32:19  lr: 0.000100  loss: 0.0365 (0.0653)  time: 3.7708  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2000/3449]  eta: 1:31:40  lr: 0.000100  loss: 0.0543 (0.0653)  time: 3.7440  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2010/3449]  eta: 1:31:04  lr: 0.000100  loss: 0.0634 (0.0653)  time: 3.8571  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2020/3449]  eta: 1:30:26  lr: 0.000100  loss: 0.0683 (0.0654)  time: 3.8732  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2030/3449]  eta: 1:29:48  lr: 0.000100  loss: 0.0695 (0.0654)  time: 3.8046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2040/3449]  eta: 1:29:10  lr: 0.000100  loss: 0.0738 (0.0655)  time: 3.8480  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2050/3449]  eta: 1:28:32  lr: 0.000100  loss: 0.0632 (0.0654)  time: 3.8332  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2060/3449]  eta: 1:27:54  lr: 0.000100  loss: 0.0490 (0.0653)  time: 3.7909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2070/3449]  eta: 1:27:16  lr: 0.000100  loss: 0.0447 (0.0652)  time: 3.7922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2080/3449]  eta: 1:26:38  lr: 0.000100  loss: 0.0531 (0.0655)  time: 3.7528  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2090/3449]  eta: 1:26:00  lr: 0.000100  loss: 0.1064 (0.0657)  time: 3.7464  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2100/3449]  eta: 1:25:22  lr: 0.000100  loss: 0.0756 (0.0657)  time: 3.7783  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2110/3449]  eta: 1:24:44  lr: 0.000100  loss: 0.0649 (0.0657)  time: 3.8380  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2120/3449]  eta: 1:24:06  lr: 0.000100  loss: 0.0553 (0.0657)  time: 3.8538  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2130/3449]  eta: 1:23:29  lr: 0.000100  loss: 0.0661 (0.0657)  time: 3.8314  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2140/3449]  eta: 1:22:51  lr: 0.000100  loss: 0.0433 (0.0656)  time: 3.8301  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2150/3449]  eta: 1:22:12  lr: 0.000100  loss: 0.0368 (0.0654)  time: 3.7377  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2160/3449]  eta: 1:21:34  lr: 0.000100  loss: 0.0416 (0.0655)  time: 3.6879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2170/3449]  eta: 1:20:55  lr: 0.000100  loss: 0.1071 (0.0657)  time: 3.7378  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2180/3449]  eta: 1:20:17  lr: 0.000100  loss: 0.0965 (0.0658)  time: 3.7161  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2190/3449]  eta: 1:19:39  lr: 0.000100  loss: 0.0654 (0.0658)  time: 3.7660  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2200/3449]  eta: 1:19:01  lr: 0.000100  loss: 0.0654 (0.0658)  time: 3.8616  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2210/3449]  eta: 1:18:23  lr: 0.000100  loss: 0.0574 (0.0657)  time: 3.8006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2220/3449]  eta: 1:17:45  lr: 0.000100  loss: 0.0453 (0.0657)  time: 3.7296  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2230/3449]  eta: 1:17:07  lr: 0.000100  loss: 0.0665 (0.0660)  time: 3.8075  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2240/3449]  eta: 1:16:29  lr: 0.000100  loss: 0.1306 (0.0663)  time: 3.8438  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2250/3449]  eta: 1:15:51  lr: 0.000100  loss: 0.0945 (0.0664)  time: 3.7764  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2260/3449]  eta: 1:15:13  lr: 0.000100  loss: 0.0708 (0.0664)  time: 3.7111  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2270/3449]  eta: 1:14:34  lr: 0.000100  loss: 0.0507 (0.0663)  time: 3.6930  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2280/3449]  eta: 1:13:57  lr: 0.000100  loss: 0.0353 (0.0661)  time: 3.7912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2290/3449]  eta: 1:13:18  lr: 0.000100  loss: 0.0305 (0.0660)  time: 3.7731  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2300/3449]  eta: 1:12:40  lr: 0.000100  loss: 0.0353 (0.0659)  time: 3.7611  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2310/3449]  eta: 1:12:03  lr: 0.000100  loss: 0.0328 (0.0658)  time: 3.8304  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2320/3449]  eta: 1:11:25  lr: 0.000100  loss: 0.0328 (0.0656)  time: 3.8478  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2330/3449]  eta: 1:10:47  lr: 0.000100  loss: 0.0338 (0.0655)  time: 3.8046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2340/3449]  eta: 1:10:09  lr: 0.000100  loss: 0.0286 (0.0653)  time: 3.7910  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2350/3449]  eta: 1:09:31  lr: 0.000100  loss: 0.0280 (0.0652)  time: 3.8327  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2360/3449]  eta: 1:08:54  lr: 0.000100  loss: 0.0391 (0.0651)  time: 3.8834  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2370/3449]  eta: 1:08:16  lr: 0.000100  loss: 0.0393 (0.0650)  time: 3.8729  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2380/3449]  eta: 1:07:38  lr: 0.000100  loss: 0.0505 (0.0652)  time: 3.8149  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2390/3449]  eta: 1:07:00  lr: 0.000100  loss: 0.0789 (0.0652)  time: 3.8193  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2400/3449]  eta: 1:06:22  lr: 0.000100  loss: 0.0567 (0.0651)  time: 3.8258  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2410/3449]  eta: 1:05:44  lr: 0.000100  loss: 0.0458 (0.0651)  time: 3.8193  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2420/3449]  eta: 1:05:06  lr: 0.000100  loss: 0.0440 (0.0650)  time: 3.7929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2430/3449]  eta: 1:04:29  lr: 0.000100  loss: 0.0563 (0.0651)  time: 3.8798  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2440/3449]  eta: 1:03:51  lr: 0.000100  loss: 0.0835 (0.0654)  time: 3.8449  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2450/3449]  eta: 1:03:12  lr: 0.000100  loss: 0.0999 (0.0654)  time: 3.6996  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2460/3449]  eta: 1:02:34  lr: 0.000100  loss: 0.0582 (0.0654)  time: 3.7098  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2470/3449]  eta: 1:01:57  lr: 0.000100  loss: 0.0502 (0.0653)  time: 3.8495  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2480/3449]  eta: 1:01:19  lr: 0.000100  loss: 0.0358 (0.0652)  time: 3.8774  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2490/3449]  eta: 1:00:41  lr: 0.000100  loss: 0.0274 (0.0650)  time: 3.8186  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2500/3449]  eta: 1:00:03  lr: 0.000100  loss: 0.0427 (0.0650)  time: 3.8263  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2510/3449]  eta: 0:59:26  lr: 0.000100  loss: 0.0579 (0.0650)  time: 3.8396  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2520/3449]  eta: 0:58:48  lr: 0.000100  loss: 0.0638 (0.0650)  time: 3.8695  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [2530/3449]  eta: 0:58:10  lr: 0.000100  loss: 0.0409 (0.0649)  time: 3.8316  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2540/3449]  eta: 0:57:32  lr: 0.000100  loss: 0.0459 (0.0650)  time: 3.7778  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2550/3449]  eta: 0:56:54  lr: 0.000100  loss: 0.0978 (0.0652)  time: 3.7735  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2560/3449]  eta: 0:56:16  lr: 0.000100  loss: 0.0813 (0.0652)  time: 3.8128  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2570/3449]  eta: 0:55:38  lr: 0.000100  loss: 0.0534 (0.0651)  time: 3.8080  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2580/3449]  eta: 0:54:59  lr: 0.000100  loss: 0.0664 (0.0653)  time: 3.7243  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2590/3449]  eta: 0:54:21  lr: 0.000100  loss: 0.0511 (0.0652)  time: 3.6871  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2600/3449]  eta: 0:53:43  lr: 0.000100  loss: 0.0332 (0.0651)  time: 3.7967  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2610/3449]  eta: 0:53:05  lr: 0.000100  loss: 0.0278 (0.0649)  time: 3.8086  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2620/3449]  eta: 0:52:27  lr: 0.000100  loss: 0.0463 (0.0650)  time: 3.7013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2630/3449]  eta: 0:51:49  lr: 0.000100  loss: 0.0882 (0.0651)  time: 3.7211  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2640/3449]  eta: 0:51:11  lr: 0.000100  loss: 0.0715 (0.0650)  time: 3.7621  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2650/3449]  eta: 0:50:33  lr: 0.000100  loss: 0.0429 (0.0650)  time: 3.8347  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2660/3449]  eta: 0:49:55  lr: 0.000100  loss: 0.0365 (0.0649)  time: 3.8693  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2670/3449]  eta: 0:49:17  lr: 0.000100  loss: 0.0398 (0.0648)  time: 3.8152  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2680/3449]  eta: 0:48:39  lr: 0.000100  loss: 0.0497 (0.0648)  time: 3.7676  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2690/3449]  eta: 0:48:01  lr: 0.000100  loss: 0.1002 (0.0651)  time: 3.7503  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2700/3449]  eta: 0:47:23  lr: 0.000100  loss: 0.1434 (0.0654)  time: 3.8002  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2710/3449]  eta: 0:46:45  lr: 0.000100  loss: 0.0960 (0.0655)  time: 3.8337  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2720/3449]  eta: 0:46:07  lr: 0.000100  loss: 0.0659 (0.0654)  time: 3.7862  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2730/3449]  eta: 0:45:29  lr: 0.000100  loss: 0.0532 (0.0654)  time: 3.7084  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2740/3449]  eta: 0:44:51  lr: 0.000100  loss: 0.0586 (0.0654)  time: 3.6976  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2750/3449]  eta: 0:44:13  lr: 0.000100  loss: 0.0572 (0.0653)  time: 3.7308  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2760/3449]  eta: 0:43:35  lr: 0.000100  loss: 0.0526 (0.0653)  time: 3.7973  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2770/3449]  eta: 0:42:57  lr: 0.000100  loss: 0.0392 (0.0652)  time: 3.8509  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2780/3449]  eta: 0:42:19  lr: 0.000100  loss: 0.0407 (0.0651)  time: 3.8026  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2790/3449]  eta: 0:41:41  lr: 0.000100  loss: 0.0575 (0.0651)  time: 3.7736  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2800/3449]  eta: 0:41:03  lr: 0.000100  loss: 0.0440 (0.0650)  time: 3.7424  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2810/3449]  eta: 0:40:25  lr: 0.000100  loss: 0.0359 (0.0649)  time: 3.7619  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2820/3449]  eta: 0:39:47  lr: 0.000100  loss: 0.0347 (0.0648)  time: 3.7981  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2830/3449]  eta: 0:39:09  lr: 0.000100  loss: 0.0366 (0.0648)  time: 3.6846  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2840/3449]  eta: 0:38:31  lr: 0.000100  loss: 0.0466 (0.0648)  time: 3.7124  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2850/3449]  eta: 0:37:52  lr: 0.000100  loss: 0.0873 (0.0650)  time: 3.7381  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2860/3449]  eta: 0:37:14  lr: 0.000100  loss: 0.0886 (0.0650)  time: 3.7205  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2870/3449]  eta: 0:36:36  lr: 0.000100  loss: 0.0886 (0.0651)  time: 3.7692  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2880/3449]  eta: 0:35:59  lr: 0.000100  loss: 0.0704 (0.0651)  time: 3.8112  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2890/3449]  eta: 0:35:21  lr: 0.000100  loss: 0.0553 (0.0650)  time: 3.7917  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2900/3449]  eta: 0:34:43  lr: 0.000100  loss: 0.0447 (0.0650)  time: 3.7360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2910/3449]  eta: 0:34:04  lr: 0.000100  loss: 0.0513 (0.0649)  time: 3.7331  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2920/3449]  eta: 0:33:27  lr: 0.000100  loss: 0.0525 (0.0649)  time: 3.7578  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2930/3449]  eta: 0:32:49  lr: 0.000100  loss: 0.0474 (0.0648)  time: 3.8075  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2940/3449]  eta: 0:32:11  lr: 0.000100  loss: 0.0498 (0.0649)  time: 3.8168  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2950/3449]  eta: 0:31:33  lr: 0.000100  loss: 0.0487 (0.0648)  time: 3.8279  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [2960/3449]  eta: 0:30:55  lr: 0.000100  loss: 0.0309 (0.0647)  time: 3.8199  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2970/3449]  eta: 0:30:17  lr: 0.000100  loss: 0.0290 (0.0646)  time: 3.7858  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2980/3449]  eta: 0:29:39  lr: 0.000100  loss: 0.0392 (0.0646)  time: 3.7916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [2990/3449]  eta: 0:29:01  lr: 0.000100  loss: 0.0390 (0.0645)  time: 3.8461  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3000/3449]  eta: 0:28:23  lr: 0.000100  loss: 0.0401 (0.0644)  time: 3.7949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3010/3449]  eta: 0:27:45  lr: 0.000100  loss: 0.0423 (0.0644)  time: 3.7555  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3020/3449]  eta: 0:27:07  lr: 0.000100  loss: 0.0406 (0.0643)  time: 3.7895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [3030/3449]  eta: 0:26:29  lr: 0.000100  loss: 0.0330 (0.0642)  time: 3.8318  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3040/3449]  eta: 0:25:51  lr: 0.000100  loss: 0.0272 (0.0641)  time: 3.7910  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3050/3449]  eta: 0:25:13  lr: 0.000100  loss: 0.0401 (0.0641)  time: 3.7271  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3060/3449]  eta: 0:24:36  lr: 0.000100  loss: 0.0494 (0.0640)  time: 3.8099  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3070/3449]  eta: 0:23:58  lr: 0.000100  loss: 0.0417 (0.0640)  time: 3.8563  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3080/3449]  eta: 0:23:20  lr: 0.000100  loss: 0.0446 (0.0640)  time: 3.8935  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3090/3449]  eta: 0:22:42  lr: 0.000100  loss: 0.0902 (0.0641)  time: 3.8596  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [3100/3449]  eta: 0:22:04  lr: 0.000100  loss: 0.0869 (0.0641)  time: 3.7822  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [3110/3449]  eta: 0:21:26  lr: 0.000100  loss: 0.0464 (0.0640)  time: 3.8211  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [3120/3449]  eta: 0:20:48  lr: 0.000100  loss: 0.0220 (0.0639)  time: 3.8273  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3130/3449]  eta: 0:20:10  lr: 0.000100  loss: 0.0213 (0.0638)  time: 3.8593  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3140/3449]  eta: 0:19:32  lr: 0.000100  loss: 0.0296 (0.0637)  time: 3.8502  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3150/3449]  eta: 0:18:54  lr: 0.000100  loss: 0.0371 (0.0636)  time: 3.8340  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3160/3449]  eta: 0:18:17  lr: 0.000100  loss: 0.0487 (0.0636)  time: 3.8752  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3170/3449]  eta: 0:17:39  lr: 0.000100  loss: 0.0424 (0.0635)  time: 3.8442  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3180/3449]  eta: 0:17:01  lr: 0.000100  loss: 0.0453 (0.0635)  time: 3.7896  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [3190/3449]  eta: 0:16:23  lr: 0.000100  loss: 0.0648 (0.0635)  time: 3.8012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3200/3449]  eta: 0:15:45  lr: 0.000100  loss: 0.0639 (0.0635)  time: 3.7764  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3210/3449]  eta: 0:15:07  lr: 0.000100  loss: 0.0564 (0.0635)  time: 3.7395  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [3220/3449]  eta: 0:14:29  lr: 0.000100  loss: 0.0563 (0.0635)  time: 3.7147  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:14]  [3230/3449]  eta: 0:13:51  lr: 0.000100  loss: 0.0645 (0.0636)  time: 3.8001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3240/3449]  eta: 0:13:13  lr: 0.000100  loss: 0.0730 (0.0636)  time: 3.9072  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3250/3449]  eta: 0:12:35  lr: 0.000100  loss: 0.0730 (0.0637)  time: 3.8129  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3260/3449]  eta: 0:11:57  lr: 0.000100  loss: 0.0869 (0.0638)  time: 3.7919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3270/3449]  eta: 0:11:19  lr: 0.000100  loss: 0.0807 (0.0638)  time: 3.8116  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3280/3449]  eta: 0:10:41  lr: 0.000100  loss: 0.0801 (0.0639)  time: 3.7986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3290/3449]  eta: 0:10:03  lr: 0.000100  loss: 0.0745 (0.0639)  time: 3.7593  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3300/3449]  eta: 0:09:25  lr: 0.000100  loss: 0.0590 (0.0639)  time: 3.7170  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3310/3449]  eta: 0:08:47  lr: 0.000100  loss: 0.0531 (0.0638)  time: 3.7205  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3320/3449]  eta: 0:08:09  lr: 0.000100  loss: 0.0422 (0.0638)  time: 3.7848  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3330/3449]  eta: 0:07:31  lr: 0.000100  loss: 0.0702 (0.0639)  time: 3.7794  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3340/3449]  eta: 0:06:53  lr: 0.000100  loss: 0.0802 (0.0640)  time: 3.7295  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3350/3449]  eta: 0:06:15  lr: 0.000100  loss: 0.0692 (0.0640)  time: 3.7866  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3360/3449]  eta: 0:05:37  lr: 0.000100  loss: 0.0754 (0.0641)  time: 3.7811  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3370/3449]  eta: 0:04:59  lr: 0.000100  loss: 0.0993 (0.0642)  time: 3.7615  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3380/3449]  eta: 0:04:21  lr: 0.000100  loss: 0.1014 (0.0643)  time: 3.7847  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3390/3449]  eta: 0:03:43  lr: 0.000100  loss: 0.0885 (0.0643)  time: 3.7826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3400/3449]  eta: 0:03:05  lr: 0.000100  loss: 0.0518 (0.0643)  time: 3.7956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3410/3449]  eta: 0:02:28  lr: 0.000100  loss: 0.0410 (0.0642)  time: 3.8566  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3420/3449]  eta: 0:01:50  lr: 0.000100  loss: 0.0551 (0.0642)  time: 3.8237  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3430/3449]  eta: 0:01:12  lr: 0.000100  loss: 0.0560 (0.0642)  time: 3.7353  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3440/3449]  eta: 0:00:34  lr: 0.000100  loss: 0.0574 (0.0641)  time: 3.7810  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0561 (0.0641)  time: 3.8383  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:14] Total time: 3:38:09 (3.7952 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0561 (0.0641)\n",
      "Valid: [epoch:14]  [ 0/14]  eta: 0:04:20  loss: 0.0550 (0.0550)  time: 18.6391  data: 0.4487  max mem: 34968\n",
      "Valid: [epoch:14]  [13/14]  eta: 0:00:18  loss: 0.0528 (0.0532)  time: 18.2506  data: 0.0323  max mem: 34968\n",
      "Valid: [epoch:14] Total time: 0:04:15 (18.2611 s / it)\n",
      "Averaged stats: loss: 0.0528 (0.0532)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_14_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.053%\n",
      "Min loss: 0.039\n",
      "Best Epoch: 3.000\n",
      "Train: [epoch:15]  [   0/3449]  eta: 5:03:16  lr: 0.000100  loss: 0.0894 (0.0894)  time: 5.2759  data: 1.1277  max mem: 34968\n",
      "Train: [epoch:15]  [  10/3449]  eta: 3:49:23  lr: 0.000100  loss: 0.0586 (0.0607)  time: 4.0023  data: 0.1026  max mem: 34968\n",
      "Train: [epoch:15]  [  20/3449]  eta: 3:42:04  lr: 0.000100  loss: 0.0444 (0.0574)  time: 3.8163  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [  30/3449]  eta: 3:40:45  lr: 0.000100  loss: 0.0420 (0.0642)  time: 3.8038  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [  40/3449]  eta: 3:40:10  lr: 0.000100  loss: 0.1007 (0.0763)  time: 3.8640  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [  50/3449]  eta: 3:39:22  lr: 0.000100  loss: 0.1019 (0.0787)  time: 3.8694  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [  60/3449]  eta: 3:38:04  lr: 0.000100  loss: 0.0690 (0.0763)  time: 3.8319  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [  70/3449]  eta: 3:37:22  lr: 0.000100  loss: 0.0578 (0.0733)  time: 3.8285  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [  80/3449]  eta: 3:36:14  lr: 0.000100  loss: 0.0605 (0.0726)  time: 3.8216  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [  90/3449]  eta: 3:35:06  lr: 0.000100  loss: 0.0667 (0.0717)  time: 3.7803  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [ 100/3449]  eta: 3:34:04  lr: 0.000100  loss: 0.0675 (0.0725)  time: 3.7713  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [ 110/3449]  eta: 3:33:02  lr: 0.000100  loss: 0.0683 (0.0716)  time: 3.7631  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 120/3449]  eta: 3:32:00  lr: 0.000100  loss: 0.0532 (0.0692)  time: 3.7493  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 130/3449]  eta: 3:31:24  lr: 0.000100  loss: 0.0353 (0.0666)  time: 3.7872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 140/3449]  eta: 3:31:29  lr: 0.000100  loss: 0.0332 (0.0640)  time: 3.9181  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 150/3449]  eta: 3:31:03  lr: 0.000100  loss: 0.0320 (0.0637)  time: 3.9483  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 160/3449]  eta: 3:30:38  lr: 0.000100  loss: 0.0583 (0.0637)  time: 3.8983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 170/3449]  eta: 3:29:48  lr: 0.000100  loss: 0.0613 (0.0635)  time: 3.8418  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [ 180/3449]  eta: 3:28:57  lr: 0.000100  loss: 0.0625 (0.0639)  time: 3.7740  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 190/3449]  eta: 3:28:05  lr: 0.000100  loss: 0.0638 (0.0644)  time: 3.7624  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 200/3449]  eta: 3:27:21  lr: 0.000100  loss: 0.0634 (0.0649)  time: 3.7764  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 210/3449]  eta: 3:26:46  lr: 0.000100  loss: 0.0633 (0.0646)  time: 3.8238  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [ 220/3449]  eta: 3:26:02  lr: 0.000100  loss: 0.0597 (0.0643)  time: 3.8214  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 230/3449]  eta: 3:25:20  lr: 0.000100  loss: 0.0561 (0.0643)  time: 3.7962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 240/3449]  eta: 3:24:33  lr: 0.000100  loss: 0.0465 (0.0635)  time: 3.7810  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 250/3449]  eta: 3:23:39  lr: 0.000100  loss: 0.0320 (0.0622)  time: 3.7332  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 260/3449]  eta: 3:23:01  lr: 0.000100  loss: 0.0262 (0.0608)  time: 3.7601  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 270/3449]  eta: 3:22:09  lr: 0.000100  loss: 0.0264 (0.0598)  time: 3.7627  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 280/3449]  eta: 3:21:28  lr: 0.000100  loss: 0.0280 (0.0587)  time: 3.7461  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 290/3449]  eta: 3:20:34  lr: 0.000100  loss: 0.0270 (0.0581)  time: 3.7264  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 300/3449]  eta: 3:19:52  lr: 0.000100  loss: 0.0581 (0.0595)  time: 3.7220  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [ 310/3449]  eta: 3:19:20  lr: 0.000100  loss: 0.0970 (0.0609)  time: 3.8223  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 320/3449]  eta: 3:18:48  lr: 0.000100  loss: 0.0643 (0.0610)  time: 3.8714  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 330/3449]  eta: 3:17:57  lr: 0.000100  loss: 0.0567 (0.0614)  time: 3.7712  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [ 340/3449]  eta: 3:17:11  lr: 0.000100  loss: 0.0681 (0.0615)  time: 3.6983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 350/3449]  eta: 3:16:26  lr: 0.000100  loss: 0.0597 (0.0614)  time: 3.7273  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 360/3449]  eta: 3:15:57  lr: 0.000100  loss: 0.0514 (0.0610)  time: 3.8178  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 370/3449]  eta: 3:15:25  lr: 0.000100  loss: 0.0390 (0.0607)  time: 3.8943  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 380/3449]  eta: 3:14:37  lr: 0.000100  loss: 0.0643 (0.0610)  time: 3.7843  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 390/3449]  eta: 3:14:03  lr: 0.000100  loss: 0.0506 (0.0604)  time: 3.7729  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 400/3449]  eta: 3:13:24  lr: 0.000100  loss: 0.0377 (0.0603)  time: 3.8233  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 410/3449]  eta: 3:12:44  lr: 0.000100  loss: 0.0587 (0.0603)  time: 3.7834  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 420/3449]  eta: 3:12:11  lr: 0.000100  loss: 0.0514 (0.0600)  time: 3.8300  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 430/3449]  eta: 3:11:36  lr: 0.000100  loss: 0.0304 (0.0594)  time: 3.8635  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 440/3449]  eta: 3:10:59  lr: 0.000100  loss: 0.0460 (0.0594)  time: 3.8364  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 450/3449]  eta: 3:10:24  lr: 0.000100  loss: 0.0611 (0.0596)  time: 3.8393  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 460/3449]  eta: 3:09:47  lr: 0.000100  loss: 0.0651 (0.0600)  time: 3.8354  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 470/3449]  eta: 3:09:04  lr: 0.000100  loss: 0.0468 (0.0595)  time: 3.7796  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 480/3449]  eta: 3:08:31  lr: 0.000100  loss: 0.0367 (0.0593)  time: 3.8137  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 490/3449]  eta: 3:07:53  lr: 0.000100  loss: 0.0350 (0.0590)  time: 3.8484  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 500/3449]  eta: 3:07:14  lr: 0.000100  loss: 0.0373 (0.0586)  time: 3.8022  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 510/3449]  eta: 3:06:33  lr: 0.000100  loss: 0.0401 (0.0586)  time: 3.7806  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 520/3449]  eta: 3:05:53  lr: 0.000100  loss: 0.0447 (0.0583)  time: 3.7714  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 530/3449]  eta: 3:05:16  lr: 0.000100  loss: 0.0404 (0.0583)  time: 3.7968  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 540/3449]  eta: 3:04:36  lr: 0.000100  loss: 0.0580 (0.0583)  time: 3.7988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 550/3449]  eta: 3:03:58  lr: 0.000100  loss: 0.0534 (0.0582)  time: 3.7961  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 560/3449]  eta: 3:03:18  lr: 0.000100  loss: 0.0553 (0.0583)  time: 3.7877  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 570/3449]  eta: 3:02:38  lr: 0.000100  loss: 0.0506 (0.0582)  time: 3.7646  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 580/3449]  eta: 3:01:51  lr: 0.000100  loss: 0.0405 (0.0580)  time: 3.6974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 590/3449]  eta: 3:01:12  lr: 0.000100  loss: 0.0373 (0.0580)  time: 3.7072  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 600/3449]  eta: 3:00:34  lr: 0.000100  loss: 0.0634 (0.0583)  time: 3.7949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 610/3449]  eta: 2:59:56  lr: 0.000100  loss: 0.0924 (0.0591)  time: 3.8043  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [ 620/3449]  eta: 2:59:17  lr: 0.000100  loss: 0.0794 (0.0593)  time: 3.7866  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 630/3449]  eta: 2:58:35  lr: 0.000100  loss: 0.0794 (0.0596)  time: 3.7491  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 640/3449]  eta: 2:57:59  lr: 0.000100  loss: 0.0498 (0.0593)  time: 3.7799  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 650/3449]  eta: 2:57:17  lr: 0.000100  loss: 0.0360 (0.0590)  time: 3.7789  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 660/3449]  eta: 2:56:37  lr: 0.000100  loss: 0.0449 (0.0590)  time: 3.7404  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 670/3449]  eta: 2:56:02  lr: 0.000100  loss: 0.0749 (0.0598)  time: 3.8106  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 680/3449]  eta: 2:55:21  lr: 0.000100  loss: 0.1201 (0.0606)  time: 3.8015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 690/3449]  eta: 2:54:43  lr: 0.000100  loss: 0.1043 (0.0609)  time: 3.7599  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 700/3449]  eta: 2:54:04  lr: 0.000100  loss: 0.0555 (0.0607)  time: 3.7853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 710/3449]  eta: 2:53:26  lr: 0.000100  loss: 0.0351 (0.0603)  time: 3.7860  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 720/3449]  eta: 2:52:51  lr: 0.000100  loss: 0.0291 (0.0598)  time: 3.8307  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 730/3449]  eta: 2:52:11  lr: 0.000100  loss: 0.0303 (0.0596)  time: 3.8110  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [ 740/3449]  eta: 2:51:34  lr: 0.000100  loss: 0.0377 (0.0592)  time: 3.7919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [ 750/3449]  eta: 2:50:54  lr: 0.000100  loss: 0.0368 (0.0590)  time: 3.7901  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [ 760/3449]  eta: 2:50:17  lr: 0.000100  loss: 0.0568 (0.0595)  time: 3.7811  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [ 770/3449]  eta: 2:49:38  lr: 0.000100  loss: 0.0916 (0.0600)  time: 3.7944  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 780/3449]  eta: 2:48:57  lr: 0.000100  loss: 0.0748 (0.0600)  time: 3.7438  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 790/3449]  eta: 2:48:17  lr: 0.000100  loss: 0.0465 (0.0598)  time: 3.7314  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 800/3449]  eta: 2:47:44  lr: 0.000100  loss: 0.0384 (0.0594)  time: 3.8434  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 810/3449]  eta: 2:47:04  lr: 0.000100  loss: 0.0246 (0.0590)  time: 3.8407  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 820/3449]  eta: 2:46:25  lr: 0.000100  loss: 0.0280 (0.0588)  time: 3.7604  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 830/3449]  eta: 2:45:48  lr: 0.000100  loss: 0.0357 (0.0584)  time: 3.7916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 840/3449]  eta: 2:45:07  lr: 0.000100  loss: 0.0276 (0.0582)  time: 3.7519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 850/3449]  eta: 2:44:28  lr: 0.000100  loss: 0.0476 (0.0583)  time: 3.7366  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 860/3449]  eta: 2:43:50  lr: 0.000100  loss: 0.0489 (0.0581)  time: 3.7908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 870/3449]  eta: 2:43:06  lr: 0.000100  loss: 0.0319 (0.0578)  time: 3.7053  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 880/3449]  eta: 2:42:27  lr: 0.000100  loss: 0.0393 (0.0580)  time: 3.6672  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 890/3449]  eta: 2:41:50  lr: 0.000100  loss: 0.0560 (0.0580)  time: 3.7932  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 900/3449]  eta: 2:41:08  lr: 0.000100  loss: 0.0560 (0.0580)  time: 3.7491  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 910/3449]  eta: 2:40:29  lr: 0.000100  loss: 0.0361 (0.0577)  time: 3.6954  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 920/3449]  eta: 2:39:51  lr: 0.000100  loss: 0.0345 (0.0577)  time: 3.7632  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 930/3449]  eta: 2:39:13  lr: 0.000100  loss: 0.0508 (0.0576)  time: 3.7962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 940/3449]  eta: 2:38:34  lr: 0.000100  loss: 0.0506 (0.0575)  time: 3.7817  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 950/3449]  eta: 2:37:58  lr: 0.000100  loss: 0.0395 (0.0575)  time: 3.7994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 960/3449]  eta: 2:37:23  lr: 0.000100  loss: 0.0523 (0.0575)  time: 3.8755  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 970/3449]  eta: 2:36:44  lr: 0.000100  loss: 0.0577 (0.0576)  time: 3.8423  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 980/3449]  eta: 2:36:08  lr: 0.000100  loss: 0.0578 (0.0576)  time: 3.8076  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [ 990/3449]  eta: 2:35:31  lr: 0.000100  loss: 0.0629 (0.0577)  time: 3.8464  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [1000/3449]  eta: 2:34:53  lr: 0.000100  loss: 0.0620 (0.0577)  time: 3.8178  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1010/3449]  eta: 2:34:16  lr: 0.000100  loss: 0.0605 (0.0577)  time: 3.8025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1020/3449]  eta: 2:33:37  lr: 0.000100  loss: 0.0552 (0.0577)  time: 3.7845  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1030/3449]  eta: 2:32:59  lr: 0.000100  loss: 0.0603 (0.0577)  time: 3.7854  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1040/3449]  eta: 2:32:21  lr: 0.000100  loss: 0.0610 (0.0577)  time: 3.8130  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1050/3449]  eta: 2:31:44  lr: 0.000100  loss: 0.0488 (0.0575)  time: 3.8029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1060/3449]  eta: 2:31:03  lr: 0.000100  loss: 0.0425 (0.0575)  time: 3.7419  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1070/3449]  eta: 2:30:26  lr: 0.000100  loss: 0.0612 (0.0575)  time: 3.7422  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1080/3449]  eta: 2:29:48  lr: 0.000100  loss: 0.0605 (0.0576)  time: 3.8033  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1090/3449]  eta: 2:29:09  lr: 0.000100  loss: 0.0796 (0.0579)  time: 3.7693  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1100/3449]  eta: 2:28:28  lr: 0.000100  loss: 0.1057 (0.0585)  time: 3.7025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1110/3449]  eta: 2:27:49  lr: 0.000100  loss: 0.0752 (0.0586)  time: 3.7132  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1120/3449]  eta: 2:27:10  lr: 0.000100  loss: 0.0672 (0.0587)  time: 3.7424  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1130/3449]  eta: 2:26:34  lr: 0.000100  loss: 0.0575 (0.0586)  time: 3.7936  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1140/3449]  eta: 2:25:57  lr: 0.000100  loss: 0.0353 (0.0583)  time: 3.8701  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1150/3449]  eta: 2:25:20  lr: 0.000100  loss: 0.0305 (0.0581)  time: 3.8516  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1160/3449]  eta: 2:24:42  lr: 0.000100  loss: 0.0280 (0.0578)  time: 3.7997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1170/3449]  eta: 2:24:05  lr: 0.000100  loss: 0.0247 (0.0575)  time: 3.8249  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1180/3449]  eta: 2:23:28  lr: 0.000100  loss: 0.0247 (0.0573)  time: 3.8425  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1190/3449]  eta: 2:22:51  lr: 0.000100  loss: 0.0313 (0.0571)  time: 3.8399  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1200/3449]  eta: 2:22:10  lr: 0.000100  loss: 0.0312 (0.0569)  time: 3.7532  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1210/3449]  eta: 2:21:35  lr: 0.000100  loss: 0.0312 (0.0569)  time: 3.7751  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1220/3449]  eta: 2:20:55  lr: 0.000100  loss: 0.0717 (0.0571)  time: 3.8101  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1230/3449]  eta: 2:20:19  lr: 0.000100  loss: 0.0660 (0.0570)  time: 3.8032  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1240/3449]  eta: 2:19:42  lr: 0.000100  loss: 0.0409 (0.0569)  time: 3.8633  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1250/3449]  eta: 2:19:02  lr: 0.000100  loss: 0.0385 (0.0567)  time: 3.7700  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1260/3449]  eta: 2:18:23  lr: 0.000100  loss: 0.0425 (0.0567)  time: 3.7244  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1270/3449]  eta: 2:17:46  lr: 0.000100  loss: 0.0527 (0.0567)  time: 3.7735  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1280/3449]  eta: 2:17:09  lr: 0.000100  loss: 0.0502 (0.0566)  time: 3.8390  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1290/3449]  eta: 2:16:31  lr: 0.000100  loss: 0.0464 (0.0565)  time: 3.8340  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1300/3449]  eta: 2:15:53  lr: 0.000100  loss: 0.0464 (0.0565)  time: 3.7921  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1310/3449]  eta: 2:15:14  lr: 0.000100  loss: 0.0525 (0.0565)  time: 3.7407  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1320/3449]  eta: 2:14:36  lr: 0.000100  loss: 0.0574 (0.0565)  time: 3.7697  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1330/3449]  eta: 2:13:59  lr: 0.000100  loss: 0.0539 (0.0565)  time: 3.8386  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1340/3449]  eta: 2:13:19  lr: 0.000100  loss: 0.0939 (0.0576)  time: 3.7524  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1350/3449]  eta: 2:12:41  lr: 0.000100  loss: 0.1515 (0.0581)  time: 3.7071  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1360/3449]  eta: 2:12:04  lr: 0.000100  loss: 0.0811 (0.0581)  time: 3.8030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1370/3449]  eta: 2:11:28  lr: 0.000100  loss: 0.0763 (0.0583)  time: 3.8866  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1380/3449]  eta: 2:10:48  lr: 0.000100  loss: 0.0700 (0.0583)  time: 3.7865  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1390/3449]  eta: 2:10:11  lr: 0.000100  loss: 0.0591 (0.0583)  time: 3.7651  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1400/3449]  eta: 2:09:33  lr: 0.000100  loss: 0.0525 (0.0582)  time: 3.8440  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1410/3449]  eta: 2:08:55  lr: 0.000100  loss: 0.0557 (0.0583)  time: 3.7864  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1420/3449]  eta: 2:08:17  lr: 0.000100  loss: 0.0738 (0.0586)  time: 3.7810  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1430/3449]  eta: 2:07:39  lr: 0.000100  loss: 0.1045 (0.0589)  time: 3.8123  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1440/3449]  eta: 2:07:03  lr: 0.000100  loss: 0.0704 (0.0589)  time: 3.8669  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1450/3449]  eta: 2:06:25  lr: 0.000100  loss: 0.0551 (0.0588)  time: 3.8578  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1460/3449]  eta: 2:05:48  lr: 0.000100  loss: 0.0379 (0.0587)  time: 3.8311  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1470/3449]  eta: 2:05:09  lr: 0.000100  loss: 0.0332 (0.0587)  time: 3.7656  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1480/3449]  eta: 2:04:30  lr: 0.000100  loss: 0.0330 (0.0585)  time: 3.6983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1490/3449]  eta: 2:03:55  lr: 0.000100  loss: 0.0299 (0.0583)  time: 3.8711  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1500/3449]  eta: 2:03:17  lr: 0.000100  loss: 0.0299 (0.0582)  time: 3.8966  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1510/3449]  eta: 2:02:37  lr: 0.000100  loss: 0.0516 (0.0582)  time: 3.7392  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1520/3449]  eta: 2:01:59  lr: 0.000100  loss: 0.0567 (0.0582)  time: 3.7396  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1530/3449]  eta: 2:01:22  lr: 0.000100  loss: 0.0583 (0.0582)  time: 3.7983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1540/3449]  eta: 2:00:43  lr: 0.000100  loss: 0.0575 (0.0582)  time: 3.7857  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1550/3449]  eta: 2:00:05  lr: 0.000100  loss: 0.0450 (0.0581)  time: 3.7677  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1560/3449]  eta: 1:59:28  lr: 0.000100  loss: 0.0429 (0.0582)  time: 3.8035  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1570/3449]  eta: 1:58:49  lr: 0.000100  loss: 0.0698 (0.0583)  time: 3.7973  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1580/3449]  eta: 1:58:12  lr: 0.000100  loss: 0.0680 (0.0583)  time: 3.8177  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1590/3449]  eta: 1:57:34  lr: 0.000100  loss: 0.0606 (0.0583)  time: 3.8323  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1600/3449]  eta: 1:56:57  lr: 0.000100  loss: 0.0602 (0.0584)  time: 3.8069  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1610/3449]  eta: 1:56:19  lr: 0.000100  loss: 0.0468 (0.0583)  time: 3.8119  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1620/3449]  eta: 1:55:41  lr: 0.000100  loss: 0.0468 (0.0584)  time: 3.8088  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1630/3449]  eta: 1:55:04  lr: 0.000100  loss: 0.0652 (0.0585)  time: 3.8306  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1640/3449]  eta: 1:54:27  lr: 0.000100  loss: 0.0551 (0.0584)  time: 3.8826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1650/3449]  eta: 1:53:49  lr: 0.000100  loss: 0.0551 (0.0584)  time: 3.8487  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [1660/3449]  eta: 1:53:11  lr: 0.000100  loss: 0.0485 (0.0583)  time: 3.7733  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1670/3449]  eta: 1:52:31  lr: 0.000100  loss: 0.0395 (0.0582)  time: 3.7061  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1680/3449]  eta: 1:51:53  lr: 0.000100  loss: 0.0421 (0.0582)  time: 3.7336  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1690/3449]  eta: 1:51:15  lr: 0.000100  loss: 0.0418 (0.0580)  time: 3.8010  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1700/3449]  eta: 1:50:37  lr: 0.000100  loss: 0.0275 (0.0579)  time: 3.7942  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1710/3449]  eta: 1:50:00  lr: 0.000100  loss: 0.0434 (0.0579)  time: 3.8029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1720/3449]  eta: 1:49:24  lr: 0.000100  loss: 0.0540 (0.0579)  time: 3.9014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1730/3449]  eta: 1:48:46  lr: 0.000100  loss: 0.0615 (0.0583)  time: 3.9263  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1740/3449]  eta: 1:48:08  lr: 0.000100  loss: 0.1147 (0.0585)  time: 3.8030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1750/3449]  eta: 1:47:30  lr: 0.000100  loss: 0.0929 (0.0586)  time: 3.7649  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1760/3449]  eta: 1:46:52  lr: 0.000100  loss: 0.0441 (0.0585)  time: 3.7894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1770/3449]  eta: 1:46:15  lr: 0.000100  loss: 0.0270 (0.0583)  time: 3.8438  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [1780/3449]  eta: 1:45:37  lr: 0.000100  loss: 0.0270 (0.0582)  time: 3.8380  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1790/3449]  eta: 1:44:59  lr: 0.000100  loss: 0.0477 (0.0582)  time: 3.8154  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1800/3449]  eta: 1:44:22  lr: 0.000100  loss: 0.0383 (0.0580)  time: 3.8490  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1810/3449]  eta: 1:43:44  lr: 0.000100  loss: 0.0349 (0.0579)  time: 3.8621  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1820/3449]  eta: 1:43:06  lr: 0.000100  loss: 0.0505 (0.0580)  time: 3.7947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1830/3449]  eta: 1:42:27  lr: 0.000100  loss: 0.0676 (0.0580)  time: 3.7189  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1840/3449]  eta: 1:41:49  lr: 0.000100  loss: 0.0441 (0.0579)  time: 3.7758  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1850/3449]  eta: 1:41:12  lr: 0.000100  loss: 0.0346 (0.0578)  time: 3.8532  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1860/3449]  eta: 1:40:34  lr: 0.000100  loss: 0.0330 (0.0577)  time: 3.8363  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1870/3449]  eta: 1:39:55  lr: 0.000100  loss: 0.0587 (0.0578)  time: 3.7582  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1880/3449]  eta: 1:39:17  lr: 0.000100  loss: 0.0555 (0.0577)  time: 3.7526  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1890/3449]  eta: 1:38:41  lr: 0.000100  loss: 0.0307 (0.0575)  time: 3.8778  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1900/3449]  eta: 1:38:03  lr: 0.000100  loss: 0.0287 (0.0575)  time: 3.9227  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1910/3449]  eta: 1:37:25  lr: 0.000100  loss: 0.0440 (0.0575)  time: 3.8184  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1920/3449]  eta: 1:36:47  lr: 0.000100  loss: 0.0390 (0.0574)  time: 3.7539  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1930/3449]  eta: 1:36:09  lr: 0.000100  loss: 0.0314 (0.0572)  time: 3.8154  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1940/3449]  eta: 1:35:32  lr: 0.000100  loss: 0.0350 (0.0572)  time: 3.8518  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1950/3449]  eta: 1:34:53  lr: 0.000100  loss: 0.0555 (0.0572)  time: 3.7703  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1960/3449]  eta: 1:34:15  lr: 0.000100  loss: 0.0626 (0.0573)  time: 3.7582  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1970/3449]  eta: 1:33:38  lr: 0.000100  loss: 0.0762 (0.0575)  time: 3.8801  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1980/3449]  eta: 1:33:00  lr: 0.000100  loss: 0.0819 (0.0576)  time: 3.8829  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [1990/3449]  eta: 1:32:21  lr: 0.000100  loss: 0.0778 (0.0577)  time: 3.7454  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2000/3449]  eta: 1:31:44  lr: 0.000100  loss: 0.0700 (0.0577)  time: 3.7447  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [2010/3449]  eta: 1:31:06  lr: 0.000100  loss: 0.0582 (0.0578)  time: 3.8344  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2020/3449]  eta: 1:30:29  lr: 0.000100  loss: 0.0699 (0.0579)  time: 3.9000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2030/3449]  eta: 1:29:51  lr: 0.000100  loss: 0.0580 (0.0579)  time: 3.8416  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2040/3449]  eta: 1:29:13  lr: 0.000100  loss: 0.0503 (0.0578)  time: 3.7659  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2050/3449]  eta: 1:28:35  lr: 0.000100  loss: 0.0503 (0.0579)  time: 3.8125  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2060/3449]  eta: 1:27:56  lr: 0.000100  loss: 0.0476 (0.0578)  time: 3.7415  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2070/3449]  eta: 1:27:18  lr: 0.000100  loss: 0.0289 (0.0576)  time: 3.7258  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2080/3449]  eta: 1:26:40  lr: 0.000100  loss: 0.0269 (0.0575)  time: 3.7917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [2090/3449]  eta: 1:26:02  lr: 0.000100  loss: 0.0263 (0.0573)  time: 3.7702  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2100/3449]  eta: 1:25:24  lr: 0.000100  loss: 0.0233 (0.0572)  time: 3.8132  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2110/3449]  eta: 1:24:47  lr: 0.000100  loss: 0.0244 (0.0571)  time: 3.8784  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2120/3449]  eta: 1:24:08  lr: 0.000100  loss: 0.0554 (0.0571)  time: 3.8179  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2130/3449]  eta: 1:23:30  lr: 0.000100  loss: 0.0486 (0.0571)  time: 3.7667  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2140/3449]  eta: 1:22:52  lr: 0.000100  loss: 0.0306 (0.0569)  time: 3.8175  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2150/3449]  eta: 1:22:14  lr: 0.000100  loss: 0.0270 (0.0568)  time: 3.7876  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2160/3449]  eta: 1:21:37  lr: 0.000100  loss: 0.0271 (0.0567)  time: 3.8125  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2170/3449]  eta: 1:20:58  lr: 0.000100  loss: 0.0438 (0.0567)  time: 3.7886  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2180/3449]  eta: 1:20:19  lr: 0.000100  loss: 0.0538 (0.0567)  time: 3.6921  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2190/3449]  eta: 1:19:42  lr: 0.000100  loss: 0.0536 (0.0567)  time: 3.7569  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2200/3449]  eta: 1:19:03  lr: 0.000100  loss: 0.0536 (0.0567)  time: 3.7868  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2210/3449]  eta: 1:18:25  lr: 0.000100  loss: 0.0493 (0.0567)  time: 3.7808  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2220/3449]  eta: 1:17:47  lr: 0.000100  loss: 0.0443 (0.0566)  time: 3.7928  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2230/3449]  eta: 1:17:10  lr: 0.000100  loss: 0.0290 (0.0565)  time: 3.8594  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2240/3449]  eta: 1:16:32  lr: 0.000100  loss: 0.0300 (0.0564)  time: 3.8436  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2250/3449]  eta: 1:15:54  lr: 0.000100  loss: 0.0332 (0.0563)  time: 3.7576  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2260/3449]  eta: 1:15:16  lr: 0.000100  loss: 0.0332 (0.0562)  time: 3.7691  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2270/3449]  eta: 1:14:38  lr: 0.000100  loss: 0.0355 (0.0562)  time: 3.7964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2280/3449]  eta: 1:14:00  lr: 0.000100  loss: 0.0541 (0.0562)  time: 3.8231  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2290/3449]  eta: 1:13:22  lr: 0.000100  loss: 0.0570 (0.0562)  time: 3.8300  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2300/3449]  eta: 1:12:44  lr: 0.000100  loss: 0.0520 (0.0562)  time: 3.7963  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2310/3449]  eta: 1:12:06  lr: 0.000100  loss: 0.0456 (0.0561)  time: 3.8243  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [2320/3449]  eta: 1:11:28  lr: 0.000100  loss: 0.0468 (0.0561)  time: 3.8370  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2330/3449]  eta: 1:10:51  lr: 0.000100  loss: 0.0488 (0.0561)  time: 3.8402  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2340/3449]  eta: 1:10:13  lr: 0.000100  loss: 0.0512 (0.0561)  time: 3.8253  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2350/3449]  eta: 1:09:35  lr: 0.000100  loss: 0.0660 (0.0561)  time: 3.7905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2360/3449]  eta: 1:08:57  lr: 0.000100  loss: 0.0692 (0.0562)  time: 3.8337  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2370/3449]  eta: 1:08:19  lr: 0.000100  loss: 0.0606 (0.0562)  time: 3.8789  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2380/3449]  eta: 1:07:41  lr: 0.000100  loss: 0.0575 (0.0562)  time: 3.7879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2390/3449]  eta: 1:07:03  lr: 0.000100  loss: 0.0554 (0.0562)  time: 3.7752  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2400/3449]  eta: 1:06:25  lr: 0.000100  loss: 0.0560 (0.0563)  time: 3.8293  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2410/3449]  eta: 1:05:47  lr: 0.000100  loss: 0.0691 (0.0564)  time: 3.7655  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2420/3449]  eta: 1:05:09  lr: 0.000100  loss: 0.0763 (0.0565)  time: 3.8404  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2430/3449]  eta: 1:04:31  lr: 0.000100  loss: 0.0352 (0.0564)  time: 3.8677  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2440/3449]  eta: 1:03:54  lr: 0.000100  loss: 0.0271 (0.0563)  time: 3.8351  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2450/3449]  eta: 1:03:15  lr: 0.000100  loss: 0.0252 (0.0562)  time: 3.7805  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2460/3449]  eta: 1:02:38  lr: 0.000100  loss: 0.0259 (0.0561)  time: 3.7725  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [2470/3449]  eta: 1:01:59  lr: 0.000100  loss: 0.0334 (0.0560)  time: 3.8181  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [2480/3449]  eta: 1:01:22  lr: 0.000100  loss: 0.0326 (0.0559)  time: 3.8738  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2490/3449]  eta: 1:00:44  lr: 0.000100  loss: 0.0331 (0.0558)  time: 3.8379  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2500/3449]  eta: 1:00:06  lr: 0.000100  loss: 0.0402 (0.0558)  time: 3.7301  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2510/3449]  eta: 0:59:28  lr: 0.000100  loss: 0.0590 (0.0558)  time: 3.7866  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2520/3449]  eta: 0:58:50  lr: 0.000100  loss: 0.0594 (0.0559)  time: 3.8003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2530/3449]  eta: 0:58:11  lr: 0.000100  loss: 0.0755 (0.0560)  time: 3.7667  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2540/3449]  eta: 0:57:34  lr: 0.000100  loss: 0.0762 (0.0560)  time: 3.8043  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [2550/3449]  eta: 0:56:56  lr: 0.000100  loss: 0.0669 (0.0560)  time: 3.8632  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [2560/3449]  eta: 0:56:18  lr: 0.000100  loss: 0.0445 (0.0560)  time: 3.8597  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2570/3449]  eta: 0:55:40  lr: 0.000100  loss: 0.0378 (0.0560)  time: 3.7781  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2580/3449]  eta: 0:55:02  lr: 0.000100  loss: 0.0350 (0.0559)  time: 3.7677  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2590/3449]  eta: 0:54:24  lr: 0.000100  loss: 0.0325 (0.0558)  time: 3.7866  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2600/3449]  eta: 0:53:46  lr: 0.000100  loss: 0.0328 (0.0557)  time: 3.7899  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [2610/3449]  eta: 0:53:08  lr: 0.000100  loss: 0.0360 (0.0556)  time: 3.8564  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [2620/3449]  eta: 0:52:30  lr: 0.000100  loss: 0.0444 (0.0557)  time: 3.7848  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2630/3449]  eta: 0:51:52  lr: 0.000100  loss: 0.0773 (0.0558)  time: 3.8439  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2640/3449]  eta: 0:51:14  lr: 0.000100  loss: 0.0842 (0.0559)  time: 3.8825  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2650/3449]  eta: 0:50:36  lr: 0.000100  loss: 0.1033 (0.0561)  time: 3.7516  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2660/3449]  eta: 0:49:58  lr: 0.000100  loss: 0.0895 (0.0562)  time: 3.7496  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2670/3449]  eta: 0:49:20  lr: 0.000100  loss: 0.0895 (0.0565)  time: 3.7474  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [2680/3449]  eta: 0:48:42  lr: 0.000100  loss: 0.1483 (0.0569)  time: 3.8202  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [2690/3449]  eta: 0:48:04  lr: 0.000100  loss: 0.1272 (0.0572)  time: 3.7815  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2700/3449]  eta: 0:47:25  lr: 0.000100  loss: 0.1154 (0.0573)  time: 3.7059  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2710/3449]  eta: 0:46:47  lr: 0.000100  loss: 0.1016 (0.0575)  time: 3.7468  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2720/3449]  eta: 0:46:09  lr: 0.000100  loss: 0.0502 (0.0574)  time: 3.7864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2730/3449]  eta: 0:45:31  lr: 0.000100  loss: 0.0464 (0.0574)  time: 3.7930  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2740/3449]  eta: 0:44:53  lr: 0.000100  loss: 0.0613 (0.0574)  time: 3.8001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2750/3449]  eta: 0:44:15  lr: 0.000100  loss: 0.0522 (0.0574)  time: 3.8104  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2760/3449]  eta: 0:43:37  lr: 0.000100  loss: 0.0457 (0.0574)  time: 3.8207  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2770/3449]  eta: 0:42:59  lr: 0.000100  loss: 0.0358 (0.0573)  time: 3.8256  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2780/3449]  eta: 0:42:22  lr: 0.000100  loss: 0.0299 (0.0572)  time: 3.8090  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2790/3449]  eta: 0:41:43  lr: 0.000100  loss: 0.0463 (0.0573)  time: 3.7606  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2800/3449]  eta: 0:41:05  lr: 0.000100  loss: 0.0621 (0.0572)  time: 3.7558  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2810/3449]  eta: 0:40:27  lr: 0.000100  loss: 0.0418 (0.0572)  time: 3.7893  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2820/3449]  eta: 0:39:49  lr: 0.000100  loss: 0.0487 (0.0572)  time: 3.7566  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2830/3449]  eta: 0:39:11  lr: 0.000100  loss: 0.0576 (0.0572)  time: 3.8361  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2840/3449]  eta: 0:38:34  lr: 0.000100  loss: 0.0837 (0.0574)  time: 3.9440  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2850/3449]  eta: 0:37:56  lr: 0.000100  loss: 0.0781 (0.0574)  time: 3.8432  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2860/3449]  eta: 0:37:18  lr: 0.000100  loss: 0.0501 (0.0574)  time: 3.7537  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2870/3449]  eta: 0:36:40  lr: 0.000100  loss: 0.0478 (0.0574)  time: 3.8176  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2880/3449]  eta: 0:36:02  lr: 0.000100  loss: 0.0517 (0.0574)  time: 3.8278  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2890/3449]  eta: 0:35:24  lr: 0.000100  loss: 0.0597 (0.0575)  time: 3.7600  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2900/3449]  eta: 0:34:46  lr: 0.000100  loss: 0.1149 (0.0578)  time: 3.7913  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2910/3449]  eta: 0:34:08  lr: 0.000100  loss: 0.0959 (0.0578)  time: 3.8102  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2920/3449]  eta: 0:33:30  lr: 0.000100  loss: 0.0706 (0.0579)  time: 3.8150  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2930/3449]  eta: 0:32:52  lr: 0.000100  loss: 0.0602 (0.0578)  time: 3.8879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2940/3449]  eta: 0:32:14  lr: 0.000100  loss: 0.0639 (0.0579)  time: 3.8117  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2950/3449]  eta: 0:31:36  lr: 0.000100  loss: 0.0670 (0.0579)  time: 3.7563  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2960/3449]  eta: 0:30:58  lr: 0.000100  loss: 0.0535 (0.0579)  time: 3.8061  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [2970/3449]  eta: 0:30:20  lr: 0.000100  loss: 0.0431 (0.0578)  time: 3.8469  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [2980/3449]  eta: 0:29:42  lr: 0.000100  loss: 0.0342 (0.0578)  time: 3.8250  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [2990/3449]  eta: 0:29:04  lr: 0.000100  loss: 0.0699 (0.0580)  time: 3.8213  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [3000/3449]  eta: 0:28:26  lr: 0.000100  loss: 0.0787 (0.0580)  time: 3.7983  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [3010/3449]  eta: 0:27:48  lr: 0.000100  loss: 0.0844 (0.0581)  time: 3.7994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3020/3449]  eta: 0:27:10  lr: 0.000100  loss: 0.0971 (0.0582)  time: 3.8543  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3030/3449]  eta: 0:26:32  lr: 0.000100  loss: 0.0534 (0.0582)  time: 3.8594  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [3040/3449]  eta: 0:25:54  lr: 0.000100  loss: 0.0428 (0.0581)  time: 3.8134  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [3050/3449]  eta: 0:25:16  lr: 0.000100  loss: 0.0340 (0.0580)  time: 3.8602  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [3060/3449]  eta: 0:24:38  lr: 0.000100  loss: 0.0390 (0.0580)  time: 3.8735  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3070/3449]  eta: 0:24:00  lr: 0.000100  loss: 0.0509 (0.0580)  time: 3.8064  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3080/3449]  eta: 0:23:22  lr: 0.000100  loss: 0.0347 (0.0579)  time: 3.8371  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3090/3449]  eta: 0:22:44  lr: 0.000100  loss: 0.0319 (0.0579)  time: 3.8057  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3100/3449]  eta: 0:22:06  lr: 0.000100  loss: 0.0255 (0.0578)  time: 3.7841  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3110/3449]  eta: 0:21:28  lr: 0.000100  loss: 0.0258 (0.0577)  time: 3.8124  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3120/3449]  eta: 0:20:50  lr: 0.000100  loss: 0.0327 (0.0576)  time: 3.7413  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3130/3449]  eta: 0:20:12  lr: 0.000100  loss: 0.0330 (0.0576)  time: 3.7259  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3140/3449]  eta: 0:19:34  lr: 0.000100  loss: 0.0512 (0.0576)  time: 3.8182  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3150/3449]  eta: 0:18:56  lr: 0.000100  loss: 0.0442 (0.0575)  time: 3.7904  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3160/3449]  eta: 0:18:18  lr: 0.000100  loss: 0.0431 (0.0575)  time: 3.7317  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3170/3449]  eta: 0:17:40  lr: 0.000100  loss: 0.0439 (0.0574)  time: 3.7465  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3180/3449]  eta: 0:17:02  lr: 0.000100  loss: 0.0403 (0.0574)  time: 3.8008  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3190/3449]  eta: 0:16:24  lr: 0.000100  loss: 0.0410 (0.0573)  time: 3.8590  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3200/3449]  eta: 0:15:46  lr: 0.000100  loss: 0.0470 (0.0573)  time: 3.8410  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [3210/3449]  eta: 0:15:08  lr: 0.000100  loss: 0.0371 (0.0572)  time: 3.8173  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:15]  [3220/3449]  eta: 0:14:30  lr: 0.000100  loss: 0.0371 (0.0572)  time: 3.7397  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3230/3449]  eta: 0:13:52  lr: 0.000100  loss: 0.0529 (0.0572)  time: 3.6923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3240/3449]  eta: 0:13:14  lr: 0.000100  loss: 0.0593 (0.0572)  time: 3.8303  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3250/3449]  eta: 0:12:36  lr: 0.000100  loss: 0.0525 (0.0572)  time: 3.8377  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3260/3449]  eta: 0:11:58  lr: 0.000100  loss: 0.0558 (0.0574)  time: 3.7628  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3270/3449]  eta: 0:11:20  lr: 0.000100  loss: 0.0837 (0.0574)  time: 3.7489  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3280/3449]  eta: 0:10:42  lr: 0.000100  loss: 0.0780 (0.0575)  time: 3.7803  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3290/3449]  eta: 0:10:04  lr: 0.000100  loss: 0.0602 (0.0575)  time: 3.8143  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3300/3449]  eta: 0:09:26  lr: 0.000100  loss: 0.0520 (0.0575)  time: 3.7885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3310/3449]  eta: 0:08:48  lr: 0.000100  loss: 0.0520 (0.0575)  time: 3.7701  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3320/3449]  eta: 0:08:10  lr: 0.000100  loss: 0.0533 (0.0575)  time: 3.7397  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3330/3449]  eta: 0:07:32  lr: 0.000100  loss: 0.0712 (0.0576)  time: 3.7440  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3340/3449]  eta: 0:06:54  lr: 0.000100  loss: 0.0942 (0.0577)  time: 3.7950  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3350/3449]  eta: 0:06:16  lr: 0.000100  loss: 0.0647 (0.0577)  time: 3.8068  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3360/3449]  eta: 0:05:38  lr: 0.000100  loss: 0.0544 (0.0577)  time: 3.8567  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3370/3449]  eta: 0:05:00  lr: 0.000100  loss: 0.0567 (0.0577)  time: 3.8314  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3380/3449]  eta: 0:04:22  lr: 0.000100  loss: 0.0473 (0.0576)  time: 3.7468  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3390/3449]  eta: 0:03:44  lr: 0.000100  loss: 0.0467 (0.0576)  time: 3.8061  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3400/3449]  eta: 0:03:06  lr: 0.000100  loss: 0.0536 (0.0577)  time: 3.8138  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3410/3449]  eta: 0:02:28  lr: 0.000100  loss: 0.0376 (0.0576)  time: 3.7635  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3420/3449]  eta: 0:01:50  lr: 0.000100  loss: 0.0300 (0.0575)  time: 3.7556  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3430/3449]  eta: 0:01:12  lr: 0.000100  loss: 0.0290 (0.0574)  time: 3.7769  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3440/3449]  eta: 0:00:34  lr: 0.000100  loss: 0.0396 (0.0574)  time: 3.7999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0565 (0.0574)  time: 3.8051  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:15] Total time: 3:38:25 (3.7997 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0565 (0.0574)\n",
      "Valid: [epoch:15]  [ 0/14]  eta: 0:04:21  loss: 0.0165 (0.0165)  time: 18.6828  data: 0.4901  max mem: 34968\n",
      "Valid: [epoch:15]  [13/14]  eta: 0:00:18  loss: 0.0177 (0.0181)  time: 18.2458  data: 0.0352  max mem: 34968\n",
      "Valid: [epoch:15] Total time: 0:04:15 (18.2541 s / it)\n",
      "Averaged stats: loss: 0.0177 (0.0181)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_15_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.018%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:16]  [   0/3449]  eta: 5:15:53  lr: 0.000100  loss: 0.0255 (0.0255)  time: 5.4955  data: 1.4471  max mem: 34968\n",
      "Train: [epoch:16]  [  10/3449]  eta: 3:47:18  lr: 0.000100  loss: 0.0763 (0.0720)  time: 3.9658  data: 0.1317  max mem: 34968\n",
      "Train: [epoch:16]  [  20/3449]  eta: 3:43:09  lr: 0.000100  loss: 0.0631 (0.0658)  time: 3.8252  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [  30/3449]  eta: 3:41:19  lr: 0.000100  loss: 0.0501 (0.0601)  time: 3.8389  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [  40/3449]  eta: 3:38:25  lr: 0.000100  loss: 0.0510 (0.0615)  time: 3.7813  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [  50/3449]  eta: 3:37:05  lr: 0.000100  loss: 0.0678 (0.0617)  time: 3.7520  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [  60/3449]  eta: 3:36:38  lr: 0.000100  loss: 0.0614 (0.0611)  time: 3.8170  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [  70/3449]  eta: 3:35:57  lr: 0.000100  loss: 0.0575 (0.0614)  time: 3.8408  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [  80/3449]  eta: 3:35:16  lr: 0.000100  loss: 0.0503 (0.0579)  time: 3.8291  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [  90/3449]  eta: 3:33:56  lr: 0.000100  loss: 0.0347 (0.0559)  time: 3.7754  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 100/3449]  eta: 3:32:32  lr: 0.000100  loss: 0.0375 (0.0547)  time: 3.7025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 110/3449]  eta: 3:31:41  lr: 0.000100  loss: 0.0620 (0.0569)  time: 3.7236  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 120/3449]  eta: 3:31:13  lr: 0.000100  loss: 0.0718 (0.0579)  time: 3.8022  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [ 130/3449]  eta: 3:30:57  lr: 0.000100  loss: 0.0607 (0.0576)  time: 3.8677  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 140/3449]  eta: 3:29:44  lr: 0.000100  loss: 0.0562 (0.0584)  time: 3.7809  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 150/3449]  eta: 3:28:46  lr: 0.000100  loss: 0.0507 (0.0574)  time: 3.6888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 160/3449]  eta: 3:28:20  lr: 0.000100  loss: 0.0434 (0.0571)  time: 3.7834  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 170/3449]  eta: 3:27:38  lr: 0.000100  loss: 0.0606 (0.0575)  time: 3.8162  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 180/3449]  eta: 3:26:56  lr: 0.000100  loss: 0.0620 (0.0578)  time: 3.7780  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 190/3449]  eta: 3:26:06  lr: 0.000100  loss: 0.0620 (0.0582)  time: 3.7524  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 200/3449]  eta: 3:25:19  lr: 0.000100  loss: 0.0774 (0.0604)  time: 3.7341  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 210/3449]  eta: 3:24:30  lr: 0.000100  loss: 0.0817 (0.0619)  time: 3.7310  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 220/3449]  eta: 3:23:58  lr: 0.000100  loss: 0.0747 (0.0620)  time: 3.7742  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 230/3449]  eta: 3:23:26  lr: 0.000100  loss: 0.0692 (0.0625)  time: 3.8295  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 240/3449]  eta: 3:22:52  lr: 0.000100  loss: 0.0422 (0.0613)  time: 3.8266  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 250/3449]  eta: 3:22:15  lr: 0.000100  loss: 0.0324 (0.0605)  time: 3.8126  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 260/3449]  eta: 3:21:32  lr: 0.000100  loss: 0.0514 (0.0616)  time: 3.7739  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 270/3449]  eta: 3:20:40  lr: 0.000100  loss: 0.0959 (0.0635)  time: 3.7095  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 280/3449]  eta: 3:20:13  lr: 0.000100  loss: 0.0802 (0.0638)  time: 3.7803  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 290/3449]  eta: 3:19:38  lr: 0.000100  loss: 0.0648 (0.0637)  time: 3.8529  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 300/3449]  eta: 3:19:00  lr: 0.000100  loss: 0.0484 (0.0629)  time: 3.8019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 310/3449]  eta: 3:18:27  lr: 0.000100  loss: 0.0334 (0.0619)  time: 3.8161  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 320/3449]  eta: 3:17:57  lr: 0.000100  loss: 0.0255 (0.0609)  time: 3.8576  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 330/3449]  eta: 3:17:24  lr: 0.000100  loss: 0.0241 (0.0598)  time: 3.8616  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 340/3449]  eta: 3:16:55  lr: 0.000100  loss: 0.0343 (0.0594)  time: 3.8756  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 350/3449]  eta: 3:16:11  lr: 0.000100  loss: 0.0506 (0.0594)  time: 3.8134  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 360/3449]  eta: 3:15:38  lr: 0.000100  loss: 0.0603 (0.0595)  time: 3.7956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 370/3449]  eta: 3:15:02  lr: 0.000100  loss: 0.0613 (0.0595)  time: 3.8392  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 380/3449]  eta: 3:14:21  lr: 0.000100  loss: 0.0695 (0.0607)  time: 3.7907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 390/3449]  eta: 3:13:35  lr: 0.000100  loss: 0.1127 (0.0621)  time: 3.7307  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 400/3449]  eta: 3:12:58  lr: 0.000100  loss: 0.0758 (0.0620)  time: 3.7555  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 410/3449]  eta: 3:12:21  lr: 0.000100  loss: 0.0635 (0.0621)  time: 3.8105  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 420/3449]  eta: 3:11:41  lr: 0.000100  loss: 0.0639 (0.0622)  time: 3.7926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 430/3449]  eta: 3:11:07  lr: 0.000100  loss: 0.0621 (0.0621)  time: 3.8122  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 440/3449]  eta: 3:10:34  lr: 0.000100  loss: 0.0646 (0.0622)  time: 3.8627  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 450/3449]  eta: 3:09:57  lr: 0.000100  loss: 0.0647 (0.0623)  time: 3.8415  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 460/3449]  eta: 3:09:15  lr: 0.000100  loss: 0.0561 (0.0621)  time: 3.7720  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 470/3449]  eta: 3:08:43  lr: 0.000100  loss: 0.0585 (0.0621)  time: 3.8195  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 480/3449]  eta: 3:08:12  lr: 0.000100  loss: 0.0585 (0.0620)  time: 3.9085  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 490/3449]  eta: 3:07:38  lr: 0.000100  loss: 0.0551 (0.0620)  time: 3.8945  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 500/3449]  eta: 3:07:00  lr: 0.000100  loss: 0.0573 (0.0623)  time: 3.8362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 510/3449]  eta: 3:06:19  lr: 0.000100  loss: 0.0824 (0.0631)  time: 3.7761  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 520/3449]  eta: 3:05:40  lr: 0.000100  loss: 0.0967 (0.0636)  time: 3.7671  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 530/3449]  eta: 3:04:58  lr: 0.000100  loss: 0.0752 (0.0637)  time: 3.7609  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 540/3449]  eta: 3:04:18  lr: 0.000100  loss: 0.0631 (0.0637)  time: 3.7473  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 550/3449]  eta: 3:03:36  lr: 0.000100  loss: 0.0704 (0.0643)  time: 3.7436  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 560/3449]  eta: 3:02:54  lr: 0.000100  loss: 0.0935 (0.0646)  time: 3.7239  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 570/3449]  eta: 3:02:16  lr: 0.000100  loss: 0.0725 (0.0645)  time: 3.7590  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 580/3449]  eta: 3:01:33  lr: 0.000100  loss: 0.0468 (0.0644)  time: 3.7534  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 590/3449]  eta: 3:00:58  lr: 0.000100  loss: 0.0468 (0.0641)  time: 3.7782  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 600/3449]  eta: 3:00:20  lr: 0.000100  loss: 0.0416 (0.0637)  time: 3.8264  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 610/3449]  eta: 2:59:48  lr: 0.000100  loss: 0.0452 (0.0635)  time: 3.8634  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 620/3449]  eta: 2:59:10  lr: 0.000100  loss: 0.0665 (0.0640)  time: 3.8633  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 630/3449]  eta: 2:58:24  lr: 0.000100  loss: 0.0881 (0.0645)  time: 3.7130  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 640/3449]  eta: 2:57:45  lr: 0.000100  loss: 0.0812 (0.0646)  time: 3.6960  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 650/3449]  eta: 2:57:12  lr: 0.000100  loss: 0.0650 (0.0645)  time: 3.8389  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 660/3449]  eta: 2:56:34  lr: 0.000100  loss: 0.0679 (0.0648)  time: 3.8566  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 670/3449]  eta: 2:55:48  lr: 0.000100  loss: 0.0934 (0.0652)  time: 3.6998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 680/3449]  eta: 2:55:08  lr: 0.000100  loss: 0.1037 (0.0661)  time: 3.6704  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 690/3449]  eta: 2:54:30  lr: 0.000100  loss: 0.0918 (0.0664)  time: 3.7748  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 700/3449]  eta: 2:53:55  lr: 0.000100  loss: 0.0618 (0.0661)  time: 3.8323  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 710/3449]  eta: 2:53:14  lr: 0.000100  loss: 0.0574 (0.0661)  time: 3.7893  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 720/3449]  eta: 2:52:35  lr: 0.000100  loss: 0.0589 (0.0659)  time: 3.7513  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 730/3449]  eta: 2:51:56  lr: 0.000100  loss: 0.0470 (0.0656)  time: 3.7709  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 740/3449]  eta: 2:51:19  lr: 0.000100  loss: 0.0417 (0.0654)  time: 3.7929  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 750/3449]  eta: 2:50:41  lr: 0.000100  loss: 0.0511 (0.0653)  time: 3.8124  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 760/3449]  eta: 2:50:04  lr: 0.000100  loss: 0.0590 (0.0653)  time: 3.7991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 770/3449]  eta: 2:49:28  lr: 0.000100  loss: 0.0521 (0.0651)  time: 3.8352  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 780/3449]  eta: 2:48:51  lr: 0.000100  loss: 0.0534 (0.0651)  time: 3.8427  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [ 790/3449]  eta: 2:48:11  lr: 0.000100  loss: 0.0721 (0.0654)  time: 3.7780  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 800/3449]  eta: 2:47:34  lr: 0.000100  loss: 0.0782 (0.0656)  time: 3.7732  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 810/3449]  eta: 2:46:54  lr: 0.000100  loss: 0.0709 (0.0656)  time: 3.7766  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 820/3449]  eta: 2:46:14  lr: 0.000100  loss: 0.0650 (0.0655)  time: 3.7401  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 830/3449]  eta: 2:45:35  lr: 0.000100  loss: 0.0395 (0.0652)  time: 3.7431  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 840/3449]  eta: 2:44:59  lr: 0.000100  loss: 0.0310 (0.0648)  time: 3.8053  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 850/3449]  eta: 2:44:19  lr: 0.000100  loss: 0.0350 (0.0645)  time: 3.7933  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 860/3449]  eta: 2:43:41  lr: 0.000100  loss: 0.0356 (0.0641)  time: 3.7589  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 870/3449]  eta: 2:43:04  lr: 0.000100  loss: 0.0326 (0.0637)  time: 3.8021  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 880/3449]  eta: 2:42:27  lr: 0.000100  loss: 0.0287 (0.0634)  time: 3.8208  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 890/3449]  eta: 2:41:47  lr: 0.000100  loss: 0.0421 (0.0634)  time: 3.7732  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 900/3449]  eta: 2:41:08  lr: 0.000100  loss: 0.0797 (0.0640)  time: 3.7415  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 910/3449]  eta: 2:40:31  lr: 0.000100  loss: 0.1167 (0.0645)  time: 3.7898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 920/3449]  eta: 2:39:54  lr: 0.000100  loss: 0.1021 (0.0649)  time: 3.8214  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 930/3449]  eta: 2:39:20  lr: 0.000100  loss: 0.0782 (0.0648)  time: 3.8867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 940/3449]  eta: 2:38:40  lr: 0.000100  loss: 0.0485 (0.0646)  time: 3.8416  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 950/3449]  eta: 2:38:03  lr: 0.000100  loss: 0.0485 (0.0645)  time: 3.7802  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 960/3449]  eta: 2:37:26  lr: 0.000100  loss: 0.0538 (0.0645)  time: 3.8329  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [ 970/3449]  eta: 2:36:48  lr: 0.000100  loss: 0.0548 (0.0644)  time: 3.8180  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 980/3449]  eta: 2:36:07  lr: 0.000100  loss: 0.0563 (0.0644)  time: 3.7197  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [ 990/3449]  eta: 2:35:30  lr: 0.000100  loss: 0.0563 (0.0643)  time: 3.7396  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1000/3449]  eta: 2:34:52  lr: 0.000100  loss: 0.0588 (0.0642)  time: 3.8148  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1010/3449]  eta: 2:34:14  lr: 0.000100  loss: 0.0426 (0.0640)  time: 3.8060  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1020/3449]  eta: 2:33:35  lr: 0.000100  loss: 0.0387 (0.0639)  time: 3.7768  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1030/3449]  eta: 2:32:57  lr: 0.000100  loss: 0.0568 (0.0638)  time: 3.7601  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1040/3449]  eta: 2:32:18  lr: 0.000100  loss: 0.0418 (0.0636)  time: 3.7610  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1050/3449]  eta: 2:31:40  lr: 0.000100  loss: 0.0321 (0.0633)  time: 3.7651  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1060/3449]  eta: 2:31:03  lr: 0.000100  loss: 0.0513 (0.0633)  time: 3.8154  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1070/3449]  eta: 2:30:27  lr: 0.000100  loss: 0.0564 (0.0632)  time: 3.8666  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1080/3449]  eta: 2:29:49  lr: 0.000100  loss: 0.0572 (0.0634)  time: 3.8445  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1090/3449]  eta: 2:29:13  lr: 0.000100  loss: 0.0927 (0.0638)  time: 3.8321  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1100/3449]  eta: 2:28:36  lr: 0.000100  loss: 0.1002 (0.0641)  time: 3.8527  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1110/3449]  eta: 2:27:58  lr: 0.000100  loss: 0.1042 (0.0644)  time: 3.8115  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1120/3449]  eta: 2:27:20  lr: 0.000100  loss: 0.0700 (0.0644)  time: 3.7998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1130/3449]  eta: 2:26:43  lr: 0.000100  loss: 0.0632 (0.0644)  time: 3.8247  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1140/3449]  eta: 2:26:03  lr: 0.000100  loss: 0.0659 (0.0644)  time: 3.7713  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1150/3449]  eta: 2:25:24  lr: 0.000100  loss: 0.0592 (0.0644)  time: 3.7303  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1160/3449]  eta: 2:24:46  lr: 0.000100  loss: 0.0525 (0.0643)  time: 3.7767  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1170/3449]  eta: 2:24:11  lr: 0.000100  loss: 0.0583 (0.0645)  time: 3.8543  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1180/3449]  eta: 2:23:34  lr: 0.000100  loss: 0.0928 (0.0647)  time: 3.8935  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1190/3449]  eta: 2:22:54  lr: 0.000100  loss: 0.0839 (0.0648)  time: 3.7795  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1200/3449]  eta: 2:22:16  lr: 0.000100  loss: 0.0570 (0.0647)  time: 3.7298  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1210/3449]  eta: 2:21:39  lr: 0.000100  loss: 0.0531 (0.0647)  time: 3.8251  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1220/3449]  eta: 2:21:01  lr: 0.000100  loss: 0.0599 (0.0647)  time: 3.8141  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1230/3449]  eta: 2:20:23  lr: 0.000100  loss: 0.0547 (0.0645)  time: 3.7735  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1240/3449]  eta: 2:19:45  lr: 0.000100  loss: 0.0481 (0.0645)  time: 3.7991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1250/3449]  eta: 2:19:08  lr: 0.000100  loss: 0.0696 (0.0646)  time: 3.8381  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1260/3449]  eta: 2:18:31  lr: 0.000100  loss: 0.0696 (0.0646)  time: 3.8690  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1270/3449]  eta: 2:17:51  lr: 0.000100  loss: 0.0611 (0.0646)  time: 3.7725  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1280/3449]  eta: 2:17:13  lr: 0.000100  loss: 0.0626 (0.0646)  time: 3.7099  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1290/3449]  eta: 2:16:33  lr: 0.000100  loss: 0.0661 (0.0647)  time: 3.7088  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1300/3449]  eta: 2:15:54  lr: 0.000100  loss: 0.0968 (0.0650)  time: 3.7041  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1310/3449]  eta: 2:15:15  lr: 0.000100  loss: 0.0968 (0.0651)  time: 3.7473  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1320/3449]  eta: 2:14:38  lr: 0.000100  loss: 0.0807 (0.0652)  time: 3.7917  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1330/3449]  eta: 2:14:00  lr: 0.000100  loss: 0.0666 (0.0651)  time: 3.8292  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1340/3449]  eta: 2:13:20  lr: 0.000100  loss: 0.0564 (0.0650)  time: 3.7319  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1350/3449]  eta: 2:12:42  lr: 0.000100  loss: 0.0616 (0.0650)  time: 3.7304  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1360/3449]  eta: 2:12:03  lr: 0.000100  loss: 0.0510 (0.0648)  time: 3.7601  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1370/3449]  eta: 2:11:24  lr: 0.000100  loss: 0.0376 (0.0646)  time: 3.7043  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1380/3449]  eta: 2:10:47  lr: 0.000100  loss: 0.0349 (0.0645)  time: 3.7872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1390/3449]  eta: 2:10:10  lr: 0.000100  loss: 0.0405 (0.0643)  time: 3.8651  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1400/3449]  eta: 2:09:32  lr: 0.000100  loss: 0.0389 (0.0641)  time: 3.8270  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1410/3449]  eta: 2:08:54  lr: 0.000100  loss: 0.0315 (0.0639)  time: 3.7850  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1420/3449]  eta: 2:08:16  lr: 0.000100  loss: 0.0431 (0.0638)  time: 3.7997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1430/3449]  eta: 2:07:39  lr: 0.000100  loss: 0.0595 (0.0638)  time: 3.8098  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1440/3449]  eta: 2:07:01  lr: 0.000100  loss: 0.0577 (0.0638)  time: 3.8033  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [1450/3449]  eta: 2:06:23  lr: 0.000100  loss: 0.0529 (0.0638)  time: 3.7849  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1460/3449]  eta: 2:05:45  lr: 0.000100  loss: 0.0482 (0.0636)  time: 3.8014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1470/3449]  eta: 2:05:07  lr: 0.000100  loss: 0.0315 (0.0634)  time: 3.7999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1480/3449]  eta: 2:04:29  lr: 0.000100  loss: 0.0276 (0.0632)  time: 3.8012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1490/3449]  eta: 2:03:52  lr: 0.000100  loss: 0.0333 (0.0630)  time: 3.8412  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1500/3449]  eta: 2:03:15  lr: 0.000100  loss: 0.0351 (0.0628)  time: 3.8437  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1510/3449]  eta: 2:02:36  lr: 0.000100  loss: 0.0348 (0.0626)  time: 3.7812  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1520/3449]  eta: 2:01:57  lr: 0.000100  loss: 0.0332 (0.0624)  time: 3.7172  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1530/3449]  eta: 2:01:17  lr: 0.000100  loss: 0.0339 (0.0623)  time: 3.6795  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1540/3449]  eta: 2:00:40  lr: 0.000100  loss: 0.0456 (0.0622)  time: 3.7362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1550/3449]  eta: 2:00:02  lr: 0.000100  loss: 0.0308 (0.0620)  time: 3.8060  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1560/3449]  eta: 1:59:23  lr: 0.000100  loss: 0.0321 (0.0620)  time: 3.7678  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1570/3449]  eta: 1:58:45  lr: 0.000100  loss: 0.0482 (0.0619)  time: 3.7684  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1580/3449]  eta: 1:58:08  lr: 0.000100  loss: 0.0413 (0.0618)  time: 3.8226  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1590/3449]  eta: 1:57:30  lr: 0.000100  loss: 0.0334 (0.0616)  time: 3.7970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1600/3449]  eta: 1:56:52  lr: 0.000100  loss: 0.0319 (0.0614)  time: 3.7981  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1610/3449]  eta: 1:56:15  lr: 0.000100  loss: 0.0361 (0.0614)  time: 3.8525  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1620/3449]  eta: 1:55:37  lr: 0.000100  loss: 0.0555 (0.0614)  time: 3.8371  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1630/3449]  eta: 1:55:00  lr: 0.000100  loss: 0.0628 (0.0615)  time: 3.8246  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1640/3449]  eta: 1:54:22  lr: 0.000100  loss: 0.0498 (0.0614)  time: 3.8019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1650/3449]  eta: 1:53:44  lr: 0.000100  loss: 0.0455 (0.0613)  time: 3.8012  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1660/3449]  eta: 1:53:07  lr: 0.000100  loss: 0.0521 (0.0613)  time: 3.8237  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1670/3449]  eta: 1:52:29  lr: 0.000100  loss: 0.0537 (0.0613)  time: 3.8266  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1680/3449]  eta: 1:51:52  lr: 0.000100  loss: 0.0527 (0.0612)  time: 3.8549  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1690/3449]  eta: 1:51:15  lr: 0.000100  loss: 0.0527 (0.0612)  time: 3.8812  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1700/3449]  eta: 1:50:38  lr: 0.000100  loss: 0.0481 (0.0612)  time: 3.8747  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1710/3449]  eta: 1:50:00  lr: 0.000100  loss: 0.0754 (0.0615)  time: 3.8281  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1720/3449]  eta: 1:49:21  lr: 0.000100  loss: 0.0900 (0.0616)  time: 3.7695  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1730/3449]  eta: 1:48:43  lr: 0.000100  loss: 0.0533 (0.0614)  time: 3.7408  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1740/3449]  eta: 1:48:04  lr: 0.000100  loss: 0.0466 (0.0614)  time: 3.7420  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1750/3449]  eta: 1:47:26  lr: 0.000100  loss: 0.0569 (0.0614)  time: 3.7596  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1760/3449]  eta: 1:46:47  lr: 0.000100  loss: 0.0557 (0.0613)  time: 3.7394  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1770/3449]  eta: 1:46:09  lr: 0.000100  loss: 0.0493 (0.0613)  time: 3.7301  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1780/3449]  eta: 1:45:31  lr: 0.000100  loss: 0.0513 (0.0613)  time: 3.7864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1790/3449]  eta: 1:44:54  lr: 0.000100  loss: 0.0457 (0.0612)  time: 3.8143  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1800/3449]  eta: 1:44:15  lr: 0.000100  loss: 0.0528 (0.0612)  time: 3.7783  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1810/3449]  eta: 1:43:37  lr: 0.000100  loss: 0.0622 (0.0613)  time: 3.7785  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1820/3449]  eta: 1:43:00  lr: 0.000100  loss: 0.0622 (0.0613)  time: 3.8173  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1830/3449]  eta: 1:42:22  lr: 0.000100  loss: 0.0442 (0.0611)  time: 3.8181  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1840/3449]  eta: 1:41:44  lr: 0.000100  loss: 0.0462 (0.0611)  time: 3.8084  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1850/3449]  eta: 1:41:07  lr: 0.000100  loss: 0.0389 (0.0610)  time: 3.8234  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1860/3449]  eta: 1:40:29  lr: 0.000100  loss: 0.0370 (0.0610)  time: 3.8533  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1870/3449]  eta: 1:39:51  lr: 0.000100  loss: 0.0533 (0.0610)  time: 3.7995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1880/3449]  eta: 1:39:13  lr: 0.000100  loss: 0.0757 (0.0611)  time: 3.7419  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1890/3449]  eta: 1:38:34  lr: 0.000100  loss: 0.0565 (0.0610)  time: 3.7456  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1900/3449]  eta: 1:37:57  lr: 0.000100  loss: 0.0530 (0.0611)  time: 3.7919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1910/3449]  eta: 1:37:19  lr: 0.000100  loss: 0.0559 (0.0610)  time: 3.8577  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1920/3449]  eta: 1:36:41  lr: 0.000100  loss: 0.0424 (0.0609)  time: 3.8343  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1930/3449]  eta: 1:36:04  lr: 0.000100  loss: 0.0354 (0.0608)  time: 3.8343  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1940/3449]  eta: 1:35:26  lr: 0.000100  loss: 0.0345 (0.0606)  time: 3.8034  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1950/3449]  eta: 1:34:48  lr: 0.000100  loss: 0.0301 (0.0605)  time: 3.7658  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1960/3449]  eta: 1:34:10  lr: 0.000100  loss: 0.0301 (0.0604)  time: 3.8044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1970/3449]  eta: 1:33:33  lr: 0.000100  loss: 0.0485 (0.0605)  time: 3.8447  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [1980/3449]  eta: 1:32:54  lr: 0.000100  loss: 0.0876 (0.0606)  time: 3.8117  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [1990/3449]  eta: 1:32:15  lr: 0.000100  loss: 0.0616 (0.0606)  time: 3.6955  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2000/3449]  eta: 1:31:37  lr: 0.000100  loss: 0.0553 (0.0606)  time: 3.7165  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2010/3449]  eta: 1:30:59  lr: 0.000100  loss: 0.0467 (0.0605)  time: 3.7927  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2020/3449]  eta: 1:30:21  lr: 0.000100  loss: 0.0473 (0.0605)  time: 3.7590  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2030/3449]  eta: 1:29:43  lr: 0.000100  loss: 0.0721 (0.0606)  time: 3.7576  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2040/3449]  eta: 1:29:04  lr: 0.000100  loss: 0.0781 (0.0607)  time: 3.7362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2050/3449]  eta: 1:28:27  lr: 0.000100  loss: 0.0621 (0.0607)  time: 3.7725  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2060/3449]  eta: 1:27:49  lr: 0.000100  loss: 0.0403 (0.0605)  time: 3.8629  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2070/3449]  eta: 1:27:11  lr: 0.000100  loss: 0.0360 (0.0604)  time: 3.8046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2080/3449]  eta: 1:26:33  lr: 0.000100  loss: 0.0354 (0.0603)  time: 3.7552  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2090/3449]  eta: 1:25:55  lr: 0.000100  loss: 0.0397 (0.0603)  time: 3.8060  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2100/3449]  eta: 1:25:17  lr: 0.000100  loss: 0.0832 (0.0605)  time: 3.8330  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [2110/3449]  eta: 1:24:40  lr: 0.000100  loss: 0.0725 (0.0605)  time: 3.8272  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2120/3449]  eta: 1:24:01  lr: 0.000100  loss: 0.0520 (0.0605)  time: 3.7856  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2130/3449]  eta: 1:23:23  lr: 0.000100  loss: 0.0580 (0.0605)  time: 3.7328  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2140/3449]  eta: 1:22:45  lr: 0.000100  loss: 0.0417 (0.0604)  time: 3.7647  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2150/3449]  eta: 1:22:08  lr: 0.000100  loss: 0.0253 (0.0602)  time: 3.8205  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2160/3449]  eta: 1:21:30  lr: 0.000100  loss: 0.0329 (0.0601)  time: 3.8432  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2170/3449]  eta: 1:20:53  lr: 0.000100  loss: 0.0437 (0.0602)  time: 3.8801  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2180/3449]  eta: 1:20:15  lr: 0.000100  loss: 0.0521 (0.0601)  time: 3.8572  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2190/3449]  eta: 1:19:37  lr: 0.000100  loss: 0.0525 (0.0601)  time: 3.8196  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2200/3449]  eta: 1:18:59  lr: 0.000100  loss: 0.0498 (0.0600)  time: 3.7973  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2210/3449]  eta: 1:18:21  lr: 0.000100  loss: 0.0336 (0.0599)  time: 3.7496  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2220/3449]  eta: 1:17:43  lr: 0.000100  loss: 0.0280 (0.0598)  time: 3.7697  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2230/3449]  eta: 1:17:04  lr: 0.000100  loss: 0.0283 (0.0597)  time: 3.7362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2240/3449]  eta: 1:16:26  lr: 0.000100  loss: 0.0487 (0.0597)  time: 3.7136  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2250/3449]  eta: 1:15:48  lr: 0.000100  loss: 0.0808 (0.0598)  time: 3.7446  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2260/3449]  eta: 1:15:10  lr: 0.000100  loss: 0.0583 (0.0598)  time: 3.7598  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2270/3449]  eta: 1:14:32  lr: 0.000100  loss: 0.0488 (0.0598)  time: 3.8256  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2280/3449]  eta: 1:13:54  lr: 0.000100  loss: 0.0734 (0.0601)  time: 3.8326  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2290/3449]  eta: 1:13:16  lr: 0.000100  loss: 0.0715 (0.0601)  time: 3.7855  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2300/3449]  eta: 1:12:38  lr: 0.000100  loss: 0.0614 (0.0601)  time: 3.7839  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2310/3449]  eta: 1:12:01  lr: 0.000100  loss: 0.0346 (0.0599)  time: 3.7972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2320/3449]  eta: 1:11:22  lr: 0.000100  loss: 0.0289 (0.0598)  time: 3.7702  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2330/3449]  eta: 1:10:45  lr: 0.000100  loss: 0.0266 (0.0597)  time: 3.8459  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2340/3449]  eta: 1:10:07  lr: 0.000100  loss: 0.0290 (0.0596)  time: 3.8689  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2350/3449]  eta: 1:09:29  lr: 0.000100  loss: 0.0293 (0.0595)  time: 3.7433  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2360/3449]  eta: 1:08:51  lr: 0.000100  loss: 0.0323 (0.0593)  time: 3.8213  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2370/3449]  eta: 1:08:13  lr: 0.000100  loss: 0.0235 (0.0593)  time: 3.8560  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2380/3449]  eta: 1:07:36  lr: 0.000100  loss: 0.0517 (0.0593)  time: 3.8194  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2390/3449]  eta: 1:06:58  lr: 0.000100  loss: 0.0574 (0.0593)  time: 3.8472  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2400/3449]  eta: 1:06:19  lr: 0.000100  loss: 0.0646 (0.0593)  time: 3.7369  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2410/3449]  eta: 1:05:41  lr: 0.000100  loss: 0.0714 (0.0594)  time: 3.6853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2420/3449]  eta: 1:05:03  lr: 0.000100  loss: 0.0610 (0.0594)  time: 3.7846  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2430/3449]  eta: 1:04:26  lr: 0.000100  loss: 0.0608 (0.0594)  time: 3.8564  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2440/3449]  eta: 1:03:48  lr: 0.000100  loss: 0.0522 (0.0594)  time: 3.8009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2450/3449]  eta: 1:03:10  lr: 0.000100  loss: 0.0567 (0.0594)  time: 3.7956  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2460/3449]  eta: 1:02:32  lr: 0.000100  loss: 0.0582 (0.0594)  time: 3.8347  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2470/3449]  eta: 1:01:54  lr: 0.000100  loss: 0.0561 (0.0594)  time: 3.8054  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2480/3449]  eta: 1:01:16  lr: 0.000100  loss: 0.0599 (0.0594)  time: 3.8170  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2490/3449]  eta: 1:00:39  lr: 0.000100  loss: 0.0569 (0.0594)  time: 3.9205  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2500/3449]  eta: 1:00:01  lr: 0.000100  loss: 0.0610 (0.0594)  time: 3.8697  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2510/3449]  eta: 0:59:23  lr: 0.000100  loss: 0.0500 (0.0594)  time: 3.7484  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2520/3449]  eta: 0:58:45  lr: 0.000100  loss: 0.0384 (0.0593)  time: 3.7808  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2530/3449]  eta: 0:58:07  lr: 0.000100  loss: 0.0433 (0.0593)  time: 3.7964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2540/3449]  eta: 0:57:29  lr: 0.000100  loss: 0.0632 (0.0593)  time: 3.7625  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2550/3449]  eta: 0:56:51  lr: 0.000100  loss: 0.0862 (0.0595)  time: 3.7570  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2560/3449]  eta: 0:56:13  lr: 0.000100  loss: 0.0892 (0.0596)  time: 3.8027  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2570/3449]  eta: 0:55:35  lr: 0.000100  loss: 0.0747 (0.0597)  time: 3.8382  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2580/3449]  eta: 0:54:57  lr: 0.000100  loss: 0.0747 (0.0597)  time: 3.8169  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2590/3449]  eta: 0:54:19  lr: 0.000100  loss: 0.0908 (0.0599)  time: 3.7826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2600/3449]  eta: 0:53:41  lr: 0.000100  loss: 0.1045 (0.0601)  time: 3.7960  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2610/3449]  eta: 0:53:04  lr: 0.000100  loss: 0.0793 (0.0601)  time: 3.8363  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2620/3449]  eta: 0:52:26  lr: 0.000100  loss: 0.0708 (0.0602)  time: 3.8246  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2630/3449]  eta: 0:51:48  lr: 0.000100  loss: 0.0663 (0.0602)  time: 3.8048  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2640/3449]  eta: 0:51:10  lr: 0.000100  loss: 0.0613 (0.0602)  time: 3.8643  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2650/3449]  eta: 0:50:32  lr: 0.000100  loss: 0.0877 (0.0604)  time: 3.7997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2660/3449]  eta: 0:49:54  lr: 0.000100  loss: 0.0951 (0.0605)  time: 3.7251  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2670/3449]  eta: 0:49:16  lr: 0.000100  loss: 0.0811 (0.0606)  time: 3.7741  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2680/3449]  eta: 0:48:38  lr: 0.000100  loss: 0.0917 (0.0607)  time: 3.8057  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2690/3449]  eta: 0:48:00  lr: 0.000100  loss: 0.0857 (0.0608)  time: 3.7693  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2700/3449]  eta: 0:47:22  lr: 0.000100  loss: 0.0914 (0.0610)  time: 3.8319  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2710/3449]  eta: 0:46:45  lr: 0.000100  loss: 0.0896 (0.0611)  time: 3.8937  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2720/3449]  eta: 0:46:07  lr: 0.000100  loss: 0.0734 (0.0611)  time: 3.8332  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2730/3449]  eta: 0:45:29  lr: 0.000100  loss: 0.0819 (0.0613)  time: 3.8521  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2740/3449]  eta: 0:44:51  lr: 0.000100  loss: 0.1007 (0.0614)  time: 3.8523  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2750/3449]  eta: 0:44:13  lr: 0.000100  loss: 0.0784 (0.0615)  time: 3.8119  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2760/3449]  eta: 0:43:35  lr: 0.000100  loss: 0.0332 (0.0613)  time: 3.7437  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [2770/3449]  eta: 0:42:57  lr: 0.000100  loss: 0.0330 (0.0613)  time: 3.7706  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2780/3449]  eta: 0:42:19  lr: 0.000100  loss: 0.0723 (0.0614)  time: 3.8804  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2790/3449]  eta: 0:41:41  lr: 0.000100  loss: 0.0756 (0.0615)  time: 3.8385  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2800/3449]  eta: 0:41:03  lr: 0.000100  loss: 0.0616 (0.0615)  time: 3.7699  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2810/3449]  eta: 0:40:25  lr: 0.000100  loss: 0.0616 (0.0615)  time: 3.7822  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2820/3449]  eta: 0:39:47  lr: 0.000100  loss: 0.0525 (0.0614)  time: 3.7952  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2830/3449]  eta: 0:39:09  lr: 0.000100  loss: 0.0309 (0.0613)  time: 3.7475  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2840/3449]  eta: 0:38:31  lr: 0.000100  loss: 0.0279 (0.0612)  time: 3.7732  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2850/3449]  eta: 0:37:53  lr: 0.000100  loss: 0.0296 (0.0611)  time: 3.7898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2860/3449]  eta: 0:37:15  lr: 0.000100  loss: 0.0405 (0.0610)  time: 3.7258  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2870/3449]  eta: 0:36:37  lr: 0.000100  loss: 0.0447 (0.0610)  time: 3.6773  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2880/3449]  eta: 0:35:59  lr: 0.000100  loss: 0.0678 (0.0611)  time: 3.7304  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2890/3449]  eta: 0:35:21  lr: 0.000100  loss: 0.0685 (0.0611)  time: 3.7648  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2900/3449]  eta: 0:34:43  lr: 0.000100  loss: 0.0452 (0.0610)  time: 3.7432  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2910/3449]  eta: 0:34:05  lr: 0.000100  loss: 0.0315 (0.0609)  time: 3.7594  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2920/3449]  eta: 0:33:27  lr: 0.000100  loss: 0.0299 (0.0608)  time: 3.8019  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [2930/3449]  eta: 0:32:49  lr: 0.000100  loss: 0.0304 (0.0607)  time: 3.8212  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2940/3449]  eta: 0:32:11  lr: 0.000100  loss: 0.0332 (0.0606)  time: 3.7803  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2950/3449]  eta: 0:31:33  lr: 0.000100  loss: 0.0337 (0.0606)  time: 3.8515  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2960/3449]  eta: 0:30:55  lr: 0.000100  loss: 0.0466 (0.0606)  time: 3.8598  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2970/3449]  eta: 0:30:17  lr: 0.000100  loss: 0.0431 (0.0605)  time: 3.8069  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2980/3449]  eta: 0:29:40  lr: 0.000100  loss: 0.0431 (0.0605)  time: 3.8378  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [2990/3449]  eta: 0:29:02  lr: 0.000100  loss: 0.0621 (0.0605)  time: 3.8350  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3000/3449]  eta: 0:28:24  lr: 0.000100  loss: 0.0621 (0.0605)  time: 3.8385  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3010/3449]  eta: 0:27:46  lr: 0.000100  loss: 0.0581 (0.0605)  time: 3.8265  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3020/3449]  eta: 0:27:08  lr: 0.000100  loss: 0.0438 (0.0604)  time: 3.7873  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3030/3449]  eta: 0:26:30  lr: 0.000100  loss: 0.0383 (0.0603)  time: 3.7534  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [3040/3449]  eta: 0:25:52  lr: 0.000100  loss: 0.0294 (0.0602)  time: 3.7232  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [3050/3449]  eta: 0:25:14  lr: 0.000100  loss: 0.0320 (0.0602)  time: 3.8340  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3060/3449]  eta: 0:24:36  lr: 0.000100  loss: 0.0328 (0.0601)  time: 3.8928  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3070/3449]  eta: 0:23:58  lr: 0.000100  loss: 0.0532 (0.0601)  time: 3.8371  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3080/3449]  eta: 0:23:20  lr: 0.000100  loss: 0.0390 (0.0600)  time: 3.8216  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3090/3449]  eta: 0:22:42  lr: 0.000100  loss: 0.0410 (0.0600)  time: 3.8426  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3100/3449]  eta: 0:22:05  lr: 0.000100  loss: 0.0747 (0.0602)  time: 3.8924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3110/3449]  eta: 0:21:27  lr: 0.000100  loss: 0.1076 (0.0603)  time: 3.8466  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3120/3449]  eta: 0:20:48  lr: 0.000100  loss: 0.1012 (0.0604)  time: 3.7441  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3130/3449]  eta: 0:20:10  lr: 0.000100  loss: 0.0642 (0.0604)  time: 3.7176  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3140/3449]  eta: 0:19:33  lr: 0.000100  loss: 0.0425 (0.0603)  time: 3.7742  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [3150/3449]  eta: 0:18:55  lr: 0.000100  loss: 0.0348 (0.0602)  time: 3.8032  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [3160/3449]  eta: 0:18:17  lr: 0.000100  loss: 0.0292 (0.0601)  time: 3.7857  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3170/3449]  eta: 0:17:39  lr: 0.000100  loss: 0.0470 (0.0601)  time: 3.7564  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3180/3449]  eta: 0:17:01  lr: 0.000100  loss: 0.0642 (0.0601)  time: 3.7972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3190/3449]  eta: 0:16:23  lr: 0.000100  loss: 0.0449 (0.0601)  time: 3.8747  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3200/3449]  eta: 0:15:45  lr: 0.000100  loss: 0.0370 (0.0600)  time: 3.8159  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [3210/3449]  eta: 0:15:07  lr: 0.000100  loss: 0.0346 (0.0600)  time: 3.8495  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3220/3449]  eta: 0:14:29  lr: 0.000100  loss: 0.0522 (0.0600)  time: 3.8587  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3230/3449]  eta: 0:13:51  lr: 0.000100  loss: 0.0401 (0.0599)  time: 3.6983  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [3240/3449]  eta: 0:13:13  lr: 0.000100  loss: 0.0401 (0.0599)  time: 3.6918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:16]  [3250/3449]  eta: 0:12:35  lr: 0.000100  loss: 0.0531 (0.0598)  time: 3.8135  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3260/3449]  eta: 0:11:57  lr: 0.000100  loss: 0.0511 (0.0598)  time: 3.8230  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3270/3449]  eta: 0:11:19  lr: 0.000100  loss: 0.0545 (0.0598)  time: 3.7993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3280/3449]  eta: 0:10:41  lr: 0.000100  loss: 0.0649 (0.0599)  time: 3.8213  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3290/3449]  eta: 0:10:03  lr: 0.000100  loss: 0.0794 (0.0599)  time: 3.8346  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3300/3449]  eta: 0:09:25  lr: 0.000100  loss: 0.0592 (0.0599)  time: 3.8172  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3310/3449]  eta: 0:08:47  lr: 0.000100  loss: 0.0480 (0.0599)  time: 3.7688  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3320/3449]  eta: 0:08:09  lr: 0.000100  loss: 0.0662 (0.0599)  time: 3.8009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3330/3449]  eta: 0:07:31  lr: 0.000100  loss: 0.0720 (0.0600)  time: 3.8202  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3340/3449]  eta: 0:06:53  lr: 0.000100  loss: 0.0760 (0.0600)  time: 3.7607  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3350/3449]  eta: 0:06:15  lr: 0.000100  loss: 0.0867 (0.0601)  time: 3.7615  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3360/3449]  eta: 0:05:37  lr: 0.000100  loss: 0.1083 (0.0602)  time: 3.7887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3370/3449]  eta: 0:04:59  lr: 0.000100  loss: 0.0687 (0.0602)  time: 3.7654  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3380/3449]  eta: 0:04:21  lr: 0.000100  loss: 0.0589 (0.0602)  time: 3.7640  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3390/3449]  eta: 0:03:43  lr: 0.000100  loss: 0.0519 (0.0602)  time: 3.6977  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3400/3449]  eta: 0:03:05  lr: 0.000100  loss: 0.0369 (0.0601)  time: 3.7770  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3410/3449]  eta: 0:02:28  lr: 0.000100  loss: 0.0303 (0.0600)  time: 3.9346  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3420/3449]  eta: 0:01:50  lr: 0.000100  loss: 0.0255 (0.0599)  time: 3.8795  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [3430/3449]  eta: 0:01:12  lr: 0.000100  loss: 0.0234 (0.0598)  time: 3.8461  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3440/3449]  eta: 0:00:34  lr: 0.000100  loss: 0.0303 (0.0598)  time: 3.8527  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0457 (0.0598)  time: 3.8873  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:16] Total time: 3:38:16 (3.7972 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0457 (0.0598)\n",
      "Valid: [epoch:16]  [ 0/14]  eta: 0:04:21  loss: 0.0337 (0.0337)  time: 18.6702  data: 0.4718  max mem: 34968\n",
      "Valid: [epoch:16]  [13/14]  eta: 0:00:18  loss: 0.0337 (0.0349)  time: 18.2394  data: 0.0339  max mem: 34968\n",
      "Valid: [epoch:16] Total time: 0:04:15 (18.2501 s / it)\n",
      "Averaged stats: loss: 0.0337 (0.0349)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_16_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.035%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:17]  [   0/3449]  eta: 4:49:53  lr: 0.000100  loss: 0.0509 (0.0509)  time: 5.0431  data: 1.4305  max mem: 34968\n",
      "Train: [epoch:17]  [  10/3449]  eta: 3:41:45  lr: 0.000100  loss: 0.0611 (0.0669)  time: 3.8691  data: 0.1303  max mem: 34968\n",
      "Train: [epoch:17]  [  20/3449]  eta: 3:38:14  lr: 0.000100  loss: 0.0500 (0.0569)  time: 3.7576  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [  30/3449]  eta: 3:37:12  lr: 0.000100  loss: 0.0500 (0.0585)  time: 3.7804  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [  40/3449]  eta: 3:37:07  lr: 0.000100  loss: 0.0564 (0.0556)  time: 3.8244  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [  50/3449]  eta: 3:35:21  lr: 0.000100  loss: 0.0410 (0.0520)  time: 3.7858  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [  60/3449]  eta: 3:35:12  lr: 0.000100  loss: 0.0348 (0.0502)  time: 3.7865  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [  70/3449]  eta: 3:34:26  lr: 0.000100  loss: 0.0581 (0.0528)  time: 3.8230  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [  80/3449]  eta: 3:33:44  lr: 0.000100  loss: 0.0688 (0.0546)  time: 3.7958  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [  90/3449]  eta: 3:33:19  lr: 0.000100  loss: 0.0720 (0.0624)  time: 3.8200  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 100/3449]  eta: 3:33:34  lr: 0.000100  loss: 0.1184 (0.0669)  time: 3.9068  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 110/3449]  eta: 3:32:35  lr: 0.000100  loss: 0.1255 (0.0728)  time: 3.8652  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 120/3449]  eta: 3:31:41  lr: 0.000100  loss: 0.1320 (0.0770)  time: 3.7593  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 130/3449]  eta: 3:30:46  lr: 0.000100  loss: 0.1307 (0.0803)  time: 3.7555  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 140/3449]  eta: 3:29:45  lr: 0.000100  loss: 0.0987 (0.0820)  time: 3.7306  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 150/3449]  eta: 3:28:46  lr: 0.000100  loss: 0.0951 (0.0834)  time: 3.7094  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 160/3449]  eta: 3:27:37  lr: 0.000100  loss: 0.0822 (0.0829)  time: 3.6784  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 170/3449]  eta: 3:26:49  lr: 0.000100  loss: 0.0635 (0.0819)  time: 3.6902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 180/3449]  eta: 3:26:29  lr: 0.000100  loss: 0.0824 (0.0848)  time: 3.8074  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 190/3449]  eta: 3:25:57  lr: 0.000100  loss: 0.0997 (0.0846)  time: 3.8540  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 200/3449]  eta: 3:25:29  lr: 0.000100  loss: 0.0567 (0.0828)  time: 3.8404  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 210/3449]  eta: 3:24:53  lr: 0.000100  loss: 0.0483 (0.0810)  time: 3.8289  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 220/3449]  eta: 3:24:10  lr: 0.000100  loss: 0.0486 (0.0801)  time: 3.7840  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 230/3449]  eta: 3:23:42  lr: 0.000100  loss: 0.0452 (0.0785)  time: 3.8155  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 240/3449]  eta: 3:23:07  lr: 0.000100  loss: 0.0483 (0.0778)  time: 3.8432  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 250/3449]  eta: 3:22:41  lr: 0.000100  loss: 0.0566 (0.0771)  time: 3.8551  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 260/3449]  eta: 3:21:50  lr: 0.000100  loss: 0.0652 (0.0770)  time: 3.7936  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 270/3449]  eta: 3:21:23  lr: 0.000100  loss: 0.0655 (0.0764)  time: 3.7916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 280/3449]  eta: 3:20:48  lr: 0.000100  loss: 0.0614 (0.0770)  time: 3.8571  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 290/3449]  eta: 3:20:12  lr: 0.000100  loss: 0.0817 (0.0773)  time: 3.8259  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 300/3449]  eta: 3:19:32  lr: 0.000100  loss: 0.0799 (0.0772)  time: 3.8027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 310/3449]  eta: 3:18:56  lr: 0.000100  loss: 0.0695 (0.0766)  time: 3.8007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 320/3449]  eta: 3:18:20  lr: 0.000100  loss: 0.0411 (0.0755)  time: 3.8219  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 330/3449]  eta: 3:17:30  lr: 0.000100  loss: 0.0344 (0.0744)  time: 3.7532  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 340/3449]  eta: 3:16:56  lr: 0.000100  loss: 0.0340 (0.0732)  time: 3.7622  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 350/3449]  eta: 3:16:17  lr: 0.000100  loss: 0.0312 (0.0722)  time: 3.8153  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 360/3449]  eta: 3:15:41  lr: 0.000100  loss: 0.0321 (0.0711)  time: 3.8038  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 370/3449]  eta: 3:15:00  lr: 0.000100  loss: 0.0349 (0.0702)  time: 3.7954  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 380/3449]  eta: 3:14:16  lr: 0.000100  loss: 0.0371 (0.0693)  time: 3.7489  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 390/3449]  eta: 3:13:39  lr: 0.000100  loss: 0.0369 (0.0685)  time: 3.7683  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 400/3449]  eta: 3:13:01  lr: 0.000100  loss: 0.0369 (0.0678)  time: 3.8045  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 410/3449]  eta: 3:12:26  lr: 0.000100  loss: 0.0393 (0.0670)  time: 3.8145  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 420/3449]  eta: 3:11:54  lr: 0.000100  loss: 0.0391 (0.0666)  time: 3.8620  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 430/3449]  eta: 3:11:06  lr: 0.000100  loss: 0.0544 (0.0666)  time: 3.7716  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 440/3449]  eta: 3:10:31  lr: 0.000100  loss: 0.0610 (0.0664)  time: 3.7468  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 450/3449]  eta: 3:10:05  lr: 0.000100  loss: 0.0614 (0.0664)  time: 3.9123  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 460/3449]  eta: 3:09:26  lr: 0.000100  loss: 0.0614 (0.0662)  time: 3.8804  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 470/3449]  eta: 3:08:46  lr: 0.000100  loss: 0.0619 (0.0667)  time: 3.7834  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 480/3449]  eta: 3:08:11  lr: 0.000100  loss: 0.0592 (0.0666)  time: 3.8145  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 490/3449]  eta: 3:07:31  lr: 0.000100  loss: 0.0464 (0.0661)  time: 3.8072  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 500/3449]  eta: 3:06:49  lr: 0.000100  loss: 0.0490 (0.0659)  time: 3.7537  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 510/3449]  eta: 3:06:05  lr: 0.000100  loss: 0.0390 (0.0653)  time: 3.7120  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 520/3449]  eta: 3:05:16  lr: 0.000100  loss: 0.0362 (0.0648)  time: 3.6514  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 530/3449]  eta: 3:04:34  lr: 0.000100  loss: 0.0343 (0.0642)  time: 3.6666  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 540/3449]  eta: 3:03:48  lr: 0.000100  loss: 0.0309 (0.0636)  time: 3.6849  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 550/3449]  eta: 3:03:08  lr: 0.000100  loss: 0.0335 (0.0632)  time: 3.6930  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 560/3449]  eta: 3:02:29  lr: 0.000100  loss: 0.0470 (0.0632)  time: 3.7547  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 570/3449]  eta: 3:01:49  lr: 0.000100  loss: 0.0636 (0.0632)  time: 3.7598  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [ 580/3449]  eta: 3:01:09  lr: 0.000100  loss: 0.0748 (0.0641)  time: 3.7464  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 590/3449]  eta: 3:00:29  lr: 0.000100  loss: 0.0685 (0.0637)  time: 3.7488  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 600/3449]  eta: 2:59:54  lr: 0.000100  loss: 0.0365 (0.0633)  time: 3.7974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 610/3449]  eta: 2:59:22  lr: 0.000100  loss: 0.0285 (0.0628)  time: 3.8806  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 620/3449]  eta: 2:58:45  lr: 0.000100  loss: 0.0444 (0.0628)  time: 3.8684  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 630/3449]  eta: 2:58:13  lr: 0.000100  loss: 0.0486 (0.0626)  time: 3.8655  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 640/3449]  eta: 2:57:39  lr: 0.000100  loss: 0.0456 (0.0625)  time: 3.9031  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 650/3449]  eta: 2:57:05  lr: 0.000100  loss: 0.0448 (0.0623)  time: 3.8861  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 660/3449]  eta: 2:56:24  lr: 0.000100  loss: 0.0605 (0.0623)  time: 3.8016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 670/3449]  eta: 2:55:47  lr: 0.000100  loss: 0.0627 (0.0624)  time: 3.7683  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 680/3449]  eta: 2:55:14  lr: 0.000100  loss: 0.0523 (0.0621)  time: 3.8694  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 690/3449]  eta: 2:54:28  lr: 0.000100  loss: 0.0343 (0.0616)  time: 3.7618  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 700/3449]  eta: 2:53:50  lr: 0.000100  loss: 0.0343 (0.0613)  time: 3.6917  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 710/3449]  eta: 2:53:12  lr: 0.000100  loss: 0.0347 (0.0611)  time: 3.7916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 720/3449]  eta: 2:52:35  lr: 0.000100  loss: 0.0557 (0.0611)  time: 3.8018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 730/3449]  eta: 2:51:57  lr: 0.000100  loss: 0.0621 (0.0612)  time: 3.8027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 740/3449]  eta: 2:51:19  lr: 0.000100  loss: 0.0621 (0.0612)  time: 3.8047  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 750/3449]  eta: 2:50:41  lr: 0.000100  loss: 0.0621 (0.0612)  time: 3.7924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 760/3449]  eta: 2:50:02  lr: 0.000100  loss: 0.0595 (0.0612)  time: 3.7751  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 770/3449]  eta: 2:49:27  lr: 0.000100  loss: 0.0582 (0.0612)  time: 3.8253  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 780/3449]  eta: 2:48:47  lr: 0.000100  loss: 0.0704 (0.0617)  time: 3.8056  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 790/3449]  eta: 2:48:05  lr: 0.000100  loss: 0.0917 (0.0621)  time: 3.7081  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 800/3449]  eta: 2:47:32  lr: 0.000100  loss: 0.0649 (0.0621)  time: 3.8044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 810/3449]  eta: 2:46:54  lr: 0.000100  loss: 0.0577 (0.0621)  time: 3.8611  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 820/3449]  eta: 2:46:18  lr: 0.000100  loss: 0.0642 (0.0626)  time: 3.8313  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 830/3449]  eta: 2:45:43  lr: 0.000100  loss: 0.1026 (0.0631)  time: 3.8844  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 840/3449]  eta: 2:45:02  lr: 0.000100  loss: 0.0747 (0.0631)  time: 3.7881  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 850/3449]  eta: 2:44:25  lr: 0.000100  loss: 0.0563 (0.0631)  time: 3.7493  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 860/3449]  eta: 2:43:46  lr: 0.000100  loss: 0.0573 (0.0630)  time: 3.7960  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 870/3449]  eta: 2:43:06  lr: 0.000100  loss: 0.0569 (0.0630)  time: 3.7499  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 880/3449]  eta: 2:42:29  lr: 0.000100  loss: 0.0593 (0.0630)  time: 3.7741  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 890/3449]  eta: 2:41:52  lr: 0.000100  loss: 0.0671 (0.0631)  time: 3.8355  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 900/3449]  eta: 2:41:12  lr: 0.000100  loss: 0.0765 (0.0632)  time: 3.7868  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 910/3449]  eta: 2:40:33  lr: 0.000100  loss: 0.0662 (0.0631)  time: 3.7412  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 920/3449]  eta: 2:39:57  lr: 0.000100  loss: 0.0522 (0.0630)  time: 3.8029  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 930/3449]  eta: 2:39:17  lr: 0.000100  loss: 0.0405 (0.0627)  time: 3.7808  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 940/3449]  eta: 2:38:42  lr: 0.000100  loss: 0.0431 (0.0627)  time: 3.8041  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 950/3449]  eta: 2:38:07  lr: 0.000100  loss: 0.0578 (0.0627)  time: 3.9137  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 960/3449]  eta: 2:37:30  lr: 0.000100  loss: 0.0644 (0.0628)  time: 3.8765  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [ 970/3449]  eta: 2:36:53  lr: 0.000100  loss: 0.0704 (0.0629)  time: 3.8273  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 980/3449]  eta: 2:36:15  lr: 0.000100  loss: 0.0693 (0.0630)  time: 3.8214  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [ 990/3449]  eta: 2:35:36  lr: 0.000100  loss: 0.0652 (0.0630)  time: 3.7886  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1000/3449]  eta: 2:35:01  lr: 0.000100  loss: 0.0613 (0.0630)  time: 3.8278  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1010/3449]  eta: 2:34:21  lr: 0.000100  loss: 0.0611 (0.0629)  time: 3.8097  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1020/3449]  eta: 2:33:44  lr: 0.000100  loss: 0.0558 (0.0629)  time: 3.7794  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1030/3449]  eta: 2:33:07  lr: 0.000100  loss: 0.0540 (0.0628)  time: 3.8248  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1040/3449]  eta: 2:32:31  lr: 0.000100  loss: 0.0566 (0.0627)  time: 3.8648  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1050/3449]  eta: 2:31:51  lr: 0.000100  loss: 0.0520 (0.0627)  time: 3.8091  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1060/3449]  eta: 2:31:09  lr: 0.000100  loss: 0.0488 (0.0626)  time: 3.6741  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1070/3449]  eta: 2:30:33  lr: 0.000100  loss: 0.0758 (0.0630)  time: 3.7567  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1080/3449]  eta: 2:29:56  lr: 0.000100  loss: 0.1040 (0.0633)  time: 3.8604  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1090/3449]  eta: 2:29:16  lr: 0.000100  loss: 0.1124 (0.0638)  time: 3.7596  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1100/3449]  eta: 2:28:40  lr: 0.000100  loss: 0.0932 (0.0640)  time: 3.7898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1110/3449]  eta: 2:27:59  lr: 0.000100  loss: 0.0803 (0.0641)  time: 3.7763  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1120/3449]  eta: 2:27:21  lr: 0.000100  loss: 0.0492 (0.0639)  time: 3.7174  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1130/3449]  eta: 2:26:41  lr: 0.000100  loss: 0.0409 (0.0637)  time: 3.7533  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1140/3449]  eta: 2:26:00  lr: 0.000100  loss: 0.0404 (0.0635)  time: 3.6807  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1150/3449]  eta: 2:25:23  lr: 0.000100  loss: 0.0380 (0.0633)  time: 3.7419  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1160/3449]  eta: 2:24:43  lr: 0.000100  loss: 0.0347 (0.0630)  time: 3.7560  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1170/3449]  eta: 2:24:06  lr: 0.000100  loss: 0.0344 (0.0629)  time: 3.7585  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1180/3449]  eta: 2:23:29  lr: 0.000100  loss: 0.0476 (0.0628)  time: 3.8593  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1190/3449]  eta: 2:22:51  lr: 0.000100  loss: 0.0473 (0.0626)  time: 3.8134  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1200/3449]  eta: 2:22:10  lr: 0.000100  loss: 0.0520 (0.0626)  time: 3.7122  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1210/3449]  eta: 2:21:31  lr: 0.000100  loss: 0.0527 (0.0625)  time: 3.6796  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1220/3449]  eta: 2:20:57  lr: 0.000100  loss: 0.0497 (0.0626)  time: 3.8516  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1230/3449]  eta: 2:20:19  lr: 0.000100  loss: 0.0615 (0.0626)  time: 3.9135  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [1240/3449]  eta: 2:19:41  lr: 0.000100  loss: 0.0615 (0.0626)  time: 3.7944  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1250/3449]  eta: 2:19:04  lr: 0.000100  loss: 0.0373 (0.0624)  time: 3.8147  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1260/3449]  eta: 2:18:25  lr: 0.000100  loss: 0.0320 (0.0625)  time: 3.7987  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1270/3449]  eta: 2:17:47  lr: 0.000100  loss: 0.0877 (0.0629)  time: 3.7711  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1280/3449]  eta: 2:17:10  lr: 0.000100  loss: 0.0587 (0.0628)  time: 3.8216  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1290/3449]  eta: 2:16:33  lr: 0.000100  loss: 0.0445 (0.0628)  time: 3.8462  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1300/3449]  eta: 2:15:57  lr: 0.000100  loss: 0.0663 (0.0628)  time: 3.8922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1310/3449]  eta: 2:15:19  lr: 0.000100  loss: 0.0562 (0.0627)  time: 3.8620  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1320/3449]  eta: 2:14:42  lr: 0.000100  loss: 0.0518 (0.0627)  time: 3.8108  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1330/3449]  eta: 2:14:04  lr: 0.000100  loss: 0.0437 (0.0625)  time: 3.8248  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1340/3449]  eta: 2:13:27  lr: 0.000100  loss: 0.0429 (0.0625)  time: 3.8106  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1350/3449]  eta: 2:12:48  lr: 0.000100  loss: 0.0500 (0.0624)  time: 3.7834  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1360/3449]  eta: 2:12:09  lr: 0.000100  loss: 0.0618 (0.0624)  time: 3.7505  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1370/3449]  eta: 2:11:32  lr: 0.000100  loss: 0.0553 (0.0624)  time: 3.7912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1380/3449]  eta: 2:10:55  lr: 0.000100  loss: 0.0507 (0.0623)  time: 3.8691  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1390/3449]  eta: 2:10:18  lr: 0.000100  loss: 0.0623 (0.0623)  time: 3.8733  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1400/3449]  eta: 2:09:40  lr: 0.000100  loss: 0.0537 (0.0623)  time: 3.8186  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1410/3449]  eta: 2:09:01  lr: 0.000100  loss: 0.0567 (0.0622)  time: 3.7476  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1420/3449]  eta: 2:08:24  lr: 0.000100  loss: 0.0574 (0.0622)  time: 3.7850  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1430/3449]  eta: 2:07:47  lr: 0.000100  loss: 0.0549 (0.0622)  time: 3.8634  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1440/3449]  eta: 2:07:09  lr: 0.000100  loss: 0.0555 (0.0623)  time: 3.8188  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1450/3449]  eta: 2:06:29  lr: 0.000100  loss: 0.0779 (0.0627)  time: 3.7321  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1460/3449]  eta: 2:05:49  lr: 0.000100  loss: 0.1292 (0.0632)  time: 3.6699  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1470/3449]  eta: 2:05:11  lr: 0.000100  loss: 0.0870 (0.0633)  time: 3.7030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1480/3449]  eta: 2:04:33  lr: 0.000100  loss: 0.0645 (0.0633)  time: 3.7874  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1490/3449]  eta: 2:03:57  lr: 0.000100  loss: 0.0463 (0.0631)  time: 3.8866  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1500/3449]  eta: 2:03:20  lr: 0.000100  loss: 0.0295 (0.0629)  time: 3.9013  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1510/3449]  eta: 2:02:41  lr: 0.000100  loss: 0.0289 (0.0626)  time: 3.7928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1520/3449]  eta: 2:02:04  lr: 0.000100  loss: 0.0272 (0.0624)  time: 3.8121  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1530/3449]  eta: 2:01:26  lr: 0.000100  loss: 0.0325 (0.0622)  time: 3.8310  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1540/3449]  eta: 2:00:47  lr: 0.000100  loss: 0.0343 (0.0621)  time: 3.7350  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1550/3449]  eta: 2:00:09  lr: 0.000100  loss: 0.0315 (0.0619)  time: 3.7358  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1560/3449]  eta: 1:59:30  lr: 0.000100  loss: 0.0354 (0.0617)  time: 3.7634  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1570/3449]  eta: 1:58:53  lr: 0.000100  loss: 0.0391 (0.0616)  time: 3.8120  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1580/3449]  eta: 1:58:15  lr: 0.000100  loss: 0.0504 (0.0618)  time: 3.8190  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1590/3449]  eta: 1:57:36  lr: 0.000100  loss: 0.0904 (0.0619)  time: 3.7432  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1600/3449]  eta: 1:56:58  lr: 0.000100  loss: 0.0717 (0.0619)  time: 3.7574  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1610/3449]  eta: 1:56:21  lr: 0.000100  loss: 0.0663 (0.0619)  time: 3.8272  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1620/3449]  eta: 1:55:43  lr: 0.000100  loss: 0.0439 (0.0617)  time: 3.8473  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1630/3449]  eta: 1:55:05  lr: 0.000100  loss: 0.0390 (0.0616)  time: 3.8167  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1640/3449]  eta: 1:54:28  lr: 0.000100  loss: 0.0390 (0.0615)  time: 3.8409  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1650/3449]  eta: 1:53:50  lr: 0.000100  loss: 0.0426 (0.0615)  time: 3.8115  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1660/3449]  eta: 1:53:11  lr: 0.000100  loss: 0.0467 (0.0614)  time: 3.7580  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1670/3449]  eta: 1:52:32  lr: 0.000100  loss: 0.0528 (0.0614)  time: 3.7355  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1680/3449]  eta: 1:51:54  lr: 0.000100  loss: 0.0750 (0.0618)  time: 3.7294  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1690/3449]  eta: 1:51:16  lr: 0.000100  loss: 0.1284 (0.0622)  time: 3.7731  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1700/3449]  eta: 1:50:38  lr: 0.000100  loss: 0.0847 (0.0623)  time: 3.7858  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1710/3449]  eta: 1:50:00  lr: 0.000100  loss: 0.0740 (0.0623)  time: 3.7865  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1720/3449]  eta: 1:49:22  lr: 0.000100  loss: 0.0663 (0.0623)  time: 3.7876  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1730/3449]  eta: 1:48:43  lr: 0.000100  loss: 0.0412 (0.0622)  time: 3.7347  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1740/3449]  eta: 1:48:06  lr: 0.000100  loss: 0.0414 (0.0622)  time: 3.7808  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1750/3449]  eta: 1:47:27  lr: 0.000100  loss: 0.0523 (0.0621)  time: 3.8263  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1760/3449]  eta: 1:46:50  lr: 0.000100  loss: 0.0450 (0.0620)  time: 3.8024  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1770/3449]  eta: 1:46:13  lr: 0.000100  loss: 0.0465 (0.0620)  time: 3.8824  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1780/3449]  eta: 1:45:34  lr: 0.000100  loss: 0.0391 (0.0618)  time: 3.7823  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1790/3449]  eta: 1:44:56  lr: 0.000100  loss: 0.0273 (0.0617)  time: 3.7499  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1800/3449]  eta: 1:44:18  lr: 0.000100  loss: 0.0562 (0.0617)  time: 3.8245  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1810/3449]  eta: 1:43:40  lr: 0.000100  loss: 0.0608 (0.0617)  time: 3.7632  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1820/3449]  eta: 1:43:02  lr: 0.000100  loss: 0.0718 (0.0619)  time: 3.7829  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1830/3449]  eta: 1:42:24  lr: 0.000100  loss: 0.0681 (0.0618)  time: 3.7972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1840/3449]  eta: 1:41:47  lr: 0.000100  loss: 0.0649 (0.0619)  time: 3.8234  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1850/3449]  eta: 1:41:10  lr: 0.000100  loss: 0.0452 (0.0617)  time: 3.8819  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1860/3449]  eta: 1:40:32  lr: 0.000100  loss: 0.0302 (0.0616)  time: 3.8873  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1870/3449]  eta: 1:39:53  lr: 0.000100  loss: 0.0289 (0.0614)  time: 3.7658  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1880/3449]  eta: 1:39:16  lr: 0.000100  loss: 0.0453 (0.0614)  time: 3.7418  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1890/3449]  eta: 1:38:38  lr: 0.000100  loss: 0.0527 (0.0613)  time: 3.8240  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [1900/3449]  eta: 1:38:00  lr: 0.000100  loss: 0.0474 (0.0613)  time: 3.8428  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1910/3449]  eta: 1:37:22  lr: 0.000100  loss: 0.0331 (0.0611)  time: 3.8097  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1920/3449]  eta: 1:36:42  lr: 0.000100  loss: 0.0315 (0.0611)  time: 3.6626  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1930/3449]  eta: 1:36:05  lr: 0.000100  loss: 0.0708 (0.0612)  time: 3.7157  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1940/3449]  eta: 1:35:28  lr: 0.000100  loss: 0.0717 (0.0612)  time: 3.9119  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1950/3449]  eta: 1:34:49  lr: 0.000100  loss: 0.0738 (0.0613)  time: 3.8029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1960/3449]  eta: 1:34:11  lr: 0.000100  loss: 0.0893 (0.0615)  time: 3.7049  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1970/3449]  eta: 1:33:32  lr: 0.000100  loss: 0.0868 (0.0615)  time: 3.6972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [1980/3449]  eta: 1:32:53  lr: 0.000100  loss: 0.0480 (0.0614)  time: 3.6758  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [1990/3449]  eta: 1:32:15  lr: 0.000100  loss: 0.0419 (0.0613)  time: 3.7426  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2000/3449]  eta: 1:31:37  lr: 0.000100  loss: 0.0489 (0.0613)  time: 3.8011  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2010/3449]  eta: 1:31:00  lr: 0.000100  loss: 0.0579 (0.0613)  time: 3.8334  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2020/3449]  eta: 1:30:23  lr: 0.000100  loss: 0.0575 (0.0613)  time: 3.8713  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2030/3449]  eta: 1:29:45  lr: 0.000100  loss: 0.0602 (0.0613)  time: 3.8792  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2040/3449]  eta: 1:29:07  lr: 0.000100  loss: 0.0568 (0.0613)  time: 3.8305  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2050/3449]  eta: 1:28:29  lr: 0.000100  loss: 0.0574 (0.0613)  time: 3.8204  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2060/3449]  eta: 1:27:51  lr: 0.000100  loss: 0.0650 (0.0613)  time: 3.7717  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2070/3449]  eta: 1:27:13  lr: 0.000100  loss: 0.0405 (0.0612)  time: 3.7880  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2080/3449]  eta: 1:26:36  lr: 0.000100  loss: 0.0621 (0.0613)  time: 3.8672  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2090/3449]  eta: 1:25:58  lr: 0.000100  loss: 0.0545 (0.0612)  time: 3.8456  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2100/3449]  eta: 1:25:20  lr: 0.000100  loss: 0.0302 (0.0610)  time: 3.8064  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2110/3449]  eta: 1:24:42  lr: 0.000100  loss: 0.0255 (0.0609)  time: 3.7466  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2120/3449]  eta: 1:24:04  lr: 0.000100  loss: 0.0412 (0.0610)  time: 3.7408  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2130/3449]  eta: 1:23:26  lr: 0.000100  loss: 0.0841 (0.0612)  time: 3.7778  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2140/3449]  eta: 1:22:48  lr: 0.000100  loss: 0.0846 (0.0614)  time: 3.8035  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2150/3449]  eta: 1:22:10  lr: 0.000100  loss: 0.0990 (0.0616)  time: 3.8238  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2160/3449]  eta: 1:21:31  lr: 0.000100  loss: 0.0787 (0.0616)  time: 3.7531  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2170/3449]  eta: 1:20:54  lr: 0.000100  loss: 0.0536 (0.0616)  time: 3.8244  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2180/3449]  eta: 1:20:16  lr: 0.000100  loss: 0.0745 (0.0617)  time: 3.8293  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2190/3449]  eta: 1:19:38  lr: 0.000100  loss: 0.0862 (0.0618)  time: 3.7610  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2200/3449]  eta: 1:19:00  lr: 0.000100  loss: 0.0629 (0.0618)  time: 3.8178  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2210/3449]  eta: 1:18:22  lr: 0.000100  loss: 0.0341 (0.0617)  time: 3.8164  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2220/3449]  eta: 1:17:44  lr: 0.000100  loss: 0.0422 (0.0617)  time: 3.7819  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2230/3449]  eta: 1:17:06  lr: 0.000100  loss: 0.0559 (0.0617)  time: 3.7908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2240/3449]  eta: 1:16:29  lr: 0.000100  loss: 0.0518 (0.0616)  time: 3.8385  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2250/3449]  eta: 1:15:52  lr: 0.000100  loss: 0.0561 (0.0616)  time: 3.9080  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2260/3449]  eta: 1:15:14  lr: 0.000100  loss: 0.0566 (0.0616)  time: 3.8626  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2270/3449]  eta: 1:14:36  lr: 0.000100  loss: 0.0560 (0.0616)  time: 3.8038  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2280/3449]  eta: 1:13:58  lr: 0.000100  loss: 0.0598 (0.0616)  time: 3.7837  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2290/3449]  eta: 1:13:19  lr: 0.000100  loss: 0.0577 (0.0616)  time: 3.7347  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2300/3449]  eta: 1:12:41  lr: 0.000100  loss: 0.0501 (0.0615)  time: 3.7639  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2310/3449]  eta: 1:12:03  lr: 0.000100  loss: 0.0501 (0.0615)  time: 3.7600  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2320/3449]  eta: 1:11:24  lr: 0.000100  loss: 0.0862 (0.0617)  time: 3.7028  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2330/3449]  eta: 1:10:47  lr: 0.000100  loss: 0.1210 (0.0619)  time: 3.7446  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2340/3449]  eta: 1:10:09  lr: 0.000100  loss: 0.0759 (0.0619)  time: 3.8175  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2350/3449]  eta: 1:09:30  lr: 0.000100  loss: 0.0698 (0.0620)  time: 3.7256  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2360/3449]  eta: 1:08:52  lr: 0.000100  loss: 0.0861 (0.0621)  time: 3.6991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2370/3449]  eta: 1:08:14  lr: 0.000100  loss: 0.0752 (0.0621)  time: 3.7876  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2380/3449]  eta: 1:07:36  lr: 0.000100  loss: 0.0526 (0.0621)  time: 3.8386  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2390/3449]  eta: 1:06:58  lr: 0.000100  loss: 0.0592 (0.0621)  time: 3.8242  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2400/3449]  eta: 1:06:20  lr: 0.000100  loss: 0.0698 (0.0621)  time: 3.7593  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2410/3449]  eta: 1:05:42  lr: 0.000100  loss: 0.0698 (0.0621)  time: 3.7446  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2420/3449]  eta: 1:05:04  lr: 0.000100  loss: 0.0559 (0.0621)  time: 3.7128  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2430/3449]  eta: 1:04:25  lr: 0.000100  loss: 0.0479 (0.0622)  time: 3.7036  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2440/3449]  eta: 1:03:48  lr: 0.000100  loss: 0.0560 (0.0621)  time: 3.7652  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2450/3449]  eta: 1:03:09  lr: 0.000100  loss: 0.0675 (0.0623)  time: 3.7521  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2460/3449]  eta: 1:02:31  lr: 0.000100  loss: 0.1056 (0.0624)  time: 3.6849  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2470/3449]  eta: 1:01:53  lr: 0.000100  loss: 0.0968 (0.0626)  time: 3.7549  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2480/3449]  eta: 1:01:15  lr: 0.000100  loss: 0.0558 (0.0625)  time: 3.8530  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2490/3449]  eta: 1:00:37  lr: 0.000100  loss: 0.0355 (0.0624)  time: 3.8320  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2500/3449]  eta: 1:00:00  lr: 0.000100  loss: 0.0266 (0.0623)  time: 3.8249  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2510/3449]  eta: 0:59:22  lr: 0.000100  loss: 0.0451 (0.0622)  time: 3.8275  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2520/3449]  eta: 0:58:44  lr: 0.000100  loss: 0.0527 (0.0622)  time: 3.7911  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2530/3449]  eta: 0:58:05  lr: 0.000100  loss: 0.0326 (0.0621)  time: 3.7211  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2540/3449]  eta: 0:57:27  lr: 0.000100  loss: 0.0322 (0.0620)  time: 3.7178  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2550/3449]  eta: 0:56:49  lr: 0.000100  loss: 0.0553 (0.0620)  time: 3.7589  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [2560/3449]  eta: 0:56:12  lr: 0.000100  loss: 0.0628 (0.0620)  time: 3.8087  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2570/3449]  eta: 0:55:34  lr: 0.000100  loss: 0.0553 (0.0620)  time: 3.8275  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2580/3449]  eta: 0:54:56  lr: 0.000100  loss: 0.0616 (0.0620)  time: 3.7745  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2590/3449]  eta: 0:54:18  lr: 0.000100  loss: 0.0806 (0.0621)  time: 3.8330  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2600/3449]  eta: 0:53:40  lr: 0.000100  loss: 0.0806 (0.0622)  time: 3.8674  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2610/3449]  eta: 0:53:02  lr: 0.000100  loss: 0.0769 (0.0622)  time: 3.7978  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2620/3449]  eta: 0:52:24  lr: 0.000100  loss: 0.0758 (0.0623)  time: 3.7406  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2630/3449]  eta: 0:51:46  lr: 0.000100  loss: 0.0691 (0.0623)  time: 3.7811  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2640/3449]  eta: 0:51:08  lr: 0.000100  loss: 0.0640 (0.0623)  time: 3.8173  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2650/3449]  eta: 0:50:31  lr: 0.000100  loss: 0.0623 (0.0623)  time: 3.8273  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2660/3449]  eta: 0:49:53  lr: 0.000100  loss: 0.0513 (0.0622)  time: 3.8613  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2670/3449]  eta: 0:49:15  lr: 0.000100  loss: 0.0559 (0.0622)  time: 3.8287  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2680/3449]  eta: 0:48:37  lr: 0.000100  loss: 0.0588 (0.0623)  time: 3.7715  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2690/3449]  eta: 0:47:59  lr: 0.000100  loss: 0.0585 (0.0622)  time: 3.7893  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2700/3449]  eta: 0:47:21  lr: 0.000100  loss: 0.0537 (0.0622)  time: 3.8362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2710/3449]  eta: 0:46:43  lr: 0.000100  loss: 0.0612 (0.0623)  time: 3.8272  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2720/3449]  eta: 0:46:06  lr: 0.000100  loss: 0.0857 (0.0624)  time: 3.8962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2730/3449]  eta: 0:45:28  lr: 0.000100  loss: 0.1008 (0.0625)  time: 3.8686  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2740/3449]  eta: 0:44:50  lr: 0.000100  loss: 0.0964 (0.0626)  time: 3.7491  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2750/3449]  eta: 0:44:12  lr: 0.000100  loss: 0.0704 (0.0626)  time: 3.7344  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2760/3449]  eta: 0:43:34  lr: 0.000100  loss: 0.0484 (0.0625)  time: 3.8127  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2770/3449]  eta: 0:42:56  lr: 0.000100  loss: 0.0421 (0.0624)  time: 3.9020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2780/3449]  eta: 0:42:18  lr: 0.000100  loss: 0.0605 (0.0625)  time: 3.8706  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2790/3449]  eta: 0:41:40  lr: 0.000100  loss: 0.0707 (0.0626)  time: 3.7893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2800/3449]  eta: 0:41:02  lr: 0.000100  loss: 0.0729 (0.0626)  time: 3.7603  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2810/3449]  eta: 0:40:25  lr: 0.000100  loss: 0.0408 (0.0625)  time: 3.8094  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2820/3449]  eta: 0:39:47  lr: 0.000100  loss: 0.0362 (0.0624)  time: 3.8151  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2830/3449]  eta: 0:39:08  lr: 0.000100  loss: 0.0320 (0.0623)  time: 3.7104  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2840/3449]  eta: 0:38:30  lr: 0.000100  loss: 0.0319 (0.0622)  time: 3.7006  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2850/3449]  eta: 0:37:52  lr: 0.000100  loss: 0.0319 (0.0621)  time: 3.7750  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2860/3449]  eta: 0:37:14  lr: 0.000100  loss: 0.0305 (0.0620)  time: 3.7305  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2870/3449]  eta: 0:36:36  lr: 0.000100  loss: 0.0373 (0.0619)  time: 3.6887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2880/3449]  eta: 0:35:58  lr: 0.000100  loss: 0.0528 (0.0619)  time: 3.7610  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2890/3449]  eta: 0:35:20  lr: 0.000100  loss: 0.0587 (0.0620)  time: 3.7867  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2900/3449]  eta: 0:34:42  lr: 0.000100  loss: 0.0750 (0.0621)  time: 3.7937  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2910/3449]  eta: 0:34:04  lr: 0.000100  loss: 0.0750 (0.0621)  time: 3.7685  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2920/3449]  eta: 0:33:26  lr: 0.000100  loss: 0.0558 (0.0620)  time: 3.7711  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2930/3449]  eta: 0:32:48  lr: 0.000100  loss: 0.0506 (0.0620)  time: 3.8139  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2940/3449]  eta: 0:32:10  lr: 0.000100  loss: 0.0444 (0.0619)  time: 3.8061  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2950/3449]  eta: 0:31:33  lr: 0.000100  loss: 0.0385 (0.0618)  time: 3.8146  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2960/3449]  eta: 0:30:55  lr: 0.000100  loss: 0.0334 (0.0617)  time: 3.8271  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2970/3449]  eta: 0:30:17  lr: 0.000100  loss: 0.0300 (0.0616)  time: 3.8343  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [2980/3449]  eta: 0:29:39  lr: 0.000100  loss: 0.0373 (0.0616)  time: 3.9019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [2990/3449]  eta: 0:29:01  lr: 0.000100  loss: 0.0695 (0.0618)  time: 3.8791  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3000/3449]  eta: 0:28:23  lr: 0.000100  loss: 0.1044 (0.0619)  time: 3.8264  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3010/3449]  eta: 0:27:45  lr: 0.000100  loss: 0.0780 (0.0619)  time: 3.8166  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3020/3449]  eta: 0:27:08  lr: 0.000100  loss: 0.0653 (0.0619)  time: 3.8194  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [3030/3449]  eta: 0:26:30  lr: 0.000100  loss: 0.0597 (0.0619)  time: 3.8015  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [3040/3449]  eta: 0:25:52  lr: 0.000100  loss: 0.0696 (0.0621)  time: 3.8038  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3050/3449]  eta: 0:25:14  lr: 0.000100  loss: 0.0742 (0.0621)  time: 3.8125  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3060/3449]  eta: 0:24:36  lr: 0.000100  loss: 0.0366 (0.0620)  time: 3.7723  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3070/3449]  eta: 0:23:58  lr: 0.000100  loss: 0.0414 (0.0619)  time: 3.7456  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3080/3449]  eta: 0:23:20  lr: 0.000100  loss: 0.0619 (0.0620)  time: 3.7929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3090/3449]  eta: 0:22:42  lr: 0.000100  loss: 0.0608 (0.0620)  time: 3.8898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3100/3449]  eta: 0:22:04  lr: 0.000100  loss: 0.0699 (0.0621)  time: 3.8180  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3110/3449]  eta: 0:21:26  lr: 0.000100  loss: 0.0699 (0.0621)  time: 3.7983  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [3120/3449]  eta: 0:20:48  lr: 0.000100  loss: 0.0625 (0.0622)  time: 3.7692  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [3130/3449]  eta: 0:20:10  lr: 0.000100  loss: 0.0534 (0.0621)  time: 3.7489  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3140/3449]  eta: 0:19:32  lr: 0.000100  loss: 0.0360 (0.0620)  time: 3.8295  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3150/3449]  eta: 0:18:54  lr: 0.000100  loss: 0.0423 (0.0621)  time: 3.7643  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3160/3449]  eta: 0:18:16  lr: 0.000100  loss: 0.0906 (0.0622)  time: 3.8003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3170/3449]  eta: 0:17:38  lr: 0.000100  loss: 0.1058 (0.0624)  time: 3.9012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3180/3449]  eta: 0:17:00  lr: 0.000100  loss: 0.0911 (0.0624)  time: 3.7585  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3190/3449]  eta: 0:16:22  lr: 0.000100  loss: 0.0614 (0.0624)  time: 3.7620  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3200/3449]  eta: 0:15:44  lr: 0.000100  loss: 0.0366 (0.0623)  time: 3.8226  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [3210/3449]  eta: 0:15:07  lr: 0.000100  loss: 0.0339 (0.0622)  time: 3.7846  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [3220/3449]  eta: 0:14:29  lr: 0.000100  loss: 0.0325 (0.0622)  time: 3.8060  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3230/3449]  eta: 0:13:51  lr: 0.000100  loss: 0.0418 (0.0621)  time: 3.8426  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3240/3449]  eta: 0:13:13  lr: 0.000100  loss: 0.0334 (0.0620)  time: 3.8905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [3250/3449]  eta: 0:12:35  lr: 0.000100  loss: 0.0366 (0.0620)  time: 3.8095  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3260/3449]  eta: 0:11:57  lr: 0.000100  loss: 0.0577 (0.0620)  time: 3.7343  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3270/3449]  eta: 0:11:19  lr: 0.000100  loss: 0.0601 (0.0621)  time: 3.7890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3280/3449]  eta: 0:10:41  lr: 0.000100  loss: 0.0758 (0.0621)  time: 3.8230  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3290/3449]  eta: 0:10:03  lr: 0.000100  loss: 0.0790 (0.0622)  time: 3.8301  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3300/3449]  eta: 0:09:25  lr: 0.000100  loss: 0.0657 (0.0622)  time: 3.8523  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3310/3449]  eta: 0:08:47  lr: 0.000100  loss: 0.0583 (0.0622)  time: 3.9134  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3320/3449]  eta: 0:08:09  lr: 0.000100  loss: 0.0353 (0.0621)  time: 3.8614  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3330/3449]  eta: 0:07:31  lr: 0.000100  loss: 0.0321 (0.0620)  time: 3.7537  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3340/3449]  eta: 0:06:53  lr: 0.000100  loss: 0.0312 (0.0619)  time: 3.7760  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3350/3449]  eta: 0:06:15  lr: 0.000100  loss: 0.0350 (0.0618)  time: 3.8339  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3360/3449]  eta: 0:05:37  lr: 0.000100  loss: 0.0488 (0.0618)  time: 3.8210  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3370/3449]  eta: 0:04:59  lr: 0.000100  loss: 0.0662 (0.0618)  time: 3.7811  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3380/3449]  eta: 0:04:21  lr: 0.000100  loss: 0.0597 (0.0618)  time: 3.7921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [3390/3449]  eta: 0:03:43  lr: 0.000100  loss: 0.0487 (0.0618)  time: 3.8174  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [3400/3449]  eta: 0:03:06  lr: 0.000100  loss: 0.0557 (0.0618)  time: 3.8246  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [3410/3449]  eta: 0:02:28  lr: 0.000100  loss: 0.0597 (0.0618)  time: 3.8414  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3420/3449]  eta: 0:01:50  lr: 0.000100  loss: 0.0538 (0.0618)  time: 3.8362  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [3430/3449]  eta: 0:01:12  lr: 0.000100  loss: 0.0601 (0.0619)  time: 3.7469  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17]  [3440/3449]  eta: 0:00:34  lr: 0.000100  loss: 0.1092 (0.0621)  time: 3.7310  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:17]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.1329 (0.0622)  time: 3.7728  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:17] Total time: 3:38:13 (3.7964 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1329 (0.0622)\n",
      "Valid: [epoch:17]  [ 0/14]  eta: 0:04:20  loss: 0.0315 (0.0315)  time: 18.6289  data: 0.4288  max mem: 34968\n",
      "Valid: [epoch:17]  [13/14]  eta: 0:00:18  loss: 0.0296 (0.0300)  time: 18.2384  data: 0.0309  max mem: 34968\n",
      "Valid: [epoch:17] Total time: 0:04:15 (18.2507 s / it)\n",
      "Averaged stats: loss: 0.0296 (0.0300)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_17_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.030%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:18]  [   0/3449]  eta: 5:17:03  lr: 0.000100  loss: 0.0576 (0.0576)  time: 5.5156  data: 1.2216  max mem: 34968\n",
      "Train: [epoch:18]  [  10/3449]  eta: 3:48:41  lr: 0.000100  loss: 0.0475 (0.0478)  time: 3.9900  data: 0.1112  max mem: 34968\n",
      "Train: [epoch:18]  [  20/3449]  eta: 3:43:20  lr: 0.000100  loss: 0.0538 (0.0585)  time: 3.8277  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [  30/3449]  eta: 3:39:59  lr: 0.000100  loss: 0.0609 (0.0579)  time: 3.7896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [  40/3449]  eta: 3:38:14  lr: 0.000100  loss: 0.0402 (0.0525)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [  50/3449]  eta: 3:36:49  lr: 0.000100  loss: 0.0315 (0.0487)  time: 3.7757  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [  60/3449]  eta: 3:36:45  lr: 0.000100  loss: 0.0315 (0.0468)  time: 3.8302  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [  70/3449]  eta: 3:35:11  lr: 0.000100  loss: 0.0451 (0.0487)  time: 3.8054  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [  80/3449]  eta: 3:34:29  lr: 0.000100  loss: 0.0562 (0.0506)  time: 3.7666  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [  90/3449]  eta: 3:33:22  lr: 0.000100  loss: 0.0562 (0.0521)  time: 3.7763  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 100/3449]  eta: 3:32:38  lr: 0.000100  loss: 0.0521 (0.0518)  time: 3.7673  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 110/3449]  eta: 3:31:26  lr: 0.000100  loss: 0.0546 (0.0535)  time: 3.7461  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 120/3449]  eta: 3:30:35  lr: 0.000100  loss: 0.0804 (0.0556)  time: 3.7244  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 130/3449]  eta: 3:29:25  lr: 0.000100  loss: 0.0665 (0.0561)  time: 3.7111  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 140/3449]  eta: 3:28:30  lr: 0.000100  loss: 0.0487 (0.0551)  time: 3.6916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 150/3449]  eta: 3:28:03  lr: 0.000100  loss: 0.0438 (0.0556)  time: 3.7715  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 160/3449]  eta: 3:27:27  lr: 0.000100  loss: 0.0568 (0.0553)  time: 3.8107  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 170/3449]  eta: 3:27:01  lr: 0.000100  loss: 0.0587 (0.0562)  time: 3.8191  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 180/3449]  eta: 3:26:16  lr: 0.000100  loss: 0.0735 (0.0572)  time: 3.7976  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 190/3449]  eta: 3:25:13  lr: 0.000100  loss: 0.0692 (0.0582)  time: 3.6950  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 200/3449]  eta: 3:24:30  lr: 0.000100  loss: 0.0633 (0.0586)  time: 3.6934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 210/3449]  eta: 3:23:56  lr: 0.000100  loss: 0.0618 (0.0584)  time: 3.7733  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 220/3449]  eta: 3:23:28  lr: 0.000100  loss: 0.0537 (0.0581)  time: 3.8214  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 230/3449]  eta: 3:23:00  lr: 0.000100  loss: 0.0596 (0.0585)  time: 3.8476  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 240/3449]  eta: 3:22:21  lr: 0.000100  loss: 0.0534 (0.0579)  time: 3.8155  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 250/3449]  eta: 3:21:43  lr: 0.000100  loss: 0.0413 (0.0576)  time: 3.7763  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 260/3449]  eta: 3:20:57  lr: 0.000100  loss: 0.0535 (0.0580)  time: 3.7483  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 270/3449]  eta: 3:20:05  lr: 0.000100  loss: 0.0737 (0.0589)  time: 3.6897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 280/3449]  eta: 3:19:11  lr: 0.000100  loss: 0.0857 (0.0607)  time: 3.6442  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 290/3449]  eta: 3:18:44  lr: 0.000100  loss: 0.1019 (0.0620)  time: 3.7497  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 300/3449]  eta: 3:18:20  lr: 0.000100  loss: 0.0751 (0.0616)  time: 3.8898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 310/3449]  eta: 3:17:44  lr: 0.000100  loss: 0.0371 (0.0608)  time: 3.8520  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 320/3449]  eta: 3:17:01  lr: 0.000100  loss: 0.0364 (0.0601)  time: 3.7593  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 330/3449]  eta: 3:16:24  lr: 0.000100  loss: 0.0380 (0.0594)  time: 3.7555  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 340/3449]  eta: 3:15:39  lr: 0.000100  loss: 0.0373 (0.0586)  time: 3.7427  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 350/3449]  eta: 3:15:04  lr: 0.000100  loss: 0.0398 (0.0589)  time: 3.7530  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 360/3449]  eta: 3:14:30  lr: 0.000100  loss: 0.0996 (0.0608)  time: 3.8184  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [ 370/3449]  eta: 3:13:51  lr: 0.000100  loss: 0.1014 (0.0618)  time: 3.7977  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 380/3449]  eta: 3:13:08  lr: 0.000100  loss: 0.0672 (0.0617)  time: 3.7377  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 390/3449]  eta: 3:12:39  lr: 0.000100  loss: 0.0492 (0.0611)  time: 3.7952  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 400/3449]  eta: 3:12:12  lr: 0.000100  loss: 0.0508 (0.0614)  time: 3.9044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 410/3449]  eta: 3:11:44  lr: 0.000100  loss: 0.0525 (0.0612)  time: 3.9222  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 420/3449]  eta: 3:11:08  lr: 0.000100  loss: 0.0431 (0.0609)  time: 3.8594  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 430/3449]  eta: 3:10:32  lr: 0.000100  loss: 0.0662 (0.0614)  time: 3.8128  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 440/3449]  eta: 3:10:05  lr: 0.000100  loss: 0.0730 (0.0615)  time: 3.8814  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 450/3449]  eta: 3:09:23  lr: 0.000100  loss: 0.0480 (0.0611)  time: 3.8383  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 460/3449]  eta: 3:08:47  lr: 0.000100  loss: 0.0470 (0.0608)  time: 3.7709  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 470/3449]  eta: 3:08:10  lr: 0.000100  loss: 0.0492 (0.0607)  time: 3.8041  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 480/3449]  eta: 3:07:37  lr: 0.000100  loss: 0.0538 (0.0607)  time: 3.8380  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 490/3449]  eta: 3:07:03  lr: 0.000100  loss: 0.0595 (0.0606)  time: 3.8658  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 500/3449]  eta: 3:06:28  lr: 0.000100  loss: 0.0622 (0.0610)  time: 3.8486  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 510/3449]  eta: 3:05:53  lr: 0.000100  loss: 0.0985 (0.0619)  time: 3.8454  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 520/3449]  eta: 3:05:13  lr: 0.000100  loss: 0.1018 (0.0626)  time: 3.8014  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 530/3449]  eta: 3:04:33  lr: 0.000100  loss: 0.0970 (0.0631)  time: 3.7587  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 540/3449]  eta: 3:03:55  lr: 0.000100  loss: 0.0701 (0.0632)  time: 3.7824  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 550/3449]  eta: 3:03:16  lr: 0.000100  loss: 0.0467 (0.0628)  time: 3.7804  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 560/3449]  eta: 3:02:44  lr: 0.000100  loss: 0.0526 (0.0629)  time: 3.8362  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 570/3449]  eta: 3:02:06  lr: 0.000100  loss: 0.0693 (0.0630)  time: 3.8518  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 580/3449]  eta: 3:01:25  lr: 0.000100  loss: 0.0567 (0.0630)  time: 3.7631  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 590/3449]  eta: 3:00:45  lr: 0.000100  loss: 0.0671 (0.0633)  time: 3.7472  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 600/3449]  eta: 3:00:08  lr: 0.000100  loss: 0.0700 (0.0634)  time: 3.7807  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 610/3449]  eta: 2:59:33  lr: 0.000100  loss: 0.0685 (0.0635)  time: 3.8320  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 620/3449]  eta: 2:58:52  lr: 0.000100  loss: 0.0642 (0.0637)  time: 3.7980  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 630/3449]  eta: 2:58:10  lr: 0.000100  loss: 0.0522 (0.0634)  time: 3.7147  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:18]  [ 640/3449]  eta: 2:57:27  lr: 0.000100  loss: 0.0493 (0.0632)  time: 3.6924  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:18]  [ 650/3449]  eta: 2:56:49  lr: 0.000100  loss: 0.0389 (0.0627)  time: 3.7334  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 660/3449]  eta: 2:56:14  lr: 0.000100  loss: 0.0332 (0.0624)  time: 3.8165  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 670/3449]  eta: 2:55:37  lr: 0.000100  loss: 0.0580 (0.0625)  time: 3.8296  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 680/3449]  eta: 2:54:56  lr: 0.000100  loss: 0.0740 (0.0626)  time: 3.7631  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 690/3449]  eta: 2:54:21  lr: 0.000100  loss: 0.0529 (0.0624)  time: 3.8004  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 700/3449]  eta: 2:53:38  lr: 0.000100  loss: 0.0433 (0.0621)  time: 3.7623  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 710/3449]  eta: 2:52:58  lr: 0.000100  loss: 0.0530 (0.0622)  time: 3.6878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 720/3449]  eta: 2:52:22  lr: 0.000100  loss: 0.0614 (0.0623)  time: 3.7917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 730/3449]  eta: 2:51:43  lr: 0.000100  loss: 0.0636 (0.0624)  time: 3.8022  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 740/3449]  eta: 2:51:02  lr: 0.000100  loss: 0.0683 (0.0624)  time: 3.7339  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 750/3449]  eta: 2:50:27  lr: 0.000100  loss: 0.0861 (0.0629)  time: 3.7873  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 760/3449]  eta: 2:49:48  lr: 0.000100  loss: 0.0977 (0.0632)  time: 3.8160  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 770/3449]  eta: 2:49:07  lr: 0.000100  loss: 0.0744 (0.0633)  time: 3.7327  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 780/3449]  eta: 2:48:30  lr: 0.000100  loss: 0.0695 (0.0633)  time: 3.7559  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 790/3449]  eta: 2:47:56  lr: 0.000100  loss: 0.0739 (0.0635)  time: 3.8511  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 800/3449]  eta: 2:47:19  lr: 0.000100  loss: 0.0559 (0.0634)  time: 3.8563  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 810/3449]  eta: 2:46:40  lr: 0.000100  loss: 0.0462 (0.0634)  time: 3.7849  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 820/3449]  eta: 2:46:03  lr: 0.000100  loss: 0.0576 (0.0635)  time: 3.7838  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 830/3449]  eta: 2:45:28  lr: 0.000100  loss: 0.0630 (0.0635)  time: 3.8518  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 840/3449]  eta: 2:44:46  lr: 0.000100  loss: 0.0667 (0.0636)  time: 3.7769  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 850/3449]  eta: 2:44:07  lr: 0.000100  loss: 0.0667 (0.0636)  time: 3.7124  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 860/3449]  eta: 2:43:30  lr: 0.000100  loss: 0.0476 (0.0634)  time: 3.7795  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 870/3449]  eta: 2:42:54  lr: 0.000100  loss: 0.0476 (0.0634)  time: 3.8359  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 880/3449]  eta: 2:42:22  lr: 0.000100  loss: 0.0619 (0.0633)  time: 3.9294  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 890/3449]  eta: 2:41:46  lr: 0.000100  loss: 0.0592 (0.0634)  time: 3.9213  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 900/3449]  eta: 2:41:07  lr: 0.000100  loss: 0.0626 (0.0634)  time: 3.8010  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [ 910/3449]  eta: 2:40:27  lr: 0.000100  loss: 0.0748 (0.0637)  time: 3.7377  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 920/3449]  eta: 2:39:49  lr: 0.000100  loss: 0.0872 (0.0640)  time: 3.7476  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 930/3449]  eta: 2:39:12  lr: 0.000100  loss: 0.0707 (0.0640)  time: 3.8109  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 940/3449]  eta: 2:38:33  lr: 0.000100  loss: 0.0732 (0.0643)  time: 3.7970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 950/3449]  eta: 2:37:56  lr: 0.000100  loss: 0.0880 (0.0646)  time: 3.7894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 960/3449]  eta: 2:37:20  lr: 0.000100  loss: 0.0880 (0.0650)  time: 3.8404  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 970/3449]  eta: 2:36:45  lr: 0.000100  loss: 0.0737 (0.0651)  time: 3.8778  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 980/3449]  eta: 2:36:04  lr: 0.000100  loss: 0.0651 (0.0653)  time: 3.7906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [ 990/3449]  eta: 2:35:28  lr: 0.000100  loss: 0.1108 (0.0658)  time: 3.7789  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1000/3449]  eta: 2:34:51  lr: 0.000100  loss: 0.0957 (0.0660)  time: 3.8650  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1010/3449]  eta: 2:34:17  lr: 0.000100  loss: 0.0661 (0.0660)  time: 3.8891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1020/3449]  eta: 2:33:36  lr: 0.000100  loss: 0.0602 (0.0659)  time: 3.8004  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [1030/3449]  eta: 2:32:57  lr: 0.000100  loss: 0.0643 (0.0659)  time: 3.7204  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1040/3449]  eta: 2:32:17  lr: 0.000100  loss: 0.0728 (0.0659)  time: 3.7384  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1050/3449]  eta: 2:31:38  lr: 0.000100  loss: 0.0369 (0.0656)  time: 3.7168  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1060/3449]  eta: 2:31:05  lr: 0.000100  loss: 0.0362 (0.0655)  time: 3.8631  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1070/3449]  eta: 2:30:28  lr: 0.000100  loss: 0.0553 (0.0654)  time: 3.9187  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1080/3449]  eta: 2:29:49  lr: 0.000100  loss: 0.0649 (0.0654)  time: 3.8005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1090/3449]  eta: 2:29:12  lr: 0.000100  loss: 0.0639 (0.0654)  time: 3.8043  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1100/3449]  eta: 2:28:32  lr: 0.000100  loss: 0.0501 (0.0652)  time: 3.7752  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1110/3449]  eta: 2:27:55  lr: 0.000100  loss: 0.0523 (0.0654)  time: 3.7632  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1120/3449]  eta: 2:27:17  lr: 0.000100  loss: 0.1031 (0.0660)  time: 3.8144  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1130/3449]  eta: 2:26:40  lr: 0.000100  loss: 0.0775 (0.0660)  time: 3.8270  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1140/3449]  eta: 2:26:02  lr: 0.000100  loss: 0.0570 (0.0659)  time: 3.8003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1150/3449]  eta: 2:25:22  lr: 0.000100  loss: 0.0567 (0.0659)  time: 3.7333  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1160/3449]  eta: 2:24:43  lr: 0.000100  loss: 0.0696 (0.0660)  time: 3.7225  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1170/3449]  eta: 2:24:05  lr: 0.000100  loss: 0.0556 (0.0658)  time: 3.7553  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1180/3449]  eta: 2:23:27  lr: 0.000100  loss: 0.0478 (0.0657)  time: 3.7839  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1190/3449]  eta: 2:22:47  lr: 0.000100  loss: 0.0525 (0.0656)  time: 3.7494  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1200/3449]  eta: 2:22:09  lr: 0.000100  loss: 0.0525 (0.0655)  time: 3.7474  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1210/3449]  eta: 2:21:29  lr: 0.000100  loss: 0.0546 (0.0655)  time: 3.7359  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1220/3449]  eta: 2:20:53  lr: 0.000100  loss: 0.0700 (0.0656)  time: 3.7932  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1230/3449]  eta: 2:20:17  lr: 0.000100  loss: 0.0610 (0.0655)  time: 3.8913  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1240/3449]  eta: 2:19:38  lr: 0.000100  loss: 0.0429 (0.0653)  time: 3.8085  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1250/3449]  eta: 2:19:01  lr: 0.000100  loss: 0.0433 (0.0653)  time: 3.7993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1260/3449]  eta: 2:18:23  lr: 0.000100  loss: 0.0478 (0.0651)  time: 3.8316  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1270/3449]  eta: 2:17:46  lr: 0.000100  loss: 0.0478 (0.0651)  time: 3.8034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1280/3449]  eta: 2:17:09  lr: 0.000100  loss: 0.0693 (0.0651)  time: 3.8256  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1290/3449]  eta: 2:16:30  lr: 0.000100  loss: 0.0553 (0.0650)  time: 3.7991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1300/3449]  eta: 2:15:52  lr: 0.000100  loss: 0.0630 (0.0651)  time: 3.7615  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1310/3449]  eta: 2:15:12  lr: 0.000100  loss: 0.0640 (0.0651)  time: 3.7182  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1320/3449]  eta: 2:14:34  lr: 0.000100  loss: 0.0637 (0.0651)  time: 3.7347  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1330/3449]  eta: 2:13:55  lr: 0.000100  loss: 0.0574 (0.0649)  time: 3.7728  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1340/3449]  eta: 2:13:18  lr: 0.000100  loss: 0.0446 (0.0648)  time: 3.7923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1350/3449]  eta: 2:12:41  lr: 0.000100  loss: 0.0415 (0.0646)  time: 3.8332  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1360/3449]  eta: 2:12:03  lr: 0.000100  loss: 0.0598 (0.0647)  time: 3.8307  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1370/3449]  eta: 2:11:26  lr: 0.000100  loss: 0.0585 (0.0645)  time: 3.8310  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1380/3449]  eta: 2:10:50  lr: 0.000100  loss: 0.0301 (0.0643)  time: 3.8648  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1390/3449]  eta: 2:10:11  lr: 0.000100  loss: 0.0402 (0.0641)  time: 3.8318  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1400/3449]  eta: 2:09:33  lr: 0.000100  loss: 0.0415 (0.0639)  time: 3.7770  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1410/3449]  eta: 2:08:56  lr: 0.000100  loss: 0.0415 (0.0638)  time: 3.8229  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1420/3449]  eta: 2:08:18  lr: 0.000100  loss: 0.0536 (0.0638)  time: 3.8276  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1430/3449]  eta: 2:07:41  lr: 0.000100  loss: 0.0661 (0.0638)  time: 3.8310  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1440/3449]  eta: 2:07:03  lr: 0.000100  loss: 0.0662 (0.0639)  time: 3.8053  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1450/3449]  eta: 2:06:24  lr: 0.000100  loss: 0.0727 (0.0641)  time: 3.7381  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1460/3449]  eta: 2:05:47  lr: 0.000100  loss: 0.0970 (0.0643)  time: 3.8134  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1470/3449]  eta: 2:05:10  lr: 0.000100  loss: 0.0820 (0.0643)  time: 3.8846  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1480/3449]  eta: 2:04:32  lr: 0.000100  loss: 0.0611 (0.0642)  time: 3.8151  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1490/3449]  eta: 2:03:53  lr: 0.000100  loss: 0.0514 (0.0642)  time: 3.7461  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1500/3449]  eta: 2:03:15  lr: 0.000100  loss: 0.0432 (0.0640)  time: 3.7334  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1510/3449]  eta: 2:02:38  lr: 0.000100  loss: 0.0313 (0.0638)  time: 3.8134  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1520/3449]  eta: 2:02:00  lr: 0.000100  loss: 0.0313 (0.0636)  time: 3.8387  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1530/3449]  eta: 2:01:23  lr: 0.000100  loss: 0.0343 (0.0634)  time: 3.8234  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1540/3449]  eta: 2:00:45  lr: 0.000100  loss: 0.0430 (0.0638)  time: 3.8355  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1550/3449]  eta: 2:00:06  lr: 0.000100  loss: 0.1214 (0.0643)  time: 3.7583  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1560/3449]  eta: 1:59:28  lr: 0.000100  loss: 0.1219 (0.0646)  time: 3.7369  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1570/3449]  eta: 1:58:48  lr: 0.000100  loss: 0.0675 (0.0646)  time: 3.7045  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1580/3449]  eta: 1:58:09  lr: 0.000100  loss: 0.0611 (0.0646)  time: 3.6975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1590/3449]  eta: 1:57:31  lr: 0.000100  loss: 0.0633 (0.0645)  time: 3.7431  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1600/3449]  eta: 1:56:52  lr: 0.000100  loss: 0.0633 (0.0646)  time: 3.7482  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1610/3449]  eta: 1:56:14  lr: 0.000100  loss: 0.0586 (0.0645)  time: 3.7398  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1620/3449]  eta: 1:55:36  lr: 0.000100  loss: 0.0742 (0.0647)  time: 3.7689  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1630/3449]  eta: 1:54:58  lr: 0.000100  loss: 0.1043 (0.0649)  time: 3.7917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1640/3449]  eta: 1:54:20  lr: 0.000100  loss: 0.0889 (0.0651)  time: 3.7786  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1650/3449]  eta: 1:53:41  lr: 0.000100  loss: 0.0726 (0.0651)  time: 3.7607  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1660/3449]  eta: 1:53:03  lr: 0.000100  loss: 0.0389 (0.0649)  time: 3.7588  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1670/3449]  eta: 1:52:26  lr: 0.000100  loss: 0.0266 (0.0646)  time: 3.8352  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1680/3449]  eta: 1:51:49  lr: 0.000100  loss: 0.0292 (0.0645)  time: 3.8604  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [1690/3449]  eta: 1:51:11  lr: 0.000100  loss: 0.0358 (0.0644)  time: 3.8122  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1700/3449]  eta: 1:50:34  lr: 0.000100  loss: 0.0524 (0.0644)  time: 3.8205  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1710/3449]  eta: 1:49:56  lr: 0.000100  loss: 0.0717 (0.0645)  time: 3.8543  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1720/3449]  eta: 1:49:19  lr: 0.000100  loss: 0.0692 (0.0645)  time: 3.8742  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1730/3449]  eta: 1:48:42  lr: 0.000100  loss: 0.0595 (0.0645)  time: 3.8621  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1740/3449]  eta: 1:48:05  lr: 0.000100  loss: 0.0701 (0.0645)  time: 3.8481  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1750/3449]  eta: 1:47:26  lr: 0.000100  loss: 0.0890 (0.0648)  time: 3.8132  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1760/3449]  eta: 1:46:49  lr: 0.000100  loss: 0.0999 (0.0650)  time: 3.8012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1770/3449]  eta: 1:46:12  lr: 0.000100  loss: 0.0824 (0.0650)  time: 3.8839  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1780/3449]  eta: 1:45:34  lr: 0.000100  loss: 0.0577 (0.0650)  time: 3.8704  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1790/3449]  eta: 1:44:54  lr: 0.000100  loss: 0.0538 (0.0650)  time: 3.6924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1800/3449]  eta: 1:44:17  lr: 0.000100  loss: 0.0538 (0.0649)  time: 3.7246  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1810/3449]  eta: 1:43:40  lr: 0.000100  loss: 0.0374 (0.0647)  time: 3.8807  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1820/3449]  eta: 1:43:02  lr: 0.000100  loss: 0.0301 (0.0646)  time: 3.8325  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1830/3449]  eta: 1:42:23  lr: 0.000100  loss: 0.0299 (0.0644)  time: 3.7441  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1840/3449]  eta: 1:41:46  lr: 0.000100  loss: 0.0299 (0.0642)  time: 3.7910  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1850/3449]  eta: 1:41:08  lr: 0.000100  loss: 0.0287 (0.0641)  time: 3.8489  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1860/3449]  eta: 1:40:30  lr: 0.000100  loss: 0.0331 (0.0639)  time: 3.8202  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1870/3449]  eta: 1:39:51  lr: 0.000100  loss: 0.0401 (0.0638)  time: 3.7456  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1880/3449]  eta: 1:39:14  lr: 0.000100  loss: 0.0545 (0.0638)  time: 3.7511  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1890/3449]  eta: 1:38:36  lr: 0.000100  loss: 0.0521 (0.0637)  time: 3.8537  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1900/3449]  eta: 1:37:58  lr: 0.000100  loss: 0.0541 (0.0637)  time: 3.8316  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1910/3449]  eta: 1:37:21  lr: 0.000100  loss: 0.0705 (0.0637)  time: 3.8224  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1920/3449]  eta: 1:36:43  lr: 0.000100  loss: 0.0635 (0.0637)  time: 3.8199  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1930/3449]  eta: 1:36:05  lr: 0.000100  loss: 0.0639 (0.0637)  time: 3.8077  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [1940/3449]  eta: 1:35:27  lr: 0.000100  loss: 0.0581 (0.0637)  time: 3.7880  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1950/3449]  eta: 1:34:48  lr: 0.000100  loss: 0.0368 (0.0635)  time: 3.7243  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1960/3449]  eta: 1:34:12  lr: 0.000100  loss: 0.0335 (0.0634)  time: 3.8359  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1970/3449]  eta: 1:33:33  lr: 0.000100  loss: 0.0475 (0.0635)  time: 3.8578  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1980/3449]  eta: 1:32:56  lr: 0.000100  loss: 0.1089 (0.0641)  time: 3.7900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [1990/3449]  eta: 1:32:17  lr: 0.000100  loss: 0.2059 (0.0648)  time: 3.7827  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2000/3449]  eta: 1:31:40  lr: 0.000100  loss: 0.1623 (0.0652)  time: 3.7844  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2010/3449]  eta: 1:31:02  lr: 0.000100  loss: 0.1206 (0.0653)  time: 3.8344  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2020/3449]  eta: 1:30:24  lr: 0.000100  loss: 0.0732 (0.0653)  time: 3.8446  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2030/3449]  eta: 1:29:47  lr: 0.000100  loss: 0.0408 (0.0652)  time: 3.8730  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2040/3449]  eta: 1:29:08  lr: 0.000100  loss: 0.0380 (0.0650)  time: 3.7990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2050/3449]  eta: 1:28:30  lr: 0.000100  loss: 0.0357 (0.0649)  time: 3.7198  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2060/3449]  eta: 1:27:52  lr: 0.000100  loss: 0.0438 (0.0649)  time: 3.7783  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2070/3449]  eta: 1:27:14  lr: 0.000100  loss: 0.0667 (0.0649)  time: 3.7809  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2080/3449]  eta: 1:26:36  lr: 0.000100  loss: 0.0555 (0.0649)  time: 3.7425  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2090/3449]  eta: 1:25:58  lr: 0.000100  loss: 0.0500 (0.0648)  time: 3.7883  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2100/3449]  eta: 1:25:20  lr: 0.000100  loss: 0.0411 (0.0646)  time: 3.8365  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2110/3449]  eta: 1:24:42  lr: 0.000100  loss: 0.0411 (0.0645)  time: 3.7888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2120/3449]  eta: 1:24:04  lr: 0.000100  loss: 0.0550 (0.0645)  time: 3.7769  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2130/3449]  eta: 1:23:25  lr: 0.000100  loss: 0.0536 (0.0644)  time: 3.7498  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2140/3449]  eta: 1:22:48  lr: 0.000100  loss: 0.0419 (0.0644)  time: 3.7904  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2150/3449]  eta: 1:22:10  lr: 0.000100  loss: 0.0640 (0.0645)  time: 3.8305  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2160/3449]  eta: 1:21:32  lr: 0.000100  loss: 0.0466 (0.0643)  time: 3.7445  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2170/3449]  eta: 1:20:53  lr: 0.000100  loss: 0.0400 (0.0642)  time: 3.7408  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2180/3449]  eta: 1:20:16  lr: 0.000100  loss: 0.0426 (0.0642)  time: 3.7891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2190/3449]  eta: 1:19:38  lr: 0.000100  loss: 0.0322 (0.0640)  time: 3.8258  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2200/3449]  eta: 1:19:00  lr: 0.000100  loss: 0.0320 (0.0639)  time: 3.7898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2210/3449]  eta: 1:18:22  lr: 0.000100  loss: 0.0346 (0.0638)  time: 3.8239  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2220/3449]  eta: 1:17:44  lr: 0.000100  loss: 0.0349 (0.0636)  time: 3.8472  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2230/3449]  eta: 1:17:06  lr: 0.000100  loss: 0.0360 (0.0635)  time: 3.7932  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2240/3449]  eta: 1:16:28  lr: 0.000100  loss: 0.0391 (0.0635)  time: 3.7137  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2250/3449]  eta: 1:15:49  lr: 0.000100  loss: 0.0558 (0.0635)  time: 3.7011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2260/3449]  eta: 1:15:11  lr: 0.000100  loss: 0.0675 (0.0635)  time: 3.7687  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2270/3449]  eta: 1:14:33  lr: 0.000100  loss: 0.0724 (0.0636)  time: 3.7203  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2280/3449]  eta: 1:13:55  lr: 0.000100  loss: 0.0660 (0.0636)  time: 3.7565  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2290/3449]  eta: 1:13:17  lr: 0.000100  loss: 0.0640 (0.0636)  time: 3.7973  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2300/3449]  eta: 1:12:39  lr: 0.000100  loss: 0.0704 (0.0637)  time: 3.7977  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2310/3449]  eta: 1:12:02  lr: 0.000100  loss: 0.0778 (0.0637)  time: 3.8697  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2320/3449]  eta: 1:11:24  lr: 0.000100  loss: 0.0599 (0.0637)  time: 3.8460  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2330/3449]  eta: 1:10:46  lr: 0.000100  loss: 0.0584 (0.0636)  time: 3.7616  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2340/3449]  eta: 1:10:08  lr: 0.000100  loss: 0.0480 (0.0635)  time: 3.7857  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [2350/3449]  eta: 1:09:30  lr: 0.000100  loss: 0.0471 (0.0635)  time: 3.8164  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2360/3449]  eta: 1:08:52  lr: 0.000100  loss: 0.0524 (0.0635)  time: 3.8186  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2370/3449]  eta: 1:08:14  lr: 0.000100  loss: 0.0513 (0.0635)  time: 3.8119  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2380/3449]  eta: 1:07:36  lr: 0.000100  loss: 0.0637 (0.0635)  time: 3.8078  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2390/3449]  eta: 1:06:58  lr: 0.000100  loss: 0.0795 (0.0636)  time: 3.8114  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2400/3449]  eta: 1:06:20  lr: 0.000100  loss: 0.0857 (0.0637)  time: 3.7828  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2410/3449]  eta: 1:05:42  lr: 0.000100  loss: 0.0708 (0.0637)  time: 3.8019  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2420/3449]  eta: 1:05:04  lr: 0.000100  loss: 0.0605 (0.0637)  time: 3.7786  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2430/3449]  eta: 1:04:26  lr: 0.000100  loss: 0.0638 (0.0637)  time: 3.7565  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2440/3449]  eta: 1:03:48  lr: 0.000100  loss: 0.0661 (0.0637)  time: 3.7699  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2450/3449]  eta: 1:03:10  lr: 0.000100  loss: 0.0692 (0.0638)  time: 3.7943  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2460/3449]  eta: 1:02:32  lr: 0.000100  loss: 0.0726 (0.0638)  time: 3.8109  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2470/3449]  eta: 1:01:54  lr: 0.000100  loss: 0.0662 (0.0638)  time: 3.7483  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2480/3449]  eta: 1:01:16  lr: 0.000100  loss: 0.0594 (0.0638)  time: 3.7315  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2490/3449]  eta: 1:00:39  lr: 0.000100  loss: 0.0641 (0.0638)  time: 3.8321  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2500/3449]  eta: 1:00:01  lr: 0.000100  loss: 0.0673 (0.0638)  time: 3.8535  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2510/3449]  eta: 0:59:23  lr: 0.000100  loss: 0.0621 (0.0638)  time: 3.7989  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2520/3449]  eta: 0:58:45  lr: 0.000100  loss: 0.0385 (0.0637)  time: 3.8428  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2530/3449]  eta: 0:58:07  lr: 0.000100  loss: 0.0368 (0.0635)  time: 3.8640  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2540/3449]  eta: 0:57:29  lr: 0.000100  loss: 0.0377 (0.0635)  time: 3.7894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2550/3449]  eta: 0:56:51  lr: 0.000100  loss: 0.0524 (0.0635)  time: 3.7480  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2560/3449]  eta: 0:56:13  lr: 0.000100  loss: 0.0524 (0.0634)  time: 3.7796  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2570/3449]  eta: 0:55:35  lr: 0.000100  loss: 0.0635 (0.0635)  time: 3.7205  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2580/3449]  eta: 0:54:56  lr: 0.000100  loss: 0.0824 (0.0636)  time: 3.6576  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2590/3449]  eta: 0:54:18  lr: 0.000100  loss: 0.0772 (0.0636)  time: 3.7464  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2600/3449]  eta: 0:53:40  lr: 0.000100  loss: 0.0589 (0.0636)  time: 3.7946  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2610/3449]  eta: 0:53:02  lr: 0.000100  loss: 0.0583 (0.0636)  time: 3.7288  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2620/3449]  eta: 0:52:24  lr: 0.000100  loss: 0.0711 (0.0637)  time: 3.7648  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2630/3449]  eta: 0:51:46  lr: 0.000100  loss: 0.0652 (0.0637)  time: 3.7824  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2640/3449]  eta: 0:51:08  lr: 0.000100  loss: 0.0519 (0.0637)  time: 3.7317  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2650/3449]  eta: 0:50:30  lr: 0.000100  loss: 0.0518 (0.0636)  time: 3.7560  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2660/3449]  eta: 0:49:52  lr: 0.000100  loss: 0.0607 (0.0636)  time: 3.7809  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2670/3449]  eta: 0:49:14  lr: 0.000100  loss: 0.0564 (0.0636)  time: 3.8276  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2680/3449]  eta: 0:48:37  lr: 0.000100  loss: 0.0469 (0.0636)  time: 3.8184  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2690/3449]  eta: 0:47:58  lr: 0.000100  loss: 0.0420 (0.0635)  time: 3.7738  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2700/3449]  eta: 0:47:20  lr: 0.000100  loss: 0.0518 (0.0635)  time: 3.7569  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2710/3449]  eta: 0:46:43  lr: 0.000100  loss: 0.0607 (0.0635)  time: 3.8221  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2720/3449]  eta: 0:46:05  lr: 0.000100  loss: 0.0479 (0.0634)  time: 3.8869  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2730/3449]  eta: 0:45:27  lr: 0.000100  loss: 0.0487 (0.0634)  time: 3.8896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2740/3449]  eta: 0:44:49  lr: 0.000100  loss: 0.0634 (0.0634)  time: 3.8392  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2750/3449]  eta: 0:44:11  lr: 0.000100  loss: 0.0630 (0.0634)  time: 3.7651  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2760/3449]  eta: 0:43:34  lr: 0.000100  loss: 0.0631 (0.0634)  time: 3.8028  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2770/3449]  eta: 0:42:56  lr: 0.000100  loss: 0.0631 (0.0635)  time: 3.8267  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2780/3449]  eta: 0:42:18  lr: 0.000100  loss: 0.0593 (0.0635)  time: 3.8008  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2790/3449]  eta: 0:41:40  lr: 0.000100  loss: 0.0548 (0.0634)  time: 3.7784  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2800/3449]  eta: 0:41:02  lr: 0.000100  loss: 0.0396 (0.0633)  time: 3.7345  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2810/3449]  eta: 0:40:24  lr: 0.000100  loss: 0.0408 (0.0633)  time: 3.7746  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2820/3449]  eta: 0:39:46  lr: 0.000100  loss: 0.0605 (0.0633)  time: 3.8139  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2830/3449]  eta: 0:39:08  lr: 0.000100  loss: 0.0646 (0.0634)  time: 3.7818  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2840/3449]  eta: 0:38:30  lr: 0.000100  loss: 0.0879 (0.0634)  time: 3.8047  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2850/3449]  eta: 0:37:52  lr: 0.000100  loss: 0.0545 (0.0634)  time: 3.8100  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2860/3449]  eta: 0:37:14  lr: 0.000100  loss: 0.0464 (0.0634)  time: 3.8233  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2870/3449]  eta: 0:36:36  lr: 0.000100  loss: 0.0481 (0.0633)  time: 3.8095  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2880/3449]  eta: 0:35:58  lr: 0.000100  loss: 0.0629 (0.0634)  time: 3.7929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2890/3449]  eta: 0:35:21  lr: 0.000100  loss: 0.0477 (0.0633)  time: 3.8425  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2900/3449]  eta: 0:34:42  lr: 0.000100  loss: 0.0353 (0.0632)  time: 3.7921  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2910/3449]  eta: 0:34:04  lr: 0.000100  loss: 0.0590 (0.0632)  time: 3.7440  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2920/3449]  eta: 0:33:26  lr: 0.000100  loss: 0.0585 (0.0632)  time: 3.7460  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2930/3449]  eta: 0:32:48  lr: 0.000100  loss: 0.0565 (0.0632)  time: 3.7693  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2940/3449]  eta: 0:32:11  lr: 0.000100  loss: 0.0763 (0.0633)  time: 3.8007  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2950/3449]  eta: 0:31:33  lr: 0.000100  loss: 0.0898 (0.0634)  time: 3.8357  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [2960/3449]  eta: 0:30:55  lr: 0.000100  loss: 0.0898 (0.0635)  time: 3.8479  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2970/3449]  eta: 0:30:17  lr: 0.000100  loss: 0.0760 (0.0635)  time: 3.8673  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2980/3449]  eta: 0:29:39  lr: 0.000100  loss: 0.0456 (0.0634)  time: 3.7932  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [2990/3449]  eta: 0:29:01  lr: 0.000100  loss: 0.0482 (0.0634)  time: 3.7433  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3000/3449]  eta: 0:28:23  lr: 0.000100  loss: 0.0582 (0.0634)  time: 3.8335  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [3010/3449]  eta: 0:27:45  lr: 0.000100  loss: 0.0758 (0.0634)  time: 3.7307  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3020/3449]  eta: 0:27:07  lr: 0.000100  loss: 0.0507 (0.0634)  time: 3.7483  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3030/3449]  eta: 0:26:29  lr: 0.000100  loss: 0.0393 (0.0633)  time: 3.8430  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3040/3449]  eta: 0:25:51  lr: 0.000100  loss: 0.0343 (0.0632)  time: 3.8255  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [3050/3449]  eta: 0:25:14  lr: 0.000100  loss: 0.0325 (0.0631)  time: 3.8871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [3060/3449]  eta: 0:24:36  lr: 0.000100  loss: 0.0374 (0.0631)  time: 3.7969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3070/3449]  eta: 0:23:57  lr: 0.000100  loss: 0.0526 (0.0630)  time: 3.6692  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3080/3449]  eta: 0:23:19  lr: 0.000100  loss: 0.0586 (0.0630)  time: 3.6401  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3090/3449]  eta: 0:22:41  lr: 0.000100  loss: 0.0618 (0.0630)  time: 3.6318  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3100/3449]  eta: 0:22:03  lr: 0.000100  loss: 0.0554 (0.0630)  time: 3.7250  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3110/3449]  eta: 0:21:25  lr: 0.000100  loss: 0.0609 (0.0631)  time: 3.7609  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [3120/3449]  eta: 0:20:47  lr: 0.000100  loss: 0.1091 (0.0633)  time: 3.7532  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3130/3449]  eta: 0:20:09  lr: 0.000100  loss: 0.1291 (0.0635)  time: 3.8042  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3140/3449]  eta: 0:19:32  lr: 0.000100  loss: 0.1291 (0.0637)  time: 3.8458  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3150/3449]  eta: 0:18:54  lr: 0.000100  loss: 0.1325 (0.0639)  time: 3.8238  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3160/3449]  eta: 0:18:16  lr: 0.000100  loss: 0.0888 (0.0639)  time: 3.8127  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3170/3449]  eta: 0:17:38  lr: 0.000100  loss: 0.0698 (0.0639)  time: 3.8765  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3180/3449]  eta: 0:17:00  lr: 0.000100  loss: 0.0685 (0.0639)  time: 3.8520  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3190/3449]  eta: 0:16:22  lr: 0.000100  loss: 0.0673 (0.0639)  time: 3.8219  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3200/3449]  eta: 0:15:44  lr: 0.000100  loss: 0.0595 (0.0639)  time: 3.7972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3210/3449]  eta: 0:15:06  lr: 0.000100  loss: 0.0427 (0.0638)  time: 3.7407  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3220/3449]  eta: 0:14:28  lr: 0.000100  loss: 0.0417 (0.0638)  time: 3.7096  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3230/3449]  eta: 0:13:50  lr: 0.000100  loss: 0.0529 (0.0637)  time: 3.7706  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [3240/3449]  eta: 0:13:12  lr: 0.000100  loss: 0.0617 (0.0637)  time: 3.8270  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [3250/3449]  eta: 0:12:34  lr: 0.000100  loss: 0.0561 (0.0637)  time: 3.7592  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [3260/3449]  eta: 0:11:56  lr: 0.000100  loss: 0.0557 (0.0637)  time: 3.7769  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3270/3449]  eta: 0:11:18  lr: 0.000100  loss: 0.0515 (0.0636)  time: 3.7949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3280/3449]  eta: 0:10:41  lr: 0.000100  loss: 0.0563 (0.0636)  time: 3.7964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3290/3449]  eta: 0:10:03  lr: 0.000100  loss: 0.0650 (0.0636)  time: 3.8537  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3300/3449]  eta: 0:09:25  lr: 0.000100  loss: 0.0780 (0.0637)  time: 3.8147  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [3310/3449]  eta: 0:08:47  lr: 0.000100  loss: 0.0809 (0.0638)  time: 3.7605  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3320/3449]  eta: 0:08:09  lr: 0.000100  loss: 0.1085 (0.0640)  time: 3.7598  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [3330/3449]  eta: 0:07:31  lr: 0.000100  loss: 0.1169 (0.0642)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3340/3449]  eta: 0:06:53  lr: 0.000100  loss: 0.1114 (0.0643)  time: 3.7613  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3350/3449]  eta: 0:06:15  lr: 0.000100  loss: 0.0741 (0.0643)  time: 3.7751  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3360/3449]  eta: 0:05:37  lr: 0.000100  loss: 0.0494 (0.0643)  time: 3.7649  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3370/3449]  eta: 0:04:59  lr: 0.000100  loss: 0.0495 (0.0642)  time: 3.7048  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3380/3449]  eta: 0:04:21  lr: 0.000100  loss: 0.0454 (0.0642)  time: 3.7530  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3390/3449]  eta: 0:03:43  lr: 0.000100  loss: 0.0392 (0.0641)  time: 3.7882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3400/3449]  eta: 0:03:05  lr: 0.000100  loss: 0.0321 (0.0640)  time: 3.7815  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3410/3449]  eta: 0:02:27  lr: 0.000100  loss: 0.0506 (0.0640)  time: 3.8190  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [3420/3449]  eta: 0:01:49  lr: 0.000100  loss: 0.0696 (0.0641)  time: 3.8177  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [3430/3449]  eta: 0:01:12  lr: 0.000100  loss: 0.0715 (0.0641)  time: 3.7981  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:18]  [3440/3449]  eta: 0:00:34  lr: 0.000100  loss: 0.0658 (0.0641)  time: 3.8016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0633 (0.0641)  time: 3.7811  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:18] Total time: 3:38:01 (3.7928 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0633 (0.0641)\n",
      "Valid: [epoch:18]  [ 0/14]  eta: 0:04:21  loss: 0.0411 (0.0411)  time: 18.6434  data: 0.4673  max mem: 34968\n",
      "Valid: [epoch:18]  [13/14]  eta: 0:00:18  loss: 0.0377 (0.0384)  time: 18.2318  data: 0.0335  max mem: 34968\n",
      "Valid: [epoch:18] Total time: 0:04:15 (18.2416 s / it)\n",
      "Averaged stats: loss: 0.0377 (0.0384)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_18_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.038%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:19]  [   0/3449]  eta: 5:10:37  lr: 0.000100  loss: 0.0592 (0.0592)  time: 5.4038  data: 1.1375  max mem: 34968\n",
      "Train: [epoch:19]  [  10/3449]  eta: 3:37:03  lr: 0.000100  loss: 0.0931 (0.0793)  time: 3.7869  data: 0.1035  max mem: 34968\n",
      "Train: [epoch:19]  [  20/3449]  eta: 3:37:53  lr: 0.000100  loss: 0.0752 (0.0750)  time: 3.7331  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [  30/3449]  eta: 3:36:12  lr: 0.000100  loss: 0.0596 (0.0674)  time: 3.7982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [  40/3449]  eta: 3:33:40  lr: 0.000100  loss: 0.0548 (0.0660)  time: 3.7066  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [  50/3449]  eta: 3:33:11  lr: 0.000100  loss: 0.0585 (0.0656)  time: 3.7156  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [  60/3449]  eta: 3:31:22  lr: 0.000100  loss: 0.0685 (0.0700)  time: 3.7038  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [  70/3449]  eta: 3:31:08  lr: 0.000100  loss: 0.0854 (0.0722)  time: 3.7128  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [  80/3449]  eta: 3:31:02  lr: 0.000100  loss: 0.0716 (0.0721)  time: 3.8090  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [  90/3449]  eta: 3:30:34  lr: 0.000100  loss: 0.0718 (0.0769)  time: 3.8050  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 100/3449]  eta: 3:30:32  lr: 0.000100  loss: 0.0695 (0.0754)  time: 3.8255  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 110/3449]  eta: 3:30:20  lr: 0.000100  loss: 0.0591 (0.0742)  time: 3.8633  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 120/3449]  eta: 3:29:55  lr: 0.000100  loss: 0.0651 (0.0742)  time: 3.8427  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 130/3449]  eta: 3:29:45  lr: 0.000100  loss: 0.0698 (0.0739)  time: 3.8589  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 140/3449]  eta: 3:29:03  lr: 0.000100  loss: 0.0698 (0.0735)  time: 3.8330  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 150/3449]  eta: 3:28:20  lr: 0.000100  loss: 0.0702 (0.0729)  time: 3.7706  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [ 160/3449]  eta: 3:27:33  lr: 0.000100  loss: 0.0607 (0.0718)  time: 3.7558  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 170/3449]  eta: 3:27:11  lr: 0.000100  loss: 0.0552 (0.0718)  time: 3.8087  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 180/3449]  eta: 3:26:56  lr: 0.000100  loss: 0.0849 (0.0727)  time: 3.8942  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 190/3449]  eta: 3:26:08  lr: 0.000100  loss: 0.0746 (0.0729)  time: 3.8291  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 200/3449]  eta: 3:25:44  lr: 0.000100  loss: 0.0719 (0.0727)  time: 3.8095  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 210/3449]  eta: 3:25:14  lr: 0.000100  loss: 0.0714 (0.0724)  time: 3.8665  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 220/3449]  eta: 3:24:25  lr: 0.000100  loss: 0.0450 (0.0708)  time: 3.7902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 230/3449]  eta: 3:23:47  lr: 0.000100  loss: 0.0462 (0.0709)  time: 3.7616  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 240/3449]  eta: 3:23:35  lr: 0.000100  loss: 0.0730 (0.0709)  time: 3.8951  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 250/3449]  eta: 3:22:47  lr: 0.000100  loss: 0.0748 (0.0713)  time: 3.8597  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 260/3449]  eta: 3:22:18  lr: 0.000100  loss: 0.0760 (0.0714)  time: 3.8049  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 270/3449]  eta: 3:21:36  lr: 0.000100  loss: 0.0705 (0.0712)  time: 3.8256  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 280/3449]  eta: 3:20:43  lr: 0.000100  loss: 0.0703 (0.0712)  time: 3.7217  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 290/3449]  eta: 3:20:01  lr: 0.000100  loss: 0.0729 (0.0713)  time: 3.7206  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 300/3449]  eta: 3:19:17  lr: 0.000100  loss: 0.0653 (0.0711)  time: 3.7546  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 310/3449]  eta: 3:18:37  lr: 0.000100  loss: 0.0653 (0.0710)  time: 3.7587  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 320/3449]  eta: 3:18:05  lr: 0.000100  loss: 0.0702 (0.0719)  time: 3.8160  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 330/3449]  eta: 3:17:25  lr: 0.000100  loss: 0.1041 (0.0733)  time: 3.8141  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 340/3449]  eta: 3:16:52  lr: 0.000100  loss: 0.0764 (0.0728)  time: 3.8139  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 350/3449]  eta: 3:16:15  lr: 0.000100  loss: 0.0480 (0.0722)  time: 3.8349  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 360/3449]  eta: 3:15:36  lr: 0.000100  loss: 0.0454 (0.0714)  time: 3.7995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 370/3449]  eta: 3:14:59  lr: 0.000100  loss: 0.0394 (0.0709)  time: 3.7994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 380/3449]  eta: 3:14:21  lr: 0.000100  loss: 0.0660 (0.0707)  time: 3.8078  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 390/3449]  eta: 3:13:36  lr: 0.000100  loss: 0.0660 (0.0705)  time: 3.7527  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 400/3449]  eta: 3:13:02  lr: 0.000100  loss: 0.0499 (0.0699)  time: 3.7788  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 410/3449]  eta: 3:12:21  lr: 0.000100  loss: 0.0449 (0.0695)  time: 3.8027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 420/3449]  eta: 3:11:41  lr: 0.000100  loss: 0.0594 (0.0695)  time: 3.7658  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 430/3449]  eta: 3:11:14  lr: 0.000100  loss: 0.0785 (0.0701)  time: 3.8654  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 440/3449]  eta: 3:10:33  lr: 0.000100  loss: 0.1106 (0.0712)  time: 3.8562  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 450/3449]  eta: 3:09:59  lr: 0.000100  loss: 0.1106 (0.0715)  time: 3.8092  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 460/3449]  eta: 3:09:23  lr: 0.000100  loss: 0.0691 (0.0715)  time: 3.8458  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 470/3449]  eta: 3:08:45  lr: 0.000100  loss: 0.0773 (0.0728)  time: 3.8186  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 480/3449]  eta: 3:08:13  lr: 0.000100  loss: 0.1172 (0.0737)  time: 3.8506  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 490/3449]  eta: 3:07:27  lr: 0.000100  loss: 0.0778 (0.0736)  time: 3.7806  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 500/3449]  eta: 3:06:44  lr: 0.000100  loss: 0.0680 (0.0735)  time: 3.6942  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 510/3449]  eta: 3:06:13  lr: 0.000100  loss: 0.0680 (0.0733)  time: 3.8177  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 520/3449]  eta: 3:05:39  lr: 0.000100  loss: 0.0695 (0.0732)  time: 3.8977  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 530/3449]  eta: 3:05:01  lr: 0.000100  loss: 0.0634 (0.0729)  time: 3.8393  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 540/3449]  eta: 3:04:20  lr: 0.000100  loss: 0.0575 (0.0728)  time: 3.7727  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 550/3449]  eta: 3:03:45  lr: 0.000100  loss: 0.0898 (0.0735)  time: 3.8045  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 560/3449]  eta: 3:03:05  lr: 0.000100  loss: 0.1109 (0.0742)  time: 3.8140  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 570/3449]  eta: 3:02:29  lr: 0.000100  loss: 0.0864 (0.0742)  time: 3.8069  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 580/3449]  eta: 3:01:48  lr: 0.000100  loss: 0.0732 (0.0740)  time: 3.7921  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 590/3449]  eta: 3:01:05  lr: 0.000100  loss: 0.0654 (0.0738)  time: 3.7224  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 600/3449]  eta: 3:00:26  lr: 0.000100  loss: 0.0689 (0.0739)  time: 3.7331  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 610/3449]  eta: 2:59:47  lr: 0.000100  loss: 0.0683 (0.0738)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 620/3449]  eta: 2:59:09  lr: 0.000100  loss: 0.0732 (0.0739)  time: 3.7889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 630/3449]  eta: 2:58:28  lr: 0.000100  loss: 0.0732 (0.0737)  time: 3.7683  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 640/3449]  eta: 2:57:48  lr: 0.000100  loss: 0.0538 (0.0734)  time: 3.7456  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 650/3449]  eta: 2:57:07  lr: 0.000100  loss: 0.0419 (0.0728)  time: 3.7454  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 660/3449]  eta: 2:56:29  lr: 0.000100  loss: 0.0443 (0.0726)  time: 3.7610  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 670/3449]  eta: 2:55:50  lr: 0.000100  loss: 0.0445 (0.0721)  time: 3.7863  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 680/3449]  eta: 2:55:06  lr: 0.000100  loss: 0.0323 (0.0715)  time: 3.7156  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 690/3449]  eta: 2:54:31  lr: 0.000100  loss: 0.0335 (0.0711)  time: 3.7519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 700/3449]  eta: 2:53:51  lr: 0.000100  loss: 0.0551 (0.0710)  time: 3.8027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 710/3449]  eta: 2:53:15  lr: 0.000100  loss: 0.0681 (0.0710)  time: 3.8000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 720/3449]  eta: 2:52:40  lr: 0.000100  loss: 0.0799 (0.0713)  time: 3.8639  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 730/3449]  eta: 2:52:01  lr: 0.000100  loss: 0.0718 (0.0712)  time: 3.8193  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 740/3449]  eta: 2:51:22  lr: 0.000100  loss: 0.0640 (0.0710)  time: 3.7663  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 750/3449]  eta: 2:50:41  lr: 0.000100  loss: 0.0538 (0.0706)  time: 3.7370  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 760/3449]  eta: 2:50:10  lr: 0.000100  loss: 0.0347 (0.0701)  time: 3.8552  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 770/3449]  eta: 2:49:31  lr: 0.000100  loss: 0.0319 (0.0697)  time: 3.8764  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 780/3449]  eta: 2:48:50  lr: 0.000100  loss: 0.0338 (0.0692)  time: 3.7353  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 790/3449]  eta: 2:48:11  lr: 0.000100  loss: 0.0366 (0.0690)  time: 3.7360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 800/3449]  eta: 2:47:32  lr: 0.000100  loss: 0.0553 (0.0689)  time: 3.7654  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 810/3449]  eta: 2:46:55  lr: 0.000100  loss: 0.0663 (0.0689)  time: 3.7989  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [ 820/3449]  eta: 2:46:15  lr: 0.000100  loss: 0.0668 (0.0689)  time: 3.7762  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 830/3449]  eta: 2:45:36  lr: 0.000100  loss: 0.0542 (0.0686)  time: 3.7410  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 840/3449]  eta: 2:44:58  lr: 0.000100  loss: 0.0410 (0.0682)  time: 3.7768  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 850/3449]  eta: 2:44:19  lr: 0.000100  loss: 0.0385 (0.0679)  time: 3.7856  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 860/3449]  eta: 2:43:43  lr: 0.000100  loss: 0.0411 (0.0676)  time: 3.8052  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 870/3449]  eta: 2:43:06  lr: 0.000100  loss: 0.0470 (0.0674)  time: 3.8283  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 880/3449]  eta: 2:42:25  lr: 0.000100  loss: 0.0530 (0.0673)  time: 3.7656  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 890/3449]  eta: 2:41:49  lr: 0.000100  loss: 0.0586 (0.0671)  time: 3.7896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 900/3449]  eta: 2:41:13  lr: 0.000100  loss: 0.0464 (0.0670)  time: 3.8451  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 910/3449]  eta: 2:40:32  lr: 0.000100  loss: 0.0636 (0.0670)  time: 3.7686  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 920/3449]  eta: 2:39:50  lr: 0.000100  loss: 0.0689 (0.0671)  time: 3.6791  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 930/3449]  eta: 2:39:15  lr: 0.000100  loss: 0.1041 (0.0675)  time: 3.7768  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [ 940/3449]  eta: 2:38:37  lr: 0.000100  loss: 0.1103 (0.0679)  time: 3.8350  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 950/3449]  eta: 2:38:01  lr: 0.000100  loss: 0.1099 (0.0683)  time: 3.8187  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 960/3449]  eta: 2:37:21  lr: 0.000100  loss: 0.1065 (0.0687)  time: 3.8057  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 970/3449]  eta: 2:36:43  lr: 0.000100  loss: 0.1064 (0.0693)  time: 3.7518  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 980/3449]  eta: 2:36:05  lr: 0.000100  loss: 0.0817 (0.0694)  time: 3.7762  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [ 990/3449]  eta: 2:35:28  lr: 0.000100  loss: 0.0552 (0.0692)  time: 3.8074  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1000/3449]  eta: 2:34:49  lr: 0.000100  loss: 0.0530 (0.0692)  time: 3.8048  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1010/3449]  eta: 2:34:12  lr: 0.000100  loss: 0.0614 (0.0690)  time: 3.7990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1020/3449]  eta: 2:33:38  lr: 0.000100  loss: 0.0547 (0.0689)  time: 3.8887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1030/3449]  eta: 2:32:58  lr: 0.000100  loss: 0.0614 (0.0691)  time: 3.8453  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1040/3449]  eta: 2:32:17  lr: 0.000100  loss: 0.0917 (0.0694)  time: 3.6779  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1050/3449]  eta: 2:31:41  lr: 0.000100  loss: 0.0740 (0.0693)  time: 3.7679  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1060/3449]  eta: 2:31:01  lr: 0.000100  loss: 0.0648 (0.0693)  time: 3.7941  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1070/3449]  eta: 2:30:26  lr: 0.000100  loss: 0.0716 (0.0695)  time: 3.8061  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1080/3449]  eta: 2:29:50  lr: 0.000100  loss: 0.0594 (0.0693)  time: 3.9073  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1090/3449]  eta: 2:29:11  lr: 0.000100  loss: 0.0407 (0.0690)  time: 3.8139  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1100/3449]  eta: 2:28:34  lr: 0.000100  loss: 0.0336 (0.0687)  time: 3.7834  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1110/3449]  eta: 2:27:54  lr: 0.000100  loss: 0.0425 (0.0686)  time: 3.7795  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1120/3449]  eta: 2:27:16  lr: 0.000100  loss: 0.0486 (0.0684)  time: 3.7526  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1130/3449]  eta: 2:26:38  lr: 0.000100  loss: 0.0367 (0.0681)  time: 3.7767  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1140/3449]  eta: 2:26:00  lr: 0.000100  loss: 0.0312 (0.0677)  time: 3.7791  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1150/3449]  eta: 2:25:22  lr: 0.000100  loss: 0.0312 (0.0675)  time: 3.7924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1160/3449]  eta: 2:24:45  lr: 0.000100  loss: 0.0378 (0.0672)  time: 3.8204  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1170/3449]  eta: 2:24:04  lr: 0.000100  loss: 0.0416 (0.0672)  time: 3.7430  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1180/3449]  eta: 2:23:26  lr: 0.000100  loss: 0.0406 (0.0670)  time: 3.7073  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1190/3449]  eta: 2:22:49  lr: 0.000100  loss: 0.0348 (0.0667)  time: 3.8136  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1200/3449]  eta: 2:22:09  lr: 0.000100  loss: 0.0363 (0.0664)  time: 3.7727  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1210/3449]  eta: 2:21:31  lr: 0.000100  loss: 0.0435 (0.0665)  time: 3.7276  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1220/3449]  eta: 2:20:53  lr: 0.000100  loss: 0.0591 (0.0663)  time: 3.8016  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1230/3449]  eta: 2:20:16  lr: 0.000100  loss: 0.0383 (0.0662)  time: 3.8151  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1240/3449]  eta: 2:19:35  lr: 0.000100  loss: 0.0782 (0.0669)  time: 3.7226  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1250/3449]  eta: 2:18:57  lr: 0.000100  loss: 0.1086 (0.0672)  time: 3.7263  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1260/3449]  eta: 2:18:20  lr: 0.000100  loss: 0.0730 (0.0671)  time: 3.8030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1270/3449]  eta: 2:17:42  lr: 0.000100  loss: 0.0477 (0.0669)  time: 3.7970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1280/3449]  eta: 2:17:02  lr: 0.000100  loss: 0.0503 (0.0669)  time: 3.7258  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1290/3449]  eta: 2:16:24  lr: 0.000100  loss: 0.0722 (0.0670)  time: 3.7426  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1300/3449]  eta: 2:15:47  lr: 0.000100  loss: 0.0666 (0.0670)  time: 3.8231  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1310/3449]  eta: 2:15:08  lr: 0.000100  loss: 0.0619 (0.0670)  time: 3.7774  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1320/3449]  eta: 2:14:31  lr: 0.000100  loss: 0.0685 (0.0671)  time: 3.7817  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1330/3449]  eta: 2:13:53  lr: 0.000100  loss: 0.0563 (0.0670)  time: 3.8208  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1340/3449]  eta: 2:13:17  lr: 0.000100  loss: 0.0674 (0.0671)  time: 3.8519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1350/3449]  eta: 2:12:40  lr: 0.000100  loss: 0.0932 (0.0674)  time: 3.8783  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1360/3449]  eta: 2:12:03  lr: 0.000100  loss: 0.1112 (0.0678)  time: 3.8537  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1370/3449]  eta: 2:11:23  lr: 0.000100  loss: 0.0827 (0.0679)  time: 3.7539  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1380/3449]  eta: 2:10:46  lr: 0.000100  loss: 0.0715 (0.0680)  time: 3.7453  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1390/3449]  eta: 2:10:07  lr: 0.000100  loss: 0.0935 (0.0686)  time: 3.7809  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1400/3449]  eta: 2:09:31  lr: 0.000100  loss: 0.1508 (0.0692)  time: 3.8379  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1410/3449]  eta: 2:08:52  lr: 0.000100  loss: 0.1018 (0.0693)  time: 3.8124  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1420/3449]  eta: 2:08:10  lr: 0.000100  loss: 0.0780 (0.0693)  time: 3.6242  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1430/3449]  eta: 2:07:31  lr: 0.000100  loss: 0.0707 (0.0694)  time: 3.6317  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1440/3449]  eta: 2:06:54  lr: 0.000100  loss: 0.0707 (0.0694)  time: 3.7546  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1450/3449]  eta: 2:06:14  lr: 0.000100  loss: 0.0807 (0.0695)  time: 3.7439  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1460/3449]  eta: 2:05:37  lr: 0.000100  loss: 0.0598 (0.0694)  time: 3.7568  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1470/3449]  eta: 2:04:59  lr: 0.000100  loss: 0.0463 (0.0692)  time: 3.8273  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [1480/3449]  eta: 2:04:23  lr: 0.000100  loss: 0.0357 (0.0690)  time: 3.8767  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1490/3449]  eta: 2:03:44  lr: 0.000100  loss: 0.0363 (0.0688)  time: 3.7878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1500/3449]  eta: 2:03:07  lr: 0.000100  loss: 0.0378 (0.0686)  time: 3.7633  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1510/3449]  eta: 2:02:28  lr: 0.000100  loss: 0.0414 (0.0685)  time: 3.8034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1520/3449]  eta: 2:01:51  lr: 0.000100  loss: 0.0689 (0.0687)  time: 3.7850  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1530/3449]  eta: 2:01:13  lr: 0.000100  loss: 0.0785 (0.0687)  time: 3.8014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1540/3449]  eta: 2:00:35  lr: 0.000100  loss: 0.0694 (0.0687)  time: 3.7912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1550/3449]  eta: 1:59:58  lr: 0.000100  loss: 0.0617 (0.0687)  time: 3.8420  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1560/3449]  eta: 1:59:21  lr: 0.000100  loss: 0.0617 (0.0687)  time: 3.8424  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1570/3449]  eta: 1:58:42  lr: 0.000100  loss: 0.0597 (0.0687)  time: 3.7977  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1580/3449]  eta: 1:58:05  lr: 0.000100  loss: 0.0375 (0.0685)  time: 3.8040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1590/3449]  eta: 1:57:27  lr: 0.000100  loss: 0.0353 (0.0683)  time: 3.7990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1600/3449]  eta: 1:56:48  lr: 0.000100  loss: 0.0425 (0.0682)  time: 3.7344  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1610/3449]  eta: 1:56:11  lr: 0.000100  loss: 0.0544 (0.0682)  time: 3.7707  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1620/3449]  eta: 1:55:34  lr: 0.000100  loss: 0.0874 (0.0684)  time: 3.8786  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1630/3449]  eta: 1:54:55  lr: 0.000100  loss: 0.0730 (0.0684)  time: 3.7956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1640/3449]  eta: 1:54:17  lr: 0.000100  loss: 0.0714 (0.0684)  time: 3.7125  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1650/3449]  eta: 1:53:38  lr: 0.000100  loss: 0.0715 (0.0683)  time: 3.7367  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1660/3449]  eta: 1:53:01  lr: 0.000100  loss: 0.0571 (0.0683)  time: 3.8100  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1670/3449]  eta: 1:52:23  lr: 0.000100  loss: 0.0644 (0.0683)  time: 3.8311  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1680/3449]  eta: 1:51:44  lr: 0.000100  loss: 0.0780 (0.0684)  time: 3.7498  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1690/3449]  eta: 1:51:06  lr: 0.000100  loss: 0.0723 (0.0683)  time: 3.7541  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1700/3449]  eta: 1:50:29  lr: 0.000100  loss: 0.0607 (0.0683)  time: 3.7837  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1710/3449]  eta: 1:49:50  lr: 0.000100  loss: 0.0601 (0.0682)  time: 3.7629  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1720/3449]  eta: 1:49:12  lr: 0.000100  loss: 0.0479 (0.0681)  time: 3.7496  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1730/3449]  eta: 1:48:34  lr: 0.000100  loss: 0.0433 (0.0679)  time: 3.7558  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1740/3449]  eta: 1:47:55  lr: 0.000100  loss: 0.0375 (0.0677)  time: 3.7535  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1750/3449]  eta: 1:47:17  lr: 0.000100  loss: 0.0319 (0.0675)  time: 3.7613  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1760/3449]  eta: 1:46:40  lr: 0.000100  loss: 0.0347 (0.0674)  time: 3.7872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1770/3449]  eta: 1:46:02  lr: 0.000100  loss: 0.0498 (0.0674)  time: 3.8187  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1780/3449]  eta: 1:45:24  lr: 0.000100  loss: 0.0647 (0.0674)  time: 3.7787  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1790/3449]  eta: 1:44:46  lr: 0.000100  loss: 0.0680 (0.0674)  time: 3.7553  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1800/3449]  eta: 1:44:08  lr: 0.000100  loss: 0.0715 (0.0675)  time: 3.7968  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1810/3449]  eta: 1:43:30  lr: 0.000100  loss: 0.0652 (0.0675)  time: 3.7903  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1820/3449]  eta: 1:42:51  lr: 0.000100  loss: 0.0852 (0.0678)  time: 3.7505  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1830/3449]  eta: 1:42:14  lr: 0.000100  loss: 0.0952 (0.0678)  time: 3.7910  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1840/3449]  eta: 1:41:36  lr: 0.000100  loss: 0.0625 (0.0678)  time: 3.8062  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1850/3449]  eta: 1:40:58  lr: 0.000100  loss: 0.0658 (0.0678)  time: 3.7519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1860/3449]  eta: 1:40:19  lr: 0.000100  loss: 0.0704 (0.0678)  time: 3.7024  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1870/3449]  eta: 1:39:40  lr: 0.000100  loss: 0.0704 (0.0678)  time: 3.7062  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1880/3449]  eta: 1:39:03  lr: 0.000100  loss: 0.0604 (0.0677)  time: 3.7815  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1890/3449]  eta: 1:38:25  lr: 0.000100  loss: 0.0583 (0.0677)  time: 3.7931  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1900/3449]  eta: 1:37:48  lr: 0.000100  loss: 0.0595 (0.0677)  time: 3.8183  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1910/3449]  eta: 1:37:10  lr: 0.000100  loss: 0.0566 (0.0676)  time: 3.8291  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1920/3449]  eta: 1:36:33  lr: 0.000100  loss: 0.0599 (0.0677)  time: 3.8553  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1930/3449]  eta: 1:35:54  lr: 0.000100  loss: 0.0763 (0.0677)  time: 3.8152  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1940/3449]  eta: 1:35:17  lr: 0.000100  loss: 0.0722 (0.0677)  time: 3.7602  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [1950/3449]  eta: 1:34:38  lr: 0.000100  loss: 0.0707 (0.0677)  time: 3.7546  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1960/3449]  eta: 1:34:00  lr: 0.000100  loss: 0.0707 (0.0678)  time: 3.7300  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1970/3449]  eta: 1:33:22  lr: 0.000100  loss: 0.0590 (0.0677)  time: 3.7943  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1980/3449]  eta: 1:32:44  lr: 0.000100  loss: 0.0527 (0.0677)  time: 3.8053  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [1990/3449]  eta: 1:32:08  lr: 0.000100  loss: 0.0808 (0.0678)  time: 3.8693  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2000/3449]  eta: 1:31:30  lr: 0.000100  loss: 0.0808 (0.0678)  time: 3.8682  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2010/3449]  eta: 1:30:52  lr: 0.000100  loss: 0.0784 (0.0679)  time: 3.7791  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2020/3449]  eta: 1:30:14  lr: 0.000100  loss: 0.1150 (0.0683)  time: 3.7839  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2030/3449]  eta: 1:29:37  lr: 0.000100  loss: 0.1141 (0.0684)  time: 3.8445  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2040/3449]  eta: 1:28:59  lr: 0.000100  loss: 0.0778 (0.0684)  time: 3.8506  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2050/3449]  eta: 1:28:22  lr: 0.000100  loss: 0.0636 (0.0683)  time: 3.8270  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2060/3449]  eta: 1:27:43  lr: 0.000100  loss: 0.0426 (0.0682)  time: 3.7701  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2070/3449]  eta: 1:27:06  lr: 0.000100  loss: 0.0353 (0.0680)  time: 3.7835  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2080/3449]  eta: 1:26:28  lr: 0.000100  loss: 0.0455 (0.0680)  time: 3.8468  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2090/3449]  eta: 1:25:49  lr: 0.000100  loss: 0.0582 (0.0680)  time: 3.7488  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2100/3449]  eta: 1:25:12  lr: 0.000100  loss: 0.0670 (0.0680)  time: 3.7696  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2110/3449]  eta: 1:24:34  lr: 0.000100  loss: 0.0689 (0.0680)  time: 3.8349  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2120/3449]  eta: 1:23:57  lr: 0.000100  loss: 0.0722 (0.0680)  time: 3.8550  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2130/3449]  eta: 1:23:18  lr: 0.000100  loss: 0.0699 (0.0680)  time: 3.8003  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [2140/3449]  eta: 1:22:41  lr: 0.000100  loss: 0.0653 (0.0680)  time: 3.7527  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2150/3449]  eta: 1:22:03  lr: 0.000100  loss: 0.0663 (0.0680)  time: 3.8001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2160/3449]  eta: 1:21:25  lr: 0.000100  loss: 0.0549 (0.0679)  time: 3.7760  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2170/3449]  eta: 1:20:47  lr: 0.000100  loss: 0.0527 (0.0680)  time: 3.7995  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2180/3449]  eta: 1:20:10  lr: 0.000100  loss: 0.0860 (0.0681)  time: 3.8454  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2190/3449]  eta: 1:19:32  lr: 0.000100  loss: 0.1019 (0.0684)  time: 3.8462  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2200/3449]  eta: 1:18:54  lr: 0.000100  loss: 0.1160 (0.0685)  time: 3.7921  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2210/3449]  eta: 1:18:16  lr: 0.000100  loss: 0.0741 (0.0685)  time: 3.7754  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2220/3449]  eta: 1:17:38  lr: 0.000100  loss: 0.0637 (0.0685)  time: 3.8143  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2230/3449]  eta: 1:17:00  lr: 0.000100  loss: 0.0669 (0.0685)  time: 3.7555  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2240/3449]  eta: 1:16:22  lr: 0.000100  loss: 0.0616 (0.0685)  time: 3.7146  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2250/3449]  eta: 1:15:43  lr: 0.000100  loss: 0.0564 (0.0685)  time: 3.7262  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2260/3449]  eta: 1:15:06  lr: 0.000100  loss: 0.0846 (0.0686)  time: 3.7944  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2270/3449]  eta: 1:14:28  lr: 0.000100  loss: 0.0774 (0.0686)  time: 3.8442  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2280/3449]  eta: 1:13:50  lr: 0.000100  loss: 0.0641 (0.0685)  time: 3.8027  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2290/3449]  eta: 1:13:12  lr: 0.000100  loss: 0.0487 (0.0684)  time: 3.8107  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2300/3449]  eta: 1:12:34  lr: 0.000100  loss: 0.0361 (0.0683)  time: 3.7737  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2310/3449]  eta: 1:11:56  lr: 0.000100  loss: 0.0369 (0.0682)  time: 3.7362  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2320/3449]  eta: 1:11:19  lr: 0.000100  loss: 0.0505 (0.0682)  time: 3.8318  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2330/3449]  eta: 1:10:41  lr: 0.000100  loss: 0.0509 (0.0681)  time: 3.8811  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2340/3449]  eta: 1:10:04  lr: 0.000100  loss: 0.0607 (0.0681)  time: 3.8658  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2350/3449]  eta: 1:09:26  lr: 0.000100  loss: 0.0626 (0.0681)  time: 3.8311  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2360/3449]  eta: 1:08:48  lr: 0.000100  loss: 0.0756 (0.0681)  time: 3.7863  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2370/3449]  eta: 1:08:10  lr: 0.000100  loss: 0.0556 (0.0681)  time: 3.7831  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2380/3449]  eta: 1:07:32  lr: 0.000100  loss: 0.0475 (0.0681)  time: 3.7779  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2390/3449]  eta: 1:06:54  lr: 0.000100  loss: 0.0509 (0.0681)  time: 3.7747  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2400/3449]  eta: 1:06:16  lr: 0.000100  loss: 0.0692 (0.0680)  time: 3.8041  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2410/3449]  eta: 1:05:38  lr: 0.000100  loss: 0.0614 (0.0680)  time: 3.7691  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2420/3449]  eta: 1:05:00  lr: 0.000100  loss: 0.0799 (0.0682)  time: 3.7564  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2430/3449]  eta: 1:04:22  lr: 0.000100  loss: 0.1050 (0.0683)  time: 3.8005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2440/3449]  eta: 1:03:44  lr: 0.000100  loss: 0.0753 (0.0682)  time: 3.7906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2450/3449]  eta: 1:03:07  lr: 0.000100  loss: 0.0359 (0.0681)  time: 3.8275  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2460/3449]  eta: 1:02:29  lr: 0.000100  loss: 0.0304 (0.0680)  time: 3.8040  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2470/3449]  eta: 1:01:51  lr: 0.000100  loss: 0.0531 (0.0680)  time: 3.7968  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2480/3449]  eta: 1:01:13  lr: 0.000100  loss: 0.0588 (0.0680)  time: 3.8338  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2490/3449]  eta: 1:00:35  lr: 0.000100  loss: 0.0695 (0.0680)  time: 3.8106  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2500/3449]  eta: 0:59:58  lr: 0.000100  loss: 0.0958 (0.0681)  time: 3.8493  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2510/3449]  eta: 0:59:20  lr: 0.000100  loss: 0.1076 (0.0683)  time: 3.8602  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2520/3449]  eta: 0:58:42  lr: 0.000100  loss: 0.1142 (0.0684)  time: 3.8178  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2530/3449]  eta: 0:58:04  lr: 0.000100  loss: 0.0610 (0.0684)  time: 3.7864  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2540/3449]  eta: 0:57:26  lr: 0.000100  loss: 0.0543 (0.0683)  time: 3.7826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2550/3449]  eta: 0:56:48  lr: 0.000100  loss: 0.0354 (0.0682)  time: 3.7997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2560/3449]  eta: 0:56:10  lr: 0.000100  loss: 0.0267 (0.0680)  time: 3.7292  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2570/3449]  eta: 0:55:32  lr: 0.000100  loss: 0.0430 (0.0682)  time: 3.6803  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2580/3449]  eta: 0:54:54  lr: 0.000100  loss: 0.0952 (0.0683)  time: 3.7227  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2590/3449]  eta: 0:54:16  lr: 0.000100  loss: 0.0796 (0.0683)  time: 3.7688  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2600/3449]  eta: 0:53:38  lr: 0.000100  loss: 0.0610 (0.0683)  time: 3.8104  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2610/3449]  eta: 0:53:00  lr: 0.000100  loss: 0.0563 (0.0683)  time: 3.8209  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2620/3449]  eta: 0:52:22  lr: 0.000100  loss: 0.0583 (0.0683)  time: 3.7753  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2630/3449]  eta: 0:51:44  lr: 0.000100  loss: 0.0606 (0.0682)  time: 3.7273  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2640/3449]  eta: 0:51:06  lr: 0.000100  loss: 0.0629 (0.0682)  time: 3.7842  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2650/3449]  eta: 0:50:28  lr: 0.000100  loss: 0.0711 (0.0683)  time: 3.8298  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2660/3449]  eta: 0:49:51  lr: 0.000100  loss: 0.0711 (0.0683)  time: 3.8273  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2670/3449]  eta: 0:49:13  lr: 0.000100  loss: 0.0777 (0.0683)  time: 3.8391  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2680/3449]  eta: 0:48:35  lr: 0.000100  loss: 0.0667 (0.0683)  time: 3.8119  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2690/3449]  eta: 0:47:57  lr: 0.000100  loss: 0.0576 (0.0683)  time: 3.8062  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2700/3449]  eta: 0:47:19  lr: 0.000100  loss: 0.0500 (0.0682)  time: 3.8192  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2710/3449]  eta: 0:46:41  lr: 0.000100  loss: 0.0452 (0.0681)  time: 3.7995  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2720/3449]  eta: 0:46:04  lr: 0.000100  loss: 0.0402 (0.0680)  time: 3.8138  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2730/3449]  eta: 0:45:26  lr: 0.000100  loss: 0.0475 (0.0680)  time: 3.8198  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2740/3449]  eta: 0:44:48  lr: 0.000100  loss: 0.0695 (0.0680)  time: 3.7991  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2750/3449]  eta: 0:44:10  lr: 0.000100  loss: 0.0649 (0.0680)  time: 3.7672  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2760/3449]  eta: 0:43:32  lr: 0.000100  loss: 0.0579 (0.0680)  time: 3.7774  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2770/3449]  eta: 0:42:54  lr: 0.000100  loss: 0.0553 (0.0679)  time: 3.7630  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2780/3449]  eta: 0:42:16  lr: 0.000100  loss: 0.0427 (0.0678)  time: 3.7236  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2790/3449]  eta: 0:41:38  lr: 0.000100  loss: 0.0635 (0.0678)  time: 3.7608  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [2800/3449]  eta: 0:41:00  lr: 0.000100  loss: 0.0597 (0.0678)  time: 3.8063  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2810/3449]  eta: 0:40:22  lr: 0.000100  loss: 0.0597 (0.0677)  time: 3.8882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2820/3449]  eta: 0:39:45  lr: 0.000100  loss: 0.0512 (0.0677)  time: 3.8857  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2830/3449]  eta: 0:39:07  lr: 0.000100  loss: 0.0468 (0.0677)  time: 3.8146  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2840/3449]  eta: 0:38:29  lr: 0.000100  loss: 0.0506 (0.0676)  time: 3.7701  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2850/3449]  eta: 0:37:51  lr: 0.000100  loss: 0.0527 (0.0676)  time: 3.7445  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2860/3449]  eta: 0:37:13  lr: 0.000100  loss: 0.0459 (0.0675)  time: 3.7502  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2870/3449]  eta: 0:36:34  lr: 0.000100  loss: 0.0401 (0.0674)  time: 3.6964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2880/3449]  eta: 0:35:57  lr: 0.000100  loss: 0.0401 (0.0673)  time: 3.7855  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2890/3449]  eta: 0:35:19  lr: 0.000100  loss: 0.0409 (0.0672)  time: 3.8361  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2900/3449]  eta: 0:34:41  lr: 0.000100  loss: 0.0388 (0.0671)  time: 3.7719  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2910/3449]  eta: 0:34:03  lr: 0.000100  loss: 0.0321 (0.0670)  time: 3.8342  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2920/3449]  eta: 0:33:25  lr: 0.000100  loss: 0.0372 (0.0669)  time: 3.8262  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2930/3449]  eta: 0:32:47  lr: 0.000100  loss: 0.0781 (0.0671)  time: 3.7721  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2940/3449]  eta: 0:32:09  lr: 0.000100  loss: 0.0617 (0.0670)  time: 3.7721  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2950/3449]  eta: 0:31:31  lr: 0.000100  loss: 0.0398 (0.0669)  time: 3.8174  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2960/3449]  eta: 0:30:54  lr: 0.000100  loss: 0.0305 (0.0668)  time: 3.9027  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2970/3449]  eta: 0:30:16  lr: 0.000100  loss: 0.0344 (0.0667)  time: 3.8661  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [2980/3449]  eta: 0:29:38  lr: 0.000100  loss: 0.0507 (0.0667)  time: 3.7853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [2990/3449]  eta: 0:29:00  lr: 0.000100  loss: 0.0507 (0.0667)  time: 3.8114  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3000/3449]  eta: 0:28:22  lr: 0.000100  loss: 0.0496 (0.0666)  time: 3.8633  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3010/3449]  eta: 0:27:45  lr: 0.000100  loss: 0.0569 (0.0666)  time: 3.9246  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3020/3449]  eta: 0:27:07  lr: 0.000100  loss: 0.0749 (0.0667)  time: 3.8771  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3030/3449]  eta: 0:26:29  lr: 0.000100  loss: 0.0749 (0.0667)  time: 3.7354  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3040/3449]  eta: 0:25:51  lr: 0.000100  loss: 0.0517 (0.0666)  time: 3.7428  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3050/3449]  eta: 0:25:13  lr: 0.000100  loss: 0.0507 (0.0666)  time: 3.7614  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3060/3449]  eta: 0:24:35  lr: 0.000100  loss: 0.0548 (0.0666)  time: 3.7504  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3070/3449]  eta: 0:23:57  lr: 0.000100  loss: 0.0545 (0.0665)  time: 3.8049  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3080/3449]  eta: 0:23:19  lr: 0.000100  loss: 0.0476 (0.0665)  time: 3.8576  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3090/3449]  eta: 0:22:41  lr: 0.000100  loss: 0.0354 (0.0664)  time: 3.8603  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3100/3449]  eta: 0:22:03  lr: 0.000100  loss: 0.0357 (0.0663)  time: 3.7583  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3110/3449]  eta: 0:21:25  lr: 0.000100  loss: 0.0506 (0.0663)  time: 3.7358  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3120/3449]  eta: 0:20:47  lr: 0.000100  loss: 0.0669 (0.0663)  time: 3.7905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3130/3449]  eta: 0:20:09  lr: 0.000100  loss: 0.0551 (0.0663)  time: 3.8017  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3140/3449]  eta: 0:19:31  lr: 0.000100  loss: 0.0679 (0.0663)  time: 3.7899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3150/3449]  eta: 0:18:53  lr: 0.000100  loss: 0.0900 (0.0665)  time: 3.7894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3160/3449]  eta: 0:18:16  lr: 0.000100  loss: 0.0957 (0.0666)  time: 3.8202  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3170/3449]  eta: 0:17:38  lr: 0.000100  loss: 0.1136 (0.0668)  time: 3.8389  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3180/3449]  eta: 0:17:00  lr: 0.000100  loss: 0.1179 (0.0669)  time: 3.8037  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3190/3449]  eta: 0:16:22  lr: 0.000100  loss: 0.1358 (0.0672)  time: 3.7527  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3200/3449]  eta: 0:15:44  lr: 0.000100  loss: 0.1192 (0.0673)  time: 3.7350  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3210/3449]  eta: 0:15:06  lr: 0.000100  loss: 0.0783 (0.0673)  time: 3.8079  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3220/3449]  eta: 0:14:28  lr: 0.000100  loss: 0.0678 (0.0673)  time: 3.8291  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3230/3449]  eta: 0:13:50  lr: 0.000100  loss: 0.0635 (0.0673)  time: 3.7415  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3240/3449]  eta: 0:13:12  lr: 0.000100  loss: 0.0560 (0.0673)  time: 3.7245  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3250/3449]  eta: 0:12:34  lr: 0.000100  loss: 0.0587 (0.0673)  time: 3.8523  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3260/3449]  eta: 0:11:56  lr: 0.000100  loss: 0.0655 (0.0673)  time: 3.8091  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3270/3449]  eta: 0:11:18  lr: 0.000100  loss: 0.0655 (0.0673)  time: 3.7198  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3280/3449]  eta: 0:10:40  lr: 0.000100  loss: 0.0576 (0.0673)  time: 3.7900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3290/3449]  eta: 0:10:03  lr: 0.000100  loss: 0.0747 (0.0674)  time: 3.8822  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3300/3449]  eta: 0:09:25  lr: 0.000100  loss: 0.0969 (0.0674)  time: 3.8889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3310/3449]  eta: 0:08:47  lr: 0.000100  loss: 0.0578 (0.0674)  time: 3.7697  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3320/3449]  eta: 0:08:09  lr: 0.000100  loss: 0.0531 (0.0674)  time: 3.7585  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3330/3449]  eta: 0:07:31  lr: 0.000100  loss: 0.0659 (0.0675)  time: 3.7890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3340/3449]  eta: 0:06:53  lr: 0.000100  loss: 0.1050 (0.0676)  time: 3.8117  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3350/3449]  eta: 0:06:15  lr: 0.000100  loss: 0.0891 (0.0676)  time: 3.8311  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3360/3449]  eta: 0:05:37  lr: 0.000100  loss: 0.0668 (0.0676)  time: 3.7982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3370/3449]  eta: 0:04:59  lr: 0.000100  loss: 0.0567 (0.0676)  time: 3.8525  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3380/3449]  eta: 0:04:21  lr: 0.000100  loss: 0.0652 (0.0676)  time: 3.8727  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3390/3449]  eta: 0:03:43  lr: 0.000100  loss: 0.0647 (0.0676)  time: 3.8183  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3400/3449]  eta: 0:03:05  lr: 0.000100  loss: 0.0510 (0.0675)  time: 3.7685  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3410/3449]  eta: 0:02:27  lr: 0.000100  loss: 0.0396 (0.0674)  time: 3.7847  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3420/3449]  eta: 0:01:50  lr: 0.000100  loss: 0.0354 (0.0673)  time: 3.8250  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3430/3449]  eta: 0:01:12  lr: 0.000100  loss: 0.0510 (0.0673)  time: 3.7935  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:19]  [3440/3449]  eta: 0:00:34  lr: 0.000100  loss: 0.0604 (0.0673)  time: 3.8214  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:19]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0610 (0.0673)  time: 3.8294  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19] Total time: 3:38:04 (3.7936 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0610 (0.0673)\n",
      "Valid: [epoch:19]  [ 0/14]  eta: 0:04:20  loss: 0.0348 (0.0348)  time: 18.6424  data: 0.4421  max mem: 34968\n",
      "Valid: [epoch:19]  [13/14]  eta: 0:00:18  loss: 0.0315 (0.0323)  time: 18.2471  data: 0.0317  max mem: 34968\n",
      "Valid: [epoch:19] Total time: 0:04:15 (18.2558 s / it)\n",
      "Averaged stats: loss: 0.0315 (0.0323)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_19_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.032%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:20]  [   0/3449]  eta: 4:15:54  lr: 0.000100  loss: 0.0499 (0.0499)  time: 4.4520  data: 1.0087  max mem: 34968\n",
      "Train: [epoch:20]  [  10/3449]  eta: 3:41:19  lr: 0.000100  loss: 0.1035 (0.1009)  time: 3.8615  data: 0.0918  max mem: 34968\n",
      "Train: [epoch:20]  [  20/3449]  eta: 3:40:59  lr: 0.000100  loss: 0.1013 (0.1001)  time: 3.8377  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [  30/3449]  eta: 3:39:08  lr: 0.000100  loss: 0.0834 (0.0860)  time: 3.8371  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [  40/3449]  eta: 3:36:13  lr: 0.000100  loss: 0.0450 (0.0759)  time: 3.7412  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [  50/3449]  eta: 3:35:42  lr: 0.000100  loss: 0.0622 (0.0783)  time: 3.7485  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [  60/3449]  eta: 3:34:22  lr: 0.000100  loss: 0.0868 (0.0802)  time: 3.7740  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [  70/3449]  eta: 3:33:30  lr: 0.000100  loss: 0.1134 (0.0850)  time: 3.7494  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [  80/3449]  eta: 3:31:28  lr: 0.000100  loss: 0.0815 (0.0836)  time: 3.6782  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [  90/3449]  eta: 3:30:39  lr: 0.000100  loss: 0.0770 (0.0832)  time: 3.6625  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 100/3449]  eta: 3:29:48  lr: 0.000100  loss: 0.0838 (0.0838)  time: 3.7285  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 110/3449]  eta: 3:29:25  lr: 0.000100  loss: 0.1026 (0.0945)  time: 3.7656  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 120/3449]  eta: 3:28:43  lr: 0.000100  loss: 0.1265 (0.0959)  time: 3.7782  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 130/3449]  eta: 3:27:23  lr: 0.000100  loss: 0.0756 (0.0941)  time: 3.6703  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 140/3449]  eta: 3:26:22  lr: 0.000100  loss: 0.0723 (0.0925)  time: 3.6213  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 150/3449]  eta: 3:25:46  lr: 0.000100  loss: 0.0688 (0.0911)  time: 3.6997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 160/3449]  eta: 3:25:29  lr: 0.000100  loss: 0.0680 (0.0895)  time: 3.7947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 170/3449]  eta: 3:24:57  lr: 0.000100  loss: 0.0661 (0.0878)  time: 3.8096  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 180/3449]  eta: 3:24:34  lr: 0.000100  loss: 0.0685 (0.0869)  time: 3.8044  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 190/3449]  eta: 3:24:17  lr: 0.000100  loss: 0.0704 (0.0858)  time: 3.8526  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 200/3449]  eta: 3:23:47  lr: 0.000100  loss: 0.0524 (0.0838)  time: 3.8433  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 210/3449]  eta: 3:23:23  lr: 0.000100  loss: 0.0520 (0.0827)  time: 3.8300  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 220/3449]  eta: 3:22:48  lr: 0.000100  loss: 0.0668 (0.0820)  time: 3.8171  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 230/3449]  eta: 3:22:11  lr: 0.000100  loss: 0.0738 (0.0823)  time: 3.7802  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 240/3449]  eta: 3:21:35  lr: 0.000100  loss: 0.0771 (0.0822)  time: 3.7777  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 250/3449]  eta: 3:20:51  lr: 0.000100  loss: 0.0683 (0.0818)  time: 3.7509  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 260/3449]  eta: 3:20:17  lr: 0.000100  loss: 0.0754 (0.0815)  time: 3.7580  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 270/3449]  eta: 3:19:43  lr: 0.000100  loss: 0.0532 (0.0803)  time: 3.7977  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 280/3449]  eta: 3:19:13  lr: 0.000100  loss: 0.0487 (0.0791)  time: 3.8188  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 290/3449]  eta: 3:18:30  lr: 0.000100  loss: 0.0463 (0.0782)  time: 3.7816  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 300/3449]  eta: 3:17:55  lr: 0.000100  loss: 0.0549 (0.0776)  time: 3.7595  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 310/3449]  eta: 3:17:19  lr: 0.000100  loss: 0.0616 (0.0774)  time: 3.7917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 320/3449]  eta: 3:16:39  lr: 0.000100  loss: 0.0673 (0.0772)  time: 3.7673  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 330/3449]  eta: 3:16:04  lr: 0.000100  loss: 0.0631 (0.0768)  time: 3.7744  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 340/3449]  eta: 3:15:22  lr: 0.000100  loss: 0.0692 (0.0765)  time: 3.7660  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 350/3449]  eta: 3:14:44  lr: 0.000100  loss: 0.0596 (0.0761)  time: 3.7468  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 360/3449]  eta: 3:14:08  lr: 0.000100  loss: 0.0578 (0.0755)  time: 3.7755  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 370/3449]  eta: 3:13:37  lr: 0.000100  loss: 0.0593 (0.0752)  time: 3.8171  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 380/3449]  eta: 3:13:07  lr: 0.000100  loss: 0.0718 (0.0752)  time: 3.8623  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 390/3449]  eta: 3:12:37  lr: 0.000100  loss: 0.0774 (0.0754)  time: 3.8739  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 400/3449]  eta: 3:12:03  lr: 0.000100  loss: 0.0703 (0.0752)  time: 3.8496  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 410/3449]  eta: 3:11:28  lr: 0.000100  loss: 0.0480 (0.0743)  time: 3.8233  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 420/3449]  eta: 3:10:53  lr: 0.000100  loss: 0.0333 (0.0734)  time: 3.8209  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 430/3449]  eta: 3:10:13  lr: 0.000100  loss: 0.0329 (0.0727)  time: 3.7824  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 440/3449]  eta: 3:09:29  lr: 0.000100  loss: 0.0446 (0.0721)  time: 3.7188  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 450/3449]  eta: 3:08:50  lr: 0.000100  loss: 0.0510 (0.0718)  time: 3.7298  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 460/3449]  eta: 3:08:02  lr: 0.000100  loss: 0.0706 (0.0723)  time: 3.6897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 470/3449]  eta: 3:07:27  lr: 0.000100  loss: 0.0895 (0.0724)  time: 3.7127  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 480/3449]  eta: 3:06:49  lr: 0.000100  loss: 0.0762 (0.0725)  time: 3.7951  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 490/3449]  eta: 3:06:15  lr: 0.000100  loss: 0.0775 (0.0726)  time: 3.8125  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 500/3449]  eta: 3:05:42  lr: 0.000100  loss: 0.0814 (0.0727)  time: 3.8461  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 510/3449]  eta: 3:05:03  lr: 0.000100  loss: 0.0749 (0.0726)  time: 3.7997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 520/3449]  eta: 3:04:23  lr: 0.000100  loss: 0.0490 (0.0719)  time: 3.7478  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 530/3449]  eta: 3:03:46  lr: 0.000100  loss: 0.0387 (0.0713)  time: 3.7664  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 540/3449]  eta: 3:03:09  lr: 0.000100  loss: 0.0394 (0.0707)  time: 3.7968  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 550/3449]  eta: 3:02:33  lr: 0.000100  loss: 0.0413 (0.0702)  time: 3.8055  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 560/3449]  eta: 3:01:56  lr: 0.000100  loss: 0.0497 (0.0702)  time: 3.8036  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 570/3449]  eta: 3:01:15  lr: 0.000100  loss: 0.0505 (0.0698)  time: 3.7540  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 580/3449]  eta: 3:00:42  lr: 0.000100  loss: 0.0440 (0.0696)  time: 3.7875  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 590/3449]  eta: 3:00:05  lr: 0.000100  loss: 0.0829 (0.0701)  time: 3.8375  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 600/3449]  eta: 2:59:24  lr: 0.000100  loss: 0.0948 (0.0702)  time: 3.7593  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [ 610/3449]  eta: 2:58:48  lr: 0.000100  loss: 0.0673 (0.0703)  time: 3.7653  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 620/3449]  eta: 2:58:17  lr: 0.000100  loss: 0.0882 (0.0706)  time: 3.8663  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 630/3449]  eta: 2:57:37  lr: 0.000100  loss: 0.0924 (0.0712)  time: 3.8284  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 640/3449]  eta: 2:57:00  lr: 0.000100  loss: 0.1040 (0.0716)  time: 3.7749  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 650/3449]  eta: 2:56:27  lr: 0.000100  loss: 0.0796 (0.0717)  time: 3.8419  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 660/3449]  eta: 2:55:48  lr: 0.000100  loss: 0.0647 (0.0717)  time: 3.8202  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 670/3449]  eta: 2:55:11  lr: 0.000100  loss: 0.0463 (0.0712)  time: 3.7821  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 680/3449]  eta: 2:54:34  lr: 0.000100  loss: 0.0429 (0.0712)  time: 3.7958  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 690/3449]  eta: 2:54:00  lr: 0.000100  loss: 0.0613 (0.0710)  time: 3.8414  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 700/3449]  eta: 2:53:19  lr: 0.000100  loss: 0.0540 (0.0708)  time: 3.7944  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 710/3449]  eta: 2:52:42  lr: 0.000100  loss: 0.0793 (0.0713)  time: 3.7551  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 720/3449]  eta: 2:52:01  lr: 0.000100  loss: 0.0796 (0.0714)  time: 3.7589  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 730/3449]  eta: 2:51:24  lr: 0.000100  loss: 0.0743 (0.0715)  time: 3.7428  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 740/3449]  eta: 2:50:47  lr: 0.000100  loss: 0.0775 (0.0716)  time: 3.8061  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 750/3449]  eta: 2:50:09  lr: 0.000100  loss: 0.0974 (0.0721)  time: 3.7919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 760/3449]  eta: 2:49:32  lr: 0.000100  loss: 0.0974 (0.0723)  time: 3.7803  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 770/3449]  eta: 2:48:51  lr: 0.000100  loss: 0.0824 (0.0724)  time: 3.7526  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 780/3449]  eta: 2:48:15  lr: 0.000100  loss: 0.1028 (0.0735)  time: 3.7654  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 790/3449]  eta: 2:47:32  lr: 0.000100  loss: 0.1250 (0.0740)  time: 3.7396  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 800/3449]  eta: 2:46:57  lr: 0.000100  loss: 0.1022 (0.0742)  time: 3.7447  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 810/3449]  eta: 2:46:16  lr: 0.000100  loss: 0.0870 (0.0742)  time: 3.7612  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 820/3449]  eta: 2:45:37  lr: 0.000100  loss: 0.0819 (0.0742)  time: 3.7209  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 830/3449]  eta: 2:45:01  lr: 0.000100  loss: 0.0739 (0.0742)  time: 3.7911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 840/3449]  eta: 2:44:20  lr: 0.000100  loss: 0.0675 (0.0741)  time: 3.7514  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 850/3449]  eta: 2:43:43  lr: 0.000100  loss: 0.0422 (0.0736)  time: 3.7402  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 860/3449]  eta: 2:43:03  lr: 0.000100  loss: 0.0377 (0.0734)  time: 3.7617  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 870/3449]  eta: 2:42:22  lr: 0.000100  loss: 0.0643 (0.0734)  time: 3.6990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 880/3449]  eta: 2:41:44  lr: 0.000100  loss: 0.0694 (0.0735)  time: 3.7214  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 890/3449]  eta: 2:41:10  lr: 0.000100  loss: 0.0641 (0.0734)  time: 3.8370  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 900/3449]  eta: 2:40:35  lr: 0.000100  loss: 0.0366 (0.0729)  time: 3.8823  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 910/3449]  eta: 2:39:55  lr: 0.000100  loss: 0.0357 (0.0727)  time: 3.7934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 920/3449]  eta: 2:39:16  lr: 0.000100  loss: 0.0543 (0.0727)  time: 3.7323  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 930/3449]  eta: 2:38:37  lr: 0.000100  loss: 0.0579 (0.0725)  time: 3.7248  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 940/3449]  eta: 2:37:58  lr: 0.000100  loss: 0.0394 (0.0721)  time: 3.7284  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 950/3449]  eta: 2:37:21  lr: 0.000100  loss: 0.0442 (0.0719)  time: 3.7771  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [ 960/3449]  eta: 2:36:44  lr: 0.000100  loss: 0.0477 (0.0717)  time: 3.8066  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 970/3449]  eta: 2:36:09  lr: 0.000100  loss: 0.0567 (0.0716)  time: 3.8447  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 980/3449]  eta: 2:35:33  lr: 0.000100  loss: 0.0574 (0.0714)  time: 3.8597  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [ 990/3449]  eta: 2:34:55  lr: 0.000100  loss: 0.0476 (0.0712)  time: 3.8054  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1000/3449]  eta: 2:34:12  lr: 0.000100  loss: 0.0380 (0.0708)  time: 3.6894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1010/3449]  eta: 2:33:35  lr: 0.000100  loss: 0.0327 (0.0705)  time: 3.6995  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1020/3449]  eta: 2:33:01  lr: 0.000100  loss: 0.0352 (0.0702)  time: 3.8558  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1030/3449]  eta: 2:32:25  lr: 0.000100  loss: 0.0433 (0.0700)  time: 3.8977  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1040/3449]  eta: 2:31:47  lr: 0.000100  loss: 0.0555 (0.0698)  time: 3.8191  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1050/3449]  eta: 2:31:10  lr: 0.000100  loss: 0.0421 (0.0695)  time: 3.7900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1060/3449]  eta: 2:30:31  lr: 0.000100  loss: 0.0441 (0.0696)  time: 3.7658  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1070/3449]  eta: 2:29:53  lr: 0.000100  loss: 0.0575 (0.0694)  time: 3.7488  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1080/3449]  eta: 2:29:15  lr: 0.000100  loss: 0.0575 (0.0697)  time: 3.7893  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1090/3449]  eta: 2:28:39  lr: 0.000100  loss: 0.1105 (0.0703)  time: 3.8179  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1100/3449]  eta: 2:28:02  lr: 0.000100  loss: 0.1109 (0.0706)  time: 3.8352  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1110/3449]  eta: 2:27:24  lr: 0.000100  loss: 0.1109 (0.0709)  time: 3.7926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1120/3449]  eta: 2:26:48  lr: 0.000100  loss: 0.0997 (0.0711)  time: 3.8290  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1130/3449]  eta: 2:26:07  lr: 0.000100  loss: 0.0854 (0.0711)  time: 3.7630  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1140/3449]  eta: 2:25:28  lr: 0.000100  loss: 0.0803 (0.0713)  time: 3.6570  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1150/3449]  eta: 2:24:50  lr: 0.000100  loss: 0.0649 (0.0712)  time: 3.7397  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1160/3449]  eta: 2:24:11  lr: 0.000100  loss: 0.0580 (0.0712)  time: 3.7630  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1170/3449]  eta: 2:23:30  lr: 0.000100  loss: 0.0514 (0.0710)  time: 3.6593  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1180/3449]  eta: 2:22:54  lr: 0.000100  loss: 0.0387 (0.0707)  time: 3.7341  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1190/3449]  eta: 2:22:18  lr: 0.000100  loss: 0.0392 (0.0705)  time: 3.8701  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1200/3449]  eta: 2:21:40  lr: 0.000100  loss: 0.0392 (0.0702)  time: 3.8231  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1210/3449]  eta: 2:21:04  lr: 0.000100  loss: 0.0437 (0.0700)  time: 3.8270  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1220/3449]  eta: 2:20:27  lr: 0.000100  loss: 0.0530 (0.0700)  time: 3.8360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1230/3449]  eta: 2:19:50  lr: 0.000100  loss: 0.0804 (0.0701)  time: 3.8322  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1240/3449]  eta: 2:19:16  lr: 0.000100  loss: 0.0908 (0.0703)  time: 3.9135  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1250/3449]  eta: 2:18:37  lr: 0.000100  loss: 0.0688 (0.0702)  time: 3.8553  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1260/3449]  eta: 2:18:01  lr: 0.000100  loss: 0.0627 (0.0702)  time: 3.8081  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [1270/3449]  eta: 2:17:23  lr: 0.000100  loss: 0.0758 (0.0704)  time: 3.8230  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1280/3449]  eta: 2:16:46  lr: 0.000100  loss: 0.0463 (0.0702)  time: 3.8098  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1290/3449]  eta: 2:16:11  lr: 0.000100  loss: 0.0582 (0.0702)  time: 3.8865  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1300/3449]  eta: 2:15:33  lr: 0.000100  loss: 0.0749 (0.0702)  time: 3.8460  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1310/3449]  eta: 2:14:56  lr: 0.000100  loss: 0.0584 (0.0701)  time: 3.8013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1320/3449]  eta: 2:14:17  lr: 0.000100  loss: 0.0456 (0.0699)  time: 3.7994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1330/3449]  eta: 2:13:39  lr: 0.000100  loss: 0.0406 (0.0697)  time: 3.7452  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1340/3449]  eta: 2:13:00  lr: 0.000100  loss: 0.0418 (0.0696)  time: 3.7312  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1350/3449]  eta: 2:12:22  lr: 0.000100  loss: 0.0687 (0.0701)  time: 3.7444  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1360/3449]  eta: 2:11:44  lr: 0.000100  loss: 0.1730 (0.0710)  time: 3.7640  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1370/3449]  eta: 2:11:05  lr: 0.000100  loss: 0.1666 (0.0716)  time: 3.7666  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1380/3449]  eta: 2:10:29  lr: 0.000100  loss: 0.1169 (0.0718)  time: 3.8084  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1390/3449]  eta: 2:09:51  lr: 0.000100  loss: 0.1039 (0.0725)  time: 3.8137  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1400/3449]  eta: 2:09:11  lr: 0.000100  loss: 0.1631 (0.0729)  time: 3.7129  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1410/3449]  eta: 2:08:33  lr: 0.000100  loss: 0.1565 (0.0733)  time: 3.7064  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1420/3449]  eta: 2:07:57  lr: 0.000100  loss: 0.1214 (0.0735)  time: 3.8272  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1430/3449]  eta: 2:07:18  lr: 0.000100  loss: 0.0902 (0.0736)  time: 3.8286  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1440/3449]  eta: 2:06:40  lr: 0.000100  loss: 0.1108 (0.0740)  time: 3.7386  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1450/3449]  eta: 2:06:01  lr: 0.000100  loss: 0.0994 (0.0741)  time: 3.7203  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1460/3449]  eta: 2:05:23  lr: 0.000100  loss: 0.0510 (0.0739)  time: 3.7412  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1470/3449]  eta: 2:04:45  lr: 0.000100  loss: 0.0430 (0.0737)  time: 3.7569  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1480/3449]  eta: 2:04:07  lr: 0.000100  loss: 0.0598 (0.0737)  time: 3.7702  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1490/3449]  eta: 2:03:28  lr: 0.000100  loss: 0.0728 (0.0738)  time: 3.7403  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1500/3449]  eta: 2:02:50  lr: 0.000100  loss: 0.0539 (0.0736)  time: 3.7331  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1510/3449]  eta: 2:02:12  lr: 0.000100  loss: 0.0329 (0.0734)  time: 3.7808  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1520/3449]  eta: 2:01:34  lr: 0.000100  loss: 0.0300 (0.0731)  time: 3.7830  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1530/3449]  eta: 2:00:55  lr: 0.000100  loss: 0.0404 (0.0730)  time: 3.7262  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1540/3449]  eta: 2:00:16  lr: 0.000100  loss: 0.0450 (0.0728)  time: 3.6791  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1550/3449]  eta: 1:59:37  lr: 0.000100  loss: 0.0406 (0.0726)  time: 3.6868  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1560/3449]  eta: 1:59:00  lr: 0.000100  loss: 0.0381 (0.0724)  time: 3.7480  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1570/3449]  eta: 1:58:23  lr: 0.000100  loss: 0.0356 (0.0722)  time: 3.8141  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1580/3449]  eta: 1:57:47  lr: 0.000100  loss: 0.0518 (0.0721)  time: 3.9057  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1590/3449]  eta: 1:57:07  lr: 0.000100  loss: 0.0683 (0.0722)  time: 3.8032  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1600/3449]  eta: 1:56:30  lr: 0.000100  loss: 0.0941 (0.0725)  time: 3.7171  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1610/3449]  eta: 1:55:52  lr: 0.000100  loss: 0.1179 (0.0727)  time: 3.7667  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1620/3449]  eta: 1:55:15  lr: 0.000100  loss: 0.0621 (0.0725)  time: 3.8083  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1630/3449]  eta: 1:54:38  lr: 0.000100  loss: 0.0563 (0.0725)  time: 3.8705  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1640/3449]  eta: 1:54:01  lr: 0.000100  loss: 0.0637 (0.0725)  time: 3.8337  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1650/3449]  eta: 1:53:22  lr: 0.000100  loss: 0.0694 (0.0726)  time: 3.7898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1660/3449]  eta: 1:52:46  lr: 0.000100  loss: 0.0745 (0.0726)  time: 3.8299  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1670/3449]  eta: 1:52:08  lr: 0.000100  loss: 0.0521 (0.0724)  time: 3.8212  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1680/3449]  eta: 1:51:29  lr: 0.000100  loss: 0.0491 (0.0723)  time: 3.7138  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1690/3449]  eta: 1:50:50  lr: 0.000100  loss: 0.0656 (0.0723)  time: 3.6803  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1700/3449]  eta: 1:50:11  lr: 0.000100  loss: 0.0656 (0.0723)  time: 3.6873  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1710/3449]  eta: 1:49:32  lr: 0.000100  loss: 0.0579 (0.0723)  time: 3.6562  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1720/3449]  eta: 1:48:54  lr: 0.000100  loss: 0.1022 (0.0726)  time: 3.6881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1730/3449]  eta: 1:48:16  lr: 0.000100  loss: 0.1382 (0.0730)  time: 3.7889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1740/3449]  eta: 1:47:39  lr: 0.000100  loss: 0.1246 (0.0733)  time: 3.8065  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1750/3449]  eta: 1:47:01  lr: 0.000100  loss: 0.1138 (0.0735)  time: 3.7848  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1760/3449]  eta: 1:46:24  lr: 0.000100  loss: 0.1188 (0.0740)  time: 3.8290  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1770/3449]  eta: 1:45:47  lr: 0.000100  loss: 0.1608 (0.0744)  time: 3.8454  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1780/3449]  eta: 1:45:07  lr: 0.000100  loss: 0.1341 (0.0747)  time: 3.7149  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1790/3449]  eta: 1:44:29  lr: 0.000100  loss: 0.0970 (0.0747)  time: 3.6695  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1800/3449]  eta: 1:43:51  lr: 0.000100  loss: 0.0933 (0.0749)  time: 3.7450  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1810/3449]  eta: 1:43:13  lr: 0.000100  loss: 0.1266 (0.0753)  time: 3.7579  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1820/3449]  eta: 1:42:36  lr: 0.000100  loss: 0.1139 (0.0754)  time: 3.7910  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1830/3449]  eta: 1:41:58  lr: 0.000100  loss: 0.0958 (0.0755)  time: 3.8042  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1840/3449]  eta: 1:41:20  lr: 0.000100  loss: 0.0743 (0.0755)  time: 3.7550  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1850/3449]  eta: 1:40:42  lr: 0.000100  loss: 0.0717 (0.0754)  time: 3.7854  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1860/3449]  eta: 1:40:04  lr: 0.000100  loss: 0.0743 (0.0754)  time: 3.7671  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1870/3449]  eta: 1:39:26  lr: 0.000100  loss: 0.0851 (0.0756)  time: 3.7389  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1880/3449]  eta: 1:38:49  lr: 0.000100  loss: 0.1117 (0.0759)  time: 3.7865  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1890/3449]  eta: 1:38:10  lr: 0.000100  loss: 0.1213 (0.0761)  time: 3.7649  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1900/3449]  eta: 1:37:32  lr: 0.000100  loss: 0.0953 (0.0762)  time: 3.7331  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1910/3449]  eta: 1:36:55  lr: 0.000100  loss: 0.0967 (0.0762)  time: 3.7809  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1920/3449]  eta: 1:36:17  lr: 0.000100  loss: 0.0826 (0.0762)  time: 3.8206  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [1930/3449]  eta: 1:35:40  lr: 0.000100  loss: 0.0687 (0.0762)  time: 3.8169  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [1940/3449]  eta: 1:35:03  lr: 0.000100  loss: 0.0687 (0.0761)  time: 3.8305  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1950/3449]  eta: 1:34:26  lr: 0.000100  loss: 0.0527 (0.0760)  time: 3.8633  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1960/3449]  eta: 1:33:48  lr: 0.000100  loss: 0.0436 (0.0758)  time: 3.8356  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1970/3449]  eta: 1:33:10  lr: 0.000100  loss: 0.0444 (0.0757)  time: 3.7698  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1980/3449]  eta: 1:32:33  lr: 0.000100  loss: 0.0622 (0.0757)  time: 3.8388  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [1990/3449]  eta: 1:31:56  lr: 0.000100  loss: 0.0669 (0.0757)  time: 3.8730  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2000/3449]  eta: 1:31:17  lr: 0.000100  loss: 0.0686 (0.0756)  time: 3.7745  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2010/3449]  eta: 1:30:40  lr: 0.000100  loss: 0.0666 (0.0757)  time: 3.7519  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2020/3449]  eta: 1:30:02  lr: 0.000100  loss: 0.0661 (0.0756)  time: 3.7926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2030/3449]  eta: 1:29:24  lr: 0.000100  loss: 0.0669 (0.0756)  time: 3.7915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2040/3449]  eta: 1:28:46  lr: 0.000100  loss: 0.0686 (0.0756)  time: 3.7686  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2050/3449]  eta: 1:28:08  lr: 0.000100  loss: 0.0582 (0.0755)  time: 3.7629  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2060/3449]  eta: 1:27:30  lr: 0.000100  loss: 0.0556 (0.0754)  time: 3.7651  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2070/3449]  eta: 1:26:53  lr: 0.000100  loss: 0.0518 (0.0753)  time: 3.8101  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2080/3449]  eta: 1:26:15  lr: 0.000100  loss: 0.0572 (0.0752)  time: 3.8362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2090/3449]  eta: 1:25:38  lr: 0.000100  loss: 0.0699 (0.0754)  time: 3.8122  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2100/3449]  eta: 1:25:01  lr: 0.000100  loss: 0.1191 (0.0757)  time: 3.8711  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2110/3449]  eta: 1:24:23  lr: 0.000100  loss: 0.0895 (0.0757)  time: 3.8484  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2120/3449]  eta: 1:23:45  lr: 0.000100  loss: 0.0549 (0.0755)  time: 3.7425  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2130/3449]  eta: 1:23:07  lr: 0.000100  loss: 0.0410 (0.0754)  time: 3.7666  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2140/3449]  eta: 1:22:30  lr: 0.000100  loss: 0.0374 (0.0752)  time: 3.8148  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2150/3449]  eta: 1:21:52  lr: 0.000100  loss: 0.0468 (0.0752)  time: 3.8377  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2160/3449]  eta: 1:21:15  lr: 0.000100  loss: 0.0508 (0.0750)  time: 3.8554  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2170/3449]  eta: 1:20:37  lr: 0.000100  loss: 0.0403 (0.0748)  time: 3.8233  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2180/3449]  eta: 1:20:00  lr: 0.000100  loss: 0.0414 (0.0748)  time: 3.8282  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2190/3449]  eta: 1:19:22  lr: 0.000100  loss: 0.0583 (0.0747)  time: 3.8079  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2200/3449]  eta: 1:18:44  lr: 0.000100  loss: 0.0615 (0.0747)  time: 3.7367  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2210/3449]  eta: 1:18:05  lr: 0.000100  loss: 0.0618 (0.0746)  time: 3.6781  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2220/3449]  eta: 1:17:27  lr: 0.000100  loss: 0.0670 (0.0746)  time: 3.6832  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2230/3449]  eta: 1:16:49  lr: 0.000100  loss: 0.0638 (0.0745)  time: 3.7674  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2240/3449]  eta: 1:16:11  lr: 0.000100  loss: 0.0610 (0.0744)  time: 3.7714  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2250/3449]  eta: 1:15:33  lr: 0.000100  loss: 0.0614 (0.0744)  time: 3.7391  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2260/3449]  eta: 1:14:56  lr: 0.000100  loss: 0.0468 (0.0742)  time: 3.7864  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2270/3449]  eta: 1:14:18  lr: 0.000100  loss: 0.0489 (0.0742)  time: 3.8684  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2280/3449]  eta: 1:13:40  lr: 0.000100  loss: 0.0738 (0.0743)  time: 3.8146  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2290/3449]  eta: 1:13:02  lr: 0.000100  loss: 0.0802 (0.0743)  time: 3.7191  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2300/3449]  eta: 1:12:24  lr: 0.000100  loss: 0.0778 (0.0742)  time: 3.7191  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2310/3449]  eta: 1:11:46  lr: 0.000100  loss: 0.0546 (0.0743)  time: 3.7498  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2320/3449]  eta: 1:11:09  lr: 0.000100  loss: 0.0847 (0.0744)  time: 3.7998  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2330/3449]  eta: 1:10:31  lr: 0.000100  loss: 0.1308 (0.0746)  time: 3.8160  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2340/3449]  eta: 1:09:53  lr: 0.000100  loss: 0.1147 (0.0747)  time: 3.7964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2350/3449]  eta: 1:09:16  lr: 0.000100  loss: 0.0857 (0.0747)  time: 3.8330  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2360/3449]  eta: 1:08:38  lr: 0.000100  loss: 0.0659 (0.0747)  time: 3.8378  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2370/3449]  eta: 1:08:00  lr: 0.000100  loss: 0.0606 (0.0746)  time: 3.7913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2380/3449]  eta: 1:07:22  lr: 0.000100  loss: 0.0581 (0.0745)  time: 3.7807  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2390/3449]  eta: 1:06:44  lr: 0.000100  loss: 0.0677 (0.0745)  time: 3.7702  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2400/3449]  eta: 1:06:07  lr: 0.000100  loss: 0.0501 (0.0744)  time: 3.8145  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2410/3449]  eta: 1:05:29  lr: 0.000100  loss: 0.0429 (0.0744)  time: 3.8079  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2420/3449]  eta: 1:04:51  lr: 0.000100  loss: 0.0429 (0.0742)  time: 3.7592  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2430/3449]  eta: 1:04:13  lr: 0.000100  loss: 0.0395 (0.0741)  time: 3.7720  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2440/3449]  eta: 1:03:35  lr: 0.000100  loss: 0.0420 (0.0740)  time: 3.7548  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2450/3449]  eta: 1:02:57  lr: 0.000100  loss: 0.0485 (0.0739)  time: 3.7634  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2460/3449]  eta: 1:02:19  lr: 0.000100  loss: 0.0584 (0.0739)  time: 3.7340  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2470/3449]  eta: 1:01:41  lr: 0.000100  loss: 0.0615 (0.0739)  time: 3.7211  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2480/3449]  eta: 1:01:04  lr: 0.000100  loss: 0.0491 (0.0739)  time: 3.8455  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2490/3449]  eta: 1:00:26  lr: 0.000100  loss: 0.0421 (0.0737)  time: 3.8084  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2500/3449]  eta: 0:59:48  lr: 0.000100  loss: 0.0366 (0.0736)  time: 3.7200  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2510/3449]  eta: 0:59:11  lr: 0.000100  loss: 0.0468 (0.0737)  time: 3.8541  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2520/3449]  eta: 0:58:33  lr: 0.000100  loss: 0.1119 (0.0739)  time: 3.8817  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2530/3449]  eta: 0:57:55  lr: 0.000100  loss: 0.1214 (0.0741)  time: 3.7728  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2540/3449]  eta: 0:57:18  lr: 0.000100  loss: 0.1214 (0.0742)  time: 3.8008  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2550/3449]  eta: 0:56:40  lr: 0.000100  loss: 0.1228 (0.0744)  time: 3.8144  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2560/3449]  eta: 0:56:02  lr: 0.000100  loss: 0.0773 (0.0744)  time: 3.7592  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2570/3449]  eta: 0:55:24  lr: 0.000100  loss: 0.0620 (0.0744)  time: 3.7879  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2580/3449]  eta: 0:54:46  lr: 0.000100  loss: 0.0599 (0.0744)  time: 3.7679  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [2590/3449]  eta: 0:54:08  lr: 0.000100  loss: 0.0451 (0.0742)  time: 3.7438  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2600/3449]  eta: 0:53:30  lr: 0.000100  loss: 0.0311 (0.0740)  time: 3.7648  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2610/3449]  eta: 0:52:52  lr: 0.000100  loss: 0.0351 (0.0739)  time: 3.7526  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2620/3449]  eta: 0:52:15  lr: 0.000100  loss: 0.0310 (0.0737)  time: 3.7608  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2630/3449]  eta: 0:51:37  lr: 0.000100  loss: 0.0330 (0.0737)  time: 3.7747  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2640/3449]  eta: 0:50:59  lr: 0.000100  loss: 0.0585 (0.0736)  time: 3.7977  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2650/3449]  eta: 0:50:21  lr: 0.000100  loss: 0.0707 (0.0737)  time: 3.8175  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2660/3449]  eta: 0:49:44  lr: 0.000100  loss: 0.0530 (0.0736)  time: 3.8041  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2670/3449]  eta: 0:49:06  lr: 0.000100  loss: 0.0523 (0.0736)  time: 3.7896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2680/3449]  eta: 0:48:28  lr: 0.000100  loss: 0.0632 (0.0735)  time: 3.7926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2690/3449]  eta: 0:47:50  lr: 0.000100  loss: 0.0618 (0.0735)  time: 3.7434  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2700/3449]  eta: 0:47:12  lr: 0.000100  loss: 0.0471 (0.0734)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2710/3449]  eta: 0:46:34  lr: 0.000100  loss: 0.0552 (0.0734)  time: 3.8180  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2720/3449]  eta: 0:45:57  lr: 0.000100  loss: 0.0697 (0.0734)  time: 3.7621  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2730/3449]  eta: 0:45:19  lr: 0.000100  loss: 0.0653 (0.0734)  time: 3.7586  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2740/3449]  eta: 0:44:41  lr: 0.000100  loss: 0.0574 (0.0733)  time: 3.7552  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2750/3449]  eta: 0:44:03  lr: 0.000100  loss: 0.0574 (0.0733)  time: 3.7732  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2760/3449]  eta: 0:43:25  lr: 0.000100  loss: 0.0646 (0.0732)  time: 3.7644  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2770/3449]  eta: 0:42:47  lr: 0.000100  loss: 0.0648 (0.0732)  time: 3.8106  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2780/3449]  eta: 0:42:10  lr: 0.000100  loss: 0.0705 (0.0732)  time: 3.9284  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2790/3449]  eta: 0:41:32  lr: 0.000100  loss: 0.0705 (0.0732)  time: 3.8664  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2800/3449]  eta: 0:40:54  lr: 0.000100  loss: 0.0631 (0.0731)  time: 3.7634  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2810/3449]  eta: 0:40:16  lr: 0.000100  loss: 0.0502 (0.0730)  time: 3.7526  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2820/3449]  eta: 0:39:39  lr: 0.000100  loss: 0.0428 (0.0730)  time: 3.7537  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2830/3449]  eta: 0:39:01  lr: 0.000100  loss: 0.0676 (0.0730)  time: 3.7864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2840/3449]  eta: 0:38:23  lr: 0.000100  loss: 0.0610 (0.0729)  time: 3.7634  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2850/3449]  eta: 0:37:45  lr: 0.000100  loss: 0.0557 (0.0729)  time: 3.6930  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2860/3449]  eta: 0:37:07  lr: 0.000100  loss: 0.0744 (0.0729)  time: 3.6843  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2870/3449]  eta: 0:36:29  lr: 0.000100  loss: 0.0794 (0.0730)  time: 3.7778  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2880/3449]  eta: 0:35:51  lr: 0.000100  loss: 0.0902 (0.0730)  time: 3.8263  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2890/3449]  eta: 0:35:13  lr: 0.000100  loss: 0.0808 (0.0730)  time: 3.7749  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2900/3449]  eta: 0:34:36  lr: 0.000100  loss: 0.0564 (0.0729)  time: 3.7560  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2910/3449]  eta: 0:33:58  lr: 0.000100  loss: 0.0338 (0.0728)  time: 3.7914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2920/3449]  eta: 0:33:20  lr: 0.000100  loss: 0.0314 (0.0726)  time: 3.8091  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2930/3449]  eta: 0:32:42  lr: 0.000100  loss: 0.0320 (0.0725)  time: 3.7474  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2940/3449]  eta: 0:32:04  lr: 0.000100  loss: 0.0366 (0.0725)  time: 3.7696  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2950/3449]  eta: 0:31:27  lr: 0.000100  loss: 0.0607 (0.0724)  time: 3.8131  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2960/3449]  eta: 0:30:49  lr: 0.000100  loss: 0.0538 (0.0724)  time: 3.7697  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [2970/3449]  eta: 0:30:11  lr: 0.000100  loss: 0.0351 (0.0722)  time: 3.7482  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2980/3449]  eta: 0:29:33  lr: 0.000100  loss: 0.0350 (0.0722)  time: 3.7191  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [2990/3449]  eta: 0:28:55  lr: 0.000100  loss: 0.0498 (0.0721)  time: 3.7003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3000/3449]  eta: 0:28:17  lr: 0.000100  loss: 0.0443 (0.0720)  time: 3.7097  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3010/3449]  eta: 0:27:39  lr: 0.000100  loss: 0.0491 (0.0720)  time: 3.7674  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3020/3449]  eta: 0:27:01  lr: 0.000100  loss: 0.0703 (0.0720)  time: 3.7838  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3030/3449]  eta: 0:26:24  lr: 0.000100  loss: 0.0652 (0.0720)  time: 3.7324  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3040/3449]  eta: 0:25:46  lr: 0.000100  loss: 0.0719 (0.0721)  time: 3.7527  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3050/3449]  eta: 0:25:08  lr: 0.000100  loss: 0.1062 (0.0722)  time: 3.7684  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3060/3449]  eta: 0:24:30  lr: 0.000100  loss: 0.0950 (0.0722)  time: 3.7708  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3070/3449]  eta: 0:23:52  lr: 0.000100  loss: 0.0717 (0.0722)  time: 3.8166  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3080/3449]  eta: 0:23:15  lr: 0.000100  loss: 0.0550 (0.0721)  time: 3.7987  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3090/3449]  eta: 0:22:37  lr: 0.000100  loss: 0.0476 (0.0721)  time: 3.7058  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3100/3449]  eta: 0:21:59  lr: 0.000100  loss: 0.0670 (0.0721)  time: 3.7184  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3110/3449]  eta: 0:21:21  lr: 0.000100  loss: 0.0554 (0.0720)  time: 3.7853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3120/3449]  eta: 0:20:43  lr: 0.000100  loss: 0.0445 (0.0719)  time: 3.8201  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3130/3449]  eta: 0:20:06  lr: 0.000100  loss: 0.0368 (0.0718)  time: 3.8496  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3140/3449]  eta: 0:19:28  lr: 0.000100  loss: 0.0351 (0.0717)  time: 3.8073  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3150/3449]  eta: 0:18:50  lr: 0.000100  loss: 0.0351 (0.0716)  time: 3.8116  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3160/3449]  eta: 0:18:12  lr: 0.000100  loss: 0.0449 (0.0716)  time: 3.8402  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3170/3449]  eta: 0:17:34  lr: 0.000100  loss: 0.0724 (0.0716)  time: 3.8286  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3180/3449]  eta: 0:16:57  lr: 0.000100  loss: 0.0735 (0.0716)  time: 3.7961  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3190/3449]  eta: 0:16:19  lr: 0.000100  loss: 0.0790 (0.0717)  time: 3.7141  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3200/3449]  eta: 0:15:41  lr: 0.000100  loss: 0.1157 (0.0718)  time: 3.7199  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3210/3449]  eta: 0:15:03  lr: 0.000100  loss: 0.1118 (0.0719)  time: 3.8097  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3220/3449]  eta: 0:14:25  lr: 0.000100  loss: 0.1036 (0.0720)  time: 3.8054  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3230/3449]  eta: 0:13:47  lr: 0.000100  loss: 0.0653 (0.0720)  time: 3.7396  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3240/3449]  eta: 0:13:10  lr: 0.000100  loss: 0.0546 (0.0719)  time: 3.7661  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [3250/3449]  eta: 0:12:32  lr: 0.000100  loss: 0.0444 (0.0718)  time: 3.7747  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3260/3449]  eta: 0:11:54  lr: 0.000100  loss: 0.0376 (0.0717)  time: 3.7195  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3270/3449]  eta: 0:11:16  lr: 0.000100  loss: 0.0615 (0.0717)  time: 3.7298  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3280/3449]  eta: 0:10:38  lr: 0.000100  loss: 0.0782 (0.0717)  time: 3.7709  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3290/3449]  eta: 0:10:01  lr: 0.000100  loss: 0.0796 (0.0717)  time: 3.8134  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3300/3449]  eta: 0:09:23  lr: 0.000100  loss: 0.0876 (0.0718)  time: 3.8289  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3310/3449]  eta: 0:08:45  lr: 0.000100  loss: 0.0746 (0.0718)  time: 3.8038  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3320/3449]  eta: 0:08:07  lr: 0.000100  loss: 0.0667 (0.0718)  time: 3.8126  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3330/3449]  eta: 0:07:29  lr: 0.000100  loss: 0.0664 (0.0718)  time: 3.8466  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3340/3449]  eta: 0:06:52  lr: 0.000100  loss: 0.0618 (0.0718)  time: 3.8143  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3350/3449]  eta: 0:06:14  lr: 0.000100  loss: 0.0434 (0.0717)  time: 3.7724  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:20]  [3360/3449]  eta: 0:05:36  lr: 0.000100  loss: 0.0518 (0.0716)  time: 3.7844  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3370/3449]  eta: 0:04:58  lr: 0.000100  loss: 0.0451 (0.0716)  time: 3.7622  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3380/3449]  eta: 0:04:20  lr: 0.000100  loss: 0.0441 (0.0715)  time: 3.7286  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3390/3449]  eta: 0:03:43  lr: 0.000100  loss: 0.0507 (0.0714)  time: 3.7656  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3400/3449]  eta: 0:03:05  lr: 0.000100  loss: 0.0365 (0.0713)  time: 3.8259  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3410/3449]  eta: 0:02:27  lr: 0.000100  loss: 0.0352 (0.0713)  time: 3.8093  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3420/3449]  eta: 0:01:49  lr: 0.000100  loss: 0.0441 (0.0712)  time: 3.8237  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3430/3449]  eta: 0:01:11  lr: 0.000100  loss: 0.0476 (0.0712)  time: 3.7608  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3440/3449]  eta: 0:00:34  lr: 0.000100  loss: 0.0415 (0.0711)  time: 3.6942  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0396 (0.0710)  time: 3.7794  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:20] Total time: 3:37:21 (3.7811 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0396 (0.0710)\n",
      "Valid: [epoch:20]  [ 0/14]  eta: 0:04:20  loss: 0.0518 (0.0518)  time: 18.6135  data: 0.4302  max mem: 34968\n",
      "Valid: [epoch:20]  [13/14]  eta: 0:00:18  loss: 0.0497 (0.0503)  time: 18.2272  data: 0.0309  max mem: 34968\n",
      "Valid: [epoch:20] Total time: 0:04:15 (18.2373 s / it)\n",
      "Averaged stats: loss: 0.0497 (0.0503)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_20_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.050%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:21]  [   0/3449]  eta: 4:34:25  lr: 0.000100  loss: 0.0738 (0.0738)  time: 4.7741  data: 1.3534  max mem: 34968\n",
      "Train: [epoch:21]  [  10/3449]  eta: 3:47:10  lr: 0.000100  loss: 0.0611 (0.0843)  time: 3.9634  data: 0.1231  max mem: 34968\n",
      "Train: [epoch:21]  [  20/3449]  eta: 3:43:02  lr: 0.000100  loss: 0.1109 (0.1087)  time: 3.8591  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [  30/3449]  eta: 3:37:43  lr: 0.000100  loss: 0.1165 (0.1095)  time: 3.7425  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [  40/3449]  eta: 3:36:18  lr: 0.000100  loss: 0.0851 (0.1018)  time: 3.7068  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [  50/3449]  eta: 3:35:30  lr: 0.000100  loss: 0.0784 (0.0979)  time: 3.7781  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [  60/3449]  eta: 3:34:20  lr: 0.000100  loss: 0.0592 (0.0912)  time: 3.7698  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [  70/3449]  eta: 3:33:27  lr: 0.000100  loss: 0.0543 (0.0873)  time: 3.7549  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [  80/3449]  eta: 3:33:00  lr: 0.000100  loss: 0.0718 (0.0858)  time: 3.7895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [  90/3449]  eta: 3:31:58  lr: 0.000100  loss: 0.0486 (0.0812)  time: 3.7733  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 100/3449]  eta: 3:31:53  lr: 0.000100  loss: 0.0431 (0.0784)  time: 3.8069  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 110/3449]  eta: 3:30:57  lr: 0.000100  loss: 0.0667 (0.0799)  time: 3.8110  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 120/3449]  eta: 3:29:50  lr: 0.000100  loss: 0.0775 (0.0797)  time: 3.7100  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 130/3449]  eta: 3:28:58  lr: 0.000100  loss: 0.0763 (0.0809)  time: 3.7042  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 140/3449]  eta: 3:28:31  lr: 0.000100  loss: 0.0917 (0.0817)  time: 3.7753  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 150/3449]  eta: 3:28:03  lr: 0.000100  loss: 0.0942 (0.0830)  time: 3.8264  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 160/3449]  eta: 3:27:23  lr: 0.000100  loss: 0.1084 (0.0850)  time: 3.7990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 170/3449]  eta: 3:26:28  lr: 0.000100  loss: 0.1084 (0.0863)  time: 3.7337  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 180/3449]  eta: 3:25:43  lr: 0.000100  loss: 0.1397 (0.0906)  time: 3.7182  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 190/3449]  eta: 3:25:14  lr: 0.000100  loss: 0.1357 (0.0918)  time: 3.7806  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 200/3449]  eta: 3:24:45  lr: 0.000100  loss: 0.1274 (0.0935)  time: 3.8276  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 210/3449]  eta: 3:23:36  lr: 0.000100  loss: 0.1208 (0.0944)  time: 3.7071  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 220/3449]  eta: 3:22:54  lr: 0.000100  loss: 0.0921 (0.0936)  time: 3.6608  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 230/3449]  eta: 3:22:38  lr: 0.000100  loss: 0.0984 (0.0960)  time: 3.8332  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 240/3449]  eta: 3:21:54  lr: 0.000100  loss: 0.1444 (0.0981)  time: 3.8292  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 250/3449]  eta: 3:21:15  lr: 0.000100  loss: 0.1348 (0.0987)  time: 3.7471  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 260/3449]  eta: 3:20:38  lr: 0.000100  loss: 0.1037 (0.0988)  time: 3.7738  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 270/3449]  eta: 3:19:55  lr: 0.000100  loss: 0.0654 (0.0973)  time: 3.7583  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 280/3449]  eta: 3:19:11  lr: 0.000100  loss: 0.0575 (0.0959)  time: 3.7224  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 290/3449]  eta: 3:18:37  lr: 0.000100  loss: 0.0682 (0.0953)  time: 3.7583  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 300/3449]  eta: 3:18:11  lr: 0.000100  loss: 0.0759 (0.0942)  time: 3.8468  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 310/3449]  eta: 3:17:25  lr: 0.000100  loss: 0.0677 (0.0937)  time: 3.7895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 320/3449]  eta: 3:16:46  lr: 0.000100  loss: 0.0590 (0.0924)  time: 3.7287  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 330/3449]  eta: 3:16:03  lr: 0.000100  loss: 0.0415 (0.0910)  time: 3.7413  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 340/3449]  eta: 3:15:26  lr: 0.000100  loss: 0.0521 (0.0907)  time: 3.7468  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 350/3449]  eta: 3:14:54  lr: 0.000100  loss: 0.0831 (0.0903)  time: 3.8068  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 360/3449]  eta: 3:14:31  lr: 0.000100  loss: 0.0875 (0.0900)  time: 3.8895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 370/3449]  eta: 3:13:50  lr: 0.000100  loss: 0.0785 (0.0895)  time: 3.8457  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 380/3449]  eta: 3:13:16  lr: 0.000100  loss: 0.0636 (0.0887)  time: 3.7852  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 390/3449]  eta: 3:12:43  lr: 0.000100  loss: 0.0486 (0.0877)  time: 3.8308  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [ 400/3449]  eta: 3:12:14  lr: 0.000100  loss: 0.0450 (0.0866)  time: 3.8628  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 410/3449]  eta: 3:11:42  lr: 0.000100  loss: 0.0430 (0.0856)  time: 3.8796  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 420/3449]  eta: 3:11:10  lr: 0.000100  loss: 0.0430 (0.0847)  time: 3.8683  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 430/3449]  eta: 3:10:34  lr: 0.000100  loss: 0.0454 (0.0838)  time: 3.8386  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 440/3449]  eta: 3:09:49  lr: 0.000100  loss: 0.0466 (0.0832)  time: 3.7470  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 450/3449]  eta: 3:09:22  lr: 0.000100  loss: 0.0613 (0.0829)  time: 3.8141  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 460/3449]  eta: 3:08:37  lr: 0.000100  loss: 0.0572 (0.0822)  time: 3.8166  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 470/3449]  eta: 3:07:59  lr: 0.000100  loss: 0.0463 (0.0814)  time: 3.7307  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 480/3449]  eta: 3:07:28  lr: 0.000100  loss: 0.0368 (0.0805)  time: 3.8374  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 490/3449]  eta: 3:06:51  lr: 0.000100  loss: 0.0347 (0.0796)  time: 3.8562  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 500/3449]  eta: 3:06:22  lr: 0.000100  loss: 0.0489 (0.0792)  time: 3.8691  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 510/3449]  eta: 3:05:41  lr: 0.000100  loss: 0.0593 (0.0791)  time: 3.8387  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 520/3449]  eta: 3:05:01  lr: 0.000100  loss: 0.0653 (0.0787)  time: 3.7466  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 530/3449]  eta: 3:04:21  lr: 0.000100  loss: 0.0557 (0.0783)  time: 3.7499  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 540/3449]  eta: 3:03:42  lr: 0.000100  loss: 0.0613 (0.0782)  time: 3.7657  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 550/3449]  eta: 3:03:04  lr: 0.000100  loss: 0.0724 (0.0783)  time: 3.7763  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 560/3449]  eta: 3:02:26  lr: 0.000100  loss: 0.0844 (0.0792)  time: 3.7858  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 570/3449]  eta: 3:01:48  lr: 0.000100  loss: 0.1206 (0.0798)  time: 3.7918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 580/3449]  eta: 3:01:04  lr: 0.000100  loss: 0.1025 (0.0799)  time: 3.7290  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 590/3449]  eta: 3:00:26  lr: 0.000100  loss: 0.0825 (0.0800)  time: 3.7186  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 600/3449]  eta: 2:59:45  lr: 0.000100  loss: 0.0666 (0.0796)  time: 3.7523  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 610/3449]  eta: 2:59:05  lr: 0.000100  loss: 0.0603 (0.0794)  time: 3.7301  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 620/3449]  eta: 2:58:31  lr: 0.000100  loss: 0.0804 (0.0797)  time: 3.7994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 630/3449]  eta: 2:57:45  lr: 0.000100  loss: 0.0906 (0.0798)  time: 3.7399  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 640/3449]  eta: 2:57:03  lr: 0.000100  loss: 0.0800 (0.0798)  time: 3.6441  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 650/3449]  eta: 2:56:26  lr: 0.000100  loss: 0.0910 (0.0804)  time: 3.7406  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 660/3449]  eta: 2:55:45  lr: 0.000100  loss: 0.1093 (0.0805)  time: 3.7593  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 670/3449]  eta: 2:55:08  lr: 0.000100  loss: 0.0486 (0.0800)  time: 3.7600  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 680/3449]  eta: 2:54:31  lr: 0.000100  loss: 0.0483 (0.0797)  time: 3.8007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 690/3449]  eta: 2:53:52  lr: 0.000100  loss: 0.0666 (0.0796)  time: 3.7728  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 700/3449]  eta: 2:53:10  lr: 0.000100  loss: 0.0722 (0.0796)  time: 3.7115  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 710/3449]  eta: 2:52:35  lr: 0.000100  loss: 0.0632 (0.0792)  time: 3.7619  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 720/3449]  eta: 2:51:56  lr: 0.000100  loss: 0.0459 (0.0788)  time: 3.8024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 730/3449]  eta: 2:51:19  lr: 0.000100  loss: 0.0487 (0.0788)  time: 3.7738  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 740/3449]  eta: 2:50:42  lr: 0.000100  loss: 0.0965 (0.0791)  time: 3.8066  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 750/3449]  eta: 2:50:04  lr: 0.000100  loss: 0.0965 (0.0791)  time: 3.7902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 760/3449]  eta: 2:49:30  lr: 0.000100  loss: 0.0512 (0.0786)  time: 3.8202  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 770/3449]  eta: 2:48:55  lr: 0.000100  loss: 0.0341 (0.0780)  time: 3.8769  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 780/3449]  eta: 2:48:18  lr: 0.000100  loss: 0.0314 (0.0774)  time: 3.8447  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 790/3449]  eta: 2:47:40  lr: 0.000100  loss: 0.0465 (0.0774)  time: 3.8026  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 800/3449]  eta: 2:47:03  lr: 0.000100  loss: 0.0799 (0.0774)  time: 3.7850  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 810/3449]  eta: 2:46:20  lr: 0.000100  loss: 0.0674 (0.0773)  time: 3.7084  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 820/3449]  eta: 2:45:42  lr: 0.000100  loss: 0.0657 (0.0773)  time: 3.7108  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 830/3449]  eta: 2:45:11  lr: 0.000100  loss: 0.0699 (0.0773)  time: 3.8862  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 840/3449]  eta: 2:44:29  lr: 0.000100  loss: 0.0479 (0.0769)  time: 3.8131  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 850/3449]  eta: 2:43:50  lr: 0.000100  loss: 0.0419 (0.0765)  time: 3.6983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 860/3449]  eta: 2:43:12  lr: 0.000100  loss: 0.0409 (0.0761)  time: 3.7728  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 870/3449]  eta: 2:42:33  lr: 0.000100  loss: 0.0463 (0.0758)  time: 3.7610  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 880/3449]  eta: 2:41:56  lr: 0.000100  loss: 0.0581 (0.0758)  time: 3.7671  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 890/3449]  eta: 2:41:22  lr: 0.000100  loss: 0.0585 (0.0755)  time: 3.8629  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 900/3449]  eta: 2:40:44  lr: 0.000100  loss: 0.0510 (0.0754)  time: 3.8492  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 910/3449]  eta: 2:40:11  lr: 0.000100  loss: 0.0665 (0.0755)  time: 3.8716  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 920/3449]  eta: 2:39:33  lr: 0.000100  loss: 0.0728 (0.0755)  time: 3.8721  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 930/3449]  eta: 2:38:55  lr: 0.000100  loss: 0.0720 (0.0754)  time: 3.7820  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [ 940/3449]  eta: 2:38:18  lr: 0.000100  loss: 0.0737 (0.0754)  time: 3.7997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 950/3449]  eta: 2:37:40  lr: 0.000100  loss: 0.0788 (0.0755)  time: 3.8022  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 960/3449]  eta: 2:37:04  lr: 0.000100  loss: 0.0501 (0.0751)  time: 3.8158  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 970/3449]  eta: 2:36:28  lr: 0.000100  loss: 0.0448 (0.0748)  time: 3.8526  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 980/3449]  eta: 2:35:53  lr: 0.000100  loss: 0.0482 (0.0746)  time: 3.8722  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [ 990/3449]  eta: 2:35:15  lr: 0.000100  loss: 0.0679 (0.0746)  time: 3.8350  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1000/3449]  eta: 2:34:36  lr: 0.000100  loss: 0.0679 (0.0746)  time: 3.7631  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1010/3449]  eta: 2:34:00  lr: 0.000100  loss: 0.0714 (0.0748)  time: 3.8035  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1020/3449]  eta: 2:33:22  lr: 0.000100  loss: 0.1266 (0.0755)  time: 3.8380  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1030/3449]  eta: 2:32:45  lr: 0.000100  loss: 0.1226 (0.0756)  time: 3.8156  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1040/3449]  eta: 2:32:06  lr: 0.000100  loss: 0.0751 (0.0757)  time: 3.7708  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1050/3449]  eta: 2:31:29  lr: 0.000100  loss: 0.0794 (0.0757)  time: 3.7749  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [1060/3449]  eta: 2:30:52  lr: 0.000100  loss: 0.0619 (0.0756)  time: 3.8392  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1070/3449]  eta: 2:30:14  lr: 0.000100  loss: 0.0635 (0.0755)  time: 3.8178  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1080/3449]  eta: 2:29:36  lr: 0.000100  loss: 0.0498 (0.0752)  time: 3.7701  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1090/3449]  eta: 2:28:58  lr: 0.000100  loss: 0.0459 (0.0750)  time: 3.7881  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1100/3449]  eta: 2:28:21  lr: 0.000100  loss: 0.0431 (0.0747)  time: 3.8179  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1110/3449]  eta: 2:27:42  lr: 0.000100  loss: 0.0395 (0.0744)  time: 3.7777  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1120/3449]  eta: 2:27:05  lr: 0.000100  loss: 0.0355 (0.0740)  time: 3.7843  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1130/3449]  eta: 2:26:27  lr: 0.000100  loss: 0.0442 (0.0739)  time: 3.7987  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1140/3449]  eta: 2:25:51  lr: 0.000100  loss: 0.0728 (0.0740)  time: 3.8410  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1150/3449]  eta: 2:25:12  lr: 0.000100  loss: 0.0697 (0.0739)  time: 3.8132  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1160/3449]  eta: 2:24:35  lr: 0.000100  loss: 0.0693 (0.0739)  time: 3.7882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1170/3449]  eta: 2:23:58  lr: 0.000100  loss: 0.0763 (0.0739)  time: 3.8436  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1180/3449]  eta: 2:23:21  lr: 0.000100  loss: 0.0965 (0.0743)  time: 3.8400  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1190/3449]  eta: 2:22:42  lr: 0.000100  loss: 0.1126 (0.0746)  time: 3.7891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1200/3449]  eta: 2:22:05  lr: 0.000100  loss: 0.0976 (0.0748)  time: 3.7741  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1210/3449]  eta: 2:21:28  lr: 0.000100  loss: 0.0927 (0.0748)  time: 3.8228  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1220/3449]  eta: 2:20:49  lr: 0.000100  loss: 0.0778 (0.0750)  time: 3.7745  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1230/3449]  eta: 2:20:10  lr: 0.000100  loss: 0.1096 (0.0754)  time: 3.7447  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1240/3449]  eta: 2:19:31  lr: 0.000100  loss: 0.1094 (0.0756)  time: 3.7377  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1250/3449]  eta: 2:18:52  lr: 0.000100  loss: 0.0979 (0.0758)  time: 3.7231  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1260/3449]  eta: 2:18:15  lr: 0.000100  loss: 0.0809 (0.0758)  time: 3.7751  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1270/3449]  eta: 2:17:36  lr: 0.000100  loss: 0.0662 (0.0757)  time: 3.7916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1280/3449]  eta: 2:17:00  lr: 0.000100  loss: 0.0582 (0.0755)  time: 3.8179  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1290/3449]  eta: 2:16:22  lr: 0.000100  loss: 0.0626 (0.0755)  time: 3.8342  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1300/3449]  eta: 2:15:44  lr: 0.000100  loss: 0.0821 (0.0756)  time: 3.7762  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1310/3449]  eta: 2:15:06  lr: 0.000100  loss: 0.0943 (0.0758)  time: 3.7822  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1320/3449]  eta: 2:14:27  lr: 0.000100  loss: 0.1198 (0.0763)  time: 3.7709  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1330/3449]  eta: 2:13:48  lr: 0.000100  loss: 0.1253 (0.0767)  time: 3.7260  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1340/3449]  eta: 2:13:11  lr: 0.000100  loss: 0.1032 (0.0769)  time: 3.7802  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1350/3449]  eta: 2:12:34  lr: 0.000100  loss: 0.0990 (0.0772)  time: 3.8288  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1360/3449]  eta: 2:11:55  lr: 0.000100  loss: 0.1326 (0.0776)  time: 3.7845  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1370/3449]  eta: 2:11:16  lr: 0.000100  loss: 0.1130 (0.0778)  time: 3.7430  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1380/3449]  eta: 2:10:38  lr: 0.000100  loss: 0.0899 (0.0778)  time: 3.7434  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1390/3449]  eta: 2:10:01  lr: 0.000100  loss: 0.0605 (0.0776)  time: 3.7856  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1400/3449]  eta: 2:09:22  lr: 0.000100  loss: 0.0608 (0.0776)  time: 3.7668  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1410/3449]  eta: 2:08:41  lr: 0.000100  loss: 0.0641 (0.0774)  time: 3.6491  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1420/3449]  eta: 2:08:01  lr: 0.000100  loss: 0.0492 (0.0772)  time: 3.6140  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1430/3449]  eta: 2:07:21  lr: 0.000100  loss: 0.0566 (0.0772)  time: 3.6362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1440/3449]  eta: 2:06:40  lr: 0.000100  loss: 0.0753 (0.0776)  time: 3.6169  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1450/3449]  eta: 2:06:00  lr: 0.000100  loss: 0.0924 (0.0776)  time: 3.5996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1460/3449]  eta: 2:05:21  lr: 0.000100  loss: 0.0550 (0.0774)  time: 3.6288  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1470/3449]  eta: 2:04:41  lr: 0.000100  loss: 0.0447 (0.0772)  time: 3.6454  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1480/3449]  eta: 2:04:01  lr: 0.000100  loss: 0.0478 (0.0771)  time: 3.6358  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1490/3449]  eta: 2:03:22  lr: 0.000100  loss: 0.0628 (0.0771)  time: 3.6567  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1500/3449]  eta: 2:02:42  lr: 0.000100  loss: 0.0825 (0.0773)  time: 3.6295  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1510/3449]  eta: 2:02:02  lr: 0.000100  loss: 0.0761 (0.0772)  time: 3.6163  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1520/3449]  eta: 2:01:23  lr: 0.000100  loss: 0.0761 (0.0774)  time: 3.6474  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1530/3449]  eta: 2:00:43  lr: 0.000100  loss: 0.0967 (0.0776)  time: 3.6272  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1540/3449]  eta: 2:00:04  lr: 0.000100  loss: 0.0881 (0.0777)  time: 3.6480  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1550/3449]  eta: 1:59:25  lr: 0.000100  loss: 0.1084 (0.0781)  time: 3.6640  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1560/3449]  eta: 1:58:46  lr: 0.000100  loss: 0.1103 (0.0782)  time: 3.6838  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1570/3449]  eta: 1:58:07  lr: 0.000100  loss: 0.0750 (0.0782)  time: 3.6946  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1580/3449]  eta: 1:57:28  lr: 0.000100  loss: 0.0635 (0.0782)  time: 3.6442  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1590/3449]  eta: 1:56:49  lr: 0.000100  loss: 0.0661 (0.0781)  time: 3.6601  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1600/3449]  eta: 1:56:10  lr: 0.000100  loss: 0.0611 (0.0779)  time: 3.6753  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1610/3449]  eta: 1:55:32  lr: 0.000100  loss: 0.0710 (0.0780)  time: 3.6982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1620/3449]  eta: 1:54:54  lr: 0.000100  loss: 0.0784 (0.0780)  time: 3.7194  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1630/3449]  eta: 1:54:15  lr: 0.000100  loss: 0.0629 (0.0779)  time: 3.7121  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1640/3449]  eta: 1:53:37  lr: 0.000100  loss: 0.0550 (0.0778)  time: 3.7312  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1650/3449]  eta: 1:53:00  lr: 0.000100  loss: 0.0587 (0.0778)  time: 3.7513  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1660/3449]  eta: 1:52:21  lr: 0.000100  loss: 0.0694 (0.0777)  time: 3.7325  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1670/3449]  eta: 1:51:41  lr: 0.000100  loss: 0.0683 (0.0776)  time: 3.6263  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1680/3449]  eta: 1:51:04  lr: 0.000100  loss: 0.0630 (0.0775)  time: 3.6563  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1690/3449]  eta: 1:50:26  lr: 0.000100  loss: 0.0681 (0.0775)  time: 3.7877  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1700/3449]  eta: 1:49:48  lr: 0.000100  loss: 0.0765 (0.0775)  time: 3.7686  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1710/3449]  eta: 1:49:11  lr: 0.000100  loss: 0.0879 (0.0776)  time: 3.7440  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [1720/3449]  eta: 1:48:32  lr: 0.000100  loss: 0.0976 (0.0779)  time: 3.7378  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1730/3449]  eta: 1:47:54  lr: 0.000100  loss: 0.0763 (0.0778)  time: 3.7276  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1740/3449]  eta: 1:47:17  lr: 0.000100  loss: 0.0686 (0.0778)  time: 3.7640  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1750/3449]  eta: 1:46:39  lr: 0.000100  loss: 0.0524 (0.0776)  time: 3.7753  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1760/3449]  eta: 1:46:01  lr: 0.000100  loss: 0.0542 (0.0775)  time: 3.7431  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1770/3449]  eta: 1:45:24  lr: 0.000100  loss: 0.0543 (0.0774)  time: 3.7446  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1780/3449]  eta: 1:44:45  lr: 0.000100  loss: 0.0611 (0.0774)  time: 3.7365  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1790/3449]  eta: 1:44:07  lr: 0.000100  loss: 0.0592 (0.0772)  time: 3.6994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1800/3449]  eta: 1:43:29  lr: 0.000100  loss: 0.0474 (0.0771)  time: 3.6961  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1810/3449]  eta: 1:42:51  lr: 0.000100  loss: 0.0389 (0.0769)  time: 3.7204  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1820/3449]  eta: 1:42:13  lr: 0.000100  loss: 0.0398 (0.0767)  time: 3.7377  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1830/3449]  eta: 1:41:35  lr: 0.000100  loss: 0.0434 (0.0765)  time: 3.7275  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1840/3449]  eta: 1:40:57  lr: 0.000100  loss: 0.0741 (0.0767)  time: 3.7089  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1850/3449]  eta: 1:40:19  lr: 0.000100  loss: 0.0929 (0.0767)  time: 3.7230  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1860/3449]  eta: 1:39:41  lr: 0.000100  loss: 0.0786 (0.0767)  time: 3.7447  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1870/3449]  eta: 1:39:03  lr: 0.000100  loss: 0.0478 (0.0765)  time: 3.7392  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1880/3449]  eta: 1:38:26  lr: 0.000100  loss: 0.0411 (0.0765)  time: 3.7728  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1890/3449]  eta: 1:37:48  lr: 0.000100  loss: 0.1084 (0.0768)  time: 3.7796  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1900/3449]  eta: 1:37:10  lr: 0.000100  loss: 0.1059 (0.0767)  time: 3.7460  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1910/3449]  eta: 1:36:32  lr: 0.000100  loss: 0.0415 (0.0765)  time: 3.7375  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1920/3449]  eta: 1:35:55  lr: 0.000100  loss: 0.0431 (0.0764)  time: 3.7679  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1930/3449]  eta: 1:35:17  lr: 0.000100  loss: 0.0575 (0.0763)  time: 3.7732  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1940/3449]  eta: 1:34:39  lr: 0.000100  loss: 0.0721 (0.0763)  time: 3.6900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1950/3449]  eta: 1:34:01  lr: 0.000100  loss: 0.0655 (0.0762)  time: 3.6868  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [1960/3449]  eta: 1:33:23  lr: 0.000100  loss: 0.0577 (0.0761)  time: 3.7452  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1970/3449]  eta: 1:32:45  lr: 0.000100  loss: 0.0513 (0.0760)  time: 3.7611  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1980/3449]  eta: 1:32:08  lr: 0.000100  loss: 0.0425 (0.0758)  time: 3.7693  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [1990/3449]  eta: 1:31:30  lr: 0.000100  loss: 0.0378 (0.0757)  time: 3.7740  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2000/3449]  eta: 1:30:52  lr: 0.000100  loss: 0.0306 (0.0754)  time: 3.7369  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2010/3449]  eta: 1:30:14  lr: 0.000100  loss: 0.0324 (0.0753)  time: 3.7071  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2020/3449]  eta: 1:29:37  lr: 0.000100  loss: 0.0348 (0.0751)  time: 3.7406  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2030/3449]  eta: 1:28:58  lr: 0.000100  loss: 0.0349 (0.0749)  time: 3.6829  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2040/3449]  eta: 1:28:20  lr: 0.000100  loss: 0.0366 (0.0747)  time: 3.6417  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2050/3449]  eta: 1:27:42  lr: 0.000100  loss: 0.0366 (0.0745)  time: 3.6994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2060/3449]  eta: 1:27:04  lr: 0.000100  loss: 0.0357 (0.0743)  time: 3.7428  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2070/3449]  eta: 1:26:26  lr: 0.000100  loss: 0.0363 (0.0742)  time: 3.7571  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2080/3449]  eta: 1:25:48  lr: 0.000100  loss: 0.0368 (0.0740)  time: 3.7062  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2090/3449]  eta: 1:25:11  lr: 0.000100  loss: 0.0538 (0.0741)  time: 3.7039  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2100/3449]  eta: 1:24:33  lr: 0.000100  loss: 0.0732 (0.0741)  time: 3.7530  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2110/3449]  eta: 1:23:55  lr: 0.000100  loss: 0.0659 (0.0740)  time: 3.7339  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2120/3449]  eta: 1:23:17  lr: 0.000100  loss: 0.0691 (0.0740)  time: 3.6733  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2130/3449]  eta: 1:22:39  lr: 0.000100  loss: 0.0696 (0.0740)  time: 3.6610  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2140/3449]  eta: 1:22:01  lr: 0.000100  loss: 0.0731 (0.0740)  time: 3.7207  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2150/3449]  eta: 1:21:23  lr: 0.000100  loss: 0.0680 (0.0739)  time: 3.7284  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2160/3449]  eta: 1:20:45  lr: 0.000100  loss: 0.0528 (0.0738)  time: 3.7123  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2170/3449]  eta: 1:20:07  lr: 0.000100  loss: 0.0515 (0.0738)  time: 3.6890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2180/3449]  eta: 1:19:29  lr: 0.000100  loss: 0.0691 (0.0738)  time: 3.7038  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2190/3449]  eta: 1:18:52  lr: 0.000100  loss: 0.0723 (0.0738)  time: 3.7531  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2200/3449]  eta: 1:18:14  lr: 0.000100  loss: 0.0446 (0.0736)  time: 3.7588  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2210/3449]  eta: 1:17:36  lr: 0.000100  loss: 0.0447 (0.0736)  time: 3.7348  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2220/3449]  eta: 1:16:58  lr: 0.000100  loss: 0.0780 (0.0737)  time: 3.7127  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2230/3449]  eta: 1:16:21  lr: 0.000100  loss: 0.0872 (0.0737)  time: 3.7150  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2240/3449]  eta: 1:15:43  lr: 0.000100  loss: 0.0568 (0.0735)  time: 3.7200  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2250/3449]  eta: 1:15:05  lr: 0.000100  loss: 0.0512 (0.0735)  time: 3.7391  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2260/3449]  eta: 1:14:28  lr: 0.000100  loss: 0.0512 (0.0734)  time: 3.7600  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2270/3449]  eta: 1:13:50  lr: 0.000100  loss: 0.0461 (0.0732)  time: 3.7523  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2280/3449]  eta: 1:13:12  lr: 0.000100  loss: 0.0412 (0.0731)  time: 3.7304  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2290/3449]  eta: 1:12:35  lr: 0.000100  loss: 0.0418 (0.0730)  time: 3.7366  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2300/3449]  eta: 1:11:57  lr: 0.000100  loss: 0.0415 (0.0729)  time: 3.7499  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2310/3449]  eta: 1:11:20  lr: 0.000100  loss: 0.0406 (0.0727)  time: 3.7800  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2320/3449]  eta: 1:10:42  lr: 0.000100  loss: 0.0605 (0.0728)  time: 3.7815  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2330/3449]  eta: 1:10:04  lr: 0.000100  loss: 0.0762 (0.0728)  time: 3.7422  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2340/3449]  eta: 1:09:27  lr: 0.000100  loss: 0.0819 (0.0729)  time: 3.7255  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2350/3449]  eta: 1:08:49  lr: 0.000100  loss: 0.0815 (0.0729)  time: 3.6787  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2360/3449]  eta: 1:08:11  lr: 0.000100  loss: 0.0875 (0.0730)  time: 3.6850  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2370/3449]  eta: 1:07:33  lr: 0.000100  loss: 0.0823 (0.0730)  time: 3.7247  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [2380/3449]  eta: 1:06:55  lr: 0.000100  loss: 0.0682 (0.0729)  time: 3.7223  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2390/3449]  eta: 1:06:18  lr: 0.000100  loss: 0.0708 (0.0729)  time: 3.7227  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2400/3449]  eta: 1:05:40  lr: 0.000100  loss: 0.0620 (0.0729)  time: 3.7201  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2410/3449]  eta: 1:05:02  lr: 0.000100  loss: 0.0403 (0.0727)  time: 3.7475  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2420/3449]  eta: 1:04:25  lr: 0.000100  loss: 0.0403 (0.0726)  time: 3.7650  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2430/3449]  eta: 1:03:47  lr: 0.000100  loss: 0.0419 (0.0725)  time: 3.7705  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2440/3449]  eta: 1:03:10  lr: 0.000100  loss: 0.0579 (0.0725)  time: 3.7720  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2450/3449]  eta: 1:02:32  lr: 0.000100  loss: 0.0733 (0.0725)  time: 3.7605  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2460/3449]  eta: 1:01:55  lr: 0.000100  loss: 0.0837 (0.0726)  time: 3.7606  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2470/3449]  eta: 1:01:17  lr: 0.000100  loss: 0.0812 (0.0726)  time: 3.7211  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2480/3449]  eta: 1:00:39  lr: 0.000100  loss: 0.0555 (0.0725)  time: 3.6984  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2490/3449]  eta: 1:00:01  lr: 0.000100  loss: 0.0557 (0.0726)  time: 3.6720  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2500/3449]  eta: 0:59:24  lr: 0.000100  loss: 0.0747 (0.0725)  time: 3.6925  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2510/3449]  eta: 0:58:46  lr: 0.000100  loss: 0.0715 (0.0726)  time: 3.7512  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2520/3449]  eta: 0:58:08  lr: 0.000100  loss: 0.1246 (0.0729)  time: 3.7525  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2530/3449]  eta: 0:57:31  lr: 0.000100  loss: 0.1209 (0.0730)  time: 3.7917  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2540/3449]  eta: 0:56:53  lr: 0.000100  loss: 0.0689 (0.0729)  time: 3.7256  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2550/3449]  eta: 0:56:15  lr: 0.000100  loss: 0.0608 (0.0729)  time: 3.6353  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2560/3449]  eta: 0:55:38  lr: 0.000100  loss: 0.0879 (0.0730)  time: 3.7124  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2570/3449]  eta: 0:55:00  lr: 0.000100  loss: 0.0944 (0.0731)  time: 3.7497  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2580/3449]  eta: 0:54:22  lr: 0.000100  loss: 0.0589 (0.0730)  time: 3.6762  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2590/3449]  eta: 0:53:45  lr: 0.000100  loss: 0.0455 (0.0729)  time: 3.6916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2600/3449]  eta: 0:53:07  lr: 0.000100  loss: 0.0465 (0.0729)  time: 3.7595  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2610/3449]  eta: 0:52:29  lr: 0.000100  loss: 0.0656 (0.0729)  time: 3.7250  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2620/3449]  eta: 0:51:51  lr: 0.000100  loss: 0.0725 (0.0730)  time: 3.6722  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2630/3449]  eta: 0:51:14  lr: 0.000100  loss: 0.1075 (0.0732)  time: 3.7122  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2640/3449]  eta: 0:50:36  lr: 0.000100  loss: 0.1106 (0.0733)  time: 3.7325  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2650/3449]  eta: 0:49:59  lr: 0.000100  loss: 0.0616 (0.0732)  time: 3.7258  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2660/3449]  eta: 0:49:21  lr: 0.000100  loss: 0.0541 (0.0731)  time: 3.6995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2670/3449]  eta: 0:48:43  lr: 0.000100  loss: 0.0582 (0.0731)  time: 3.7104  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2680/3449]  eta: 0:48:06  lr: 0.000100  loss: 0.0562 (0.0730)  time: 3.7515  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2690/3449]  eta: 0:47:28  lr: 0.000100  loss: 0.0447 (0.0729)  time: 3.7649  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2700/3449]  eta: 0:46:51  lr: 0.000100  loss: 0.0485 (0.0729)  time: 3.7467  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2710/3449]  eta: 0:46:13  lr: 0.000100  loss: 0.0581 (0.0728)  time: 3.6841  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2720/3449]  eta: 0:45:35  lr: 0.000100  loss: 0.0462 (0.0727)  time: 3.6844  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2730/3449]  eta: 0:44:58  lr: 0.000100  loss: 0.0448 (0.0726)  time: 3.7239  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2740/3449]  eta: 0:44:20  lr: 0.000100  loss: 0.0464 (0.0727)  time: 3.7112  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2750/3449]  eta: 0:43:42  lr: 0.000100  loss: 0.1055 (0.0729)  time: 3.6883  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2760/3449]  eta: 0:43:05  lr: 0.000100  loss: 0.0943 (0.0729)  time: 3.7539  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2770/3449]  eta: 0:42:27  lr: 0.000100  loss: 0.0892 (0.0731)  time: 3.7383  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2780/3449]  eta: 0:41:50  lr: 0.000100  loss: 0.1031 (0.0732)  time: 3.6780  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2790/3449]  eta: 0:41:12  lr: 0.000100  loss: 0.1223 (0.0735)  time: 3.7342  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2800/3449]  eta: 0:40:34  lr: 0.000100  loss: 0.1265 (0.0737)  time: 3.7558  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2810/3449]  eta: 0:39:57  lr: 0.000100  loss: 0.0944 (0.0737)  time: 3.7127  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2820/3449]  eta: 0:39:19  lr: 0.000100  loss: 0.0894 (0.0738)  time: 3.7186  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2830/3449]  eta: 0:38:42  lr: 0.000100  loss: 0.0719 (0.0737)  time: 3.7219  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2840/3449]  eta: 0:38:04  lr: 0.000100  loss: 0.0719 (0.0738)  time: 3.6728  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2850/3449]  eta: 0:37:26  lr: 0.000100  loss: 0.0996 (0.0740)  time: 3.6992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2860/3449]  eta: 0:36:49  lr: 0.000100  loss: 0.1331 (0.0742)  time: 3.7438  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2870/3449]  eta: 0:36:11  lr: 0.000100  loss: 0.1113 (0.0743)  time: 3.7256  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2880/3449]  eta: 0:35:34  lr: 0.000100  loss: 0.1142 (0.0745)  time: 3.7084  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2890/3449]  eta: 0:34:56  lr: 0.000100  loss: 0.0836 (0.0745)  time: 3.6701  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2900/3449]  eta: 0:34:18  lr: 0.000100  loss: 0.0493 (0.0744)  time: 3.6350  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2910/3449]  eta: 0:33:41  lr: 0.000100  loss: 0.0393 (0.0743)  time: 3.6599  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2920/3449]  eta: 0:33:03  lr: 0.000100  loss: 0.0378 (0.0742)  time: 3.6818  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2930/3449]  eta: 0:32:25  lr: 0.000100  loss: 0.0393 (0.0740)  time: 3.6563  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2940/3449]  eta: 0:31:48  lr: 0.000100  loss: 0.0393 (0.0739)  time: 3.6497  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2950/3449]  eta: 0:31:10  lr: 0.000100  loss: 0.0336 (0.0738)  time: 3.6436  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2960/3449]  eta: 0:30:32  lr: 0.000100  loss: 0.0324 (0.0737)  time: 3.6305  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [2970/3449]  eta: 0:29:55  lr: 0.000100  loss: 0.0335 (0.0736)  time: 3.6447  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2980/3449]  eta: 0:29:17  lr: 0.000100  loss: 0.0579 (0.0736)  time: 3.6777  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [2990/3449]  eta: 0:28:39  lr: 0.000100  loss: 0.0950 (0.0738)  time: 3.6722  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3000/3449]  eta: 0:28:02  lr: 0.000100  loss: 0.1140 (0.0739)  time: 3.6983  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3010/3449]  eta: 0:27:24  lr: 0.000100  loss: 0.0814 (0.0738)  time: 3.7008  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3020/3449]  eta: 0:26:47  lr: 0.000100  loss: 0.0572 (0.0738)  time: 3.6450  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3030/3449]  eta: 0:26:09  lr: 0.000100  loss: 0.0528 (0.0737)  time: 3.6983  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [3040/3449]  eta: 0:25:32  lr: 0.000100  loss: 0.0718 (0.0738)  time: 3.6807  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3050/3449]  eta: 0:24:54  lr: 0.000100  loss: 0.0743 (0.0738)  time: 3.6152  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3060/3449]  eta: 0:24:17  lr: 0.000100  loss: 0.0575 (0.0737)  time: 3.6612  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3070/3449]  eta: 0:23:39  lr: 0.000100  loss: 0.0637 (0.0737)  time: 3.6923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3080/3449]  eta: 0:23:01  lr: 0.000100  loss: 0.0675 (0.0737)  time: 3.6873  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3090/3449]  eta: 0:22:24  lr: 0.000100  loss: 0.0771 (0.0737)  time: 3.7323  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3100/3449]  eta: 0:21:46  lr: 0.000100  loss: 0.0829 (0.0737)  time: 3.6761  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3110/3449]  eta: 0:21:09  lr: 0.000100  loss: 0.0783 (0.0737)  time: 3.6181  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3120/3449]  eta: 0:20:31  lr: 0.000100  loss: 0.0632 (0.0737)  time: 3.6690  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3130/3449]  eta: 0:19:54  lr: 0.000100  loss: 0.0473 (0.0736)  time: 3.6625  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3140/3449]  eta: 0:19:16  lr: 0.000100  loss: 0.0691 (0.0737)  time: 3.6592  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3150/3449]  eta: 0:18:39  lr: 0.000100  loss: 0.0712 (0.0736)  time: 3.6696  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3160/3449]  eta: 0:18:01  lr: 0.000100  loss: 0.0447 (0.0735)  time: 3.6461  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3170/3449]  eta: 0:17:24  lr: 0.000100  loss: 0.0459 (0.0735)  time: 3.6671  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3180/3449]  eta: 0:16:46  lr: 0.000100  loss: 0.0602 (0.0735)  time: 3.7196  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3190/3449]  eta: 0:16:09  lr: 0.000100  loss: 0.0606 (0.0734)  time: 3.6971  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3200/3449]  eta: 0:15:31  lr: 0.000100  loss: 0.0510 (0.0734)  time: 3.6631  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3210/3449]  eta: 0:14:54  lr: 0.000100  loss: 0.0450 (0.0733)  time: 3.6781  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3220/3449]  eta: 0:14:16  lr: 0.000100  loss: 0.0426 (0.0732)  time: 3.6926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3230/3449]  eta: 0:13:39  lr: 0.000100  loss: 0.0418 (0.0731)  time: 3.6719  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3240/3449]  eta: 0:13:02  lr: 0.000100  loss: 0.0388 (0.0730)  time: 3.6966  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3250/3449]  eta: 0:12:24  lr: 0.000100  loss: 0.0386 (0.0729)  time: 3.6889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3260/3449]  eta: 0:11:47  lr: 0.000100  loss: 0.0429 (0.0729)  time: 3.6660  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3270/3449]  eta: 0:11:09  lr: 0.000100  loss: 0.0706 (0.0728)  time: 3.6842  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3280/3449]  eta: 0:10:32  lr: 0.000100  loss: 0.0706 (0.0729)  time: 3.6589  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3290/3449]  eta: 0:09:54  lr: 0.000100  loss: 0.1030 (0.0731)  time: 3.6514  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3300/3449]  eta: 0:09:17  lr: 0.000100  loss: 0.0680 (0.0730)  time: 3.6961  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3310/3449]  eta: 0:08:39  lr: 0.000100  loss: 0.0608 (0.0730)  time: 3.6947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3320/3449]  eta: 0:08:02  lr: 0.000100  loss: 0.0630 (0.0731)  time: 3.6367  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3330/3449]  eta: 0:07:25  lr: 0.000100  loss: 0.0952 (0.0732)  time: 3.6603  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3340/3449]  eta: 0:06:47  lr: 0.000100  loss: 0.1296 (0.0733)  time: 3.7170  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3350/3449]  eta: 0:06:10  lr: 0.000100  loss: 0.1150 (0.0734)  time: 3.7031  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3360/3449]  eta: 0:05:32  lr: 0.000100  loss: 0.0793 (0.0734)  time: 3.6694  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3370/3449]  eta: 0:04:55  lr: 0.000100  loss: 0.0790 (0.0734)  time: 3.6741  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3380/3449]  eta: 0:04:18  lr: 0.000100  loss: 0.0782 (0.0734)  time: 3.6920  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3390/3449]  eta: 0:03:40  lr: 0.000100  loss: 0.0793 (0.0734)  time: 3.6782  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3400/3449]  eta: 0:03:03  lr: 0.000100  loss: 0.0814 (0.0734)  time: 3.6447  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3410/3449]  eta: 0:02:25  lr: 0.000100  loss: 0.0723 (0.0734)  time: 3.6304  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3420/3449]  eta: 0:01:48  lr: 0.000100  loss: 0.0615 (0.0734)  time: 3.6410  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:21]  [3430/3449]  eta: 0:01:11  lr: 0.000100  loss: 0.0484 (0.0733)  time: 3.6470  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3440/3449]  eta: 0:00:33  lr: 0.000100  loss: 0.0565 (0.0733)  time: 3.6555  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0697 (0.0733)  time: 3.6383  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:21] Total time: 3:34:50 (3.7374 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0697 (0.0733)\n",
      "Valid: [epoch:21]  [ 0/14]  eta: 0:04:22  loss: 0.0725 (0.0725)  time: 18.7329  data: 0.5250  max mem: 34968\n",
      "Valid: [epoch:21]  [13/14]  eta: 0:00:18  loss: 0.0725 (0.0723)  time: 18.2472  data: 0.0377  max mem: 34968\n",
      "Valid: [epoch:21] Total time: 0:04:15 (18.2596 s / it)\n",
      "Averaged stats: loss: 0.0725 (0.0723)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_21_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.072%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:22]  [   0/3449]  eta: 4:47:27  lr: 0.000100  loss: 0.1040 (0.1040)  time: 5.0006  data: 1.5488  max mem: 34968\n",
      "Train: [epoch:22]  [  10/3449]  eta: 3:38:48  lr: 0.000100  loss: 0.0925 (0.0872)  time: 3.8176  data: 0.1409  max mem: 34968\n",
      "Train: [epoch:22]  [  20/3449]  eta: 3:33:29  lr: 0.000100  loss: 0.0838 (0.0828)  time: 3.6722  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [  30/3449]  eta: 3:31:01  lr: 0.000100  loss: 0.0786 (0.0827)  time: 3.6402  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [  40/3449]  eta: 3:29:47  lr: 0.000100  loss: 0.0732 (0.0784)  time: 3.6471  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [  50/3449]  eta: 3:28:54  lr: 0.000100  loss: 0.0687 (0.0785)  time: 3.6637  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [  60/3449]  eta: 3:27:49  lr: 0.000100  loss: 0.0761 (0.0787)  time: 3.6526  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [  70/3449]  eta: 3:27:06  lr: 0.000100  loss: 0.0761 (0.0774)  time: 3.6516  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [  80/3449]  eta: 3:26:45  lr: 0.000100  loss: 0.0666 (0.0757)  time: 3.6908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [  90/3449]  eta: 3:26:06  lr: 0.000100  loss: 0.0740 (0.0772)  time: 3.6964  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 100/3449]  eta: 3:25:37  lr: 0.000100  loss: 0.0556 (0.0744)  time: 3.6907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 110/3449]  eta: 3:25:00  lr: 0.000100  loss: 0.0556 (0.0788)  time: 3.6944  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 120/3449]  eta: 3:24:20  lr: 0.000100  loss: 0.1567 (0.0874)  time: 3.6788  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 130/3449]  eta: 3:23:34  lr: 0.000100  loss: 0.1551 (0.0911)  time: 3.6599  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 140/3449]  eta: 3:23:11  lr: 0.000100  loss: 0.1260 (0.0930)  time: 3.6932  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 150/3449]  eta: 3:22:31  lr: 0.000100  loss: 0.0994 (0.0923)  time: 3.7028  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 160/3449]  eta: 3:21:51  lr: 0.000100  loss: 0.0771 (0.0911)  time: 3.6686  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 170/3449]  eta: 3:21:28  lr: 0.000100  loss: 0.0632 (0.0895)  time: 3.7127  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 180/3449]  eta: 3:20:41  lr: 0.000100  loss: 0.0503 (0.0870)  time: 3.6912  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:22]  [ 190/3449]  eta: 3:19:57  lr: 0.000100  loss: 0.0491 (0.0851)  time: 3.6341  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 200/3449]  eta: 3:19:11  lr: 0.000100  loss: 0.0592 (0.0854)  time: 3.6353  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 210/3449]  eta: 3:18:27  lr: 0.000100  loss: 0.0921 (0.0858)  time: 3.6286  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 220/3449]  eta: 3:17:48  lr: 0.000100  loss: 0.0988 (0.0869)  time: 3.6459  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 230/3449]  eta: 3:17:11  lr: 0.000100  loss: 0.1115 (0.0877)  time: 3.6689  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 240/3449]  eta: 3:16:27  lr: 0.000100  loss: 0.0835 (0.0870)  time: 3.6467  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 250/3449]  eta: 3:15:55  lr: 0.000100  loss: 0.0705 (0.0869)  time: 3.6667  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 260/3449]  eta: 3:15:19  lr: 0.000100  loss: 0.0681 (0.0863)  time: 3.6943  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 270/3449]  eta: 3:14:37  lr: 0.000100  loss: 0.0661 (0.0860)  time: 3.6524  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 280/3449]  eta: 3:13:53  lr: 0.000100  loss: 0.0650 (0.0847)  time: 3.6215  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 290/3449]  eta: 3:13:17  lr: 0.000100  loss: 0.0437 (0.0833)  time: 3.6429  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 300/3449]  eta: 3:12:47  lr: 0.000100  loss: 0.0456 (0.0820)  time: 3.7039  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 310/3449]  eta: 3:12:06  lr: 0.000100  loss: 0.0404 (0.0805)  time: 3.6855  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 320/3449]  eta: 3:11:23  lr: 0.000100  loss: 0.0404 (0.0800)  time: 3.6220  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 330/3449]  eta: 3:10:43  lr: 0.000100  loss: 0.0787 (0.0802)  time: 3.6202  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 340/3449]  eta: 3:09:59  lr: 0.000100  loss: 0.0809 (0.0802)  time: 3.6135  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 350/3449]  eta: 3:09:26  lr: 0.000100  loss: 0.0763 (0.0803)  time: 3.6478  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 360/3449]  eta: 3:08:55  lr: 0.000100  loss: 0.0760 (0.0800)  time: 3.7186  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 370/3449]  eta: 3:08:20  lr: 0.000100  loss: 0.0821 (0.0813)  time: 3.7116  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 380/3449]  eta: 3:07:51  lr: 0.000100  loss: 0.1481 (0.0827)  time: 3.7267  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 390/3449]  eta: 3:07:18  lr: 0.000100  loss: 0.1031 (0.0829)  time: 3.7439  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 400/3449]  eta: 3:06:41  lr: 0.000100  loss: 0.0593 (0.0819)  time: 3.6961  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 410/3449]  eta: 3:06:02  lr: 0.000100  loss: 0.0589 (0.0817)  time: 3.6596  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 420/3449]  eta: 3:05:33  lr: 0.000100  loss: 0.0779 (0.0817)  time: 3.7139  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 430/3449]  eta: 3:05:00  lr: 0.000100  loss: 0.0779 (0.0815)  time: 3.7530  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 440/3449]  eta: 3:04:33  lr: 0.000100  loss: 0.0660 (0.0811)  time: 3.7754  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 450/3449]  eta: 3:04:02  lr: 0.000100  loss: 0.0633 (0.0809)  time: 3.7974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 460/3449]  eta: 3:03:39  lr: 0.000100  loss: 0.0633 (0.0813)  time: 3.8301  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 470/3449]  eta: 3:03:07  lr: 0.000100  loss: 0.0936 (0.0825)  time: 3.8221  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 480/3449]  eta: 3:02:39  lr: 0.000100  loss: 0.0894 (0.0823)  time: 3.7960  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 490/3449]  eta: 3:02:12  lr: 0.000100  loss: 0.0773 (0.0823)  time: 3.8440  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 500/3449]  eta: 3:01:38  lr: 0.000100  loss: 0.0973 (0.0836)  time: 3.8042  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 510/3449]  eta: 3:01:10  lr: 0.000100  loss: 0.1494 (0.0852)  time: 3.8008  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 520/3449]  eta: 3:00:40  lr: 0.000100  loss: 0.1407 (0.0860)  time: 3.8337  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 530/3449]  eta: 3:00:05  lr: 0.000100  loss: 0.0983 (0.0861)  time: 3.7834  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 540/3449]  eta: 2:59:27  lr: 0.000100  loss: 0.0786 (0.0859)  time: 3.7107  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 550/3449]  eta: 2:58:52  lr: 0.000100  loss: 0.0730 (0.0857)  time: 3.7101  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 560/3449]  eta: 2:58:25  lr: 0.000100  loss: 0.0704 (0.0855)  time: 3.8186  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 570/3449]  eta: 2:57:57  lr: 0.000100  loss: 0.0730 (0.0854)  time: 3.8934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 580/3449]  eta: 2:57:22  lr: 0.000100  loss: 0.0551 (0.0847)  time: 3.8156  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 590/3449]  eta: 2:56:45  lr: 0.000100  loss: 0.0431 (0.0847)  time: 3.7291  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 600/3449]  eta: 2:56:16  lr: 0.000100  loss: 0.1018 (0.0851)  time: 3.8023  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 610/3449]  eta: 2:55:36  lr: 0.000100  loss: 0.1018 (0.0852)  time: 3.7619  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 620/3449]  eta: 2:55:00  lr: 0.000100  loss: 0.1005 (0.0856)  time: 3.6946  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 630/3449]  eta: 2:54:29  lr: 0.000100  loss: 0.0767 (0.0853)  time: 3.7928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 640/3449]  eta: 2:53:55  lr: 0.000100  loss: 0.0629 (0.0851)  time: 3.8143  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 650/3449]  eta: 2:53:22  lr: 0.000100  loss: 0.0610 (0.0846)  time: 3.7975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 660/3449]  eta: 2:52:45  lr: 0.000100  loss: 0.0479 (0.0843)  time: 3.7686  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 670/3449]  eta: 2:52:14  lr: 0.000100  loss: 0.0443 (0.0837)  time: 3.7907  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 680/3449]  eta: 2:51:41  lr: 0.000100  loss: 0.0404 (0.0831)  time: 3.8425  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 690/3449]  eta: 2:51:10  lr: 0.000100  loss: 0.0365 (0.0824)  time: 3.8593  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 700/3449]  eta: 2:50:34  lr: 0.000100  loss: 0.0396 (0.0821)  time: 3.8168  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 710/3449]  eta: 2:50:00  lr: 0.000100  loss: 0.0620 (0.0821)  time: 3.7815  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 720/3449]  eta: 2:49:31  lr: 0.000100  loss: 0.0816 (0.0823)  time: 3.8722  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 730/3449]  eta: 2:48:56  lr: 0.000100  loss: 0.0992 (0.0827)  time: 3.8553  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 740/3449]  eta: 2:48:21  lr: 0.000100  loss: 0.1041 (0.0828)  time: 3.7833  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 750/3449]  eta: 2:47:48  lr: 0.000100  loss: 0.1080 (0.0833)  time: 3.8194  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 760/3449]  eta: 2:47:09  lr: 0.000100  loss: 0.1087 (0.0836)  time: 3.7688  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 770/3449]  eta: 2:46:33  lr: 0.000100  loss: 0.0855 (0.0835)  time: 3.7301  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 780/3449]  eta: 2:45:56  lr: 0.000100  loss: 0.0545 (0.0831)  time: 3.7566  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 790/3449]  eta: 2:45:23  lr: 0.000100  loss: 0.0472 (0.0826)  time: 3.7996  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 800/3449]  eta: 2:44:44  lr: 0.000100  loss: 0.0454 (0.0822)  time: 3.7679  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 810/3449]  eta: 2:43:59  lr: 0.000100  loss: 0.0448 (0.0817)  time: 3.5803  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 820/3449]  eta: 2:43:13  lr: 0.000100  loss: 0.0439 (0.0813)  time: 3.4806  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 830/3449]  eta: 2:42:29  lr: 0.000100  loss: 0.0419 (0.0808)  time: 3.4826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 840/3449]  eta: 2:41:45  lr: 0.000100  loss: 0.0452 (0.0806)  time: 3.5027  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:22]  [ 850/3449]  eta: 2:41:11  lr: 0.000100  loss: 0.0612 (0.0804)  time: 3.6688  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 860/3449]  eta: 2:40:36  lr: 0.000100  loss: 0.0743 (0.0803)  time: 3.8131  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 870/3449]  eta: 2:40:03  lr: 0.000100  loss: 0.0651 (0.0801)  time: 3.8277  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 880/3449]  eta: 2:39:29  lr: 0.000100  loss: 0.0553 (0.0797)  time: 3.8436  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 890/3449]  eta: 2:38:55  lr: 0.000100  loss: 0.0539 (0.0795)  time: 3.8475  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 900/3449]  eta: 2:38:20  lr: 0.000100  loss: 0.0725 (0.0798)  time: 3.8167  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 910/3449]  eta: 2:37:41  lr: 0.000100  loss: 0.1028 (0.0798)  time: 3.7311  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 920/3449]  eta: 2:37:05  lr: 0.000100  loss: 0.0783 (0.0798)  time: 3.7251  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 930/3449]  eta: 2:36:29  lr: 0.000100  loss: 0.0522 (0.0795)  time: 3.7687  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 940/3449]  eta: 2:35:54  lr: 0.000100  loss: 0.0522 (0.0793)  time: 3.7865  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [ 950/3449]  eta: 2:35:19  lr: 0.000100  loss: 0.0674 (0.0792)  time: 3.8124  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 960/3449]  eta: 2:34:41  lr: 0.000100  loss: 0.0717 (0.0792)  time: 3.7626  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 970/3449]  eta: 2:34:05  lr: 0.000100  loss: 0.0619 (0.0789)  time: 3.7490  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 980/3449]  eta: 2:33:27  lr: 0.000100  loss: 0.0494 (0.0786)  time: 3.7437  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [ 990/3449]  eta: 2:32:50  lr: 0.000100  loss: 0.0494 (0.0784)  time: 3.7226  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1000/3449]  eta: 2:32:14  lr: 0.000100  loss: 0.0634 (0.0784)  time: 3.7580  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1010/3449]  eta: 2:31:39  lr: 0.000100  loss: 0.0801 (0.0784)  time: 3.7956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1020/3449]  eta: 2:31:03  lr: 0.000100  loss: 0.0823 (0.0787)  time: 3.8059  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1030/3449]  eta: 2:30:27  lr: 0.000100  loss: 0.1163 (0.0792)  time: 3.7986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1040/3449]  eta: 2:29:47  lr: 0.000100  loss: 0.1134 (0.0794)  time: 3.7145  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1050/3449]  eta: 2:29:13  lr: 0.000100  loss: 0.0664 (0.0792)  time: 3.7310  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1060/3449]  eta: 2:28:36  lr: 0.000100  loss: 0.0624 (0.0792)  time: 3.7924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1070/3449]  eta: 2:27:55  lr: 0.000100  loss: 0.0624 (0.0790)  time: 3.6580  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1080/3449]  eta: 2:27:16  lr: 0.000100  loss: 0.0792 (0.0795)  time: 3.6204  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1090/3449]  eta: 2:26:40  lr: 0.000100  loss: 0.1558 (0.0805)  time: 3.7369  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1100/3449]  eta: 2:26:04  lr: 0.000100  loss: 0.1684 (0.0812)  time: 3.7832  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1110/3449]  eta: 2:25:27  lr: 0.000100  loss: 0.1165 (0.0813)  time: 3.7686  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1120/3449]  eta: 2:24:50  lr: 0.000100  loss: 0.0712 (0.0812)  time: 3.7461  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1130/3449]  eta: 2:24:14  lr: 0.000100  loss: 0.0750 (0.0812)  time: 3.7692  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1140/3449]  eta: 2:23:38  lr: 0.000100  loss: 0.0752 (0.0811)  time: 3.8080  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1150/3449]  eta: 2:22:57  lr: 0.000100  loss: 0.0684 (0.0809)  time: 3.6513  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1160/3449]  eta: 2:22:17  lr: 0.000100  loss: 0.0478 (0.0807)  time: 3.5658  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1170/3449]  eta: 2:21:40  lr: 0.000100  loss: 0.0572 (0.0805)  time: 3.6800  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1180/3449]  eta: 2:21:06  lr: 0.000100  loss: 0.0445 (0.0802)  time: 3.8230  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1190/3449]  eta: 2:20:26  lr: 0.000100  loss: 0.0393 (0.0801)  time: 3.7587  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1200/3449]  eta: 2:19:50  lr: 0.000100  loss: 0.0681 (0.0800)  time: 3.7032  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1210/3449]  eta: 2:19:13  lr: 0.000100  loss: 0.0712 (0.0801)  time: 3.7632  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1220/3449]  eta: 2:18:36  lr: 0.000100  loss: 0.0715 (0.0800)  time: 3.7529  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1230/3449]  eta: 2:18:00  lr: 0.000100  loss: 0.0727 (0.0800)  time: 3.7810  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1240/3449]  eta: 2:17:21  lr: 0.000100  loss: 0.0822 (0.0800)  time: 3.7207  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1250/3449]  eta: 2:16:45  lr: 0.000100  loss: 0.0727 (0.0799)  time: 3.7204  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1260/3449]  eta: 2:16:07  lr: 0.000100  loss: 0.0663 (0.0798)  time: 3.7454  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1270/3449]  eta: 2:15:30  lr: 0.000100  loss: 0.0722 (0.0799)  time: 3.7292  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1280/3449]  eta: 2:14:54  lr: 0.000100  loss: 0.1038 (0.0802)  time: 3.7790  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1290/3449]  eta: 2:14:17  lr: 0.000100  loss: 0.1012 (0.0802)  time: 3.7597  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1300/3449]  eta: 2:13:40  lr: 0.000100  loss: 0.0501 (0.0799)  time: 3.7400  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1310/3449]  eta: 2:13:03  lr: 0.000100  loss: 0.0382 (0.0796)  time: 3.7639  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1320/3449]  eta: 2:12:26  lr: 0.000100  loss: 0.0403 (0.0793)  time: 3.7576  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1330/3449]  eta: 2:11:51  lr: 0.000100  loss: 0.0456 (0.0792)  time: 3.8095  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1340/3449]  eta: 2:11:14  lr: 0.000100  loss: 0.0833 (0.0797)  time: 3.8141  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1350/3449]  eta: 2:10:40  lr: 0.000100  loss: 0.1164 (0.0798)  time: 3.8521  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1360/3449]  eta: 2:10:02  lr: 0.000100  loss: 0.0801 (0.0798)  time: 3.8049  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1370/3449]  eta: 2:09:24  lr: 0.000100  loss: 0.0801 (0.0799)  time: 3.7011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1380/3449]  eta: 2:08:49  lr: 0.000100  loss: 0.0720 (0.0797)  time: 3.7928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1390/3449]  eta: 2:08:12  lr: 0.000100  loss: 0.0656 (0.0797)  time: 3.8232  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1400/3449]  eta: 2:07:36  lr: 0.000100  loss: 0.0757 (0.0798)  time: 3.8239  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1410/3449]  eta: 2:07:00  lr: 0.000100  loss: 0.0637 (0.0796)  time: 3.8265  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1420/3449]  eta: 2:06:24  lr: 0.000100  loss: 0.0553 (0.0795)  time: 3.8118  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1430/3449]  eta: 2:05:47  lr: 0.000100  loss: 0.0747 (0.0795)  time: 3.7991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1440/3449]  eta: 2:05:11  lr: 0.000100  loss: 0.0765 (0.0794)  time: 3.7982  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1450/3449]  eta: 2:04:34  lr: 0.000100  loss: 0.0785 (0.0795)  time: 3.7875  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1460/3449]  eta: 2:03:56  lr: 0.000100  loss: 0.0930 (0.0796)  time: 3.7523  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1470/3449]  eta: 2:03:18  lr: 0.000100  loss: 0.1009 (0.0797)  time: 3.7039  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1480/3449]  eta: 2:02:40  lr: 0.000100  loss: 0.0731 (0.0796)  time: 3.6916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1490/3449]  eta: 2:02:04  lr: 0.000100  loss: 0.0656 (0.0796)  time: 3.7648  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1500/3449]  eta: 2:01:27  lr: 0.000100  loss: 0.0770 (0.0795)  time: 3.7864  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:22]  [1510/3449]  eta: 2:00:51  lr: 0.000100  loss: 0.0773 (0.0797)  time: 3.7994  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1520/3449]  eta: 2:00:13  lr: 0.000100  loss: 0.0934 (0.0798)  time: 3.7934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1530/3449]  eta: 1:59:37  lr: 0.000100  loss: 0.0883 (0.0798)  time: 3.7964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1540/3449]  eta: 1:59:00  lr: 0.000100  loss: 0.0802 (0.0799)  time: 3.8063  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1550/3449]  eta: 1:58:23  lr: 0.000100  loss: 0.0708 (0.0798)  time: 3.7710  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1560/3449]  eta: 1:57:45  lr: 0.000100  loss: 0.0708 (0.0798)  time: 3.7131  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1570/3449]  eta: 1:57:09  lr: 0.000100  loss: 0.0744 (0.0798)  time: 3.7825  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1580/3449]  eta: 1:56:33  lr: 0.000100  loss: 0.0850 (0.0802)  time: 3.8467  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1590/3449]  eta: 1:55:55  lr: 0.000100  loss: 0.1119 (0.0804)  time: 3.7427  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1600/3449]  eta: 1:55:18  lr: 0.000100  loss: 0.0742 (0.0802)  time: 3.7438  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1610/3449]  eta: 1:54:40  lr: 0.000100  loss: 0.0459 (0.0800)  time: 3.7629  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1620/3449]  eta: 1:54:04  lr: 0.000100  loss: 0.0393 (0.0797)  time: 3.7884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1630/3449]  eta: 1:53:26  lr: 0.000100  loss: 0.0417 (0.0796)  time: 3.7741  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1640/3449]  eta: 1:52:49  lr: 0.000100  loss: 0.0712 (0.0796)  time: 3.7405  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1650/3449]  eta: 1:52:12  lr: 0.000100  loss: 0.0794 (0.0796)  time: 3.7631  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1660/3449]  eta: 1:51:35  lr: 0.000100  loss: 0.0792 (0.0795)  time: 3.7716  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1670/3449]  eta: 1:50:57  lr: 0.000100  loss: 0.0715 (0.0795)  time: 3.7438  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1680/3449]  eta: 1:50:20  lr: 0.000100  loss: 0.0809 (0.0795)  time: 3.7569  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1690/3449]  eta: 1:49:43  lr: 0.000100  loss: 0.0822 (0.0796)  time: 3.7827  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1700/3449]  eta: 1:49:06  lr: 0.000100  loss: 0.0654 (0.0794)  time: 3.7737  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1710/3449]  eta: 1:48:29  lr: 0.000100  loss: 0.0654 (0.0794)  time: 3.7741  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1720/3449]  eta: 1:47:52  lr: 0.000100  loss: 0.0722 (0.0793)  time: 3.7651  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1730/3449]  eta: 1:47:14  lr: 0.000100  loss: 0.0681 (0.0793)  time: 3.7298  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1740/3449]  eta: 1:46:36  lr: 0.000100  loss: 0.0607 (0.0791)  time: 3.7209  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1750/3449]  eta: 1:46:00  lr: 0.000100  loss: 0.0419 (0.0789)  time: 3.8048  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1760/3449]  eta: 1:45:23  lr: 0.000100  loss: 0.0567 (0.0789)  time: 3.8019  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1770/3449]  eta: 1:44:46  lr: 0.000100  loss: 0.0774 (0.0789)  time: 3.8166  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1780/3449]  eta: 1:44:10  lr: 0.000100  loss: 0.0639 (0.0788)  time: 3.8700  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1790/3449]  eta: 1:43:32  lr: 0.000100  loss: 0.0760 (0.0789)  time: 3.7675  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1800/3449]  eta: 1:42:55  lr: 0.000100  loss: 0.0786 (0.0788)  time: 3.7412  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1810/3449]  eta: 1:42:19  lr: 0.000100  loss: 0.0885 (0.0790)  time: 3.8763  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1820/3449]  eta: 1:41:42  lr: 0.000100  loss: 0.1257 (0.0794)  time: 3.8443  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1830/3449]  eta: 1:41:05  lr: 0.000100  loss: 0.1262 (0.0796)  time: 3.7714  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1840/3449]  eta: 1:40:29  lr: 0.000100  loss: 0.1083 (0.0797)  time: 3.8432  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1850/3449]  eta: 1:39:52  lr: 0.000100  loss: 0.0681 (0.0796)  time: 3.8491  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1860/3449]  eta: 1:39:16  lr: 0.000100  loss: 0.0680 (0.0796)  time: 3.8634  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [1870/3449]  eta: 1:38:36  lr: 0.000100  loss: 0.0680 (0.0794)  time: 3.6953  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1880/3449]  eta: 1:37:56  lr: 0.000100  loss: 0.0720 (0.0795)  time: 3.4745  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1890/3449]  eta: 1:37:17  lr: 0.000100  loss: 0.0530 (0.0794)  time: 3.4788  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1900/3449]  eta: 1:36:37  lr: 0.000100  loss: 0.0500 (0.0793)  time: 3.4815  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1910/3449]  eta: 1:35:58  lr: 0.000100  loss: 0.0660 (0.0792)  time: 3.4844  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1920/3449]  eta: 1:35:18  lr: 0.000100  loss: 0.0609 (0.0791)  time: 3.4871  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1930/3449]  eta: 1:34:39  lr: 0.000100  loss: 0.0402 (0.0789)  time: 3.4872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1940/3449]  eta: 1:33:59  lr: 0.000100  loss: 0.0456 (0.0789)  time: 3.4857  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1950/3449]  eta: 1:33:20  lr: 0.000100  loss: 0.0778 (0.0789)  time: 3.4826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1960/3449]  eta: 1:32:41  lr: 0.000100  loss: 0.0808 (0.0789)  time: 3.4801  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1970/3449]  eta: 1:32:01  lr: 0.000100  loss: 0.0744 (0.0788)  time: 3.4781  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1980/3449]  eta: 1:31:22  lr: 0.000100  loss: 0.0719 (0.0788)  time: 3.4781  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [1990/3449]  eta: 1:30:43  lr: 0.000100  loss: 0.0535 (0.0786)  time: 3.4784  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2000/3449]  eta: 1:30:04  lr: 0.000100  loss: 0.0499 (0.0785)  time: 3.4766  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2010/3449]  eta: 1:29:25  lr: 0.000100  loss: 0.0483 (0.0784)  time: 3.4776  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2020/3449]  eta: 1:28:46  lr: 0.000100  loss: 0.0608 (0.0784)  time: 3.4776  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2030/3449]  eta: 1:28:07  lr: 0.000100  loss: 0.0598 (0.0782)  time: 3.4765  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2040/3449]  eta: 1:27:28  lr: 0.000100  loss: 0.0403 (0.0780)  time: 3.4751  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2050/3449]  eta: 1:26:49  lr: 0.000100  loss: 0.0371 (0.0779)  time: 3.4746  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2060/3449]  eta: 1:26:10  lr: 0.000100  loss: 0.0411 (0.0777)  time: 3.4768  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2070/3449]  eta: 1:25:31  lr: 0.000100  loss: 0.0442 (0.0776)  time: 3.4774  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2080/3449]  eta: 1:24:52  lr: 0.000100  loss: 0.0561 (0.0776)  time: 3.4764  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2090/3449]  eta: 1:24:13  lr: 0.000100  loss: 0.0764 (0.0776)  time: 3.4761  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2100/3449]  eta: 1:23:35  lr: 0.000100  loss: 0.0761 (0.0776)  time: 3.4772  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2110/3449]  eta: 1:22:56  lr: 0.000100  loss: 0.0691 (0.0775)  time: 3.4768  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2120/3449]  eta: 1:22:17  lr: 0.000100  loss: 0.0495 (0.0773)  time: 3.4771  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2130/3449]  eta: 1:21:39  lr: 0.000100  loss: 0.0369 (0.0772)  time: 3.4780  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2140/3449]  eta: 1:21:00  lr: 0.000100  loss: 0.0653 (0.0772)  time: 3.4769  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2150/3449]  eta: 1:20:21  lr: 0.000100  loss: 0.0752 (0.0772)  time: 3.4757  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2160/3449]  eta: 1:19:43  lr: 0.000100  loss: 0.0702 (0.0772)  time: 3.4754  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:22]  [2170/3449]  eta: 1:19:04  lr: 0.000100  loss: 0.0441 (0.0770)  time: 3.4758  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2180/3449]  eta: 1:18:26  lr: 0.000100  loss: 0.0385 (0.0768)  time: 3.4769  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2190/3449]  eta: 1:17:48  lr: 0.000100  loss: 0.0443 (0.0767)  time: 3.4777  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2200/3449]  eta: 1:17:09  lr: 0.000100  loss: 0.0491 (0.0766)  time: 3.4781  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2210/3449]  eta: 1:16:31  lr: 0.000100  loss: 0.0491 (0.0764)  time: 3.4781  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2220/3449]  eta: 1:15:53  lr: 0.000100  loss: 0.0441 (0.0763)  time: 3.4780  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2230/3449]  eta: 1:15:14  lr: 0.000100  loss: 0.0423 (0.0761)  time: 3.4779  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2240/3449]  eta: 1:14:36  lr: 0.000100  loss: 0.0432 (0.0760)  time: 3.4772  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2250/3449]  eta: 1:13:58  lr: 0.000100  loss: 0.0374 (0.0758)  time: 3.4769  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2260/3449]  eta: 1:13:20  lr: 0.000100  loss: 0.0352 (0.0758)  time: 3.4778  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2270/3449]  eta: 1:12:41  lr: 0.000100  loss: 0.0683 (0.0759)  time: 3.4789  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2280/3449]  eta: 1:12:03  lr: 0.000100  loss: 0.1011 (0.0762)  time: 3.4792  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2290/3449]  eta: 1:11:25  lr: 0.000100  loss: 0.1228 (0.0763)  time: 3.4780  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2300/3449]  eta: 1:10:47  lr: 0.000100  loss: 0.0825 (0.0763)  time: 3.4760  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2310/3449]  eta: 1:10:09  lr: 0.000100  loss: 0.0713 (0.0763)  time: 3.4769  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2320/3449]  eta: 1:09:31  lr: 0.000100  loss: 0.0897 (0.0765)  time: 3.4780  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2330/3449]  eta: 1:08:53  lr: 0.000100  loss: 0.1578 (0.0768)  time: 3.4765  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2340/3449]  eta: 1:08:15  lr: 0.000100  loss: 0.0913 (0.0769)  time: 3.4759  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2350/3449]  eta: 1:07:37  lr: 0.000100  loss: 0.0834 (0.0769)  time: 3.4748  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2360/3449]  eta: 1:06:59  lr: 0.000100  loss: 0.0806 (0.0769)  time: 3.4733  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2370/3449]  eta: 1:06:21  lr: 0.000100  loss: 0.0782 (0.0769)  time: 3.4745  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2380/3449]  eta: 1:05:43  lr: 0.000100  loss: 0.0763 (0.0770)  time: 3.4760  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2390/3449]  eta: 1:05:06  lr: 0.000100  loss: 0.1091 (0.0771)  time: 3.4760  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2400/3449]  eta: 1:04:28  lr: 0.000100  loss: 0.1266 (0.0774)  time: 3.4762  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2410/3449]  eta: 1:03:50  lr: 0.000100  loss: 0.1400 (0.0776)  time: 3.4766  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2420/3449]  eta: 1:03:12  lr: 0.000100  loss: 0.0840 (0.0776)  time: 3.4763  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2430/3449]  eta: 1:02:35  lr: 0.000100  loss: 0.0771 (0.0776)  time: 3.4768  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2440/3449]  eta: 1:01:57  lr: 0.000100  loss: 0.0741 (0.0776)  time: 3.4769  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2450/3449]  eta: 1:01:19  lr: 0.000100  loss: 0.0679 (0.0776)  time: 3.4768  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2460/3449]  eta: 1:00:41  lr: 0.000100  loss: 0.0765 (0.0777)  time: 3.4765  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2470/3449]  eta: 1:00:04  lr: 0.000100  loss: 0.0879 (0.0776)  time: 3.4766  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2480/3449]  eta: 0:59:26  lr: 0.000100  loss: 0.0464 (0.0775)  time: 3.4759  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2490/3449]  eta: 0:58:49  lr: 0.000100  loss: 0.0431 (0.0774)  time: 3.4738  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2500/3449]  eta: 0:58:11  lr: 0.000100  loss: 0.0605 (0.0774)  time: 3.4739  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2510/3449]  eta: 0:57:33  lr: 0.000100  loss: 0.1029 (0.0777)  time: 3.4762  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2520/3449]  eta: 0:56:56  lr: 0.000100  loss: 0.1272 (0.0778)  time: 3.4785  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2530/3449]  eta: 0:56:18  lr: 0.000100  loss: 0.0908 (0.0779)  time: 3.4790  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2540/3449]  eta: 0:55:41  lr: 0.000100  loss: 0.0847 (0.0779)  time: 3.4788  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2550/3449]  eta: 0:55:03  lr: 0.000100  loss: 0.0708 (0.0778)  time: 3.4782  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2560/3449]  eta: 0:54:26  lr: 0.000100  loss: 0.0734 (0.0779)  time: 3.4791  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2570/3449]  eta: 0:53:49  lr: 0.000100  loss: 0.0952 (0.0779)  time: 3.4801  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2580/3449]  eta: 0:53:11  lr: 0.000100  loss: 0.0838 (0.0779)  time: 3.4789  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2590/3449]  eta: 0:52:34  lr: 0.000100  loss: 0.0798 (0.0779)  time: 3.4782  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2600/3449]  eta: 0:51:57  lr: 0.000100  loss: 0.0798 (0.0779)  time: 3.4790  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2610/3449]  eta: 0:51:19  lr: 0.000100  loss: 0.0815 (0.0779)  time: 3.4787  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2620/3449]  eta: 0:50:42  lr: 0.000100  loss: 0.0742 (0.0779)  time: 3.4785  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2630/3449]  eta: 0:50:05  lr: 0.000100  loss: 0.0732 (0.0779)  time: 3.4791  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2640/3449]  eta: 0:49:27  lr: 0.000100  loss: 0.0708 (0.0779)  time: 3.4797  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2650/3449]  eta: 0:48:50  lr: 0.000100  loss: 0.0726 (0.0779)  time: 3.4801  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2660/3449]  eta: 0:48:13  lr: 0.000100  loss: 0.0726 (0.0778)  time: 3.4801  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2670/3449]  eta: 0:47:36  lr: 0.000100  loss: 0.0633 (0.0778)  time: 3.4810  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2680/3449]  eta: 0:46:58  lr: 0.000100  loss: 0.0628 (0.0778)  time: 3.4825  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2690/3449]  eta: 0:46:21  lr: 0.000100  loss: 0.0785 (0.0778)  time: 3.4836  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2700/3449]  eta: 0:45:44  lr: 0.000100  loss: 0.0999 (0.0779)  time: 3.4844  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2710/3449]  eta: 0:45:07  lr: 0.000100  loss: 0.0778 (0.0778)  time: 3.4852  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2720/3449]  eta: 0:44:30  lr: 0.000100  loss: 0.0723 (0.0778)  time: 3.4853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2730/3449]  eta: 0:43:53  lr: 0.000100  loss: 0.0578 (0.0777)  time: 3.4841  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2740/3449]  eta: 0:43:16  lr: 0.000100  loss: 0.0384 (0.0775)  time: 3.4827  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2750/3449]  eta: 0:42:39  lr: 0.000100  loss: 0.0388 (0.0774)  time: 3.4821  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2760/3449]  eta: 0:42:02  lr: 0.000100  loss: 0.0567 (0.0774)  time: 3.4819  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2770/3449]  eta: 0:41:24  lr: 0.000100  loss: 0.0790 (0.0774)  time: 3.4810  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2780/3449]  eta: 0:40:47  lr: 0.000100  loss: 0.0838 (0.0775)  time: 3.4790  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2790/3449]  eta: 0:40:10  lr: 0.000100  loss: 0.1139 (0.0776)  time: 3.4780  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2800/3449]  eta: 0:39:33  lr: 0.000100  loss: 0.0880 (0.0776)  time: 3.4778  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2810/3449]  eta: 0:38:56  lr: 0.000100  loss: 0.0843 (0.0777)  time: 3.4777  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2820/3449]  eta: 0:38:19  lr: 0.000100  loss: 0.0678 (0.0776)  time: 3.4787  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:22]  [2830/3449]  eta: 0:37:43  lr: 0.000100  loss: 0.0613 (0.0777)  time: 3.4806  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2840/3449]  eta: 0:37:06  lr: 0.000100  loss: 0.0851 (0.0777)  time: 3.4810  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2850/3449]  eta: 0:36:29  lr: 0.000100  loss: 0.0787 (0.0777)  time: 3.4795  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2860/3449]  eta: 0:35:52  lr: 0.000100  loss: 0.0633 (0.0777)  time: 3.4798  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2870/3449]  eta: 0:35:15  lr: 0.000100  loss: 0.0628 (0.0777)  time: 3.4806  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2880/3449]  eta: 0:34:38  lr: 0.000100  loss: 0.0740 (0.0776)  time: 3.4811  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2890/3449]  eta: 0:34:01  lr: 0.000100  loss: 0.0930 (0.0778)  time: 3.4811  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2900/3449]  eta: 0:33:24  lr: 0.000100  loss: 0.1150 (0.0779)  time: 3.4827  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2910/3449]  eta: 0:32:47  lr: 0.000100  loss: 0.0825 (0.0779)  time: 3.4842  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2920/3449]  eta: 0:32:11  lr: 0.000100  loss: 0.0675 (0.0778)  time: 3.5414  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2930/3449]  eta: 0:31:35  lr: 0.000100  loss: 0.0586 (0.0777)  time: 3.7095  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2940/3449]  eta: 0:30:58  lr: 0.000100  loss: 0.0434 (0.0776)  time: 3.8000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2950/3449]  eta: 0:30:22  lr: 0.000100  loss: 0.0564 (0.0777)  time: 3.6740  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2960/3449]  eta: 0:29:45  lr: 0.000100  loss: 0.0766 (0.0777)  time: 3.7053  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2970/3449]  eta: 0:29:09  lr: 0.000100  loss: 0.0513 (0.0776)  time: 3.8499  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2980/3449]  eta: 0:28:33  lr: 0.000100  loss: 0.0369 (0.0774)  time: 3.8229  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [2990/3449]  eta: 0:27:57  lr: 0.000100  loss: 0.0478 (0.0774)  time: 3.8078  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3000/3449]  eta: 0:27:20  lr: 0.000100  loss: 0.0810 (0.0775)  time: 3.7874  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3010/3449]  eta: 0:26:44  lr: 0.000100  loss: 0.0588 (0.0774)  time: 3.7629  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3020/3449]  eta: 0:26:08  lr: 0.000100  loss: 0.0647 (0.0774)  time: 3.7702  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3030/3449]  eta: 0:25:31  lr: 0.000100  loss: 0.0752 (0.0775)  time: 3.7703  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [3040/3449]  eta: 0:24:55  lr: 0.000100  loss: 0.0984 (0.0776)  time: 3.7511  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3050/3449]  eta: 0:24:18  lr: 0.000100  loss: 0.0751 (0.0775)  time: 3.7400  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3060/3449]  eta: 0:23:42  lr: 0.000100  loss: 0.0381 (0.0774)  time: 3.7535  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3070/3449]  eta: 0:23:05  lr: 0.000100  loss: 0.0381 (0.0773)  time: 3.7037  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3080/3449]  eta: 0:22:29  lr: 0.000100  loss: 0.0526 (0.0773)  time: 3.6943  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [3090/3449]  eta: 0:21:52  lr: 0.000100  loss: 0.0955 (0.0774)  time: 3.7396  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [3100/3449]  eta: 0:21:16  lr: 0.000100  loss: 0.1328 (0.0776)  time: 3.7792  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3110/3449]  eta: 0:20:40  lr: 0.000100  loss: 0.0888 (0.0776)  time: 3.8023  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3120/3449]  eta: 0:20:03  lr: 0.000100  loss: 0.0539 (0.0775)  time: 3.8218  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3130/3449]  eta: 0:19:27  lr: 0.000100  loss: 0.0442 (0.0774)  time: 3.8148  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3140/3449]  eta: 0:18:50  lr: 0.000100  loss: 0.0413 (0.0773)  time: 3.7909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3150/3449]  eta: 0:18:14  lr: 0.000100  loss: 0.0611 (0.0775)  time: 3.8135  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3160/3449]  eta: 0:17:37  lr: 0.000100  loss: 0.1148 (0.0775)  time: 3.8157  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3170/3449]  eta: 0:17:01  lr: 0.000100  loss: 0.0585 (0.0775)  time: 3.7780  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [3180/3449]  eta: 0:16:24  lr: 0.000100  loss: 0.0565 (0.0775)  time: 3.7597  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [3190/3449]  eta: 0:15:48  lr: 0.000100  loss: 0.0535 (0.0774)  time: 3.7445  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3200/3449]  eta: 0:15:11  lr: 0.000100  loss: 0.0496 (0.0773)  time: 3.6672  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3210/3449]  eta: 0:14:35  lr: 0.000100  loss: 0.0702 (0.0773)  time: 3.6755  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3220/3449]  eta: 0:13:58  lr: 0.000100  loss: 0.0735 (0.0773)  time: 3.6459  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3230/3449]  eta: 0:13:21  lr: 0.000100  loss: 0.0711 (0.0773)  time: 3.6995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3240/3449]  eta: 0:12:45  lr: 0.000100  loss: 0.0815 (0.0773)  time: 3.8335  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3250/3449]  eta: 0:12:08  lr: 0.000100  loss: 0.0712 (0.0772)  time: 3.8003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3260/3449]  eta: 0:11:32  lr: 0.000100  loss: 0.0542 (0.0771)  time: 3.7664  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3270/3449]  eta: 0:10:55  lr: 0.000100  loss: 0.0456 (0.0770)  time: 3.7067  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3280/3449]  eta: 0:10:18  lr: 0.000100  loss: 0.0445 (0.0769)  time: 3.6509  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3290/3449]  eta: 0:09:42  lr: 0.000100  loss: 0.0456 (0.0769)  time: 3.7205  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3300/3449]  eta: 0:09:05  lr: 0.000100  loss: 0.0663 (0.0769)  time: 3.7576  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3310/3449]  eta: 0:08:29  lr: 0.000100  loss: 0.0699 (0.0768)  time: 3.8168  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3320/3449]  eta: 0:07:52  lr: 0.000100  loss: 0.0712 (0.0768)  time: 3.8657  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3330/3449]  eta: 0:07:16  lr: 0.000100  loss: 0.0790 (0.0768)  time: 3.8007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3340/3449]  eta: 0:06:39  lr: 0.000100  loss: 0.0756 (0.0768)  time: 3.7495  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3350/3449]  eta: 0:06:02  lr: 0.000100  loss: 0.0740 (0.0768)  time: 3.7859  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3360/3449]  eta: 0:05:26  lr: 0.000100  loss: 0.0687 (0.0768)  time: 3.7994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3370/3449]  eta: 0:04:49  lr: 0.000100  loss: 0.0572 (0.0768)  time: 3.6404  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3380/3449]  eta: 0:04:12  lr: 0.000100  loss: 0.0576 (0.0767)  time: 3.6851  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [3390/3449]  eta: 0:03:36  lr: 0.000100  loss: 0.0688 (0.0767)  time: 3.7557  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [3400/3449]  eta: 0:02:59  lr: 0.000100  loss: 0.1046 (0.0768)  time: 3.7233  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:22]  [3410/3449]  eta: 0:02:23  lr: 0.000100  loss: 0.1084 (0.0769)  time: 3.8278  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3420/3449]  eta: 0:01:46  lr: 0.000100  loss: 0.1010 (0.0770)  time: 3.8312  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3430/3449]  eta: 0:01:09  lr: 0.000100  loss: 0.0932 (0.0770)  time: 3.6818  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3440/3449]  eta: 0:00:33  lr: 0.000100  loss: 0.0669 (0.0770)  time: 3.6426  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0470 (0.0769)  time: 3.6343  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:22] Total time: 3:30:46 (3.6668 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0470 (0.0769)\n",
      "Valid: [epoch:22]  [ 0/14]  eta: 0:04:21  loss: 0.0271 (0.0271)  time: 18.6979  data: 0.5084  max mem: 34968\n",
      "Valid: [epoch:22]  [13/14]  eta: 0:00:18  loss: 0.0252 (0.0254)  time: 18.2463  data: 0.0365  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:22] Total time: 0:04:15 (18.2600 s / it)\n",
      "Averaged stats: loss: 0.0252 (0.0254)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_22_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.025%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:23]  [   0/3449]  eta: 5:37:00  lr: 0.000100  loss: 0.0330 (0.0330)  time: 5.8626  data: 1.4686  max mem: 34968\n",
      "Train: [epoch:23]  [  10/3449]  eta: 3:44:28  lr: 0.000100  loss: 0.0330 (0.0345)  time: 3.9163  data: 0.1336  max mem: 34968\n",
      "Train: [epoch:23]  [  20/3449]  eta: 3:37:55  lr: 0.000100  loss: 0.0332 (0.0365)  time: 3.7106  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [  30/3449]  eta: 3:37:08  lr: 0.000100  loss: 0.0351 (0.0371)  time: 3.7523  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [  40/3449]  eta: 3:36:13  lr: 0.000100  loss: 0.0503 (0.0482)  time: 3.7977  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [  50/3449]  eta: 3:35:20  lr: 0.000100  loss: 0.0714 (0.0526)  time: 3.7870  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [  60/3449]  eta: 3:33:08  lr: 0.000100  loss: 0.0625 (0.0555)  time: 3.7082  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [  70/3449]  eta: 3:32:21  lr: 0.000100  loss: 0.0739 (0.0587)  time: 3.6929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [  80/3449]  eta: 3:30:52  lr: 0.000100  loss: 0.0772 (0.0616)  time: 3.7000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [  90/3449]  eta: 3:31:05  lr: 0.000100  loss: 0.0810 (0.0639)  time: 3.7701  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 100/3449]  eta: 3:31:04  lr: 0.000100  loss: 0.0915 (0.0679)  time: 3.8870  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 110/3449]  eta: 3:31:08  lr: 0.000100  loss: 0.1196 (0.0733)  time: 3.9003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 120/3449]  eta: 3:30:53  lr: 0.000100  loss: 0.0893 (0.0735)  time: 3.8992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 130/3449]  eta: 3:29:54  lr: 0.000100  loss: 0.0730 (0.0742)  time: 3.7994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 140/3449]  eta: 3:29:24  lr: 0.000100  loss: 0.0865 (0.0747)  time: 3.7724  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 150/3449]  eta: 3:28:30  lr: 0.000100  loss: 0.0812 (0.0745)  time: 3.7748  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 160/3449]  eta: 3:27:49  lr: 0.000100  loss: 0.0813 (0.0778)  time: 3.7521  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 170/3449]  eta: 3:27:12  lr: 0.000100  loss: 0.1147 (0.0807)  time: 3.7865  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 180/3449]  eta: 3:26:32  lr: 0.000100  loss: 0.1147 (0.0817)  time: 3.7867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 190/3449]  eta: 3:25:52  lr: 0.000100  loss: 0.1102 (0.0829)  time: 3.7788  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 200/3449]  eta: 3:25:02  lr: 0.000100  loss: 0.0944 (0.0846)  time: 3.7480  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 210/3449]  eta: 3:24:34  lr: 0.000100  loss: 0.0860 (0.0850)  time: 3.7853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 220/3449]  eta: 3:23:46  lr: 0.000100  loss: 0.1119 (0.0863)  time: 3.7841  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 230/3449]  eta: 3:23:10  lr: 0.000100  loss: 0.0996 (0.0871)  time: 3.7572  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 240/3449]  eta: 3:22:25  lr: 0.000100  loss: 0.0827 (0.0861)  time: 3.7661  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 250/3449]  eta: 3:21:59  lr: 0.000100  loss: 0.0575 (0.0848)  time: 3.8060  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 260/3449]  eta: 3:21:12  lr: 0.000100  loss: 0.0523 (0.0835)  time: 3.7959  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 270/3449]  eta: 3:20:13  lr: 0.000100  loss: 0.0525 (0.0823)  time: 3.6611  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 280/3449]  eta: 3:19:36  lr: 0.000100  loss: 0.0528 (0.0813)  time: 3.6970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 290/3449]  eta: 3:18:55  lr: 0.000100  loss: 0.0461 (0.0799)  time: 3.7674  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 300/3449]  eta: 3:18:00  lr: 0.000100  loss: 0.0424 (0.0790)  time: 3.6820  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 310/3449]  eta: 3:17:11  lr: 0.000100  loss: 0.0636 (0.0788)  time: 3.6369  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 320/3449]  eta: 3:16:24  lr: 0.000100  loss: 0.0657 (0.0784)  time: 3.6652  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 330/3449]  eta: 3:15:50  lr: 0.000100  loss: 0.0527 (0.0776)  time: 3.7390  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 340/3449]  eta: 3:15:01  lr: 0.000100  loss: 0.0669 (0.0776)  time: 3.7269  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 350/3449]  eta: 3:14:12  lr: 0.000100  loss: 0.0684 (0.0772)  time: 3.6400  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 360/3449]  eta: 3:13:36  lr: 0.000100  loss: 0.0670 (0.0775)  time: 3.7067  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 370/3449]  eta: 3:13:06  lr: 0.000100  loss: 0.0781 (0.0774)  time: 3.8156  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [ 380/3449]  eta: 3:12:33  lr: 0.000100  loss: 0.0680 (0.0770)  time: 3.8388  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [ 390/3449]  eta: 3:11:58  lr: 0.000100  loss: 0.0602 (0.0766)  time: 3.8070  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [ 400/3449]  eta: 3:11:17  lr: 0.000100  loss: 0.0697 (0.0767)  time: 3.7576  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [ 410/3449]  eta: 3:10:43  lr: 0.000100  loss: 0.0697 (0.0761)  time: 3.7698  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 420/3449]  eta: 3:10:13  lr: 0.000100  loss: 0.0508 (0.0755)  time: 3.8445  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [ 430/3449]  eta: 3:09:28  lr: 0.000100  loss: 0.0497 (0.0752)  time: 3.7703  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [ 440/3449]  eta: 3:08:50  lr: 0.000100  loss: 0.0625 (0.0763)  time: 3.7126  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 450/3449]  eta: 3:08:18  lr: 0.000100  loss: 0.1321 (0.0773)  time: 3.8012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 460/3449]  eta: 3:07:40  lr: 0.000100  loss: 0.1168 (0.0779)  time: 3.8009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 470/3449]  eta: 3:07:03  lr: 0.000100  loss: 0.1188 (0.0790)  time: 3.7748  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 480/3449]  eta: 3:06:25  lr: 0.000100  loss: 0.1486 (0.0807)  time: 3.7701  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 490/3449]  eta: 3:05:47  lr: 0.000100  loss: 0.1419 (0.0814)  time: 3.7603  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 500/3449]  eta: 3:05:13  lr: 0.000100  loss: 0.0630 (0.0807)  time: 3.8017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 510/3449]  eta: 3:04:37  lr: 0.000100  loss: 0.0530 (0.0804)  time: 3.8143  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 520/3449]  eta: 3:04:00  lr: 0.000100  loss: 0.0757 (0.0806)  time: 3.7826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 530/3449]  eta: 3:03:26  lr: 0.000100  loss: 0.0789 (0.0807)  time: 3.8044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 540/3449]  eta: 3:02:50  lr: 0.000100  loss: 0.0732 (0.0808)  time: 3.8217  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 550/3449]  eta: 3:02:16  lr: 0.000100  loss: 0.0941 (0.0816)  time: 3.8262  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 560/3449]  eta: 3:01:34  lr: 0.000100  loss: 0.1107 (0.0820)  time: 3.7666  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 570/3449]  eta: 3:00:57  lr: 0.000100  loss: 0.0865 (0.0818)  time: 3.7385  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 580/3449]  eta: 3:00:22  lr: 0.000100  loss: 0.0707 (0.0818)  time: 3.8115  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 590/3449]  eta: 2:59:43  lr: 0.000100  loss: 0.0757 (0.0817)  time: 3.7857  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 600/3449]  eta: 2:59:04  lr: 0.000100  loss: 0.0773 (0.0818)  time: 3.7450  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 610/3449]  eta: 2:58:32  lr: 0.000100  loss: 0.0856 (0.0818)  time: 3.8142  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 620/3449]  eta: 2:57:54  lr: 0.000100  loss: 0.0649 (0.0813)  time: 3.8315  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:23]  [ 630/3449]  eta: 2:57:17  lr: 0.000100  loss: 0.0363 (0.0806)  time: 3.7875  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 640/3449]  eta: 2:56:39  lr: 0.000100  loss: 0.0343 (0.0799)  time: 3.7700  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 650/3449]  eta: 2:56:08  lr: 0.000100  loss: 0.0343 (0.0793)  time: 3.8395  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 660/3449]  eta: 2:55:33  lr: 0.000100  loss: 0.0535 (0.0793)  time: 3.8917  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 670/3449]  eta: 2:54:56  lr: 0.000100  loss: 0.0711 (0.0793)  time: 3.8177  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 680/3449]  eta: 2:54:19  lr: 0.000100  loss: 0.0568 (0.0791)  time: 3.7945  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 690/3449]  eta: 2:53:44  lr: 0.000100  loss: 0.0857 (0.0802)  time: 3.8228  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 700/3449]  eta: 2:53:08  lr: 0.000100  loss: 0.1325 (0.0808)  time: 3.8363  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 710/3449]  eta: 2:52:27  lr: 0.000100  loss: 0.1252 (0.0815)  time: 3.7634  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 720/3449]  eta: 2:51:46  lr: 0.000100  loss: 0.0897 (0.0813)  time: 3.6930  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 730/3449]  eta: 2:51:12  lr: 0.000100  loss: 0.0675 (0.0812)  time: 3.7890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 740/3449]  eta: 2:50:37  lr: 0.000100  loss: 0.0692 (0.0811)  time: 3.8604  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 750/3449]  eta: 2:49:59  lr: 0.000100  loss: 0.0745 (0.0810)  time: 3.8100  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 760/3449]  eta: 2:49:20  lr: 0.000100  loss: 0.0646 (0.0807)  time: 3.7682  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 770/3449]  eta: 2:48:45  lr: 0.000100  loss: 0.0405 (0.0801)  time: 3.8042  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 780/3449]  eta: 2:48:08  lr: 0.000100  loss: 0.0350 (0.0796)  time: 3.8271  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [ 790/3449]  eta: 2:47:30  lr: 0.000100  loss: 0.0365 (0.0791)  time: 3.7832  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 800/3449]  eta: 2:46:53  lr: 0.000100  loss: 0.0365 (0.0785)  time: 3.7813  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 810/3449]  eta: 2:46:14  lr: 0.000100  loss: 0.0329 (0.0780)  time: 3.7768  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 820/3449]  eta: 2:45:36  lr: 0.000100  loss: 0.0380 (0.0777)  time: 3.7659  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 830/3449]  eta: 2:44:59  lr: 0.000100  loss: 0.0572 (0.0775)  time: 3.7835  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 840/3449]  eta: 2:44:23  lr: 0.000100  loss: 0.0678 (0.0775)  time: 3.8177  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 850/3449]  eta: 2:43:46  lr: 0.000100  loss: 0.0707 (0.0774)  time: 3.8237  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 860/3449]  eta: 2:43:11  lr: 0.000100  loss: 0.0770 (0.0779)  time: 3.8459  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 870/3449]  eta: 2:42:32  lr: 0.000100  loss: 0.1360 (0.0785)  time: 3.8163  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 880/3449]  eta: 2:41:52  lr: 0.000100  loss: 0.1103 (0.0787)  time: 3.7200  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 890/3449]  eta: 2:41:14  lr: 0.000100  loss: 0.0721 (0.0785)  time: 3.7332  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 900/3449]  eta: 2:40:38  lr: 0.000100  loss: 0.0670 (0.0785)  time: 3.8109  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 910/3449]  eta: 2:39:59  lr: 0.000100  loss: 0.0670 (0.0784)  time: 3.7860  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 920/3449]  eta: 2:39:17  lr: 0.000100  loss: 0.0573 (0.0781)  time: 3.6902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 930/3449]  eta: 2:38:43  lr: 0.000100  loss: 0.0550 (0.0779)  time: 3.7744  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 940/3449]  eta: 2:38:09  lr: 0.000100  loss: 0.0425 (0.0775)  time: 3.9083  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 950/3449]  eta: 2:37:29  lr: 0.000100  loss: 0.0414 (0.0772)  time: 3.8125  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 960/3449]  eta: 2:36:51  lr: 0.000100  loss: 0.0463 (0.0770)  time: 3.7410  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 970/3449]  eta: 2:36:15  lr: 0.000100  loss: 0.0504 (0.0766)  time: 3.8196  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 980/3449]  eta: 2:35:39  lr: 0.000100  loss: 0.0476 (0.0766)  time: 3.8469  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [ 990/3449]  eta: 2:34:56  lr: 0.000100  loss: 0.0912 (0.0769)  time: 3.7080  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1000/3449]  eta: 2:34:13  lr: 0.000100  loss: 0.1030 (0.0771)  time: 3.5703  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1010/3449]  eta: 2:33:33  lr: 0.000100  loss: 0.1015 (0.0773)  time: 3.6310  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1020/3449]  eta: 2:32:54  lr: 0.000100  loss: 0.1099 (0.0779)  time: 3.7142  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1030/3449]  eta: 2:32:15  lr: 0.000100  loss: 0.1194 (0.0781)  time: 3.7224  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1040/3449]  eta: 2:31:35  lr: 0.000100  loss: 0.0835 (0.0781)  time: 3.6973  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1050/3449]  eta: 2:30:56  lr: 0.000100  loss: 0.0824 (0.0781)  time: 3.7014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1060/3449]  eta: 2:30:19  lr: 0.000100  loss: 0.0795 (0.0781)  time: 3.7630  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1070/3449]  eta: 2:29:38  lr: 0.000100  loss: 0.0795 (0.0781)  time: 3.7186  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1080/3449]  eta: 2:29:00  lr: 0.000100  loss: 0.0667 (0.0780)  time: 3.6951  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1090/3449]  eta: 2:28:23  lr: 0.000100  loss: 0.0684 (0.0782)  time: 3.7905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1100/3449]  eta: 2:27:46  lr: 0.000100  loss: 0.0835 (0.0783)  time: 3.8245  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1110/3449]  eta: 2:27:07  lr: 0.000100  loss: 0.0835 (0.0784)  time: 3.7608  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1120/3449]  eta: 2:26:30  lr: 0.000100  loss: 0.0778 (0.0784)  time: 3.7477  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1130/3449]  eta: 2:25:52  lr: 0.000100  loss: 0.0849 (0.0784)  time: 3.7859  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1140/3449]  eta: 2:25:12  lr: 0.000100  loss: 0.0843 (0.0785)  time: 3.7143  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1150/3449]  eta: 2:24:32  lr: 0.000100  loss: 0.0762 (0.0784)  time: 3.6647  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1160/3449]  eta: 2:23:56  lr: 0.000100  loss: 0.0705 (0.0783)  time: 3.7685  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1170/3449]  eta: 2:23:15  lr: 0.000100  loss: 0.0481 (0.0780)  time: 3.7404  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1180/3449]  eta: 2:22:34  lr: 0.000100  loss: 0.0508 (0.0778)  time: 3.5976  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [1190/3449]  eta: 2:21:57  lr: 0.000100  loss: 0.0523 (0.0776)  time: 3.6830  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1200/3449]  eta: 2:21:19  lr: 0.000100  loss: 0.0491 (0.0775)  time: 3.7870  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1210/3449]  eta: 2:20:38  lr: 0.000100  loss: 0.0712 (0.0775)  time: 3.6868  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1220/3449]  eta: 2:20:01  lr: 0.000100  loss: 0.0762 (0.0777)  time: 3.7038  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1230/3449]  eta: 2:19:24  lr: 0.000100  loss: 0.0770 (0.0779)  time: 3.7982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1240/3449]  eta: 2:18:46  lr: 0.000100  loss: 0.0699 (0.0778)  time: 3.7611  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1250/3449]  eta: 2:18:08  lr: 0.000100  loss: 0.0620 (0.0778)  time: 3.7474  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1260/3449]  eta: 2:17:29  lr: 0.000100  loss: 0.0620 (0.0777)  time: 3.7382  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1270/3449]  eta: 2:16:51  lr: 0.000100  loss: 0.0539 (0.0775)  time: 3.7353  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1280/3449]  eta: 2:16:14  lr: 0.000100  loss: 0.0539 (0.0776)  time: 3.7734  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:23]  [1290/3449]  eta: 2:15:37  lr: 0.000100  loss: 0.1019 (0.0779)  time: 3.8040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1300/3449]  eta: 2:14:58  lr: 0.000100  loss: 0.0680 (0.0777)  time: 3.7439  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1310/3449]  eta: 2:14:19  lr: 0.000100  loss: 0.0503 (0.0775)  time: 3.7062  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1320/3449]  eta: 2:13:42  lr: 0.000100  loss: 0.0696 (0.0775)  time: 3.7607  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1330/3449]  eta: 2:13:06  lr: 0.000100  loss: 0.0702 (0.0775)  time: 3.8144  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1340/3449]  eta: 2:12:28  lr: 0.000100  loss: 0.0702 (0.0774)  time: 3.7992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1350/3449]  eta: 2:11:50  lr: 0.000100  loss: 0.0764 (0.0774)  time: 3.7701  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1360/3449]  eta: 2:11:12  lr: 0.000100  loss: 0.0769 (0.0775)  time: 3.7540  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1370/3449]  eta: 2:10:35  lr: 0.000100  loss: 0.1020 (0.0778)  time: 3.7810  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1380/3449]  eta: 2:09:59  lr: 0.000100  loss: 0.1454 (0.0782)  time: 3.8515  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1390/3449]  eta: 2:09:22  lr: 0.000100  loss: 0.1067 (0.0782)  time: 3.8416  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1400/3449]  eta: 2:08:45  lr: 0.000100  loss: 0.0810 (0.0783)  time: 3.8063  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [1410/3449]  eta: 2:08:06  lr: 0.000100  loss: 0.0967 (0.0784)  time: 3.7544  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1420/3449]  eta: 2:07:28  lr: 0.000100  loss: 0.0575 (0.0782)  time: 3.7413  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1430/3449]  eta: 2:06:50  lr: 0.000100  loss: 0.0534 (0.0783)  time: 3.7551  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1440/3449]  eta: 2:06:14  lr: 0.000100  loss: 0.0945 (0.0785)  time: 3.8019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1450/3449]  eta: 2:05:37  lr: 0.000100  loss: 0.1057 (0.0787)  time: 3.8443  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1460/3449]  eta: 2:05:01  lr: 0.000100  loss: 0.1211 (0.0791)  time: 3.8474  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [1470/3449]  eta: 2:04:21  lr: 0.000100  loss: 0.1276 (0.0795)  time: 3.7659  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [1480/3449]  eta: 2:03:44  lr: 0.000100  loss: 0.1328 (0.0799)  time: 3.7411  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [1490/3449]  eta: 2:03:08  lr: 0.000100  loss: 0.1352 (0.0803)  time: 3.8362  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [1500/3449]  eta: 2:02:31  lr: 0.000100  loss: 0.1458 (0.0807)  time: 3.8298  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1510/3449]  eta: 2:01:53  lr: 0.000100  loss: 0.0846 (0.0807)  time: 3.8052  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1520/3449]  eta: 2:01:15  lr: 0.000100  loss: 0.0546 (0.0804)  time: 3.7787  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1530/3449]  eta: 2:00:38  lr: 0.000100  loss: 0.0419 (0.0802)  time: 3.7894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1540/3449]  eta: 2:00:01  lr: 0.000100  loss: 0.0434 (0.0800)  time: 3.8224  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1550/3449]  eta: 1:59:24  lr: 0.000100  loss: 0.0568 (0.0799)  time: 3.8290  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1560/3449]  eta: 1:58:47  lr: 0.000100  loss: 0.0620 (0.0798)  time: 3.8360  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [1570/3449]  eta: 1:58:09  lr: 0.000100  loss: 0.0718 (0.0798)  time: 3.7899  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [1580/3449]  eta: 1:57:32  lr: 0.000100  loss: 0.0699 (0.0797)  time: 3.7715  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [1590/3449]  eta: 1:56:53  lr: 0.000100  loss: 0.0699 (0.0797)  time: 3.7625  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1600/3449]  eta: 1:56:16  lr: 0.000100  loss: 0.0744 (0.0797)  time: 3.7631  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1610/3449]  eta: 1:55:39  lr: 0.000100  loss: 0.0693 (0.0796)  time: 3.8185  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1620/3449]  eta: 1:55:01  lr: 0.000100  loss: 0.0685 (0.0796)  time: 3.7831  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1630/3449]  eta: 1:54:23  lr: 0.000100  loss: 0.0680 (0.0795)  time: 3.7404  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1640/3449]  eta: 1:53:44  lr: 0.000100  loss: 0.0622 (0.0795)  time: 3.7352  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1650/3449]  eta: 1:53:07  lr: 0.000100  loss: 0.0735 (0.0795)  time: 3.7534  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1660/3449]  eta: 1:52:29  lr: 0.000100  loss: 0.0713 (0.0794)  time: 3.7851  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1670/3449]  eta: 1:51:52  lr: 0.000100  loss: 0.0547 (0.0792)  time: 3.8081  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1680/3449]  eta: 1:51:14  lr: 0.000100  loss: 0.0398 (0.0790)  time: 3.7757  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1690/3449]  eta: 1:50:36  lr: 0.000100  loss: 0.0526 (0.0790)  time: 3.7467  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1700/3449]  eta: 1:49:58  lr: 0.000100  loss: 0.0702 (0.0789)  time: 3.7637  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1710/3449]  eta: 1:49:19  lr: 0.000100  loss: 0.0539 (0.0787)  time: 3.6934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1720/3449]  eta: 1:48:42  lr: 0.000100  loss: 0.0413 (0.0786)  time: 3.7360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1730/3449]  eta: 1:48:04  lr: 0.000100  loss: 0.0618 (0.0785)  time: 3.8106  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1740/3449]  eta: 1:47:26  lr: 0.000100  loss: 0.0637 (0.0784)  time: 3.7333  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1750/3449]  eta: 1:46:48  lr: 0.000100  loss: 0.0637 (0.0784)  time: 3.7462  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1760/3449]  eta: 1:46:11  lr: 0.000100  loss: 0.0541 (0.0782)  time: 3.7937  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1770/3449]  eta: 1:45:32  lr: 0.000100  loss: 0.0546 (0.0781)  time: 3.7003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1780/3449]  eta: 1:44:54  lr: 0.000100  loss: 0.0530 (0.0780)  time: 3.6891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1790/3449]  eta: 1:44:16  lr: 0.000100  loss: 0.0483 (0.0778)  time: 3.7636  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1800/3449]  eta: 1:43:38  lr: 0.000100  loss: 0.0625 (0.0778)  time: 3.7733  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1810/3449]  eta: 1:43:00  lr: 0.000100  loss: 0.0711 (0.0778)  time: 3.7476  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1820/3449]  eta: 1:42:23  lr: 0.000100  loss: 0.0690 (0.0777)  time: 3.7671  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [1830/3449]  eta: 1:41:45  lr: 0.000100  loss: 0.0753 (0.0777)  time: 3.7943  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [1840/3449]  eta: 1:41:07  lr: 0.000100  loss: 0.1075 (0.0780)  time: 3.7588  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1850/3449]  eta: 1:40:30  lr: 0.000100  loss: 0.1467 (0.0784)  time: 3.7659  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1860/3449]  eta: 1:39:52  lr: 0.000100  loss: 0.1576 (0.0788)  time: 3.8059  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1870/3449]  eta: 1:39:15  lr: 0.000100  loss: 0.0854 (0.0788)  time: 3.8129  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1880/3449]  eta: 1:38:37  lr: 0.000100  loss: 0.0741 (0.0788)  time: 3.7899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1890/3449]  eta: 1:38:00  lr: 0.000100  loss: 0.0640 (0.0788)  time: 3.8081  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1900/3449]  eta: 1:37:22  lr: 0.000100  loss: 0.0517 (0.0786)  time: 3.7801  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1910/3449]  eta: 1:36:45  lr: 0.000100  loss: 0.0494 (0.0786)  time: 3.7631  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1920/3449]  eta: 1:36:07  lr: 0.000100  loss: 0.0775 (0.0786)  time: 3.7863  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1930/3449]  eta: 1:35:29  lr: 0.000100  loss: 0.0742 (0.0786)  time: 3.7948  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1940/3449]  eta: 1:34:52  lr: 0.000100  loss: 0.0622 (0.0784)  time: 3.7849  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:23]  [1950/3449]  eta: 1:34:14  lr: 0.000100  loss: 0.0576 (0.0783)  time: 3.7666  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1960/3449]  eta: 1:33:37  lr: 0.000100  loss: 0.0750 (0.0784)  time: 3.7983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1970/3449]  eta: 1:32:58  lr: 0.000100  loss: 0.0716 (0.0783)  time: 3.7562  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1980/3449]  eta: 1:32:20  lr: 0.000100  loss: 0.0682 (0.0783)  time: 3.7265  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [1990/3449]  eta: 1:31:43  lr: 0.000100  loss: 0.0688 (0.0782)  time: 3.7597  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2000/3449]  eta: 1:31:04  lr: 0.000100  loss: 0.0578 (0.0781)  time: 3.7360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2010/3449]  eta: 1:30:27  lr: 0.000100  loss: 0.0480 (0.0780)  time: 3.7288  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2020/3449]  eta: 1:29:48  lr: 0.000100  loss: 0.0658 (0.0781)  time: 3.7268  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2030/3449]  eta: 1:29:11  lr: 0.000100  loss: 0.0889 (0.0781)  time: 3.7639  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2040/3449]  eta: 1:28:33  lr: 0.000100  loss: 0.0590 (0.0780)  time: 3.8071  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2050/3449]  eta: 1:27:56  lr: 0.000100  loss: 0.0526 (0.0779)  time: 3.8151  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2060/3449]  eta: 1:27:18  lr: 0.000100  loss: 0.0452 (0.0777)  time: 3.7761  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [2070/3449]  eta: 1:26:40  lr: 0.000100  loss: 0.0407 (0.0775)  time: 3.7280  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2080/3449]  eta: 1:26:02  lr: 0.000100  loss: 0.0415 (0.0774)  time: 3.7222  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2090/3449]  eta: 1:25:25  lr: 0.000100  loss: 0.0425 (0.0772)  time: 3.7763  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2100/3449]  eta: 1:24:46  lr: 0.000100  loss: 0.0391 (0.0770)  time: 3.7444  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2110/3449]  eta: 1:24:08  lr: 0.000100  loss: 0.0379 (0.0769)  time: 3.6927  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2120/3449]  eta: 1:23:31  lr: 0.000100  loss: 0.0384 (0.0767)  time: 3.7584  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2130/3449]  eta: 1:22:54  lr: 0.000100  loss: 0.0330 (0.0765)  time: 3.8279  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2140/3449]  eta: 1:22:16  lr: 0.000100  loss: 0.0382 (0.0765)  time: 3.8305  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2150/3449]  eta: 1:21:38  lr: 0.000100  loss: 0.0567 (0.0764)  time: 3.7714  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2160/3449]  eta: 1:21:00  lr: 0.000100  loss: 0.0660 (0.0764)  time: 3.7535  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2170/3449]  eta: 1:20:23  lr: 0.000100  loss: 0.0660 (0.0764)  time: 3.7890  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [2180/3449]  eta: 1:19:45  lr: 0.000100  loss: 0.0863 (0.0764)  time: 3.8193  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2190/3449]  eta: 1:19:08  lr: 0.000100  loss: 0.0822 (0.0764)  time: 3.7879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2200/3449]  eta: 1:18:30  lr: 0.000100  loss: 0.0693 (0.0764)  time: 3.7426  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [2210/3449]  eta: 1:17:52  lr: 0.000100  loss: 0.0719 (0.0764)  time: 3.7129  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [2220/3449]  eta: 1:17:14  lr: 0.000100  loss: 0.0626 (0.0763)  time: 3.7346  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2230/3449]  eta: 1:16:36  lr: 0.000100  loss: 0.0384 (0.0761)  time: 3.7710  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2240/3449]  eta: 1:15:59  lr: 0.000100  loss: 0.0373 (0.0760)  time: 3.7876  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2250/3449]  eta: 1:15:21  lr: 0.000100  loss: 0.0714 (0.0760)  time: 3.7600  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2260/3449]  eta: 1:14:43  lr: 0.000100  loss: 0.0762 (0.0760)  time: 3.7145  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2270/3449]  eta: 1:14:05  lr: 0.000100  loss: 0.0756 (0.0760)  time: 3.6948  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2280/3449]  eta: 1:13:27  lr: 0.000100  loss: 0.0661 (0.0760)  time: 3.7854  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2290/3449]  eta: 1:12:49  lr: 0.000100  loss: 0.0693 (0.0762)  time: 3.7876  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2300/3449]  eta: 1:12:11  lr: 0.000100  loss: 0.0938 (0.0762)  time: 3.7202  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2310/3449]  eta: 1:11:34  lr: 0.000100  loss: 0.0729 (0.0761)  time: 3.7451  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2320/3449]  eta: 1:10:56  lr: 0.000100  loss: 0.0707 (0.0761)  time: 3.7389  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2330/3449]  eta: 1:10:18  lr: 0.000100  loss: 0.0685 (0.0761)  time: 3.7321  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2340/3449]  eta: 1:09:40  lr: 0.000100  loss: 0.0674 (0.0761)  time: 3.7217  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2350/3449]  eta: 1:09:02  lr: 0.000100  loss: 0.0633 (0.0760)  time: 3.7075  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2360/3449]  eta: 1:08:24  lr: 0.000100  loss: 0.0503 (0.0759)  time: 3.7203  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2370/3449]  eta: 1:07:46  lr: 0.000100  loss: 0.0467 (0.0758)  time: 3.7392  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2380/3449]  eta: 1:07:09  lr: 0.000100  loss: 0.0607 (0.0760)  time: 3.7862  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2390/3449]  eta: 1:06:31  lr: 0.000100  loss: 0.1625 (0.0764)  time: 3.7797  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2400/3449]  eta: 1:05:53  lr: 0.000100  loss: 0.1328 (0.0766)  time: 3.7405  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2410/3449]  eta: 1:05:15  lr: 0.000100  loss: 0.1180 (0.0768)  time: 3.7184  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2420/3449]  eta: 1:04:38  lr: 0.000100  loss: 0.0960 (0.0768)  time: 3.7362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2430/3449]  eta: 1:04:00  lr: 0.000100  loss: 0.0749 (0.0768)  time: 3.7604  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2440/3449]  eta: 1:03:22  lr: 0.000100  loss: 0.0677 (0.0768)  time: 3.7380  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2450/3449]  eta: 1:02:44  lr: 0.000100  loss: 0.0664 (0.0768)  time: 3.7323  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2460/3449]  eta: 1:02:07  lr: 0.000100  loss: 0.0840 (0.0769)  time: 3.7776  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2470/3449]  eta: 1:01:29  lr: 0.000100  loss: 0.1002 (0.0769)  time: 3.7888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2480/3449]  eta: 1:00:51  lr: 0.000100  loss: 0.1002 (0.0770)  time: 3.7347  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2490/3449]  eta: 1:00:13  lr: 0.000100  loss: 0.0796 (0.0770)  time: 3.7114  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2500/3449]  eta: 0:59:35  lr: 0.000100  loss: 0.0763 (0.0770)  time: 3.7200  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2510/3449]  eta: 0:58:57  lr: 0.000100  loss: 0.0821 (0.0770)  time: 3.7381  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2520/3449]  eta: 0:58:20  lr: 0.000100  loss: 0.0843 (0.0770)  time: 3.7969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2530/3449]  eta: 0:57:42  lr: 0.000100  loss: 0.0703 (0.0770)  time: 3.7969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2540/3449]  eta: 0:57:04  lr: 0.000100  loss: 0.0588 (0.0769)  time: 3.7320  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2550/3449]  eta: 0:56:27  lr: 0.000100  loss: 0.0653 (0.0769)  time: 3.7132  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2560/3449]  eta: 0:55:49  lr: 0.000100  loss: 0.0616 (0.0768)  time: 3.7320  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2570/3449]  eta: 0:55:11  lr: 0.000100  loss: 0.0640 (0.0768)  time: 3.7773  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2580/3449]  eta: 0:54:34  lr: 0.000100  loss: 0.0640 (0.0768)  time: 3.7828  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2590/3449]  eta: 0:53:56  lr: 0.000100  loss: 0.0648 (0.0768)  time: 3.7614  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2600/3449]  eta: 0:53:18  lr: 0.000100  loss: 0.0732 (0.0768)  time: 3.7546  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:23]  [2610/3449]  eta: 0:52:40  lr: 0.000100  loss: 0.0732 (0.0768)  time: 3.7273  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2620/3449]  eta: 0:52:02  lr: 0.000100  loss: 0.0569 (0.0767)  time: 3.7016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2630/3449]  eta: 0:51:25  lr: 0.000100  loss: 0.0583 (0.0767)  time: 3.7860  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2640/3449]  eta: 0:50:47  lr: 0.000100  loss: 0.0598 (0.0766)  time: 3.7821  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2650/3449]  eta: 0:50:09  lr: 0.000100  loss: 0.0695 (0.0766)  time: 3.7229  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2660/3449]  eta: 0:49:32  lr: 0.000100  loss: 0.1005 (0.0768)  time: 3.7458  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2670/3449]  eta: 0:48:54  lr: 0.000100  loss: 0.1277 (0.0769)  time: 3.7801  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2680/3449]  eta: 0:48:17  lr: 0.000100  loss: 0.0705 (0.0769)  time: 3.8128  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2690/3449]  eta: 0:47:39  lr: 0.000100  loss: 0.0456 (0.0768)  time: 3.7792  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2700/3449]  eta: 0:47:01  lr: 0.000100  loss: 0.0398 (0.0766)  time: 3.7245  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2710/3449]  eta: 0:46:23  lr: 0.000100  loss: 0.0444 (0.0766)  time: 3.7365  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2720/3449]  eta: 0:45:46  lr: 0.000100  loss: 0.0569 (0.0765)  time: 3.7690  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2730/3449]  eta: 0:45:08  lr: 0.000100  loss: 0.0569 (0.0766)  time: 3.7983  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [2740/3449]  eta: 0:44:30  lr: 0.000100  loss: 0.1071 (0.0768)  time: 3.7742  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2750/3449]  eta: 0:43:53  lr: 0.000100  loss: 0.0874 (0.0767)  time: 3.7405  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2760/3449]  eta: 0:43:15  lr: 0.000100  loss: 0.0563 (0.0767)  time: 3.7843  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2770/3449]  eta: 0:42:37  lr: 0.000100  loss: 0.0563 (0.0766)  time: 3.7793  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2780/3449]  eta: 0:42:00  lr: 0.000100  loss: 0.0683 (0.0767)  time: 3.8115  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2790/3449]  eta: 0:41:22  lr: 0.000100  loss: 0.0686 (0.0767)  time: 3.8209  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2800/3449]  eta: 0:40:45  lr: 0.000100  loss: 0.0543 (0.0766)  time: 3.8344  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2810/3449]  eta: 0:40:07  lr: 0.000100  loss: 0.0498 (0.0765)  time: 3.8020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2820/3449]  eta: 0:39:29  lr: 0.000100  loss: 0.0465 (0.0764)  time: 3.7344  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2830/3449]  eta: 0:38:52  lr: 0.000100  loss: 0.0480 (0.0763)  time: 3.7650  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2840/3449]  eta: 0:38:14  lr: 0.000100  loss: 0.0540 (0.0762)  time: 3.7582  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2850/3449]  eta: 0:37:36  lr: 0.000100  loss: 0.0993 (0.0764)  time: 3.7396  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [2860/3449]  eta: 0:36:59  lr: 0.000100  loss: 0.1150 (0.0765)  time: 3.7425  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2870/3449]  eta: 0:36:21  lr: 0.000100  loss: 0.1141 (0.0766)  time: 3.8277  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2880/3449]  eta: 0:35:43  lr: 0.000100  loss: 0.1082 (0.0767)  time: 3.8269  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2890/3449]  eta: 0:35:06  lr: 0.000100  loss: 0.0496 (0.0766)  time: 3.7562  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2900/3449]  eta: 0:34:28  lr: 0.000100  loss: 0.0548 (0.0765)  time: 3.7890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2910/3449]  eta: 0:33:51  lr: 0.000100  loss: 0.0628 (0.0765)  time: 3.8059  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2920/3449]  eta: 0:33:13  lr: 0.000100  loss: 0.0517 (0.0764)  time: 3.7644  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2930/3449]  eta: 0:32:35  lr: 0.000100  loss: 0.0451 (0.0763)  time: 3.7259  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2940/3449]  eta: 0:31:57  lr: 0.000100  loss: 0.0382 (0.0761)  time: 3.7042  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2950/3449]  eta: 0:31:20  lr: 0.000100  loss: 0.0321 (0.0760)  time: 3.7296  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2960/3449]  eta: 0:30:42  lr: 0.000100  loss: 0.0350 (0.0759)  time: 3.8297  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2970/3449]  eta: 0:30:04  lr: 0.000100  loss: 0.0371 (0.0758)  time: 3.8031  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2980/3449]  eta: 0:29:27  lr: 0.000100  loss: 0.0362 (0.0756)  time: 3.7470  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [2990/3449]  eta: 0:28:49  lr: 0.000100  loss: 0.0374 (0.0755)  time: 3.7547  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3000/3449]  eta: 0:28:11  lr: 0.000100  loss: 0.0374 (0.0754)  time: 3.7292  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [3010/3449]  eta: 0:27:33  lr: 0.000100  loss: 0.0374 (0.0753)  time: 3.7345  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3020/3449]  eta: 0:26:56  lr: 0.000100  loss: 0.0407 (0.0752)  time: 3.8430  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3030/3449]  eta: 0:26:18  lr: 0.000100  loss: 0.0407 (0.0751)  time: 3.8293  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3040/3449]  eta: 0:25:41  lr: 0.000100  loss: 0.0438 (0.0751)  time: 3.7398  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3050/3449]  eta: 0:25:03  lr: 0.000100  loss: 0.0728 (0.0751)  time: 3.7510  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3060/3449]  eta: 0:24:25  lr: 0.000100  loss: 0.0749 (0.0751)  time: 3.7328  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3070/3449]  eta: 0:23:48  lr: 0.000100  loss: 0.0778 (0.0751)  time: 3.7595  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3080/3449]  eta: 0:23:10  lr: 0.000100  loss: 0.0699 (0.0752)  time: 3.7745  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3090/3449]  eta: 0:22:32  lr: 0.000100  loss: 0.0744 (0.0752)  time: 3.8105  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3100/3449]  eta: 0:21:54  lr: 0.000100  loss: 0.0744 (0.0752)  time: 3.7133  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3110/3449]  eta: 0:21:17  lr: 0.000100  loss: 0.0829 (0.0753)  time: 3.6613  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3120/3449]  eta: 0:20:39  lr: 0.000100  loss: 0.0683 (0.0752)  time: 3.7483  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3130/3449]  eta: 0:20:01  lr: 0.000100  loss: 0.0546 (0.0751)  time: 3.7440  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3140/3449]  eta: 0:19:24  lr: 0.000100  loss: 0.0453 (0.0750)  time: 3.8032  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3150/3449]  eta: 0:18:46  lr: 0.000100  loss: 0.0470 (0.0750)  time: 3.8321  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3160/3449]  eta: 0:18:08  lr: 0.000100  loss: 0.0859 (0.0752)  time: 3.7689  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3170/3449]  eta: 0:17:31  lr: 0.000100  loss: 0.1513 (0.0755)  time: 3.7235  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3180/3449]  eta: 0:16:53  lr: 0.000100  loss: 0.1344 (0.0757)  time: 3.7215  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3190/3449]  eta: 0:16:15  lr: 0.000100  loss: 0.1029 (0.0757)  time: 3.7192  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3200/3449]  eta: 0:15:38  lr: 0.000100  loss: 0.0679 (0.0756)  time: 3.7988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3210/3449]  eta: 0:15:00  lr: 0.000100  loss: 0.0636 (0.0756)  time: 3.8079  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [3220/3449]  eta: 0:14:22  lr: 0.000100  loss: 0.0539 (0.0755)  time: 3.7492  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3230/3449]  eta: 0:13:45  lr: 0.000100  loss: 0.0539 (0.0755)  time: 3.7384  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3240/3449]  eta: 0:13:07  lr: 0.000100  loss: 0.0714 (0.0755)  time: 3.7334  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3250/3449]  eta: 0:12:29  lr: 0.000100  loss: 0.0770 (0.0755)  time: 3.7522  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3260/3449]  eta: 0:11:51  lr: 0.000100  loss: 0.0848 (0.0756)  time: 3.7205  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:23]  [3270/3449]  eta: 0:11:14  lr: 0.000100  loss: 0.0746 (0.0755)  time: 3.6998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3280/3449]  eta: 0:10:36  lr: 0.000100  loss: 0.0598 (0.0755)  time: 3.7784  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3290/3449]  eta: 0:09:58  lr: 0.000100  loss: 0.0494 (0.0754)  time: 3.8123  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3300/3449]  eta: 0:09:21  lr: 0.000100  loss: 0.0531 (0.0753)  time: 3.7575  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3310/3449]  eta: 0:08:43  lr: 0.000100  loss: 0.0508 (0.0753)  time: 3.8267  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3320/3449]  eta: 0:08:05  lr: 0.000100  loss: 0.0448 (0.0752)  time: 3.8161  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3330/3449]  eta: 0:07:28  lr: 0.000100  loss: 0.0479 (0.0751)  time: 3.7311  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3340/3449]  eta: 0:06:50  lr: 0.000100  loss: 0.0475 (0.0750)  time: 3.7366  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3350/3449]  eta: 0:06:12  lr: 0.000100  loss: 0.0553 (0.0750)  time: 3.7861  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3360/3449]  eta: 0:05:35  lr: 0.000100  loss: 0.0591 (0.0749)  time: 3.7988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3370/3449]  eta: 0:04:57  lr: 0.000100  loss: 0.0581 (0.0749)  time: 3.7282  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3380/3449]  eta: 0:04:19  lr: 0.000100  loss: 0.0673 (0.0749)  time: 3.6938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [3390/3449]  eta: 0:03:42  lr: 0.000100  loss: 0.0489 (0.0748)  time: 3.7196  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [3400/3449]  eta: 0:03:04  lr: 0.000100  loss: 0.0326 (0.0747)  time: 3.7711  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3410/3449]  eta: 0:02:26  lr: 0.000100  loss: 0.0386 (0.0746)  time: 3.7772  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3420/3449]  eta: 0:01:49  lr: 0.000100  loss: 0.0386 (0.0745)  time: 3.7315  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:23]  [3430/3449]  eta: 0:01:11  lr: 0.000100  loss: 0.0369 (0.0744)  time: 3.7032  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3440/3449]  eta: 0:00:33  lr: 0.000100  loss: 0.0428 (0.0743)  time: 3.7701  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0636 (0.0743)  time: 3.7547  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:23] Total time: 3:36:30 (3.7665 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0636 (0.0743)\n",
      "Valid: [epoch:23]  [ 0/14]  eta: 0:04:21  loss: 0.0516 (0.0516)  time: 18.6573  data: 0.4684  max mem: 34968\n",
      "Valid: [epoch:23]  [13/14]  eta: 0:00:18  loss: 0.0501 (0.0504)  time: 18.2381  data: 0.0336  max mem: 34968\n",
      "Valid: [epoch:23] Total time: 0:04:15 (18.2477 s / it)\n",
      "Averaged stats: loss: 0.0501 (0.0504)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_23_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.050%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:24]  [   0/3449]  eta: 4:51:44  lr: 0.000100  loss: 0.0671 (0.0671)  time: 5.0753  data: 1.3822  max mem: 34968\n",
      "Train: [epoch:24]  [  10/3449]  eta: 3:40:22  lr: 0.000100  loss: 0.0807 (0.0889)  time: 3.8448  data: 0.1258  max mem: 34968\n",
      "Train: [epoch:24]  [  20/3449]  eta: 3:34:53  lr: 0.000100  loss: 0.1016 (0.1180)  time: 3.6945  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [  30/3449]  eta: 3:34:59  lr: 0.000100  loss: 0.1093 (0.1098)  time: 3.7336  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [  40/3449]  eta: 3:33:43  lr: 0.000100  loss: 0.1209 (0.1259)  time: 3.7634  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [  50/3449]  eta: 3:32:58  lr: 0.000100  loss: 0.1850 (0.1410)  time: 3.7383  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [  60/3449]  eta: 3:31:52  lr: 0.000100  loss: 0.1344 (0.1364)  time: 3.7295  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [  70/3449]  eta: 3:30:56  lr: 0.000100  loss: 0.0984 (0.1306)  time: 3.7108  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [  80/3449]  eta: 3:30:23  lr: 0.000100  loss: 0.0736 (0.1229)  time: 3.7335  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [  90/3449]  eta: 3:29:59  lr: 0.000100  loss: 0.0577 (0.1155)  time: 3.7691  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 100/3449]  eta: 3:29:39  lr: 0.000100  loss: 0.0545 (0.1108)  time: 3.7947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 110/3449]  eta: 3:28:50  lr: 0.000100  loss: 0.0570 (0.1060)  time: 3.7613  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 120/3449]  eta: 3:28:11  lr: 0.000100  loss: 0.0519 (0.1009)  time: 3.7320  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 130/3449]  eta: 3:27:34  lr: 0.000100  loss: 0.0617 (0.1000)  time: 3.7507  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 140/3449]  eta: 3:26:57  lr: 0.000100  loss: 0.0893 (0.0990)  time: 3.7540  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 150/3449]  eta: 3:26:14  lr: 0.000100  loss: 0.0907 (0.0983)  time: 3.7418  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 160/3449]  eta: 3:25:26  lr: 0.000100  loss: 0.0907 (0.0974)  time: 3.7153  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 170/3449]  eta: 3:24:40  lr: 0.000100  loss: 0.0806 (0.0960)  time: 3.7024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 180/3449]  eta: 3:23:57  lr: 0.000100  loss: 0.0694 (0.0945)  time: 3.7092  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 190/3449]  eta: 3:23:17  lr: 0.000100  loss: 0.0574 (0.0921)  time: 3.7205  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 200/3449]  eta: 3:22:43  lr: 0.000100  loss: 0.0591 (0.0917)  time: 3.7448  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 210/3449]  eta: 3:21:59  lr: 0.000100  loss: 0.0765 (0.0911)  time: 3.7311  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 220/3449]  eta: 3:21:37  lr: 0.000100  loss: 0.0734 (0.0910)  time: 3.7758  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 230/3449]  eta: 3:20:57  lr: 0.000100  loss: 0.0804 (0.0909)  time: 3.7906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 240/3449]  eta: 3:20:22  lr: 0.000100  loss: 0.0794 (0.0903)  time: 3.7439  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 250/3449]  eta: 3:19:55  lr: 0.000100  loss: 0.0794 (0.0901)  time: 3.7951  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 260/3449]  eta: 3:19:10  lr: 0.000100  loss: 0.0604 (0.0885)  time: 3.7585  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 270/3449]  eta: 3:18:35  lr: 0.000100  loss: 0.0639 (0.0885)  time: 3.7300  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 280/3449]  eta: 3:18:03  lr: 0.000100  loss: 0.0845 (0.0881)  time: 3.7836  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 290/3449]  eta: 3:17:28  lr: 0.000100  loss: 0.0704 (0.0878)  time: 3.7816  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 300/3449]  eta: 3:16:49  lr: 0.000100  loss: 0.0843 (0.0880)  time: 3.7530  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 310/3449]  eta: 3:16:07  lr: 0.000100  loss: 0.0701 (0.0871)  time: 3.7211  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 320/3449]  eta: 3:15:29  lr: 0.000100  loss: 0.0693 (0.0872)  time: 3.7248  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [ 330/3449]  eta: 3:14:53  lr: 0.000100  loss: 0.0915 (0.0877)  time: 3.7553  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 340/3449]  eta: 3:14:11  lr: 0.000100  loss: 0.0895 (0.0879)  time: 3.7310  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 350/3449]  eta: 3:13:41  lr: 0.000100  loss: 0.0818 (0.0876)  time: 3.7680  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 360/3449]  eta: 3:13:08  lr: 0.000100  loss: 0.0802 (0.0874)  time: 3.8209  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 370/3449]  eta: 3:12:30  lr: 0.000100  loss: 0.0833 (0.0870)  time: 3.7705  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 380/3449]  eta: 3:11:52  lr: 0.000100  loss: 0.0775 (0.0870)  time: 3.7435  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [ 390/3449]  eta: 3:11:14  lr: 0.000100  loss: 0.0658 (0.0860)  time: 3.7454  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 400/3449]  eta: 3:10:46  lr: 0.000100  loss: 0.0404 (0.0848)  time: 3.8088  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 410/3449]  eta: 3:10:06  lr: 0.000100  loss: 0.0404 (0.0839)  time: 3.8015  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:24]  [ 420/3449]  eta: 3:09:38  lr: 0.000100  loss: 0.0494 (0.0832)  time: 3.8088  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 430/3449]  eta: 3:09:01  lr: 0.000100  loss: 0.0701 (0.0832)  time: 3.8219  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 440/3449]  eta: 3:08:21  lr: 0.000100  loss: 0.0755 (0.0830)  time: 3.7425  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 450/3449]  eta: 3:07:41  lr: 0.000100  loss: 0.0775 (0.0830)  time: 3.7217  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 460/3449]  eta: 3:07:03  lr: 0.000100  loss: 0.0775 (0.0827)  time: 3.7282  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 470/3449]  eta: 3:06:26  lr: 0.000100  loss: 0.0835 (0.0828)  time: 3.7556  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 480/3449]  eta: 3:05:52  lr: 0.000100  loss: 0.0835 (0.0827)  time: 3.7882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 490/3449]  eta: 3:05:12  lr: 0.000100  loss: 0.0749 (0.0826)  time: 3.7626  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 500/3449]  eta: 3:04:33  lr: 0.000100  loss: 0.0737 (0.0824)  time: 3.7225  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 510/3449]  eta: 3:03:55  lr: 0.000100  loss: 0.0618 (0.0819)  time: 3.7414  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 520/3449]  eta: 3:03:20  lr: 0.000100  loss: 0.0579 (0.0817)  time: 3.7772  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 530/3449]  eta: 3:02:40  lr: 0.000100  loss: 0.0707 (0.0815)  time: 3.7595  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 540/3449]  eta: 3:02:03  lr: 0.000100  loss: 0.0518 (0.0807)  time: 3.7388  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 550/3449]  eta: 3:01:25  lr: 0.000100  loss: 0.0368 (0.0799)  time: 3.7483  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 560/3449]  eta: 3:00:51  lr: 0.000100  loss: 0.0322 (0.0791)  time: 3.7810  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 570/3449]  eta: 3:00:13  lr: 0.000100  loss: 0.0338 (0.0785)  time: 3.7906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 580/3449]  eta: 2:59:33  lr: 0.000100  loss: 0.0539 (0.0784)  time: 3.7345  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 590/3449]  eta: 2:58:56  lr: 0.000100  loss: 0.0793 (0.0789)  time: 3.7378  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 600/3449]  eta: 2:58:23  lr: 0.000100  loss: 0.1022 (0.0798)  time: 3.8088  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 610/3449]  eta: 2:57:49  lr: 0.000100  loss: 0.0860 (0.0797)  time: 3.8360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 620/3449]  eta: 2:57:09  lr: 0.000100  loss: 0.0622 (0.0793)  time: 3.7676  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 630/3449]  eta: 2:56:31  lr: 0.000100  loss: 0.0630 (0.0791)  time: 3.7361  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 640/3449]  eta: 2:55:57  lr: 0.000100  loss: 0.0361 (0.0784)  time: 3.7967  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 650/3449]  eta: 2:55:23  lr: 0.000100  loss: 0.0338 (0.0778)  time: 3.8352  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 660/3449]  eta: 2:54:45  lr: 0.000100  loss: 0.0335 (0.0772)  time: 3.7957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 670/3449]  eta: 2:54:04  lr: 0.000100  loss: 0.0335 (0.0766)  time: 3.7185  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 680/3449]  eta: 2:53:27  lr: 0.000100  loss: 0.0385 (0.0764)  time: 3.7242  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 690/3449]  eta: 2:52:51  lr: 0.000100  loss: 0.0617 (0.0762)  time: 3.7832  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 700/3449]  eta: 2:52:16  lr: 0.000100  loss: 0.0594 (0.0761)  time: 3.8041  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 710/3449]  eta: 2:51:36  lr: 0.000100  loss: 0.0853 (0.0765)  time: 3.7641  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 720/3449]  eta: 2:50:57  lr: 0.000100  loss: 0.0927 (0.0766)  time: 3.7084  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [ 730/3449]  eta: 2:50:21  lr: 0.000100  loss: 0.0815 (0.0768)  time: 3.7550  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 740/3449]  eta: 2:49:41  lr: 0.000100  loss: 0.0895 (0.0770)  time: 3.7534  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 750/3449]  eta: 2:49:05  lr: 0.000100  loss: 0.0860 (0.0769)  time: 3.7488  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 760/3449]  eta: 2:48:27  lr: 0.000100  loss: 0.0580 (0.0767)  time: 3.7792  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 770/3449]  eta: 2:47:49  lr: 0.000100  loss: 0.0669 (0.0767)  time: 3.7457  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 780/3449]  eta: 2:47:11  lr: 0.000100  loss: 0.0759 (0.0769)  time: 3.7434  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 790/3449]  eta: 2:46:32  lr: 0.000100  loss: 0.0983 (0.0776)  time: 3.7277  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 800/3449]  eta: 2:45:55  lr: 0.000100  loss: 0.0872 (0.0775)  time: 3.7401  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 810/3449]  eta: 2:45:17  lr: 0.000100  loss: 0.0567 (0.0771)  time: 3.7642  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 820/3449]  eta: 2:44:38  lr: 0.000100  loss: 0.0400 (0.0766)  time: 3.7380  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 830/3449]  eta: 2:44:02  lr: 0.000100  loss: 0.0360 (0.0762)  time: 3.7661  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 840/3449]  eta: 2:43:26  lr: 0.000100  loss: 0.0384 (0.0758)  time: 3.7997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 850/3449]  eta: 2:42:48  lr: 0.000100  loss: 0.0642 (0.0760)  time: 3.7683  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 860/3449]  eta: 2:42:12  lr: 0.000100  loss: 0.1012 (0.0766)  time: 3.7784  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 870/3449]  eta: 2:41:34  lr: 0.000100  loss: 0.1395 (0.0773)  time: 3.7764  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 880/3449]  eta: 2:40:55  lr: 0.000100  loss: 0.1248 (0.0779)  time: 3.7306  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 890/3449]  eta: 2:40:19  lr: 0.000100  loss: 0.1094 (0.0780)  time: 3.7612  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 900/3449]  eta: 2:39:41  lr: 0.000100  loss: 0.0768 (0.0779)  time: 3.7812  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 910/3449]  eta: 2:39:03  lr: 0.000100  loss: 0.0694 (0.0779)  time: 3.7511  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 920/3449]  eta: 2:38:28  lr: 0.000100  loss: 0.0760 (0.0778)  time: 3.7936  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 930/3449]  eta: 2:37:50  lr: 0.000100  loss: 0.0753 (0.0778)  time: 3.7960  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 940/3449]  eta: 2:37:13  lr: 0.000100  loss: 0.0866 (0.0781)  time: 3.7630  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 950/3449]  eta: 2:36:34  lr: 0.000100  loss: 0.1125 (0.0786)  time: 3.7447  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 960/3449]  eta: 2:35:58  lr: 0.000100  loss: 0.1074 (0.0789)  time: 3.7639  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 970/3449]  eta: 2:35:20  lr: 0.000100  loss: 0.0877 (0.0789)  time: 3.7920  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 980/3449]  eta: 2:34:43  lr: 0.000100  loss: 0.0775 (0.0789)  time: 3.7647  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [ 990/3449]  eta: 2:34:05  lr: 0.000100  loss: 0.0848 (0.0792)  time: 3.7534  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1000/3449]  eta: 2:33:27  lr: 0.000100  loss: 0.0797 (0.0791)  time: 3.7449  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1010/3449]  eta: 2:32:48  lr: 0.000100  loss: 0.0688 (0.0791)  time: 3.7284  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1020/3449]  eta: 2:32:13  lr: 0.000100  loss: 0.0812 (0.0792)  time: 3.7776  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1030/3449]  eta: 2:31:35  lr: 0.000100  loss: 0.0869 (0.0793)  time: 3.8034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1040/3449]  eta: 2:30:56  lr: 0.000100  loss: 0.0723 (0.0792)  time: 3.7391  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1050/3449]  eta: 2:30:17  lr: 0.000100  loss: 0.0709 (0.0791)  time: 3.6990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1060/3449]  eta: 2:29:39  lr: 0.000100  loss: 0.0714 (0.0792)  time: 3.7137  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1070/3449]  eta: 2:29:01  lr: 0.000100  loss: 0.0873 (0.0791)  time: 3.7342  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:24]  [1080/3449]  eta: 2:28:22  lr: 0.000100  loss: 0.0659 (0.0789)  time: 3.7080  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1090/3449]  eta: 2:27:46  lr: 0.000100  loss: 0.0555 (0.0787)  time: 3.7743  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1100/3449]  eta: 2:27:09  lr: 0.000100  loss: 0.0449 (0.0784)  time: 3.8204  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1110/3449]  eta: 2:26:30  lr: 0.000100  loss: 0.0388 (0.0781)  time: 3.7406  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1120/3449]  eta: 2:25:52  lr: 0.000100  loss: 0.0403 (0.0778)  time: 3.7195  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1130/3449]  eta: 2:25:18  lr: 0.000100  loss: 0.0511 (0.0776)  time: 3.8176  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1140/3449]  eta: 2:24:39  lr: 0.000100  loss: 0.0471 (0.0774)  time: 3.8136  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1150/3449]  eta: 2:24:01  lr: 0.000100  loss: 0.0481 (0.0771)  time: 3.7347  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1160/3449]  eta: 2:23:22  lr: 0.000100  loss: 0.0546 (0.0769)  time: 3.7089  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1170/3449]  eta: 2:22:44  lr: 0.000100  loss: 0.0509 (0.0767)  time: 3.7114  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1180/3449]  eta: 2:22:06  lr: 0.000100  loss: 0.0507 (0.0764)  time: 3.7170  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1190/3449]  eta: 2:21:28  lr: 0.000100  loss: 0.0658 (0.0766)  time: 3.7250  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1200/3449]  eta: 2:20:50  lr: 0.000100  loss: 0.0891 (0.0770)  time: 3.7441  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1210/3449]  eta: 2:20:12  lr: 0.000100  loss: 0.0920 (0.0770)  time: 3.7459  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1220/3449]  eta: 2:19:35  lr: 0.000100  loss: 0.0920 (0.0771)  time: 3.7529  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1230/3449]  eta: 2:18:56  lr: 0.000100  loss: 0.0645 (0.0769)  time: 3.7339  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1240/3449]  eta: 2:18:20  lr: 0.000100  loss: 0.0425 (0.0766)  time: 3.7737  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1250/3449]  eta: 2:17:42  lr: 0.000100  loss: 0.0335 (0.0764)  time: 3.7800  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1260/3449]  eta: 2:17:04  lr: 0.000100  loss: 0.0355 (0.0761)  time: 3.7330  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1270/3449]  eta: 2:16:25  lr: 0.000100  loss: 0.0367 (0.0758)  time: 3.7157  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1280/3449]  eta: 2:15:48  lr: 0.000100  loss: 0.0445 (0.0756)  time: 3.7188  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1290/3449]  eta: 2:15:10  lr: 0.000100  loss: 0.0445 (0.0754)  time: 3.7417  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1300/3449]  eta: 2:14:32  lr: 0.000100  loss: 0.0433 (0.0752)  time: 3.7414  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1310/3449]  eta: 2:13:55  lr: 0.000100  loss: 0.0430 (0.0750)  time: 3.7785  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1320/3449]  eta: 2:13:18  lr: 0.000100  loss: 0.0502 (0.0751)  time: 3.7963  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1330/3449]  eta: 2:12:40  lr: 0.000100  loss: 0.0787 (0.0752)  time: 3.7491  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1340/3449]  eta: 2:12:02  lr: 0.000100  loss: 0.0801 (0.0753)  time: 3.7261  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1350/3449]  eta: 2:11:24  lr: 0.000100  loss: 0.1117 (0.0755)  time: 3.7368  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1360/3449]  eta: 2:10:46  lr: 0.000100  loss: 0.0759 (0.0754)  time: 3.7434  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1370/3449]  eta: 2:10:08  lr: 0.000100  loss: 0.0538 (0.0752)  time: 3.7303  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1380/3449]  eta: 2:09:31  lr: 0.000100  loss: 0.0480 (0.0750)  time: 3.7388  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1390/3449]  eta: 2:08:52  lr: 0.000100  loss: 0.0447 (0.0748)  time: 3.7312  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1400/3449]  eta: 2:08:17  lr: 0.000100  loss: 0.0382 (0.0745)  time: 3.7972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1410/3449]  eta: 2:07:39  lr: 0.000100  loss: 0.0423 (0.0744)  time: 3.8018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1420/3449]  eta: 2:07:02  lr: 0.000100  loss: 0.0573 (0.0743)  time: 3.7485  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1430/3449]  eta: 2:06:24  lr: 0.000100  loss: 0.0646 (0.0743)  time: 3.7590  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1440/3449]  eta: 2:05:46  lr: 0.000100  loss: 0.0738 (0.0744)  time: 3.7338  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1450/3449]  eta: 2:05:07  lr: 0.000100  loss: 0.0876 (0.0744)  time: 3.6849  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1460/3449]  eta: 2:04:30  lr: 0.000100  loss: 0.0685 (0.0744)  time: 3.7360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1470/3449]  eta: 2:03:53  lr: 0.000100  loss: 0.0518 (0.0742)  time: 3.8274  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1480/3449]  eta: 2:03:15  lr: 0.000100  loss: 0.0395 (0.0740)  time: 3.7685  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1490/3449]  eta: 2:02:37  lr: 0.000100  loss: 0.0492 (0.0738)  time: 3.7276  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1500/3449]  eta: 2:02:00  lr: 0.000100  loss: 0.0550 (0.0738)  time: 3.7313  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1510/3449]  eta: 2:01:22  lr: 0.000100  loss: 0.0794 (0.0739)  time: 3.7354  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1520/3449]  eta: 2:00:45  lr: 0.000100  loss: 0.0948 (0.0741)  time: 3.7950  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1530/3449]  eta: 2:00:07  lr: 0.000100  loss: 0.0930 (0.0741)  time: 3.7828  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1540/3449]  eta: 1:59:28  lr: 0.000100  loss: 0.0734 (0.0741)  time: 3.6755  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1550/3449]  eta: 1:58:52  lr: 0.000100  loss: 0.0734 (0.0741)  time: 3.7252  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1560/3449]  eta: 1:58:14  lr: 0.000100  loss: 0.0560 (0.0740)  time: 3.7765  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1570/3449]  eta: 1:57:36  lr: 0.000100  loss: 0.0509 (0.0738)  time: 3.7508  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1580/3449]  eta: 1:56:58  lr: 0.000100  loss: 0.0492 (0.0737)  time: 3.7387  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1590/3449]  eta: 1:56:20  lr: 0.000100  loss: 0.0511 (0.0737)  time: 3.7081  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1600/3449]  eta: 1:55:42  lr: 0.000100  loss: 0.0608 (0.0737)  time: 3.7247  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1610/3449]  eta: 1:55:06  lr: 0.000100  loss: 0.0814 (0.0738)  time: 3.7908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1620/3449]  eta: 1:54:29  lr: 0.000100  loss: 0.0828 (0.0739)  time: 3.8121  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1630/3449]  eta: 1:53:51  lr: 0.000100  loss: 0.0831 (0.0739)  time: 3.7546  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1640/3449]  eta: 1:53:13  lr: 0.000100  loss: 0.0722 (0.0739)  time: 3.7499  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1650/3449]  eta: 1:52:37  lr: 0.000100  loss: 0.0481 (0.0736)  time: 3.8239  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1660/3449]  eta: 1:52:00  lr: 0.000100  loss: 0.0395 (0.0735)  time: 3.8308  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1670/3449]  eta: 1:51:22  lr: 0.000100  loss: 0.0586 (0.0735)  time: 3.7467  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1680/3449]  eta: 1:50:44  lr: 0.000100  loss: 0.0903 (0.0736)  time: 3.7097  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1690/3449]  eta: 1:50:07  lr: 0.000100  loss: 0.0856 (0.0736)  time: 3.7746  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1700/3449]  eta: 1:49:30  lr: 0.000100  loss: 0.0761 (0.0737)  time: 3.8168  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1710/3449]  eta: 1:48:52  lr: 0.000100  loss: 0.0971 (0.0739)  time: 3.7721  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1720/3449]  eta: 1:48:14  lr: 0.000100  loss: 0.0948 (0.0739)  time: 3.7360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1730/3449]  eta: 1:47:37  lr: 0.000100  loss: 0.0482 (0.0738)  time: 3.7380  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:24]  [1740/3449]  eta: 1:47:00  lr: 0.000100  loss: 0.0423 (0.0736)  time: 3.8211  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1750/3449]  eta: 1:46:22  lr: 0.000100  loss: 0.0413 (0.0734)  time: 3.8047  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1760/3449]  eta: 1:45:45  lr: 0.000100  loss: 0.0459 (0.0733)  time: 3.7294  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1770/3449]  eta: 1:45:07  lr: 0.000100  loss: 0.0501 (0.0732)  time: 3.7307  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1780/3449]  eta: 1:44:29  lr: 0.000100  loss: 0.0472 (0.0731)  time: 3.7149  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1790/3449]  eta: 1:43:51  lr: 0.000100  loss: 0.0610 (0.0731)  time: 3.7300  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1800/3449]  eta: 1:43:14  lr: 0.000100  loss: 0.0776 (0.0732)  time: 3.7547  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1810/3449]  eta: 1:42:36  lr: 0.000100  loss: 0.0750 (0.0731)  time: 3.7428  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1820/3449]  eta: 1:41:58  lr: 0.000100  loss: 0.0657 (0.0732)  time: 3.7234  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1830/3449]  eta: 1:41:21  lr: 0.000100  loss: 0.0670 (0.0731)  time: 3.7768  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1840/3449]  eta: 1:40:43  lr: 0.000100  loss: 0.0522 (0.0730)  time: 3.7508  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1850/3449]  eta: 1:40:06  lr: 0.000100  loss: 0.0497 (0.0729)  time: 3.7537  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1860/3449]  eta: 1:39:28  lr: 0.000100  loss: 0.0662 (0.0730)  time: 3.7728  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1870/3449]  eta: 1:38:50  lr: 0.000100  loss: 0.0888 (0.0731)  time: 3.7223  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1880/3449]  eta: 1:38:12  lr: 0.000100  loss: 0.0626 (0.0730)  time: 3.7345  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1890/3449]  eta: 1:37:34  lr: 0.000100  loss: 0.0498 (0.0729)  time: 3.7021  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1900/3449]  eta: 1:36:57  lr: 0.000100  loss: 0.0566 (0.0729)  time: 3.7057  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1910/3449]  eta: 1:36:19  lr: 0.000100  loss: 0.0627 (0.0728)  time: 3.7273  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1920/3449]  eta: 1:35:41  lr: 0.000100  loss: 0.0595 (0.0727)  time: 3.7277  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1930/3449]  eta: 1:35:04  lr: 0.000100  loss: 0.0562 (0.0726)  time: 3.7706  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1940/3449]  eta: 1:34:26  lr: 0.000100  loss: 0.0432 (0.0725)  time: 3.7642  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1950/3449]  eta: 1:33:49  lr: 0.000100  loss: 0.0428 (0.0724)  time: 3.7529  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1960/3449]  eta: 1:33:11  lr: 0.000100  loss: 0.0685 (0.0725)  time: 3.7524  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1970/3449]  eta: 1:32:33  lr: 0.000100  loss: 0.0628 (0.0724)  time: 3.7368  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [1980/3449]  eta: 1:31:55  lr: 0.000100  loss: 0.0494 (0.0723)  time: 3.7279  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [1990/3449]  eta: 1:31:17  lr: 0.000100  loss: 0.0652 (0.0723)  time: 3.7029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2000/3449]  eta: 1:30:40  lr: 0.000100  loss: 0.0917 (0.0724)  time: 3.7231  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2010/3449]  eta: 1:30:03  lr: 0.000100  loss: 0.0631 (0.0723)  time: 3.8036  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2020/3449]  eta: 1:29:26  lr: 0.000100  loss: 0.0449 (0.0722)  time: 3.8239  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2030/3449]  eta: 1:28:48  lr: 0.000100  loss: 0.0453 (0.0722)  time: 3.7725  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2040/3449]  eta: 1:28:10  lr: 0.000100  loss: 0.0680 (0.0722)  time: 3.7325  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2050/3449]  eta: 1:27:33  lr: 0.000100  loss: 0.0778 (0.0724)  time: 3.7269  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2060/3449]  eta: 1:26:55  lr: 0.000100  loss: 0.1129 (0.0727)  time: 3.7427  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2070/3449]  eta: 1:26:17  lr: 0.000100  loss: 0.1002 (0.0726)  time: 3.7204  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2080/3449]  eta: 1:25:39  lr: 0.000100  loss: 0.0706 (0.0726)  time: 3.7150  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2090/3449]  eta: 1:25:02  lr: 0.000100  loss: 0.0813 (0.0727)  time: 3.7362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2100/3449]  eta: 1:24:24  lr: 0.000100  loss: 0.0704 (0.0727)  time: 3.7302  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2110/3449]  eta: 1:23:46  lr: 0.000100  loss: 0.0581 (0.0726)  time: 3.7162  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2120/3449]  eta: 1:23:09  lr: 0.000100  loss: 0.0542 (0.0726)  time: 3.7809  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2130/3449]  eta: 1:22:32  lr: 0.000100  loss: 0.0614 (0.0725)  time: 3.8103  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2140/3449]  eta: 1:21:54  lr: 0.000100  loss: 0.0609 (0.0725)  time: 3.7539  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2150/3449]  eta: 1:21:17  lr: 0.000100  loss: 0.0543 (0.0724)  time: 3.7785  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2160/3449]  eta: 1:20:39  lr: 0.000100  loss: 0.0478 (0.0723)  time: 3.7791  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2170/3449]  eta: 1:20:02  lr: 0.000100  loss: 0.0436 (0.0721)  time: 3.7414  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2180/3449]  eta: 1:19:24  lr: 0.000100  loss: 0.0445 (0.0720)  time: 3.7519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2190/3449]  eta: 1:18:47  lr: 0.000100  loss: 0.0459 (0.0719)  time: 3.7485  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2200/3449]  eta: 1:18:09  lr: 0.000100  loss: 0.0484 (0.0718)  time: 3.7516  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2210/3449]  eta: 1:17:31  lr: 0.000100  loss: 0.0519 (0.0718)  time: 3.7415  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2220/3449]  eta: 1:16:54  lr: 0.000100  loss: 0.0718 (0.0718)  time: 3.7197  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2230/3449]  eta: 1:16:16  lr: 0.000100  loss: 0.0882 (0.0722)  time: 3.7185  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2240/3449]  eta: 1:15:38  lr: 0.000100  loss: 0.1660 (0.0726)  time: 3.7178  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2250/3449]  eta: 1:15:00  lr: 0.000100  loss: 0.1164 (0.0727)  time: 3.7267  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2260/3449]  eta: 1:14:23  lr: 0.000100  loss: 0.0923 (0.0728)  time: 3.7704  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2270/3449]  eta: 1:13:46  lr: 0.000100  loss: 0.0837 (0.0728)  time: 3.8019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2280/3449]  eta: 1:13:08  lr: 0.000100  loss: 0.1011 (0.0730)  time: 3.7789  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2290/3449]  eta: 1:12:31  lr: 0.000100  loss: 0.1180 (0.0731)  time: 3.7347  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2300/3449]  eta: 1:11:53  lr: 0.000100  loss: 0.0932 (0.0732)  time: 3.7255  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2310/3449]  eta: 1:11:15  lr: 0.000100  loss: 0.0767 (0.0732)  time: 3.7236  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2320/3449]  eta: 1:10:38  lr: 0.000100  loss: 0.0808 (0.0733)  time: 3.7120  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2330/3449]  eta: 1:10:00  lr: 0.000100  loss: 0.0819 (0.0734)  time: 3.7128  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2340/3449]  eta: 1:09:23  lr: 0.000100  loss: 0.0805 (0.0735)  time: 3.8120  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2350/3449]  eta: 1:08:45  lr: 0.000100  loss: 0.0732 (0.0734)  time: 3.8247  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2360/3449]  eta: 1:08:08  lr: 0.000100  loss: 0.0732 (0.0735)  time: 3.7269  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2370/3449]  eta: 1:07:30  lr: 0.000100  loss: 0.0932 (0.0736)  time: 3.7471  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2380/3449]  eta: 1:06:53  lr: 0.000100  loss: 0.0533 (0.0735)  time: 3.7528  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2390/3449]  eta: 1:06:15  lr: 0.000100  loss: 0.0434 (0.0734)  time: 3.7047  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:24]  [2400/3449]  eta: 1:05:37  lr: 0.000100  loss: 0.0393 (0.0732)  time: 3.7004  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2410/3449]  eta: 1:04:59  lr: 0.000100  loss: 0.0484 (0.0732)  time: 3.7160  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2420/3449]  eta: 1:04:21  lr: 0.000100  loss: 0.0574 (0.0732)  time: 3.6976  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2430/3449]  eta: 1:03:44  lr: 0.000100  loss: 0.0516 (0.0731)  time: 3.6955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2440/3449]  eta: 1:03:06  lr: 0.000100  loss: 0.0659 (0.0731)  time: 3.7607  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2450/3449]  eta: 1:02:29  lr: 0.000100  loss: 0.0737 (0.0732)  time: 3.7962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2460/3449]  eta: 1:01:52  lr: 0.000100  loss: 0.0773 (0.0732)  time: 3.7720  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2470/3449]  eta: 1:01:14  lr: 0.000100  loss: 0.0773 (0.0732)  time: 3.7500  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2480/3449]  eta: 1:00:36  lr: 0.000100  loss: 0.0647 (0.0732)  time: 3.7183  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2490/3449]  eta: 0:59:59  lr: 0.000100  loss: 0.0712 (0.0732)  time: 3.7377  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2500/3449]  eta: 0:59:21  lr: 0.000100  loss: 0.0696 (0.0732)  time: 3.7699  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2510/3449]  eta: 0:58:44  lr: 0.000100  loss: 0.0660 (0.0732)  time: 3.8023  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2520/3449]  eta: 0:58:07  lr: 0.000100  loss: 0.0906 (0.0733)  time: 3.7937  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2530/3449]  eta: 0:57:29  lr: 0.000100  loss: 0.0914 (0.0733)  time: 3.7510  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2540/3449]  eta: 0:56:51  lr: 0.000100  loss: 0.0775 (0.0733)  time: 3.7470  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2550/3449]  eta: 0:56:14  lr: 0.000100  loss: 0.0769 (0.0734)  time: 3.8118  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2560/3449]  eta: 0:55:37  lr: 0.000100  loss: 0.0700 (0.0734)  time: 3.8345  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2570/3449]  eta: 0:54:59  lr: 0.000100  loss: 0.0683 (0.0734)  time: 3.7506  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2580/3449]  eta: 0:54:21  lr: 0.000100  loss: 0.0682 (0.0734)  time: 3.7055  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2590/3449]  eta: 0:53:44  lr: 0.000100  loss: 0.0651 (0.0734)  time: 3.7304  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2600/3449]  eta: 0:53:07  lr: 0.000100  loss: 0.0594 (0.0734)  time: 3.7941  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2610/3449]  eta: 0:52:29  lr: 0.000100  loss: 0.0619 (0.0733)  time: 3.7969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2620/3449]  eta: 0:51:52  lr: 0.000100  loss: 0.0615 (0.0733)  time: 3.7533  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2630/3449]  eta: 0:51:14  lr: 0.000100  loss: 0.0586 (0.0733)  time: 3.7990  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2640/3449]  eta: 0:50:37  lr: 0.000100  loss: 0.0594 (0.0733)  time: 3.8108  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2650/3449]  eta: 0:49:59  lr: 0.000100  loss: 0.0916 (0.0734)  time: 3.7546  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2660/3449]  eta: 0:49:22  lr: 0.000100  loss: 0.0837 (0.0734)  time: 3.7415  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2670/3449]  eta: 0:48:44  lr: 0.000100  loss: 0.0636 (0.0733)  time: 3.7496  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2680/3449]  eta: 0:48:07  lr: 0.000100  loss: 0.0571 (0.0732)  time: 3.7614  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2690/3449]  eta: 0:47:29  lr: 0.000100  loss: 0.0492 (0.0732)  time: 3.7633  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2700/3449]  eta: 0:46:52  lr: 0.000100  loss: 0.0449 (0.0731)  time: 3.7732  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2710/3449]  eta: 0:46:14  lr: 0.000100  loss: 0.0496 (0.0731)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2720/3449]  eta: 0:45:37  lr: 0.000100  loss: 0.0720 (0.0731)  time: 3.7609  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2730/3449]  eta: 0:44:59  lr: 0.000100  loss: 0.0759 (0.0731)  time: 3.7325  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2740/3449]  eta: 0:44:21  lr: 0.000100  loss: 0.0735 (0.0731)  time: 3.7236  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2750/3449]  eta: 0:43:44  lr: 0.000100  loss: 0.0673 (0.0731)  time: 3.7355  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2760/3449]  eta: 0:43:06  lr: 0.000100  loss: 0.0727 (0.0731)  time: 3.7691  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2770/3449]  eta: 0:42:29  lr: 0.000100  loss: 0.0756 (0.0731)  time: 3.8059  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2780/3449]  eta: 0:41:51  lr: 0.000100  loss: 0.0625 (0.0731)  time: 3.7648  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2790/3449]  eta: 0:41:14  lr: 0.000100  loss: 0.0578 (0.0730)  time: 3.7296  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2800/3449]  eta: 0:40:36  lr: 0.000100  loss: 0.0677 (0.0730)  time: 3.7133  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2810/3449]  eta: 0:39:59  lr: 0.000100  loss: 0.0677 (0.0730)  time: 3.7436  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2820/3449]  eta: 0:39:21  lr: 0.000100  loss: 0.1019 (0.0732)  time: 3.7353  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2830/3449]  eta: 0:38:43  lr: 0.000100  loss: 0.1000 (0.0733)  time: 3.7156  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2840/3449]  eta: 0:38:06  lr: 0.000100  loss: 0.0743 (0.0732)  time: 3.7582  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2850/3449]  eta: 0:37:28  lr: 0.000100  loss: 0.0712 (0.0732)  time: 3.7495  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2860/3449]  eta: 0:36:51  lr: 0.000100  loss: 0.0799 (0.0733)  time: 3.8002  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2870/3449]  eta: 0:36:13  lr: 0.000100  loss: 0.0746 (0.0733)  time: 3.7912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2880/3449]  eta: 0:35:36  lr: 0.000100  loss: 0.0742 (0.0733)  time: 3.6988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2890/3449]  eta: 0:34:58  lr: 0.000100  loss: 0.0894 (0.0734)  time: 3.7119  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2900/3449]  eta: 0:34:20  lr: 0.000100  loss: 0.1141 (0.0736)  time: 3.7118  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2910/3449]  eta: 0:33:43  lr: 0.000100  loss: 0.1139 (0.0737)  time: 3.7491  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2920/3449]  eta: 0:33:05  lr: 0.000100  loss: 0.1103 (0.0738)  time: 3.7835  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2930/3449]  eta: 0:32:28  lr: 0.000100  loss: 0.0912 (0.0739)  time: 3.7375  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2940/3449]  eta: 0:31:50  lr: 0.000100  loss: 0.0748 (0.0738)  time: 3.7302  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2950/3449]  eta: 0:31:13  lr: 0.000100  loss: 0.0787 (0.0739)  time: 3.7417  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2960/3449]  eta: 0:30:35  lr: 0.000100  loss: 0.0629 (0.0738)  time: 3.7362  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2970/3449]  eta: 0:29:58  lr: 0.000100  loss: 0.0575 (0.0738)  time: 3.7584  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [2980/3449]  eta: 0:29:20  lr: 0.000100  loss: 0.0557 (0.0737)  time: 3.7812  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [2990/3449]  eta: 0:28:42  lr: 0.000100  loss: 0.0375 (0.0736)  time: 3.7476  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3000/3449]  eta: 0:28:05  lr: 0.000100  loss: 0.0358 (0.0735)  time: 3.7502  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [3010/3449]  eta: 0:27:27  lr: 0.000100  loss: 0.0492 (0.0735)  time: 3.7494  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3020/3449]  eta: 0:26:50  lr: 0.000100  loss: 0.0773 (0.0735)  time: 3.7200  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3030/3449]  eta: 0:26:12  lr: 0.000100  loss: 0.0746 (0.0735)  time: 3.7761  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3040/3449]  eta: 0:25:35  lr: 0.000100  loss: 0.0487 (0.0734)  time: 3.7689  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [3050/3449]  eta: 0:24:57  lr: 0.000100  loss: 0.0376 (0.0733)  time: 3.7606  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:24]  [3060/3449]  eta: 0:24:20  lr: 0.000100  loss: 0.0464 (0.0732)  time: 3.7903  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3070/3449]  eta: 0:23:42  lr: 0.000100  loss: 0.0626 (0.0731)  time: 3.7963  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3080/3449]  eta: 0:23:05  lr: 0.000100  loss: 0.0661 (0.0731)  time: 3.8097  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [3090/3449]  eta: 0:22:27  lr: 0.000100  loss: 0.0718 (0.0731)  time: 3.7607  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [3100/3449]  eta: 0:21:50  lr: 0.000100  loss: 0.0633 (0.0732)  time: 3.7298  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3110/3449]  eta: 0:21:12  lr: 0.000100  loss: 0.0967 (0.0733)  time: 3.7354  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3120/3449]  eta: 0:20:35  lr: 0.000100  loss: 0.1156 (0.0735)  time: 3.7245  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3130/3449]  eta: 0:19:57  lr: 0.000100  loss: 0.1001 (0.0735)  time: 3.7125  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3140/3449]  eta: 0:19:19  lr: 0.000100  loss: 0.1001 (0.0736)  time: 3.7180  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3150/3449]  eta: 0:18:42  lr: 0.000100  loss: 0.1258 (0.0739)  time: 3.7532  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3160/3449]  eta: 0:18:04  lr: 0.000100  loss: 0.0975 (0.0739)  time: 3.7761  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3170/3449]  eta: 0:17:27  lr: 0.000100  loss: 0.0897 (0.0739)  time: 3.7360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3180/3449]  eta: 0:16:49  lr: 0.000100  loss: 0.0666 (0.0739)  time: 3.7252  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3190/3449]  eta: 0:16:12  lr: 0.000100  loss: 0.0559 (0.0738)  time: 3.7455  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3200/3449]  eta: 0:15:34  lr: 0.000100  loss: 0.0749 (0.0738)  time: 3.7246  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3210/3449]  eta: 0:14:57  lr: 0.000100  loss: 0.0659 (0.0738)  time: 3.7166  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3220/3449]  eta: 0:14:19  lr: 0.000100  loss: 0.0545 (0.0737)  time: 3.7405  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3230/3449]  eta: 0:13:42  lr: 0.000100  loss: 0.0479 (0.0736)  time: 3.7513  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3240/3449]  eta: 0:13:04  lr: 0.000100  loss: 0.0451 (0.0735)  time: 3.7630  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3250/3449]  eta: 0:12:26  lr: 0.000100  loss: 0.0459 (0.0735)  time: 3.7888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3260/3449]  eta: 0:11:49  lr: 0.000100  loss: 0.0500 (0.0734)  time: 3.7679  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3270/3449]  eta: 0:11:11  lr: 0.000100  loss: 0.0657 (0.0735)  time: 3.7596  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3280/3449]  eta: 0:10:34  lr: 0.000100  loss: 0.0844 (0.0736)  time: 3.7754  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3290/3449]  eta: 0:09:56  lr: 0.000100  loss: 0.0782 (0.0736)  time: 3.7641  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3300/3449]  eta: 0:09:19  lr: 0.000100  loss: 0.0717 (0.0735)  time: 3.7714  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3310/3449]  eta: 0:08:41  lr: 0.000100  loss: 0.0710 (0.0735)  time: 3.7607  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3320/3449]  eta: 0:08:04  lr: 0.000100  loss: 0.0736 (0.0735)  time: 3.7360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3330/3449]  eta: 0:07:26  lr: 0.000100  loss: 0.0760 (0.0736)  time: 3.7730  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3340/3449]  eta: 0:06:49  lr: 0.000100  loss: 0.0711 (0.0736)  time: 3.7972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3350/3449]  eta: 0:06:11  lr: 0.000100  loss: 0.0759 (0.0736)  time: 3.7452  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3360/3449]  eta: 0:05:34  lr: 0.000100  loss: 0.0759 (0.0736)  time: 3.7463  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3370/3449]  eta: 0:04:56  lr: 0.000100  loss: 0.1028 (0.0738)  time: 3.8055  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3380/3449]  eta: 0:04:19  lr: 0.000100  loss: 0.0980 (0.0738)  time: 3.7973  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3390/3449]  eta: 0:03:41  lr: 0.000100  loss: 0.0654 (0.0738)  time: 3.7325  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3400/3449]  eta: 0:03:03  lr: 0.000100  loss: 0.0654 (0.0738)  time: 3.7103  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3410/3449]  eta: 0:02:26  lr: 0.000100  loss: 0.0656 (0.0737)  time: 3.7969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3420/3449]  eta: 0:01:48  lr: 0.000100  loss: 0.0712 (0.0737)  time: 3.7771  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3430/3449]  eta: 0:01:11  lr: 0.000100  loss: 0.0771 (0.0738)  time: 3.7221  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24]  [3440/3449]  eta: 0:00:33  lr: 0.000100  loss: 0.0771 (0.0738)  time: 3.7497  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:24]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0693 (0.0737)  time: 3.7183  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:24] Total time: 3:35:48 (3.7542 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0693 (0.0737)\n",
      "Valid: [epoch:24]  [ 0/14]  eta: 0:04:20  loss: 0.0324 (0.0324)  time: 18.6005  data: 0.4054  max mem: 34968\n",
      "Valid: [epoch:24]  [13/14]  eta: 0:00:18  loss: 0.0332 (0.0334)  time: 18.2465  data: 0.0291  max mem: 34968\n",
      "Valid: [epoch:24] Total time: 0:04:15 (18.2601 s / it)\n",
      "Averaged stats: loss: 0.0332 (0.0334)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_24_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.033%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:25]  [   0/3449]  eta: 4:57:28  lr: 0.000100  loss: 0.0503 (0.0503)  time: 5.1750  data: 1.3535  max mem: 34968\n",
      "Train: [epoch:25]  [  10/3449]  eta: 3:39:02  lr: 0.000100  loss: 0.0424 (0.0450)  time: 3.8216  data: 0.1232  max mem: 34968\n",
      "Train: [epoch:25]  [  20/3449]  eta: 3:35:07  lr: 0.000100  loss: 0.0421 (0.0473)  time: 3.6937  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [  30/3449]  eta: 3:33:00  lr: 0.000100  loss: 0.0682 (0.0665)  time: 3.6923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [  40/3449]  eta: 3:32:53  lr: 0.000100  loss: 0.0768 (0.0681)  time: 3.7291  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [  50/3449]  eta: 3:32:49  lr: 0.000100  loss: 0.0643 (0.0646)  time: 3.7858  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [  60/3449]  eta: 3:31:35  lr: 0.000100  loss: 0.0446 (0.0616)  time: 3.7440  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [  70/3449]  eta: 3:30:42  lr: 0.000100  loss: 0.0437 (0.0606)  time: 3.7018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [  80/3449]  eta: 3:29:48  lr: 0.000100  loss: 0.0518 (0.0603)  time: 3.7075  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [  90/3449]  eta: 3:29:17  lr: 0.000100  loss: 0.0428 (0.0580)  time: 3.7286  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 100/3449]  eta: 3:28:39  lr: 0.000100  loss: 0.0428 (0.0584)  time: 3.7452  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 110/3449]  eta: 3:28:17  lr: 0.000100  loss: 0.0689 (0.0612)  time: 3.7631  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 120/3449]  eta: 3:27:36  lr: 0.000100  loss: 0.0833 (0.0631)  time: 3.7593  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 130/3449]  eta: 3:26:48  lr: 0.000100  loss: 0.0833 (0.0647)  time: 3.7145  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 140/3449]  eta: 3:26:12  lr: 0.000100  loss: 0.0730 (0.0648)  time: 3.7227  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 150/3449]  eta: 3:25:37  lr: 0.000100  loss: 0.0565 (0.0640)  time: 3.7464  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 160/3449]  eta: 3:25:25  lr: 0.000100  loss: 0.0629 (0.0650)  time: 3.8062  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 170/3449]  eta: 3:24:50  lr: 0.000100  loss: 0.0827 (0.0660)  time: 3.8144  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 180/3449]  eta: 3:24:13  lr: 0.000100  loss: 0.0813 (0.0666)  time: 3.7554  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 190/3449]  eta: 3:23:29  lr: 0.000100  loss: 0.0772 (0.0671)  time: 3.7289  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 200/3449]  eta: 3:22:51  lr: 0.000100  loss: 0.0783 (0.0675)  time: 3.7293  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:25]  [ 210/3449]  eta: 3:22:32  lr: 0.000100  loss: 0.0629 (0.0669)  time: 3.8053  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 220/3449]  eta: 3:22:04  lr: 0.000100  loss: 0.0619 (0.0669)  time: 3.8391  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 230/3449]  eta: 3:21:22  lr: 0.000100  loss: 0.0828 (0.0682)  time: 3.7681  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 240/3449]  eta: 3:20:35  lr: 0.000100  loss: 0.1146 (0.0705)  time: 3.7043  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 250/3449]  eta: 3:19:57  lr: 0.000100  loss: 0.1048 (0.0714)  time: 3.7154  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 260/3449]  eta: 3:19:20  lr: 0.000100  loss: 0.1006 (0.0724)  time: 3.7498  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 270/3449]  eta: 3:18:39  lr: 0.000100  loss: 0.0856 (0.0726)  time: 3.7385  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 280/3449]  eta: 3:18:11  lr: 0.000100  loss: 0.0810 (0.0730)  time: 3.7788  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 290/3449]  eta: 3:17:29  lr: 0.000100  loss: 0.0892 (0.0734)  time: 3.7737  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 300/3449]  eta: 3:16:55  lr: 0.000100  loss: 0.0777 (0.0733)  time: 3.7456  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 310/3449]  eta: 3:16:18  lr: 0.000100  loss: 0.0659 (0.0730)  time: 3.7691  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 320/3449]  eta: 3:15:32  lr: 0.000100  loss: 0.0454 (0.0722)  time: 3.7141  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 330/3449]  eta: 3:14:50  lr: 0.000100  loss: 0.0425 (0.0716)  time: 3.6841  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 340/3449]  eta: 3:14:15  lr: 0.000100  loss: 0.0629 (0.0725)  time: 3.7354  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 350/3449]  eta: 3:13:36  lr: 0.000100  loss: 0.1141 (0.0738)  time: 3.7513  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 360/3449]  eta: 3:13:06  lr: 0.000100  loss: 0.1112 (0.0744)  time: 3.7836  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 370/3449]  eta: 3:12:28  lr: 0.000100  loss: 0.0624 (0.0740)  time: 3.7908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 380/3449]  eta: 3:11:51  lr: 0.000100  loss: 0.0430 (0.0730)  time: 3.7520  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 390/3449]  eta: 3:11:14  lr: 0.000100  loss: 0.0432 (0.0729)  time: 3.7582  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 400/3449]  eta: 3:10:36  lr: 0.000100  loss: 0.0858 (0.0743)  time: 3.7494  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 410/3449]  eta: 3:09:55  lr: 0.000100  loss: 0.1322 (0.0758)  time: 3.7225  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 420/3449]  eta: 3:09:17  lr: 0.000100  loss: 0.1060 (0.0763)  time: 3.7264  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 430/3449]  eta: 3:08:48  lr: 0.000100  loss: 0.0717 (0.0757)  time: 3.8069  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 440/3449]  eta: 3:08:06  lr: 0.000100  loss: 0.0558 (0.0756)  time: 3.7758  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [ 450/3449]  eta: 3:07:28  lr: 0.000100  loss: 0.0854 (0.0758)  time: 3.7160  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 460/3449]  eta: 3:06:50  lr: 0.000100  loss: 0.0636 (0.0754)  time: 3.7422  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 470/3449]  eta: 3:06:20  lr: 0.000100  loss: 0.0535 (0.0748)  time: 3.8089  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 480/3449]  eta: 3:05:41  lr: 0.000100  loss: 0.0489 (0.0744)  time: 3.7991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 490/3449]  eta: 3:05:01  lr: 0.000100  loss: 0.0522 (0.0740)  time: 3.7218  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 500/3449]  eta: 3:04:25  lr: 0.000100  loss: 0.0556 (0.0737)  time: 3.7442  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 510/3449]  eta: 3:03:53  lr: 0.000100  loss: 0.0749 (0.0742)  time: 3.8084  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 520/3449]  eta: 3:03:13  lr: 0.000100  loss: 0.0934 (0.0744)  time: 3.7854  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 530/3449]  eta: 3:02:37  lr: 0.000100  loss: 0.0745 (0.0744)  time: 3.7464  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 540/3449]  eta: 3:01:59  lr: 0.000100  loss: 0.0615 (0.0740)  time: 3.7590  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 550/3449]  eta: 3:01:20  lr: 0.000100  loss: 0.0584 (0.0737)  time: 3.7371  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 560/3449]  eta: 3:00:42  lr: 0.000100  loss: 0.0552 (0.0733)  time: 3.7340  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 570/3449]  eta: 3:00:09  lr: 0.000100  loss: 0.0476 (0.0729)  time: 3.7962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 580/3449]  eta: 2:59:28  lr: 0.000100  loss: 0.0442 (0.0724)  time: 3.7643  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 590/3449]  eta: 2:58:50  lr: 0.000100  loss: 0.0413 (0.0719)  time: 3.7106  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 600/3449]  eta: 2:58:11  lr: 0.000100  loss: 0.0500 (0.0719)  time: 3.7366  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 610/3449]  eta: 2:57:33  lr: 0.000100  loss: 0.0819 (0.0721)  time: 3.7344  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 620/3449]  eta: 2:56:55  lr: 0.000100  loss: 0.0989 (0.0728)  time: 3.7379  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 630/3449]  eta: 2:56:16  lr: 0.000100  loss: 0.0741 (0.0728)  time: 3.7244  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 640/3449]  eta: 2:55:36  lr: 0.000100  loss: 0.0661 (0.0727)  time: 3.7103  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 650/3449]  eta: 2:54:58  lr: 0.000100  loss: 0.0684 (0.0727)  time: 3.7192  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 660/3449]  eta: 2:54:21  lr: 0.000100  loss: 0.0861 (0.0729)  time: 3.7518  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 670/3449]  eta: 2:53:48  lr: 0.000100  loss: 0.0871 (0.0731)  time: 3.8115  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 680/3449]  eta: 2:53:11  lr: 0.000100  loss: 0.0817 (0.0731)  time: 3.8109  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 690/3449]  eta: 2:52:34  lr: 0.000100  loss: 0.0817 (0.0734)  time: 3.7721  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 700/3449]  eta: 2:51:56  lr: 0.000100  loss: 0.0599 (0.0730)  time: 3.7580  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 710/3449]  eta: 2:51:18  lr: 0.000100  loss: 0.0435 (0.0726)  time: 3.7353  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [ 720/3449]  eta: 2:50:38  lr: 0.000100  loss: 0.0467 (0.0724)  time: 3.7128  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 730/3449]  eta: 2:50:00  lr: 0.000100  loss: 0.0663 (0.0725)  time: 3.7093  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 740/3449]  eta: 2:49:21  lr: 0.000100  loss: 0.0814 (0.0726)  time: 3.7199  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 750/3449]  eta: 2:48:42  lr: 0.000100  loss: 0.0797 (0.0726)  time: 3.7225  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 760/3449]  eta: 2:48:07  lr: 0.000100  loss: 0.0604 (0.0724)  time: 3.7670  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [ 770/3449]  eta: 2:47:30  lr: 0.000100  loss: 0.0579 (0.0722)  time: 3.7880  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [ 780/3449]  eta: 2:46:53  lr: 0.000100  loss: 0.0669 (0.0723)  time: 3.7657  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [ 790/3449]  eta: 2:46:15  lr: 0.000100  loss: 0.0831 (0.0724)  time: 3.7586  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [ 800/3449]  eta: 2:45:36  lr: 0.000100  loss: 0.0778 (0.0725)  time: 3.7241  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 810/3449]  eta: 2:45:03  lr: 0.000100  loss: 0.0656 (0.0725)  time: 3.7845  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 820/3449]  eta: 2:44:25  lr: 0.000100  loss: 0.0625 (0.0723)  time: 3.8118  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 830/3449]  eta: 2:43:50  lr: 0.000100  loss: 0.0609 (0.0723)  time: 3.7849  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 840/3449]  eta: 2:43:12  lr: 0.000100  loss: 0.0869 (0.0726)  time: 3.7863  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 850/3449]  eta: 2:42:33  lr: 0.000100  loss: 0.0664 (0.0724)  time: 3.7348  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 860/3449]  eta: 2:41:56  lr: 0.000100  loss: 0.0510 (0.0724)  time: 3.7360  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:25]  [ 870/3449]  eta: 2:41:17  lr: 0.000100  loss: 0.1053 (0.0730)  time: 3.7383  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 880/3449]  eta: 2:40:38  lr: 0.000100  loss: 0.1076 (0.0732)  time: 3.7141  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 890/3449]  eta: 2:40:04  lr: 0.000100  loss: 0.0667 (0.0732)  time: 3.7872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 900/3449]  eta: 2:39:27  lr: 0.000100  loss: 0.0669 (0.0734)  time: 3.8103  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 910/3449]  eta: 2:38:50  lr: 0.000100  loss: 0.0714 (0.0734)  time: 3.7666  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 920/3449]  eta: 2:38:13  lr: 0.000100  loss: 0.0698 (0.0734)  time: 3.7744  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 930/3449]  eta: 2:37:34  lr: 0.000100  loss: 0.0696 (0.0734)  time: 3.7273  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 940/3449]  eta: 2:36:56  lr: 0.000100  loss: 0.0695 (0.0734)  time: 3.7159  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 950/3449]  eta: 2:36:17  lr: 0.000100  loss: 0.0647 (0.0734)  time: 3.7293  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 960/3449]  eta: 2:35:41  lr: 0.000100  loss: 0.0572 (0.0732)  time: 3.7647  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 970/3449]  eta: 2:35:04  lr: 0.000100  loss: 0.0624 (0.0732)  time: 3.7842  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 980/3449]  eta: 2:34:27  lr: 0.000100  loss: 0.0787 (0.0733)  time: 3.7642  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [ 990/3449]  eta: 2:33:49  lr: 0.000100  loss: 0.0573 (0.0730)  time: 3.7598  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1000/3449]  eta: 2:33:13  lr: 0.000100  loss: 0.0427 (0.0728)  time: 3.7877  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1010/3449]  eta: 2:32:38  lr: 0.000100  loss: 0.0431 (0.0725)  time: 3.8274  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1020/3449]  eta: 2:32:00  lr: 0.000100  loss: 0.0514 (0.0723)  time: 3.7876  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1030/3449]  eta: 2:31:22  lr: 0.000100  loss: 0.0662 (0.0725)  time: 3.7409  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1040/3449]  eta: 2:30:43  lr: 0.000100  loss: 0.0931 (0.0728)  time: 3.7219  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [1050/3449]  eta: 2:30:05  lr: 0.000100  loss: 0.1172 (0.0733)  time: 3.7205  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1060/3449]  eta: 2:29:28  lr: 0.000100  loss: 0.0904 (0.0733)  time: 3.7439  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1070/3449]  eta: 2:28:51  lr: 0.000100  loss: 0.0715 (0.0734)  time: 3.7859  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1080/3449]  eta: 2:28:14  lr: 0.000100  loss: 0.0952 (0.0738)  time: 3.7870  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1090/3449]  eta: 2:27:36  lr: 0.000100  loss: 0.1143 (0.0742)  time: 3.7517  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [1100/3449]  eta: 2:26:58  lr: 0.000100  loss: 0.0811 (0.0742)  time: 3.7383  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1110/3449]  eta: 2:26:20  lr: 0.000100  loss: 0.0570 (0.0740)  time: 3.7248  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1120/3449]  eta: 2:25:43  lr: 0.000100  loss: 0.0460 (0.0737)  time: 3.7499  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1130/3449]  eta: 2:25:04  lr: 0.000100  loss: 0.0500 (0.0735)  time: 3.7252  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1140/3449]  eta: 2:24:25  lr: 0.000100  loss: 0.0500 (0.0733)  time: 3.6931  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1150/3449]  eta: 2:23:47  lr: 0.000100  loss: 0.0478 (0.0732)  time: 3.7258  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1160/3449]  eta: 2:23:11  lr: 0.000100  loss: 0.0662 (0.0732)  time: 3.7567  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1170/3449]  eta: 2:22:32  lr: 0.000100  loss: 0.0790 (0.0733)  time: 3.7467  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1180/3449]  eta: 2:21:55  lr: 0.000100  loss: 0.0879 (0.0734)  time: 3.7342  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [1190/3449]  eta: 2:21:17  lr: 0.000100  loss: 0.0713 (0.0734)  time: 3.7426  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1200/3449]  eta: 2:20:39  lr: 0.000100  loss: 0.0800 (0.0736)  time: 3.7290  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1210/3449]  eta: 2:20:01  lr: 0.000100  loss: 0.0778 (0.0736)  time: 3.7207  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1220/3449]  eta: 2:19:25  lr: 0.000100  loss: 0.0666 (0.0735)  time: 3.7752  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1230/3449]  eta: 2:18:46  lr: 0.000100  loss: 0.0655 (0.0735)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1240/3449]  eta: 2:18:10  lr: 0.000100  loss: 0.0567 (0.0734)  time: 3.7553  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1250/3449]  eta: 2:17:31  lr: 0.000100  loss: 0.0561 (0.0732)  time: 3.7448  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1260/3449]  eta: 2:16:53  lr: 0.000100  loss: 0.0590 (0.0731)  time: 3.7143  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1270/3449]  eta: 2:16:15  lr: 0.000100  loss: 0.0571 (0.0730)  time: 3.7176  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1280/3449]  eta: 2:15:36  lr: 0.000100  loss: 0.0437 (0.0727)  time: 3.6940  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1290/3449]  eta: 2:14:58  lr: 0.000100  loss: 0.0353 (0.0724)  time: 3.7127  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1300/3449]  eta: 2:14:22  lr: 0.000100  loss: 0.0367 (0.0722)  time: 3.7666  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1310/3449]  eta: 2:13:45  lr: 0.000100  loss: 0.0414 (0.0720)  time: 3.8098  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1320/3449]  eta: 2:13:07  lr: 0.000100  loss: 0.0413 (0.0717)  time: 3.7525  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1330/3449]  eta: 2:12:29  lr: 0.000100  loss: 0.0417 (0.0716)  time: 3.7136  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1340/3449]  eta: 2:11:52  lr: 0.000100  loss: 0.0435 (0.0713)  time: 3.7605  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1350/3449]  eta: 2:11:15  lr: 0.000100  loss: 0.0364 (0.0711)  time: 3.7739  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1360/3449]  eta: 2:10:37  lr: 0.000100  loss: 0.0338 (0.0709)  time: 3.7432  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1370/3449]  eta: 2:10:00  lr: 0.000100  loss: 0.0354 (0.0708)  time: 3.7573  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1380/3449]  eta: 2:09:22  lr: 0.000100  loss: 0.0693 (0.0710)  time: 3.7589  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1390/3449]  eta: 2:08:44  lr: 0.000100  loss: 0.0878 (0.0710)  time: 3.7232  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1400/3449]  eta: 2:08:06  lr: 0.000100  loss: 0.0872 (0.0712)  time: 3.7218  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1410/3449]  eta: 2:07:28  lr: 0.000100  loss: 0.0849 (0.0713)  time: 3.7279  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1420/3449]  eta: 2:06:50  lr: 0.000100  loss: 0.0742 (0.0713)  time: 3.7129  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [1430/3449]  eta: 2:06:13  lr: 0.000100  loss: 0.0760 (0.0713)  time: 3.7455  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [1440/3449]  eta: 2:05:35  lr: 0.000100  loss: 0.0783 (0.0714)  time: 3.7644  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1450/3449]  eta: 2:04:58  lr: 0.000100  loss: 0.0777 (0.0714)  time: 3.7452  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1460/3449]  eta: 2:04:21  lr: 0.000100  loss: 0.0777 (0.0715)  time: 3.7656  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1470/3449]  eta: 2:03:43  lr: 0.000100  loss: 0.0630 (0.0714)  time: 3.7722  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1480/3449]  eta: 2:03:06  lr: 0.000100  loss: 0.0653 (0.0714)  time: 3.7514  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1490/3449]  eta: 2:02:28  lr: 0.000100  loss: 0.0828 (0.0715)  time: 3.7339  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1500/3449]  eta: 2:01:51  lr: 0.000100  loss: 0.0735 (0.0715)  time: 3.7609  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1510/3449]  eta: 2:01:13  lr: 0.000100  loss: 0.0701 (0.0715)  time: 3.7484  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1520/3449]  eta: 2:00:36  lr: 0.000100  loss: 0.0701 (0.0715)  time: 3.7690  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:25]  [1530/3449]  eta: 1:59:59  lr: 0.000100  loss: 0.0730 (0.0715)  time: 3.8061  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1540/3449]  eta: 1:59:22  lr: 0.000100  loss: 0.0625 (0.0715)  time: 3.7680  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1550/3449]  eta: 1:58:44  lr: 0.000100  loss: 0.0625 (0.0714)  time: 3.7621  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [1560/3449]  eta: 1:58:06  lr: 0.000100  loss: 0.0613 (0.0713)  time: 3.7502  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1570/3449]  eta: 1:57:30  lr: 0.000100  loss: 0.0546 (0.0712)  time: 3.7941  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1580/3449]  eta: 1:56:53  lr: 0.000100  loss: 0.0421 (0.0710)  time: 3.8375  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1590/3449]  eta: 1:56:15  lr: 0.000100  loss: 0.0363 (0.0709)  time: 3.7624  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1600/3449]  eta: 1:55:39  lr: 0.000100  loss: 0.0593 (0.0708)  time: 3.7919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1610/3449]  eta: 1:55:01  lr: 0.000100  loss: 0.0745 (0.0709)  time: 3.7752  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1620/3449]  eta: 1:54:23  lr: 0.000100  loss: 0.0723 (0.0709)  time: 3.6949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1630/3449]  eta: 1:53:45  lr: 0.000100  loss: 0.0562 (0.0708)  time: 3.7351  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1640/3449]  eta: 1:53:07  lr: 0.000100  loss: 0.0634 (0.0710)  time: 3.7319  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1650/3449]  eta: 1:52:29  lr: 0.000100  loss: 0.1330 (0.0715)  time: 3.6963  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1660/3449]  eta: 1:51:51  lr: 0.000100  loss: 0.1843 (0.0722)  time: 3.7005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1670/3449]  eta: 1:51:13  lr: 0.000100  loss: 0.1572 (0.0724)  time: 3.7205  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1680/3449]  eta: 1:50:36  lr: 0.000100  loss: 0.0686 (0.0723)  time: 3.7507  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1690/3449]  eta: 1:49:59  lr: 0.000100  loss: 0.0575 (0.0722)  time: 3.7568  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1700/3449]  eta: 1:49:21  lr: 0.000100  loss: 0.0395 (0.0720)  time: 3.7433  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1710/3449]  eta: 1:48:43  lr: 0.000100  loss: 0.0497 (0.0720)  time: 3.7427  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1720/3449]  eta: 1:48:06  lr: 0.000100  loss: 0.0833 (0.0720)  time: 3.7325  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1730/3449]  eta: 1:47:30  lr: 0.000100  loss: 0.0836 (0.0721)  time: 3.8296  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1740/3449]  eta: 1:46:53  lr: 0.000100  loss: 0.0734 (0.0721)  time: 3.8638  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1750/3449]  eta: 1:46:15  lr: 0.000100  loss: 0.0734 (0.0721)  time: 3.7821  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1760/3449]  eta: 1:45:37  lr: 0.000100  loss: 0.0734 (0.0721)  time: 3.7347  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1770/3449]  eta: 1:45:00  lr: 0.000100  loss: 0.0669 (0.0722)  time: 3.7303  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1780/3449]  eta: 1:44:22  lr: 0.000100  loss: 0.1286 (0.0727)  time: 3.7463  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1790/3449]  eta: 1:43:46  lr: 0.000100  loss: 0.1600 (0.0731)  time: 3.7898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1800/3449]  eta: 1:43:08  lr: 0.000100  loss: 0.1193 (0.0733)  time: 3.8042  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1810/3449]  eta: 1:42:30  lr: 0.000100  loss: 0.0832 (0.0732)  time: 3.7506  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1820/3449]  eta: 1:41:53  lr: 0.000100  loss: 0.0670 (0.0733)  time: 3.7768  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1830/3449]  eta: 1:41:16  lr: 0.000100  loss: 0.0698 (0.0732)  time: 3.7813  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1840/3449]  eta: 1:40:38  lr: 0.000100  loss: 0.0696 (0.0733)  time: 3.7343  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1850/3449]  eta: 1:40:01  lr: 0.000100  loss: 0.0626 (0.0732)  time: 3.7424  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1860/3449]  eta: 1:39:24  lr: 0.000100  loss: 0.0614 (0.0732)  time: 3.8137  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [1870/3449]  eta: 1:38:47  lr: 0.000100  loss: 0.0879 (0.0733)  time: 3.8215  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1880/3449]  eta: 1:38:09  lr: 0.000100  loss: 0.0860 (0.0733)  time: 3.7512  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1890/3449]  eta: 1:37:32  lr: 0.000100  loss: 0.0792 (0.0733)  time: 3.7584  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1900/3449]  eta: 1:36:54  lr: 0.000100  loss: 0.0687 (0.0732)  time: 3.7583  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1910/3449]  eta: 1:36:16  lr: 0.000100  loss: 0.0608 (0.0732)  time: 3.7017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1920/3449]  eta: 1:35:39  lr: 0.000100  loss: 0.0680 (0.0733)  time: 3.7493  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1930/3449]  eta: 1:35:01  lr: 0.000100  loss: 0.0881 (0.0735)  time: 3.7805  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1940/3449]  eta: 1:34:24  lr: 0.000100  loss: 0.1193 (0.0738)  time: 3.7662  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1950/3449]  eta: 1:33:46  lr: 0.000100  loss: 0.1303 (0.0740)  time: 3.7519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1960/3449]  eta: 1:33:09  lr: 0.000100  loss: 0.1066 (0.0742)  time: 3.7359  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1970/3449]  eta: 1:32:31  lr: 0.000100  loss: 0.0654 (0.0740)  time: 3.7369  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1980/3449]  eta: 1:31:53  lr: 0.000100  loss: 0.0454 (0.0739)  time: 3.7045  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [1990/3449]  eta: 1:31:16  lr: 0.000100  loss: 0.0620 (0.0739)  time: 3.7422  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [2000/3449]  eta: 1:30:39  lr: 0.000100  loss: 0.0829 (0.0740)  time: 3.8256  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2010/3449]  eta: 1:30:01  lr: 0.000100  loss: 0.0822 (0.0740)  time: 3.7854  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2020/3449]  eta: 1:29:23  lr: 0.000100  loss: 0.0844 (0.0741)  time: 3.7265  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2030/3449]  eta: 1:28:45  lr: 0.000100  loss: 0.0844 (0.0741)  time: 3.6949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2040/3449]  eta: 1:28:07  lr: 0.000100  loss: 0.0696 (0.0740)  time: 3.6821  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2050/3449]  eta: 1:27:30  lr: 0.000100  loss: 0.0776 (0.0742)  time: 3.7509  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2060/3449]  eta: 1:26:52  lr: 0.000100  loss: 0.0945 (0.0743)  time: 3.7639  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2070/3449]  eta: 1:26:15  lr: 0.000100  loss: 0.0835 (0.0743)  time: 3.7237  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2080/3449]  eta: 1:25:37  lr: 0.000100  loss: 0.0828 (0.0743)  time: 3.7189  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2090/3449]  eta: 1:24:59  lr: 0.000100  loss: 0.0681 (0.0743)  time: 3.7502  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2100/3449]  eta: 1:24:22  lr: 0.000100  loss: 0.0670 (0.0742)  time: 3.7819  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2110/3449]  eta: 1:23:44  lr: 0.000100  loss: 0.0592 (0.0741)  time: 3.7554  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2120/3449]  eta: 1:23:07  lr: 0.000100  loss: 0.0515 (0.0740)  time: 3.7089  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2130/3449]  eta: 1:22:29  lr: 0.000100  loss: 0.0552 (0.0739)  time: 3.7142  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2140/3449]  eta: 1:21:51  lr: 0.000100  loss: 0.0552 (0.0739)  time: 3.7401  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2150/3449]  eta: 1:21:14  lr: 0.000100  loss: 0.0563 (0.0738)  time: 3.7622  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2160/3449]  eta: 1:20:36  lr: 0.000100  loss: 0.0384 (0.0736)  time: 3.7605  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [2170/3449]  eta: 1:19:59  lr: 0.000100  loss: 0.0355 (0.0734)  time: 3.7319  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [2180/3449]  eta: 1:19:21  lr: 0.000100  loss: 0.0337 (0.0733)  time: 3.7590  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:25]  [2190/3449]  eta: 1:18:44  lr: 0.000100  loss: 0.0337 (0.0731)  time: 3.7788  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2200/3449]  eta: 1:18:07  lr: 0.000100  loss: 0.0398 (0.0730)  time: 3.7732  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2210/3449]  eta: 1:17:30  lr: 0.000100  loss: 0.0357 (0.0728)  time: 3.8193  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2220/3449]  eta: 1:16:51  lr: 0.000100  loss: 0.0357 (0.0726)  time: 3.7364  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2230/3449]  eta: 1:16:14  lr: 0.000100  loss: 0.0364 (0.0725)  time: 3.7462  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2240/3449]  eta: 1:15:37  lr: 0.000100  loss: 0.0364 (0.0723)  time: 3.7976  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2250/3449]  eta: 1:14:59  lr: 0.000100  loss: 0.0378 (0.0722)  time: 3.7362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2260/3449]  eta: 1:14:22  lr: 0.000100  loss: 0.0602 (0.0722)  time: 3.7985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2270/3449]  eta: 1:13:45  lr: 0.000100  loss: 0.0757 (0.0723)  time: 3.8338  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2280/3449]  eta: 1:13:07  lr: 0.000100  loss: 0.0678 (0.0722)  time: 3.7628  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2290/3449]  eta: 1:12:30  lr: 0.000100  loss: 0.0736 (0.0723)  time: 3.7643  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2300/3449]  eta: 1:11:52  lr: 0.000100  loss: 0.0824 (0.0723)  time: 3.7472  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2310/3449]  eta: 1:11:15  lr: 0.000100  loss: 0.0818 (0.0724)  time: 3.7158  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2320/3449]  eta: 1:10:37  lr: 0.000100  loss: 0.0687 (0.0723)  time: 3.7519  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2330/3449]  eta: 1:10:00  lr: 0.000100  loss: 0.0545 (0.0722)  time: 3.7703  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2340/3449]  eta: 1:09:23  lr: 0.000100  loss: 0.0417 (0.0721)  time: 3.8098  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2350/3449]  eta: 1:08:45  lr: 0.000100  loss: 0.0584 (0.0721)  time: 3.7884  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2360/3449]  eta: 1:08:07  lr: 0.000100  loss: 0.0645 (0.0721)  time: 3.7341  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2370/3449]  eta: 1:07:30  lr: 0.000100  loss: 0.0997 (0.0723)  time: 3.7610  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2380/3449]  eta: 1:06:52  lr: 0.000100  loss: 0.0983 (0.0724)  time: 3.7461  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2390/3449]  eta: 1:06:14  lr: 0.000100  loss: 0.0820 (0.0724)  time: 3.7109  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2400/3449]  eta: 1:05:37  lr: 0.000100  loss: 0.0641 (0.0723)  time: 3.7748  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2410/3449]  eta: 1:05:00  lr: 0.000100  loss: 0.0495 (0.0722)  time: 3.7962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2420/3449]  eta: 1:04:22  lr: 0.000100  loss: 0.0681 (0.0722)  time: 3.7534  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2430/3449]  eta: 1:03:45  lr: 0.000100  loss: 0.0766 (0.0722)  time: 3.7350  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2440/3449]  eta: 1:03:07  lr: 0.000100  loss: 0.0669 (0.0723)  time: 3.7214  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2450/3449]  eta: 1:02:29  lr: 0.000100  loss: 0.1181 (0.0726)  time: 3.7289  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2460/3449]  eta: 1:01:52  lr: 0.000100  loss: 0.1758 (0.0732)  time: 3.7431  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2470/3449]  eta: 1:01:14  lr: 0.000100  loss: 0.1716 (0.0735)  time: 3.7170  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2480/3449]  eta: 1:00:36  lr: 0.000100  loss: 0.1003 (0.0736)  time: 3.7013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2490/3449]  eta: 0:59:59  lr: 0.000100  loss: 0.0714 (0.0735)  time: 3.7911  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2500/3449]  eta: 0:59:21  lr: 0.000100  loss: 0.0477 (0.0735)  time: 3.7864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2510/3449]  eta: 0:58:44  lr: 0.000100  loss: 0.0899 (0.0738)  time: 3.7225  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2520/3449]  eta: 0:58:06  lr: 0.000100  loss: 0.1074 (0.0739)  time: 3.7245  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2530/3449]  eta: 0:57:28  lr: 0.000100  loss: 0.0733 (0.0739)  time: 3.7021  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2540/3449]  eta: 0:56:51  lr: 0.000100  loss: 0.0730 (0.0740)  time: 3.7064  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2550/3449]  eta: 0:56:13  lr: 0.000100  loss: 0.0795 (0.0740)  time: 3.6959  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2560/3449]  eta: 0:55:36  lr: 0.000100  loss: 0.0703 (0.0740)  time: 3.7497  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2570/3449]  eta: 0:54:58  lr: 0.000100  loss: 0.0632 (0.0739)  time: 3.7094  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2580/3449]  eta: 0:54:20  lr: 0.000100  loss: 0.0674 (0.0740)  time: 3.7413  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2590/3449]  eta: 0:53:43  lr: 0.000100  loss: 0.0590 (0.0739)  time: 3.8170  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2600/3449]  eta: 0:53:05  lr: 0.000100  loss: 0.0423 (0.0738)  time: 3.7589  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2610/3449]  eta: 0:52:28  lr: 0.000100  loss: 0.0440 (0.0738)  time: 3.8285  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2620/3449]  eta: 0:51:51  lr: 0.000100  loss: 0.0774 (0.0738)  time: 3.8272  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2630/3449]  eta: 0:51:14  lr: 0.000100  loss: 0.0593 (0.0737)  time: 3.8148  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2640/3449]  eta: 0:50:36  lr: 0.000100  loss: 0.0438 (0.0736)  time: 3.7906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [2650/3449]  eta: 0:49:58  lr: 0.000100  loss: 0.0583 (0.0736)  time: 3.6977  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [2660/3449]  eta: 0:49:21  lr: 0.000100  loss: 0.0709 (0.0737)  time: 3.7202  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2670/3449]  eta: 0:48:44  lr: 0.000100  loss: 0.0770 (0.0737)  time: 3.8168  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2680/3449]  eta: 0:48:06  lr: 0.000100  loss: 0.0725 (0.0737)  time: 3.8119  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2690/3449]  eta: 0:47:29  lr: 0.000100  loss: 0.0756 (0.0737)  time: 3.7621  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2700/3449]  eta: 0:46:51  lr: 0.000100  loss: 0.0869 (0.0740)  time: 3.7485  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2710/3449]  eta: 0:46:13  lr: 0.000100  loss: 0.1422 (0.0742)  time: 3.7325  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2720/3449]  eta: 0:45:36  lr: 0.000100  loss: 0.0791 (0.0742)  time: 3.7590  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2730/3449]  eta: 0:44:58  lr: 0.000100  loss: 0.0598 (0.0741)  time: 3.7656  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2740/3449]  eta: 0:44:21  lr: 0.000100  loss: 0.0598 (0.0741)  time: 3.7494  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2750/3449]  eta: 0:43:43  lr: 0.000100  loss: 0.0656 (0.0741)  time: 3.7434  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2760/3449]  eta: 0:43:06  lr: 0.000100  loss: 0.0776 (0.0741)  time: 3.7456  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2770/3449]  eta: 0:42:28  lr: 0.000100  loss: 0.0776 (0.0742)  time: 3.7404  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2780/3449]  eta: 0:41:50  lr: 0.000100  loss: 0.0753 (0.0742)  time: 3.7158  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2790/3449]  eta: 0:41:13  lr: 0.000100  loss: 0.0737 (0.0742)  time: 3.7227  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2800/3449]  eta: 0:40:35  lr: 0.000100  loss: 0.0725 (0.0742)  time: 3.7264  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2810/3449]  eta: 0:39:58  lr: 0.000100  loss: 0.0854 (0.0742)  time: 3.7913  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2820/3449]  eta: 0:39:20  lr: 0.000100  loss: 0.0879 (0.0742)  time: 3.8102  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2830/3449]  eta: 0:38:43  lr: 0.000100  loss: 0.0664 (0.0742)  time: 3.7473  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2840/3449]  eta: 0:38:05  lr: 0.000100  loss: 0.0655 (0.0742)  time: 3.7180  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:25]  [2850/3449]  eta: 0:37:28  lr: 0.000100  loss: 0.0669 (0.0741)  time: 3.7024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2860/3449]  eta: 0:36:51  lr: 0.000100  loss: 0.0699 (0.0741)  time: 3.8398  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [2870/3449]  eta: 0:36:13  lr: 0.000100  loss: 0.0767 (0.0741)  time: 3.8538  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2880/3449]  eta: 0:35:35  lr: 0.000100  loss: 0.0664 (0.0741)  time: 3.7354  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2890/3449]  eta: 0:34:58  lr: 0.000100  loss: 0.0518 (0.0740)  time: 3.7353  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2900/3449]  eta: 0:34:20  lr: 0.000100  loss: 0.0527 (0.0739)  time: 3.7490  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2910/3449]  eta: 0:33:43  lr: 0.000100  loss: 0.0535 (0.0738)  time: 3.7440  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2920/3449]  eta: 0:33:05  lr: 0.000100  loss: 0.0565 (0.0739)  time: 3.7564  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2930/3449]  eta: 0:32:28  lr: 0.000100  loss: 0.0761 (0.0738)  time: 3.7404  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2940/3449]  eta: 0:31:50  lr: 0.000100  loss: 0.0739 (0.0739)  time: 3.7758  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2950/3449]  eta: 0:31:13  lr: 0.000100  loss: 0.0743 (0.0739)  time: 3.8172  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2960/3449]  eta: 0:30:35  lr: 0.000100  loss: 0.0873 (0.0740)  time: 3.7481  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2970/3449]  eta: 0:29:58  lr: 0.000100  loss: 0.0861 (0.0740)  time: 3.7298  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2980/3449]  eta: 0:29:20  lr: 0.000100  loss: 0.0704 (0.0740)  time: 3.7318  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [2990/3449]  eta: 0:28:42  lr: 0.000100  loss: 0.0653 (0.0740)  time: 3.7302  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3000/3449]  eta: 0:28:05  lr: 0.000100  loss: 0.0514 (0.0739)  time: 3.7369  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3010/3449]  eta: 0:27:27  lr: 0.000100  loss: 0.0493 (0.0738)  time: 3.7712  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3020/3449]  eta: 0:26:50  lr: 0.000100  loss: 0.0516 (0.0737)  time: 3.7615  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3030/3449]  eta: 0:26:12  lr: 0.000100  loss: 0.0505 (0.0737)  time: 3.7453  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3040/3449]  eta: 0:25:35  lr: 0.000100  loss: 0.0445 (0.0735)  time: 3.7501  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3050/3449]  eta: 0:24:57  lr: 0.000100  loss: 0.0356 (0.0735)  time: 3.7249  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3060/3449]  eta: 0:24:20  lr: 0.000100  loss: 0.0533 (0.0734)  time: 3.7410  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3070/3449]  eta: 0:23:42  lr: 0.000100  loss: 0.0534 (0.0733)  time: 3.7295  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [3080/3449]  eta: 0:23:04  lr: 0.000100  loss: 0.0513 (0.0732)  time: 3.7158  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3090/3449]  eta: 0:22:27  lr: 0.000100  loss: 0.0528 (0.0732)  time: 3.7777  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3100/3449]  eta: 0:21:50  lr: 0.000100  loss: 0.0730 (0.0733)  time: 3.8119  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3110/3449]  eta: 0:21:12  lr: 0.000100  loss: 0.0658 (0.0732)  time: 3.7988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3120/3449]  eta: 0:20:35  lr: 0.000100  loss: 0.0728 (0.0732)  time: 3.7900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3130/3449]  eta: 0:19:57  lr: 0.000100  loss: 0.0563 (0.0732)  time: 3.7537  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3140/3449]  eta: 0:19:19  lr: 0.000100  loss: 0.0563 (0.0733)  time: 3.7443  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3150/3449]  eta: 0:18:42  lr: 0.000100  loss: 0.1241 (0.0734)  time: 3.8117  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3160/3449]  eta: 0:18:05  lr: 0.000100  loss: 0.0857 (0.0734)  time: 3.8427  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3170/3449]  eta: 0:17:27  lr: 0.000100  loss: 0.0684 (0.0734)  time: 3.7725  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3180/3449]  eta: 0:16:49  lr: 0.000100  loss: 0.0560 (0.0734)  time: 3.7322  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3190/3449]  eta: 0:16:12  lr: 0.000100  loss: 0.0581 (0.0733)  time: 3.7544  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3200/3449]  eta: 0:15:34  lr: 0.000100  loss: 0.0492 (0.0733)  time: 3.7518  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3210/3449]  eta: 0:14:57  lr: 0.000100  loss: 0.0522 (0.0733)  time: 3.7403  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3220/3449]  eta: 0:14:19  lr: 0.000100  loss: 0.0545 (0.0732)  time: 3.8067  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3230/3449]  eta: 0:13:42  lr: 0.000100  loss: 0.0594 (0.0732)  time: 3.7919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3240/3449]  eta: 0:13:04  lr: 0.000100  loss: 0.0921 (0.0733)  time: 3.7222  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3250/3449]  eta: 0:12:27  lr: 0.000100  loss: 0.0954 (0.0734)  time: 3.7472  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3260/3449]  eta: 0:11:49  lr: 0.000100  loss: 0.0887 (0.0734)  time: 3.7606  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3270/3449]  eta: 0:11:12  lr: 0.000100  loss: 0.0719 (0.0734)  time: 3.7416  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3280/3449]  eta: 0:10:34  lr: 0.000100  loss: 0.0703 (0.0734)  time: 3.8069  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3290/3449]  eta: 0:09:57  lr: 0.000100  loss: 0.0595 (0.0733)  time: 3.8264  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [3300/3449]  eta: 0:09:19  lr: 0.000100  loss: 0.0438 (0.0732)  time: 3.7686  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:25]  [3310/3449]  eta: 0:08:41  lr: 0.000100  loss: 0.0489 (0.0732)  time: 3.7529  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3320/3449]  eta: 0:08:04  lr: 0.000100  loss: 0.0452 (0.0731)  time: 3.7354  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3330/3449]  eta: 0:07:26  lr: 0.000100  loss: 0.0392 (0.0730)  time: 3.7397  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3340/3449]  eta: 0:06:49  lr: 0.000100  loss: 0.0441 (0.0730)  time: 3.7365  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3350/3449]  eta: 0:06:11  lr: 0.000100  loss: 0.0576 (0.0729)  time: 3.7359  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3360/3449]  eta: 0:05:34  lr: 0.000100  loss: 0.0573 (0.0729)  time: 3.7895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3370/3449]  eta: 0:04:56  lr: 0.000100  loss: 0.0876 (0.0731)  time: 3.7649  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3380/3449]  eta: 0:04:19  lr: 0.000100  loss: 0.1314 (0.0732)  time: 3.7689  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3390/3449]  eta: 0:03:41  lr: 0.000100  loss: 0.0995 (0.0733)  time: 3.7888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3400/3449]  eta: 0:03:03  lr: 0.000100  loss: 0.0710 (0.0733)  time: 3.7265  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3410/3449]  eta: 0:02:26  lr: 0.000100  loss: 0.0697 (0.0733)  time: 3.7297  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3420/3449]  eta: 0:01:48  lr: 0.000100  loss: 0.0684 (0.0733)  time: 3.7592  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3430/3449]  eta: 0:01:11  lr: 0.000100  loss: 0.0652 (0.0732)  time: 3.8343  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3440/3449]  eta: 0:00:33  lr: 0.000100  loss: 0.0748 (0.0733)  time: 3.8194  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0826 (0.0734)  time: 3.7477  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:25] Total time: 3:35:50 (3.7550 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0826 (0.0734)\n",
      "Valid: [epoch:25]  [ 0/14]  eta: 0:04:20  loss: 0.1105 (0.1105)  time: 18.6263  data: 0.4431  max mem: 34968\n",
      "Valid: [epoch:25]  [13/14]  eta: 0:00:18  loss: 0.1107 (0.1135)  time: 18.2310  data: 0.0318  max mem: 34968\n",
      "Valid: [epoch:25] Total time: 0:04:15 (18.2399 s / it)\n",
      "Averaged stats: loss: 0.1107 (0.1135)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_25_input_n_20.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the network on the 14 valid images: 0.113%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:26]  [   0/3449]  eta: 4:32:38  lr: 0.000100  loss: 0.1381 (0.1381)  time: 4.7429  data: 1.3082  max mem: 34968\n",
      "Train: [epoch:26]  [  10/3449]  eta: 3:41:29  lr: 0.000100  loss: 0.1168 (0.1158)  time: 3.8645  data: 0.1190  max mem: 34968\n",
      "Train: [epoch:26]  [  20/3449]  eta: 3:38:40  lr: 0.000100  loss: 0.1168 (0.1159)  time: 3.7805  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [  30/3449]  eta: 3:36:12  lr: 0.000100  loss: 0.1288 (0.1218)  time: 3.7554  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [  40/3449]  eta: 3:35:08  lr: 0.000100  loss: 0.0885 (0.1115)  time: 3.7446  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [  50/3449]  eta: 3:33:02  lr: 0.000100  loss: 0.0530 (0.0981)  time: 3.7091  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [  60/3449]  eta: 3:32:31  lr: 0.000100  loss: 0.0454 (0.0900)  time: 3.7133  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [  70/3449]  eta: 3:31:45  lr: 0.000100  loss: 0.0532 (0.0866)  time: 3.7588  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [  80/3449]  eta: 3:30:51  lr: 0.000100  loss: 0.0669 (0.0848)  time: 3.7329  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [  90/3449]  eta: 3:30:39  lr: 0.000100  loss: 0.0729 (0.0838)  time: 3.7729  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 100/3449]  eta: 3:30:23  lr: 0.000100  loss: 0.0748 (0.0833)  time: 3.8261  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 110/3449]  eta: 3:29:39  lr: 0.000100  loss: 0.0711 (0.0820)  time: 3.7875  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 120/3449]  eta: 3:29:06  lr: 0.000100  loss: 0.0623 (0.0797)  time: 3.7660  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 130/3449]  eta: 3:28:21  lr: 0.000100  loss: 0.0668 (0.0804)  time: 3.7616  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 140/3449]  eta: 3:27:35  lr: 0.000100  loss: 0.0651 (0.0790)  time: 3.7358  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 150/3449]  eta: 3:26:48  lr: 0.000100  loss: 0.0495 (0.0773)  time: 3.7281  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [ 160/3449]  eta: 3:25:54  lr: 0.000100  loss: 0.0498 (0.0761)  time: 3.7021  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 170/3449]  eta: 3:25:14  lr: 0.000100  loss: 0.0588 (0.0752)  time: 3.7120  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 180/3449]  eta: 3:24:55  lr: 0.000100  loss: 0.0578 (0.0742)  time: 3.8020  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [ 190/3449]  eta: 3:24:20  lr: 0.000100  loss: 0.0477 (0.0727)  time: 3.8173  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [ 200/3449]  eta: 3:23:43  lr: 0.000100  loss: 0.0374 (0.0711)  time: 3.7696  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 210/3449]  eta: 3:23:02  lr: 0.000100  loss: 0.0533 (0.0706)  time: 3.7516  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 220/3449]  eta: 3:22:08  lr: 0.000100  loss: 0.0674 (0.0712)  time: 3.6947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 230/3449]  eta: 3:21:26  lr: 0.000100  loss: 0.0802 (0.0721)  time: 3.6859  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 240/3449]  eta: 3:20:51  lr: 0.000100  loss: 0.1209 (0.0750)  time: 3.7492  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 250/3449]  eta: 3:20:07  lr: 0.000100  loss: 0.1204 (0.0764)  time: 3.7420  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 260/3449]  eta: 3:19:23  lr: 0.000100  loss: 0.1007 (0.0772)  time: 3.7016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 270/3449]  eta: 3:18:38  lr: 0.000100  loss: 0.0870 (0.0774)  time: 3.6936  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 280/3449]  eta: 3:18:00  lr: 0.000100  loss: 0.1047 (0.0802)  time: 3.7189  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 290/3449]  eta: 3:17:20  lr: 0.000100  loss: 0.1342 (0.0814)  time: 3.7334  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 300/3449]  eta: 3:16:38  lr: 0.000100  loss: 0.0962 (0.0811)  time: 3.7147  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 310/3449]  eta: 3:16:09  lr: 0.000100  loss: 0.0667 (0.0806)  time: 3.7666  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 320/3449]  eta: 3:15:26  lr: 0.000100  loss: 0.0533 (0.0795)  time: 3.7623  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 330/3449]  eta: 3:14:48  lr: 0.000100  loss: 0.0503 (0.0790)  time: 3.7211  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 340/3449]  eta: 3:14:05  lr: 0.000100  loss: 0.0548 (0.0781)  time: 3.7134  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 350/3449]  eta: 3:13:28  lr: 0.000100  loss: 0.0522 (0.0775)  time: 3.7163  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 360/3449]  eta: 3:12:41  lr: 0.000100  loss: 0.0626 (0.0775)  time: 3.6901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 370/3449]  eta: 3:12:07  lr: 0.000100  loss: 0.0736 (0.0773)  time: 3.7104  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 380/3449]  eta: 3:11:35  lr: 0.000100  loss: 0.0619 (0.0768)  time: 3.8009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 390/3449]  eta: 3:10:55  lr: 0.000100  loss: 0.0619 (0.0768)  time: 3.7606  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 400/3449]  eta: 3:10:16  lr: 0.000100  loss: 0.0754 (0.0767)  time: 3.7153  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 410/3449]  eta: 3:09:40  lr: 0.000100  loss: 0.0677 (0.0766)  time: 3.7480  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 420/3449]  eta: 3:09:07  lr: 0.000100  loss: 0.0639 (0.0762)  time: 3.7888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [ 430/3449]  eta: 3:08:31  lr: 0.000100  loss: 0.0587 (0.0758)  time: 3.7844  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 440/3449]  eta: 3:07:53  lr: 0.000100  loss: 0.0473 (0.0752)  time: 3.7500  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 450/3449]  eta: 3:07:14  lr: 0.000100  loss: 0.0621 (0.0752)  time: 3.7357  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 460/3449]  eta: 3:06:40  lr: 0.000100  loss: 0.0745 (0.0753)  time: 3.7639  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 470/3449]  eta: 3:06:05  lr: 0.000100  loss: 0.0736 (0.0753)  time: 3.7884  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 480/3449]  eta: 3:05:29  lr: 0.000100  loss: 0.0723 (0.0751)  time: 3.7826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 490/3449]  eta: 3:04:56  lr: 0.000100  loss: 0.0477 (0.0744)  time: 3.8012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 500/3449]  eta: 3:04:14  lr: 0.000100  loss: 0.0388 (0.0737)  time: 3.7410  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 510/3449]  eta: 3:03:35  lr: 0.000100  loss: 0.0364 (0.0730)  time: 3.6951  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 520/3449]  eta: 3:03:02  lr: 0.000100  loss: 0.0350 (0.0723)  time: 3.7750  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 530/3449]  eta: 3:02:23  lr: 0.000100  loss: 0.0335 (0.0716)  time: 3.7724  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 540/3449]  eta: 3:01:45  lr: 0.000100  loss: 0.0360 (0.0713)  time: 3.7308  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 550/3449]  eta: 3:01:09  lr: 0.000100  loss: 0.0605 (0.0713)  time: 3.7572  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 560/3449]  eta: 3:00:31  lr: 0.000100  loss: 0.0580 (0.0710)  time: 3.7596  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 570/3449]  eta: 2:59:55  lr: 0.000100  loss: 0.0580 (0.0709)  time: 3.7666  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 580/3449]  eta: 2:59:23  lr: 0.000100  loss: 0.0664 (0.0707)  time: 3.8230  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 590/3449]  eta: 2:58:46  lr: 0.000100  loss: 0.0614 (0.0705)  time: 3.8069  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 600/3449]  eta: 2:58:08  lr: 0.000100  loss: 0.0569 (0.0702)  time: 3.7549  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [ 610/3449]  eta: 2:57:30  lr: 0.000100  loss: 0.0535 (0.0700)  time: 3.7475  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [ 620/3449]  eta: 2:56:52  lr: 0.000100  loss: 0.0527 (0.0697)  time: 3.7405  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 630/3449]  eta: 2:56:18  lr: 0.000100  loss: 0.0560 (0.0696)  time: 3.7811  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 640/3449]  eta: 2:55:43  lr: 0.000100  loss: 0.0584 (0.0695)  time: 3.8175  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:26]  [ 650/3449]  eta: 2:55:06  lr: 0.000100  loss: 0.0578 (0.0692)  time: 3.7890  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [ 660/3449]  eta: 2:54:28  lr: 0.000100  loss: 0.0434 (0.0688)  time: 3.7535  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 670/3449]  eta: 2:53:54  lr: 0.000100  loss: 0.0412 (0.0684)  time: 3.7835  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 680/3449]  eta: 2:53:16  lr: 0.000100  loss: 0.0400 (0.0681)  time: 3.7965  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 690/3449]  eta: 2:52:37  lr: 0.000100  loss: 0.0529 (0.0679)  time: 3.7332  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 700/3449]  eta: 2:51:59  lr: 0.000100  loss: 0.0554 (0.0682)  time: 3.7317  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 710/3449]  eta: 2:51:26  lr: 0.000100  loss: 0.0645 (0.0682)  time: 3.8052  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 720/3449]  eta: 2:50:44  lr: 0.000100  loss: 0.0631 (0.0682)  time: 3.7470  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 730/3449]  eta: 2:50:09  lr: 0.000100  loss: 0.0630 (0.0681)  time: 3.7408  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 740/3449]  eta: 2:49:31  lr: 0.000100  loss: 0.0535 (0.0678)  time: 3.7887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 750/3449]  eta: 2:48:51  lr: 0.000100  loss: 0.0441 (0.0676)  time: 3.7140  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 760/3449]  eta: 2:48:13  lr: 0.000100  loss: 0.0506 (0.0674)  time: 3.7047  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 770/3449]  eta: 2:47:34  lr: 0.000100  loss: 0.0574 (0.0673)  time: 3.7234  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [ 780/3449]  eta: 2:47:00  lr: 0.000100  loss: 0.0610 (0.0674)  time: 3.7906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [ 790/3449]  eta: 2:46:24  lr: 0.000100  loss: 0.0868 (0.0678)  time: 3.8285  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 800/3449]  eta: 2:45:46  lr: 0.000100  loss: 0.1051 (0.0683)  time: 3.7629  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 810/3449]  eta: 2:45:09  lr: 0.000100  loss: 0.0766 (0.0684)  time: 3.7609  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 820/3449]  eta: 2:44:31  lr: 0.000100  loss: 0.0766 (0.0687)  time: 3.7551  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 830/3449]  eta: 2:43:51  lr: 0.000100  loss: 0.0831 (0.0689)  time: 3.7097  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 840/3449]  eta: 2:43:18  lr: 0.000100  loss: 0.0925 (0.0695)  time: 3.7901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 850/3449]  eta: 2:42:39  lr: 0.000100  loss: 0.0891 (0.0696)  time: 3.7912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 860/3449]  eta: 2:42:01  lr: 0.000100  loss: 0.0500 (0.0693)  time: 3.7293  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 870/3449]  eta: 2:41:22  lr: 0.000100  loss: 0.0453 (0.0690)  time: 3.7329  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 880/3449]  eta: 2:40:44  lr: 0.000100  loss: 0.0411 (0.0687)  time: 3.7270  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 890/3449]  eta: 2:40:05  lr: 0.000100  loss: 0.0354 (0.0683)  time: 3.7254  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 900/3449]  eta: 2:39:30  lr: 0.000100  loss: 0.0466 (0.0682)  time: 3.7630  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 910/3449]  eta: 2:38:52  lr: 0.000100  loss: 0.0558 (0.0681)  time: 3.7874  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 920/3449]  eta: 2:38:16  lr: 0.000100  loss: 0.0378 (0.0677)  time: 3.7717  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 930/3449]  eta: 2:37:38  lr: 0.000100  loss: 0.0318 (0.0674)  time: 3.7651  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 940/3449]  eta: 2:37:01  lr: 0.000100  loss: 0.0331 (0.0671)  time: 3.7586  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 950/3449]  eta: 2:36:24  lr: 0.000100  loss: 0.0331 (0.0668)  time: 3.7721  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 960/3449]  eta: 2:35:45  lr: 0.000100  loss: 0.0352 (0.0665)  time: 3.7528  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 970/3449]  eta: 2:35:07  lr: 0.000100  loss: 0.0416 (0.0663)  time: 3.7314  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 980/3449]  eta: 2:34:31  lr: 0.000100  loss: 0.0418 (0.0661)  time: 3.7617  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [ 990/3449]  eta: 2:33:57  lr: 0.000100  loss: 0.0418 (0.0659)  time: 3.8515  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1000/3449]  eta: 2:33:19  lr: 0.000100  loss: 0.0507 (0.0658)  time: 3.8270  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1010/3449]  eta: 2:32:40  lr: 0.000100  loss: 0.0619 (0.0660)  time: 3.7228  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1020/3449]  eta: 2:32:02  lr: 0.000100  loss: 0.0661 (0.0660)  time: 3.7114  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1030/3449]  eta: 2:31:23  lr: 0.000100  loss: 0.0693 (0.0661)  time: 3.7133  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1040/3449]  eta: 2:30:47  lr: 0.000100  loss: 0.0705 (0.0661)  time: 3.7666  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1050/3449]  eta: 2:30:08  lr: 0.000100  loss: 0.0570 (0.0660)  time: 3.7644  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1060/3449]  eta: 2:29:32  lr: 0.000100  loss: 0.0651 (0.0662)  time: 3.7628  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1070/3449]  eta: 2:28:55  lr: 0.000100  loss: 0.0897 (0.0666)  time: 3.7974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1080/3449]  eta: 2:28:17  lr: 0.000100  loss: 0.0883 (0.0669)  time: 3.7451  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1090/3449]  eta: 2:27:40  lr: 0.000100  loss: 0.0763 (0.0669)  time: 3.7638  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1100/3449]  eta: 2:27:03  lr: 0.000100  loss: 0.0700 (0.0671)  time: 3.7816  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1110/3449]  eta: 2:26:26  lr: 0.000100  loss: 0.0878 (0.0672)  time: 3.7630  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1120/3449]  eta: 2:25:48  lr: 0.000100  loss: 0.0821 (0.0672)  time: 3.7582  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1130/3449]  eta: 2:25:10  lr: 0.000100  loss: 0.0566 (0.0670)  time: 3.7374  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [1140/3449]  eta: 2:24:32  lr: 0.000100  loss: 0.0367 (0.0668)  time: 3.7311  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1150/3449]  eta: 2:23:55  lr: 0.000100  loss: 0.0380 (0.0667)  time: 3.7625  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1160/3449]  eta: 2:23:17  lr: 0.000100  loss: 0.0687 (0.0669)  time: 3.7688  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1170/3449]  eta: 2:22:38  lr: 0.000100  loss: 0.0798 (0.0670)  time: 3.7262  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1180/3449]  eta: 2:22:03  lr: 0.000100  loss: 0.0654 (0.0670)  time: 3.7761  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1190/3449]  eta: 2:21:25  lr: 0.000100  loss: 0.0730 (0.0671)  time: 3.8046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1200/3449]  eta: 2:20:48  lr: 0.000100  loss: 0.0861 (0.0673)  time: 3.7676  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1210/3449]  eta: 2:20:10  lr: 0.000100  loss: 0.0923 (0.0676)  time: 3.7497  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1220/3449]  eta: 2:19:32  lr: 0.000100  loss: 0.0982 (0.0678)  time: 3.7277  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1230/3449]  eta: 2:18:57  lr: 0.000100  loss: 0.0845 (0.0679)  time: 3.8092  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1240/3449]  eta: 2:18:17  lr: 0.000100  loss: 0.0723 (0.0679)  time: 3.7637  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1250/3449]  eta: 2:17:41  lr: 0.000100  loss: 0.0621 (0.0678)  time: 3.7449  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1260/3449]  eta: 2:17:04  lr: 0.000100  loss: 0.0558 (0.0677)  time: 3.8192  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1270/3449]  eta: 2:16:26  lr: 0.000100  loss: 0.0526 (0.0677)  time: 3.7699  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1280/3449]  eta: 2:15:48  lr: 0.000100  loss: 0.0659 (0.0680)  time: 3.7388  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1290/3449]  eta: 2:15:10  lr: 0.000100  loss: 0.1142 (0.0685)  time: 3.7308  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1300/3449]  eta: 2:14:32  lr: 0.000100  loss: 0.1242 (0.0688)  time: 3.7336  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:26]  [1310/3449]  eta: 2:13:54  lr: 0.000100  loss: 0.1010 (0.0688)  time: 3.7341  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1320/3449]  eta: 2:13:16  lr: 0.000100  loss: 0.0624 (0.0688)  time: 3.7277  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1330/3449]  eta: 2:12:40  lr: 0.000100  loss: 0.0680 (0.0688)  time: 3.7617  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1340/3449]  eta: 2:12:02  lr: 0.000100  loss: 0.0671 (0.0688)  time: 3.7939  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1350/3449]  eta: 2:11:24  lr: 0.000100  loss: 0.0518 (0.0686)  time: 3.7327  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [1360/3449]  eta: 2:10:46  lr: 0.000100  loss: 0.0338 (0.0684)  time: 3.7045  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1370/3449]  eta: 2:10:09  lr: 0.000100  loss: 0.0378 (0.0682)  time: 3.7641  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1380/3449]  eta: 2:09:31  lr: 0.000100  loss: 0.0523 (0.0682)  time: 3.7606  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1390/3449]  eta: 2:08:53  lr: 0.000100  loss: 0.0650 (0.0681)  time: 3.7498  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1400/3449]  eta: 2:08:16  lr: 0.000100  loss: 0.0608 (0.0681)  time: 3.7848  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1410/3449]  eta: 2:07:38  lr: 0.000100  loss: 0.0598 (0.0681)  time: 3.7561  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1420/3449]  eta: 2:07:02  lr: 0.000100  loss: 0.0566 (0.0680)  time: 3.7951  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1430/3449]  eta: 2:06:25  lr: 0.000100  loss: 0.0551 (0.0680)  time: 3.8242  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [1440/3449]  eta: 2:05:47  lr: 0.000100  loss: 0.0534 (0.0679)  time: 3.7511  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1450/3449]  eta: 2:05:09  lr: 0.000100  loss: 0.0548 (0.0679)  time: 3.7046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1460/3449]  eta: 2:04:31  lr: 0.000100  loss: 0.0895 (0.0682)  time: 3.7071  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1470/3449]  eta: 2:03:53  lr: 0.000100  loss: 0.0992 (0.0683)  time: 3.7294  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1480/3449]  eta: 2:03:15  lr: 0.000100  loss: 0.0860 (0.0684)  time: 3.7600  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1490/3449]  eta: 2:02:38  lr: 0.000100  loss: 0.0832 (0.0685)  time: 3.7596  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1500/3449]  eta: 2:01:58  lr: 0.000100  loss: 0.0601 (0.0684)  time: 3.6710  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1510/3449]  eta: 2:01:22  lr: 0.000100  loss: 0.0466 (0.0682)  time: 3.7235  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1520/3449]  eta: 2:00:45  lr: 0.000100  loss: 0.0470 (0.0681)  time: 3.8185  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1530/3449]  eta: 2:00:07  lr: 0.000100  loss: 0.0423 (0.0679)  time: 3.7622  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1540/3449]  eta: 1:59:29  lr: 0.000100  loss: 0.0319 (0.0677)  time: 3.7253  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1550/3449]  eta: 1:58:51  lr: 0.000100  loss: 0.0308 (0.0674)  time: 3.7156  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1560/3449]  eta: 1:58:13  lr: 0.000100  loss: 0.0306 (0.0672)  time: 3.7136  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1570/3449]  eta: 1:57:36  lr: 0.000100  loss: 0.0362 (0.0671)  time: 3.7800  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1580/3449]  eta: 1:56:59  lr: 0.000100  loss: 0.0414 (0.0669)  time: 3.8174  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1590/3449]  eta: 1:56:22  lr: 0.000100  loss: 0.0420 (0.0668)  time: 3.7866  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1600/3449]  eta: 1:55:44  lr: 0.000100  loss: 0.0475 (0.0667)  time: 3.7637  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1610/3449]  eta: 1:55:06  lr: 0.000100  loss: 0.0433 (0.0665)  time: 3.7388  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1620/3449]  eta: 1:54:28  lr: 0.000100  loss: 0.0418 (0.0664)  time: 3.7182  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1630/3449]  eta: 1:53:50  lr: 0.000100  loss: 0.0469 (0.0663)  time: 3.7097  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1640/3449]  eta: 1:53:12  lr: 0.000100  loss: 0.0454 (0.0662)  time: 3.7089  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1650/3449]  eta: 1:52:34  lr: 0.000100  loss: 0.0463 (0.0661)  time: 3.7054  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [1660/3449]  eta: 1:51:57  lr: 0.000100  loss: 0.0457 (0.0660)  time: 3.7804  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [1670/3449]  eta: 1:51:20  lr: 0.000100  loss: 0.0449 (0.0658)  time: 3.8173  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [1680/3449]  eta: 1:50:42  lr: 0.000100  loss: 0.0410 (0.0657)  time: 3.7600  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1690/3449]  eta: 1:50:07  lr: 0.000100  loss: 0.0383 (0.0655)  time: 3.8367  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1700/3449]  eta: 1:49:29  lr: 0.000100  loss: 0.0451 (0.0655)  time: 3.8280  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1710/3449]  eta: 1:48:51  lr: 0.000100  loss: 0.0447 (0.0653)  time: 3.7333  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1720/3449]  eta: 1:48:14  lr: 0.000100  loss: 0.0314 (0.0651)  time: 3.7580  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1730/3449]  eta: 1:47:36  lr: 0.000100  loss: 0.0285 (0.0649)  time: 3.7515  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1740/3449]  eta: 1:46:58  lr: 0.000100  loss: 0.0295 (0.0647)  time: 3.7149  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1750/3449]  eta: 1:46:20  lr: 0.000100  loss: 0.0320 (0.0646)  time: 3.7205  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1760/3449]  eta: 1:45:42  lr: 0.000100  loss: 0.0362 (0.0645)  time: 3.7403  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1770/3449]  eta: 1:45:05  lr: 0.000100  loss: 0.0540 (0.0644)  time: 3.7333  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1780/3449]  eta: 1:44:27  lr: 0.000100  loss: 0.0601 (0.0644)  time: 3.7578  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1790/3449]  eta: 1:43:50  lr: 0.000100  loss: 0.0581 (0.0643)  time: 3.7723  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1800/3449]  eta: 1:43:12  lr: 0.000100  loss: 0.0596 (0.0642)  time: 3.7264  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1810/3449]  eta: 1:42:35  lr: 0.000100  loss: 0.0433 (0.0641)  time: 3.7868  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1820/3449]  eta: 1:41:58  lr: 0.000100  loss: 0.0361 (0.0640)  time: 3.8149  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1830/3449]  eta: 1:41:20  lr: 0.000100  loss: 0.0352 (0.0638)  time: 3.7345  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1840/3449]  eta: 1:40:42  lr: 0.000100  loss: 0.0366 (0.0637)  time: 3.7369  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1850/3449]  eta: 1:40:06  lr: 0.000100  loss: 0.0421 (0.0636)  time: 3.8210  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1860/3449]  eta: 1:39:28  lr: 0.000100  loss: 0.0366 (0.0634)  time: 3.8066  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [1870/3449]  eta: 1:38:50  lr: 0.000100  loss: 0.0349 (0.0633)  time: 3.7440  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [1880/3449]  eta: 1:38:12  lr: 0.000100  loss: 0.0353 (0.0632)  time: 3.7295  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1890/3449]  eta: 1:37:34  lr: 0.000100  loss: 0.0394 (0.0631)  time: 3.7033  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1900/3449]  eta: 1:36:57  lr: 0.000100  loss: 0.0422 (0.0630)  time: 3.7060  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1910/3449]  eta: 1:36:18  lr: 0.000100  loss: 0.0442 (0.0629)  time: 3.6617  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1920/3449]  eta: 1:35:41  lr: 0.000100  loss: 0.0524 (0.0629)  time: 3.7173  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1930/3449]  eta: 1:35:03  lr: 0.000100  loss: 0.0537 (0.0628)  time: 3.7657  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1940/3449]  eta: 1:34:25  lr: 0.000100  loss: 0.0357 (0.0627)  time: 3.6994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1950/3449]  eta: 1:33:47  lr: 0.000100  loss: 0.0326 (0.0625)  time: 3.7022  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1960/3449]  eta: 1:33:10  lr: 0.000100  loss: 0.0339 (0.0624)  time: 3.7420  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:26]  [1970/3449]  eta: 1:32:32  lr: 0.000100  loss: 0.0399 (0.0623)  time: 3.7279  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1980/3449]  eta: 1:31:54  lr: 0.000100  loss: 0.0391 (0.0622)  time: 3.7256  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [1990/3449]  eta: 1:31:17  lr: 0.000100  loss: 0.0367 (0.0621)  time: 3.7619  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2000/3449]  eta: 1:30:39  lr: 0.000100  loss: 0.0357 (0.0619)  time: 3.7534  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [2010/3449]  eta: 1:30:01  lr: 0.000100  loss: 0.0451 (0.0619)  time: 3.7366  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2020/3449]  eta: 1:29:25  lr: 0.000100  loss: 0.0495 (0.0619)  time: 3.8017  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [2030/3449]  eta: 1:28:47  lr: 0.000100  loss: 0.0462 (0.0618)  time: 3.7850  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2040/3449]  eta: 1:28:09  lr: 0.000100  loss: 0.0450 (0.0617)  time: 3.7183  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2050/3449]  eta: 1:27:31  lr: 0.000100  loss: 0.0515 (0.0617)  time: 3.7482  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2060/3449]  eta: 1:26:54  lr: 0.000100  loss: 0.0509 (0.0616)  time: 3.7465  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2070/3449]  eta: 1:26:16  lr: 0.000100  loss: 0.0416 (0.0615)  time: 3.7336  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [2080/3449]  eta: 1:25:39  lr: 0.000100  loss: 0.0357 (0.0614)  time: 3.7423  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2090/3449]  eta: 1:25:01  lr: 0.000100  loss: 0.0405 (0.0613)  time: 3.7276  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2100/3449]  eta: 1:24:23  lr: 0.000100  loss: 0.0423 (0.0612)  time: 3.7267  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2110/3449]  eta: 1:23:45  lr: 0.000100  loss: 0.0324 (0.0611)  time: 3.7330  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2120/3449]  eta: 1:23:08  lr: 0.000100  loss: 0.0305 (0.0609)  time: 3.7460  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2130/3449]  eta: 1:22:31  lr: 0.000100  loss: 0.0342 (0.0608)  time: 3.7863  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2140/3449]  eta: 1:21:53  lr: 0.000100  loss: 0.0319 (0.0606)  time: 3.7941  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [2150/3449]  eta: 1:21:16  lr: 0.000100  loss: 0.0291 (0.0605)  time: 3.7771  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [2160/3449]  eta: 1:20:38  lr: 0.000100  loss: 0.0291 (0.0604)  time: 3.7548  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2170/3449]  eta: 1:20:01  lr: 0.000100  loss: 0.0287 (0.0602)  time: 3.7963  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2180/3449]  eta: 1:19:24  lr: 0.000100  loss: 0.0275 (0.0601)  time: 3.8145  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2190/3449]  eta: 1:18:46  lr: 0.000100  loss: 0.0280 (0.0600)  time: 3.7499  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2200/3449]  eta: 1:18:08  lr: 0.000100  loss: 0.0286 (0.0598)  time: 3.7205  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [2210/3449]  eta: 1:17:31  lr: 0.000100  loss: 0.0276 (0.0597)  time: 3.7463  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2220/3449]  eta: 1:16:53  lr: 0.000100  loss: 0.0288 (0.0595)  time: 3.7572  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2230/3449]  eta: 1:16:16  lr: 0.000100  loss: 0.0299 (0.0594)  time: 3.7368  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2240/3449]  eta: 1:15:38  lr: 0.000100  loss: 0.0295 (0.0593)  time: 3.7324  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2250/3449]  eta: 1:15:00  lr: 0.000100  loss: 0.0313 (0.0592)  time: 3.7248  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2260/3449]  eta: 1:14:22  lr: 0.000100  loss: 0.0326 (0.0591)  time: 3.7021  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2270/3449]  eta: 1:13:45  lr: 0.000100  loss: 0.0287 (0.0589)  time: 3.7461  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [2280/3449]  eta: 1:13:08  lr: 0.000100  loss: 0.0290 (0.0588)  time: 3.7670  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [2290/3449]  eta: 1:12:30  lr: 0.000100  loss: 0.0363 (0.0588)  time: 3.7394  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2300/3449]  eta: 1:11:53  lr: 0.000100  loss: 0.0429 (0.0587)  time: 3.7798  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2310/3449]  eta: 1:11:15  lr: 0.000100  loss: 0.0474 (0.0587)  time: 3.7826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2320/3449]  eta: 1:10:38  lr: 0.000100  loss: 0.0605 (0.0587)  time: 3.7414  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2330/3449]  eta: 1:10:00  lr: 0.000100  loss: 0.0625 (0.0587)  time: 3.6949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2340/3449]  eta: 1:09:22  lr: 0.000100  loss: 0.0445 (0.0586)  time: 3.7037  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2350/3449]  eta: 1:08:45  lr: 0.000100  loss: 0.0421 (0.0586)  time: 3.7680  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2360/3449]  eta: 1:08:07  lr: 0.000100  loss: 0.0420 (0.0585)  time: 3.7701  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2370/3449]  eta: 1:07:29  lr: 0.000100  loss: 0.0400 (0.0584)  time: 3.7472  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2380/3449]  eta: 1:06:52  lr: 0.000100  loss: 0.0353 (0.0583)  time: 3.7311  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2390/3449]  eta: 1:06:14  lr: 0.000100  loss: 0.0313 (0.0582)  time: 3.7238  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2400/3449]  eta: 1:05:37  lr: 0.000100  loss: 0.0307 (0.0581)  time: 3.7303  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2410/3449]  eta: 1:04:59  lr: 0.000100  loss: 0.0300 (0.0580)  time: 3.7424  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2420/3449]  eta: 1:04:22  lr: 0.000100  loss: 0.0300 (0.0579)  time: 3.8015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2430/3449]  eta: 1:03:44  lr: 0.000100  loss: 0.0306 (0.0578)  time: 3.8181  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2440/3449]  eta: 1:03:07  lr: 0.000100  loss: 0.0324 (0.0577)  time: 3.7529  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2450/3449]  eta: 1:02:29  lr: 0.000100  loss: 0.0320 (0.0576)  time: 3.7400  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2460/3449]  eta: 1:01:52  lr: 0.000100  loss: 0.0320 (0.0575)  time: 3.7563  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2470/3449]  eta: 1:01:14  lr: 0.000100  loss: 0.0401 (0.0574)  time: 3.7563  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2480/3449]  eta: 1:00:37  lr: 0.000100  loss: 0.0449 (0.0574)  time: 3.7375  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2490/3449]  eta: 0:59:59  lr: 0.000100  loss: 0.0572 (0.0574)  time: 3.7861  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2500/3449]  eta: 0:59:22  lr: 0.000100  loss: 0.0513 (0.0574)  time: 3.8089  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2510/3449]  eta: 0:58:44  lr: 0.000100  loss: 0.0515 (0.0574)  time: 3.7366  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2520/3449]  eta: 0:58:07  lr: 0.000100  loss: 0.0534 (0.0574)  time: 3.7758  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2530/3449]  eta: 0:57:30  lr: 0.000100  loss: 0.0368 (0.0573)  time: 3.8287  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2540/3449]  eta: 0:56:52  lr: 0.000100  loss: 0.0313 (0.0572)  time: 3.7736  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2550/3449]  eta: 0:56:15  lr: 0.000100  loss: 0.0318 (0.0571)  time: 3.7432  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2560/3449]  eta: 0:55:37  lr: 0.000100  loss: 0.0341 (0.0571)  time: 3.7622  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [2570/3449]  eta: 0:55:00  lr: 0.000100  loss: 0.0435 (0.0570)  time: 3.7757  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2580/3449]  eta: 0:54:22  lr: 0.000100  loss: 0.0322 (0.0569)  time: 3.7889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2590/3449]  eta: 0:53:44  lr: 0.000100  loss: 0.0291 (0.0568)  time: 3.7527  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2600/3449]  eta: 0:53:07  lr: 0.000100  loss: 0.0304 (0.0567)  time: 3.7187  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2610/3449]  eta: 0:52:29  lr: 0.000100  loss: 0.0367 (0.0567)  time: 3.7261  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2620/3449]  eta: 0:51:52  lr: 0.000100  loss: 0.0383 (0.0566)  time: 3.8013  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:26]  [2630/3449]  eta: 0:51:15  lr: 0.000100  loss: 0.0369 (0.0566)  time: 3.8321  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2640/3449]  eta: 0:50:37  lr: 0.000100  loss: 0.0525 (0.0566)  time: 3.7452  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2650/3449]  eta: 0:49:59  lr: 0.000100  loss: 0.0511 (0.0566)  time: 3.7052  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2660/3449]  eta: 0:49:22  lr: 0.000100  loss: 0.0511 (0.0566)  time: 3.7379  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2670/3449]  eta: 0:48:44  lr: 0.000100  loss: 0.0532 (0.0565)  time: 3.7966  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2680/3449]  eta: 0:48:07  lr: 0.000100  loss: 0.0490 (0.0565)  time: 3.7970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2690/3449]  eta: 0:47:29  lr: 0.000100  loss: 0.0429 (0.0565)  time: 3.7621  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2700/3449]  eta: 0:46:52  lr: 0.000100  loss: 0.0513 (0.0564)  time: 3.7971  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2710/3449]  eta: 0:46:14  lr: 0.000100  loss: 0.0487 (0.0564)  time: 3.7962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2720/3449]  eta: 0:45:37  lr: 0.000100  loss: 0.0439 (0.0563)  time: 3.7806  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2730/3449]  eta: 0:44:59  lr: 0.000100  loss: 0.0347 (0.0563)  time: 3.7747  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2740/3449]  eta: 0:44:22  lr: 0.000100  loss: 0.0351 (0.0562)  time: 3.8124  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2750/3449]  eta: 0:43:45  lr: 0.000100  loss: 0.0333 (0.0561)  time: 3.8013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2760/3449]  eta: 0:43:07  lr: 0.000100  loss: 0.0325 (0.0561)  time: 3.7320  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2770/3449]  eta: 0:42:29  lr: 0.000100  loss: 0.0377 (0.0560)  time: 3.7044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2780/3449]  eta: 0:41:52  lr: 0.000100  loss: 0.0388 (0.0559)  time: 3.7215  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2790/3449]  eta: 0:41:14  lr: 0.000100  loss: 0.0375 (0.0559)  time: 3.7570  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2800/3449]  eta: 0:40:37  lr: 0.000100  loss: 0.0405 (0.0558)  time: 3.7355  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2810/3449]  eta: 0:39:59  lr: 0.000100  loss: 0.0459 (0.0558)  time: 3.7490  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2820/3449]  eta: 0:39:21  lr: 0.000100  loss: 0.0444 (0.0558)  time: 3.7130  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [2830/3449]  eta: 0:38:44  lr: 0.000100  loss: 0.0422 (0.0557)  time: 3.7009  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [2840/3449]  eta: 0:38:06  lr: 0.000100  loss: 0.0402 (0.0557)  time: 3.7958  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2850/3449]  eta: 0:37:29  lr: 0.000100  loss: 0.0402 (0.0556)  time: 3.8101  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2860/3449]  eta: 0:36:51  lr: 0.000100  loss: 0.0402 (0.0556)  time: 3.7318  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2870/3449]  eta: 0:36:14  lr: 0.000100  loss: 0.0356 (0.0555)  time: 3.7120  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2880/3449]  eta: 0:35:36  lr: 0.000100  loss: 0.0339 (0.0554)  time: 3.7719  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2890/3449]  eta: 0:34:58  lr: 0.000100  loss: 0.0347 (0.0554)  time: 3.7017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2900/3449]  eta: 0:34:21  lr: 0.000100  loss: 0.0365 (0.0553)  time: 3.7145  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2910/3449]  eta: 0:33:43  lr: 0.000100  loss: 0.0380 (0.0553)  time: 3.7894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2920/3449]  eta: 0:33:06  lr: 0.000100  loss: 0.0345 (0.0552)  time: 3.7493  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2930/3449]  eta: 0:32:28  lr: 0.000100  loss: 0.0304 (0.0551)  time: 3.7642  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2940/3449]  eta: 0:31:51  lr: 0.000100  loss: 0.0298 (0.0550)  time: 3.8044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2950/3449]  eta: 0:31:13  lr: 0.000100  loss: 0.0311 (0.0550)  time: 3.7877  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2960/3449]  eta: 0:30:36  lr: 0.000100  loss: 0.0305 (0.0549)  time: 3.7454  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2970/3449]  eta: 0:29:58  lr: 0.000100  loss: 0.0300 (0.0548)  time: 3.7584  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2980/3449]  eta: 0:29:21  lr: 0.000100  loss: 0.0302 (0.0547)  time: 3.7848  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [2990/3449]  eta: 0:28:43  lr: 0.000100  loss: 0.0324 (0.0547)  time: 3.7907  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3000/3449]  eta: 0:28:05  lr: 0.000100  loss: 0.0278 (0.0546)  time: 3.7228  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3010/3449]  eta: 0:27:28  lr: 0.000100  loss: 0.0269 (0.0545)  time: 3.6947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3020/3449]  eta: 0:26:50  lr: 0.000100  loss: 0.0301 (0.0544)  time: 3.7334  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3030/3449]  eta: 0:26:13  lr: 0.000100  loss: 0.0314 (0.0543)  time: 3.7637  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3040/3449]  eta: 0:25:35  lr: 0.000100  loss: 0.0293 (0.0543)  time: 3.7803  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3050/3449]  eta: 0:24:58  lr: 0.000100  loss: 0.0292 (0.0542)  time: 3.7684  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3060/3449]  eta: 0:24:20  lr: 0.000100  loss: 0.0304 (0.0541)  time: 3.8007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3070/3449]  eta: 0:23:43  lr: 0.000100  loss: 0.0304 (0.0540)  time: 3.8140  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3080/3449]  eta: 0:23:05  lr: 0.000100  loss: 0.0303 (0.0540)  time: 3.7613  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3090/3449]  eta: 0:22:28  lr: 0.000100  loss: 0.0291 (0.0539)  time: 3.7617  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3100/3449]  eta: 0:21:50  lr: 0.000100  loss: 0.0292 (0.0538)  time: 3.7493  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [3110/3449]  eta: 0:21:12  lr: 0.000100  loss: 0.0384 (0.0538)  time: 3.7106  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3120/3449]  eta: 0:20:35  lr: 0.000100  loss: 0.0384 (0.0537)  time: 3.8019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3130/3449]  eta: 0:19:58  lr: 0.000100  loss: 0.0333 (0.0537)  time: 3.8161  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3140/3449]  eta: 0:19:20  lr: 0.000100  loss: 0.0421 (0.0536)  time: 3.7308  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3150/3449]  eta: 0:18:42  lr: 0.000100  loss: 0.0421 (0.0536)  time: 3.7828  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3160/3449]  eta: 0:18:05  lr: 0.000100  loss: 0.0321 (0.0535)  time: 3.8026  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3170/3449]  eta: 0:17:27  lr: 0.000100  loss: 0.0352 (0.0535)  time: 3.7748  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3180/3449]  eta: 0:16:50  lr: 0.000100  loss: 0.0357 (0.0535)  time: 3.7390  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [3190/3449]  eta: 0:16:12  lr: 0.000100  loss: 0.0319 (0.0534)  time: 3.7273  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3200/3449]  eta: 0:15:35  lr: 0.000100  loss: 0.0317 (0.0533)  time: 3.7429  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3210/3449]  eta: 0:14:57  lr: 0.000100  loss: 0.0308 (0.0533)  time: 3.7675  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3220/3449]  eta: 0:14:20  lr: 0.000100  loss: 0.0388 (0.0533)  time: 3.8081  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3230/3449]  eta: 0:13:42  lr: 0.000100  loss: 0.0473 (0.0532)  time: 3.7798  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3240/3449]  eta: 0:13:04  lr: 0.000100  loss: 0.0473 (0.0532)  time: 3.7532  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3250/3449]  eta: 0:12:27  lr: 0.000100  loss: 0.0543 (0.0532)  time: 3.7655  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3260/3449]  eta: 0:11:49  lr: 0.000100  loss: 0.0462 (0.0532)  time: 3.7984  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3270/3449]  eta: 0:11:12  lr: 0.000100  loss: 0.0396 (0.0532)  time: 3.7700  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3280/3449]  eta: 0:10:34  lr: 0.000100  loss: 0.0434 (0.0531)  time: 3.7211  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:26]  [3290/3449]  eta: 0:09:57  lr: 0.000100  loss: 0.0434 (0.0531)  time: 3.7369  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3300/3449]  eta: 0:09:19  lr: 0.000100  loss: 0.0412 (0.0531)  time: 3.7445  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3310/3449]  eta: 0:08:42  lr: 0.000100  loss: 0.0432 (0.0530)  time: 3.7495  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [3320/3449]  eta: 0:08:04  lr: 0.000100  loss: 0.0393 (0.0530)  time: 3.7309  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3330/3449]  eta: 0:07:26  lr: 0.000100  loss: 0.0316 (0.0529)  time: 3.7858  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3340/3449]  eta: 0:06:49  lr: 0.000100  loss: 0.0307 (0.0529)  time: 3.8136  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:26]  [3350/3449]  eta: 0:06:11  lr: 0.000100  loss: 0.0323 (0.0528)  time: 3.7764  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3360/3449]  eta: 0:05:34  lr: 0.000100  loss: 0.0297 (0.0527)  time: 3.7581  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3370/3449]  eta: 0:04:56  lr: 0.000100  loss: 0.0288 (0.0527)  time: 3.7661  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3380/3449]  eta: 0:04:19  lr: 0.000100  loss: 0.0266 (0.0526)  time: 3.7947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3390/3449]  eta: 0:03:41  lr: 0.000100  loss: 0.0272 (0.0525)  time: 3.7524  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3400/3449]  eta: 0:03:04  lr: 0.000100  loss: 0.0277 (0.0525)  time: 3.7048  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3410/3449]  eta: 0:02:26  lr: 0.000100  loss: 0.0300 (0.0524)  time: 3.7514  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3420/3449]  eta: 0:01:48  lr: 0.000100  loss: 0.0321 (0.0524)  time: 3.8062  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3430/3449]  eta: 0:01:11  lr: 0.000100  loss: 0.0331 (0.0523)  time: 3.7794  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3440/3449]  eta: 0:00:33  lr: 0.000100  loss: 0.0320 (0.0523)  time: 3.7202  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0337 (0.0522)  time: 3.7350  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:26] Total time: 3:35:55 (3.7563 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0337 (0.0522)\n",
      "Valid: [epoch:26]  [ 0/14]  eta: 0:04:21  loss: 0.0273 (0.0273)  time: 18.6593  data: 0.4648  max mem: 34968\n",
      "Valid: [epoch:26]  [13/14]  eta: 0:00:18  loss: 0.0260 (0.0262)  time: 18.2545  data: 0.0334  max mem: 34968\n",
      "Valid: [epoch:26] Total time: 0:04:15 (18.2685 s / it)\n",
      "Averaged stats: loss: 0.0260 (0.0262)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_26_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.026%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:27]  [   0/3449]  eta: 4:44:41  lr: 0.000100  loss: 0.0288 (0.0288)  time: 4.9526  data: 1.5208  max mem: 34968\n",
      "Train: [epoch:27]  [  10/3449]  eta: 3:41:21  lr: 0.000100  loss: 0.0357 (0.0377)  time: 3.8619  data: 0.1383  max mem: 34968\n",
      "Train: [epoch:27]  [  20/3449]  eta: 3:38:11  lr: 0.000100  loss: 0.0338 (0.0338)  time: 3.7612  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [  30/3449]  eta: 3:36:03  lr: 0.000100  loss: 0.0324 (0.0341)  time: 3.7528  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [  40/3449]  eta: 3:35:02  lr: 0.000100  loss: 0.0343 (0.0345)  time: 3.7503  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [  50/3449]  eta: 3:33:43  lr: 0.000100  loss: 0.0347 (0.0350)  time: 3.7437  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [  60/3449]  eta: 3:32:47  lr: 0.000100  loss: 0.0328 (0.0346)  time: 3.7316  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [  70/3449]  eta: 3:32:05  lr: 0.000100  loss: 0.0307 (0.0339)  time: 3.7485  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [  80/3449]  eta: 3:31:36  lr: 0.000100  loss: 0.0292 (0.0338)  time: 3.7728  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [  90/3449]  eta: 3:30:49  lr: 0.000100  loss: 0.0306 (0.0336)  time: 3.7660  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 100/3449]  eta: 3:30:00  lr: 0.000100  loss: 0.0299 (0.0334)  time: 3.7363  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 110/3449]  eta: 3:29:01  lr: 0.000100  loss: 0.0296 (0.0332)  time: 3.7109  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 120/3449]  eta: 3:29:03  lr: 0.000100  loss: 0.0309 (0.0331)  time: 3.7972  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 130/3449]  eta: 3:27:52  lr: 0.000100  loss: 0.0303 (0.0330)  time: 3.7683  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 140/3449]  eta: 3:27:35  lr: 0.000100  loss: 0.0315 (0.0330)  time: 3.7407  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 150/3449]  eta: 3:27:05  lr: 0.000100  loss: 0.0308 (0.0329)  time: 3.8230  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 160/3449]  eta: 3:26:21  lr: 0.000100  loss: 0.0362 (0.0336)  time: 3.7660  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 170/3449]  eta: 3:25:42  lr: 0.000100  loss: 0.0370 (0.0341)  time: 3.7455  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 180/3449]  eta: 3:25:08  lr: 0.000100  loss: 0.0330 (0.0338)  time: 3.7718  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 190/3449]  eta: 3:24:32  lr: 0.000100  loss: 0.0306 (0.0340)  time: 3.7805  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 200/3449]  eta: 3:24:05  lr: 0.000100  loss: 0.0335 (0.0341)  time: 3.8033  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 210/3449]  eta: 3:23:19  lr: 0.000100  loss: 0.0333 (0.0341)  time: 3.7738  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 220/3449]  eta: 3:22:31  lr: 0.000100  loss: 0.0300 (0.0339)  time: 3.7062  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 230/3449]  eta: 3:22:02  lr: 0.000100  loss: 0.0296 (0.0339)  time: 3.7590  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 240/3449]  eta: 3:21:25  lr: 0.000100  loss: 0.0330 (0.0339)  time: 3.7959  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 250/3449]  eta: 3:20:43  lr: 0.000100  loss: 0.0330 (0.0338)  time: 3.7501  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 260/3449]  eta: 3:20:27  lr: 0.000100  loss: 0.0311 (0.0337)  time: 3.8390  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 270/3449]  eta: 3:19:48  lr: 0.000100  loss: 0.0307 (0.0336)  time: 3.8536  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 280/3449]  eta: 3:19:11  lr: 0.000100  loss: 0.0304 (0.0337)  time: 3.7671  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 290/3449]  eta: 3:18:27  lr: 0.000100  loss: 0.0298 (0.0336)  time: 3.7447  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 300/3449]  eta: 3:18:03  lr: 0.000100  loss: 0.0286 (0.0335)  time: 3.8093  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 310/3449]  eta: 3:17:23  lr: 0.000100  loss: 0.0288 (0.0334)  time: 3.8258  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 320/3449]  eta: 3:16:36  lr: 0.000100  loss: 0.0290 (0.0332)  time: 3.7124  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 330/3449]  eta: 3:15:53  lr: 0.000100  loss: 0.0309 (0.0333)  time: 3.6977  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 340/3449]  eta: 3:15:28  lr: 0.000100  loss: 0.0308 (0.0332)  time: 3.8118  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 350/3449]  eta: 3:14:52  lr: 0.000100  loss: 0.0318 (0.0334)  time: 3.8479  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 360/3449]  eta: 3:14:06  lr: 0.000100  loss: 0.0329 (0.0333)  time: 3.7316  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 370/3449]  eta: 3:13:19  lr: 0.000100  loss: 0.0310 (0.0334)  time: 3.6657  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 380/3449]  eta: 3:12:35  lr: 0.000100  loss: 0.0302 (0.0333)  time: 3.6755  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 390/3449]  eta: 3:11:48  lr: 0.000100  loss: 0.0289 (0.0332)  time: 3.6674  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 400/3449]  eta: 3:11:10  lr: 0.000100  loss: 0.0282 (0.0331)  time: 3.7005  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 410/3449]  eta: 3:10:30  lr: 0.000100  loss: 0.0278 (0.0330)  time: 3.7418  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 420/3449]  eta: 3:09:45  lr: 0.000100  loss: 0.0321 (0.0330)  time: 3.6946  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 430/3449]  eta: 3:08:58  lr: 0.000100  loss: 0.0383 (0.0334)  time: 3.6414  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:27]  [ 440/3449]  eta: 3:08:08  lr: 0.000100  loss: 0.0499 (0.0338)  time: 3.5979  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 450/3449]  eta: 3:07:25  lr: 0.000100  loss: 0.0513 (0.0342)  time: 3.6236  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 460/3449]  eta: 3:06:43  lr: 0.000100  loss: 0.0461 (0.0344)  time: 3.6733  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 470/3449]  eta: 3:05:56  lr: 0.000100  loss: 0.0404 (0.0345)  time: 3.6421  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 480/3449]  eta: 3:05:11  lr: 0.000100  loss: 0.0402 (0.0346)  time: 3.6134  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 490/3449]  eta: 3:04:31  lr: 0.000100  loss: 0.0383 (0.0347)  time: 3.6595  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 500/3449]  eta: 3:03:45  lr: 0.000100  loss: 0.0330 (0.0347)  time: 3.6456  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 510/3449]  eta: 3:03:12  lr: 0.000100  loss: 0.0340 (0.0349)  time: 3.7011  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 520/3449]  eta: 3:02:28  lr: 0.000100  loss: 0.0434 (0.0353)  time: 3.7238  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 530/3449]  eta: 3:01:46  lr: 0.000100  loss: 0.0434 (0.0354)  time: 3.6430  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 540/3449]  eta: 3:01:02  lr: 0.000100  loss: 0.0385 (0.0355)  time: 3.6339  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 550/3449]  eta: 3:00:20  lr: 0.000100  loss: 0.0366 (0.0355)  time: 3.6276  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 560/3449]  eta: 2:59:44  lr: 0.000100  loss: 0.0366 (0.0356)  time: 3.6951  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 570/3449]  eta: 2:58:59  lr: 0.000100  loss: 0.0355 (0.0355)  time: 3.6748  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 580/3449]  eta: 2:58:18  lr: 0.000100  loss: 0.0300 (0.0355)  time: 3.6173  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 590/3449]  eta: 2:57:37  lr: 0.000100  loss: 0.0308 (0.0354)  time: 3.6503  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 600/3449]  eta: 2:56:48  lr: 0.000100  loss: 0.0318 (0.0354)  time: 3.5734  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 610/3449]  eta: 2:56:00  lr: 0.000100  loss: 0.0329 (0.0354)  time: 3.4877  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 620/3449]  eta: 2:55:20  lr: 0.000100  loss: 0.0300 (0.0353)  time: 3.5710  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 630/3449]  eta: 2:54:39  lr: 0.000100  loss: 0.0281 (0.0352)  time: 3.6452  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 640/3449]  eta: 2:54:05  lr: 0.000100  loss: 0.0279 (0.0351)  time: 3.7099  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 650/3449]  eta: 2:53:23  lr: 0.000100  loss: 0.0303 (0.0351)  time: 3.6982  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 660/3449]  eta: 2:52:42  lr: 0.000100  loss: 0.0311 (0.0350)  time: 3.6250  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 670/3449]  eta: 2:52:03  lr: 0.000100  loss: 0.0321 (0.0350)  time: 3.6478  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 680/3449]  eta: 2:51:22  lr: 0.000100  loss: 0.0297 (0.0349)  time: 3.6389  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 690/3449]  eta: 2:50:41  lr: 0.000100  loss: 0.0276 (0.0348)  time: 3.6228  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 700/3449]  eta: 2:50:03  lr: 0.000100  loss: 0.0300 (0.0349)  time: 3.6492  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 710/3449]  eta: 2:49:24  lr: 0.000100  loss: 0.0356 (0.0348)  time: 3.6769  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 720/3449]  eta: 2:48:45  lr: 0.000100  loss: 0.0352 (0.0349)  time: 3.6632  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 730/3449]  eta: 2:48:11  lr: 0.000100  loss: 0.0356 (0.0349)  time: 3.7219  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 740/3449]  eta: 2:47:30  lr: 0.000100  loss: 0.0311 (0.0348)  time: 3.6997  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 750/3449]  eta: 2:46:50  lr: 0.000100  loss: 0.0306 (0.0349)  time: 3.6170  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 760/3449]  eta: 2:46:13  lr: 0.000100  loss: 0.0368 (0.0350)  time: 3.6715  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 770/3449]  eta: 2:45:31  lr: 0.000100  loss: 0.0350 (0.0350)  time: 3.6328  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 780/3449]  eta: 2:44:47  lr: 0.000100  loss: 0.0367 (0.0351)  time: 3.5420  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 790/3449]  eta: 2:44:09  lr: 0.000100  loss: 0.0427 (0.0352)  time: 3.6004  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 800/3449]  eta: 2:43:29  lr: 0.000100  loss: 0.0381 (0.0352)  time: 3.6401  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 810/3449]  eta: 2:42:53  lr: 0.000100  loss: 0.0412 (0.0354)  time: 3.6648  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 820/3449]  eta: 2:42:14  lr: 0.000100  loss: 0.0379 (0.0354)  time: 3.6894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 830/3449]  eta: 2:41:35  lr: 0.000100  loss: 0.0364 (0.0354)  time: 3.6509  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 840/3449]  eta: 2:41:00  lr: 0.000100  loss: 0.0377 (0.0354)  time: 3.7055  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 850/3449]  eta: 2:40:20  lr: 0.000100  loss: 0.0321 (0.0354)  time: 3.6849  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 860/3449]  eta: 2:39:45  lr: 0.000100  loss: 0.0304 (0.0353)  time: 3.6856  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 870/3449]  eta: 2:39:07  lr: 0.000100  loss: 0.0357 (0.0354)  time: 3.7230  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 880/3449]  eta: 2:38:29  lr: 0.000100  loss: 0.0408 (0.0355)  time: 3.6620  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 890/3449]  eta: 2:37:52  lr: 0.000100  loss: 0.0408 (0.0356)  time: 3.6881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 900/3449]  eta: 2:37:15  lr: 0.000100  loss: 0.0440 (0.0357)  time: 3.7153  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 910/3449]  eta: 2:36:37  lr: 0.000100  loss: 0.0440 (0.0358)  time: 3.6816  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [ 920/3449]  eta: 2:36:01  lr: 0.000100  loss: 0.0366 (0.0358)  time: 3.6983  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 930/3449]  eta: 2:35:22  lr: 0.000100  loss: 0.0319 (0.0358)  time: 3.6798  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 940/3449]  eta: 2:34:44  lr: 0.000100  loss: 0.0293 (0.0357)  time: 3.6508  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 950/3449]  eta: 2:34:05  lr: 0.000100  loss: 0.0285 (0.0356)  time: 3.6350  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 960/3449]  eta: 2:33:26  lr: 0.000100  loss: 0.0285 (0.0356)  time: 3.6081  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 970/3449]  eta: 2:32:43  lr: 0.000100  loss: 0.0305 (0.0355)  time: 3.5524  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 980/3449]  eta: 2:32:01  lr: 0.000100  loss: 0.0292 (0.0354)  time: 3.4859  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [ 990/3449]  eta: 2:31:35  lr: 0.000100  loss: 0.0288 (0.0354)  time: 3.8122  data: 0.3312  max mem: 34968\n",
      "Train: [epoch:27]  [1000/3449]  eta: 2:30:53  lr: 0.000100  loss: 0.0288 (0.0353)  time: 3.8216  data: 0.3312  max mem: 34968\n",
      "Train: [epoch:27]  [1010/3449]  eta: 2:30:11  lr: 0.000100  loss: 0.0288 (0.0353)  time: 3.4917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1020/3449]  eta: 2:29:29  lr: 0.000100  loss: 0.0330 (0.0353)  time: 3.4826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1030/3449]  eta: 2:28:47  lr: 0.000100  loss: 0.0323 (0.0352)  time: 3.4834  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1040/3449]  eta: 2:28:05  lr: 0.000100  loss: 0.0314 (0.0352)  time: 3.4826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1050/3449]  eta: 2:27:24  lr: 0.000100  loss: 0.0292 (0.0351)  time: 3.4812  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1060/3449]  eta: 2:26:44  lr: 0.000100  loss: 0.0278 (0.0351)  time: 3.5114  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1070/3449]  eta: 2:26:03  lr: 0.000100  loss: 0.0286 (0.0350)  time: 3.5338  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1080/3449]  eta: 2:25:22  lr: 0.000100  loss: 0.0294 (0.0350)  time: 3.5017  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1090/3449]  eta: 2:24:41  lr: 0.000100  loss: 0.0317 (0.0350)  time: 3.4794  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:27]  [1100/3449]  eta: 2:24:00  lr: 0.000100  loss: 0.0286 (0.0349)  time: 3.4811  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1110/3449]  eta: 2:23:19  lr: 0.000100  loss: 0.0277 (0.0348)  time: 3.4815  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1120/3449]  eta: 2:22:38  lr: 0.000100  loss: 0.0292 (0.0348)  time: 3.4815  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1130/3449]  eta: 2:21:57  lr: 0.000100  loss: 0.0299 (0.0347)  time: 3.4824  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1140/3449]  eta: 2:21:17  lr: 0.000100  loss: 0.0292 (0.0347)  time: 3.4837  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1150/3449]  eta: 2:20:36  lr: 0.000100  loss: 0.0292 (0.0347)  time: 3.4859  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1160/3449]  eta: 2:19:56  lr: 0.000100  loss: 0.0296 (0.0346)  time: 3.4870  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1170/3449]  eta: 2:19:16  lr: 0.000100  loss: 0.0305 (0.0346)  time: 3.4855  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1180/3449]  eta: 2:18:36  lr: 0.000100  loss: 0.0309 (0.0346)  time: 3.4842  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1190/3449]  eta: 2:17:56  lr: 0.000100  loss: 0.0313 (0.0346)  time: 3.4838  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1200/3449]  eta: 2:17:16  lr: 0.000100  loss: 0.0313 (0.0346)  time: 3.4826  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1210/3449]  eta: 2:16:36  lr: 0.000100  loss: 0.0322 (0.0346)  time: 3.4822  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1220/3449]  eta: 2:15:56  lr: 0.000100  loss: 0.0322 (0.0345)  time: 3.4822  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1230/3449]  eta: 2:15:16  lr: 0.000100  loss: 0.0277 (0.0345)  time: 3.4825  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1240/3449]  eta: 2:14:37  lr: 0.000100  loss: 0.0306 (0.0345)  time: 3.5067  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1250/3449]  eta: 2:14:00  lr: 0.000100  loss: 0.0306 (0.0345)  time: 3.5833  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1260/3449]  eta: 2:13:23  lr: 0.000100  loss: 0.0286 (0.0345)  time: 3.6360  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1270/3449]  eta: 2:12:48  lr: 0.000100  loss: 0.0286 (0.0344)  time: 3.6837  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1280/3449]  eta: 2:12:11  lr: 0.000100  loss: 0.0337 (0.0345)  time: 3.6964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1290/3449]  eta: 2:11:36  lr: 0.000100  loss: 0.0344 (0.0345)  time: 3.6997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1300/3449]  eta: 2:10:59  lr: 0.000100  loss: 0.0329 (0.0345)  time: 3.6824  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1310/3449]  eta: 2:10:23  lr: 0.000100  loss: 0.0349 (0.0345)  time: 3.6517  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1320/3449]  eta: 2:09:46  lr: 0.000100  loss: 0.0369 (0.0346)  time: 3.6528  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1330/3449]  eta: 2:09:10  lr: 0.000100  loss: 0.0376 (0.0346)  time: 3.6694  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1340/3449]  eta: 2:08:34  lr: 0.000100  loss: 0.0381 (0.0347)  time: 3.7030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1350/3449]  eta: 2:07:58  lr: 0.000100  loss: 0.0351 (0.0347)  time: 3.7001  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1360/3449]  eta: 2:07:23  lr: 0.000100  loss: 0.0330 (0.0347)  time: 3.7238  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1370/3449]  eta: 2:06:46  lr: 0.000100  loss: 0.0305 (0.0346)  time: 3.6880  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1380/3449]  eta: 2:06:10  lr: 0.000100  loss: 0.0279 (0.0346)  time: 3.6665  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1390/3449]  eta: 2:05:33  lr: 0.000100  loss: 0.0318 (0.0347)  time: 3.6757  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1400/3449]  eta: 2:04:56  lr: 0.000100  loss: 0.0437 (0.0348)  time: 3.6318  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1410/3449]  eta: 2:04:19  lr: 0.000100  loss: 0.0395 (0.0348)  time: 3.6319  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1420/3449]  eta: 2:03:43  lr: 0.000100  loss: 0.0327 (0.0348)  time: 3.6529  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1430/3449]  eta: 2:03:06  lr: 0.000100  loss: 0.0315 (0.0348)  time: 3.6355  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1440/3449]  eta: 2:02:29  lr: 0.000100  loss: 0.0289 (0.0347)  time: 3.6231  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1450/3449]  eta: 2:01:52  lr: 0.000100  loss: 0.0279 (0.0347)  time: 3.6470  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1460/3449]  eta: 2:01:15  lr: 0.000100  loss: 0.0279 (0.0346)  time: 3.6543  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1470/3449]  eta: 2:00:37  lr: 0.000100  loss: 0.0281 (0.0346)  time: 3.5995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1480/3449]  eta: 2:00:01  lr: 0.000100  loss: 0.0280 (0.0345)  time: 3.6277  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1490/3449]  eta: 1:59:24  lr: 0.000100  loss: 0.0277 (0.0345)  time: 3.6494  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1500/3449]  eta: 1:58:47  lr: 0.000100  loss: 0.0277 (0.0345)  time: 3.6258  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1510/3449]  eta: 1:58:12  lr: 0.000100  loss: 0.0275 (0.0345)  time: 3.6994  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1520/3449]  eta: 1:57:36  lr: 0.000100  loss: 0.0294 (0.0345)  time: 3.7132  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1530/3449]  eta: 1:56:59  lr: 0.000100  loss: 0.0318 (0.0344)  time: 3.6746  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1540/3449]  eta: 1:56:22  lr: 0.000100  loss: 0.0309 (0.0344)  time: 3.6481  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1550/3449]  eta: 1:55:46  lr: 0.000100  loss: 0.0309 (0.0345)  time: 3.6461  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1560/3449]  eta: 1:55:09  lr: 0.000100  loss: 0.0316 (0.0345)  time: 3.6698  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1570/3449]  eta: 1:54:33  lr: 0.000100  loss: 0.0316 (0.0345)  time: 3.6617  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1580/3449]  eta: 1:53:56  lr: 0.000100  loss: 0.0358 (0.0345)  time: 3.6506  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1590/3449]  eta: 1:53:19  lr: 0.000100  loss: 0.0358 (0.0345)  time: 3.6312  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1600/3449]  eta: 1:52:42  lr: 0.000100  loss: 0.0295 (0.0345)  time: 3.6366  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1610/3449]  eta: 1:52:06  lr: 0.000100  loss: 0.0296 (0.0345)  time: 3.6661  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1620/3449]  eta: 1:51:29  lr: 0.000100  loss: 0.0359 (0.0346)  time: 3.6486  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1630/3449]  eta: 1:50:52  lr: 0.000100  loss: 0.0418 (0.0346)  time: 3.6377  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1640/3449]  eta: 1:50:16  lr: 0.000100  loss: 0.0374 (0.0347)  time: 3.6650  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1650/3449]  eta: 1:49:39  lr: 0.000100  loss: 0.0338 (0.0347)  time: 3.6564  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1660/3449]  eta: 1:49:02  lr: 0.000100  loss: 0.0287 (0.0346)  time: 3.6297  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1670/3449]  eta: 1:48:26  lr: 0.000100  loss: 0.0276 (0.0346)  time: 3.6723  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1680/3449]  eta: 1:47:50  lr: 0.000100  loss: 0.0278 (0.0346)  time: 3.7034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1690/3449]  eta: 1:47:13  lr: 0.000100  loss: 0.0299 (0.0345)  time: 3.6573  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1700/3449]  eta: 1:46:38  lr: 0.000100  loss: 0.0298 (0.0345)  time: 3.6855  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1710/3449]  eta: 1:46:01  lr: 0.000100  loss: 0.0300 (0.0345)  time: 3.7180  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1720/3449]  eta: 1:45:25  lr: 0.000100  loss: 0.0312 (0.0345)  time: 3.6896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1730/3449]  eta: 1:44:48  lr: 0.000100  loss: 0.0298 (0.0344)  time: 3.6561  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1740/3449]  eta: 1:44:11  lr: 0.000100  loss: 0.0283 (0.0344)  time: 3.6056  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1750/3449]  eta: 1:43:35  lr: 0.000100  loss: 0.0283 (0.0344)  time: 3.6412  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:27]  [1760/3449]  eta: 1:42:57  lr: 0.000100  loss: 0.0273 (0.0344)  time: 3.6463  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1770/3449]  eta: 1:42:21  lr: 0.000100  loss: 0.0266 (0.0343)  time: 3.6496  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1780/3449]  eta: 1:41:44  lr: 0.000100  loss: 0.0290 (0.0343)  time: 3.6434  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1790/3449]  eta: 1:41:08  lr: 0.000100  loss: 0.0305 (0.0343)  time: 3.6618  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1800/3449]  eta: 1:40:31  lr: 0.000100  loss: 0.0285 (0.0343)  time: 3.6610  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1810/3449]  eta: 1:39:54  lr: 0.000100  loss: 0.0266 (0.0342)  time: 3.6248  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1820/3449]  eta: 1:39:18  lr: 0.000100  loss: 0.0266 (0.0342)  time: 3.6680  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1830/3449]  eta: 1:38:41  lr: 0.000100  loss: 0.0276 (0.0341)  time: 3.6664  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1840/3449]  eta: 1:38:05  lr: 0.000100  loss: 0.0291 (0.0341)  time: 3.6843  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1850/3449]  eta: 1:37:28  lr: 0.000100  loss: 0.0291 (0.0341)  time: 3.6671  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1860/3449]  eta: 1:36:51  lr: 0.000100  loss: 0.0290 (0.0341)  time: 3.6195  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1870/3449]  eta: 1:36:15  lr: 0.000100  loss: 0.0287 (0.0340)  time: 3.6275  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1880/3449]  eta: 1:35:38  lr: 0.000100  loss: 0.0274 (0.0340)  time: 3.6584  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1890/3449]  eta: 1:35:02  lr: 0.000100  loss: 0.0271 (0.0340)  time: 3.6670  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1900/3449]  eta: 1:34:25  lr: 0.000100  loss: 0.0309 (0.0340)  time: 3.6590  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1910/3449]  eta: 1:33:49  lr: 0.000100  loss: 0.0310 (0.0340)  time: 3.6890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1920/3449]  eta: 1:33:12  lr: 0.000100  loss: 0.0276 (0.0340)  time: 3.6800  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1930/3449]  eta: 1:32:35  lr: 0.000100  loss: 0.0281 (0.0340)  time: 3.6128  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1940/3449]  eta: 1:31:59  lr: 0.000100  loss: 0.0279 (0.0340)  time: 3.6196  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1950/3449]  eta: 1:31:22  lr: 0.000100  loss: 0.0277 (0.0339)  time: 3.6468  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [1960/3449]  eta: 1:30:46  lr: 0.000100  loss: 0.0290 (0.0339)  time: 3.6620  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1970/3449]  eta: 1:30:09  lr: 0.000100  loss: 0.0298 (0.0339)  time: 3.6638  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1980/3449]  eta: 1:29:32  lr: 0.000100  loss: 0.0287 (0.0339)  time: 3.6346  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [1990/3449]  eta: 1:28:57  lr: 0.000100  loss: 0.0287 (0.0339)  time: 3.7129  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2000/3449]  eta: 1:28:20  lr: 0.000100  loss: 0.0355 (0.0339)  time: 3.7064  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2010/3449]  eta: 1:27:43  lr: 0.000100  loss: 0.0440 (0.0340)  time: 3.6248  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2020/3449]  eta: 1:27:06  lr: 0.000100  loss: 0.0440 (0.0340)  time: 3.6013  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2030/3449]  eta: 1:26:30  lr: 0.000100  loss: 0.0397 (0.0340)  time: 3.6564  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2040/3449]  eta: 1:25:53  lr: 0.000100  loss: 0.0324 (0.0340)  time: 3.6689  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2050/3449]  eta: 1:25:16  lr: 0.000100  loss: 0.0305 (0.0340)  time: 3.6520  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2060/3449]  eta: 1:24:41  lr: 0.000100  loss: 0.0337 (0.0340)  time: 3.7451  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2070/3449]  eta: 1:24:04  lr: 0.000100  loss: 0.0351 (0.0341)  time: 3.6982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2080/3449]  eta: 1:23:28  lr: 0.000100  loss: 0.0337 (0.0340)  time: 3.6587  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2090/3449]  eta: 1:22:51  lr: 0.000100  loss: 0.0318 (0.0340)  time: 3.6995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2100/3449]  eta: 1:22:14  lr: 0.000100  loss: 0.0334 (0.0340)  time: 3.6566  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2110/3449]  eta: 1:21:38  lr: 0.000100  loss: 0.0381 (0.0340)  time: 3.6182  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2120/3449]  eta: 1:21:01  lr: 0.000100  loss: 0.0385 (0.0341)  time: 3.6254  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2130/3449]  eta: 1:20:24  lr: 0.000100  loss: 0.0326 (0.0340)  time: 3.6152  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2140/3449]  eta: 1:19:47  lr: 0.000100  loss: 0.0276 (0.0340)  time: 3.5711  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2150/3449]  eta: 1:19:09  lr: 0.000100  loss: 0.0287 (0.0340)  time: 3.5121  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2160/3449]  eta: 1:18:31  lr: 0.000100  loss: 0.0286 (0.0340)  time: 3.4843  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2170/3449]  eta: 1:17:54  lr: 0.000100  loss: 0.0303 (0.0340)  time: 3.4853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2180/3449]  eta: 1:17:16  lr: 0.000100  loss: 0.0302 (0.0340)  time: 3.4856  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2190/3449]  eta: 1:16:39  lr: 0.000100  loss: 0.0311 (0.0340)  time: 3.4856  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2200/3449]  eta: 1:16:01  lr: 0.000100  loss: 0.0322 (0.0340)  time: 3.4867  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2210/3449]  eta: 1:15:24  lr: 0.000100  loss: 0.0317 (0.0340)  time: 3.4882  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2220/3449]  eta: 1:14:46  lr: 0.000100  loss: 0.0285 (0.0340)  time: 3.4907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2230/3449]  eta: 1:14:10  lr: 0.000100  loss: 0.0269 (0.0339)  time: 3.5486  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2240/3449]  eta: 1:13:33  lr: 0.000100  loss: 0.0304 (0.0339)  time: 3.6087  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2250/3449]  eta: 1:12:57  lr: 0.000100  loss: 0.0309 (0.0339)  time: 3.6507  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2260/3449]  eta: 1:12:20  lr: 0.000100  loss: 0.0254 (0.0339)  time: 3.6548  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2270/3449]  eta: 1:11:44  lr: 0.000100  loss: 0.0301 (0.0339)  time: 3.6658  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2280/3449]  eta: 1:11:07  lr: 0.000100  loss: 0.0358 (0.0339)  time: 3.6930  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2290/3449]  eta: 1:10:31  lr: 0.000100  loss: 0.0368 (0.0339)  time: 3.6573  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2300/3449]  eta: 1:09:54  lr: 0.000100  loss: 0.0368 (0.0339)  time: 3.6322  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2310/3449]  eta: 1:09:18  lr: 0.000100  loss: 0.0333 (0.0339)  time: 3.6391  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2320/3449]  eta: 1:08:41  lr: 0.000100  loss: 0.0322 (0.0339)  time: 3.6362  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2330/3449]  eta: 1:08:05  lr: 0.000100  loss: 0.0319 (0.0339)  time: 3.6594  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2340/3449]  eta: 1:07:28  lr: 0.000100  loss: 0.0301 (0.0339)  time: 3.6701  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2350/3449]  eta: 1:06:52  lr: 0.000100  loss: 0.0303 (0.0339)  time: 3.6614  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2360/3449]  eta: 1:06:15  lr: 0.000100  loss: 0.0313 (0.0339)  time: 3.6514  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2370/3449]  eta: 1:05:39  lr: 0.000100  loss: 0.0303 (0.0339)  time: 3.6342  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2380/3449]  eta: 1:05:02  lr: 0.000100  loss: 0.0300 (0.0339)  time: 3.6567  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2390/3449]  eta: 1:04:26  lr: 0.000100  loss: 0.0296 (0.0339)  time: 3.6637  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2400/3449]  eta: 1:03:49  lr: 0.000100  loss: 0.0276 (0.0338)  time: 3.6595  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2410/3449]  eta: 1:03:13  lr: 0.000100  loss: 0.0279 (0.0338)  time: 3.7146  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:27]  [2420/3449]  eta: 1:02:37  lr: 0.000100  loss: 0.0283 (0.0338)  time: 3.7136  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2430/3449]  eta: 1:02:00  lr: 0.000100  loss: 0.0326 (0.0338)  time: 3.6259  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2440/3449]  eta: 1:01:24  lr: 0.000100  loss: 0.0380 (0.0339)  time: 3.6173  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2450/3449]  eta: 1:00:47  lr: 0.000100  loss: 0.0380 (0.0339)  time: 3.6514  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2460/3449]  eta: 1:00:11  lr: 0.000100  loss: 0.0325 (0.0339)  time: 3.7269  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2470/3449]  eta: 0:59:35  lr: 0.000100  loss: 0.0290 (0.0339)  time: 3.7139  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2480/3449]  eta: 0:58:58  lr: 0.000100  loss: 0.0290 (0.0339)  time: 3.6612  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2490/3449]  eta: 0:58:22  lr: 0.000100  loss: 0.0291 (0.0339)  time: 3.6602  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2500/3449]  eta: 0:57:45  lr: 0.000100  loss: 0.0286 (0.0338)  time: 3.6308  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2510/3449]  eta: 0:57:09  lr: 0.000100  loss: 0.0285 (0.0338)  time: 3.6432  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2520/3449]  eta: 0:56:33  lr: 0.000100  loss: 0.0292 (0.0338)  time: 3.7431  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2530/3449]  eta: 0:55:56  lr: 0.000100  loss: 0.0348 (0.0338)  time: 3.7383  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2540/3449]  eta: 0:55:19  lr: 0.000100  loss: 0.0351 (0.0338)  time: 3.6387  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2550/3449]  eta: 0:54:43  lr: 0.000100  loss: 0.0337 (0.0338)  time: 3.6746  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2560/3449]  eta: 0:54:07  lr: 0.000100  loss: 0.0338 (0.0338)  time: 3.6721  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2570/3449]  eta: 0:53:30  lr: 0.000100  loss: 0.0322 (0.0338)  time: 3.6302  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2580/3449]  eta: 0:52:54  lr: 0.000100  loss: 0.0290 (0.0338)  time: 3.6551  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2590/3449]  eta: 0:52:17  lr: 0.000100  loss: 0.0272 (0.0338)  time: 3.6565  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2600/3449]  eta: 0:51:40  lr: 0.000100  loss: 0.0268 (0.0338)  time: 3.6346  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2610/3449]  eta: 0:51:04  lr: 0.000100  loss: 0.0290 (0.0338)  time: 3.6539  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2620/3449]  eta: 0:50:27  lr: 0.000100  loss: 0.0290 (0.0338)  time: 3.6389  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2630/3449]  eta: 0:49:51  lr: 0.000100  loss: 0.0272 (0.0337)  time: 3.6248  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2640/3449]  eta: 0:49:14  lr: 0.000100  loss: 0.0265 (0.0337)  time: 3.6842  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2650/3449]  eta: 0:48:38  lr: 0.000100  loss: 0.0272 (0.0337)  time: 3.6908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2660/3449]  eta: 0:48:01  lr: 0.000100  loss: 0.0282 (0.0337)  time: 3.6525  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2670/3449]  eta: 0:47:25  lr: 0.000100  loss: 0.0275 (0.0336)  time: 3.6946  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2680/3449]  eta: 0:46:49  lr: 0.000100  loss: 0.0283 (0.0336)  time: 3.7047  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2690/3449]  eta: 0:46:12  lr: 0.000100  loss: 0.0288 (0.0336)  time: 3.6034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2700/3449]  eta: 0:45:35  lr: 0.000100  loss: 0.0300 (0.0336)  time: 3.5175  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2710/3449]  eta: 0:44:58  lr: 0.000100  loss: 0.0299 (0.0336)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2720/3449]  eta: 0:44:21  lr: 0.000100  loss: 0.0291 (0.0335)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2730/3449]  eta: 0:43:44  lr: 0.000100  loss: 0.0289 (0.0335)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2740/3449]  eta: 0:43:07  lr: 0.000100  loss: 0.0284 (0.0335)  time: 3.4935  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2750/3449]  eta: 0:42:30  lr: 0.000100  loss: 0.0283 (0.0335)  time: 3.4938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2760/3449]  eta: 0:41:53  lr: 0.000100  loss: 0.0265 (0.0335)  time: 3.4957  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2770/3449]  eta: 0:41:16  lr: 0.000100  loss: 0.0269 (0.0335)  time: 3.4994  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2780/3449]  eta: 0:40:40  lr: 0.000100  loss: 0.0281 (0.0335)  time: 3.5010  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2790/3449]  eta: 0:40:03  lr: 0.000100  loss: 0.0255 (0.0334)  time: 3.4988  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [2800/3449]  eta: 0:39:26  lr: 0.000100  loss: 0.0245 (0.0334)  time: 3.4978  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2810/3449]  eta: 0:38:49  lr: 0.000100  loss: 0.0281 (0.0334)  time: 3.4983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2820/3449]  eta: 0:38:12  lr: 0.000100  loss: 0.0304 (0.0334)  time: 3.4974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2830/3449]  eta: 0:37:36  lr: 0.000100  loss: 0.0315 (0.0334)  time: 3.4975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2840/3449]  eta: 0:36:59  lr: 0.000100  loss: 0.0267 (0.0334)  time: 3.4978  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2850/3449]  eta: 0:36:22  lr: 0.000100  loss: 0.0283 (0.0333)  time: 3.4979  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2860/3449]  eta: 0:35:45  lr: 0.000100  loss: 0.0287 (0.0333)  time: 3.4982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2870/3449]  eta: 0:35:09  lr: 0.000100  loss: 0.0269 (0.0333)  time: 3.4982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2880/3449]  eta: 0:34:32  lr: 0.000100  loss: 0.0269 (0.0333)  time: 3.4988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2890/3449]  eta: 0:33:55  lr: 0.000100  loss: 0.0263 (0.0333)  time: 3.5005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2900/3449]  eta: 0:33:19  lr: 0.000100  loss: 0.0249 (0.0332)  time: 3.5016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2910/3449]  eta: 0:32:42  lr: 0.000100  loss: 0.0255 (0.0332)  time: 3.5021  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2920/3449]  eta: 0:32:05  lr: 0.000100  loss: 0.0284 (0.0332)  time: 3.5028  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2930/3449]  eta: 0:31:29  lr: 0.000100  loss: 0.0271 (0.0332)  time: 3.5040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2940/3449]  eta: 0:30:52  lr: 0.000100  loss: 0.0288 (0.0332)  time: 3.5040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2950/3449]  eta: 0:30:15  lr: 0.000100  loss: 0.0281 (0.0331)  time: 3.5021  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2960/3449]  eta: 0:29:39  lr: 0.000100  loss: 0.0285 (0.0331)  time: 3.5020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2970/3449]  eta: 0:29:02  lr: 0.000100  loss: 0.0312 (0.0331)  time: 3.5036  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2980/3449]  eta: 0:28:26  lr: 0.000100  loss: 0.0298 (0.0331)  time: 3.5032  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [2990/3449]  eta: 0:27:49  lr: 0.000100  loss: 0.0281 (0.0331)  time: 3.5018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3000/3449]  eta: 0:27:12  lr: 0.000100  loss: 0.0283 (0.0331)  time: 3.5019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3010/3449]  eta: 0:26:36  lr: 0.000100  loss: 0.0309 (0.0331)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3020/3449]  eta: 0:25:59  lr: 0.000100  loss: 0.0300 (0.0331)  time: 3.5002  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3030/3449]  eta: 0:25:23  lr: 0.000100  loss: 0.0289 (0.0331)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3040/3449]  eta: 0:24:46  lr: 0.000100  loss: 0.0289 (0.0331)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3050/3449]  eta: 0:24:10  lr: 0.000100  loss: 0.0296 (0.0331)  time: 3.4987  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3060/3449]  eta: 0:23:33  lr: 0.000100  loss: 0.0287 (0.0331)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3070/3449]  eta: 0:22:57  lr: 0.000100  loss: 0.0272 (0.0331)  time: 3.4978  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:27]  [3080/3449]  eta: 0:22:20  lr: 0.000100  loss: 0.0296 (0.0331)  time: 3.4977  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [3090/3449]  eta: 0:21:44  lr: 0.000100  loss: 0.0303 (0.0331)  time: 3.4990  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [3100/3449]  eta: 0:21:07  lr: 0.000100  loss: 0.0306 (0.0331)  time: 3.4994  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [3110/3449]  eta: 0:20:31  lr: 0.000100  loss: 0.0312 (0.0331)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3120/3449]  eta: 0:19:54  lr: 0.000100  loss: 0.0331 (0.0331)  time: 3.4989  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3130/3449]  eta: 0:19:18  lr: 0.000100  loss: 0.0332 (0.0331)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3140/3449]  eta: 0:18:41  lr: 0.000100  loss: 0.0300 (0.0331)  time: 3.5016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3150/3449]  eta: 0:18:05  lr: 0.000100  loss: 0.0273 (0.0331)  time: 3.4997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3160/3449]  eta: 0:17:28  lr: 0.000100  loss: 0.0297 (0.0331)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3170/3449]  eta: 0:16:52  lr: 0.000100  loss: 0.0301 (0.0331)  time: 3.4980  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3180/3449]  eta: 0:16:16  lr: 0.000100  loss: 0.0286 (0.0331)  time: 3.4981  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [3190/3449]  eta: 0:15:39  lr: 0.000100  loss: 0.0287 (0.0331)  time: 3.4979  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3200/3449]  eta: 0:15:03  lr: 0.000100  loss: 0.0313 (0.0331)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3210/3449]  eta: 0:14:27  lr: 0.000100  loss: 0.0334 (0.0331)  time: 3.4980  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3220/3449]  eta: 0:13:50  lr: 0.000100  loss: 0.0304 (0.0331)  time: 3.4951  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3230/3449]  eta: 0:13:14  lr: 0.000100  loss: 0.0289 (0.0330)  time: 3.4948  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3240/3449]  eta: 0:12:37  lr: 0.000100  loss: 0.0269 (0.0330)  time: 3.4959  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3250/3449]  eta: 0:12:01  lr: 0.000100  loss: 0.0307 (0.0330)  time: 3.4971  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3260/3449]  eta: 0:11:25  lr: 0.000100  loss: 0.0295 (0.0330)  time: 3.4978  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [3270/3449]  eta: 0:10:48  lr: 0.000100  loss: 0.0296 (0.0330)  time: 3.4974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3280/3449]  eta: 0:10:12  lr: 0.000100  loss: 0.0286 (0.0330)  time: 3.4955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3290/3449]  eta: 0:09:36  lr: 0.000100  loss: 0.0274 (0.0330)  time: 3.4944  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3300/3449]  eta: 0:08:59  lr: 0.000100  loss: 0.0271 (0.0330)  time: 3.4949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3310/3449]  eta: 0:08:23  lr: 0.000100  loss: 0.0270 (0.0330)  time: 3.4949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3320/3449]  eta: 0:07:47  lr: 0.000100  loss: 0.0287 (0.0330)  time: 3.4958  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [3330/3449]  eta: 0:07:11  lr: 0.000100  loss: 0.0278 (0.0329)  time: 3.4975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3340/3449]  eta: 0:06:34  lr: 0.000100  loss: 0.0274 (0.0329)  time: 3.4966  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3350/3449]  eta: 0:05:58  lr: 0.000100  loss: 0.0282 (0.0329)  time: 3.4961  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [3360/3449]  eta: 0:05:22  lr: 0.000100  loss: 0.0262 (0.0329)  time: 3.4976  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [3370/3449]  eta: 0:04:46  lr: 0.000100  loss: 0.0270 (0.0329)  time: 3.4973  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3380/3449]  eta: 0:04:09  lr: 0.000100  loss: 0.0281 (0.0329)  time: 3.4980  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3390/3449]  eta: 0:03:33  lr: 0.000100  loss: 0.0295 (0.0329)  time: 3.5005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3400/3449]  eta: 0:02:57  lr: 0.000100  loss: 0.0320 (0.0329)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3410/3449]  eta: 0:02:21  lr: 0.000100  loss: 0.0340 (0.0329)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3420/3449]  eta: 0:01:44  lr: 0.000100  loss: 0.0292 (0.0329)  time: 3.4988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3430/3449]  eta: 0:01:08  lr: 0.000100  loss: 0.0283 (0.0329)  time: 3.4981  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:27]  [3440/3449]  eta: 0:00:32  lr: 0.000100  loss: 0.0274 (0.0329)  time: 3.4980  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0274 (0.0329)  time: 3.4983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:27] Total time: 3:28:01 (3.6188 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0274 (0.0329)\n",
      "Valid: [epoch:27]  [ 0/14]  eta: 0:04:27  loss: 0.0226 (0.0226)  time: 19.1263  data: 0.8704  max mem: 34968\n",
      "Valid: [epoch:27]  [13/14]  eta: 0:00:18  loss: 0.0251 (0.0254)  time: 18.3299  data: 0.0624  max mem: 34968\n",
      "Valid: [epoch:27] Total time: 0:04:16 (18.3456 s / it)\n",
      "Averaged stats: loss: 0.0251 (0.0254)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_27_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.025%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:28]  [   0/3449]  eta: 4:48:33  lr: 0.000100  loss: 0.0327 (0.0327)  time: 5.0199  data: 1.5427  max mem: 34968\n",
      "Train: [epoch:28]  [  10/3449]  eta: 3:28:13  lr: 0.000100  loss: 0.0292 (0.0308)  time: 3.6328  data: 0.1404  max mem: 34968\n",
      "Train: [epoch:28]  [  20/3449]  eta: 3:23:53  lr: 0.000100  loss: 0.0262 (0.0281)  time: 3.4952  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [  30/3449]  eta: 3:21:59  lr: 0.000100  loss: 0.0256 (0.0290)  time: 3.4962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [  40/3449]  eta: 3:20:45  lr: 0.000100  loss: 0.0299 (0.0291)  time: 3.4974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [  50/3449]  eta: 3:19:46  lr: 0.000100  loss: 0.0287 (0.0292)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [  60/3449]  eta: 3:18:56  lr: 0.000100  loss: 0.0281 (0.0297)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [  70/3449]  eta: 3:18:10  lr: 0.000100  loss: 0.0282 (0.0294)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [  80/3449]  eta: 3:17:27  lr: 0.000100  loss: 0.0276 (0.0289)  time: 3.4995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [  90/3449]  eta: 3:16:46  lr: 0.000100  loss: 0.0282 (0.0296)  time: 3.5010  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 100/3449]  eta: 3:16:06  lr: 0.000100  loss: 0.0310 (0.0297)  time: 3.5016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 110/3449]  eta: 3:15:27  lr: 0.000100  loss: 0.0303 (0.0300)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 120/3449]  eta: 3:14:48  lr: 0.000100  loss: 0.0303 (0.0303)  time: 3.4998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 130/3449]  eta: 3:14:11  lr: 0.000100  loss: 0.0291 (0.0301)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 140/3449]  eta: 3:13:34  lr: 0.000100  loss: 0.0296 (0.0302)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 150/3449]  eta: 3:12:57  lr: 0.000100  loss: 0.0296 (0.0301)  time: 3.5020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 160/3449]  eta: 3:12:20  lr: 0.000100  loss: 0.0295 (0.0306)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 170/3449]  eta: 3:11:43  lr: 0.000100  loss: 0.0286 (0.0306)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 180/3449]  eta: 3:11:06  lr: 0.000100  loss: 0.0282 (0.0305)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 190/3449]  eta: 3:10:30  lr: 0.000100  loss: 0.0303 (0.0306)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 200/3449]  eta: 3:09:53  lr: 0.000100  loss: 0.0329 (0.0311)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 210/3449]  eta: 3:09:17  lr: 0.000100  loss: 0.0336 (0.0314)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 220/3449]  eta: 3:08:41  lr: 0.000100  loss: 0.0322 (0.0314)  time: 3.4986  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:28]  [ 230/3449]  eta: 3:08:05  lr: 0.000100  loss: 0.0335 (0.0318)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 240/3449]  eta: 3:07:29  lr: 0.000100  loss: 0.0419 (0.0325)  time: 3.5000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 250/3449]  eta: 3:06:53  lr: 0.000100  loss: 0.0490 (0.0331)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 260/3449]  eta: 3:06:17  lr: 0.000100  loss: 0.0466 (0.0335)  time: 3.4989  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 270/3449]  eta: 3:05:42  lr: 0.000100  loss: 0.0427 (0.0337)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 280/3449]  eta: 3:05:07  lr: 0.000100  loss: 0.0348 (0.0337)  time: 3.5019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 290/3449]  eta: 3:04:31  lr: 0.000100  loss: 0.0308 (0.0337)  time: 3.5020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 300/3449]  eta: 3:03:56  lr: 0.000100  loss: 0.0306 (0.0336)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 310/3449]  eta: 3:03:21  lr: 0.000100  loss: 0.0321 (0.0336)  time: 3.5023  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 320/3449]  eta: 3:02:45  lr: 0.000100  loss: 0.0326 (0.0336)  time: 3.5031  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 330/3449]  eta: 3:02:10  lr: 0.000100  loss: 0.0321 (0.0337)  time: 3.5023  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 340/3449]  eta: 3:01:35  lr: 0.000100  loss: 0.0283 (0.0335)  time: 3.5015  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [ 350/3449]  eta: 3:01:00  lr: 0.000100  loss: 0.0282 (0.0334)  time: 3.5025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 360/3449]  eta: 3:00:24  lr: 0.000100  loss: 0.0274 (0.0333)  time: 3.5037  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 370/3449]  eta: 2:59:49  lr: 0.000100  loss: 0.0301 (0.0333)  time: 3.5045  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 380/3449]  eta: 2:59:14  lr: 0.000100  loss: 0.0313 (0.0334)  time: 3.5046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 390/3449]  eta: 2:58:39  lr: 0.000100  loss: 0.0304 (0.0335)  time: 3.5033  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 400/3449]  eta: 2:58:04  lr: 0.000100  loss: 0.0291 (0.0333)  time: 3.5027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 410/3449]  eta: 2:57:29  lr: 0.000100  loss: 0.0299 (0.0332)  time: 3.5040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 420/3449]  eta: 2:56:54  lr: 0.000100  loss: 0.0307 (0.0331)  time: 3.5043  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 430/3449]  eta: 2:56:19  lr: 0.000100  loss: 0.0305 (0.0331)  time: 3.5041  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [ 440/3449]  eta: 2:55:44  lr: 0.000100  loss: 0.0283 (0.0329)  time: 3.5038  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 450/3449]  eta: 2:55:09  lr: 0.000100  loss: 0.0283 (0.0329)  time: 3.5028  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 460/3449]  eta: 2:54:34  lr: 0.000100  loss: 0.0280 (0.0328)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 470/3449]  eta: 2:53:58  lr: 0.000100  loss: 0.0292 (0.0329)  time: 3.5031  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 480/3449]  eta: 2:53:23  lr: 0.000100  loss: 0.0311 (0.0328)  time: 3.5024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 490/3449]  eta: 2:52:48  lr: 0.000100  loss: 0.0309 (0.0328)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 500/3449]  eta: 2:52:13  lr: 0.000100  loss: 0.0297 (0.0327)  time: 3.5034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 510/3449]  eta: 2:51:38  lr: 0.000100  loss: 0.0297 (0.0328)  time: 3.5035  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 520/3449]  eta: 2:51:03  lr: 0.000100  loss: 0.0318 (0.0328)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 530/3449]  eta: 2:50:27  lr: 0.000100  loss: 0.0285 (0.0327)  time: 3.5007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 540/3449]  eta: 2:49:52  lr: 0.000100  loss: 0.0274 (0.0327)  time: 3.5012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 550/3449]  eta: 2:49:17  lr: 0.000100  loss: 0.0271 (0.0326)  time: 3.5018  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [ 560/3449]  eta: 2:48:42  lr: 0.000100  loss: 0.0300 (0.0326)  time: 3.5017  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [ 570/3449]  eta: 2:48:07  lr: 0.000100  loss: 0.0297 (0.0325)  time: 3.5012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 580/3449]  eta: 2:47:32  lr: 0.000100  loss: 0.0295 (0.0325)  time: 3.4998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 590/3449]  eta: 2:46:56  lr: 0.000100  loss: 0.0291 (0.0325)  time: 3.4994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 600/3449]  eta: 2:46:21  lr: 0.000100  loss: 0.0295 (0.0325)  time: 3.4995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 610/3449]  eta: 2:45:46  lr: 0.000100  loss: 0.0295 (0.0325)  time: 3.4998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 620/3449]  eta: 2:45:11  lr: 0.000100  loss: 0.0300 (0.0325)  time: 3.5005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 630/3449]  eta: 2:44:36  lr: 0.000100  loss: 0.0294 (0.0324)  time: 3.5007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 640/3449]  eta: 2:44:00  lr: 0.000100  loss: 0.0276 (0.0324)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 650/3449]  eta: 2:43:25  lr: 0.000100  loss: 0.0306 (0.0324)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 660/3449]  eta: 2:42:50  lr: 0.000100  loss: 0.0295 (0.0324)  time: 3.5010  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 670/3449]  eta: 2:42:15  lr: 0.000100  loss: 0.0289 (0.0323)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 680/3449]  eta: 2:41:40  lr: 0.000100  loss: 0.0303 (0.0323)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 690/3449]  eta: 2:41:05  lr: 0.000100  loss: 0.0293 (0.0324)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 700/3449]  eta: 2:40:29  lr: 0.000100  loss: 0.0275 (0.0323)  time: 3.4999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 710/3449]  eta: 2:39:54  lr: 0.000100  loss: 0.0287 (0.0323)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 720/3449]  eta: 2:39:19  lr: 0.000100  loss: 0.0320 (0.0323)  time: 3.5024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 730/3449]  eta: 2:38:44  lr: 0.000100  loss: 0.0303 (0.0322)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 740/3449]  eta: 2:38:09  lr: 0.000100  loss: 0.0278 (0.0322)  time: 3.5010  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 750/3449]  eta: 2:37:34  lr: 0.000100  loss: 0.0283 (0.0322)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 760/3449]  eta: 2:36:59  lr: 0.000100  loss: 0.0290 (0.0321)  time: 3.5004  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 770/3449]  eta: 2:36:24  lr: 0.000100  loss: 0.0290 (0.0321)  time: 3.4989  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 780/3449]  eta: 2:35:49  lr: 0.000100  loss: 0.0269 (0.0320)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 790/3449]  eta: 2:35:14  lr: 0.000100  loss: 0.0281 (0.0320)  time: 3.5010  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 800/3449]  eta: 2:34:38  lr: 0.000100  loss: 0.0284 (0.0320)  time: 3.5004  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 810/3449]  eta: 2:34:03  lr: 0.000100  loss: 0.0262 (0.0319)  time: 3.4997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 820/3449]  eta: 2:33:28  lr: 0.000100  loss: 0.0246 (0.0318)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 830/3449]  eta: 2:32:53  lr: 0.000100  loss: 0.0289 (0.0319)  time: 3.5034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 840/3449]  eta: 2:32:18  lr: 0.000100  loss: 0.0289 (0.0318)  time: 3.5031  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 850/3449]  eta: 2:31:43  lr: 0.000100  loss: 0.0281 (0.0319)  time: 3.5025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 860/3449]  eta: 2:31:08  lr: 0.000100  loss: 0.0297 (0.0319)  time: 3.5036  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 870/3449]  eta: 2:30:33  lr: 0.000100  loss: 0.0331 (0.0319)  time: 3.5052  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 880/3449]  eta: 2:29:58  lr: 0.000100  loss: 0.0341 (0.0320)  time: 3.5052  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:28]  [ 890/3449]  eta: 2:29:23  lr: 0.000100  loss: 0.0357 (0.0321)  time: 3.5050  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 900/3449]  eta: 2:28:48  lr: 0.000100  loss: 0.0331 (0.0321)  time: 3.5049  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 910/3449]  eta: 2:28:13  lr: 0.000100  loss: 0.0338 (0.0321)  time: 3.5052  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 920/3449]  eta: 2:27:38  lr: 0.000100  loss: 0.0344 (0.0322)  time: 3.5059  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [ 930/3449]  eta: 2:27:04  lr: 0.000100  loss: 0.0334 (0.0322)  time: 3.5057  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [ 940/3449]  eta: 2:26:29  lr: 0.000100  loss: 0.0280 (0.0321)  time: 3.5047  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 950/3449]  eta: 2:25:53  lr: 0.000100  loss: 0.0304 (0.0322)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 960/3449]  eta: 2:25:18  lr: 0.000100  loss: 0.0309 (0.0321)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 970/3449]  eta: 2:24:43  lr: 0.000100  loss: 0.0285 (0.0321)  time: 3.5033  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 980/3449]  eta: 2:24:08  lr: 0.000100  loss: 0.0305 (0.0321)  time: 3.5021  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [ 990/3449]  eta: 2:23:33  lr: 0.000100  loss: 0.0294 (0.0321)  time: 3.5012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1000/3449]  eta: 2:22:58  lr: 0.000100  loss: 0.0272 (0.0320)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1010/3449]  eta: 2:22:23  lr: 0.000100  loss: 0.0280 (0.0320)  time: 3.5030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1020/3449]  eta: 2:21:48  lr: 0.000100  loss: 0.0304 (0.0320)  time: 3.5041  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1030/3449]  eta: 2:21:13  lr: 0.000100  loss: 0.0304 (0.0320)  time: 3.5036  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1040/3449]  eta: 2:20:38  lr: 0.000100  loss: 0.0283 (0.0319)  time: 3.5021  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1050/3449]  eta: 2:20:03  lr: 0.000100  loss: 0.0275 (0.0319)  time: 3.5004  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1060/3449]  eta: 2:19:28  lr: 0.000100  loss: 0.0275 (0.0319)  time: 3.5007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1070/3449]  eta: 2:18:53  lr: 0.000100  loss: 0.0311 (0.0319)  time: 3.5024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1080/3449]  eta: 2:18:18  lr: 0.000100  loss: 0.0304 (0.0319)  time: 3.5030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1090/3449]  eta: 2:17:43  lr: 0.000100  loss: 0.0298 (0.0319)  time: 3.5019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1100/3449]  eta: 2:17:08  lr: 0.000100  loss: 0.0305 (0.0319)  time: 3.5028  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1110/3449]  eta: 2:16:33  lr: 0.000100  loss: 0.0304 (0.0319)  time: 3.5037  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1120/3449]  eta: 2:15:58  lr: 0.000100  loss: 0.0288 (0.0318)  time: 3.5045  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1130/3449]  eta: 2:15:23  lr: 0.000100  loss: 0.0314 (0.0318)  time: 3.5062  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1140/3449]  eta: 2:14:48  lr: 0.000100  loss: 0.0288 (0.0318)  time: 3.5069  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1150/3449]  eta: 2:14:13  lr: 0.000100  loss: 0.0288 (0.0318)  time: 3.5076  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1160/3449]  eta: 2:13:38  lr: 0.000100  loss: 0.0292 (0.0318)  time: 3.5075  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1170/3449]  eta: 2:13:03  lr: 0.000100  loss: 0.0306 (0.0318)  time: 3.5072  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1180/3449]  eta: 2:12:28  lr: 0.000100  loss: 0.0334 (0.0318)  time: 3.5076  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1190/3449]  eta: 2:11:53  lr: 0.000100  loss: 0.0325 (0.0318)  time: 3.5074  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1200/3449]  eta: 2:11:18  lr: 0.000100  loss: 0.0287 (0.0318)  time: 3.5061  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1210/3449]  eta: 2:10:43  lr: 0.000100  loss: 0.0287 (0.0318)  time: 3.5063  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1220/3449]  eta: 2:10:08  lr: 0.000100  loss: 0.0285 (0.0318)  time: 3.5070  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1230/3449]  eta: 2:09:33  lr: 0.000100  loss: 0.0282 (0.0317)  time: 3.5065  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1240/3449]  eta: 2:08:58  lr: 0.000100  loss: 0.0282 (0.0317)  time: 3.5066  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1250/3449]  eta: 2:08:23  lr: 0.000100  loss: 0.0295 (0.0317)  time: 3.5069  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1260/3449]  eta: 2:07:48  lr: 0.000100  loss: 0.0301 (0.0318)  time: 3.5072  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1270/3449]  eta: 2:07:13  lr: 0.000100  loss: 0.0286 (0.0317)  time: 3.5082  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1280/3449]  eta: 2:06:38  lr: 0.000100  loss: 0.0265 (0.0317)  time: 3.5072  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1290/3449]  eta: 2:06:03  lr: 0.000100  loss: 0.0273 (0.0317)  time: 3.5066  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1300/3449]  eta: 2:05:29  lr: 0.000100  loss: 0.0291 (0.0317)  time: 3.5089  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1310/3449]  eta: 2:04:54  lr: 0.000100  loss: 0.0289 (0.0316)  time: 3.5084  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1320/3449]  eta: 2:04:19  lr: 0.000100  loss: 0.0274 (0.0316)  time: 3.5072  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1330/3449]  eta: 2:03:44  lr: 0.000100  loss: 0.0291 (0.0316)  time: 3.5076  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1340/3449]  eta: 2:03:09  lr: 0.000100  loss: 0.0300 (0.0316)  time: 3.5070  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1350/3449]  eta: 2:02:34  lr: 0.000100  loss: 0.0280 (0.0316)  time: 3.5074  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1360/3449]  eta: 2:01:59  lr: 0.000100  loss: 0.0304 (0.0316)  time: 3.5069  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1370/3449]  eta: 2:01:24  lr: 0.000100  loss: 0.0305 (0.0316)  time: 3.5060  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1380/3449]  eta: 2:00:49  lr: 0.000100  loss: 0.0294 (0.0315)  time: 3.5054  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1390/3449]  eta: 2:00:14  lr: 0.000100  loss: 0.0290 (0.0315)  time: 3.5044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1400/3449]  eta: 1:59:39  lr: 0.000100  loss: 0.0290 (0.0315)  time: 3.5035  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1410/3449]  eta: 1:59:04  lr: 0.000100  loss: 0.0298 (0.0315)  time: 3.5050  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1420/3449]  eta: 1:58:29  lr: 0.000100  loss: 0.0298 (0.0315)  time: 3.5068  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1430/3449]  eta: 1:57:54  lr: 0.000100  loss: 0.0293 (0.0315)  time: 3.5062  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1440/3449]  eta: 1:57:19  lr: 0.000100  loss: 0.0296 (0.0315)  time: 3.5069  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1450/3449]  eta: 1:56:44  lr: 0.000100  loss: 0.0280 (0.0315)  time: 3.5104  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1460/3449]  eta: 1:56:09  lr: 0.000100  loss: 0.0289 (0.0315)  time: 3.5108  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1470/3449]  eta: 1:55:34  lr: 0.000100  loss: 0.0311 (0.0315)  time: 3.5095  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1480/3449]  eta: 1:54:59  lr: 0.000100  loss: 0.0291 (0.0315)  time: 3.5095  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1490/3449]  eta: 1:54:24  lr: 0.000100  loss: 0.0289 (0.0315)  time: 3.5071  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1500/3449]  eta: 1:53:49  lr: 0.000100  loss: 0.0301 (0.0314)  time: 3.5066  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1510/3449]  eta: 1:53:14  lr: 0.000100  loss: 0.0302 (0.0315)  time: 3.5083  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1520/3449]  eta: 1:52:39  lr: 0.000100  loss: 0.0325 (0.0315)  time: 3.5080  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1530/3449]  eta: 1:52:04  lr: 0.000100  loss: 0.0313 (0.0315)  time: 3.5074  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1540/3449]  eta: 1:51:29  lr: 0.000100  loss: 0.0298 (0.0315)  time: 3.5066  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:28]  [1550/3449]  eta: 1:50:54  lr: 0.000100  loss: 0.0296 (0.0315)  time: 3.5057  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1560/3449]  eta: 1:50:19  lr: 0.000100  loss: 0.0310 (0.0315)  time: 3.5064  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1570/3449]  eta: 1:49:44  lr: 0.000100  loss: 0.0302 (0.0315)  time: 3.5062  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1580/3449]  eta: 1:49:09  lr: 0.000100  loss: 0.0302 (0.0315)  time: 3.5055  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1590/3449]  eta: 1:48:34  lr: 0.000100  loss: 0.0312 (0.0315)  time: 3.5056  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1600/3449]  eta: 1:47:59  lr: 0.000100  loss: 0.0282 (0.0315)  time: 3.5057  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1610/3449]  eta: 1:47:24  lr: 0.000100  loss: 0.0300 (0.0314)  time: 3.5058  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1620/3449]  eta: 1:46:49  lr: 0.000100  loss: 0.0305 (0.0314)  time: 3.5070  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1630/3449]  eta: 1:46:14  lr: 0.000100  loss: 0.0295 (0.0314)  time: 3.5070  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1640/3449]  eta: 1:45:39  lr: 0.000100  loss: 0.0286 (0.0314)  time: 3.5061  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1650/3449]  eta: 1:45:04  lr: 0.000100  loss: 0.0274 (0.0314)  time: 3.5057  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1660/3449]  eta: 1:44:29  lr: 0.000100  loss: 0.0274 (0.0314)  time: 3.5054  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1670/3449]  eta: 1:43:54  lr: 0.000100  loss: 0.0278 (0.0313)  time: 3.5053  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1680/3449]  eta: 1:43:19  lr: 0.000100  loss: 0.0291 (0.0313)  time: 3.5058  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1690/3449]  eta: 1:42:43  lr: 0.000100  loss: 0.0298 (0.0314)  time: 3.5059  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1700/3449]  eta: 1:42:08  lr: 0.000100  loss: 0.0289 (0.0313)  time: 3.5049  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1710/3449]  eta: 1:41:33  lr: 0.000100  loss: 0.0294 (0.0314)  time: 3.5040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1720/3449]  eta: 1:40:58  lr: 0.000100  loss: 0.0327 (0.0314)  time: 3.5055  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1730/3449]  eta: 1:40:23  lr: 0.000100  loss: 0.0324 (0.0314)  time: 3.5061  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1740/3449]  eta: 1:39:48  lr: 0.000100  loss: 0.0310 (0.0314)  time: 3.5050  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1750/3449]  eta: 1:39:13  lr: 0.000100  loss: 0.0306 (0.0314)  time: 3.5053  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1760/3449]  eta: 1:38:38  lr: 0.000100  loss: 0.0286 (0.0314)  time: 3.5051  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1770/3449]  eta: 1:38:03  lr: 0.000100  loss: 0.0282 (0.0314)  time: 3.5050  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1780/3449]  eta: 1:37:28  lr: 0.000100  loss: 0.0288 (0.0314)  time: 3.5051  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1790/3449]  eta: 1:36:53  lr: 0.000100  loss: 0.0289 (0.0313)  time: 3.5046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1800/3449]  eta: 1:36:18  lr: 0.000100  loss: 0.0283 (0.0313)  time: 3.5040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1810/3449]  eta: 1:35:43  lr: 0.000100  loss: 0.0282 (0.0313)  time: 3.5041  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1820/3449]  eta: 1:35:08  lr: 0.000100  loss: 0.0300 (0.0313)  time: 3.5036  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1830/3449]  eta: 1:34:33  lr: 0.000100  loss: 0.0303 (0.0313)  time: 3.5026  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1840/3449]  eta: 1:33:58  lr: 0.000100  loss: 0.0291 (0.0313)  time: 3.5035  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1850/3449]  eta: 1:33:23  lr: 0.000100  loss: 0.0291 (0.0313)  time: 3.5046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1860/3449]  eta: 1:32:48  lr: 0.000100  loss: 0.0281 (0.0313)  time: 3.5042  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1870/3449]  eta: 1:32:13  lr: 0.000100  loss: 0.0281 (0.0313)  time: 3.5040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1880/3449]  eta: 1:31:38  lr: 0.000100  loss: 0.0313 (0.0313)  time: 3.5035  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1890/3449]  eta: 1:31:03  lr: 0.000100  loss: 0.0290 (0.0313)  time: 3.5025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1900/3449]  eta: 1:30:28  lr: 0.000100  loss: 0.0290 (0.0313)  time: 3.5032  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1910/3449]  eta: 1:29:53  lr: 0.000100  loss: 0.0315 (0.0313)  time: 3.5044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1920/3449]  eta: 1:29:18  lr: 0.000100  loss: 0.0274 (0.0312)  time: 3.5047  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1930/3449]  eta: 1:28:42  lr: 0.000100  loss: 0.0273 (0.0312)  time: 3.5046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1940/3449]  eta: 1:28:07  lr: 0.000100  loss: 0.0287 (0.0312)  time: 3.5046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1950/3449]  eta: 1:27:32  lr: 0.000100  loss: 0.0285 (0.0312)  time: 3.5055  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1960/3449]  eta: 1:26:57  lr: 0.000100  loss: 0.0285 (0.0312)  time: 3.5043  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1970/3449]  eta: 1:26:22  lr: 0.000100  loss: 0.0286 (0.0312)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [1980/3449]  eta: 1:25:47  lr: 0.000100  loss: 0.0291 (0.0312)  time: 3.5029  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [1990/3449]  eta: 1:25:12  lr: 0.000100  loss: 0.0310 (0.0312)  time: 3.5031  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2000/3449]  eta: 1:24:37  lr: 0.000100  loss: 0.0296 (0.0312)  time: 3.5039  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2010/3449]  eta: 1:24:02  lr: 0.000100  loss: 0.0266 (0.0312)  time: 3.5036  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2020/3449]  eta: 1:23:27  lr: 0.000100  loss: 0.0277 (0.0312)  time: 3.5034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2030/3449]  eta: 1:22:52  lr: 0.000100  loss: 0.0306 (0.0312)  time: 3.5030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2040/3449]  eta: 1:22:17  lr: 0.000100  loss: 0.0306 (0.0312)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2050/3449]  eta: 1:21:42  lr: 0.000100  loss: 0.0282 (0.0312)  time: 3.5009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2060/3449]  eta: 1:21:07  lr: 0.000100  loss: 0.0277 (0.0312)  time: 3.5009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2070/3449]  eta: 1:20:32  lr: 0.000100  loss: 0.0279 (0.0312)  time: 3.5018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2080/3449]  eta: 1:19:57  lr: 0.000100  loss: 0.0280 (0.0312)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2090/3449]  eta: 1:19:22  lr: 0.000100  loss: 0.0278 (0.0312)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2100/3449]  eta: 1:18:47  lr: 0.000100  loss: 0.0278 (0.0312)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2110/3449]  eta: 1:18:11  lr: 0.000100  loss: 0.0266 (0.0312)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2120/3449]  eta: 1:17:36  lr: 0.000100  loss: 0.0266 (0.0312)  time: 3.4980  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2130/3449]  eta: 1:17:01  lr: 0.000100  loss: 0.0290 (0.0312)  time: 3.4997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2140/3449]  eta: 1:16:26  lr: 0.000100  loss: 0.0289 (0.0311)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2150/3449]  eta: 1:15:51  lr: 0.000100  loss: 0.0295 (0.0311)  time: 3.4993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2160/3449]  eta: 1:15:16  lr: 0.000100  loss: 0.0295 (0.0311)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2170/3449]  eta: 1:14:41  lr: 0.000100  loss: 0.0285 (0.0311)  time: 3.5007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2180/3449]  eta: 1:14:06  lr: 0.000100  loss: 0.0283 (0.0311)  time: 3.4999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2190/3449]  eta: 1:13:31  lr: 0.000100  loss: 0.0300 (0.0311)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2200/3449]  eta: 1:12:56  lr: 0.000100  loss: 0.0306 (0.0311)  time: 3.5015  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:28]  [2210/3449]  eta: 1:12:21  lr: 0.000100  loss: 0.0292 (0.0312)  time: 3.5028  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2220/3449]  eta: 1:11:46  lr: 0.000100  loss: 0.0298 (0.0312)  time: 3.5034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2230/3449]  eta: 1:11:11  lr: 0.000100  loss: 0.0293 (0.0311)  time: 3.5042  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2240/3449]  eta: 1:10:36  lr: 0.000100  loss: 0.0289 (0.0311)  time: 3.5054  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2250/3449]  eta: 1:10:01  lr: 0.000100  loss: 0.0303 (0.0311)  time: 3.5062  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [2260/3449]  eta: 1:09:26  lr: 0.000100  loss: 0.0292 (0.0311)  time: 3.5067  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2270/3449]  eta: 1:08:51  lr: 0.000100  loss: 0.0281 (0.0311)  time: 3.5063  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2280/3449]  eta: 1:08:16  lr: 0.000100  loss: 0.0274 (0.0311)  time: 3.5060  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2290/3449]  eta: 1:07:41  lr: 0.000100  loss: 0.0276 (0.0311)  time: 3.5068  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2300/3449]  eta: 1:07:06  lr: 0.000100  loss: 0.0292 (0.0311)  time: 3.5067  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2310/3449]  eta: 1:06:31  lr: 0.000100  loss: 0.0280 (0.0311)  time: 3.5059  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2320/3449]  eta: 1:05:56  lr: 0.000100  loss: 0.0282 (0.0311)  time: 3.5064  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2330/3449]  eta: 1:05:21  lr: 0.000100  loss: 0.0289 (0.0311)  time: 3.5055  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2340/3449]  eta: 1:04:45  lr: 0.000100  loss: 0.0292 (0.0311)  time: 3.5045  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2350/3449]  eta: 1:04:10  lr: 0.000100  loss: 0.0298 (0.0311)  time: 3.5051  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2360/3449]  eta: 1:03:35  lr: 0.000100  loss: 0.0299 (0.0311)  time: 3.5051  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2370/3449]  eta: 1:03:00  lr: 0.000100  loss: 0.0300 (0.0311)  time: 3.5040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2380/3449]  eta: 1:02:25  lr: 0.000100  loss: 0.0289 (0.0311)  time: 3.5027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2390/3449]  eta: 1:01:50  lr: 0.000100  loss: 0.0290 (0.0311)  time: 3.5038  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2400/3449]  eta: 1:01:15  lr: 0.000100  loss: 0.0286 (0.0311)  time: 3.5030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2410/3449]  eta: 1:00:40  lr: 0.000100  loss: 0.0288 (0.0311)  time: 3.5024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2420/3449]  eta: 1:00:05  lr: 0.000100  loss: 0.0298 (0.0310)  time: 3.5035  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2430/3449]  eta: 0:59:30  lr: 0.000100  loss: 0.0301 (0.0310)  time: 3.5024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2440/3449]  eta: 0:58:55  lr: 0.000100  loss: 0.0307 (0.0310)  time: 3.5012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2450/3449]  eta: 0:58:20  lr: 0.000100  loss: 0.0275 (0.0310)  time: 3.5016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2460/3449]  eta: 0:57:45  lr: 0.000100  loss: 0.0272 (0.0310)  time: 3.5036  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2470/3449]  eta: 0:57:10  lr: 0.000100  loss: 0.0323 (0.0311)  time: 3.5045  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2480/3449]  eta: 0:56:35  lr: 0.000100  loss: 0.0417 (0.0312)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2490/3449]  eta: 0:56:00  lr: 0.000100  loss: 0.0519 (0.0313)  time: 3.4991  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [2500/3449]  eta: 0:55:25  lr: 0.000100  loss: 0.0519 (0.0314)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2510/3449]  eta: 0:54:50  lr: 0.000100  loss: 0.0381 (0.0314)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2520/3449]  eta: 0:54:15  lr: 0.000100  loss: 0.0337 (0.0314)  time: 3.5022  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2530/3449]  eta: 0:53:40  lr: 0.000100  loss: 0.0334 (0.0314)  time: 3.5026  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2540/3449]  eta: 0:53:05  lr: 0.000100  loss: 0.0306 (0.0314)  time: 3.5021  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2550/3449]  eta: 0:52:30  lr: 0.000100  loss: 0.0297 (0.0314)  time: 3.5044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2560/3449]  eta: 0:51:54  lr: 0.000100  loss: 0.0274 (0.0314)  time: 3.5063  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [2570/3449]  eta: 0:51:19  lr: 0.000100  loss: 0.0301 (0.0314)  time: 3.5066  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [2580/3449]  eta: 0:50:44  lr: 0.000100  loss: 0.0333 (0.0314)  time: 3.5072  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2590/3449]  eta: 0:50:09  lr: 0.000100  loss: 0.0323 (0.0314)  time: 3.5074  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2600/3449]  eta: 0:49:34  lr: 0.000100  loss: 0.0290 (0.0314)  time: 3.5067  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2610/3449]  eta: 0:48:59  lr: 0.000100  loss: 0.0279 (0.0314)  time: 3.5065  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2620/3449]  eta: 0:48:24  lr: 0.000100  loss: 0.0296 (0.0314)  time: 3.5072  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2630/3449]  eta: 0:47:49  lr: 0.000100  loss: 0.0286 (0.0314)  time: 3.5066  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2640/3449]  eta: 0:47:14  lr: 0.000100  loss: 0.0287 (0.0314)  time: 3.5055  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2650/3449]  eta: 0:46:39  lr: 0.000100  loss: 0.0286 (0.0314)  time: 3.5049  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2660/3449]  eta: 0:46:04  lr: 0.000100  loss: 0.0290 (0.0314)  time: 3.5045  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2670/3449]  eta: 0:45:29  lr: 0.000100  loss: 0.0311 (0.0314)  time: 3.5054  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2680/3449]  eta: 0:44:54  lr: 0.000100  loss: 0.0298 (0.0314)  time: 3.5044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2690/3449]  eta: 0:44:19  lr: 0.000100  loss: 0.0286 (0.0314)  time: 3.5036  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2700/3449]  eta: 0:43:44  lr: 0.000100  loss: 0.0291 (0.0314)  time: 3.5043  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2710/3449]  eta: 0:43:09  lr: 0.000100  loss: 0.0308 (0.0314)  time: 3.5036  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [2720/3449]  eta: 0:42:34  lr: 0.000100  loss: 0.0298 (0.0314)  time: 3.5020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2730/3449]  eta: 0:41:59  lr: 0.000100  loss: 0.0280 (0.0314)  time: 3.5013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2740/3449]  eta: 0:41:24  lr: 0.000100  loss: 0.0283 (0.0314)  time: 3.5010  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [2750/3449]  eta: 0:40:49  lr: 0.000100  loss: 0.0299 (0.0314)  time: 3.5018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2760/3449]  eta: 0:40:14  lr: 0.000100  loss: 0.0294 (0.0314)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2770/3449]  eta: 0:39:39  lr: 0.000100  loss: 0.0287 (0.0314)  time: 3.5018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2780/3449]  eta: 0:39:04  lr: 0.000100  loss: 0.0289 (0.0314)  time: 3.5002  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [2790/3449]  eta: 0:38:29  lr: 0.000100  loss: 0.0297 (0.0314)  time: 3.5000  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [2800/3449]  eta: 0:37:54  lr: 0.000100  loss: 0.0287 (0.0314)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2810/3449]  eta: 0:37:18  lr: 0.000100  loss: 0.0273 (0.0314)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2820/3449]  eta: 0:36:43  lr: 0.000100  loss: 0.0293 (0.0314)  time: 3.5008  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2830/3449]  eta: 0:36:08  lr: 0.000100  loss: 0.0301 (0.0314)  time: 3.5007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2840/3449]  eta: 0:35:33  lr: 0.000100  loss: 0.0293 (0.0314)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2850/3449]  eta: 0:34:58  lr: 0.000100  loss: 0.0283 (0.0314)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2860/3449]  eta: 0:34:23  lr: 0.000100  loss: 0.0283 (0.0314)  time: 3.5007  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:28]  [2870/3449]  eta: 0:33:48  lr: 0.000100  loss: 0.0276 (0.0314)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2880/3449]  eta: 0:33:13  lr: 0.000100  loss: 0.0287 (0.0314)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2890/3449]  eta: 0:32:38  lr: 0.000100  loss: 0.0287 (0.0314)  time: 3.5013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2900/3449]  eta: 0:32:03  lr: 0.000100  loss: 0.0320 (0.0314)  time: 3.5024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2910/3449]  eta: 0:31:28  lr: 0.000100  loss: 0.0316 (0.0314)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2920/3449]  eta: 0:30:53  lr: 0.000100  loss: 0.0308 (0.0314)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2930/3449]  eta: 0:30:18  lr: 0.000100  loss: 0.0294 (0.0314)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2940/3449]  eta: 0:29:43  lr: 0.000100  loss: 0.0294 (0.0314)  time: 3.4991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2950/3449]  eta: 0:29:08  lr: 0.000100  loss: 0.0283 (0.0314)  time: 3.4993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2960/3449]  eta: 0:28:33  lr: 0.000100  loss: 0.0280 (0.0314)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [2970/3449]  eta: 0:27:58  lr: 0.000100  loss: 0.0283 (0.0314)  time: 3.5003  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [2980/3449]  eta: 0:27:23  lr: 0.000100  loss: 0.0279 (0.0314)  time: 3.5010  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [2990/3449]  eta: 0:26:48  lr: 0.000100  loss: 0.0293 (0.0314)  time: 3.5008  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3000/3449]  eta: 0:26:13  lr: 0.000100  loss: 0.0278 (0.0314)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3010/3449]  eta: 0:25:38  lr: 0.000100  loss: 0.0277 (0.0314)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3020/3449]  eta: 0:25:03  lr: 0.000100  loss: 0.0298 (0.0314)  time: 3.5012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3030/3449]  eta: 0:24:28  lr: 0.000100  loss: 0.0303 (0.0314)  time: 3.5008  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3040/3449]  eta: 0:23:53  lr: 0.000100  loss: 0.0293 (0.0314)  time: 3.5018  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [3050/3449]  eta: 0:23:17  lr: 0.000100  loss: 0.0293 (0.0314)  time: 3.5029  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [3060/3449]  eta: 0:22:42  lr: 0.000100  loss: 0.0293 (0.0314)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3070/3449]  eta: 0:22:07  lr: 0.000100  loss: 0.0287 (0.0314)  time: 3.5024  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [3080/3449]  eta: 0:21:32  lr: 0.000100  loss: 0.0297 (0.0314)  time: 3.5033  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [3090/3449]  eta: 0:20:57  lr: 0.000100  loss: 0.0295 (0.0314)  time: 3.5026  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3100/3449]  eta: 0:20:22  lr: 0.000100  loss: 0.0291 (0.0313)  time: 3.5008  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3110/3449]  eta: 0:19:47  lr: 0.000100  loss: 0.0292 (0.0313)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3120/3449]  eta: 0:19:12  lr: 0.000100  loss: 0.0302 (0.0313)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3130/3449]  eta: 0:18:37  lr: 0.000100  loss: 0.0276 (0.0313)  time: 3.5021  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [3140/3449]  eta: 0:18:02  lr: 0.000100  loss: 0.0276 (0.0313)  time: 3.5018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3150/3449]  eta: 0:17:27  lr: 0.000100  loss: 0.0289 (0.0313)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3160/3449]  eta: 0:16:52  lr: 0.000100  loss: 0.0298 (0.0313)  time: 3.5023  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3170/3449]  eta: 0:16:17  lr: 0.000100  loss: 0.0312 (0.0313)  time: 3.5031  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3180/3449]  eta: 0:15:42  lr: 0.000100  loss: 0.0312 (0.0313)  time: 3.5034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3190/3449]  eta: 0:15:07  lr: 0.000100  loss: 0.0287 (0.0313)  time: 3.5034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3200/3449]  eta: 0:14:32  lr: 0.000100  loss: 0.0287 (0.0313)  time: 3.5028  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3210/3449]  eta: 0:13:57  lr: 0.000100  loss: 0.0295 (0.0313)  time: 3.5027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3220/3449]  eta: 0:13:22  lr: 0.000100  loss: 0.0282 (0.0313)  time: 3.5027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3230/3449]  eta: 0:12:47  lr: 0.000100  loss: 0.0290 (0.0313)  time: 3.5024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3240/3449]  eta: 0:12:12  lr: 0.000100  loss: 0.0298 (0.0313)  time: 3.5027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3250/3449]  eta: 0:11:37  lr: 0.000100  loss: 0.0289 (0.0313)  time: 3.5027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3260/3449]  eta: 0:11:02  lr: 0.000100  loss: 0.0265 (0.0313)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3270/3449]  eta: 0:10:27  lr: 0.000100  loss: 0.0264 (0.0313)  time: 3.5016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3280/3449]  eta: 0:09:52  lr: 0.000100  loss: 0.0272 (0.0313)  time: 3.5007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3290/3449]  eta: 0:09:17  lr: 0.000100  loss: 0.0273 (0.0313)  time: 3.5021  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3300/3449]  eta: 0:08:42  lr: 0.000100  loss: 0.0281 (0.0313)  time: 3.5039  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3310/3449]  eta: 0:08:06  lr: 0.000100  loss: 0.0307 (0.0313)  time: 3.5045  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:28]  [3320/3449]  eta: 0:07:31  lr: 0.000100  loss: 0.0310 (0.0313)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3330/3449]  eta: 0:06:56  lr: 0.000100  loss: 0.0311 (0.0313)  time: 3.5025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3340/3449]  eta: 0:06:21  lr: 0.000100  loss: 0.0305 (0.0313)  time: 3.5013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3350/3449]  eta: 0:05:46  lr: 0.000100  loss: 0.0274 (0.0313)  time: 3.5004  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3360/3449]  eta: 0:05:11  lr: 0.000100  loss: 0.0298 (0.0313)  time: 3.5016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3370/3449]  eta: 0:04:36  lr: 0.000100  loss: 0.0330 (0.0313)  time: 3.5002  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3380/3449]  eta: 0:04:01  lr: 0.000100  loss: 0.0302 (0.0313)  time: 3.5000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3390/3449]  eta: 0:03:26  lr: 0.000100  loss: 0.0294 (0.0313)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3400/3449]  eta: 0:02:51  lr: 0.000100  loss: 0.0294 (0.0313)  time: 3.5013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3410/3449]  eta: 0:02:16  lr: 0.000100  loss: 0.0293 (0.0313)  time: 3.5026  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3420/3449]  eta: 0:01:41  lr: 0.000100  loss: 0.0303 (0.0313)  time: 3.5044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3430/3449]  eta: 0:01:06  lr: 0.000100  loss: 0.0306 (0.0313)  time: 3.5036  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3440/3449]  eta: 0:00:31  lr: 0.000100  loss: 0.0282 (0.0313)  time: 3.5020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0290 (0.0312)  time: 3.5004  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:28] Total time: 3:21:24 (3.5037 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0290 (0.0312)\n",
      "Valid: [epoch:28]  [ 0/14]  eta: 0:04:25  loss: 0.0276 (0.0276)  time: 18.9894  data: 0.6643  max mem: 34968\n",
      "Valid: [epoch:28]  [13/14]  eta: 0:00:18  loss: 0.0261 (0.0264)  time: 18.3896  data: 0.0477  max mem: 34968\n",
      "Valid: [epoch:28] Total time: 0:04:17 (18.4020 s / it)\n",
      "Averaged stats: loss: 0.0261 (0.0264)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_28_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.026%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:29]  [   0/3449]  eta: 4:52:01  lr: 0.000100  loss: 0.0329 (0.0329)  time: 5.0801  data: 1.6341  max mem: 34968\n",
      "Train: [epoch:29]  [  10/3449]  eta: 3:27:53  lr: 0.000100  loss: 0.0307 (0.0306)  time: 3.6270  data: 0.1487  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:29]  [  20/3449]  eta: 3:23:54  lr: 0.000100  loss: 0.0304 (0.0316)  time: 3.4923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [  30/3449]  eta: 3:22:09  lr: 0.000100  loss: 0.0293 (0.0308)  time: 3.5042  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [  40/3449]  eta: 3:20:57  lr: 0.000100  loss: 0.0278 (0.0302)  time: 3.5045  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [  50/3449]  eta: 3:19:57  lr: 0.000100  loss: 0.0286 (0.0314)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [  60/3449]  eta: 3:19:05  lr: 0.000100  loss: 0.0307 (0.0315)  time: 3.5000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [  70/3449]  eta: 3:18:18  lr: 0.000100  loss: 0.0307 (0.0315)  time: 3.4998  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [  80/3449]  eta: 3:17:34  lr: 0.000100  loss: 0.0293 (0.0306)  time: 3.5002  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [  90/3449]  eta: 3:16:52  lr: 0.000100  loss: 0.0296 (0.0306)  time: 3.5005  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 100/3449]  eta: 3:16:10  lr: 0.000100  loss: 0.0300 (0.0305)  time: 3.4974  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 110/3449]  eta: 3:15:28  lr: 0.000100  loss: 0.0276 (0.0307)  time: 3.4934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 120/3449]  eta: 3:14:46  lr: 0.000100  loss: 0.0275 (0.0305)  time: 3.4911  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 130/3449]  eta: 3:14:06  lr: 0.000100  loss: 0.0279 (0.0305)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 140/3449]  eta: 3:13:27  lr: 0.000100  loss: 0.0294 (0.0306)  time: 3.4903  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 150/3449]  eta: 3:12:48  lr: 0.000100  loss: 0.0305 (0.0306)  time: 3.4913  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 160/3449]  eta: 3:12:10  lr: 0.000100  loss: 0.0308 (0.0305)  time: 3.4919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 170/3449]  eta: 3:11:32  lr: 0.000100  loss: 0.0288 (0.0304)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 180/3449]  eta: 3:10:54  lr: 0.000100  loss: 0.0288 (0.0305)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 190/3449]  eta: 3:10:17  lr: 0.000100  loss: 0.0322 (0.0308)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 200/3449]  eta: 3:09:40  lr: 0.000100  loss: 0.0304 (0.0307)  time: 3.4911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 210/3449]  eta: 3:09:03  lr: 0.000100  loss: 0.0304 (0.0307)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 220/3449]  eta: 3:08:27  lr: 0.000100  loss: 0.0316 (0.0309)  time: 3.4914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 230/3449]  eta: 3:07:50  lr: 0.000100  loss: 0.0308 (0.0310)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 240/3449]  eta: 3:07:14  lr: 0.000100  loss: 0.0276 (0.0309)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 250/3449]  eta: 3:06:38  lr: 0.000100  loss: 0.0289 (0.0309)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 260/3449]  eta: 3:06:02  lr: 0.000100  loss: 0.0299 (0.0309)  time: 3.4914  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 270/3449]  eta: 3:05:26  lr: 0.000100  loss: 0.0294 (0.0312)  time: 3.4911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 280/3449]  eta: 3:04:50  lr: 0.000100  loss: 0.0315 (0.0312)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 290/3449]  eta: 3:04:14  lr: 0.000100  loss: 0.0293 (0.0311)  time: 3.4901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 300/3449]  eta: 3:03:38  lr: 0.000100  loss: 0.0293 (0.0311)  time: 3.4907  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 310/3449]  eta: 3:03:02  lr: 0.000100  loss: 0.0294 (0.0311)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 320/3449]  eta: 3:02:26  lr: 0.000100  loss: 0.0287 (0.0311)  time: 3.4904  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 330/3449]  eta: 3:01:50  lr: 0.000100  loss: 0.0288 (0.0312)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 340/3449]  eta: 3:01:14  lr: 0.000100  loss: 0.0289 (0.0312)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 350/3449]  eta: 3:00:39  lr: 0.000100  loss: 0.0285 (0.0311)  time: 3.4901  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 360/3449]  eta: 3:00:03  lr: 0.000100  loss: 0.0260 (0.0311)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 370/3449]  eta: 2:59:28  lr: 0.000100  loss: 0.0270 (0.0310)  time: 3.4920  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 380/3449]  eta: 2:58:53  lr: 0.000100  loss: 0.0272 (0.0309)  time: 3.4920  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 390/3449]  eta: 2:58:17  lr: 0.000100  loss: 0.0296 (0.0309)  time: 3.4912  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 400/3449]  eta: 2:57:42  lr: 0.000100  loss: 0.0312 (0.0310)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 410/3449]  eta: 2:57:06  lr: 0.000100  loss: 0.0305 (0.0309)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 420/3449]  eta: 2:56:31  lr: 0.000100  loss: 0.0305 (0.0310)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 430/3449]  eta: 2:55:56  lr: 0.000100  loss: 0.0319 (0.0310)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 440/3449]  eta: 2:55:20  lr: 0.000100  loss: 0.0342 (0.0311)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 450/3449]  eta: 2:54:45  lr: 0.000100  loss: 0.0338 (0.0313)  time: 3.4933  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 460/3449]  eta: 2:54:10  lr: 0.000100  loss: 0.0325 (0.0313)  time: 3.4928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 470/3449]  eta: 2:53:35  lr: 0.000100  loss: 0.0312 (0.0313)  time: 3.4917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 480/3449]  eta: 2:52:59  lr: 0.000100  loss: 0.0312 (0.0313)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 490/3449]  eta: 2:52:24  lr: 0.000100  loss: 0.0301 (0.0313)  time: 3.4923  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 500/3449]  eta: 2:51:49  lr: 0.000100  loss: 0.0308 (0.0313)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 510/3449]  eta: 2:51:14  lr: 0.000100  loss: 0.0304 (0.0313)  time: 3.4927  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 520/3449]  eta: 2:50:39  lr: 0.000100  loss: 0.0288 (0.0312)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 530/3449]  eta: 2:50:04  lr: 0.000100  loss: 0.0285 (0.0312)  time: 3.4923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 540/3449]  eta: 2:49:29  lr: 0.000100  loss: 0.0287 (0.0312)  time: 3.4942  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 550/3449]  eta: 2:48:54  lr: 0.000100  loss: 0.0287 (0.0312)  time: 3.4969  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 560/3449]  eta: 2:48:19  lr: 0.000100  loss: 0.0312 (0.0312)  time: 3.4966  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 570/3449]  eta: 2:47:44  lr: 0.000100  loss: 0.0340 (0.0313)  time: 3.4958  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 580/3449]  eta: 2:47:09  lr: 0.000100  loss: 0.0301 (0.0313)  time: 3.4947  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 590/3449]  eta: 2:46:34  lr: 0.000100  loss: 0.0296 (0.0313)  time: 3.4945  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 600/3449]  eta: 2:45:59  lr: 0.000100  loss: 0.0312 (0.0313)  time: 3.4963  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 610/3449]  eta: 2:45:24  lr: 0.000100  loss: 0.0294 (0.0313)  time: 3.4956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 620/3449]  eta: 2:44:49  lr: 0.000100  loss: 0.0297 (0.0313)  time: 3.4928  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 630/3449]  eta: 2:44:14  lr: 0.000100  loss: 0.0299 (0.0313)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 640/3449]  eta: 2:43:38  lr: 0.000100  loss: 0.0293 (0.0313)  time: 3.4891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 650/3449]  eta: 2:43:03  lr: 0.000100  loss: 0.0295 (0.0313)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 660/3449]  eta: 2:42:28  lr: 0.000100  loss: 0.0309 (0.0313)  time: 3.4880  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 670/3449]  eta: 2:41:53  lr: 0.000100  loss: 0.0311 (0.0314)  time: 3.4879  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:29]  [ 680/3449]  eta: 2:41:17  lr: 0.000100  loss: 0.0318 (0.0314)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 690/3449]  eta: 2:40:42  lr: 0.000100  loss: 0.0285 (0.0313)  time: 3.4909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 700/3449]  eta: 2:40:07  lr: 0.000100  loss: 0.0274 (0.0313)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 710/3449]  eta: 2:39:32  lr: 0.000100  loss: 0.0316 (0.0313)  time: 3.4935  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 720/3449]  eta: 2:38:57  lr: 0.000100  loss: 0.0325 (0.0313)  time: 3.4949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 730/3449]  eta: 2:38:23  lr: 0.000100  loss: 0.0311 (0.0314)  time: 3.4965  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 740/3449]  eta: 2:37:48  lr: 0.000100  loss: 0.0292 (0.0314)  time: 3.4983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 750/3449]  eta: 2:37:13  lr: 0.000100  loss: 0.0293 (0.0314)  time: 3.4994  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 760/3449]  eta: 2:36:38  lr: 0.000100  loss: 0.0303 (0.0313)  time: 3.5004  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 770/3449]  eta: 2:36:03  lr: 0.000100  loss: 0.0301 (0.0313)  time: 3.4991  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 780/3449]  eta: 2:35:28  lr: 0.000100  loss: 0.0317 (0.0313)  time: 3.4982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 790/3449]  eta: 2:34:54  lr: 0.000100  loss: 0.0312 (0.0313)  time: 3.4989  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 800/3449]  eta: 2:34:19  lr: 0.000100  loss: 0.0304 (0.0313)  time: 3.4991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 810/3449]  eta: 2:33:44  lr: 0.000100  loss: 0.0300 (0.0313)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 820/3449]  eta: 2:33:09  lr: 0.000100  loss: 0.0262 (0.0312)  time: 3.4974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 830/3449]  eta: 2:32:34  lr: 0.000100  loss: 0.0286 (0.0312)  time: 3.4967  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 840/3449]  eta: 2:31:59  lr: 0.000100  loss: 0.0299 (0.0312)  time: 3.4967  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 850/3449]  eta: 2:31:24  lr: 0.000100  loss: 0.0301 (0.0312)  time: 3.4968  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 860/3449]  eta: 2:30:49  lr: 0.000100  loss: 0.0285 (0.0312)  time: 3.4973  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 870/3449]  eta: 2:30:14  lr: 0.000100  loss: 0.0306 (0.0312)  time: 3.4975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 880/3449]  eta: 2:29:39  lr: 0.000100  loss: 0.0316 (0.0312)  time: 3.4955  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 890/3449]  eta: 2:29:05  lr: 0.000100  loss: 0.0316 (0.0313)  time: 3.4961  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 900/3449]  eta: 2:28:30  lr: 0.000100  loss: 0.0345 (0.0313)  time: 3.4980  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [ 910/3449]  eta: 2:27:55  lr: 0.000100  loss: 0.0310 (0.0313)  time: 3.4964  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 920/3449]  eta: 2:27:20  lr: 0.000100  loss: 0.0310 (0.0313)  time: 3.4948  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 930/3449]  eta: 2:26:45  lr: 0.000100  loss: 0.0315 (0.0313)  time: 3.4943  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 940/3449]  eta: 2:26:10  lr: 0.000100  loss: 0.0303 (0.0313)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 950/3449]  eta: 2:25:35  lr: 0.000100  loss: 0.0303 (0.0314)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 960/3449]  eta: 2:25:00  lr: 0.000100  loss: 0.0309 (0.0314)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 970/3449]  eta: 2:24:25  lr: 0.000100  loss: 0.0293 (0.0314)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 980/3449]  eta: 2:23:50  lr: 0.000100  loss: 0.0311 (0.0314)  time: 3.4957  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [ 990/3449]  eta: 2:23:15  lr: 0.000100  loss: 0.0319 (0.0314)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1000/3449]  eta: 2:22:40  lr: 0.000100  loss: 0.0315 (0.0315)  time: 3.4994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1010/3449]  eta: 2:22:05  lr: 0.000100  loss: 0.0292 (0.0315)  time: 3.4987  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1020/3449]  eta: 2:21:30  lr: 0.000100  loss: 0.0292 (0.0315)  time: 3.4984  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1030/3449]  eta: 2:20:55  lr: 0.000100  loss: 0.0318 (0.0315)  time: 3.4965  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1040/3449]  eta: 2:20:20  lr: 0.000100  loss: 0.0294 (0.0315)  time: 3.4961  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1050/3449]  eta: 2:19:46  lr: 0.000100  loss: 0.0276 (0.0315)  time: 3.4981  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1060/3449]  eta: 2:19:11  lr: 0.000100  loss: 0.0353 (0.0316)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1070/3449]  eta: 2:18:36  lr: 0.000100  loss: 0.0353 (0.0316)  time: 3.4968  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1080/3449]  eta: 2:18:01  lr: 0.000100  loss: 0.0328 (0.0317)  time: 3.4954  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1090/3449]  eta: 2:17:26  lr: 0.000100  loss: 0.0343 (0.0317)  time: 3.4947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1100/3449]  eta: 2:16:51  lr: 0.000100  loss: 0.0371 (0.0318)  time: 3.4936  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1110/3449]  eta: 2:16:16  lr: 0.000100  loss: 0.0408 (0.0319)  time: 3.4932  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1120/3449]  eta: 2:15:41  lr: 0.000100  loss: 0.0353 (0.0319)  time: 3.4929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1130/3449]  eta: 2:15:06  lr: 0.000100  loss: 0.0353 (0.0319)  time: 3.4935  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1140/3449]  eta: 2:14:31  lr: 0.000100  loss: 0.0364 (0.0320)  time: 3.4944  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1150/3449]  eta: 2:13:56  lr: 0.000100  loss: 0.0318 (0.0320)  time: 3.4945  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1160/3449]  eta: 2:13:21  lr: 0.000100  loss: 0.0302 (0.0320)  time: 3.4946  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1170/3449]  eta: 2:12:46  lr: 0.000100  loss: 0.0309 (0.0319)  time: 3.4942  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1180/3449]  eta: 2:12:11  lr: 0.000100  loss: 0.0317 (0.0320)  time: 3.4955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1190/3449]  eta: 2:11:36  lr: 0.000100  loss: 0.0297 (0.0319)  time: 3.4970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1200/3449]  eta: 2:11:01  lr: 0.000100  loss: 0.0295 (0.0319)  time: 3.4957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1210/3449]  eta: 2:10:26  lr: 0.000100  loss: 0.0295 (0.0319)  time: 3.4963  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1220/3449]  eta: 2:09:51  lr: 0.000100  loss: 0.0284 (0.0319)  time: 3.4972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1230/3449]  eta: 2:09:16  lr: 0.000100  loss: 0.0287 (0.0319)  time: 3.4969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1240/3449]  eta: 2:08:41  lr: 0.000100  loss: 0.0291 (0.0319)  time: 3.4985  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1250/3449]  eta: 2:08:06  lr: 0.000100  loss: 0.0305 (0.0319)  time: 3.4997  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1260/3449]  eta: 2:07:32  lr: 0.000100  loss: 0.0311 (0.0319)  time: 3.4997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1270/3449]  eta: 2:06:57  lr: 0.000100  loss: 0.0311 (0.0319)  time: 3.4988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1280/3449]  eta: 2:06:22  lr: 0.000100  loss: 0.0301 (0.0319)  time: 3.4977  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1290/3449]  eta: 2:05:47  lr: 0.000100  loss: 0.0293 (0.0319)  time: 3.4975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1300/3449]  eta: 2:05:12  lr: 0.000100  loss: 0.0302 (0.0319)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1310/3449]  eta: 2:04:37  lr: 0.000100  loss: 0.0317 (0.0319)  time: 3.4988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1320/3449]  eta: 2:04:02  lr: 0.000100  loss: 0.0296 (0.0319)  time: 3.4987  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1330/3449]  eta: 2:03:27  lr: 0.000100  loss: 0.0301 (0.0319)  time: 3.4973  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:29]  [1340/3449]  eta: 2:02:52  lr: 0.000100  loss: 0.0290 (0.0319)  time: 3.4953  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1350/3449]  eta: 2:02:17  lr: 0.000100  loss: 0.0290 (0.0319)  time: 3.4954  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1360/3449]  eta: 2:01:42  lr: 0.000100  loss: 0.0298 (0.0319)  time: 3.4955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1370/3449]  eta: 2:01:07  lr: 0.000100  loss: 0.0305 (0.0319)  time: 3.4958  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1380/3449]  eta: 2:00:32  lr: 0.000100  loss: 0.0302 (0.0319)  time: 3.4976  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1390/3449]  eta: 1:59:57  lr: 0.000100  loss: 0.0302 (0.0319)  time: 3.4990  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1400/3449]  eta: 1:59:22  lr: 0.000100  loss: 0.0315 (0.0319)  time: 3.4970  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1410/3449]  eta: 1:58:47  lr: 0.000100  loss: 0.0297 (0.0318)  time: 3.4957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1420/3449]  eta: 1:58:13  lr: 0.000100  loss: 0.0274 (0.0319)  time: 3.4964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1430/3449]  eta: 1:57:38  lr: 0.000100  loss: 0.0320 (0.0319)  time: 3.4959  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1440/3449]  eta: 1:57:03  lr: 0.000100  loss: 0.0281 (0.0318)  time: 3.4960  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1450/3449]  eta: 1:56:28  lr: 0.000100  loss: 0.0287 (0.0318)  time: 3.4965  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1460/3449]  eta: 1:55:53  lr: 0.000100  loss: 0.0290 (0.0318)  time: 3.4948  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1470/3449]  eta: 1:55:18  lr: 0.000100  loss: 0.0296 (0.0318)  time: 3.4937  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1480/3449]  eta: 1:54:43  lr: 0.000100  loss: 0.0307 (0.0318)  time: 3.4944  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1490/3449]  eta: 1:54:08  lr: 0.000100  loss: 0.0317 (0.0318)  time: 3.4946  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1500/3449]  eta: 1:53:33  lr: 0.000100  loss: 0.0305 (0.0318)  time: 3.4952  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1510/3449]  eta: 1:52:58  lr: 0.000100  loss: 0.0297 (0.0318)  time: 3.4954  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1520/3449]  eta: 1:52:23  lr: 0.000100  loss: 0.0305 (0.0318)  time: 3.4955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1530/3449]  eta: 1:51:48  lr: 0.000100  loss: 0.0305 (0.0318)  time: 3.4962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1540/3449]  eta: 1:51:13  lr: 0.000100  loss: 0.0302 (0.0318)  time: 3.4960  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1550/3449]  eta: 1:50:38  lr: 0.000100  loss: 0.0309 (0.0318)  time: 3.4960  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1560/3449]  eta: 1:50:03  lr: 0.000100  loss: 0.0306 (0.0318)  time: 3.4958  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1570/3449]  eta: 1:49:28  lr: 0.000100  loss: 0.0299 (0.0318)  time: 3.4962  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1580/3449]  eta: 1:48:53  lr: 0.000100  loss: 0.0299 (0.0317)  time: 3.4952  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1590/3449]  eta: 1:48:18  lr: 0.000100  loss: 0.0306 (0.0318)  time: 3.4928  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1600/3449]  eta: 1:47:43  lr: 0.000100  loss: 0.0303 (0.0318)  time: 3.4928  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1610/3449]  eta: 1:47:08  lr: 0.000100  loss: 0.0291 (0.0317)  time: 3.4923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1620/3449]  eta: 1:46:33  lr: 0.000100  loss: 0.0291 (0.0317)  time: 3.4903  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1630/3449]  eta: 1:45:58  lr: 0.000100  loss: 0.0300 (0.0317)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1640/3449]  eta: 1:45:23  lr: 0.000100  loss: 0.0306 (0.0317)  time: 3.4896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1650/3449]  eta: 1:44:48  lr: 0.000100  loss: 0.0313 (0.0317)  time: 3.4885  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1660/3449]  eta: 1:44:13  lr: 0.000100  loss: 0.0313 (0.0317)  time: 3.4881  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1670/3449]  eta: 1:43:38  lr: 0.000100  loss: 0.0306 (0.0317)  time: 3.4900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1680/3449]  eta: 1:43:03  lr: 0.000100  loss: 0.0300 (0.0317)  time: 3.4914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1690/3449]  eta: 1:42:28  lr: 0.000100  loss: 0.0300 (0.0317)  time: 3.4911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1700/3449]  eta: 1:41:53  lr: 0.000100  loss: 0.0278 (0.0317)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1710/3449]  eta: 1:41:18  lr: 0.000100  loss: 0.0262 (0.0317)  time: 3.4923  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1720/3449]  eta: 1:40:43  lr: 0.000100  loss: 0.0267 (0.0317)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1730/3449]  eta: 1:40:08  lr: 0.000100  loss: 0.0283 (0.0317)  time: 3.4926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1740/3449]  eta: 1:39:33  lr: 0.000100  loss: 0.0307 (0.0317)  time: 3.4932  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1750/3449]  eta: 1:38:58  lr: 0.000100  loss: 0.0303 (0.0317)  time: 3.4934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1760/3449]  eta: 1:38:23  lr: 0.000100  loss: 0.0301 (0.0317)  time: 3.4942  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1770/3449]  eta: 1:37:48  lr: 0.000100  loss: 0.0309 (0.0317)  time: 3.4950  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1780/3449]  eta: 1:37:13  lr: 0.000100  loss: 0.0318 (0.0317)  time: 3.4950  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1790/3449]  eta: 1:36:38  lr: 0.000100  loss: 0.0310 (0.0317)  time: 3.4957  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1800/3449]  eta: 1:36:03  lr: 0.000100  loss: 0.0301 (0.0317)  time: 3.4964  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1810/3449]  eta: 1:35:28  lr: 0.000100  loss: 0.0303 (0.0317)  time: 3.4953  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1820/3449]  eta: 1:34:53  lr: 0.000100  loss: 0.0299 (0.0317)  time: 3.4965  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1830/3449]  eta: 1:34:18  lr: 0.000100  loss: 0.0323 (0.0317)  time: 3.4967  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1840/3449]  eta: 1:33:44  lr: 0.000100  loss: 0.0319 (0.0317)  time: 3.4944  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1850/3449]  eta: 1:33:09  lr: 0.000100  loss: 0.0287 (0.0317)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1860/3449]  eta: 1:32:34  lr: 0.000100  loss: 0.0306 (0.0317)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1870/3449]  eta: 1:31:59  lr: 0.000100  loss: 0.0282 (0.0317)  time: 3.4935  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1880/3449]  eta: 1:31:24  lr: 0.000100  loss: 0.0276 (0.0317)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1890/3449]  eta: 1:30:49  lr: 0.000100  loss: 0.0278 (0.0316)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1900/3449]  eta: 1:30:14  lr: 0.000100  loss: 0.0287 (0.0316)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1910/3449]  eta: 1:29:39  lr: 0.000100  loss: 0.0300 (0.0317)  time: 3.4902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1920/3449]  eta: 1:29:04  lr: 0.000100  loss: 0.0305 (0.0317)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1930/3449]  eta: 1:28:29  lr: 0.000100  loss: 0.0299 (0.0317)  time: 3.4904  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [1940/3449]  eta: 1:27:54  lr: 0.000100  loss: 0.0302 (0.0317)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1950/3449]  eta: 1:27:19  lr: 0.000100  loss: 0.0308 (0.0317)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1960/3449]  eta: 1:26:44  lr: 0.000100  loss: 0.0300 (0.0317)  time: 3.4855  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1970/3449]  eta: 1:26:09  lr: 0.000100  loss: 0.0303 (0.0316)  time: 3.4859  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1980/3449]  eta: 1:25:34  lr: 0.000100  loss: 0.0303 (0.0316)  time: 3.4877  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [1990/3449]  eta: 1:24:59  lr: 0.000100  loss: 0.0298 (0.0316)  time: 3.4898  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:29]  [2000/3449]  eta: 1:24:24  lr: 0.000100  loss: 0.0307 (0.0316)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2010/3449]  eta: 1:23:49  lr: 0.000100  loss: 0.0297 (0.0316)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2020/3449]  eta: 1:23:14  lr: 0.000100  loss: 0.0288 (0.0316)  time: 3.4893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2030/3449]  eta: 1:22:39  lr: 0.000100  loss: 0.0295 (0.0316)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2040/3449]  eta: 1:22:04  lr: 0.000100  loss: 0.0313 (0.0316)  time: 3.4903  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2050/3449]  eta: 1:21:29  lr: 0.000100  loss: 0.0322 (0.0316)  time: 3.4907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2060/3449]  eta: 1:20:54  lr: 0.000100  loss: 0.0307 (0.0316)  time: 3.4907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2070/3449]  eta: 1:20:19  lr: 0.000100  loss: 0.0308 (0.0316)  time: 3.4900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2080/3449]  eta: 1:19:44  lr: 0.000100  loss: 0.0316 (0.0316)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2090/3449]  eta: 1:19:09  lr: 0.000100  loss: 0.0325 (0.0316)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2100/3449]  eta: 1:18:34  lr: 0.000100  loss: 0.0297 (0.0316)  time: 3.4893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2110/3449]  eta: 1:17:59  lr: 0.000100  loss: 0.0274 (0.0316)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2120/3449]  eta: 1:17:24  lr: 0.000100  loss: 0.0270 (0.0316)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2130/3449]  eta: 1:16:49  lr: 0.000100  loss: 0.0297 (0.0316)  time: 3.4875  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2140/3449]  eta: 1:16:14  lr: 0.000100  loss: 0.0297 (0.0316)  time: 3.4881  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2150/3449]  eta: 1:15:39  lr: 0.000100  loss: 0.0297 (0.0316)  time: 3.4885  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2160/3449]  eta: 1:15:04  lr: 0.000100  loss: 0.0309 (0.0316)  time: 3.4901  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2170/3449]  eta: 1:14:29  lr: 0.000100  loss: 0.0278 (0.0315)  time: 3.4915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2180/3449]  eta: 1:13:54  lr: 0.000100  loss: 0.0312 (0.0316)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2190/3449]  eta: 1:13:19  lr: 0.000100  loss: 0.0310 (0.0316)  time: 3.4925  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2200/3449]  eta: 1:12:44  lr: 0.000100  loss: 0.0299 (0.0316)  time: 3.4946  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2210/3449]  eta: 1:12:09  lr: 0.000100  loss: 0.0294 (0.0315)  time: 3.4951  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2220/3449]  eta: 1:11:34  lr: 0.000100  loss: 0.0295 (0.0316)  time: 3.4938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2230/3449]  eta: 1:10:59  lr: 0.000100  loss: 0.0300 (0.0316)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2240/3449]  eta: 1:10:24  lr: 0.000100  loss: 0.0301 (0.0316)  time: 3.4918  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2250/3449]  eta: 1:09:49  lr: 0.000100  loss: 0.0315 (0.0316)  time: 3.4926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2260/3449]  eta: 1:09:14  lr: 0.000100  loss: 0.0281 (0.0315)  time: 3.4936  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2270/3449]  eta: 1:08:39  lr: 0.000100  loss: 0.0266 (0.0315)  time: 3.4929  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2280/3449]  eta: 1:08:05  lr: 0.000100  loss: 0.0283 (0.0315)  time: 3.4925  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2290/3449]  eta: 1:07:30  lr: 0.000100  loss: 0.0292 (0.0315)  time: 3.4930  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2300/3449]  eta: 1:06:55  lr: 0.000100  loss: 0.0288 (0.0315)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2310/3449]  eta: 1:06:20  lr: 0.000100  loss: 0.0289 (0.0315)  time: 3.4930  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2320/3449]  eta: 1:05:45  lr: 0.000100  loss: 0.0289 (0.0315)  time: 3.4925  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2330/3449]  eta: 1:05:10  lr: 0.000100  loss: 0.0295 (0.0315)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2340/3449]  eta: 1:04:35  lr: 0.000100  loss: 0.0312 (0.0315)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2350/3449]  eta: 1:04:00  lr: 0.000100  loss: 0.0313 (0.0315)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2360/3449]  eta: 1:03:25  lr: 0.000100  loss: 0.0309 (0.0315)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2370/3449]  eta: 1:02:50  lr: 0.000100  loss: 0.0306 (0.0315)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2380/3449]  eta: 1:02:15  lr: 0.000100  loss: 0.0296 (0.0315)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2390/3449]  eta: 1:01:40  lr: 0.000100  loss: 0.0301 (0.0315)  time: 3.4910  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2400/3449]  eta: 1:01:05  lr: 0.000100  loss: 0.0313 (0.0315)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2410/3449]  eta: 1:00:30  lr: 0.000100  loss: 0.0312 (0.0315)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2420/3449]  eta: 0:59:55  lr: 0.000100  loss: 0.0312 (0.0315)  time: 3.4892  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2430/3449]  eta: 0:59:20  lr: 0.000100  loss: 0.0304 (0.0315)  time: 3.4893  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2440/3449]  eta: 0:58:45  lr: 0.000100  loss: 0.0296 (0.0315)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2450/3449]  eta: 0:58:10  lr: 0.000100  loss: 0.0298 (0.0315)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2460/3449]  eta: 0:57:35  lr: 0.000100  loss: 0.0295 (0.0315)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2470/3449]  eta: 0:57:00  lr: 0.000100  loss: 0.0295 (0.0315)  time: 3.4912  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2480/3449]  eta: 0:56:25  lr: 0.000100  loss: 0.0318 (0.0315)  time: 3.4890  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2490/3449]  eta: 0:55:50  lr: 0.000100  loss: 0.0312 (0.0315)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2500/3449]  eta: 0:55:15  lr: 0.000100  loss: 0.0288 (0.0315)  time: 3.4899  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2510/3449]  eta: 0:54:40  lr: 0.000100  loss: 0.0305 (0.0316)  time: 3.4905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2520/3449]  eta: 0:54:06  lr: 0.000100  loss: 0.0305 (0.0315)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2530/3449]  eta: 0:53:31  lr: 0.000100  loss: 0.0288 (0.0315)  time: 3.4931  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2540/3449]  eta: 0:52:56  lr: 0.000100  loss: 0.0285 (0.0315)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2550/3449]  eta: 0:52:21  lr: 0.000100  loss: 0.0292 (0.0315)  time: 3.4928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2560/3449]  eta: 0:51:46  lr: 0.000100  loss: 0.0292 (0.0315)  time: 3.4943  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2570/3449]  eta: 0:51:11  lr: 0.000100  loss: 0.0298 (0.0315)  time: 3.4939  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2580/3449]  eta: 0:50:36  lr: 0.000100  loss: 0.0304 (0.0315)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2590/3449]  eta: 0:50:01  lr: 0.000100  loss: 0.0304 (0.0315)  time: 3.4917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2600/3449]  eta: 0:49:26  lr: 0.000100  loss: 0.0318 (0.0315)  time: 3.4912  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2610/3449]  eta: 0:48:51  lr: 0.000100  loss: 0.0308 (0.0315)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2620/3449]  eta: 0:48:16  lr: 0.000100  loss: 0.0305 (0.0315)  time: 3.4930  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2630/3449]  eta: 0:47:41  lr: 0.000100  loss: 0.0305 (0.0315)  time: 3.4940  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2640/3449]  eta: 0:47:06  lr: 0.000100  loss: 0.0300 (0.0315)  time: 3.4947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2650/3449]  eta: 0:46:31  lr: 0.000100  loss: 0.0277 (0.0315)  time: 3.4944  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:29]  [2660/3449]  eta: 0:45:56  lr: 0.000100  loss: 0.0293 (0.0315)  time: 3.4950  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2670/3449]  eta: 0:45:21  lr: 0.000100  loss: 0.0317 (0.0315)  time: 3.4950  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2680/3449]  eta: 0:44:46  lr: 0.000100  loss: 0.0317 (0.0316)  time: 3.4938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2690/3449]  eta: 0:44:12  lr: 0.000100  loss: 0.0306 (0.0315)  time: 3.4934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2700/3449]  eta: 0:43:37  lr: 0.000100  loss: 0.0318 (0.0316)  time: 3.4929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2710/3449]  eta: 0:43:02  lr: 0.000100  loss: 0.0335 (0.0316)  time: 3.4925  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2720/3449]  eta: 0:42:27  lr: 0.000100  loss: 0.0305 (0.0316)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2730/3449]  eta: 0:41:52  lr: 0.000100  loss: 0.0292 (0.0316)  time: 3.4903  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2740/3449]  eta: 0:41:17  lr: 0.000100  loss: 0.0301 (0.0316)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2750/3449]  eta: 0:40:42  lr: 0.000100  loss: 0.0315 (0.0316)  time: 3.4904  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2760/3449]  eta: 0:40:07  lr: 0.000100  loss: 0.0347 (0.0316)  time: 3.4909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2770/3449]  eta: 0:39:32  lr: 0.000100  loss: 0.0455 (0.0318)  time: 3.4904  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2780/3449]  eta: 0:38:57  lr: 0.000100  loss: 0.0523 (0.0318)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2790/3449]  eta: 0:38:22  lr: 0.000100  loss: 0.0415 (0.0319)  time: 3.4884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2800/3449]  eta: 0:37:47  lr: 0.000100  loss: 0.0368 (0.0319)  time: 3.4873  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2810/3449]  eta: 0:37:12  lr: 0.000100  loss: 0.0338 (0.0319)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2820/3449]  eta: 0:36:37  lr: 0.000100  loss: 0.0316 (0.0319)  time: 3.4909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2830/3449]  eta: 0:36:02  lr: 0.000100  loss: 0.0295 (0.0319)  time: 3.4923  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2840/3449]  eta: 0:35:27  lr: 0.000100  loss: 0.0291 (0.0319)  time: 3.4938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2850/3449]  eta: 0:34:52  lr: 0.000100  loss: 0.0295 (0.0319)  time: 3.4945  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2860/3449]  eta: 0:34:17  lr: 0.000100  loss: 0.0315 (0.0319)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2870/3449]  eta: 0:33:42  lr: 0.000100  loss: 0.0293 (0.0319)  time: 3.4917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2880/3449]  eta: 0:33:08  lr: 0.000100  loss: 0.0293 (0.0319)  time: 3.4921  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2890/3449]  eta: 0:32:33  lr: 0.000100  loss: 0.0277 (0.0319)  time: 3.4923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2900/3449]  eta: 0:31:58  lr: 0.000100  loss: 0.0284 (0.0319)  time: 3.4919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2910/3449]  eta: 0:31:23  lr: 0.000100  loss: 0.0309 (0.0319)  time: 3.4915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2920/3449]  eta: 0:30:48  lr: 0.000100  loss: 0.0313 (0.0319)  time: 3.4927  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2930/3449]  eta: 0:30:13  lr: 0.000100  loss: 0.0309 (0.0319)  time: 3.4931  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2940/3449]  eta: 0:29:38  lr: 0.000100  loss: 0.0311 (0.0319)  time: 3.4932  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [2950/3449]  eta: 0:29:03  lr: 0.000100  loss: 0.0318 (0.0319)  time: 3.4951  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2960/3449]  eta: 0:28:28  lr: 0.000100  loss: 0.0312 (0.0319)  time: 3.4946  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2970/3449]  eta: 0:27:53  lr: 0.000100  loss: 0.0294 (0.0319)  time: 3.4933  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2980/3449]  eta: 0:27:18  lr: 0.000100  loss: 0.0286 (0.0319)  time: 3.4925  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [2990/3449]  eta: 0:26:43  lr: 0.000100  loss: 0.0289 (0.0319)  time: 3.4909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3000/3449]  eta: 0:26:08  lr: 0.000100  loss: 0.0304 (0.0319)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3010/3449]  eta: 0:25:33  lr: 0.000100  loss: 0.0311 (0.0319)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3020/3449]  eta: 0:24:58  lr: 0.000100  loss: 0.0311 (0.0319)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3030/3449]  eta: 0:24:23  lr: 0.000100  loss: 0.0297 (0.0319)  time: 3.4897  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3040/3449]  eta: 0:23:48  lr: 0.000100  loss: 0.0297 (0.0319)  time: 3.4897  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3050/3449]  eta: 0:23:14  lr: 0.000100  loss: 0.0294 (0.0319)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3060/3449]  eta: 0:22:39  lr: 0.000100  loss: 0.0294 (0.0319)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3070/3449]  eta: 0:22:04  lr: 0.000100  loss: 0.0305 (0.0319)  time: 3.4893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3080/3449]  eta: 0:21:29  lr: 0.000100  loss: 0.0313 (0.0319)  time: 3.4910  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3090/3449]  eta: 0:20:54  lr: 0.000100  loss: 0.0288 (0.0319)  time: 3.4918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3100/3449]  eta: 0:20:19  lr: 0.000100  loss: 0.0276 (0.0319)  time: 3.4897  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3110/3449]  eta: 0:19:44  lr: 0.000100  loss: 0.0317 (0.0319)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3120/3449]  eta: 0:19:09  lr: 0.000100  loss: 0.0301 (0.0319)  time: 3.4893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3130/3449]  eta: 0:18:34  lr: 0.000100  loss: 0.0301 (0.0319)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3140/3449]  eta: 0:17:59  lr: 0.000100  loss: 0.0302 (0.0319)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3150/3449]  eta: 0:17:24  lr: 0.000100  loss: 0.0301 (0.0319)  time: 3.4861  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3160/3449]  eta: 0:16:49  lr: 0.000100  loss: 0.0306 (0.0319)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3170/3449]  eta: 0:16:14  lr: 0.000100  loss: 0.0306 (0.0319)  time: 3.4884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3180/3449]  eta: 0:15:39  lr: 0.000100  loss: 0.0309 (0.0319)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3190/3449]  eta: 0:15:04  lr: 0.000100  loss: 0.0311 (0.0319)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3200/3449]  eta: 0:14:29  lr: 0.000100  loss: 0.0324 (0.0319)  time: 3.4910  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3210/3449]  eta: 0:13:54  lr: 0.000100  loss: 0.0324 (0.0319)  time: 3.4919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [3220/3449]  eta: 0:13:20  lr: 0.000100  loss: 0.0303 (0.0319)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3230/3449]  eta: 0:12:45  lr: 0.000100  loss: 0.0306 (0.0319)  time: 3.4907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3240/3449]  eta: 0:12:10  lr: 0.000100  loss: 0.0337 (0.0319)  time: 3.4899  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3250/3449]  eta: 0:11:35  lr: 0.000100  loss: 0.0312 (0.0319)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3260/3449]  eta: 0:11:00  lr: 0.000100  loss: 0.0267 (0.0319)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3270/3449]  eta: 0:10:25  lr: 0.000100  loss: 0.0293 (0.0319)  time: 3.4892  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3280/3449]  eta: 0:09:50  lr: 0.000100  loss: 0.0283 (0.0319)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3290/3449]  eta: 0:09:15  lr: 0.000100  loss: 0.0265 (0.0319)  time: 3.4891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3300/3449]  eta: 0:08:40  lr: 0.000100  loss: 0.0287 (0.0319)  time: 3.4877  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29]  [3310/3449]  eta: 0:08:05  lr: 0.000100  loss: 0.0302 (0.0319)  time: 3.4875  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:29]  [3320/3449]  eta: 0:07:30  lr: 0.000100  loss: 0.0296 (0.0319)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3330/3449]  eta: 0:06:55  lr: 0.000100  loss: 0.0288 (0.0319)  time: 3.4914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3340/3449]  eta: 0:06:20  lr: 0.000100  loss: 0.0281 (0.0319)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3350/3449]  eta: 0:05:45  lr: 0.000100  loss: 0.0309 (0.0319)  time: 3.4923  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3360/3449]  eta: 0:05:10  lr: 0.000100  loss: 0.0315 (0.0319)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3370/3449]  eta: 0:04:35  lr: 0.000100  loss: 0.0320 (0.0319)  time: 3.4904  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3380/3449]  eta: 0:04:01  lr: 0.000100  loss: 0.0320 (0.0319)  time: 3.4896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3390/3449]  eta: 0:03:26  lr: 0.000100  loss: 0.0322 (0.0319)  time: 3.4890  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3400/3449]  eta: 0:02:51  lr: 0.000100  loss: 0.0314 (0.0319)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3410/3449]  eta: 0:02:16  lr: 0.000100  loss: 0.0324 (0.0319)  time: 3.4866  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3420/3449]  eta: 0:01:41  lr: 0.000100  loss: 0.0347 (0.0319)  time: 3.4884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3430/3449]  eta: 0:01:06  lr: 0.000100  loss: 0.0342 (0.0319)  time: 3.4885  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3440/3449]  eta: 0:00:31  lr: 0.000100  loss: 0.0310 (0.0319)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:29]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0293 (0.0319)  time: 3.4870  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:29] Total time: 3:20:48 (3.4934 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0293 (0.0319)\n",
      "Valid: [epoch:29]  [ 0/14]  eta: 0:04:25  loss: 0.0262 (0.0262)  time: 18.9738  data: 0.7293  max mem: 34968\n",
      "Valid: [epoch:29]  [13/14]  eta: 0:00:18  loss: 0.0269 (0.0272)  time: 18.3014  data: 0.0523  max mem: 34968\n",
      "Valid: [epoch:29] Total time: 0:04:16 (18.3176 s / it)\n",
      "Averaged stats: loss: 0.0269 (0.0272)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_29_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.027%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:30]  [   0/3449]  eta: 4:25:36  lr: 0.000100  loss: 0.0314 (0.0314)  time: 4.6205  data: 1.1662  max mem: 34968\n",
      "Train: [epoch:30]  [  10/3449]  eta: 3:25:33  lr: 0.000100  loss: 0.0324 (0.0321)  time: 3.5864  data: 0.1062  max mem: 34968\n",
      "Train: [epoch:30]  [  20/3449]  eta: 3:22:07  lr: 0.000100  loss: 0.0305 (0.0310)  time: 3.4826  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [  30/3449]  eta: 3:20:35  lr: 0.000100  loss: 0.0306 (0.0315)  time: 3.4837  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [  40/3449]  eta: 3:19:32  lr: 0.000100  loss: 0.0322 (0.0314)  time: 3.4859  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [  50/3449]  eta: 3:18:40  lr: 0.000100  loss: 0.0299 (0.0313)  time: 3.4871  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [  60/3449]  eta: 3:17:56  lr: 0.000100  loss: 0.0300 (0.0314)  time: 3.4886  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [  70/3449]  eta: 3:17:14  lr: 0.000100  loss: 0.0315 (0.0312)  time: 3.4901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [  80/3449]  eta: 3:16:35  lr: 0.000100  loss: 0.0274 (0.0307)  time: 3.4911  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [  90/3449]  eta: 3:15:56  lr: 0.000100  loss: 0.0302 (0.0309)  time: 3.4917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 100/3449]  eta: 3:15:19  lr: 0.000100  loss: 0.0320 (0.0312)  time: 3.4925  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 110/3449]  eta: 3:14:43  lr: 0.000100  loss: 0.0299 (0.0310)  time: 3.4945  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 120/3449]  eta: 3:14:07  lr: 0.000100  loss: 0.0287 (0.0306)  time: 3.4959  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 130/3449]  eta: 3:13:31  lr: 0.000100  loss: 0.0280 (0.0303)  time: 3.4945  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 140/3449]  eta: 3:12:55  lr: 0.000100  loss: 0.0307 (0.0305)  time: 3.4941  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 150/3449]  eta: 3:12:19  lr: 0.000100  loss: 0.0308 (0.0307)  time: 3.4941  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 160/3449]  eta: 3:11:43  lr: 0.000100  loss: 0.0317 (0.0309)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 170/3449]  eta: 3:11:07  lr: 0.000100  loss: 0.0329 (0.0312)  time: 3.4927  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 180/3449]  eta: 3:10:31  lr: 0.000100  loss: 0.0297 (0.0311)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 190/3449]  eta: 3:09:55  lr: 0.000100  loss: 0.0297 (0.0310)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 200/3449]  eta: 3:09:19  lr: 0.000100  loss: 0.0308 (0.0311)  time: 3.4929  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 210/3449]  eta: 3:08:44  lr: 0.000100  loss: 0.0326 (0.0311)  time: 3.4918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 220/3449]  eta: 3:08:08  lr: 0.000100  loss: 0.0331 (0.0313)  time: 3.4909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 230/3449]  eta: 3:07:32  lr: 0.000100  loss: 0.0337 (0.0314)  time: 3.4918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 240/3449]  eta: 3:06:57  lr: 0.000100  loss: 0.0322 (0.0314)  time: 3.4923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [ 250/3449]  eta: 3:06:22  lr: 0.000100  loss: 0.0318 (0.0317)  time: 3.4918  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [ 260/3449]  eta: 3:05:46  lr: 0.000100  loss: 0.0320 (0.0319)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 270/3449]  eta: 3:05:11  lr: 0.000100  loss: 0.0314 (0.0319)  time: 3.4936  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 280/3449]  eta: 3:04:36  lr: 0.000100  loss: 0.0300 (0.0319)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 290/3449]  eta: 3:04:01  lr: 0.000100  loss: 0.0315 (0.0319)  time: 3.4923  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 300/3449]  eta: 3:03:25  lr: 0.000100  loss: 0.0319 (0.0320)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 310/3449]  eta: 3:02:50  lr: 0.000100  loss: 0.0312 (0.0320)  time: 3.4911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 320/3449]  eta: 3:02:14  lr: 0.000100  loss: 0.0284 (0.0319)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 330/3449]  eta: 3:01:39  lr: 0.000100  loss: 0.0292 (0.0318)  time: 3.4879  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 340/3449]  eta: 3:01:03  lr: 0.000100  loss: 0.0293 (0.0318)  time: 3.4862  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 350/3449]  eta: 3:00:28  lr: 0.000100  loss: 0.0307 (0.0318)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 360/3449]  eta: 2:59:52  lr: 0.000100  loss: 0.0304 (0.0318)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 370/3449]  eta: 2:59:17  lr: 0.000100  loss: 0.0297 (0.0318)  time: 3.4892  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 380/3449]  eta: 2:58:42  lr: 0.000100  loss: 0.0307 (0.0320)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 390/3449]  eta: 2:58:07  lr: 0.000100  loss: 0.0349 (0.0321)  time: 3.4931  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [ 400/3449]  eta: 2:57:32  lr: 0.000100  loss: 0.0334 (0.0322)  time: 3.4949  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 410/3449]  eta: 2:56:57  lr: 0.000100  loss: 0.0334 (0.0323)  time: 3.4964  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 420/3449]  eta: 2:56:23  lr: 0.000100  loss: 0.0366 (0.0325)  time: 3.4972  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 430/3449]  eta: 2:55:48  lr: 0.000100  loss: 0.0346 (0.0325)  time: 3.4976  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 440/3449]  eta: 2:55:13  lr: 0.000100  loss: 0.0335 (0.0325)  time: 3.4976  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 450/3449]  eta: 2:54:38  lr: 0.000100  loss: 0.0335 (0.0326)  time: 3.4967  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 460/3449]  eta: 2:54:04  lr: 0.000100  loss: 0.0306 (0.0325)  time: 3.4953  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:30]  [ 470/3449]  eta: 2:53:29  lr: 0.000100  loss: 0.0306 (0.0325)  time: 3.4945  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 480/3449]  eta: 2:52:54  lr: 0.000100  loss: 0.0314 (0.0325)  time: 3.4939  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 490/3449]  eta: 2:52:19  lr: 0.000100  loss: 0.0293 (0.0325)  time: 3.4933  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [ 500/3449]  eta: 2:51:44  lr: 0.000100  loss: 0.0293 (0.0325)  time: 3.4938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 510/3449]  eta: 2:51:09  lr: 0.000100  loss: 0.0299 (0.0325)  time: 3.4928  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [ 520/3449]  eta: 2:50:34  lr: 0.000100  loss: 0.0300 (0.0324)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 530/3449]  eta: 2:49:59  lr: 0.000100  loss: 0.0300 (0.0324)  time: 3.4939  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 540/3449]  eta: 2:49:24  lr: 0.000100  loss: 0.0312 (0.0324)  time: 3.4941  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 550/3449]  eta: 2:48:49  lr: 0.000100  loss: 0.0327 (0.0324)  time: 3.4928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 560/3449]  eta: 2:48:14  lr: 0.000100  loss: 0.0318 (0.0324)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 570/3449]  eta: 2:47:38  lr: 0.000100  loss: 0.0295 (0.0324)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 580/3449]  eta: 2:47:03  lr: 0.000100  loss: 0.0318 (0.0324)  time: 3.4899  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 590/3449]  eta: 2:46:28  lr: 0.000100  loss: 0.0324 (0.0324)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 600/3449]  eta: 2:45:53  lr: 0.000100  loss: 0.0309 (0.0323)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 610/3449]  eta: 2:45:18  lr: 0.000100  loss: 0.0299 (0.0323)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 620/3449]  eta: 2:44:43  lr: 0.000100  loss: 0.0322 (0.0323)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 630/3449]  eta: 2:44:08  lr: 0.000100  loss: 0.0322 (0.0323)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 640/3449]  eta: 2:43:33  lr: 0.000100  loss: 0.0292 (0.0323)  time: 3.4910  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 650/3449]  eta: 2:42:58  lr: 0.000100  loss: 0.0302 (0.0323)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 660/3449]  eta: 2:42:23  lr: 0.000100  loss: 0.0310 (0.0323)  time: 3.4909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 670/3449]  eta: 2:41:48  lr: 0.000100  loss: 0.0331 (0.0324)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 680/3449]  eta: 2:41:13  lr: 0.000100  loss: 0.0333 (0.0323)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 690/3449]  eta: 2:40:38  lr: 0.000100  loss: 0.0313 (0.0324)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 700/3449]  eta: 2:40:03  lr: 0.000100  loss: 0.0322 (0.0324)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 710/3449]  eta: 2:39:28  lr: 0.000100  loss: 0.0322 (0.0324)  time: 3.4925  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [ 720/3449]  eta: 2:38:53  lr: 0.000100  loss: 0.0311 (0.0324)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 730/3449]  eta: 2:38:18  lr: 0.000100  loss: 0.0288 (0.0323)  time: 3.4967  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 740/3449]  eta: 2:37:43  lr: 0.000100  loss: 0.0288 (0.0323)  time: 3.4980  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 750/3449]  eta: 2:37:09  lr: 0.000100  loss: 0.0295 (0.0323)  time: 3.4974  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 760/3449]  eta: 2:36:34  lr: 0.000100  loss: 0.0319 (0.0323)  time: 3.4973  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 770/3449]  eta: 2:35:59  lr: 0.000100  loss: 0.0308 (0.0323)  time: 3.4969  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 780/3449]  eta: 2:35:24  lr: 0.000100  loss: 0.0317 (0.0323)  time: 3.4969  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 790/3449]  eta: 2:34:49  lr: 0.000100  loss: 0.0317 (0.0323)  time: 3.4978  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 800/3449]  eta: 2:34:15  lr: 0.000100  loss: 0.0330 (0.0324)  time: 3.4984  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 810/3449]  eta: 2:33:40  lr: 0.000100  loss: 0.0304 (0.0323)  time: 3.4985  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 820/3449]  eta: 2:33:05  lr: 0.000100  loss: 0.0302 (0.0323)  time: 3.4989  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 830/3449]  eta: 2:32:30  lr: 0.000100  loss: 0.0318 (0.0323)  time: 3.4982  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 840/3449]  eta: 2:31:56  lr: 0.000100  loss: 0.0302 (0.0323)  time: 3.4983  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 850/3449]  eta: 2:31:21  lr: 0.000100  loss: 0.0316 (0.0323)  time: 3.4987  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 860/3449]  eta: 2:30:46  lr: 0.000100  loss: 0.0308 (0.0323)  time: 3.4984  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 870/3449]  eta: 2:30:11  lr: 0.000100  loss: 0.0308 (0.0324)  time: 3.4987  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 880/3449]  eta: 2:29:36  lr: 0.000100  loss: 0.0378 (0.0325)  time: 3.4977  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 890/3449]  eta: 2:29:01  lr: 0.000100  loss: 0.0375 (0.0325)  time: 3.4963  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 900/3449]  eta: 2:28:26  lr: 0.000100  loss: 0.0362 (0.0326)  time: 3.4974  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 910/3449]  eta: 2:27:52  lr: 0.000100  loss: 0.0357 (0.0327)  time: 3.4985  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 920/3449]  eta: 2:27:17  lr: 0.000100  loss: 0.0323 (0.0327)  time: 3.4973  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 930/3449]  eta: 2:26:42  lr: 0.000100  loss: 0.0328 (0.0327)  time: 3.4961  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 940/3449]  eta: 2:26:07  lr: 0.000100  loss: 0.0308 (0.0327)  time: 3.4963  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 950/3449]  eta: 2:25:32  lr: 0.000100  loss: 0.0308 (0.0326)  time: 3.4968  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 960/3449]  eta: 2:24:57  lr: 0.000100  loss: 0.0328 (0.0327)  time: 3.4963  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 970/3449]  eta: 2:24:22  lr: 0.000100  loss: 0.0343 (0.0327)  time: 3.4958  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 980/3449]  eta: 2:23:47  lr: 0.000100  loss: 0.0323 (0.0327)  time: 3.4963  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [ 990/3449]  eta: 2:23:12  lr: 0.000100  loss: 0.0320 (0.0327)  time: 3.4962  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1000/3449]  eta: 2:22:38  lr: 0.000100  loss: 0.0305 (0.0327)  time: 3.4957  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1010/3449]  eta: 2:22:03  lr: 0.000100  loss: 0.0313 (0.0327)  time: 3.4944  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1020/3449]  eta: 2:21:28  lr: 0.000100  loss: 0.0315 (0.0327)  time: 3.4928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1030/3449]  eta: 2:20:53  lr: 0.000100  loss: 0.0310 (0.0327)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1040/3449]  eta: 2:20:18  lr: 0.000100  loss: 0.0304 (0.0327)  time: 3.4923  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1050/3449]  eta: 2:19:43  lr: 0.000100  loss: 0.0304 (0.0327)  time: 3.4929  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1060/3449]  eta: 2:19:08  lr: 0.000100  loss: 0.0305 (0.0327)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1070/3449]  eta: 2:18:33  lr: 0.000100  loss: 0.0289 (0.0326)  time: 3.4941  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1080/3449]  eta: 2:17:58  lr: 0.000100  loss: 0.0308 (0.0326)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1090/3449]  eta: 2:17:23  lr: 0.000100  loss: 0.0327 (0.0326)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1100/3449]  eta: 2:16:48  lr: 0.000100  loss: 0.0330 (0.0327)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1110/3449]  eta: 2:16:13  lr: 0.000100  loss: 0.0305 (0.0327)  time: 3.4940  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1120/3449]  eta: 2:15:38  lr: 0.000100  loss: 0.0310 (0.0327)  time: 3.4941  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:30]  [1130/3449]  eta: 2:15:03  lr: 0.000100  loss: 0.0327 (0.0327)  time: 3.4949  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1140/3449]  eta: 2:14:28  lr: 0.000100  loss: 0.0331 (0.0327)  time: 3.4958  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1150/3449]  eta: 2:13:53  lr: 0.000100  loss: 0.0320 (0.0327)  time: 3.4958  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1160/3449]  eta: 2:13:18  lr: 0.000100  loss: 0.0320 (0.0328)  time: 3.4957  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1170/3449]  eta: 2:12:43  lr: 0.000100  loss: 0.0315 (0.0327)  time: 3.4962  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1180/3449]  eta: 2:12:08  lr: 0.000100  loss: 0.0308 (0.0328)  time: 3.4965  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1190/3449]  eta: 2:11:33  lr: 0.000100  loss: 0.0314 (0.0328)  time: 3.4956  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1200/3449]  eta: 2:10:59  lr: 0.000100  loss: 0.0325 (0.0328)  time: 3.4939  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1210/3449]  eta: 2:10:24  lr: 0.000100  loss: 0.0301 (0.0328)  time: 3.4933  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1220/3449]  eta: 2:09:49  lr: 0.000100  loss: 0.0294 (0.0328)  time: 3.4933  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1230/3449]  eta: 2:09:14  lr: 0.000100  loss: 0.0322 (0.0328)  time: 3.4930  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1240/3449]  eta: 2:08:39  lr: 0.000100  loss: 0.0311 (0.0328)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1250/3449]  eta: 2:08:04  lr: 0.000100  loss: 0.0307 (0.0328)  time: 3.4947  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1260/3449]  eta: 2:07:29  lr: 0.000100  loss: 0.0322 (0.0328)  time: 3.4962  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1270/3449]  eta: 2:06:54  lr: 0.000100  loss: 0.0303 (0.0327)  time: 3.4972  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1280/3449]  eta: 2:06:19  lr: 0.000100  loss: 0.0271 (0.0327)  time: 3.4976  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1290/3449]  eta: 2:05:44  lr: 0.000100  loss: 0.0292 (0.0327)  time: 3.4974  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1300/3449]  eta: 2:05:09  lr: 0.000100  loss: 0.0314 (0.0327)  time: 3.4973  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1310/3449]  eta: 2:04:34  lr: 0.000100  loss: 0.0292 (0.0327)  time: 3.4976  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1320/3449]  eta: 2:03:59  lr: 0.000100  loss: 0.0295 (0.0327)  time: 3.4973  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1330/3449]  eta: 2:03:25  lr: 0.000100  loss: 0.0326 (0.0327)  time: 3.4973  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1340/3449]  eta: 2:02:50  lr: 0.000100  loss: 0.0313 (0.0327)  time: 3.4974  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1350/3449]  eta: 2:02:15  lr: 0.000100  loss: 0.0313 (0.0327)  time: 3.4964  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1360/3449]  eta: 2:01:40  lr: 0.000100  loss: 0.0308 (0.0327)  time: 3.4954  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1370/3449]  eta: 2:01:05  lr: 0.000100  loss: 0.0303 (0.0327)  time: 3.4952  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1380/3449]  eta: 2:00:30  lr: 0.000100  loss: 0.0294 (0.0327)  time: 3.4954  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1390/3449]  eta: 1:59:55  lr: 0.000100  loss: 0.0291 (0.0326)  time: 3.4957  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1400/3449]  eta: 1:59:20  lr: 0.000100  loss: 0.0291 (0.0326)  time: 3.4962  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1410/3449]  eta: 1:58:45  lr: 0.000100  loss: 0.0302 (0.0326)  time: 3.4964  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1420/3449]  eta: 1:58:10  lr: 0.000100  loss: 0.0306 (0.0326)  time: 3.4961  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1430/3449]  eta: 1:57:35  lr: 0.000100  loss: 0.0313 (0.0326)  time: 3.4960  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1440/3449]  eta: 1:57:00  lr: 0.000100  loss: 0.0305 (0.0326)  time: 3.4958  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1450/3449]  eta: 1:56:25  lr: 0.000100  loss: 0.0300 (0.0326)  time: 3.4952  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1460/3449]  eta: 1:55:50  lr: 0.000100  loss: 0.0295 (0.0326)  time: 3.4955  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1470/3449]  eta: 1:55:16  lr: 0.000100  loss: 0.0320 (0.0326)  time: 3.4952  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1480/3449]  eta: 1:54:41  lr: 0.000100  loss: 0.0317 (0.0326)  time: 3.4952  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1490/3449]  eta: 1:54:06  lr: 0.000100  loss: 0.0299 (0.0326)  time: 3.4960  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1500/3449]  eta: 1:53:31  lr: 0.000100  loss: 0.0306 (0.0326)  time: 3.4957  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1510/3449]  eta: 1:52:56  lr: 0.000100  loss: 0.0318 (0.0326)  time: 3.4938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1520/3449]  eta: 1:52:21  lr: 0.000100  loss: 0.0302 (0.0326)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1530/3449]  eta: 1:51:46  lr: 0.000100  loss: 0.0302 (0.0326)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1540/3449]  eta: 1:51:11  lr: 0.000100  loss: 0.0325 (0.0326)  time: 3.4914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1550/3449]  eta: 1:50:36  lr: 0.000100  loss: 0.0325 (0.0326)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1560/3449]  eta: 1:50:01  lr: 0.000100  loss: 0.0316 (0.0326)  time: 3.4930  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1570/3449]  eta: 1:49:26  lr: 0.000100  loss: 0.0321 (0.0326)  time: 3.4917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1580/3449]  eta: 1:48:51  lr: 0.000100  loss: 0.0326 (0.0326)  time: 3.4899  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1590/3449]  eta: 1:48:16  lr: 0.000100  loss: 0.0322 (0.0326)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1600/3449]  eta: 1:47:41  lr: 0.000100  loss: 0.0305 (0.0326)  time: 3.4899  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1610/3449]  eta: 1:47:06  lr: 0.000100  loss: 0.0304 (0.0326)  time: 3.4876  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1620/3449]  eta: 1:46:31  lr: 0.000100  loss: 0.0300 (0.0326)  time: 3.4878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1630/3449]  eta: 1:45:56  lr: 0.000100  loss: 0.0330 (0.0326)  time: 3.4877  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1640/3449]  eta: 1:45:21  lr: 0.000100  loss: 0.0315 (0.0326)  time: 3.4880  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1650/3449]  eta: 1:44:46  lr: 0.000100  loss: 0.0320 (0.0326)  time: 3.4882  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1660/3449]  eta: 1:44:11  lr: 0.000100  loss: 0.0320 (0.0326)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1670/3449]  eta: 1:43:36  lr: 0.000100  loss: 0.0319 (0.0326)  time: 3.4891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1680/3449]  eta: 1:43:01  lr: 0.000100  loss: 0.0321 (0.0327)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1690/3449]  eta: 1:42:26  lr: 0.000100  loss: 0.0318 (0.0326)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1700/3449]  eta: 1:41:51  lr: 0.000100  loss: 0.0309 (0.0326)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1710/3449]  eta: 1:41:16  lr: 0.000100  loss: 0.0301 (0.0326)  time: 3.4903  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1720/3449]  eta: 1:40:41  lr: 0.000100  loss: 0.0323 (0.0327)  time: 3.4918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1730/3449]  eta: 1:40:06  lr: 0.000100  loss: 0.0347 (0.0327)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1740/3449]  eta: 1:39:31  lr: 0.000100  loss: 0.0325 (0.0327)  time: 3.4927  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1750/3449]  eta: 1:38:56  lr: 0.000100  loss: 0.0316 (0.0327)  time: 3.4939  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1760/3449]  eta: 1:38:21  lr: 0.000100  loss: 0.0319 (0.0327)  time: 3.4946  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1770/3449]  eta: 1:37:46  lr: 0.000100  loss: 0.0317 (0.0327)  time: 3.4949  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1780/3449]  eta: 1:37:11  lr: 0.000100  loss: 0.0319 (0.0327)  time: 3.4942  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:30]  [1790/3449]  eta: 1:36:36  lr: 0.000100  loss: 0.0345 (0.0327)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1800/3449]  eta: 1:36:01  lr: 0.000100  loss: 0.0337 (0.0327)  time: 3.4912  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1810/3449]  eta: 1:35:26  lr: 0.000100  loss: 0.0349 (0.0327)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1820/3449]  eta: 1:34:51  lr: 0.000100  loss: 0.0356 (0.0327)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1830/3449]  eta: 1:34:16  lr: 0.000100  loss: 0.0351 (0.0328)  time: 3.4878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1840/3449]  eta: 1:33:41  lr: 0.000100  loss: 0.0385 (0.0328)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1850/3449]  eta: 1:33:06  lr: 0.000100  loss: 0.0368 (0.0328)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1860/3449]  eta: 1:32:31  lr: 0.000100  loss: 0.0321 (0.0328)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1870/3449]  eta: 1:31:56  lr: 0.000100  loss: 0.0309 (0.0328)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1880/3449]  eta: 1:31:22  lr: 0.000100  loss: 0.0316 (0.0328)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1890/3449]  eta: 1:30:47  lr: 0.000100  loss: 0.0311 (0.0328)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1900/3449]  eta: 1:30:12  lr: 0.000100  loss: 0.0296 (0.0328)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1910/3449]  eta: 1:29:37  lr: 0.000100  loss: 0.0296 (0.0328)  time: 3.4910  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1920/3449]  eta: 1:29:02  lr: 0.000100  loss: 0.0306 (0.0328)  time: 3.4892  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1930/3449]  eta: 1:28:27  lr: 0.000100  loss: 0.0294 (0.0328)  time: 3.4880  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1940/3449]  eta: 1:27:52  lr: 0.000100  loss: 0.0300 (0.0328)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1950/3449]  eta: 1:27:17  lr: 0.000100  loss: 0.0300 (0.0328)  time: 3.4890  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1960/3449]  eta: 1:26:42  lr: 0.000100  loss: 0.0311 (0.0328)  time: 3.4917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1970/3449]  eta: 1:26:07  lr: 0.000100  loss: 0.0321 (0.0328)  time: 3.4918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1980/3449]  eta: 1:25:32  lr: 0.000100  loss: 0.0317 (0.0328)  time: 3.4917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [1990/3449]  eta: 1:24:57  lr: 0.000100  loss: 0.0319 (0.0328)  time: 3.4899  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2000/3449]  eta: 1:24:22  lr: 0.000100  loss: 0.0292 (0.0328)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2010/3449]  eta: 1:23:47  lr: 0.000100  loss: 0.0270 (0.0328)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2020/3449]  eta: 1:23:12  lr: 0.000100  loss: 0.0304 (0.0328)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2030/3449]  eta: 1:22:37  lr: 0.000100  loss: 0.0311 (0.0328)  time: 3.4925  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2040/3449]  eta: 1:22:02  lr: 0.000100  loss: 0.0311 (0.0328)  time: 3.4923  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2050/3449]  eta: 1:21:27  lr: 0.000100  loss: 0.0297 (0.0328)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2060/3449]  eta: 1:20:52  lr: 0.000100  loss: 0.0300 (0.0328)  time: 3.4928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2070/3449]  eta: 1:20:17  lr: 0.000100  loss: 0.0333 (0.0328)  time: 3.4928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2080/3449]  eta: 1:19:42  lr: 0.000100  loss: 0.0335 (0.0328)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2090/3449]  eta: 1:19:07  lr: 0.000100  loss: 0.0321 (0.0328)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2100/3449]  eta: 1:18:32  lr: 0.000100  loss: 0.0308 (0.0328)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2110/3449]  eta: 1:17:57  lr: 0.000100  loss: 0.0289 (0.0328)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2120/3449]  eta: 1:17:22  lr: 0.000100  loss: 0.0274 (0.0328)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2130/3449]  eta: 1:16:48  lr: 0.000100  loss: 0.0310 (0.0328)  time: 3.4938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2140/3449]  eta: 1:16:13  lr: 0.000100  loss: 0.0310 (0.0328)  time: 3.4909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2150/3449]  eta: 1:15:38  lr: 0.000100  loss: 0.0301 (0.0328)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2160/3449]  eta: 1:15:03  lr: 0.000100  loss: 0.0314 (0.0328)  time: 3.4896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2170/3449]  eta: 1:14:28  lr: 0.000100  loss: 0.0310 (0.0328)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2180/3449]  eta: 1:13:53  lr: 0.000100  loss: 0.0286 (0.0327)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2190/3449]  eta: 1:13:18  lr: 0.000100  loss: 0.0314 (0.0328)  time: 3.4933  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2200/3449]  eta: 1:12:43  lr: 0.000100  loss: 0.0324 (0.0328)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2210/3449]  eta: 1:12:08  lr: 0.000100  loss: 0.0307 (0.0328)  time: 3.4918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2220/3449]  eta: 1:11:33  lr: 0.000100  loss: 0.0290 (0.0328)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2230/3449]  eta: 1:10:58  lr: 0.000100  loss: 0.0287 (0.0328)  time: 3.4892  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2240/3449]  eta: 1:10:23  lr: 0.000100  loss: 0.0313 (0.0328)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2250/3449]  eta: 1:09:49  lr: 0.000100  loss: 0.0342 (0.0328)  time: 3.5922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2260/3449]  eta: 1:09:15  lr: 0.000100  loss: 0.0339 (0.0328)  time: 3.7037  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2270/3449]  eta: 1:08:42  lr: 0.000100  loss: 0.0310 (0.0328)  time: 3.7269  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2280/3449]  eta: 1:08:08  lr: 0.000100  loss: 0.0302 (0.0328)  time: 3.7388  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2290/3449]  eta: 1:07:34  lr: 0.000100  loss: 0.0316 (0.0328)  time: 3.7090  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2300/3449]  eta: 1:07:00  lr: 0.000100  loss: 0.0307 (0.0328)  time: 3.7309  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2310/3449]  eta: 1:06:26  lr: 0.000100  loss: 0.0291 (0.0327)  time: 3.7534  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2320/3449]  eta: 1:05:53  lr: 0.000100  loss: 0.0303 (0.0327)  time: 3.7259  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2330/3449]  eta: 1:05:19  lr: 0.000100  loss: 0.0303 (0.0327)  time: 3.7285  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2340/3449]  eta: 1:04:44  lr: 0.000100  loss: 0.0312 (0.0328)  time: 3.7038  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2350/3449]  eta: 1:04:10  lr: 0.000100  loss: 0.0325 (0.0328)  time: 3.6956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2360/3449]  eta: 1:03:37  lr: 0.000100  loss: 0.0316 (0.0328)  time: 3.7628  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2370/3449]  eta: 1:03:03  lr: 0.000100  loss: 0.0304 (0.0328)  time: 3.7664  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2380/3449]  eta: 1:02:29  lr: 0.000100  loss: 0.0306 (0.0328)  time: 3.7128  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2390/3449]  eta: 1:01:55  lr: 0.000100  loss: 0.0310 (0.0328)  time: 3.7226  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2400/3449]  eta: 1:01:20  lr: 0.000100  loss: 0.0333 (0.0328)  time: 3.7361  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2410/3449]  eta: 1:00:46  lr: 0.000100  loss: 0.0345 (0.0328)  time: 3.7334  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2420/3449]  eta: 1:00:12  lr: 0.000100  loss: 0.0376 (0.0328)  time: 3.7440  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2430/3449]  eta: 0:59:38  lr: 0.000100  loss: 0.0376 (0.0329)  time: 3.7194  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2440/3449]  eta: 0:59:04  lr: 0.000100  loss: 0.0356 (0.0329)  time: 3.7002  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:30]  [2450/3449]  eta: 0:58:30  lr: 0.000100  loss: 0.0324 (0.0328)  time: 3.7406  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2460/3449]  eta: 0:57:55  lr: 0.000100  loss: 0.0297 (0.0328)  time: 3.7319  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2470/3449]  eta: 0:57:21  lr: 0.000100  loss: 0.0296 (0.0328)  time: 3.7119  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2480/3449]  eta: 0:56:47  lr: 0.000100  loss: 0.0307 (0.0328)  time: 3.7368  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2490/3449]  eta: 0:56:12  lr: 0.000100  loss: 0.0307 (0.0328)  time: 3.7243  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2500/3449]  eta: 0:55:38  lr: 0.000100  loss: 0.0297 (0.0328)  time: 3.7125  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2510/3449]  eta: 0:55:03  lr: 0.000100  loss: 0.0283 (0.0328)  time: 3.7201  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2520/3449]  eta: 0:54:29  lr: 0.000100  loss: 0.0310 (0.0328)  time: 3.7107  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2530/3449]  eta: 0:53:54  lr: 0.000100  loss: 0.0317 (0.0328)  time: 3.7206  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2540/3449]  eta: 0:53:20  lr: 0.000100  loss: 0.0307 (0.0328)  time: 3.7296  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2550/3449]  eta: 0:52:45  lr: 0.000100  loss: 0.0290 (0.0328)  time: 3.6957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2560/3449]  eta: 0:52:11  lr: 0.000100  loss: 0.0300 (0.0328)  time: 3.7147  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2570/3449]  eta: 0:51:36  lr: 0.000100  loss: 0.0340 (0.0328)  time: 3.7538  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2580/3449]  eta: 0:51:02  lr: 0.000100  loss: 0.0340 (0.0328)  time: 3.7170  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2590/3449]  eta: 0:50:27  lr: 0.000100  loss: 0.0325 (0.0328)  time: 3.7022  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2600/3449]  eta: 0:49:52  lr: 0.000100  loss: 0.0306 (0.0328)  time: 3.7093  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2610/3449]  eta: 0:49:18  lr: 0.000100  loss: 0.0294 (0.0328)  time: 3.7018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2620/3449]  eta: 0:48:43  lr: 0.000100  loss: 0.0312 (0.0328)  time: 3.7039  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2630/3449]  eta: 0:48:09  lr: 0.000100  loss: 0.0309 (0.0328)  time: 3.7405  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2640/3449]  eta: 0:47:34  lr: 0.000100  loss: 0.0294 (0.0328)  time: 3.7539  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2650/3449]  eta: 0:46:59  lr: 0.000100  loss: 0.0300 (0.0327)  time: 3.7276  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2660/3449]  eta: 0:46:25  lr: 0.000100  loss: 0.0304 (0.0327)  time: 3.7461  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2670/3449]  eta: 0:45:50  lr: 0.000100  loss: 0.0291 (0.0327)  time: 3.7420  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2680/3449]  eta: 0:45:15  lr: 0.000100  loss: 0.0301 (0.0327)  time: 3.7000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2690/3449]  eta: 0:44:40  lr: 0.000100  loss: 0.0323 (0.0327)  time: 3.7059  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2700/3449]  eta: 0:44:06  lr: 0.000100  loss: 0.0323 (0.0327)  time: 3.7299  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2710/3449]  eta: 0:43:31  lr: 0.000100  loss: 0.0333 (0.0328)  time: 3.7099  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2720/3449]  eta: 0:42:56  lr: 0.000100  loss: 0.0341 (0.0328)  time: 3.7264  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2730/3449]  eta: 0:42:21  lr: 0.000100  loss: 0.0330 (0.0328)  time: 3.7481  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2740/3449]  eta: 0:41:46  lr: 0.000100  loss: 0.0314 (0.0328)  time: 3.7040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2750/3449]  eta: 0:41:11  lr: 0.000100  loss: 0.0316 (0.0328)  time: 3.7083  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2760/3449]  eta: 0:40:36  lr: 0.000100  loss: 0.0310 (0.0328)  time: 3.7199  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2770/3449]  eta: 0:40:01  lr: 0.000100  loss: 0.0310 (0.0328)  time: 3.6905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2780/3449]  eta: 0:39:26  lr: 0.000100  loss: 0.0292 (0.0328)  time: 3.7055  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2790/3449]  eta: 0:38:52  lr: 0.000100  loss: 0.0284 (0.0328)  time: 3.7444  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2800/3449]  eta: 0:38:17  lr: 0.000100  loss: 0.0306 (0.0328)  time: 3.7465  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2810/3449]  eta: 0:37:42  lr: 0.000100  loss: 0.0306 (0.0328)  time: 3.7804  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2820/3449]  eta: 0:37:07  lr: 0.000100  loss: 0.0306 (0.0328)  time: 3.7304  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2830/3449]  eta: 0:36:32  lr: 0.000100  loss: 0.0306 (0.0328)  time: 3.6804  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2840/3449]  eta: 0:35:57  lr: 0.000100  loss: 0.0301 (0.0328)  time: 3.7346  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2850/3449]  eta: 0:35:22  lr: 0.000100  loss: 0.0297 (0.0327)  time: 3.7381  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2860/3449]  eta: 0:34:47  lr: 0.000100  loss: 0.0297 (0.0328)  time: 3.7419  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2870/3449]  eta: 0:34:12  lr: 0.000100  loss: 0.0320 (0.0328)  time: 3.7530  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2880/3449]  eta: 0:33:37  lr: 0.000100  loss: 0.0321 (0.0327)  time: 3.7288  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2890/3449]  eta: 0:33:01  lr: 0.000100  loss: 0.0307 (0.0327)  time: 3.7071  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2900/3449]  eta: 0:32:26  lr: 0.000100  loss: 0.0302 (0.0328)  time: 3.7189  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2910/3449]  eta: 0:31:51  lr: 0.000100  loss: 0.0303 (0.0327)  time: 3.7050  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2920/3449]  eta: 0:31:16  lr: 0.000100  loss: 0.0297 (0.0327)  time: 3.7046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2930/3449]  eta: 0:30:41  lr: 0.000100  loss: 0.0301 (0.0328)  time: 3.7364  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2940/3449]  eta: 0:30:06  lr: 0.000100  loss: 0.0341 (0.0328)  time: 3.7383  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [2950/3449]  eta: 0:29:30  lr: 0.000100  loss: 0.0321 (0.0328)  time: 3.7343  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2960/3449]  eta: 0:28:55  lr: 0.000100  loss: 0.0310 (0.0328)  time: 3.7497  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2970/3449]  eta: 0:28:20  lr: 0.000100  loss: 0.0313 (0.0328)  time: 3.7206  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2980/3449]  eta: 0:27:45  lr: 0.000100  loss: 0.0297 (0.0328)  time: 3.7000  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [2990/3449]  eta: 0:27:10  lr: 0.000100  loss: 0.0294 (0.0328)  time: 3.7378  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3000/3449]  eta: 0:26:34  lr: 0.000100  loss: 0.0304 (0.0328)  time: 3.7288  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3010/3449]  eta: 0:25:59  lr: 0.000100  loss: 0.0320 (0.0328)  time: 3.7190  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3020/3449]  eta: 0:25:24  lr: 0.000100  loss: 0.0320 (0.0328)  time: 3.7572  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3030/3449]  eta: 0:24:49  lr: 0.000100  loss: 0.0315 (0.0328)  time: 3.7355  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3040/3449]  eta: 0:24:13  lr: 0.000100  loss: 0.0310 (0.0328)  time: 3.7016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3050/3449]  eta: 0:23:38  lr: 0.000100  loss: 0.0273 (0.0328)  time: 3.7369  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3060/3449]  eta: 0:23:03  lr: 0.000100  loss: 0.0273 (0.0328)  time: 3.7327  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3070/3449]  eta: 0:22:27  lr: 0.000100  loss: 0.0283 (0.0328)  time: 3.7012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3080/3449]  eta: 0:21:52  lr: 0.000100  loss: 0.0308 (0.0328)  time: 3.7345  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3090/3449]  eta: 0:21:17  lr: 0.000100  loss: 0.0313 (0.0328)  time: 3.7414  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3100/3449]  eta: 0:20:41  lr: 0.000100  loss: 0.0313 (0.0328)  time: 3.7098  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:30]  [3110/3449]  eta: 0:20:06  lr: 0.000100  loss: 0.0334 (0.0328)  time: 3.7269  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3120/3449]  eta: 0:19:30  lr: 0.000100  loss: 0.0312 (0.0328)  time: 3.7160  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3130/3449]  eta: 0:18:55  lr: 0.000100  loss: 0.0295 (0.0328)  time: 3.7040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3140/3449]  eta: 0:18:20  lr: 0.000100  loss: 0.0290 (0.0328)  time: 3.7555  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3150/3449]  eta: 0:17:44  lr: 0.000100  loss: 0.0296 (0.0328)  time: 3.7460  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3160/3449]  eta: 0:17:09  lr: 0.000100  loss: 0.0314 (0.0328)  time: 3.7382  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3170/3449]  eta: 0:16:33  lr: 0.000100  loss: 0.0317 (0.0328)  time: 3.7798  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3180/3449]  eta: 0:15:58  lr: 0.000100  loss: 0.0307 (0.0328)  time: 3.7638  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3190/3449]  eta: 0:15:22  lr: 0.000100  loss: 0.0288 (0.0328)  time: 3.7108  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3200/3449]  eta: 0:14:47  lr: 0.000100  loss: 0.0293 (0.0328)  time: 3.7176  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3210/3449]  eta: 0:14:11  lr: 0.000100  loss: 0.0293 (0.0328)  time: 3.7320  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3220/3449]  eta: 0:13:36  lr: 0.000100  loss: 0.0317 (0.0328)  time: 3.7098  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:30]  [3230/3449]  eta: 0:13:00  lr: 0.000100  loss: 0.0325 (0.0328)  time: 3.7398  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:30]  [3240/3449]  eta: 0:12:25  lr: 0.000100  loss: 0.0330 (0.0328)  time: 3.7518  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [3250/3449]  eta: 0:11:49  lr: 0.000100  loss: 0.0330 (0.0328)  time: 3.6964  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [3260/3449]  eta: 0:11:14  lr: 0.000100  loss: 0.0322 (0.0328)  time: 3.7051  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3270/3449]  eta: 0:10:38  lr: 0.000100  loss: 0.0391 (0.0328)  time: 3.7352  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3280/3449]  eta: 0:10:02  lr: 0.000100  loss: 0.0395 (0.0328)  time: 3.7401  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3290/3449]  eta: 0:09:27  lr: 0.000100  loss: 0.0334 (0.0328)  time: 3.7308  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3300/3449]  eta: 0:08:51  lr: 0.000100  loss: 0.0304 (0.0328)  time: 3.7050  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3310/3449]  eta: 0:08:16  lr: 0.000100  loss: 0.0293 (0.0328)  time: 3.7049  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [3320/3449]  eta: 0:07:40  lr: 0.000100  loss: 0.0293 (0.0328)  time: 3.7080  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3330/3449]  eta: 0:07:04  lr: 0.000100  loss: 0.0327 (0.0328)  time: 3.7461  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3340/3449]  eta: 0:06:29  lr: 0.000100  loss: 0.0317 (0.0328)  time: 3.7317  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3350/3449]  eta: 0:05:53  lr: 0.000100  loss: 0.0291 (0.0328)  time: 3.6976  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3360/3449]  eta: 0:05:17  lr: 0.000100  loss: 0.0301 (0.0328)  time: 3.6970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3370/3449]  eta: 0:04:42  lr: 0.000100  loss: 0.0306 (0.0328)  time: 3.7040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3380/3449]  eta: 0:04:06  lr: 0.000100  loss: 0.0306 (0.0328)  time: 3.7412  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3390/3449]  eta: 0:03:30  lr: 0.000100  loss: 0.0314 (0.0328)  time: 3.7056  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3400/3449]  eta: 0:02:55  lr: 0.000100  loss: 0.0321 (0.0328)  time: 3.6752  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [3410/3449]  eta: 0:02:19  lr: 0.000100  loss: 0.0324 (0.0328)  time: 3.7361  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:30]  [3420/3449]  eta: 0:01:43  lr: 0.000100  loss: 0.0307 (0.0328)  time: 3.7409  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3430/3449]  eta: 0:01:07  lr: 0.000100  loss: 0.0307 (0.0328)  time: 3.7101  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3440/3449]  eta: 0:00:32  lr: 0.000100  loss: 0.0314 (0.0328)  time: 3.7276  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0312 (0.0328)  time: 3.7065  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:30] Total time: 3:25:28 (3.5746 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0312 (0.0328)\n",
      "Valid: [epoch:30]  [ 0/14]  eta: 0:04:24  loss: 0.0294 (0.0294)  time: 18.8799  data: 0.6310  max mem: 34968\n",
      "Valid: [epoch:30]  [13/14]  eta: 0:00:18  loss: 0.0278 (0.0281)  time: 18.2914  data: 0.0452  max mem: 34968\n",
      "Valid: [epoch:30] Total time: 0:04:16 (18.3048 s / it)\n",
      "Averaged stats: loss: 0.0278 (0.0281)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_30_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.028%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:31]  [   0/3449]  eta: 5:16:36  lr: 0.000100  loss: 0.0340 (0.0340)  time: 5.5079  data: 1.4737  max mem: 34968\n",
      "Train: [epoch:31]  [  10/3449]  eta: 3:44:28  lr: 0.000100  loss: 0.0340 (0.0369)  time: 3.9165  data: 0.1341  max mem: 34968\n",
      "Train: [epoch:31]  [  20/3449]  eta: 3:40:50  lr: 0.000100  loss: 0.0323 (0.0359)  time: 3.7819  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [  30/3449]  eta: 3:36:47  lr: 0.000100  loss: 0.0311 (0.0343)  time: 3.7430  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [  40/3449]  eta: 3:35:06  lr: 0.000100  loss: 0.0305 (0.0343)  time: 3.7039  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [  50/3449]  eta: 3:34:12  lr: 0.000100  loss: 0.0322 (0.0345)  time: 3.7454  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [  60/3449]  eta: 3:33:15  lr: 0.000100  loss: 0.0329 (0.0344)  time: 3.7540  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [  70/3449]  eta: 3:32:04  lr: 0.000100  loss: 0.0342 (0.0344)  time: 3.7257  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [  80/3449]  eta: 3:31:45  lr: 0.000100  loss: 0.0314 (0.0338)  time: 3.7591  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [  90/3449]  eta: 3:30:45  lr: 0.000100  loss: 0.0316 (0.0341)  time: 3.7615  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [ 100/3449]  eta: 3:29:52  lr: 0.000100  loss: 0.0315 (0.0339)  time: 3.7149  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [ 110/3449]  eta: 3:29:16  lr: 0.000100  loss: 0.0288 (0.0333)  time: 3.7422  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 120/3449]  eta: 3:28:23  lr: 0.000100  loss: 0.0297 (0.0334)  time: 3.7336  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 130/3449]  eta: 3:27:47  lr: 0.000100  loss: 0.0321 (0.0332)  time: 3.7331  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 140/3449]  eta: 3:27:18  lr: 0.000100  loss: 0.0321 (0.0335)  time: 3.7783  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 150/3449]  eta: 3:26:31  lr: 0.000100  loss: 0.0322 (0.0335)  time: 3.7544  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 160/3449]  eta: 3:25:44  lr: 0.000100  loss: 0.0322 (0.0335)  time: 3.7122  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 170/3449]  eta: 3:25:12  lr: 0.000100  loss: 0.0332 (0.0339)  time: 3.7448  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 180/3449]  eta: 3:24:25  lr: 0.000100  loss: 0.0310 (0.0336)  time: 3.7435  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 190/3449]  eta: 3:23:29  lr: 0.000100  loss: 0.0307 (0.0335)  time: 3.6747  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 200/3449]  eta: 3:22:34  lr: 0.000100  loss: 0.0323 (0.0336)  time: 3.6400  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 210/3449]  eta: 3:21:50  lr: 0.000100  loss: 0.0330 (0.0336)  time: 3.6665  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 220/3449]  eta: 3:21:19  lr: 0.000100  loss: 0.0329 (0.0336)  time: 3.7394  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 230/3449]  eta: 3:20:39  lr: 0.000100  loss: 0.0324 (0.0337)  time: 3.7520  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 240/3449]  eta: 3:20:07  lr: 0.000100  loss: 0.0325 (0.0338)  time: 3.7514  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 250/3449]  eta: 3:19:37  lr: 0.000100  loss: 0.0334 (0.0338)  time: 3.7932  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:31]  [ 260/3449]  eta: 3:18:50  lr: 0.000100  loss: 0.0325 (0.0339)  time: 3.7351  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [ 270/3449]  eta: 3:18:13  lr: 0.000100  loss: 0.0318 (0.0337)  time: 3.7024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 280/3449]  eta: 3:17:39  lr: 0.000100  loss: 0.0317 (0.0337)  time: 3.7564  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 290/3449]  eta: 3:16:55  lr: 0.000100  loss: 0.0319 (0.0336)  time: 3.7265  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 300/3449]  eta: 3:16:14  lr: 0.000100  loss: 0.0323 (0.0336)  time: 3.6968  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 310/3449]  eta: 3:15:41  lr: 0.000100  loss: 0.0324 (0.0337)  time: 3.7457  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 320/3449]  eta: 3:14:54  lr: 0.000100  loss: 0.0333 (0.0338)  time: 3.7106  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [ 330/3449]  eta: 3:14:08  lr: 0.000100  loss: 0.0367 (0.0340)  time: 3.6447  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 340/3449]  eta: 3:13:34  lr: 0.000100  loss: 0.0382 (0.0342)  time: 3.7099  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 350/3449]  eta: 3:12:54  lr: 0.000100  loss: 0.0382 (0.0344)  time: 3.7376  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 360/3449]  eta: 3:12:13  lr: 0.000100  loss: 0.0380 (0.0346)  time: 3.6993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 370/3449]  eta: 3:11:36  lr: 0.000100  loss: 0.0452 (0.0349)  time: 3.7177  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 380/3449]  eta: 3:10:57  lr: 0.000100  loss: 0.0408 (0.0351)  time: 3.7221  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 390/3449]  eta: 3:10:20  lr: 0.000100  loss: 0.0396 (0.0353)  time: 3.7203  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 400/3449]  eta: 3:09:48  lr: 0.000100  loss: 0.0365 (0.0353)  time: 3.7741  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 410/3449]  eta: 3:09:09  lr: 0.000100  loss: 0.0327 (0.0352)  time: 3.7609  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 420/3449]  eta: 3:08:31  lr: 0.000100  loss: 0.0329 (0.0352)  time: 3.7186  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 430/3449]  eta: 3:07:55  lr: 0.000100  loss: 0.0330 (0.0351)  time: 3.7371  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 440/3449]  eta: 3:07:13  lr: 0.000100  loss: 0.0309 (0.0350)  time: 3.7118  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 450/3449]  eta: 3:06:38  lr: 0.000100  loss: 0.0309 (0.0350)  time: 3.7217  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [ 460/3449]  eta: 3:06:02  lr: 0.000100  loss: 0.0317 (0.0349)  time: 3.7582  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [ 470/3449]  eta: 3:05:23  lr: 0.000100  loss: 0.0326 (0.0350)  time: 3.7317  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 480/3449]  eta: 3:04:49  lr: 0.000100  loss: 0.0326 (0.0349)  time: 3.7470  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 490/3449]  eta: 3:04:09  lr: 0.000100  loss: 0.0305 (0.0349)  time: 3.7406  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 500/3449]  eta: 3:03:29  lr: 0.000100  loss: 0.0307 (0.0348)  time: 3.6951  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 510/3449]  eta: 3:02:54  lr: 0.000100  loss: 0.0311 (0.0348)  time: 3.7307  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 520/3449]  eta: 3:02:16  lr: 0.000100  loss: 0.0307 (0.0347)  time: 3.7448  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 530/3449]  eta: 3:01:40  lr: 0.000100  loss: 0.0308 (0.0347)  time: 3.7379  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 540/3449]  eta: 3:01:07  lr: 0.000100  loss: 0.0313 (0.0346)  time: 3.7840  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 550/3449]  eta: 3:00:27  lr: 0.000100  loss: 0.0316 (0.0346)  time: 3.7500  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 560/3449]  eta: 2:59:49  lr: 0.000100  loss: 0.0332 (0.0346)  time: 3.7095  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 570/3449]  eta: 2:59:14  lr: 0.000100  loss: 0.0352 (0.0347)  time: 3.7532  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 580/3449]  eta: 2:58:31  lr: 0.000100  loss: 0.0347 (0.0348)  time: 3.6992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 590/3449]  eta: 2:57:50  lr: 0.000100  loss: 0.0346 (0.0348)  time: 3.6374  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 600/3449]  eta: 2:57:10  lr: 0.000100  loss: 0.0323 (0.0347)  time: 3.6630  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 610/3449]  eta: 2:56:32  lr: 0.000100  loss: 0.0325 (0.0348)  time: 3.7000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 620/3449]  eta: 2:55:56  lr: 0.000100  loss: 0.0337 (0.0348)  time: 3.7369  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 630/3449]  eta: 2:55:19  lr: 0.000100  loss: 0.0319 (0.0347)  time: 3.7442  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 640/3449]  eta: 2:54:39  lr: 0.000100  loss: 0.0338 (0.0347)  time: 3.7046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 650/3449]  eta: 2:54:01  lr: 0.000100  loss: 0.0319 (0.0347)  time: 3.6975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 660/3449]  eta: 2:53:21  lr: 0.000100  loss: 0.0327 (0.0348)  time: 3.6987  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 670/3449]  eta: 2:52:44  lr: 0.000100  loss: 0.0358 (0.0348)  time: 3.6997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 680/3449]  eta: 2:52:09  lr: 0.000100  loss: 0.0305 (0.0347)  time: 3.7542  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 690/3449]  eta: 2:51:30  lr: 0.000100  loss: 0.0305 (0.0347)  time: 3.7361  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 700/3449]  eta: 2:50:53  lr: 0.000100  loss: 0.0291 (0.0346)  time: 3.7093  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 710/3449]  eta: 2:50:17  lr: 0.000100  loss: 0.0324 (0.0346)  time: 3.7492  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 720/3449]  eta: 2:49:39  lr: 0.000100  loss: 0.0334 (0.0346)  time: 3.7379  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 730/3449]  eta: 2:49:01  lr: 0.000100  loss: 0.0330 (0.0346)  time: 3.7079  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 740/3449]  eta: 2:48:25  lr: 0.000100  loss: 0.0322 (0.0345)  time: 3.7445  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 750/3449]  eta: 2:47:46  lr: 0.000100  loss: 0.0313 (0.0345)  time: 3.7285  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 760/3449]  eta: 2:47:08  lr: 0.000100  loss: 0.0335 (0.0346)  time: 3.7011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 770/3449]  eta: 2:46:32  lr: 0.000100  loss: 0.0335 (0.0346)  time: 3.7359  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 780/3449]  eta: 2:45:55  lr: 0.000100  loss: 0.0322 (0.0346)  time: 3.7426  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 790/3449]  eta: 2:45:15  lr: 0.000100  loss: 0.0322 (0.0345)  time: 3.7033  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [ 800/3449]  eta: 2:44:39  lr: 0.000100  loss: 0.0315 (0.0346)  time: 3.7168  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 810/3449]  eta: 2:44:01  lr: 0.000100  loss: 0.0319 (0.0346)  time: 3.7246  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 820/3449]  eta: 2:43:21  lr: 0.000100  loss: 0.0301 (0.0346)  time: 3.6755  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 830/3449]  eta: 2:42:44  lr: 0.000100  loss: 0.0304 (0.0346)  time: 3.6929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 840/3449]  eta: 2:42:06  lr: 0.000100  loss: 0.0346 (0.0346)  time: 3.7149  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 850/3449]  eta: 2:41:25  lr: 0.000100  loss: 0.0313 (0.0346)  time: 3.6627  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 860/3449]  eta: 2:40:41  lr: 0.000100  loss: 0.0313 (0.0346)  time: 3.5555  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 870/3449]  eta: 2:39:57  lr: 0.000100  loss: 0.0306 (0.0346)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 880/3449]  eta: 2:39:13  lr: 0.000100  loss: 0.0324 (0.0345)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 890/3449]  eta: 2:38:29  lr: 0.000100  loss: 0.0327 (0.0346)  time: 3.4924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 900/3449]  eta: 2:37:46  lr: 0.000100  loss: 0.0332 (0.0346)  time: 3.4952  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 910/3449]  eta: 2:37:03  lr: 0.000100  loss: 0.0315 (0.0346)  time: 3.4981  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:31]  [ 920/3449]  eta: 2:36:20  lr: 0.000100  loss: 0.0320 (0.0346)  time: 3.4979  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 930/3449]  eta: 2:35:37  lr: 0.000100  loss: 0.0318 (0.0346)  time: 3.4977  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 940/3449]  eta: 2:34:54  lr: 0.000100  loss: 0.0318 (0.0345)  time: 3.4965  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 950/3449]  eta: 2:34:12  lr: 0.000100  loss: 0.0316 (0.0345)  time: 3.4966  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 960/3449]  eta: 2:33:29  lr: 0.000100  loss: 0.0317 (0.0345)  time: 3.4966  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 970/3449]  eta: 2:32:47  lr: 0.000100  loss: 0.0320 (0.0345)  time: 3.4952  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 980/3449]  eta: 2:32:05  lr: 0.000100  loss: 0.0321 (0.0345)  time: 3.4958  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [ 990/3449]  eta: 2:31:23  lr: 0.000100  loss: 0.0320 (0.0345)  time: 3.4957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1000/3449]  eta: 2:30:41  lr: 0.000100  loss: 0.0284 (0.0344)  time: 3.4954  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1010/3449]  eta: 2:30:00  lr: 0.000100  loss: 0.0303 (0.0344)  time: 3.4964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1020/3449]  eta: 2:29:18  lr: 0.000100  loss: 0.0325 (0.0344)  time: 3.4953  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1030/3449]  eta: 2:28:37  lr: 0.000100  loss: 0.0305 (0.0344)  time: 3.4946  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1040/3449]  eta: 2:27:55  lr: 0.000100  loss: 0.0313 (0.0344)  time: 3.4946  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1050/3449]  eta: 2:27:14  lr: 0.000100  loss: 0.0338 (0.0344)  time: 3.4934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1060/3449]  eta: 2:26:33  lr: 0.000100  loss: 0.0326 (0.0344)  time: 3.4938  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1070/3449]  eta: 2:25:52  lr: 0.000100  loss: 0.0311 (0.0344)  time: 3.4941  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1080/3449]  eta: 2:25:11  lr: 0.000100  loss: 0.0311 (0.0344)  time: 3.4933  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1090/3449]  eta: 2:24:30  lr: 0.000100  loss: 0.0354 (0.0344)  time: 3.4940  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1100/3449]  eta: 2:23:50  lr: 0.000100  loss: 0.0369 (0.0345)  time: 3.4940  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1110/3449]  eta: 2:23:09  lr: 0.000100  loss: 0.0326 (0.0344)  time: 3.4939  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1120/3449]  eta: 2:22:29  lr: 0.000100  loss: 0.0326 (0.0345)  time: 3.4943  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1130/3449]  eta: 2:21:49  lr: 0.000100  loss: 0.0337 (0.0344)  time: 3.4950  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1140/3449]  eta: 2:21:08  lr: 0.000100  loss: 0.0335 (0.0345)  time: 3.4975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1150/3449]  eta: 2:20:28  lr: 0.000100  loss: 0.0332 (0.0344)  time: 3.4982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1160/3449]  eta: 2:19:48  lr: 0.000100  loss: 0.0332 (0.0345)  time: 3.4981  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1170/3449]  eta: 2:19:08  lr: 0.000100  loss: 0.0321 (0.0344)  time: 3.4979  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1180/3449]  eta: 2:18:29  lr: 0.000100  loss: 0.0333 (0.0345)  time: 3.4963  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1190/3449]  eta: 2:17:49  lr: 0.000100  loss: 0.0338 (0.0345)  time: 3.4969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1200/3449]  eta: 2:17:09  lr: 0.000100  loss: 0.0306 (0.0344)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1210/3449]  eta: 2:16:30  lr: 0.000100  loss: 0.0292 (0.0344)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1220/3449]  eta: 2:15:50  lr: 0.000100  loss: 0.0296 (0.0344)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1230/3449]  eta: 2:15:11  lr: 0.000100  loss: 0.0310 (0.0344)  time: 3.4984  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1240/3449]  eta: 2:14:31  lr: 0.000100  loss: 0.0312 (0.0343)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1250/3449]  eta: 2:13:52  lr: 0.000100  loss: 0.0321 (0.0343)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1260/3449]  eta: 2:13:13  lr: 0.000100  loss: 0.0332 (0.0343)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1270/3449]  eta: 2:12:34  lr: 0.000100  loss: 0.0323 (0.0343)  time: 3.4989  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1280/3449]  eta: 2:11:55  lr: 0.000100  loss: 0.0318 (0.0343)  time: 3.4969  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1290/3449]  eta: 2:11:16  lr: 0.000100  loss: 0.0315 (0.0343)  time: 3.4971  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1300/3449]  eta: 2:10:37  lr: 0.000100  loss: 0.0379 (0.0344)  time: 3.4983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1310/3449]  eta: 2:09:58  lr: 0.000100  loss: 0.0438 (0.0345)  time: 3.4974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1320/3449]  eta: 2:09:19  lr: 0.000100  loss: 0.0375 (0.0345)  time: 3.4965  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1330/3449]  eta: 2:08:40  lr: 0.000100  loss: 0.0356 (0.0345)  time: 3.4950  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1340/3449]  eta: 2:08:01  lr: 0.000100  loss: 0.0368 (0.0346)  time: 3.4942  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1350/3449]  eta: 2:07:23  lr: 0.000100  loss: 0.0359 (0.0346)  time: 3.4949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1360/3449]  eta: 2:06:44  lr: 0.000100  loss: 0.0315 (0.0346)  time: 3.4930  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1370/3449]  eta: 2:06:05  lr: 0.000100  loss: 0.0315 (0.0346)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1380/3449]  eta: 2:05:27  lr: 0.000100  loss: 0.0320 (0.0345)  time: 3.4914  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1390/3449]  eta: 2:04:48  lr: 0.000100  loss: 0.0330 (0.0345)  time: 3.4920  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1400/3449]  eta: 2:04:10  lr: 0.000100  loss: 0.0348 (0.0345)  time: 3.4938  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1410/3449]  eta: 2:03:31  lr: 0.000100  loss: 0.0331 (0.0345)  time: 3.4955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1420/3449]  eta: 2:02:53  lr: 0.000100  loss: 0.0320 (0.0345)  time: 3.4953  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1430/3449]  eta: 2:02:15  lr: 0.000100  loss: 0.0317 (0.0345)  time: 3.4970  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1440/3449]  eta: 2:01:36  lr: 0.000100  loss: 0.0318 (0.0345)  time: 3.4973  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1450/3449]  eta: 2:00:58  lr: 0.000100  loss: 0.0327 (0.0345)  time: 3.4975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1460/3449]  eta: 2:00:20  lr: 0.000100  loss: 0.0307 (0.0345)  time: 3.4993  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1470/3449]  eta: 1:59:42  lr: 0.000100  loss: 0.0289 (0.0344)  time: 3.4989  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1480/3449]  eta: 1:59:04  lr: 0.000100  loss: 0.0289 (0.0345)  time: 3.4985  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1490/3449]  eta: 1:58:26  lr: 0.000100  loss: 0.0316 (0.0344)  time: 3.4987  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1500/3449]  eta: 1:57:48  lr: 0.000100  loss: 0.0316 (0.0344)  time: 3.4979  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1510/3449]  eta: 1:57:10  lr: 0.000100  loss: 0.0341 (0.0345)  time: 3.4973  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1520/3449]  eta: 1:56:32  lr: 0.000100  loss: 0.0330 (0.0344)  time: 3.4977  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1530/3449]  eta: 1:55:54  lr: 0.000100  loss: 0.0339 (0.0345)  time: 3.4984  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1540/3449]  eta: 1:55:17  lr: 0.000100  loss: 0.0346 (0.0345)  time: 3.4986  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1550/3449]  eta: 1:54:39  lr: 0.000100  loss: 0.0320 (0.0345)  time: 3.4968  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1560/3449]  eta: 1:54:01  lr: 0.000100  loss: 0.0338 (0.0346)  time: 3.4956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1570/3449]  eta: 1:53:23  lr: 0.000100  loss: 0.0323 (0.0345)  time: 3.4947  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:31]  [1580/3449]  eta: 1:52:46  lr: 0.000100  loss: 0.0314 (0.0345)  time: 3.4938  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1590/3449]  eta: 1:52:08  lr: 0.000100  loss: 0.0318 (0.0345)  time: 3.4952  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1600/3449]  eta: 1:51:30  lr: 0.000100  loss: 0.0303 (0.0345)  time: 3.4952  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1610/3449]  eta: 1:50:53  lr: 0.000100  loss: 0.0317 (0.0344)  time: 3.4956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1620/3449]  eta: 1:50:15  lr: 0.000100  loss: 0.0306 (0.0344)  time: 3.4961  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1630/3449]  eta: 1:49:38  lr: 0.000100  loss: 0.0294 (0.0344)  time: 3.4957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1640/3449]  eta: 1:49:00  lr: 0.000100  loss: 0.0323 (0.0344)  time: 3.4962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1650/3449]  eta: 1:48:23  lr: 0.000100  loss: 0.0331 (0.0344)  time: 3.4956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1660/3449]  eta: 1:47:45  lr: 0.000100  loss: 0.0310 (0.0344)  time: 3.4962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1670/3449]  eta: 1:47:08  lr: 0.000100  loss: 0.0304 (0.0344)  time: 3.4962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1680/3449]  eta: 1:46:31  lr: 0.000100  loss: 0.0308 (0.0343)  time: 3.4956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1690/3449]  eta: 1:45:53  lr: 0.000100  loss: 0.0315 (0.0343)  time: 3.4955  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1700/3449]  eta: 1:45:16  lr: 0.000100  loss: 0.0330 (0.0344)  time: 3.4936  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1710/3449]  eta: 1:44:39  lr: 0.000100  loss: 0.0338 (0.0344)  time: 3.4939  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1720/3449]  eta: 1:44:01  lr: 0.000100  loss: 0.0336 (0.0344)  time: 3.4959  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1730/3449]  eta: 1:43:24  lr: 0.000100  loss: 0.0333 (0.0344)  time: 3.4955  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1740/3449]  eta: 1:42:47  lr: 0.000100  loss: 0.0334 (0.0343)  time: 3.4956  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1750/3449]  eta: 1:42:10  lr: 0.000100  loss: 0.0327 (0.0344)  time: 3.4964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1760/3449]  eta: 1:41:33  lr: 0.000100  loss: 0.0311 (0.0344)  time: 3.4969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1770/3449]  eta: 1:40:55  lr: 0.000100  loss: 0.0307 (0.0344)  time: 3.4972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1780/3449]  eta: 1:40:18  lr: 0.000100  loss: 0.0311 (0.0344)  time: 3.4964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1790/3449]  eta: 1:39:41  lr: 0.000100  loss: 0.0316 (0.0344)  time: 3.4974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1800/3449]  eta: 1:39:04  lr: 0.000100  loss: 0.0356 (0.0344)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1810/3449]  eta: 1:38:27  lr: 0.000100  loss: 0.0338 (0.0344)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1820/3449]  eta: 1:37:50  lr: 0.000100  loss: 0.0299 (0.0344)  time: 3.5003  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1830/3449]  eta: 1:37:13  lr: 0.000100  loss: 0.0308 (0.0344)  time: 3.5002  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1840/3449]  eta: 1:36:36  lr: 0.000100  loss: 0.0311 (0.0343)  time: 3.5005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1850/3449]  eta: 1:35:59  lr: 0.000100  loss: 0.0321 (0.0344)  time: 3.5009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1860/3449]  eta: 1:35:22  lr: 0.000100  loss: 0.0330 (0.0344)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1870/3449]  eta: 1:34:46  lr: 0.000100  loss: 0.0299 (0.0343)  time: 3.4998  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1880/3449]  eta: 1:34:09  lr: 0.000100  loss: 0.0308 (0.0343)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1890/3449]  eta: 1:33:32  lr: 0.000100  loss: 0.0318 (0.0343)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1900/3449]  eta: 1:32:55  lr: 0.000100  loss: 0.0321 (0.0343)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1910/3449]  eta: 1:32:18  lr: 0.000100  loss: 0.0326 (0.0343)  time: 3.4977  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1920/3449]  eta: 1:31:41  lr: 0.000100  loss: 0.0323 (0.0343)  time: 3.4962  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1930/3449]  eta: 1:31:05  lr: 0.000100  loss: 0.0335 (0.0343)  time: 3.4959  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1940/3449]  eta: 1:30:28  lr: 0.000100  loss: 0.0326 (0.0343)  time: 3.4965  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1950/3449]  eta: 1:29:51  lr: 0.000100  loss: 0.0316 (0.0342)  time: 3.4968  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [1960/3449]  eta: 1:29:14  lr: 0.000100  loss: 0.0328 (0.0342)  time: 3.4946  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1970/3449]  eta: 1:28:38  lr: 0.000100  loss: 0.0313 (0.0342)  time: 3.4936  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1980/3449]  eta: 1:28:01  lr: 0.000100  loss: 0.0302 (0.0342)  time: 3.4936  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [1990/3449]  eta: 1:27:24  lr: 0.000100  loss: 0.0313 (0.0342)  time: 3.4934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2000/3449]  eta: 1:26:48  lr: 0.000100  loss: 0.0314 (0.0342)  time: 3.4947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2010/3449]  eta: 1:26:11  lr: 0.000100  loss: 0.0314 (0.0342)  time: 3.4940  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2020/3449]  eta: 1:25:34  lr: 0.000100  loss: 0.0291 (0.0342)  time: 3.4931  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2030/3449]  eta: 1:24:58  lr: 0.000100  loss: 0.0291 (0.0341)  time: 3.4924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2040/3449]  eta: 1:24:21  lr: 0.000100  loss: 0.0304 (0.0341)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2050/3449]  eta: 1:23:44  lr: 0.000100  loss: 0.0307 (0.0341)  time: 3.4945  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2060/3449]  eta: 1:23:08  lr: 0.000100  loss: 0.0313 (0.0341)  time: 3.4957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2070/3449]  eta: 1:22:31  lr: 0.000100  loss: 0.0314 (0.0341)  time: 3.4954  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2080/3449]  eta: 1:21:55  lr: 0.000100  loss: 0.0317 (0.0341)  time: 3.4973  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2090/3449]  eta: 1:21:18  lr: 0.000100  loss: 0.0321 (0.0341)  time: 3.4994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2100/3449]  eta: 1:20:42  lr: 0.000100  loss: 0.0315 (0.0341)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2110/3449]  eta: 1:20:05  lr: 0.000100  loss: 0.0314 (0.0341)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2120/3449]  eta: 1:19:29  lr: 0.000100  loss: 0.0336 (0.0341)  time: 3.5009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2130/3449]  eta: 1:18:53  lr: 0.000100  loss: 0.0338 (0.0341)  time: 3.5014  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2140/3449]  eta: 1:18:16  lr: 0.000100  loss: 0.0328 (0.0340)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2150/3449]  eta: 1:17:40  lr: 0.000100  loss: 0.0328 (0.0341)  time: 3.5016  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2160/3449]  eta: 1:17:03  lr: 0.000100  loss: 0.0317 (0.0340)  time: 3.5010  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2170/3449]  eta: 1:16:27  lr: 0.000100  loss: 0.0316 (0.0340)  time: 3.4992  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2180/3449]  eta: 1:15:51  lr: 0.000100  loss: 0.0326 (0.0340)  time: 3.4997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2190/3449]  eta: 1:15:14  lr: 0.000100  loss: 0.0345 (0.0340)  time: 3.5009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2200/3449]  eta: 1:14:38  lr: 0.000100  loss: 0.0347 (0.0340)  time: 3.5012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2210/3449]  eta: 1:14:02  lr: 0.000100  loss: 0.0303 (0.0340)  time: 3.5002  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2220/3449]  eta: 1:13:25  lr: 0.000100  loss: 0.0302 (0.0340)  time: 3.5000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2230/3449]  eta: 1:12:49  lr: 0.000100  loss: 0.0320 (0.0340)  time: 3.5003  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:31]  [2240/3449]  eta: 1:12:13  lr: 0.000100  loss: 0.0319 (0.0340)  time: 3.4991  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2250/3449]  eta: 1:11:36  lr: 0.000100  loss: 0.0319 (0.0340)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2260/3449]  eta: 1:11:00  lr: 0.000100  loss: 0.0324 (0.0340)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2270/3449]  eta: 1:10:24  lr: 0.000100  loss: 0.0319 (0.0340)  time: 3.5002  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2280/3449]  eta: 1:09:48  lr: 0.000100  loss: 0.0304 (0.0340)  time: 3.4999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2290/3449]  eta: 1:09:11  lr: 0.000100  loss: 0.0304 (0.0340)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2300/3449]  eta: 1:08:35  lr: 0.000100  loss: 0.0316 (0.0339)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2310/3449]  eta: 1:07:59  lr: 0.000100  loss: 0.0317 (0.0339)  time: 3.5016  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2320/3449]  eta: 1:07:23  lr: 0.000100  loss: 0.0333 (0.0340)  time: 3.5030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2330/3449]  eta: 1:06:46  lr: 0.000100  loss: 0.0334 (0.0340)  time: 3.5022  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2340/3449]  eta: 1:06:10  lr: 0.000100  loss: 0.0333 (0.0340)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2350/3449]  eta: 1:05:34  lr: 0.000100  loss: 0.0363 (0.0340)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2360/3449]  eta: 1:04:58  lr: 0.000100  loss: 0.0350 (0.0340)  time: 3.5018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2370/3449]  eta: 1:04:22  lr: 0.000100  loss: 0.0314 (0.0340)  time: 3.5013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2380/3449]  eta: 1:03:46  lr: 0.000100  loss: 0.0349 (0.0340)  time: 3.5009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2390/3449]  eta: 1:03:09  lr: 0.000100  loss: 0.0310 (0.0340)  time: 3.5007  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2400/3449]  eta: 1:02:33  lr: 0.000100  loss: 0.0287 (0.0340)  time: 3.5005  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2410/3449]  eta: 1:01:57  lr: 0.000100  loss: 0.0290 (0.0340)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2420/3449]  eta: 1:01:21  lr: 0.000100  loss: 0.0335 (0.0340)  time: 3.5013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2430/3449]  eta: 1:00:45  lr: 0.000100  loss: 0.0335 (0.0340)  time: 3.5005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2440/3449]  eta: 1:00:09  lr: 0.000100  loss: 0.0314 (0.0340)  time: 3.5008  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2450/3449]  eta: 0:59:33  lr: 0.000100  loss: 0.0304 (0.0340)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2460/3449]  eta: 0:58:57  lr: 0.000100  loss: 0.0302 (0.0340)  time: 3.5002  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2470/3449]  eta: 0:58:21  lr: 0.000100  loss: 0.0323 (0.0340)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2480/3449]  eta: 0:57:45  lr: 0.000100  loss: 0.0342 (0.0340)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2490/3449]  eta: 0:57:09  lr: 0.000100  loss: 0.0323 (0.0340)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2500/3449]  eta: 0:56:33  lr: 0.000100  loss: 0.0328 (0.0340)  time: 3.4993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2510/3449]  eta: 0:55:56  lr: 0.000100  loss: 0.0328 (0.0340)  time: 3.4983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2520/3449]  eta: 0:55:20  lr: 0.000100  loss: 0.0305 (0.0339)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2530/3449]  eta: 0:54:44  lr: 0.000100  loss: 0.0312 (0.0339)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2540/3449]  eta: 0:54:08  lr: 0.000100  loss: 0.0315 (0.0339)  time: 3.5002  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2550/3449]  eta: 0:53:32  lr: 0.000100  loss: 0.0317 (0.0339)  time: 3.5010  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2560/3449]  eta: 0:52:56  lr: 0.000100  loss: 0.0317 (0.0339)  time: 3.5010  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2570/3449]  eta: 0:52:20  lr: 0.000100  loss: 0.0326 (0.0339)  time: 3.5018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2580/3449]  eta: 0:51:44  lr: 0.000100  loss: 0.0342 (0.0340)  time: 3.5033  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2590/3449]  eta: 0:51:09  lr: 0.000100  loss: 0.0347 (0.0340)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2600/3449]  eta: 0:50:33  lr: 0.000100  loss: 0.0327 (0.0340)  time: 3.5036  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2610/3449]  eta: 0:49:57  lr: 0.000100  loss: 0.0321 (0.0340)  time: 3.5040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2620/3449]  eta: 0:49:21  lr: 0.000100  loss: 0.0329 (0.0340)  time: 3.5031  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2630/3449]  eta: 0:48:45  lr: 0.000100  loss: 0.0326 (0.0340)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2640/3449]  eta: 0:48:09  lr: 0.000100  loss: 0.0326 (0.0340)  time: 3.5032  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2650/3449]  eta: 0:47:33  lr: 0.000100  loss: 0.0319 (0.0340)  time: 3.5025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2660/3449]  eta: 0:46:57  lr: 0.000100  loss: 0.0319 (0.0340)  time: 3.5028  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2670/3449]  eta: 0:46:21  lr: 0.000100  loss: 0.0326 (0.0340)  time: 3.5024  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2680/3449]  eta: 0:45:45  lr: 0.000100  loss: 0.0329 (0.0340)  time: 3.5009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2690/3449]  eta: 0:45:09  lr: 0.000100  loss: 0.0319 (0.0340)  time: 3.5005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2700/3449]  eta: 0:44:33  lr: 0.000100  loss: 0.0312 (0.0340)  time: 3.5000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2710/3449]  eta: 0:43:57  lr: 0.000100  loss: 0.0302 (0.0339)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2720/3449]  eta: 0:43:22  lr: 0.000100  loss: 0.0317 (0.0339)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2730/3449]  eta: 0:42:46  lr: 0.000100  loss: 0.0311 (0.0340)  time: 3.4981  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2740/3449]  eta: 0:42:10  lr: 0.000100  loss: 0.0312 (0.0340)  time: 3.4998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2750/3449]  eta: 0:41:34  lr: 0.000100  loss: 0.0313 (0.0339)  time: 3.4999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2760/3449]  eta: 0:40:58  lr: 0.000100  loss: 0.0294 (0.0339)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2770/3449]  eta: 0:40:22  lr: 0.000100  loss: 0.0304 (0.0339)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2780/3449]  eta: 0:39:46  lr: 0.000100  loss: 0.0322 (0.0339)  time: 3.4976  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2790/3449]  eta: 0:39:11  lr: 0.000100  loss: 0.0325 (0.0339)  time: 3.4978  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2800/3449]  eta: 0:38:35  lr: 0.000100  loss: 0.0301 (0.0339)  time: 3.4968  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2810/3449]  eta: 0:37:59  lr: 0.000100  loss: 0.0301 (0.0339)  time: 3.4966  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2820/3449]  eta: 0:37:23  lr: 0.000100  loss: 0.0311 (0.0339)  time: 3.4972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2830/3449]  eta: 0:36:47  lr: 0.000100  loss: 0.0357 (0.0339)  time: 3.4967  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2840/3449]  eta: 0:36:11  lr: 0.000100  loss: 0.0332 (0.0339)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2850/3449]  eta: 0:35:36  lr: 0.000100  loss: 0.0320 (0.0339)  time: 3.4994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2860/3449]  eta: 0:35:00  lr: 0.000100  loss: 0.0328 (0.0339)  time: 3.5005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2870/3449]  eta: 0:34:24  lr: 0.000100  loss: 0.0326 (0.0339)  time: 3.5026  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2880/3449]  eta: 0:33:48  lr: 0.000100  loss: 0.0325 (0.0339)  time: 3.5027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2890/3449]  eta: 0:33:12  lr: 0.000100  loss: 0.0327 (0.0339)  time: 3.5046  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:31]  [2900/3449]  eta: 0:32:37  lr: 0.000100  loss: 0.0342 (0.0340)  time: 3.5064  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2910/3449]  eta: 0:32:01  lr: 0.000100  loss: 0.0342 (0.0340)  time: 3.5046  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2920/3449]  eta: 0:31:25  lr: 0.000100  loss: 0.0344 (0.0340)  time: 3.5051  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2930/3449]  eta: 0:30:49  lr: 0.000100  loss: 0.0353 (0.0340)  time: 3.5055  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2940/3449]  eta: 0:30:14  lr: 0.000100  loss: 0.0350 (0.0340)  time: 3.5041  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [2950/3449]  eta: 0:29:38  lr: 0.000100  loss: 0.0322 (0.0340)  time: 3.5044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2960/3449]  eta: 0:29:02  lr: 0.000100  loss: 0.0295 (0.0340)  time: 3.5037  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2970/3449]  eta: 0:28:26  lr: 0.000100  loss: 0.0318 (0.0340)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2980/3449]  eta: 0:27:51  lr: 0.000100  loss: 0.0318 (0.0340)  time: 3.5043  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [2990/3449]  eta: 0:27:15  lr: 0.000100  loss: 0.0325 (0.0340)  time: 3.5044  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3000/3449]  eta: 0:26:39  lr: 0.000100  loss: 0.0326 (0.0340)  time: 3.5034  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3010/3449]  eta: 0:26:04  lr: 0.000100  loss: 0.0330 (0.0340)  time: 3.5038  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3020/3449]  eta: 0:25:28  lr: 0.000100  loss: 0.0346 (0.0340)  time: 3.5041  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3030/3449]  eta: 0:24:52  lr: 0.000100  loss: 0.0317 (0.0340)  time: 3.5043  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3040/3449]  eta: 0:24:16  lr: 0.000100  loss: 0.0311 (0.0340)  time: 3.5035  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3050/3449]  eta: 0:23:41  lr: 0.000100  loss: 0.0316 (0.0340)  time: 3.5031  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3060/3449]  eta: 0:23:05  lr: 0.000100  loss: 0.0324 (0.0340)  time: 3.5025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3070/3449]  eta: 0:22:29  lr: 0.000100  loss: 0.0311 (0.0340)  time: 3.5016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3080/3449]  eta: 0:21:54  lr: 0.000100  loss: 0.0311 (0.0340)  time: 3.5019  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [3090/3449]  eta: 0:21:18  lr: 0.000100  loss: 0.0305 (0.0340)  time: 3.5008  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3100/3449]  eta: 0:20:42  lr: 0.000100  loss: 0.0308 (0.0340)  time: 3.5005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3110/3449]  eta: 0:20:07  lr: 0.000100  loss: 0.0328 (0.0340)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3120/3449]  eta: 0:19:31  lr: 0.000100  loss: 0.0323 (0.0340)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3130/3449]  eta: 0:18:55  lr: 0.000100  loss: 0.0293 (0.0339)  time: 3.4997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3140/3449]  eta: 0:18:20  lr: 0.000100  loss: 0.0299 (0.0339)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3150/3449]  eta: 0:17:44  lr: 0.000100  loss: 0.0315 (0.0339)  time: 3.4979  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3160/3449]  eta: 0:17:08  lr: 0.000100  loss: 0.0292 (0.0339)  time: 3.4984  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3170/3449]  eta: 0:16:33  lr: 0.000100  loss: 0.0298 (0.0339)  time: 3.4986  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [3180/3449]  eta: 0:15:57  lr: 0.000100  loss: 0.0333 (0.0339)  time: 3.4994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3190/3449]  eta: 0:15:21  lr: 0.000100  loss: 0.0339 (0.0339)  time: 3.4998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3200/3449]  eta: 0:14:46  lr: 0.000100  loss: 0.0341 (0.0340)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3210/3449]  eta: 0:14:10  lr: 0.000100  loss: 0.0323 (0.0340)  time: 3.5009  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [3220/3449]  eta: 0:13:34  lr: 0.000100  loss: 0.0305 (0.0339)  time: 3.5016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3230/3449]  eta: 0:12:59  lr: 0.000100  loss: 0.0316 (0.0340)  time: 3.5026  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3240/3449]  eta: 0:12:23  lr: 0.000100  loss: 0.0334 (0.0340)  time: 3.5025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3250/3449]  eta: 0:11:48  lr: 0.000100  loss: 0.0342 (0.0340)  time: 3.5016  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3260/3449]  eta: 0:11:12  lr: 0.000100  loss: 0.0310 (0.0339)  time: 3.5023  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3270/3449]  eta: 0:10:36  lr: 0.000100  loss: 0.0309 (0.0339)  time: 3.5020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3280/3449]  eta: 0:10:01  lr: 0.000100  loss: 0.0322 (0.0340)  time: 3.5023  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3290/3449]  eta: 0:09:25  lr: 0.000100  loss: 0.0335 (0.0340)  time: 3.5022  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3300/3449]  eta: 0:08:50  lr: 0.000100  loss: 0.0335 (0.0340)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3310/3449]  eta: 0:08:14  lr: 0.000100  loss: 0.0328 (0.0340)  time: 3.5023  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [3320/3449]  eta: 0:07:38  lr: 0.000100  loss: 0.0338 (0.0340)  time: 3.5023  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3330/3449]  eta: 0:07:03  lr: 0.000100  loss: 0.0338 (0.0340)  time: 3.5025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3340/3449]  eta: 0:06:27  lr: 0.000100  loss: 0.0355 (0.0340)  time: 3.5029  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [3350/3449]  eta: 0:05:52  lr: 0.000100  loss: 0.0355 (0.0340)  time: 3.5024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3360/3449]  eta: 0:05:16  lr: 0.000100  loss: 0.0355 (0.0340)  time: 3.5025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3370/3449]  eta: 0:04:40  lr: 0.000100  loss: 0.0341 (0.0340)  time: 3.5022  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3380/3449]  eta: 0:04:05  lr: 0.000100  loss: 0.0337 (0.0340)  time: 3.5016  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [3390/3449]  eta: 0:03:29  lr: 0.000100  loss: 0.0331 (0.0340)  time: 3.5009  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [3400/3449]  eta: 0:02:54  lr: 0.000100  loss: 0.0331 (0.0340)  time: 3.4997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3410/3449]  eta: 0:02:18  lr: 0.000100  loss: 0.0333 (0.0341)  time: 3.4997  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [3420/3449]  eta: 0:01:43  lr: 0.000100  loss: 0.0341 (0.0341)  time: 3.4996  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:31]  [3430/3449]  eta: 0:01:07  lr: 0.000100  loss: 0.0341 (0.0341)  time: 3.4989  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3440/3449]  eta: 0:00:31  lr: 0.000100  loss: 0.0330 (0.0341)  time: 3.4988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0322 (0.0341)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:31] Total time: 3:24:21 (3.5552 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0322 (0.0341)\n",
      "Valid: [epoch:31]  [ 0/14]  eta: 0:04:25  loss: 0.0305 (0.0305)  time: 18.9712  data: 0.6299  max mem: 34968\n",
      "Valid: [epoch:31]  [13/14]  eta: 0:00:18  loss: 0.0289 (0.0292)  time: 18.3995  data: 0.0452  max mem: 34968\n",
      "Valid: [epoch:31] Total time: 0:04:17 (18.4126 s / it)\n",
      "Averaged stats: loss: 0.0289 (0.0292)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_31_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.029%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:32]  [   0/3449]  eta: 4:37:00  lr: 0.000100  loss: 0.0380 (0.0380)  time: 4.8190  data: 1.3359  max mem: 34968\n",
      "Train: [epoch:32]  [  10/3449]  eta: 3:27:32  lr: 0.000100  loss: 0.0350 (0.0380)  time: 3.6208  data: 0.1216  max mem: 34968\n",
      "Train: [epoch:32]  [  20/3449]  eta: 3:23:42  lr: 0.000100  loss: 0.0347 (0.0362)  time: 3.5018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [  30/3449]  eta: 3:21:57  lr: 0.000100  loss: 0.0313 (0.0345)  time: 3.5020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [  40/3449]  eta: 3:20:47  lr: 0.000100  loss: 0.0298 (0.0355)  time: 3.5023  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:32]  [  50/3449]  eta: 3:19:50  lr: 0.000100  loss: 0.0328 (0.0359)  time: 3.5020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [  60/3449]  eta: 3:19:01  lr: 0.000100  loss: 0.0332 (0.0356)  time: 3.5020  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [  70/3449]  eta: 3:18:16  lr: 0.000100  loss: 0.0337 (0.0352)  time: 3.5028  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [  80/3449]  eta: 3:17:33  lr: 0.000100  loss: 0.0337 (0.0351)  time: 3.5023  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [  90/3449]  eta: 3:16:52  lr: 0.000100  loss: 0.0347 (0.0354)  time: 3.5019  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 100/3449]  eta: 3:16:11  lr: 0.000100  loss: 0.0362 (0.0359)  time: 3.5011  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 110/3449]  eta: 3:15:32  lr: 0.000100  loss: 0.0337 (0.0355)  time: 3.5016  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 120/3449]  eta: 3:14:54  lr: 0.000100  loss: 0.0316 (0.0353)  time: 3.5025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 130/3449]  eta: 3:14:16  lr: 0.000100  loss: 0.0344 (0.0352)  time: 3.5030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 140/3449]  eta: 3:13:39  lr: 0.000100  loss: 0.0363 (0.0356)  time: 3.5040  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 150/3449]  eta: 3:13:02  lr: 0.000100  loss: 0.0354 (0.0355)  time: 3.5024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 160/3449]  eta: 3:12:25  lr: 0.000100  loss: 0.0340 (0.0356)  time: 3.5012  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 170/3449]  eta: 3:11:48  lr: 0.000100  loss: 0.0336 (0.0353)  time: 3.5018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 180/3449]  eta: 3:11:12  lr: 0.000100  loss: 0.0302 (0.0350)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 190/3449]  eta: 3:10:35  lr: 0.000100  loss: 0.0302 (0.0348)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 200/3449]  eta: 3:09:58  lr: 0.000100  loss: 0.0321 (0.0349)  time: 3.4995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 210/3449]  eta: 3:09:22  lr: 0.000100  loss: 0.0326 (0.0347)  time: 3.4991  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 220/3449]  eta: 3:08:46  lr: 0.000100  loss: 0.0323 (0.0348)  time: 3.5002  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 230/3449]  eta: 3:08:09  lr: 0.000100  loss: 0.0329 (0.0349)  time: 3.5000  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 240/3449]  eta: 3:07:34  lr: 0.000100  loss: 0.0337 (0.0349)  time: 3.5013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 250/3449]  eta: 3:06:58  lr: 0.000100  loss: 0.0341 (0.0350)  time: 3.5013  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 260/3449]  eta: 3:06:22  lr: 0.000100  loss: 0.0316 (0.0350)  time: 3.5004  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 270/3449]  eta: 3:05:46  lr: 0.000100  loss: 0.0316 (0.0349)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 280/3449]  eta: 3:05:10  lr: 0.000100  loss: 0.0337 (0.0348)  time: 3.5005  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 290/3449]  eta: 3:04:35  lr: 0.000100  loss: 0.0334 (0.0347)  time: 3.5027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 300/3449]  eta: 3:04:00  lr: 0.000100  loss: 0.0338 (0.0348)  time: 3.5036  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 310/3449]  eta: 3:03:25  lr: 0.000100  loss: 0.0339 (0.0349)  time: 3.5031  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 320/3449]  eta: 3:02:49  lr: 0.000100  loss: 0.0341 (0.0348)  time: 3.5038  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 330/3449]  eta: 3:02:14  lr: 0.000100  loss: 0.0345 (0.0348)  time: 3.5035  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 340/3449]  eta: 3:01:39  lr: 0.000100  loss: 0.0333 (0.0346)  time: 3.5032  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 350/3449]  eta: 3:01:03  lr: 0.000100  loss: 0.0319 (0.0345)  time: 3.5024  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 360/3449]  eta: 3:00:28  lr: 0.000100  loss: 0.0346 (0.0347)  time: 3.5019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 370/3449]  eta: 2:59:53  lr: 0.000100  loss: 0.0350 (0.0346)  time: 3.5022  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 380/3449]  eta: 2:59:17  lr: 0.000100  loss: 0.0323 (0.0345)  time: 3.5012  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 390/3449]  eta: 2:58:42  lr: 0.000100  loss: 0.0323 (0.0345)  time: 3.5006  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 400/3449]  eta: 2:58:06  lr: 0.000100  loss: 0.0321 (0.0345)  time: 3.5010  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 410/3449]  eta: 2:57:31  lr: 0.000100  loss: 0.0338 (0.0346)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 420/3449]  eta: 2:56:56  lr: 0.000100  loss: 0.0337 (0.0347)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 430/3449]  eta: 2:56:20  lr: 0.000100  loss: 0.0317 (0.0346)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 440/3449]  eta: 2:55:45  lr: 0.000100  loss: 0.0329 (0.0347)  time: 3.5004  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 450/3449]  eta: 2:55:10  lr: 0.000100  loss: 0.0364 (0.0347)  time: 3.4996  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 460/3449]  eta: 2:54:34  lr: 0.000100  loss: 0.0323 (0.0346)  time: 3.4995  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 470/3449]  eta: 2:53:59  lr: 0.000100  loss: 0.0335 (0.0347)  time: 3.5004  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 480/3449]  eta: 2:53:24  lr: 0.000100  loss: 0.0344 (0.0348)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 490/3449]  eta: 2:52:48  lr: 0.000100  loss: 0.0327 (0.0347)  time: 3.4998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 500/3449]  eta: 2:52:13  lr: 0.000100  loss: 0.0303 (0.0346)  time: 3.4998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 510/3449]  eta: 2:51:38  lr: 0.000100  loss: 0.0320 (0.0346)  time: 3.4993  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 520/3449]  eta: 2:51:02  lr: 0.000100  loss: 0.0328 (0.0347)  time: 3.4990  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 530/3449]  eta: 2:50:27  lr: 0.000100  loss: 0.0339 (0.0347)  time: 3.4990  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 540/3449]  eta: 2:49:52  lr: 0.000100  loss: 0.0326 (0.0346)  time: 3.5002  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 550/3449]  eta: 2:49:17  lr: 0.000100  loss: 0.0325 (0.0346)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 560/3449]  eta: 2:48:42  lr: 0.000100  loss: 0.0341 (0.0346)  time: 3.5007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 570/3449]  eta: 2:48:06  lr: 0.000100  loss: 0.0326 (0.0345)  time: 3.5000  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 580/3449]  eta: 2:47:31  lr: 0.000100  loss: 0.0304 (0.0344)  time: 3.4997  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 590/3449]  eta: 2:46:56  lr: 0.000100  loss: 0.0333 (0.0344)  time: 3.5007  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 600/3449]  eta: 2:46:21  lr: 0.000100  loss: 0.0333 (0.0344)  time: 3.5019  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 610/3449]  eta: 2:45:46  lr: 0.000100  loss: 0.0339 (0.0345)  time: 3.5011  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 620/3449]  eta: 2:45:10  lr: 0.000100  loss: 0.0334 (0.0344)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 630/3449]  eta: 2:44:35  lr: 0.000100  loss: 0.0327 (0.0345)  time: 3.4980  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 640/3449]  eta: 2:44:00  lr: 0.000100  loss: 0.0318 (0.0344)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 650/3449]  eta: 2:43:25  lr: 0.000100  loss: 0.0333 (0.0345)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 660/3449]  eta: 2:42:49  lr: 0.000100  loss: 0.0335 (0.0345)  time: 3.4988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 670/3449]  eta: 2:42:14  lr: 0.000100  loss: 0.0354 (0.0346)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 680/3449]  eta: 2:41:39  lr: 0.000100  loss: 0.0354 (0.0346)  time: 3.4982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 690/3449]  eta: 2:41:04  lr: 0.000100  loss: 0.0309 (0.0346)  time: 3.4993  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 700/3449]  eta: 2:40:29  lr: 0.000100  loss: 0.0327 (0.0346)  time: 3.4995  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:32]  [ 710/3449]  eta: 2:39:54  lr: 0.000100  loss: 0.0357 (0.0347)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 720/3449]  eta: 2:39:18  lr: 0.000100  loss: 0.0354 (0.0346)  time: 3.4993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 730/3449]  eta: 2:38:43  lr: 0.000100  loss: 0.0326 (0.0346)  time: 3.4991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 740/3449]  eta: 2:38:08  lr: 0.000100  loss: 0.0326 (0.0347)  time: 3.4995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 750/3449]  eta: 2:37:33  lr: 0.000100  loss: 0.0327 (0.0347)  time: 3.5004  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 760/3449]  eta: 2:36:58  lr: 0.000100  loss: 0.0323 (0.0346)  time: 3.4997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 770/3449]  eta: 2:36:23  lr: 0.000100  loss: 0.0325 (0.0347)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 780/3449]  eta: 2:35:48  lr: 0.000100  loss: 0.0337 (0.0347)  time: 3.5026  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 790/3449]  eta: 2:35:13  lr: 0.000100  loss: 0.0335 (0.0348)  time: 3.5031  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 800/3449]  eta: 2:34:38  lr: 0.000100  loss: 0.0389 (0.0349)  time: 3.5042  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 810/3449]  eta: 2:34:03  lr: 0.000100  loss: 0.0376 (0.0349)  time: 3.5024  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 820/3449]  eta: 2:33:28  lr: 0.000100  loss: 0.0416 (0.0351)  time: 3.5022  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 830/3449]  eta: 2:32:53  lr: 0.000100  loss: 0.0428 (0.0351)  time: 3.5033  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 840/3449]  eta: 2:32:18  lr: 0.000100  loss: 0.0364 (0.0351)  time: 3.5063  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 850/3449]  eta: 2:31:43  lr: 0.000100  loss: 0.0333 (0.0351)  time: 3.5054  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 860/3449]  eta: 2:31:08  lr: 0.000100  loss: 0.0342 (0.0351)  time: 3.5004  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 870/3449]  eta: 2:30:33  lr: 0.000100  loss: 0.0309 (0.0351)  time: 3.4993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 880/3449]  eta: 2:29:57  lr: 0.000100  loss: 0.0336 (0.0351)  time: 3.4987  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 890/3449]  eta: 2:29:22  lr: 0.000100  loss: 0.0336 (0.0352)  time: 3.4985  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 900/3449]  eta: 2:28:47  lr: 0.000100  loss: 0.0340 (0.0352)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 910/3449]  eta: 2:28:12  lr: 0.000100  loss: 0.0340 (0.0351)  time: 3.4978  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 920/3449]  eta: 2:27:37  lr: 0.000100  loss: 0.0341 (0.0352)  time: 3.4971  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 930/3449]  eta: 2:27:02  lr: 0.000100  loss: 0.0337 (0.0351)  time: 3.4977  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 940/3449]  eta: 2:26:27  lr: 0.000100  loss: 0.0317 (0.0351)  time: 3.4976  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 950/3449]  eta: 2:25:51  lr: 0.000100  loss: 0.0351 (0.0352)  time: 3.4977  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 960/3449]  eta: 2:25:16  lr: 0.000100  loss: 0.0363 (0.0352)  time: 3.4972  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [ 970/3449]  eta: 2:24:41  lr: 0.000100  loss: 0.0346 (0.0352)  time: 3.4968  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 980/3449]  eta: 2:24:06  lr: 0.000100  loss: 0.0337 (0.0352)  time: 3.4979  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [ 990/3449]  eta: 2:23:31  lr: 0.000100  loss: 0.0327 (0.0352)  time: 3.4976  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1000/3449]  eta: 2:22:56  lr: 0.000100  loss: 0.0314 (0.0352)  time: 3.4980  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1010/3449]  eta: 2:22:21  lr: 0.000100  loss: 0.0318 (0.0352)  time: 3.4988  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1020/3449]  eta: 2:21:46  lr: 0.000100  loss: 0.0318 (0.0351)  time: 3.4978  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1030/3449]  eta: 2:21:10  lr: 0.000100  loss: 0.0304 (0.0351)  time: 3.4976  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1040/3449]  eta: 2:20:35  lr: 0.000100  loss: 0.0304 (0.0351)  time: 3.4979  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1050/3449]  eta: 2:20:00  lr: 0.000100  loss: 0.0303 (0.0350)  time: 3.4970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1060/3449]  eta: 2:19:25  lr: 0.000100  loss: 0.0303 (0.0350)  time: 3.4969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1070/3449]  eta: 2:18:50  lr: 0.000100  loss: 0.0324 (0.0350)  time: 3.4979  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1080/3449]  eta: 2:18:15  lr: 0.000100  loss: 0.0302 (0.0349)  time: 3.4985  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1090/3449]  eta: 2:17:40  lr: 0.000100  loss: 0.0313 (0.0349)  time: 3.4973  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1100/3449]  eta: 2:17:05  lr: 0.000100  loss: 0.0326 (0.0349)  time: 3.4980  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1110/3449]  eta: 2:16:30  lr: 0.000100  loss: 0.0323 (0.0349)  time: 3.4979  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1120/3449]  eta: 2:15:54  lr: 0.000100  loss: 0.0338 (0.0350)  time: 3.4964  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1130/3449]  eta: 2:15:19  lr: 0.000100  loss: 0.0346 (0.0350)  time: 3.4975  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1140/3449]  eta: 2:14:44  lr: 0.000100  loss: 0.0341 (0.0350)  time: 3.4984  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1150/3449]  eta: 2:14:09  lr: 0.000100  loss: 0.0337 (0.0349)  time: 3.4989  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1160/3449]  eta: 2:13:34  lr: 0.000100  loss: 0.0337 (0.0350)  time: 3.4988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1170/3449]  eta: 2:12:59  lr: 0.000100  loss: 0.0328 (0.0350)  time: 3.4987  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1180/3449]  eta: 2:12:24  lr: 0.000100  loss: 0.0315 (0.0349)  time: 3.4991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1190/3449]  eta: 2:11:49  lr: 0.000100  loss: 0.0346 (0.0350)  time: 3.4967  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1200/3449]  eta: 2:11:14  lr: 0.000100  loss: 0.0341 (0.0349)  time: 3.4944  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1210/3449]  eta: 2:10:39  lr: 0.000100  loss: 0.0318 (0.0349)  time: 3.4942  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1220/3449]  eta: 2:10:03  lr: 0.000100  loss: 0.0307 (0.0349)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1230/3449]  eta: 2:09:28  lr: 0.000100  loss: 0.0323 (0.0349)  time: 3.4921  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1240/3449]  eta: 2:08:53  lr: 0.000100  loss: 0.0338 (0.0349)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1250/3449]  eta: 2:08:18  lr: 0.000100  loss: 0.0331 (0.0349)  time: 3.4890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1260/3449]  eta: 2:07:43  lr: 0.000100  loss: 0.0345 (0.0349)  time: 3.4874  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1270/3449]  eta: 2:07:07  lr: 0.000100  loss: 0.0347 (0.0349)  time: 3.4863  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1280/3449]  eta: 2:06:32  lr: 0.000100  loss: 0.0316 (0.0349)  time: 3.4844  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1290/3449]  eta: 2:05:57  lr: 0.000100  loss: 0.0324 (0.0349)  time: 3.4836  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1300/3449]  eta: 2:05:21  lr: 0.000100  loss: 0.0322 (0.0349)  time: 3.4842  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1310/3449]  eta: 2:04:46  lr: 0.000100  loss: 0.0321 (0.0349)  time: 3.4838  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1320/3449]  eta: 2:04:11  lr: 0.000100  loss: 0.0308 (0.0348)  time: 3.4841  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1330/3449]  eta: 2:03:36  lr: 0.000100  loss: 0.0326 (0.0348)  time: 3.4853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1340/3449]  eta: 2:03:00  lr: 0.000100  loss: 0.0345 (0.0348)  time: 3.4857  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1350/3449]  eta: 2:02:25  lr: 0.000100  loss: 0.0323 (0.0348)  time: 3.4847  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1360/3449]  eta: 2:01:50  lr: 0.000100  loss: 0.0323 (0.0348)  time: 3.4844  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:32]  [1370/3449]  eta: 2:01:15  lr: 0.000100  loss: 0.0345 (0.0348)  time: 3.4845  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1380/3449]  eta: 2:00:40  lr: 0.000100  loss: 0.0337 (0.0348)  time: 3.4852  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1390/3449]  eta: 2:00:04  lr: 0.000100  loss: 0.0344 (0.0348)  time: 3.4865  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1400/3449]  eta: 1:59:29  lr: 0.000100  loss: 0.0345 (0.0348)  time: 3.4867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1410/3449]  eta: 1:58:54  lr: 0.000100  loss: 0.0331 (0.0349)  time: 3.4872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1420/3449]  eta: 1:58:19  lr: 0.000100  loss: 0.0343 (0.0349)  time: 3.4874  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1430/3449]  eta: 1:57:44  lr: 0.000100  loss: 0.0319 (0.0348)  time: 3.4870  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1440/3449]  eta: 1:57:09  lr: 0.000100  loss: 0.0304 (0.0348)  time: 3.4867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1450/3449]  eta: 1:56:33  lr: 0.000100  loss: 0.0322 (0.0348)  time: 3.4864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1460/3449]  eta: 1:55:58  lr: 0.000100  loss: 0.0318 (0.0348)  time: 3.4856  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1470/3449]  eta: 1:55:23  lr: 0.000100  loss: 0.0311 (0.0348)  time: 3.4854  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1480/3449]  eta: 1:54:48  lr: 0.000100  loss: 0.0342 (0.0348)  time: 3.4861  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1490/3449]  eta: 1:54:13  lr: 0.000100  loss: 0.0342 (0.0348)  time: 3.4861  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1500/3449]  eta: 1:53:38  lr: 0.000100  loss: 0.0351 (0.0349)  time: 3.4850  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1510/3449]  eta: 1:53:03  lr: 0.000100  loss: 0.0371 (0.0349)  time: 3.4852  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1520/3449]  eta: 1:52:27  lr: 0.000100  loss: 0.0342 (0.0348)  time: 3.4854  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1530/3449]  eta: 1:51:52  lr: 0.000100  loss: 0.0337 (0.0348)  time: 3.4851  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1540/3449]  eta: 1:51:17  lr: 0.000100  loss: 0.0344 (0.0349)  time: 3.4854  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1550/3449]  eta: 1:50:42  lr: 0.000100  loss: 0.0326 (0.0348)  time: 3.4855  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1560/3449]  eta: 1:50:07  lr: 0.000100  loss: 0.0340 (0.0349)  time: 3.4848  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1570/3449]  eta: 1:49:32  lr: 0.000100  loss: 0.0344 (0.0349)  time: 3.4844  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1580/3449]  eta: 1:48:57  lr: 0.000100  loss: 0.0320 (0.0349)  time: 3.4850  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1590/3449]  eta: 1:48:21  lr: 0.000100  loss: 0.0322 (0.0348)  time: 3.4847  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1600/3449]  eta: 1:47:46  lr: 0.000100  loss: 0.0323 (0.0348)  time: 3.4845  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1610/3449]  eta: 1:47:11  lr: 0.000100  loss: 0.0331 (0.0348)  time: 3.4844  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1620/3449]  eta: 1:46:36  lr: 0.000100  loss: 0.0330 (0.0348)  time: 3.4849  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1630/3449]  eta: 1:46:01  lr: 0.000100  loss: 0.0317 (0.0348)  time: 3.4859  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1640/3449]  eta: 1:45:26  lr: 0.000100  loss: 0.0329 (0.0348)  time: 3.4859  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1650/3449]  eta: 1:44:51  lr: 0.000100  loss: 0.0320 (0.0348)  time: 3.4866  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1660/3449]  eta: 1:44:16  lr: 0.000100  loss: 0.0335 (0.0348)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1670/3449]  eta: 1:43:41  lr: 0.000100  loss: 0.0356 (0.0348)  time: 3.4903  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1680/3449]  eta: 1:43:06  lr: 0.000100  loss: 0.0348 (0.0348)  time: 3.4901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1690/3449]  eta: 1:42:31  lr: 0.000100  loss: 0.0345 (0.0348)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1700/3449]  eta: 1:41:56  lr: 0.000100  loss: 0.0342 (0.0348)  time: 3.4904  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1710/3449]  eta: 1:41:21  lr: 0.000100  loss: 0.0313 (0.0348)  time: 3.4916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1720/3449]  eta: 1:40:46  lr: 0.000100  loss: 0.0328 (0.0348)  time: 3.4926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1730/3449]  eta: 1:40:11  lr: 0.000100  loss: 0.0330 (0.0348)  time: 3.4926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1740/3449]  eta: 1:39:36  lr: 0.000100  loss: 0.0330 (0.0348)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1750/3449]  eta: 1:39:01  lr: 0.000100  loss: 0.0332 (0.0348)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1760/3449]  eta: 1:38:25  lr: 0.000100  loss: 0.0330 (0.0348)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1770/3449]  eta: 1:37:50  lr: 0.000100  loss: 0.0323 (0.0348)  time: 3.4914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [1780/3449]  eta: 1:37:15  lr: 0.000100  loss: 0.0337 (0.0348)  time: 3.4920  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1790/3449]  eta: 1:36:40  lr: 0.000100  loss: 0.0344 (0.0348)  time: 3.4914  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1800/3449]  eta: 1:36:05  lr: 0.000100  loss: 0.0343 (0.0348)  time: 3.4907  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1810/3449]  eta: 1:35:30  lr: 0.000100  loss: 0.0348 (0.0349)  time: 3.4905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1820/3449]  eta: 1:34:55  lr: 0.000100  loss: 0.0338 (0.0348)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1830/3449]  eta: 1:34:20  lr: 0.000100  loss: 0.0353 (0.0349)  time: 3.4917  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1840/3449]  eta: 1:33:45  lr: 0.000100  loss: 0.0353 (0.0348)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1850/3449]  eta: 1:33:10  lr: 0.000100  loss: 0.0332 (0.0348)  time: 3.4905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1860/3449]  eta: 1:32:35  lr: 0.000100  loss: 0.0333 (0.0348)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1870/3449]  eta: 1:32:00  lr: 0.000100  loss: 0.0329 (0.0348)  time: 3.4898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1880/3449]  eta: 1:31:25  lr: 0.000100  loss: 0.0329 (0.0348)  time: 3.4901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1890/3449]  eta: 1:30:50  lr: 0.000100  loss: 0.0319 (0.0348)  time: 3.4907  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1900/3449]  eta: 1:30:15  lr: 0.000100  loss: 0.0305 (0.0348)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1910/3449]  eta: 1:29:40  lr: 0.000100  loss: 0.0361 (0.0349)  time: 3.4909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1920/3449]  eta: 1:29:05  lr: 0.000100  loss: 0.0363 (0.0349)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1930/3449]  eta: 1:28:30  lr: 0.000100  loss: 0.0364 (0.0349)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1940/3449]  eta: 1:27:55  lr: 0.000100  loss: 0.0404 (0.0350)  time: 3.4890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1950/3449]  eta: 1:27:20  lr: 0.000100  loss: 0.0395 (0.0350)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1960/3449]  eta: 1:26:45  lr: 0.000100  loss: 0.0403 (0.0350)  time: 3.4894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1970/3449]  eta: 1:26:10  lr: 0.000100  loss: 0.0500 (0.0352)  time: 3.4903  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1980/3449]  eta: 1:25:35  lr: 0.000100  loss: 0.0636 (0.0353)  time: 3.4924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [1990/3449]  eta: 1:25:00  lr: 0.000100  loss: 0.0510 (0.0353)  time: 3.4939  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2000/3449]  eta: 1:24:25  lr: 0.000100  loss: 0.0376 (0.0353)  time: 3.4941  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2010/3449]  eta: 1:23:50  lr: 0.000100  loss: 0.0354 (0.0353)  time: 3.4941  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2020/3449]  eta: 1:23:15  lr: 0.000100  loss: 0.0304 (0.0353)  time: 3.4943  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:32]  [2030/3449]  eta: 1:22:40  lr: 0.000100  loss: 0.0331 (0.0353)  time: 3.4943  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2040/3449]  eta: 1:22:05  lr: 0.000100  loss: 0.0342 (0.0353)  time: 3.4934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2050/3449]  eta: 1:21:30  lr: 0.000100  loss: 0.0338 (0.0353)  time: 3.4939  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2060/3449]  eta: 1:20:55  lr: 0.000100  loss: 0.0340 (0.0353)  time: 3.4947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2070/3449]  eta: 1:20:20  lr: 0.000100  loss: 0.0318 (0.0353)  time: 3.4947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2080/3449]  eta: 1:19:45  lr: 0.000100  loss: 0.0318 (0.0353)  time: 3.4949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2090/3449]  eta: 1:19:11  lr: 0.000100  loss: 0.0334 (0.0352)  time: 3.4947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2100/3449]  eta: 1:18:36  lr: 0.000100  loss: 0.0333 (0.0352)  time: 3.4925  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2110/3449]  eta: 1:18:01  lr: 0.000100  loss: 0.0313 (0.0352)  time: 3.4915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2120/3449]  eta: 1:17:26  lr: 0.000100  loss: 0.0313 (0.0352)  time: 3.4923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2130/3449]  eta: 1:16:51  lr: 0.000100  loss: 0.0338 (0.0352)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2140/3449]  eta: 1:16:16  lr: 0.000100  loss: 0.0335 (0.0352)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2150/3449]  eta: 1:15:41  lr: 0.000100  loss: 0.0323 (0.0352)  time: 3.4877  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2160/3449]  eta: 1:15:06  lr: 0.000100  loss: 0.0313 (0.0352)  time: 3.4872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2170/3449]  eta: 1:14:31  lr: 0.000100  loss: 0.0317 (0.0352)  time: 3.4877  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2180/3449]  eta: 1:13:56  lr: 0.000100  loss: 0.0311 (0.0351)  time: 3.4876  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2190/3449]  eta: 1:13:21  lr: 0.000100  loss: 0.0333 (0.0352)  time: 3.4869  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2200/3449]  eta: 1:12:46  lr: 0.000100  loss: 0.0351 (0.0352)  time: 3.4859  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2210/3449]  eta: 1:12:11  lr: 0.000100  loss: 0.0315 (0.0351)  time: 3.4855  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2220/3449]  eta: 1:11:35  lr: 0.000100  loss: 0.0315 (0.0352)  time: 3.4855  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2230/3449]  eta: 1:11:00  lr: 0.000100  loss: 0.0345 (0.0352)  time: 3.4854  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2240/3449]  eta: 1:10:25  lr: 0.000100  loss: 0.0331 (0.0352)  time: 3.4866  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2250/3449]  eta: 1:09:50  lr: 0.000100  loss: 0.0319 (0.0352)  time: 3.4870  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2260/3449]  eta: 1:09:15  lr: 0.000100  loss: 0.0315 (0.0351)  time: 3.4869  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2270/3449]  eta: 1:08:40  lr: 0.000100  loss: 0.0318 (0.0352)  time: 3.4877  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2280/3449]  eta: 1:08:06  lr: 0.000100  loss: 0.0318 (0.0351)  time: 3.4889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2290/3449]  eta: 1:07:31  lr: 0.000100  loss: 0.0318 (0.0351)  time: 3.4905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2300/3449]  eta: 1:06:56  lr: 0.000100  loss: 0.0340 (0.0351)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2310/3449]  eta: 1:06:21  lr: 0.000100  loss: 0.0349 (0.0352)  time: 3.4912  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2320/3449]  eta: 1:05:46  lr: 0.000100  loss: 0.0358 (0.0352)  time: 3.4925  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2330/3449]  eta: 1:05:11  lr: 0.000100  loss: 0.0358 (0.0352)  time: 3.4925  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2340/3449]  eta: 1:04:36  lr: 0.000100  loss: 0.0334 (0.0352)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2350/3449]  eta: 1:04:01  lr: 0.000100  loss: 0.0332 (0.0352)  time: 3.4925  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2360/3449]  eta: 1:03:26  lr: 0.000100  loss: 0.0324 (0.0352)  time: 3.4913  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2370/3449]  eta: 1:02:51  lr: 0.000100  loss: 0.0370 (0.0352)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2380/3449]  eta: 1:02:16  lr: 0.000100  loss: 0.0371 (0.0352)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2390/3449]  eta: 1:01:41  lr: 0.000100  loss: 0.0326 (0.0352)  time: 3.4905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2400/3449]  eta: 1:01:06  lr: 0.000100  loss: 0.0312 (0.0352)  time: 3.4898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2410/3449]  eta: 1:00:31  lr: 0.000100  loss: 0.0312 (0.0352)  time: 3.4882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2420/3449]  eta: 0:59:56  lr: 0.000100  loss: 0.0335 (0.0352)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2430/3449]  eta: 0:59:21  lr: 0.000100  loss: 0.0335 (0.0352)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2440/3449]  eta: 0:58:46  lr: 0.000100  loss: 0.0313 (0.0352)  time: 3.4900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2450/3449]  eta: 0:58:11  lr: 0.000100  loss: 0.0332 (0.0352)  time: 3.4896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2460/3449]  eta: 0:57:36  lr: 0.000100  loss: 0.0316 (0.0352)  time: 3.4891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2470/3449]  eta: 0:57:01  lr: 0.000100  loss: 0.0313 (0.0352)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2480/3449]  eta: 0:56:26  lr: 0.000100  loss: 0.0313 (0.0351)  time: 3.4890  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2490/3449]  eta: 0:55:51  lr: 0.000100  loss: 0.0325 (0.0352)  time: 3.4868  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2500/3449]  eta: 0:55:16  lr: 0.000100  loss: 0.0345 (0.0352)  time: 3.4870  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2510/3449]  eta: 0:54:41  lr: 0.000100  loss: 0.0342 (0.0351)  time: 3.4873  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2520/3449]  eta: 0:54:06  lr: 0.000100  loss: 0.0307 (0.0351)  time: 3.4852  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2530/3449]  eta: 0:53:31  lr: 0.000100  loss: 0.0337 (0.0351)  time: 3.4843  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2540/3449]  eta: 0:52:56  lr: 0.000100  loss: 0.0328 (0.0351)  time: 3.4853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2550/3449]  eta: 0:52:21  lr: 0.000100  loss: 0.0326 (0.0351)  time: 3.4860  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2560/3449]  eta: 0:51:46  lr: 0.000100  loss: 0.0326 (0.0351)  time: 3.4866  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2570/3449]  eta: 0:51:11  lr: 0.000100  loss: 0.0324 (0.0351)  time: 3.4875  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2580/3449]  eta: 0:50:36  lr: 0.000100  loss: 0.0330 (0.0352)  time: 3.4873  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2590/3449]  eta: 0:50:01  lr: 0.000100  loss: 0.0330 (0.0352)  time: 3.4880  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2600/3449]  eta: 0:49:26  lr: 0.000100  loss: 0.0311 (0.0351)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2610/3449]  eta: 0:48:51  lr: 0.000100  loss: 0.0307 (0.0351)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2620/3449]  eta: 0:48:16  lr: 0.000100  loss: 0.0343 (0.0351)  time: 3.4929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2630/3449]  eta: 0:47:42  lr: 0.000100  loss: 0.0326 (0.0351)  time: 3.4924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2640/3449]  eta: 0:47:07  lr: 0.000100  loss: 0.0326 (0.0351)  time: 3.4921  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2650/3449]  eta: 0:46:32  lr: 0.000100  loss: 0.0342 (0.0351)  time: 3.4918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2660/3449]  eta: 0:45:57  lr: 0.000100  loss: 0.0346 (0.0351)  time: 3.4911  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2670/3449]  eta: 0:45:22  lr: 0.000100  loss: 0.0353 (0.0351)  time: 3.4909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2680/3449]  eta: 0:44:47  lr: 0.000100  loss: 0.0349 (0.0351)  time: 3.4916  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:32]  [2690/3449]  eta: 0:44:12  lr: 0.000100  loss: 0.0352 (0.0351)  time: 3.4918  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2700/3449]  eta: 0:43:37  lr: 0.000100  loss: 0.0364 (0.0351)  time: 3.4916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2710/3449]  eta: 0:43:02  lr: 0.000100  loss: 0.0322 (0.0351)  time: 3.4917  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2720/3449]  eta: 0:42:27  lr: 0.000100  loss: 0.0316 (0.0351)  time: 3.4918  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2730/3449]  eta: 0:41:52  lr: 0.000100  loss: 0.0311 (0.0351)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2740/3449]  eta: 0:41:17  lr: 0.000100  loss: 0.0334 (0.0351)  time: 3.4904  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2750/3449]  eta: 0:40:42  lr: 0.000100  loss: 0.0348 (0.0351)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2760/3449]  eta: 0:40:07  lr: 0.000100  loss: 0.0344 (0.0351)  time: 3.4900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2770/3449]  eta: 0:39:32  lr: 0.000100  loss: 0.0327 (0.0351)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2780/3449]  eta: 0:38:57  lr: 0.000100  loss: 0.0315 (0.0351)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2790/3449]  eta: 0:38:22  lr: 0.000100  loss: 0.0311 (0.0351)  time: 3.4883  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2800/3449]  eta: 0:37:47  lr: 0.000100  loss: 0.0318 (0.0351)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2810/3449]  eta: 0:37:12  lr: 0.000100  loss: 0.0318 (0.0351)  time: 3.4884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2820/3449]  eta: 0:36:37  lr: 0.000100  loss: 0.0326 (0.0351)  time: 3.4876  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2830/3449]  eta: 0:36:02  lr: 0.000100  loss: 0.0347 (0.0351)  time: 3.4872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2840/3449]  eta: 0:35:27  lr: 0.000100  loss: 0.0335 (0.0351)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2850/3449]  eta: 0:34:53  lr: 0.000100  loss: 0.0356 (0.0351)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2860/3449]  eta: 0:34:18  lr: 0.000100  loss: 0.0355 (0.0351)  time: 3.4891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2870/3449]  eta: 0:33:43  lr: 0.000100  loss: 0.0339 (0.0351)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2880/3449]  eta: 0:33:08  lr: 0.000100  loss: 0.0316 (0.0351)  time: 3.4877  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2890/3449]  eta: 0:32:33  lr: 0.000100  loss: 0.0313 (0.0351)  time: 3.4879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2900/3449]  eta: 0:31:58  lr: 0.000100  loss: 0.0400 (0.0352)  time: 3.4893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2910/3449]  eta: 0:31:23  lr: 0.000100  loss: 0.0533 (0.0352)  time: 3.4895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2920/3449]  eta: 0:30:48  lr: 0.000100  loss: 0.0420 (0.0353)  time: 3.4892  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2930/3449]  eta: 0:30:13  lr: 0.000100  loss: 0.0368 (0.0353)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2940/3449]  eta: 0:29:38  lr: 0.000100  loss: 0.0360 (0.0353)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [2950/3449]  eta: 0:29:03  lr: 0.000100  loss: 0.0331 (0.0353)  time: 3.4880  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2960/3449]  eta: 0:28:28  lr: 0.000100  loss: 0.0346 (0.0353)  time: 3.4870  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2970/3449]  eta: 0:27:53  lr: 0.000100  loss: 0.0350 (0.0353)  time: 3.4867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2980/3449]  eta: 0:27:18  lr: 0.000100  loss: 0.0312 (0.0353)  time: 3.4865  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [2990/3449]  eta: 0:26:43  lr: 0.000100  loss: 0.0312 (0.0353)  time: 3.4874  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3000/3449]  eta: 0:26:08  lr: 0.000100  loss: 0.0312 (0.0353)  time: 3.4878  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3010/3449]  eta: 0:25:33  lr: 0.000100  loss: 0.0313 (0.0353)  time: 3.4874  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3020/3449]  eta: 0:24:58  lr: 0.000100  loss: 0.0330 (0.0353)  time: 3.4867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3030/3449]  eta: 0:24:23  lr: 0.000100  loss: 0.0329 (0.0353)  time: 3.4860  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3040/3449]  eta: 0:23:48  lr: 0.000100  loss: 0.0329 (0.0353)  time: 3.4860  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3050/3449]  eta: 0:23:14  lr: 0.000100  loss: 0.0312 (0.0352)  time: 3.4868  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3060/3449]  eta: 0:22:39  lr: 0.000100  loss: 0.0322 (0.0352)  time: 3.4879  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3070/3449]  eta: 0:22:04  lr: 0.000100  loss: 0.0327 (0.0352)  time: 3.4873  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3080/3449]  eta: 0:21:29  lr: 0.000100  loss: 0.0340 (0.0353)  time: 3.4869  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3090/3449]  eta: 0:20:54  lr: 0.000100  loss: 0.0338 (0.0353)  time: 3.4864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3100/3449]  eta: 0:20:19  lr: 0.000100  loss: 0.0338 (0.0353)  time: 3.4858  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3110/3449]  eta: 0:19:44  lr: 0.000100  loss: 0.0354 (0.0353)  time: 3.4862  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3120/3449]  eta: 0:19:09  lr: 0.000100  loss: 0.0332 (0.0353)  time: 3.4864  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3130/3449]  eta: 0:18:34  lr: 0.000100  loss: 0.0327 (0.0352)  time: 3.4857  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3140/3449]  eta: 0:17:59  lr: 0.000100  loss: 0.0309 (0.0352)  time: 3.4858  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3150/3449]  eta: 0:17:24  lr: 0.000100  loss: 0.0309 (0.0352)  time: 3.4869  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3160/3449]  eta: 0:16:49  lr: 0.000100  loss: 0.0346 (0.0352)  time: 3.4875  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3170/3449]  eta: 0:16:14  lr: 0.000100  loss: 0.0365 (0.0352)  time: 3.4895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3180/3449]  eta: 0:15:39  lr: 0.000100  loss: 0.0365 (0.0352)  time: 3.4907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3190/3449]  eta: 0:15:04  lr: 0.000100  loss: 0.0378 (0.0352)  time: 3.4910  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3200/3449]  eta: 0:14:29  lr: 0.000100  loss: 0.0341 (0.0352)  time: 3.4929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3210/3449]  eta: 0:13:54  lr: 0.000100  loss: 0.0344 (0.0353)  time: 3.4937  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3220/3449]  eta: 0:13:20  lr: 0.000100  loss: 0.0345 (0.0352)  time: 3.4929  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3230/3449]  eta: 0:12:45  lr: 0.000100  loss: 0.0327 (0.0352)  time: 3.4923  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3240/3449]  eta: 0:12:10  lr: 0.000100  loss: 0.0335 (0.0352)  time: 3.4918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3250/3449]  eta: 0:11:35  lr: 0.000100  loss: 0.0350 (0.0352)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3260/3449]  eta: 0:11:00  lr: 0.000100  loss: 0.0362 (0.0353)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3270/3449]  eta: 0:10:25  lr: 0.000100  loss: 0.0334 (0.0352)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3280/3449]  eta: 0:09:50  lr: 0.000100  loss: 0.0325 (0.0352)  time: 3.4920  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3290/3449]  eta: 0:09:15  lr: 0.000100  loss: 0.0315 (0.0352)  time: 3.4919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3300/3449]  eta: 0:08:40  lr: 0.000100  loss: 0.0347 (0.0352)  time: 3.4915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3310/3449]  eta: 0:08:05  lr: 0.000100  loss: 0.0364 (0.0352)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3320/3449]  eta: 0:07:30  lr: 0.000100  loss: 0.0333 (0.0353)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3330/3449]  eta: 0:06:55  lr: 0.000100  loss: 0.0319 (0.0352)  time: 3.4895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3340/3449]  eta: 0:06:20  lr: 0.000100  loss: 0.0317 (0.0352)  time: 3.4904  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:32]  [3350/3449]  eta: 0:05:45  lr: 0.000100  loss: 0.0330 (0.0352)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3360/3449]  eta: 0:05:10  lr: 0.000100  loss: 0.0330 (0.0352)  time: 3.4896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3370/3449]  eta: 0:04:35  lr: 0.000100  loss: 0.0327 (0.0352)  time: 3.4896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3380/3449]  eta: 0:04:01  lr: 0.000100  loss: 0.0327 (0.0352)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3390/3449]  eta: 0:03:26  lr: 0.000100  loss: 0.0337 (0.0352)  time: 3.4898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3400/3449]  eta: 0:02:51  lr: 0.000100  loss: 0.0337 (0.0352)  time: 3.4892  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3410/3449]  eta: 0:02:16  lr: 0.000100  loss: 0.0331 (0.0352)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3420/3449]  eta: 0:01:41  lr: 0.000100  loss: 0.0324 (0.0352)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32]  [3430/3449]  eta: 0:01:06  lr: 0.000100  loss: 0.0329 (0.0352)  time: 3.4899  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3440/3449]  eta: 0:00:31  lr: 0.000100  loss: 0.0320 (0.0352)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:32]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0309 (0.0352)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:32] Total time: 3:20:48 (3.4934 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0309 (0.0352)\n",
      "Valid: [epoch:32]  [ 0/14]  eta: 0:04:24  loss: 0.0281 (0.0281)  time: 18.8572  data: 0.6065  max mem: 34968\n",
      "Valid: [epoch:32]  [13/14]  eta: 0:00:18  loss: 0.0297 (0.0301)  time: 18.3121  data: 0.0435  max mem: 34968\n",
      "Valid: [epoch:32] Total time: 0:04:16 (18.3225 s / it)\n",
      "Averaged stats: loss: 0.0297 (0.0301)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_32_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.030%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:33]  [   0/3449]  eta: 4:28:13  lr: 0.000100  loss: 0.0537 (0.0537)  time: 4.6661  data: 1.2178  max mem: 34968\n",
      "Train: [epoch:33]  [  10/3449]  eta: 3:25:58  lr: 0.000100  loss: 0.0361 (0.0388)  time: 3.5937  data: 0.1109  max mem: 34968\n",
      "Train: [epoch:33]  [  20/3449]  eta: 3:22:47  lr: 0.000100  loss: 0.0342 (0.0366)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [  30/3449]  eta: 3:21:20  lr: 0.000100  loss: 0.0330 (0.0356)  time: 3.5002  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [  40/3449]  eta: 3:20:18  lr: 0.000100  loss: 0.0346 (0.0373)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [  50/3449]  eta: 3:19:26  lr: 0.000100  loss: 0.0350 (0.0371)  time: 3.5007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [  60/3449]  eta: 3:18:40  lr: 0.000100  loss: 0.0341 (0.0363)  time: 3.5007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [  70/3449]  eta: 3:17:57  lr: 0.000100  loss: 0.0346 (0.0364)  time: 3.5010  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [  80/3449]  eta: 3:17:15  lr: 0.000100  loss: 0.0342 (0.0357)  time: 3.5002  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [  90/3449]  eta: 3:16:35  lr: 0.000100  loss: 0.0325 (0.0353)  time: 3.4999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 100/3449]  eta: 3:15:57  lr: 0.000100  loss: 0.0334 (0.0354)  time: 3.5007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 110/3449]  eta: 3:15:19  lr: 0.000100  loss: 0.0298 (0.0350)  time: 3.5012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 120/3449]  eta: 3:14:41  lr: 0.000100  loss: 0.0313 (0.0347)  time: 3.5009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 130/3449]  eta: 3:14:03  lr: 0.000100  loss: 0.0314 (0.0345)  time: 3.4994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 140/3449]  eta: 3:13:26  lr: 0.000100  loss: 0.0314 (0.0343)  time: 3.4980  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 150/3449]  eta: 3:12:49  lr: 0.000100  loss: 0.0326 (0.0346)  time: 3.4976  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 160/3449]  eta: 3:12:12  lr: 0.000100  loss: 0.0346 (0.0346)  time: 3.4981  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 170/3449]  eta: 3:11:35  lr: 0.000100  loss: 0.0343 (0.0345)  time: 3.4991  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 180/3449]  eta: 3:10:59  lr: 0.000100  loss: 0.0325 (0.0346)  time: 3.4985  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 190/3449]  eta: 3:10:22  lr: 0.000100  loss: 0.0338 (0.0348)  time: 3.4971  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 200/3449]  eta: 3:09:46  lr: 0.000100  loss: 0.0341 (0.0348)  time: 3.4976  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 210/3449]  eta: 3:09:10  lr: 0.000100  loss: 0.0352 (0.0349)  time: 3.4970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 220/3449]  eta: 3:08:33  lr: 0.000100  loss: 0.0354 (0.0349)  time: 3.4951  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 230/3449]  eta: 3:07:57  lr: 0.000100  loss: 0.0354 (0.0351)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 240/3449]  eta: 3:07:20  lr: 0.000100  loss: 0.0345 (0.0350)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 250/3449]  eta: 3:06:44  lr: 0.000100  loss: 0.0353 (0.0352)  time: 3.4917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 260/3449]  eta: 3:06:07  lr: 0.000100  loss: 0.0360 (0.0352)  time: 3.4903  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 270/3449]  eta: 3:05:30  lr: 0.000100  loss: 0.0345 (0.0352)  time: 3.4894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 280/3449]  eta: 3:04:54  lr: 0.000100  loss: 0.0349 (0.0355)  time: 3.4889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 290/3449]  eta: 3:04:17  lr: 0.000100  loss: 0.0349 (0.0355)  time: 3.4875  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 300/3449]  eta: 3:03:41  lr: 0.000100  loss: 0.0340 (0.0356)  time: 3.4862  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 310/3449]  eta: 3:03:05  lr: 0.000100  loss: 0.0339 (0.0356)  time: 3.4867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 320/3449]  eta: 3:02:29  lr: 0.000100  loss: 0.0339 (0.0356)  time: 3.4882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 330/3449]  eta: 3:01:53  lr: 0.000100  loss: 0.0342 (0.0355)  time: 3.4893  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 340/3449]  eta: 3:01:17  lr: 0.000100  loss: 0.0343 (0.0355)  time: 3.4900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 350/3449]  eta: 3:00:41  lr: 0.000100  loss: 0.0330 (0.0355)  time: 3.4891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 360/3449]  eta: 3:00:05  lr: 0.000100  loss: 0.0337 (0.0355)  time: 3.4887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 370/3449]  eta: 2:59:30  lr: 0.000100  loss: 0.0337 (0.0354)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 380/3449]  eta: 2:58:54  lr: 0.000100  loss: 0.0334 (0.0355)  time: 3.4914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 390/3449]  eta: 2:58:19  lr: 0.000100  loss: 0.0365 (0.0356)  time: 3.4902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 400/3449]  eta: 2:57:43  lr: 0.000100  loss: 0.0354 (0.0356)  time: 3.4898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 410/3449]  eta: 2:57:08  lr: 0.000100  loss: 0.0329 (0.0356)  time: 3.4902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 420/3449]  eta: 2:56:32  lr: 0.000100  loss: 0.0322 (0.0356)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 430/3449]  eta: 2:55:56  lr: 0.000100  loss: 0.0322 (0.0356)  time: 3.4896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 440/3449]  eta: 2:55:21  lr: 0.000100  loss: 0.0358 (0.0357)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 450/3449]  eta: 2:54:46  lr: 0.000100  loss: 0.0366 (0.0357)  time: 3.4901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 460/3449]  eta: 2:54:10  lr: 0.000100  loss: 0.0360 (0.0357)  time: 3.4901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 470/3449]  eta: 2:53:35  lr: 0.000100  loss: 0.0356 (0.0357)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 480/3449]  eta: 2:53:00  lr: 0.000100  loss: 0.0356 (0.0359)  time: 3.4902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 490/3449]  eta: 2:52:24  lr: 0.000100  loss: 0.0344 (0.0358)  time: 3.4893  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:33]  [ 500/3449]  eta: 2:51:49  lr: 0.000100  loss: 0.0336 (0.0359)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 510/3449]  eta: 2:51:14  lr: 0.000100  loss: 0.0342 (0.0359)  time: 3.4896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 520/3449]  eta: 2:50:38  lr: 0.000100  loss: 0.0362 (0.0359)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 530/3449]  eta: 2:50:03  lr: 0.000100  loss: 0.0356 (0.0361)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 540/3449]  eta: 2:49:28  lr: 0.000100  loss: 0.0359 (0.0361)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 550/3449]  eta: 2:48:52  lr: 0.000100  loss: 0.0347 (0.0361)  time: 3.4887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 560/3449]  eta: 2:48:17  lr: 0.000100  loss: 0.0347 (0.0361)  time: 3.4892  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 570/3449]  eta: 2:47:42  lr: 0.000100  loss: 0.0347 (0.0361)  time: 3.4896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 580/3449]  eta: 2:47:07  lr: 0.000100  loss: 0.0347 (0.0361)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 590/3449]  eta: 2:46:32  lr: 0.000100  loss: 0.0334 (0.0361)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 600/3449]  eta: 2:45:57  lr: 0.000100  loss: 0.0353 (0.0361)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 610/3449]  eta: 2:45:22  lr: 0.000100  loss: 0.0374 (0.0362)  time: 3.4940  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 620/3449]  eta: 2:44:47  lr: 0.000100  loss: 0.0354 (0.0361)  time: 3.4948  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 630/3449]  eta: 2:44:12  lr: 0.000100  loss: 0.0329 (0.0361)  time: 3.4947  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 640/3449]  eta: 2:43:37  lr: 0.000100  loss: 0.0337 (0.0361)  time: 3.4938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 650/3449]  eta: 2:43:02  lr: 0.000100  loss: 0.0338 (0.0361)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 660/3449]  eta: 2:42:26  lr: 0.000100  loss: 0.0338 (0.0361)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 670/3449]  eta: 2:41:51  lr: 0.000100  loss: 0.0348 (0.0361)  time: 3.4911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 680/3449]  eta: 2:41:16  lr: 0.000100  loss: 0.0346 (0.0361)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 690/3449]  eta: 2:40:41  lr: 0.000100  loss: 0.0344 (0.0361)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 700/3449]  eta: 2:40:06  lr: 0.000100  loss: 0.0348 (0.0362)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 710/3449]  eta: 2:39:31  lr: 0.000100  loss: 0.0349 (0.0361)  time: 3.4914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 720/3449]  eta: 2:38:56  lr: 0.000100  loss: 0.0335 (0.0361)  time: 3.4907  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 730/3449]  eta: 2:38:21  lr: 0.000100  loss: 0.0337 (0.0361)  time: 3.4903  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 740/3449]  eta: 2:37:46  lr: 0.000100  loss: 0.0361 (0.0361)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 750/3449]  eta: 2:37:11  lr: 0.000100  loss: 0.0337 (0.0361)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 760/3449]  eta: 2:36:35  lr: 0.000100  loss: 0.0336 (0.0360)  time: 3.4914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 770/3449]  eta: 2:36:00  lr: 0.000100  loss: 0.0332 (0.0361)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 780/3449]  eta: 2:35:25  lr: 0.000100  loss: 0.0332 (0.0361)  time: 3.4915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 790/3449]  eta: 2:34:50  lr: 0.000100  loss: 0.0351 (0.0361)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 800/3449]  eta: 2:34:15  lr: 0.000100  loss: 0.0332 (0.0361)  time: 3.4885  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 810/3449]  eta: 2:33:40  lr: 0.000100  loss: 0.0317 (0.0360)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 820/3449]  eta: 2:33:05  lr: 0.000100  loss: 0.0331 (0.0360)  time: 3.4882  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 830/3449]  eta: 2:32:30  lr: 0.000100  loss: 0.0348 (0.0361)  time: 3.4869  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 840/3449]  eta: 2:31:55  lr: 0.000100  loss: 0.0346 (0.0360)  time: 3.4874  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 850/3449]  eta: 2:31:20  lr: 0.000100  loss: 0.0338 (0.0360)  time: 3.4885  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 860/3449]  eta: 2:30:44  lr: 0.000100  loss: 0.0335 (0.0361)  time: 3.4879  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 870/3449]  eta: 2:30:09  lr: 0.000100  loss: 0.0343 (0.0361)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 880/3449]  eta: 2:29:34  lr: 0.000100  loss: 0.0352 (0.0360)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 890/3449]  eta: 2:28:59  lr: 0.000100  loss: 0.0352 (0.0360)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 900/3449]  eta: 2:28:24  lr: 0.000100  loss: 0.0351 (0.0360)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 910/3449]  eta: 2:27:50  lr: 0.000100  loss: 0.0348 (0.0360)  time: 3.4944  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 920/3449]  eta: 2:27:15  lr: 0.000100  loss: 0.0336 (0.0360)  time: 3.4945  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [ 930/3449]  eta: 2:26:40  lr: 0.000100  loss: 0.0336 (0.0361)  time: 3.4937  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 940/3449]  eta: 2:26:05  lr: 0.000100  loss: 0.0323 (0.0360)  time: 3.4918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 950/3449]  eta: 2:25:30  lr: 0.000100  loss: 0.0321 (0.0360)  time: 3.4893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 960/3449]  eta: 2:24:55  lr: 0.000100  loss: 0.0305 (0.0360)  time: 3.4877  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 970/3449]  eta: 2:24:19  lr: 0.000100  loss: 0.0344 (0.0361)  time: 3.4878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 980/3449]  eta: 2:23:44  lr: 0.000100  loss: 0.0375 (0.0361)  time: 3.4876  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [ 990/3449]  eta: 2:23:09  lr: 0.000100  loss: 0.0273 (0.0360)  time: 3.4874  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1000/3449]  eta: 2:22:34  lr: 0.000100  loss: 0.0324 (0.0360)  time: 3.4883  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1010/3449]  eta: 2:21:59  lr: 0.000100  loss: 0.0336 (0.0360)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1020/3449]  eta: 2:21:24  lr: 0.000100  loss: 0.0371 (0.0361)  time: 3.4896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1030/3449]  eta: 2:20:49  lr: 0.000100  loss: 0.0352 (0.0360)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1040/3449]  eta: 2:20:14  lr: 0.000100  loss: 0.0329 (0.0360)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1050/3449]  eta: 2:19:39  lr: 0.000100  loss: 0.0346 (0.0360)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1060/3449]  eta: 2:19:04  lr: 0.000100  loss: 0.0356 (0.0360)  time: 3.4903  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1070/3449]  eta: 2:18:29  lr: 0.000100  loss: 0.0334 (0.0360)  time: 3.4899  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1080/3449]  eta: 2:17:54  lr: 0.000100  loss: 0.0333 (0.0360)  time: 3.4904  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1090/3449]  eta: 2:17:19  lr: 0.000100  loss: 0.0334 (0.0360)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1100/3449]  eta: 2:16:44  lr: 0.000100  loss: 0.0324 (0.0359)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1110/3449]  eta: 2:16:09  lr: 0.000100  loss: 0.0319 (0.0359)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1120/3449]  eta: 2:15:34  lr: 0.000100  loss: 0.0341 (0.0359)  time: 3.4878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1130/3449]  eta: 2:14:59  lr: 0.000100  loss: 0.0357 (0.0359)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1140/3449]  eta: 2:14:24  lr: 0.000100  loss: 0.0335 (0.0359)  time: 3.4897  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1150/3449]  eta: 2:13:49  lr: 0.000100  loss: 0.0335 (0.0359)  time: 3.4904  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:33]  [1160/3449]  eta: 2:13:14  lr: 0.000100  loss: 0.0340 (0.0359)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1170/3449]  eta: 2:12:39  lr: 0.000100  loss: 0.0340 (0.0359)  time: 3.4912  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1180/3449]  eta: 2:12:04  lr: 0.000100  loss: 0.0325 (0.0359)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1190/3449]  eta: 2:11:30  lr: 0.000100  loss: 0.0325 (0.0359)  time: 3.4940  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1200/3449]  eta: 2:10:55  lr: 0.000100  loss: 0.0331 (0.0359)  time: 3.4941  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1210/3449]  eta: 2:10:20  lr: 0.000100  loss: 0.0316 (0.0359)  time: 3.4952  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1220/3449]  eta: 2:09:45  lr: 0.000100  loss: 0.0339 (0.0359)  time: 3.4958  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1230/3449]  eta: 2:09:10  lr: 0.000100  loss: 0.0358 (0.0360)  time: 3.4956  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1240/3449]  eta: 2:08:35  lr: 0.000100  loss: 0.0360 (0.0360)  time: 3.4954  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1250/3449]  eta: 2:08:00  lr: 0.000100  loss: 0.0353 (0.0360)  time: 3.4960  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1260/3449]  eta: 2:07:25  lr: 0.000100  loss: 0.0338 (0.0360)  time: 3.4956  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1270/3449]  eta: 2:06:51  lr: 0.000100  loss: 0.0329 (0.0360)  time: 3.4956  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1280/3449]  eta: 2:06:16  lr: 0.000100  loss: 0.0342 (0.0360)  time: 3.4953  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1290/3449]  eta: 2:05:41  lr: 0.000100  loss: 0.0350 (0.0360)  time: 3.4942  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1300/3449]  eta: 2:05:06  lr: 0.000100  loss: 0.0366 (0.0360)  time: 3.4941  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1310/3449]  eta: 2:04:31  lr: 0.000100  loss: 0.0341 (0.0360)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1320/3449]  eta: 2:03:56  lr: 0.000100  loss: 0.0337 (0.0360)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1330/3449]  eta: 2:03:21  lr: 0.000100  loss: 0.0337 (0.0360)  time: 3.4925  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1340/3449]  eta: 2:02:46  lr: 0.000100  loss: 0.0318 (0.0360)  time: 3.4921  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1350/3449]  eta: 2:02:11  lr: 0.000100  loss: 0.0324 (0.0360)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1360/3449]  eta: 2:01:36  lr: 0.000100  loss: 0.0404 (0.0360)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1370/3449]  eta: 2:01:01  lr: 0.000100  loss: 0.0421 (0.0361)  time: 3.4917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1380/3449]  eta: 2:00:26  lr: 0.000100  loss: 0.0395 (0.0361)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1390/3449]  eta: 1:59:51  lr: 0.000100  loss: 0.0397 (0.0362)  time: 3.4907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1400/3449]  eta: 1:59:16  lr: 0.000100  loss: 0.0399 (0.0362)  time: 3.4895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1410/3449]  eta: 1:58:41  lr: 0.000100  loss: 0.0371 (0.0362)  time: 3.4895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1420/3449]  eta: 1:58:06  lr: 0.000100  loss: 0.0380 (0.0363)  time: 3.4895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1430/3449]  eta: 1:57:31  lr: 0.000100  loss: 0.0353 (0.0363)  time: 3.4901  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1440/3449]  eta: 1:56:56  lr: 0.000100  loss: 0.0347 (0.0363)  time: 3.4902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1450/3449]  eta: 1:56:21  lr: 0.000100  loss: 0.0339 (0.0363)  time: 3.4884  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1460/3449]  eta: 1:55:46  lr: 0.000100  loss: 0.0334 (0.0362)  time: 3.4891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1470/3449]  eta: 1:55:12  lr: 0.000100  loss: 0.0334 (0.0362)  time: 3.4904  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1480/3449]  eta: 1:54:37  lr: 0.000100  loss: 0.0337 (0.0362)  time: 3.4886  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1490/3449]  eta: 1:54:02  lr: 0.000100  loss: 0.0340 (0.0362)  time: 3.4874  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1500/3449]  eta: 1:53:27  lr: 0.000100  loss: 0.0348 (0.0362)  time: 3.4876  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1510/3449]  eta: 1:52:52  lr: 0.000100  loss: 0.0339 (0.0362)  time: 3.4883  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1520/3449]  eta: 1:52:17  lr: 0.000100  loss: 0.0340 (0.0362)  time: 3.4891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1530/3449]  eta: 1:51:42  lr: 0.000100  loss: 0.0340 (0.0362)  time: 3.4892  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1540/3449]  eta: 1:51:07  lr: 0.000100  loss: 0.0351 (0.0362)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1550/3449]  eta: 1:50:32  lr: 0.000100  loss: 0.0361 (0.0362)  time: 3.4882  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1560/3449]  eta: 1:49:57  lr: 0.000100  loss: 0.0353 (0.0362)  time: 3.4878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1570/3449]  eta: 1:49:22  lr: 0.000100  loss: 0.0341 (0.0362)  time: 3.4872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1580/3449]  eta: 1:48:47  lr: 0.000100  loss: 0.0346 (0.0362)  time: 3.4871  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1590/3449]  eta: 1:48:12  lr: 0.000100  loss: 0.0347 (0.0362)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1600/3449]  eta: 1:47:37  lr: 0.000100  loss: 0.0318 (0.0362)  time: 3.4866  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1610/3449]  eta: 1:47:02  lr: 0.000100  loss: 0.0334 (0.0362)  time: 3.4861  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1620/3449]  eta: 1:46:27  lr: 0.000100  loss: 0.0334 (0.0362)  time: 3.4855  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1630/3449]  eta: 1:45:52  lr: 0.000100  loss: 0.0329 (0.0362)  time: 3.4866  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1640/3449]  eta: 1:45:17  lr: 0.000100  loss: 0.0362 (0.0362)  time: 3.4877  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1650/3449]  eta: 1:44:42  lr: 0.000100  loss: 0.0385 (0.0363)  time: 3.4884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1660/3449]  eta: 1:44:07  lr: 0.000100  loss: 0.0343 (0.0362)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1670/3449]  eta: 1:43:32  lr: 0.000100  loss: 0.0343 (0.0362)  time: 3.4883  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1680/3449]  eta: 1:42:57  lr: 0.000100  loss: 0.0349 (0.0362)  time: 3.4879  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1690/3449]  eta: 1:42:22  lr: 0.000100  loss: 0.0348 (0.0363)  time: 3.4877  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1700/3449]  eta: 1:41:47  lr: 0.000100  loss: 0.0359 (0.0363)  time: 3.4879  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1710/3449]  eta: 1:41:12  lr: 0.000100  loss: 0.0344 (0.0363)  time: 3.4870  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1720/3449]  eta: 1:40:37  lr: 0.000100  loss: 0.0311 (0.0362)  time: 3.4864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1730/3449]  eta: 1:40:02  lr: 0.000100  loss: 0.0336 (0.0362)  time: 3.4872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1740/3449]  eta: 1:39:27  lr: 0.000100  loss: 0.0341 (0.0362)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1750/3449]  eta: 1:38:52  lr: 0.000100  loss: 0.0338 (0.0362)  time: 3.4871  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1760/3449]  eta: 1:38:17  lr: 0.000100  loss: 0.0331 (0.0362)  time: 3.4882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1770/3449]  eta: 1:37:42  lr: 0.000100  loss: 0.0362 (0.0362)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1780/3449]  eta: 1:37:07  lr: 0.000100  loss: 0.0362 (0.0362)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1790/3449]  eta: 1:36:32  lr: 0.000100  loss: 0.0330 (0.0362)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1800/3449]  eta: 1:35:58  lr: 0.000100  loss: 0.0324 (0.0362)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1810/3449]  eta: 1:35:23  lr: 0.000100  loss: 0.0335 (0.0362)  time: 3.4905  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:33]  [1820/3449]  eta: 1:34:48  lr: 0.000100  loss: 0.0336 (0.0362)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1830/3449]  eta: 1:34:13  lr: 0.000100  loss: 0.0343 (0.0362)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1840/3449]  eta: 1:33:38  lr: 0.000100  loss: 0.0351 (0.0362)  time: 3.4900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1850/3449]  eta: 1:33:03  lr: 0.000100  loss: 0.0340 (0.0362)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1860/3449]  eta: 1:32:28  lr: 0.000100  loss: 0.0326 (0.0362)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1870/3449]  eta: 1:31:53  lr: 0.000100  loss: 0.0326 (0.0362)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1880/3449]  eta: 1:31:18  lr: 0.000100  loss: 0.0308 (0.0362)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1890/3449]  eta: 1:30:43  lr: 0.000100  loss: 0.0299 (0.0361)  time: 3.4875  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1900/3449]  eta: 1:30:08  lr: 0.000100  loss: 0.0352 (0.0361)  time: 3.4865  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1910/3449]  eta: 1:29:33  lr: 0.000100  loss: 0.0362 (0.0361)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1920/3449]  eta: 1:28:58  lr: 0.000100  loss: 0.0327 (0.0361)  time: 3.4873  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1930/3449]  eta: 1:28:23  lr: 0.000100  loss: 0.0339 (0.0362)  time: 3.4874  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1940/3449]  eta: 1:27:48  lr: 0.000100  loss: 0.0340 (0.0361)  time: 3.4880  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1950/3449]  eta: 1:27:13  lr: 0.000100  loss: 0.0354 (0.0362)  time: 3.4884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1960/3449]  eta: 1:26:38  lr: 0.000100  loss: 0.0389 (0.0362)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [1970/3449]  eta: 1:26:03  lr: 0.000100  loss: 0.0369 (0.0362)  time: 3.4878  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1980/3449]  eta: 1:25:29  lr: 0.000100  loss: 0.0358 (0.0362)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [1990/3449]  eta: 1:24:54  lr: 0.000100  loss: 0.0352 (0.0362)  time: 3.4884  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2000/3449]  eta: 1:24:19  lr: 0.000100  loss: 0.0338 (0.0362)  time: 3.4880  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2010/3449]  eta: 1:23:44  lr: 0.000100  loss: 0.0334 (0.0362)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2020/3449]  eta: 1:23:09  lr: 0.000100  loss: 0.0351 (0.0362)  time: 3.4878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2030/3449]  eta: 1:22:34  lr: 0.000100  loss: 0.0350 (0.0362)  time: 3.4871  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2040/3449]  eta: 1:21:59  lr: 0.000100  loss: 0.0330 (0.0362)  time: 3.4871  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2050/3449]  eta: 1:21:24  lr: 0.000100  loss: 0.0321 (0.0361)  time: 3.4872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2060/3449]  eta: 1:20:49  lr: 0.000100  loss: 0.0326 (0.0361)  time: 3.4867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2070/3449]  eta: 1:20:14  lr: 0.000100  loss: 0.0336 (0.0361)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2080/3449]  eta: 1:19:39  lr: 0.000100  loss: 0.0349 (0.0362)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2090/3449]  eta: 1:19:04  lr: 0.000100  loss: 0.0338 (0.0361)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2100/3449]  eta: 1:18:29  lr: 0.000100  loss: 0.0328 (0.0361)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2110/3449]  eta: 1:17:54  lr: 0.000100  loss: 0.0334 (0.0361)  time: 3.4892  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2120/3449]  eta: 1:17:19  lr: 0.000100  loss: 0.0324 (0.0361)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2130/3449]  eta: 1:16:44  lr: 0.000100  loss: 0.0344 (0.0361)  time: 3.4890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2140/3449]  eta: 1:16:10  lr: 0.000100  loss: 0.0349 (0.0361)  time: 3.4904  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2150/3449]  eta: 1:15:35  lr: 0.000100  loss: 0.0333 (0.0361)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2160/3449]  eta: 1:15:00  lr: 0.000100  loss: 0.0332 (0.0361)  time: 3.4902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2170/3449]  eta: 1:14:25  lr: 0.000100  loss: 0.0327 (0.0361)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2180/3449]  eta: 1:13:50  lr: 0.000100  loss: 0.0327 (0.0361)  time: 3.4897  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2190/3449]  eta: 1:13:15  lr: 0.000100  loss: 0.0368 (0.0361)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2200/3449]  eta: 1:12:40  lr: 0.000100  loss: 0.0368 (0.0361)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2210/3449]  eta: 1:12:05  lr: 0.000100  loss: 0.0343 (0.0361)  time: 3.4874  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2220/3449]  eta: 1:11:30  lr: 0.000100  loss: 0.0352 (0.0360)  time: 3.4873  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2230/3449]  eta: 1:10:55  lr: 0.000100  loss: 0.0343 (0.0361)  time: 3.4865  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2240/3449]  eta: 1:10:20  lr: 0.000100  loss: 0.0343 (0.0360)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2250/3449]  eta: 1:09:45  lr: 0.000100  loss: 0.0334 (0.0360)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2260/3449]  eta: 1:09:10  lr: 0.000100  loss: 0.0357 (0.0360)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2270/3449]  eta: 1:08:36  lr: 0.000100  loss: 0.0357 (0.0360)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2280/3449]  eta: 1:08:01  lr: 0.000100  loss: 0.0338 (0.0360)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2290/3449]  eta: 1:07:26  lr: 0.000100  loss: 0.0325 (0.0360)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2300/3449]  eta: 1:06:51  lr: 0.000100  loss: 0.0326 (0.0360)  time: 3.4929  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2310/3449]  eta: 1:06:16  lr: 0.000100  loss: 0.0326 (0.0360)  time: 3.4936  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2320/3449]  eta: 1:05:41  lr: 0.000100  loss: 0.0305 (0.0360)  time: 3.4930  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2330/3449]  eta: 1:05:06  lr: 0.000100  loss: 0.0340 (0.0360)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2340/3449]  eta: 1:04:31  lr: 0.000100  loss: 0.0342 (0.0360)  time: 3.4925  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2350/3449]  eta: 1:03:56  lr: 0.000100  loss: 0.0335 (0.0359)  time: 3.4911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2360/3449]  eta: 1:03:21  lr: 0.000100  loss: 0.0330 (0.0359)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2370/3449]  eta: 1:02:46  lr: 0.000100  loss: 0.0339 (0.0359)  time: 3.4930  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2380/3449]  eta: 1:02:12  lr: 0.000100  loss: 0.0361 (0.0360)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2390/3449]  eta: 1:01:37  lr: 0.000100  loss: 0.0351 (0.0360)  time: 3.4935  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2400/3449]  eta: 1:01:02  lr: 0.000100  loss: 0.0342 (0.0360)  time: 3.4933  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2410/3449]  eta: 1:00:27  lr: 0.000100  loss: 0.0341 (0.0360)  time: 3.4928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2420/3449]  eta: 0:59:52  lr: 0.000100  loss: 0.0341 (0.0360)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2430/3449]  eta: 0:59:17  lr: 0.000100  loss: 0.0357 (0.0360)  time: 3.4912  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2440/3449]  eta: 0:58:42  lr: 0.000100  loss: 0.0343 (0.0360)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2450/3449]  eta: 0:58:07  lr: 0.000100  loss: 0.0330 (0.0360)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2460/3449]  eta: 0:57:32  lr: 0.000100  loss: 0.0327 (0.0360)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2470/3449]  eta: 0:56:57  lr: 0.000100  loss: 0.0354 (0.0360)  time: 3.4905  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:33]  [2480/3449]  eta: 0:56:22  lr: 0.000100  loss: 0.0361 (0.0360)  time: 3.4907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2490/3449]  eta: 0:55:48  lr: 0.000100  loss: 0.0351 (0.0360)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2500/3449]  eta: 0:55:13  lr: 0.000100  loss: 0.0339 (0.0359)  time: 3.4892  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2510/3449]  eta: 0:54:38  lr: 0.000100  loss: 0.0344 (0.0359)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2520/3449]  eta: 0:54:03  lr: 0.000100  loss: 0.0334 (0.0359)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2530/3449]  eta: 0:53:28  lr: 0.000100  loss: 0.0322 (0.0359)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2540/3449]  eta: 0:52:53  lr: 0.000100  loss: 0.0326 (0.0359)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2550/3449]  eta: 0:52:18  lr: 0.000100  loss: 0.0322 (0.0359)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2560/3449]  eta: 0:51:43  lr: 0.000100  loss: 0.0358 (0.0359)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2570/3449]  eta: 0:51:08  lr: 0.000100  loss: 0.0378 (0.0359)  time: 3.4893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2580/3449]  eta: 0:50:33  lr: 0.000100  loss: 0.0347 (0.0359)  time: 3.4893  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2590/3449]  eta: 0:49:58  lr: 0.000100  loss: 0.0342 (0.0359)  time: 3.4896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2600/3449]  eta: 0:49:23  lr: 0.000100  loss: 0.0337 (0.0359)  time: 3.4909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [2610/3449]  eta: 0:48:49  lr: 0.000100  loss: 0.0339 (0.0360)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2620/3449]  eta: 0:48:14  lr: 0.000100  loss: 0.0347 (0.0360)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2630/3449]  eta: 0:47:39  lr: 0.000100  loss: 0.0338 (0.0360)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2640/3449]  eta: 0:47:04  lr: 0.000100  loss: 0.0302 (0.0359)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2650/3449]  eta: 0:46:29  lr: 0.000100  loss: 0.0328 (0.0359)  time: 3.4929  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2660/3449]  eta: 0:45:54  lr: 0.000100  loss: 0.0347 (0.0359)  time: 3.4929  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2670/3449]  eta: 0:45:19  lr: 0.000100  loss: 0.0316 (0.0359)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2680/3449]  eta: 0:44:44  lr: 0.000100  loss: 0.0350 (0.0359)  time: 3.4931  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2690/3449]  eta: 0:44:09  lr: 0.000100  loss: 0.0374 (0.0359)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2700/3449]  eta: 0:43:34  lr: 0.000100  loss: 0.0355 (0.0359)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2710/3449]  eta: 0:42:59  lr: 0.000100  loss: 0.0352 (0.0359)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2720/3449]  eta: 0:42:25  lr: 0.000100  loss: 0.0340 (0.0359)  time: 3.4929  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2730/3449]  eta: 0:41:50  lr: 0.000100  loss: 0.0331 (0.0359)  time: 3.4929  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2740/3449]  eta: 0:41:15  lr: 0.000100  loss: 0.0309 (0.0359)  time: 3.4910  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2750/3449]  eta: 0:40:40  lr: 0.000100  loss: 0.0330 (0.0359)  time: 3.4903  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2760/3449]  eta: 0:40:05  lr: 0.000100  loss: 0.0344 (0.0359)  time: 3.4904  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2770/3449]  eta: 0:39:30  lr: 0.000100  loss: 0.0344 (0.0359)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2780/3449]  eta: 0:38:55  lr: 0.000100  loss: 0.0332 (0.0359)  time: 3.4903  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2790/3449]  eta: 0:38:20  lr: 0.000100  loss: 0.0308 (0.0359)  time: 3.4909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2800/3449]  eta: 0:37:45  lr: 0.000100  loss: 0.0347 (0.0359)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2810/3449]  eta: 0:37:10  lr: 0.000100  loss: 0.0354 (0.0359)  time: 3.4912  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2820/3449]  eta: 0:36:35  lr: 0.000100  loss: 0.0371 (0.0359)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2830/3449]  eta: 0:36:01  lr: 0.000100  loss: 0.0376 (0.0359)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2840/3449]  eta: 0:35:26  lr: 0.000100  loss: 0.0351 (0.0359)  time: 3.4904  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2850/3449]  eta: 0:34:51  lr: 0.000100  loss: 0.0372 (0.0359)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2860/3449]  eta: 0:34:16  lr: 0.000100  loss: 0.0356 (0.0359)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2870/3449]  eta: 0:33:41  lr: 0.000100  loss: 0.0336 (0.0359)  time: 3.4918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2880/3449]  eta: 0:33:06  lr: 0.000100  loss: 0.0323 (0.0359)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2890/3449]  eta: 0:32:31  lr: 0.000100  loss: 0.0313 (0.0359)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2900/3449]  eta: 0:31:56  lr: 0.000100  loss: 0.0291 (0.0359)  time: 3.4868  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2910/3449]  eta: 0:31:21  lr: 0.000100  loss: 0.0346 (0.0359)  time: 3.4874  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2920/3449]  eta: 0:30:46  lr: 0.000100  loss: 0.0354 (0.0359)  time: 3.4878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2930/3449]  eta: 0:30:11  lr: 0.000100  loss: 0.0352 (0.0359)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2940/3449]  eta: 0:29:36  lr: 0.000100  loss: 0.0337 (0.0359)  time: 3.4891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2950/3449]  eta: 0:29:02  lr: 0.000100  loss: 0.0337 (0.0359)  time: 3.4899  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2960/3449]  eta: 0:28:27  lr: 0.000100  loss: 0.0340 (0.0359)  time: 3.4892  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2970/3449]  eta: 0:27:52  lr: 0.000100  loss: 0.0340 (0.0359)  time: 3.4885  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2980/3449]  eta: 0:27:17  lr: 0.000100  loss: 0.0323 (0.0359)  time: 3.4893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [2990/3449]  eta: 0:26:42  lr: 0.000100  loss: 0.0345 (0.0359)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3000/3449]  eta: 0:26:07  lr: 0.000100  loss: 0.0345 (0.0359)  time: 3.4896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3010/3449]  eta: 0:25:32  lr: 0.000100  loss: 0.0325 (0.0359)  time: 3.4882  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3020/3449]  eta: 0:24:57  lr: 0.000100  loss: 0.0307 (0.0359)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3030/3449]  eta: 0:24:22  lr: 0.000100  loss: 0.0343 (0.0359)  time: 3.4870  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [3040/3449]  eta: 0:23:47  lr: 0.000100  loss: 0.0367 (0.0359)  time: 3.4886  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [3050/3449]  eta: 0:23:12  lr: 0.000100  loss: 0.0377 (0.0359)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3060/3449]  eta: 0:22:38  lr: 0.000100  loss: 0.0396 (0.0360)  time: 3.4911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3070/3449]  eta: 0:22:03  lr: 0.000100  loss: 0.0386 (0.0360)  time: 3.4909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3080/3449]  eta: 0:21:28  lr: 0.000100  loss: 0.0343 (0.0360)  time: 3.4911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3090/3449]  eta: 0:20:53  lr: 0.000100  loss: 0.0340 (0.0360)  time: 3.4912  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3100/3449]  eta: 0:20:18  lr: 0.000100  loss: 0.0315 (0.0360)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3110/3449]  eta: 0:19:43  lr: 0.000100  loss: 0.0376 (0.0360)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3120/3449]  eta: 0:19:08  lr: 0.000100  loss: 0.0375 (0.0360)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3130/3449]  eta: 0:18:33  lr: 0.000100  loss: 0.0336 (0.0360)  time: 3.4920  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:33]  [3140/3449]  eta: 0:17:58  lr: 0.000100  loss: 0.0348 (0.0360)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3150/3449]  eta: 0:17:23  lr: 0.000100  loss: 0.0355 (0.0360)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3160/3449]  eta: 0:16:48  lr: 0.000100  loss: 0.0345 (0.0360)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3170/3449]  eta: 0:16:13  lr: 0.000100  loss: 0.0351 (0.0360)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3180/3449]  eta: 0:15:39  lr: 0.000100  loss: 0.0342 (0.0360)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3190/3449]  eta: 0:15:04  lr: 0.000100  loss: 0.0358 (0.0360)  time: 3.4893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3200/3449]  eta: 0:14:29  lr: 0.000100  loss: 0.0358 (0.0360)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3210/3449]  eta: 0:13:54  lr: 0.000100  loss: 0.0362 (0.0360)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3220/3449]  eta: 0:13:19  lr: 0.000100  loss: 0.0343 (0.0360)  time: 3.4869  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3230/3449]  eta: 0:12:44  lr: 0.000100  loss: 0.0325 (0.0360)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3240/3449]  eta: 0:12:09  lr: 0.000100  loss: 0.0365 (0.0360)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3250/3449]  eta: 0:11:34  lr: 0.000100  loss: 0.0371 (0.0360)  time: 3.4914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3260/3449]  eta: 0:10:59  lr: 0.000100  loss: 0.0346 (0.0360)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3270/3449]  eta: 0:10:24  lr: 0.000100  loss: 0.0343 (0.0360)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3280/3449]  eta: 0:09:49  lr: 0.000100  loss: 0.0338 (0.0360)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3290/3449]  eta: 0:09:15  lr: 0.000100  loss: 0.0335 (0.0360)  time: 3.4944  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3300/3449]  eta: 0:08:40  lr: 0.000100  loss: 0.0341 (0.0360)  time: 3.4939  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3310/3449]  eta: 0:08:05  lr: 0.000100  loss: 0.0355 (0.0360)  time: 3.4945  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [3320/3449]  eta: 0:07:30  lr: 0.000100  loss: 0.0365 (0.0360)  time: 3.4964  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3330/3449]  eta: 0:06:55  lr: 0.000100  loss: 0.0360 (0.0360)  time: 3.4964  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3340/3449]  eta: 0:06:20  lr: 0.000100  loss: 0.0360 (0.0360)  time: 3.4966  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3350/3449]  eta: 0:05:45  lr: 0.000100  loss: 0.0356 (0.0360)  time: 3.4969  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3360/3449]  eta: 0:05:10  lr: 0.000100  loss: 0.0324 (0.0360)  time: 3.4961  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3370/3449]  eta: 0:04:35  lr: 0.000100  loss: 0.0329 (0.0360)  time: 3.4953  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [3380/3449]  eta: 0:04:00  lr: 0.000100  loss: 0.0322 (0.0360)  time: 3.4942  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [3390/3449]  eta: 0:03:25  lr: 0.000100  loss: 0.0329 (0.0360)  time: 3.4921  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [3400/3449]  eta: 0:02:51  lr: 0.000100  loss: 0.0338 (0.0360)  time: 3.4907  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:33]  [3410/3449]  eta: 0:02:16  lr: 0.000100  loss: 0.0337 (0.0360)  time: 3.4904  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3420/3449]  eta: 0:01:41  lr: 0.000100  loss: 0.0337 (0.0360)  time: 3.4914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3430/3449]  eta: 0:01:06  lr: 0.000100  loss: 0.0349 (0.0360)  time: 3.4922  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3440/3449]  eta: 0:00:31  lr: 0.000100  loss: 0.0356 (0.0360)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0364 (0.0360)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:33] Total time: 3:20:41 (3.4913 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0364 (0.0360)\n",
      "Valid: [epoch:33]  [ 0/14]  eta: 0:04:23  loss: 0.0323 (0.0323)  time: 18.8414  data: 0.5922  max mem: 34968\n",
      "Valid: [epoch:33]  [13/14]  eta: 0:00:18  loss: 0.0306 (0.0310)  time: 18.2995  data: 0.0425  max mem: 34968\n",
      "Valid: [epoch:33] Total time: 0:04:16 (18.3133 s / it)\n",
      "Averaged stats: loss: 0.0306 (0.0310)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_33_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.031%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:34]  [   0/3449]  eta: 4:35:15  lr: 0.000100  loss: 0.0256 (0.0256)  time: 4.7884  data: 1.3439  max mem: 34968\n",
      "Train: [epoch:34]  [  10/3449]  eta: 3:26:18  lr: 0.000100  loss: 0.0372 (0.0404)  time: 3.5994  data: 0.1224  max mem: 34968\n",
      "Train: [epoch:34]  [  20/3449]  eta: 3:22:37  lr: 0.000100  loss: 0.0367 (0.0389)  time: 3.4835  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [  30/3449]  eta: 3:20:57  lr: 0.000100  loss: 0.0354 (0.0385)  time: 3.4864  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [  40/3449]  eta: 3:19:48  lr: 0.000100  loss: 0.0330 (0.0374)  time: 3.4864  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [  50/3449]  eta: 3:18:54  lr: 0.000100  loss: 0.0336 (0.0368)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [  60/3449]  eta: 3:18:07  lr: 0.000100  loss: 0.0346 (0.0371)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [  70/3449]  eta: 3:17:23  lr: 0.000100  loss: 0.0335 (0.0370)  time: 3.4901  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [  80/3449]  eta: 3:16:42  lr: 0.000100  loss: 0.0335 (0.0372)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [  90/3449]  eta: 3:16:01  lr: 0.000100  loss: 0.0363 (0.0369)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 100/3449]  eta: 3:15:22  lr: 0.000100  loss: 0.0360 (0.0366)  time: 3.4890  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 110/3449]  eta: 3:14:43  lr: 0.000100  loss: 0.0326 (0.0365)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 120/3449]  eta: 3:14:06  lr: 0.000100  loss: 0.0335 (0.0362)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 130/3449]  eta: 3:13:29  lr: 0.000100  loss: 0.0345 (0.0364)  time: 3.4907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 140/3449]  eta: 3:12:52  lr: 0.000100  loss: 0.0348 (0.0363)  time: 3.4907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 150/3449]  eta: 3:12:16  lr: 0.000100  loss: 0.0352 (0.0363)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 160/3449]  eta: 3:11:39  lr: 0.000100  loss: 0.0356 (0.0362)  time: 3.4893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 170/3449]  eta: 3:11:03  lr: 0.000100  loss: 0.0377 (0.0368)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 180/3449]  eta: 3:10:26  lr: 0.000100  loss: 0.0370 (0.0367)  time: 3.4863  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 190/3449]  eta: 3:09:49  lr: 0.000100  loss: 0.0338 (0.0368)  time: 3.4844  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 200/3449]  eta: 3:09:12  lr: 0.000100  loss: 0.0391 (0.0371)  time: 3.4850  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 210/3449]  eta: 3:08:36  lr: 0.000100  loss: 0.0371 (0.0371)  time: 3.4848  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 220/3449]  eta: 3:08:00  lr: 0.000100  loss: 0.0363 (0.0371)  time: 3.4844  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 230/3449]  eta: 3:07:24  lr: 0.000100  loss: 0.0371 (0.0371)  time: 3.4853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 240/3449]  eta: 3:06:47  lr: 0.000100  loss: 0.0383 (0.0371)  time: 3.4848  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 250/3449]  eta: 3:06:11  lr: 0.000100  loss: 0.0376 (0.0371)  time: 3.4832  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 260/3449]  eta: 3:05:35  lr: 0.000100  loss: 0.0349 (0.0370)  time: 3.4830  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 270/3449]  eta: 3:04:59  lr: 0.000100  loss: 0.0343 (0.0371)  time: 3.4841  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 280/3449]  eta: 3:04:24  lr: 0.000100  loss: 0.0337 (0.0370)  time: 3.4841  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:34]  [ 290/3449]  eta: 3:03:48  lr: 0.000100  loss: 0.0332 (0.0370)  time: 3.4837  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 300/3449]  eta: 3:03:12  lr: 0.000100  loss: 0.0351 (0.0372)  time: 3.4840  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 310/3449]  eta: 3:02:37  lr: 0.000100  loss: 0.0352 (0.0370)  time: 3.4842  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 320/3449]  eta: 3:02:01  lr: 0.000100  loss: 0.0329 (0.0370)  time: 3.4841  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 330/3449]  eta: 3:01:26  lr: 0.000100  loss: 0.0373 (0.0371)  time: 3.4836  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 340/3449]  eta: 3:00:50  lr: 0.000100  loss: 0.0368 (0.0370)  time: 3.4856  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 350/3449]  eta: 3:00:15  lr: 0.000100  loss: 0.0349 (0.0369)  time: 3.4890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 360/3449]  eta: 2:59:41  lr: 0.000100  loss: 0.0340 (0.0369)  time: 3.4914  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 370/3449]  eta: 2:59:06  lr: 0.000100  loss: 0.0341 (0.0368)  time: 3.4928  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 380/3449]  eta: 2:58:31  lr: 0.000100  loss: 0.0341 (0.0369)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 390/3449]  eta: 2:57:57  lr: 0.000100  loss: 0.0368 (0.0369)  time: 3.4917  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 400/3449]  eta: 2:57:22  lr: 0.000100  loss: 0.0347 (0.0368)  time: 3.4905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 410/3449]  eta: 2:56:47  lr: 0.000100  loss: 0.0347 (0.0368)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 420/3449]  eta: 2:56:12  lr: 0.000100  loss: 0.0331 (0.0367)  time: 3.4905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 430/3449]  eta: 2:55:37  lr: 0.000100  loss: 0.0331 (0.0367)  time: 3.4884  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 440/3449]  eta: 2:55:02  lr: 0.000100  loss: 0.0351 (0.0367)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 450/3449]  eta: 2:54:27  lr: 0.000100  loss: 0.0347 (0.0366)  time: 3.4886  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 460/3449]  eta: 2:53:52  lr: 0.000100  loss: 0.0355 (0.0366)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 470/3449]  eta: 2:53:17  lr: 0.000100  loss: 0.0372 (0.0366)  time: 3.4886  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 480/3449]  eta: 2:52:42  lr: 0.000100  loss: 0.0340 (0.0366)  time: 3.4887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 490/3449]  eta: 2:52:07  lr: 0.000100  loss: 0.0347 (0.0366)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 500/3449]  eta: 2:51:32  lr: 0.000100  loss: 0.0366 (0.0366)  time: 3.4874  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 510/3449]  eta: 2:50:57  lr: 0.000100  loss: 0.0360 (0.0366)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 520/3449]  eta: 2:50:22  lr: 0.000100  loss: 0.0347 (0.0366)  time: 3.4912  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 530/3449]  eta: 2:49:47  lr: 0.000100  loss: 0.0370 (0.0366)  time: 3.4905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 540/3449]  eta: 2:49:12  lr: 0.000100  loss: 0.0357 (0.0366)  time: 3.4896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 550/3449]  eta: 2:48:37  lr: 0.000100  loss: 0.0350 (0.0366)  time: 3.4889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 560/3449]  eta: 2:48:02  lr: 0.000100  loss: 0.0321 (0.0365)  time: 3.4878  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 570/3449]  eta: 2:47:27  lr: 0.000100  loss: 0.0321 (0.0366)  time: 3.4894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 580/3449]  eta: 2:46:52  lr: 0.000100  loss: 0.0378 (0.0366)  time: 3.4904  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 590/3449]  eta: 2:46:17  lr: 0.000100  loss: 0.0359 (0.0366)  time: 3.4898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 600/3449]  eta: 2:45:42  lr: 0.000100  loss: 0.0376 (0.0367)  time: 3.4890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 610/3449]  eta: 2:45:07  lr: 0.000100  loss: 0.0357 (0.0367)  time: 3.4889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 620/3449]  eta: 2:44:33  lr: 0.000100  loss: 0.0336 (0.0366)  time: 3.4907  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 630/3449]  eta: 2:43:58  lr: 0.000100  loss: 0.0336 (0.0366)  time: 3.4903  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 640/3449]  eta: 2:43:23  lr: 0.000100  loss: 0.0337 (0.0366)  time: 3.4889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 650/3449]  eta: 2:42:48  lr: 0.000100  loss: 0.0380 (0.0367)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 660/3449]  eta: 2:42:13  lr: 0.000100  loss: 0.0370 (0.0367)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 670/3449]  eta: 2:41:38  lr: 0.000100  loss: 0.0357 (0.0367)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 680/3449]  eta: 2:41:03  lr: 0.000100  loss: 0.0381 (0.0368)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 690/3449]  eta: 2:40:28  lr: 0.000100  loss: 0.0393 (0.0368)  time: 3.4909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 700/3449]  eta: 2:39:54  lr: 0.000100  loss: 0.0383 (0.0368)  time: 3.4927  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 710/3449]  eta: 2:39:19  lr: 0.000100  loss: 0.0380 (0.0368)  time: 3.4924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 720/3449]  eta: 2:38:44  lr: 0.000100  loss: 0.0381 (0.0369)  time: 3.4904  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 730/3449]  eta: 2:38:09  lr: 0.000100  loss: 0.0393 (0.0370)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 740/3449]  eta: 2:37:34  lr: 0.000100  loss: 0.0366 (0.0369)  time: 3.4910  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 750/3449]  eta: 2:36:59  lr: 0.000100  loss: 0.0366 (0.0370)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 760/3449]  eta: 2:36:24  lr: 0.000100  loss: 0.0396 (0.0370)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 770/3449]  eta: 2:35:49  lr: 0.000100  loss: 0.0372 (0.0370)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 780/3449]  eta: 2:35:14  lr: 0.000100  loss: 0.0393 (0.0371)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 790/3449]  eta: 2:34:40  lr: 0.000100  loss: 0.0393 (0.0371)  time: 3.4904  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 800/3449]  eta: 2:34:05  lr: 0.000100  loss: 0.0342 (0.0371)  time: 3.4891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [ 810/3449]  eta: 2:33:30  lr: 0.000100  loss: 0.0331 (0.0370)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 820/3449]  eta: 2:32:55  lr: 0.000100  loss: 0.0353 (0.0371)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 830/3449]  eta: 2:32:20  lr: 0.000100  loss: 0.0399 (0.0372)  time: 3.4915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 840/3449]  eta: 2:31:45  lr: 0.000100  loss: 0.0370 (0.0372)  time: 3.4914  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 850/3449]  eta: 2:31:10  lr: 0.000100  loss: 0.0347 (0.0372)  time: 3.4920  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 860/3449]  eta: 2:30:35  lr: 0.000100  loss: 0.0363 (0.0373)  time: 3.4928  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 870/3449]  eta: 2:30:01  lr: 0.000100  loss: 0.0391 (0.0374)  time: 3.4923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 880/3449]  eta: 2:29:26  lr: 0.000100  loss: 0.0370 (0.0374)  time: 3.4935  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 890/3449]  eta: 2:28:51  lr: 0.000100  loss: 0.0359 (0.0374)  time: 3.4916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 900/3449]  eta: 2:28:16  lr: 0.000100  loss: 0.0351 (0.0374)  time: 3.4896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 910/3449]  eta: 2:27:41  lr: 0.000100  loss: 0.0344 (0.0374)  time: 3.4910  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 920/3449]  eta: 2:27:06  lr: 0.000100  loss: 0.0343 (0.0374)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 930/3449]  eta: 2:26:31  lr: 0.000100  loss: 0.0346 (0.0374)  time: 3.4883  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 940/3449]  eta: 2:25:56  lr: 0.000100  loss: 0.0353 (0.0374)  time: 3.4894  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:34]  [ 950/3449]  eta: 2:25:21  lr: 0.000100  loss: 0.0353 (0.0375)  time: 3.4902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 960/3449]  eta: 2:24:47  lr: 0.000100  loss: 0.0376 (0.0375)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 970/3449]  eta: 2:24:12  lr: 0.000100  loss: 0.0418 (0.0376)  time: 3.4911  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 980/3449]  eta: 2:23:37  lr: 0.000100  loss: 0.0367 (0.0376)  time: 3.4901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [ 990/3449]  eta: 2:23:02  lr: 0.000100  loss: 0.0353 (0.0376)  time: 3.4903  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1000/3449]  eta: 2:22:27  lr: 0.000100  loss: 0.0336 (0.0375)  time: 3.4904  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1010/3449]  eta: 2:21:52  lr: 0.000100  loss: 0.0325 (0.0375)  time: 3.4896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1020/3449]  eta: 2:21:17  lr: 0.000100  loss: 0.0398 (0.0376)  time: 3.4890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1030/3449]  eta: 2:20:42  lr: 0.000100  loss: 0.0444 (0.0376)  time: 3.4909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1040/3449]  eta: 2:20:07  lr: 0.000100  loss: 0.0416 (0.0377)  time: 3.4901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1050/3449]  eta: 2:19:32  lr: 0.000100  loss: 0.0365 (0.0377)  time: 3.4879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1060/3449]  eta: 2:18:57  lr: 0.000100  loss: 0.0361 (0.0377)  time: 3.4873  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1070/3449]  eta: 2:18:22  lr: 0.000100  loss: 0.0355 (0.0377)  time: 3.4868  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1080/3449]  eta: 2:17:47  lr: 0.000100  loss: 0.0343 (0.0377)  time: 3.4861  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1090/3449]  eta: 2:17:12  lr: 0.000100  loss: 0.0347 (0.0376)  time: 3.4860  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1100/3449]  eta: 2:16:38  lr: 0.000100  loss: 0.0350 (0.0377)  time: 3.4889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1110/3449]  eta: 2:16:03  lr: 0.000100  loss: 0.0350 (0.0377)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1120/3449]  eta: 2:15:28  lr: 0.000100  loss: 0.0358 (0.0377)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1130/3449]  eta: 2:14:53  lr: 0.000100  loss: 0.0358 (0.0377)  time: 3.4915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1140/3449]  eta: 2:14:18  lr: 0.000100  loss: 0.0345 (0.0377)  time: 3.4941  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1150/3449]  eta: 2:13:43  lr: 0.000100  loss: 0.0314 (0.0376)  time: 3.4953  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1160/3449]  eta: 2:13:09  lr: 0.000100  loss: 0.0335 (0.0376)  time: 3.4957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1170/3449]  eta: 2:12:34  lr: 0.000100  loss: 0.0362 (0.0376)  time: 3.4973  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1180/3449]  eta: 2:11:59  lr: 0.000100  loss: 0.0368 (0.0376)  time: 3.4983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1190/3449]  eta: 2:11:24  lr: 0.000100  loss: 0.0368 (0.0377)  time: 3.4977  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1200/3449]  eta: 2:10:49  lr: 0.000100  loss: 0.0349 (0.0376)  time: 3.4969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1210/3449]  eta: 2:10:15  lr: 0.000100  loss: 0.0339 (0.0376)  time: 3.4976  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1220/3449]  eta: 2:09:40  lr: 0.000100  loss: 0.0364 (0.0376)  time: 3.4987  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1230/3449]  eta: 2:09:05  lr: 0.000100  loss: 0.0360 (0.0376)  time: 3.4983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1240/3449]  eta: 2:08:30  lr: 0.000100  loss: 0.0358 (0.0376)  time: 3.4972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1250/3449]  eta: 2:07:56  lr: 0.000100  loss: 0.0358 (0.0376)  time: 3.4964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1260/3449]  eta: 2:07:21  lr: 0.000100  loss: 0.0350 (0.0375)  time: 3.4960  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1270/3449]  eta: 2:06:46  lr: 0.000100  loss: 0.0356 (0.0375)  time: 3.4949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1280/3449]  eta: 2:06:11  lr: 0.000100  loss: 0.0353 (0.0375)  time: 3.4946  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1290/3449]  eta: 2:05:36  lr: 0.000100  loss: 0.0351 (0.0375)  time: 3.4964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1300/3449]  eta: 2:05:01  lr: 0.000100  loss: 0.0362 (0.0375)  time: 3.4964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1310/3449]  eta: 2:04:27  lr: 0.000100  loss: 0.0364 (0.0375)  time: 3.4957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1320/3449]  eta: 2:03:52  lr: 0.000100  loss: 0.0328 (0.0375)  time: 3.4960  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1330/3449]  eta: 2:03:17  lr: 0.000100  loss: 0.0338 (0.0375)  time: 3.4958  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1340/3449]  eta: 2:02:42  lr: 0.000100  loss: 0.0338 (0.0375)  time: 3.4948  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1350/3449]  eta: 2:02:07  lr: 0.000100  loss: 0.0321 (0.0375)  time: 3.4938  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1360/3449]  eta: 2:01:32  lr: 0.000100  loss: 0.0348 (0.0375)  time: 3.4935  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1370/3449]  eta: 2:00:57  lr: 0.000100  loss: 0.0397 (0.0376)  time: 3.4935  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1380/3449]  eta: 2:00:23  lr: 0.000100  loss: 0.0366 (0.0376)  time: 3.4920  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1390/3449]  eta: 1:59:48  lr: 0.000100  loss: 0.0356 (0.0376)  time: 3.4916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1400/3449]  eta: 1:59:13  lr: 0.000100  loss: 0.0367 (0.0375)  time: 3.4927  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1410/3449]  eta: 1:58:38  lr: 0.000100  loss: 0.0359 (0.0376)  time: 3.4930  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1420/3449]  eta: 1:58:03  lr: 0.000100  loss: 0.0358 (0.0375)  time: 3.4932  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1430/3449]  eta: 1:57:28  lr: 0.000100  loss: 0.0320 (0.0375)  time: 3.4924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1440/3449]  eta: 1:56:53  lr: 0.000100  loss: 0.0332 (0.0375)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [1450/3449]  eta: 1:56:18  lr: 0.000100  loss: 0.0354 (0.0375)  time: 3.4909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1460/3449]  eta: 1:55:43  lr: 0.000100  loss: 0.0362 (0.0375)  time: 3.4898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1470/3449]  eta: 1:55:08  lr: 0.000100  loss: 0.0348 (0.0375)  time: 3.4916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1480/3449]  eta: 1:54:34  lr: 0.000100  loss: 0.0338 (0.0375)  time: 3.4911  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1490/3449]  eta: 1:53:59  lr: 0.000100  loss: 0.0363 (0.0375)  time: 3.4894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1500/3449]  eta: 1:53:24  lr: 0.000100  loss: 0.0368 (0.0375)  time: 3.4902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1510/3449]  eta: 1:52:49  lr: 0.000100  loss: 0.0364 (0.0375)  time: 3.4895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1520/3449]  eta: 1:52:14  lr: 0.000100  loss: 0.0353 (0.0375)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1530/3449]  eta: 1:51:39  lr: 0.000100  loss: 0.0347 (0.0375)  time: 3.4895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1540/3449]  eta: 1:51:04  lr: 0.000100  loss: 0.0327 (0.0375)  time: 3.4884  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1550/3449]  eta: 1:50:29  lr: 0.000100  loss: 0.0347 (0.0375)  time: 3.4882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1560/3449]  eta: 1:49:54  lr: 0.000100  loss: 0.0397 (0.0375)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1570/3449]  eta: 1:49:19  lr: 0.000100  loss: 0.0338 (0.0375)  time: 3.4882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1580/3449]  eta: 1:48:44  lr: 0.000100  loss: 0.0338 (0.0375)  time: 3.4902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1590/3449]  eta: 1:48:09  lr: 0.000100  loss: 0.0368 (0.0375)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1600/3449]  eta: 1:47:34  lr: 0.000100  loss: 0.0343 (0.0375)  time: 3.4906  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:34]  [1610/3449]  eta: 1:46:59  lr: 0.000100  loss: 0.0343 (0.0374)  time: 3.4909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1620/3449]  eta: 1:46:25  lr: 0.000100  loss: 0.0357 (0.0375)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1630/3449]  eta: 1:45:50  lr: 0.000100  loss: 0.0347 (0.0374)  time: 3.4896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1640/3449]  eta: 1:45:15  lr: 0.000100  loss: 0.0343 (0.0374)  time: 3.4901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1650/3449]  eta: 1:44:40  lr: 0.000100  loss: 0.0344 (0.0374)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1660/3449]  eta: 1:44:05  lr: 0.000100  loss: 0.0360 (0.0374)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1670/3449]  eta: 1:43:30  lr: 0.000100  loss: 0.0369 (0.0374)  time: 3.4924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1680/3449]  eta: 1:42:55  lr: 0.000100  loss: 0.0377 (0.0374)  time: 3.4931  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1690/3449]  eta: 1:42:20  lr: 0.000100  loss: 0.0374 (0.0374)  time: 3.4921  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1700/3449]  eta: 1:41:45  lr: 0.000100  loss: 0.0362 (0.0374)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1710/3449]  eta: 1:41:10  lr: 0.000100  loss: 0.0349 (0.0374)  time: 3.4928  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1720/3449]  eta: 1:40:36  lr: 0.000100  loss: 0.0366 (0.0374)  time: 3.4936  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1730/3449]  eta: 1:40:01  lr: 0.000100  loss: 0.0381 (0.0374)  time: 3.4932  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1740/3449]  eta: 1:39:26  lr: 0.000100  loss: 0.0380 (0.0375)  time: 3.4919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1750/3449]  eta: 1:38:51  lr: 0.000100  loss: 0.0365 (0.0375)  time: 3.4901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1760/3449]  eta: 1:38:16  lr: 0.000100  loss: 0.0357 (0.0374)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1770/3449]  eta: 1:37:41  lr: 0.000100  loss: 0.0353 (0.0374)  time: 3.4892  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1780/3449]  eta: 1:37:06  lr: 0.000100  loss: 0.0369 (0.0374)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1790/3449]  eta: 1:36:31  lr: 0.000100  loss: 0.0389 (0.0374)  time: 3.4892  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1800/3449]  eta: 1:35:56  lr: 0.000100  loss: 0.0390 (0.0375)  time: 3.4909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1810/3449]  eta: 1:35:21  lr: 0.000100  loss: 0.0429 (0.0375)  time: 3.4907  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1820/3449]  eta: 1:34:46  lr: 0.000100  loss: 0.0400 (0.0375)  time: 3.4896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1830/3449]  eta: 1:34:11  lr: 0.000100  loss: 0.0402 (0.0376)  time: 3.4893  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1840/3449]  eta: 1:33:36  lr: 0.000100  loss: 0.0410 (0.0376)  time: 3.4891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1850/3449]  eta: 1:33:02  lr: 0.000100  loss: 0.0394 (0.0376)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1860/3449]  eta: 1:32:27  lr: 0.000100  loss: 0.0356 (0.0376)  time: 3.4869  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1870/3449]  eta: 1:31:52  lr: 0.000100  loss: 0.0340 (0.0376)  time: 3.4856  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1880/3449]  eta: 1:31:17  lr: 0.000100  loss: 0.0338 (0.0376)  time: 3.4857  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [1890/3449]  eta: 1:30:42  lr: 0.000100  loss: 0.0365 (0.0376)  time: 3.4868  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [1900/3449]  eta: 1:30:07  lr: 0.000100  loss: 0.0369 (0.0376)  time: 3.4879  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [1910/3449]  eta: 1:29:32  lr: 0.000100  loss: 0.0343 (0.0376)  time: 3.4884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [1920/3449]  eta: 1:28:57  lr: 0.000100  loss: 0.0341 (0.0376)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [1930/3449]  eta: 1:28:22  lr: 0.000100  loss: 0.0357 (0.0376)  time: 3.4875  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1940/3449]  eta: 1:27:47  lr: 0.000100  loss: 0.0377 (0.0376)  time: 3.4880  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1950/3449]  eta: 1:27:12  lr: 0.000100  loss: 0.0371 (0.0376)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1960/3449]  eta: 1:26:37  lr: 0.000100  loss: 0.0357 (0.0376)  time: 3.4884  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [1970/3449]  eta: 1:26:02  lr: 0.000100  loss: 0.0350 (0.0376)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [1980/3449]  eta: 1:25:27  lr: 0.000100  loss: 0.0370 (0.0376)  time: 3.4883  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [1990/3449]  eta: 1:24:52  lr: 0.000100  loss: 0.0365 (0.0376)  time: 3.4879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2000/3449]  eta: 1:24:18  lr: 0.000100  loss: 0.0342 (0.0376)  time: 3.4875  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2010/3449]  eta: 1:23:43  lr: 0.000100  loss: 0.0334 (0.0376)  time: 3.4876  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2020/3449]  eta: 1:23:08  lr: 0.000100  loss: 0.0338 (0.0376)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [2030/3449]  eta: 1:22:33  lr: 0.000100  loss: 0.0353 (0.0375)  time: 3.4880  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2040/3449]  eta: 1:21:58  lr: 0.000100  loss: 0.0363 (0.0375)  time: 3.4873  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2050/3449]  eta: 1:21:23  lr: 0.000100  loss: 0.0347 (0.0375)  time: 3.4879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2060/3449]  eta: 1:20:48  lr: 0.000100  loss: 0.0326 (0.0375)  time: 3.4865  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2070/3449]  eta: 1:20:13  lr: 0.000100  loss: 0.0336 (0.0375)  time: 3.4853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2080/3449]  eta: 1:19:38  lr: 0.000100  loss: 0.0375 (0.0375)  time: 3.4860  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2090/3449]  eta: 1:19:03  lr: 0.000100  loss: 0.0354 (0.0375)  time: 3.4863  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2100/3449]  eta: 1:18:28  lr: 0.000100  loss: 0.0340 (0.0375)  time: 3.4867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2110/3449]  eta: 1:17:53  lr: 0.000100  loss: 0.0319 (0.0375)  time: 3.4869  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [2120/3449]  eta: 1:17:18  lr: 0.000100  loss: 0.0334 (0.0375)  time: 3.4862  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [2130/3449]  eta: 1:16:43  lr: 0.000100  loss: 0.0358 (0.0375)  time: 3.4870  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [2140/3449]  eta: 1:16:09  lr: 0.000100  loss: 0.0340 (0.0375)  time: 3.4872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2150/3449]  eta: 1:15:34  lr: 0.000100  loss: 0.0331 (0.0375)  time: 3.4869  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2160/3449]  eta: 1:14:59  lr: 0.000100  loss: 0.0344 (0.0375)  time: 3.4870  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2170/3449]  eta: 1:14:24  lr: 0.000100  loss: 0.0343 (0.0374)  time: 3.4873  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2180/3449]  eta: 1:13:49  lr: 0.000100  loss: 0.0337 (0.0374)  time: 3.4881  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2190/3449]  eta: 1:13:14  lr: 0.000100  loss: 0.0346 (0.0374)  time: 3.4886  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2200/3449]  eta: 1:12:39  lr: 0.000100  loss: 0.0377 (0.0374)  time: 3.4892  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2210/3449]  eta: 1:12:04  lr: 0.000100  loss: 0.0365 (0.0374)  time: 3.4877  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2220/3449]  eta: 1:11:29  lr: 0.000100  loss: 0.0335 (0.0374)  time: 3.4872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2230/3449]  eta: 1:10:54  lr: 0.000100  loss: 0.0350 (0.0374)  time: 3.4881  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2240/3449]  eta: 1:10:19  lr: 0.000100  loss: 0.0366 (0.0374)  time: 3.4887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2250/3449]  eta: 1:09:44  lr: 0.000100  loss: 0.0368 (0.0375)  time: 3.4896  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [2260/3449]  eta: 1:09:10  lr: 0.000100  loss: 0.0371 (0.0375)  time: 3.4891  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:34]  [2270/3449]  eta: 1:08:35  lr: 0.000100  loss: 0.0353 (0.0375)  time: 3.4883  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2280/3449]  eta: 1:08:00  lr: 0.000100  loss: 0.0348 (0.0375)  time: 3.4894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2290/3449]  eta: 1:07:25  lr: 0.000100  loss: 0.0363 (0.0375)  time: 3.4887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2300/3449]  eta: 1:06:50  lr: 0.000100  loss: 0.0355 (0.0375)  time: 3.4869  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2310/3449]  eta: 1:06:15  lr: 0.000100  loss: 0.0347 (0.0375)  time: 3.4862  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2320/3449]  eta: 1:05:40  lr: 0.000100  loss: 0.0347 (0.0375)  time: 3.4852  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2330/3449]  eta: 1:05:05  lr: 0.000100  loss: 0.0355 (0.0375)  time: 3.4850  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2340/3449]  eta: 1:04:30  lr: 0.000100  loss: 0.0349 (0.0374)  time: 3.4857  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2350/3449]  eta: 1:03:55  lr: 0.000100  loss: 0.0352 (0.0375)  time: 3.4864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2360/3449]  eta: 1:03:20  lr: 0.000100  loss: 0.0372 (0.0375)  time: 3.4867  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2370/3449]  eta: 1:02:45  lr: 0.000100  loss: 0.0355 (0.0375)  time: 3.4850  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2380/3449]  eta: 1:02:10  lr: 0.000100  loss: 0.0355 (0.0375)  time: 3.4831  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2390/3449]  eta: 1:01:36  lr: 0.000100  loss: 0.0340 (0.0374)  time: 3.4838  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2400/3449]  eta: 1:01:01  lr: 0.000100  loss: 0.0318 (0.0374)  time: 3.4842  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2410/3449]  eta: 1:00:26  lr: 0.000100  loss: 0.0306 (0.0374)  time: 3.4852  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2420/3449]  eta: 0:59:51  lr: 0.000100  loss: 0.0356 (0.0374)  time: 3.4856  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2430/3449]  eta: 0:59:16  lr: 0.000100  loss: 0.0357 (0.0374)  time: 3.4841  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2440/3449]  eta: 0:58:41  lr: 0.000100  loss: 0.0349 (0.0374)  time: 3.4833  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2450/3449]  eta: 0:58:06  lr: 0.000100  loss: 0.0346 (0.0374)  time: 3.4828  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [2460/3449]  eta: 0:57:31  lr: 0.000100  loss: 0.0352 (0.0374)  time: 3.4814  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2470/3449]  eta: 0:56:56  lr: 0.000100  loss: 0.0349 (0.0374)  time: 3.4828  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2480/3449]  eta: 0:56:21  lr: 0.000100  loss: 0.0364 (0.0374)  time: 3.4857  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2490/3449]  eta: 0:55:46  lr: 0.000100  loss: 0.0370 (0.0374)  time: 3.4851  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2500/3449]  eta: 0:55:11  lr: 0.000100  loss: 0.0362 (0.0374)  time: 3.4841  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2510/3449]  eta: 0:54:36  lr: 0.000100  loss: 0.0362 (0.0374)  time: 3.4852  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2520/3449]  eta: 0:54:02  lr: 0.000100  loss: 0.0346 (0.0374)  time: 3.4846  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2530/3449]  eta: 0:53:27  lr: 0.000100  loss: 0.0326 (0.0374)  time: 3.4834  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2540/3449]  eta: 0:52:52  lr: 0.000100  loss: 0.0317 (0.0374)  time: 3.4849  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2550/3449]  eta: 0:52:17  lr: 0.000100  loss: 0.0311 (0.0374)  time: 3.4861  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2560/3449]  eta: 0:51:42  lr: 0.000100  loss: 0.0352 (0.0374)  time: 3.4862  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2570/3449]  eta: 0:51:07  lr: 0.000100  loss: 0.0366 (0.0374)  time: 3.4864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2580/3449]  eta: 0:50:32  lr: 0.000100  loss: 0.0353 (0.0374)  time: 3.4863  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2590/3449]  eta: 0:49:57  lr: 0.000100  loss: 0.0351 (0.0374)  time: 3.4860  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2600/3449]  eta: 0:49:22  lr: 0.000100  loss: 0.0349 (0.0374)  time: 3.4864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2610/3449]  eta: 0:48:47  lr: 0.000100  loss: 0.0320 (0.0373)  time: 3.4853  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2620/3449]  eta: 0:48:12  lr: 0.000100  loss: 0.0344 (0.0373)  time: 3.4857  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [2630/3449]  eta: 0:47:38  lr: 0.000100  loss: 0.0363 (0.0373)  time: 3.4876  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [2640/3449]  eta: 0:47:03  lr: 0.000100  loss: 0.0352 (0.0373)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [2650/3449]  eta: 0:46:28  lr: 0.000100  loss: 0.0345 (0.0373)  time: 3.4854  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2660/3449]  eta: 0:45:53  lr: 0.000100  loss: 0.0350 (0.0374)  time: 3.4844  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2670/3449]  eta: 0:45:18  lr: 0.000100  loss: 0.0353 (0.0374)  time: 3.4846  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2680/3449]  eta: 0:44:43  lr: 0.000100  loss: 0.0365 (0.0374)  time: 3.4856  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2690/3449]  eta: 0:44:08  lr: 0.000100  loss: 0.0349 (0.0374)  time: 3.4869  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2700/3449]  eta: 0:43:33  lr: 0.000100  loss: 0.0366 (0.0374)  time: 3.4866  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2710/3449]  eta: 0:42:58  lr: 0.000100  loss: 0.0376 (0.0374)  time: 3.4887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2720/3449]  eta: 0:42:23  lr: 0.000100  loss: 0.0361 (0.0374)  time: 3.4909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2730/3449]  eta: 0:41:48  lr: 0.000100  loss: 0.0352 (0.0374)  time: 3.4902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2740/3449]  eta: 0:41:14  lr: 0.000100  loss: 0.0352 (0.0374)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2750/3449]  eta: 0:40:39  lr: 0.000100  loss: 0.0352 (0.0374)  time: 3.4925  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2760/3449]  eta: 0:40:04  lr: 0.000100  loss: 0.0337 (0.0374)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2770/3449]  eta: 0:39:29  lr: 0.000100  loss: 0.0375 (0.0374)  time: 3.4920  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2780/3449]  eta: 0:38:54  lr: 0.000100  loss: 0.0347 (0.0374)  time: 3.4915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2790/3449]  eta: 0:38:19  lr: 0.000100  loss: 0.0321 (0.0373)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2800/3449]  eta: 0:37:44  lr: 0.000100  loss: 0.0306 (0.0373)  time: 3.4914  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2810/3449]  eta: 0:37:09  lr: 0.000100  loss: 0.0334 (0.0373)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2820/3449]  eta: 0:36:34  lr: 0.000100  loss: 0.0370 (0.0373)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2830/3449]  eta: 0:36:00  lr: 0.000100  loss: 0.0384 (0.0373)  time: 3.4909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2840/3449]  eta: 0:35:25  lr: 0.000100  loss: 0.0340 (0.0373)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2850/3449]  eta: 0:34:50  lr: 0.000100  loss: 0.0322 (0.0373)  time: 3.4919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2860/3449]  eta: 0:34:15  lr: 0.000100  loss: 0.0371 (0.0373)  time: 3.4910  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2870/3449]  eta: 0:33:40  lr: 0.000100  loss: 0.0360 (0.0373)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [2880/3449]  eta: 0:33:05  lr: 0.000100  loss: 0.0344 (0.0373)  time: 3.4934  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2890/3449]  eta: 0:32:30  lr: 0.000100  loss: 0.0373 (0.0373)  time: 3.4949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2900/3449]  eta: 0:31:55  lr: 0.000100  loss: 0.0393 (0.0373)  time: 3.4941  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2910/3449]  eta: 0:31:20  lr: 0.000100  loss: 0.0393 (0.0374)  time: 3.4935  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2920/3449]  eta: 0:30:46  lr: 0.000100  loss: 0.0375 (0.0374)  time: 3.4907  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:34]  [2930/3449]  eta: 0:30:11  lr: 0.000100  loss: 0.0375 (0.0374)  time: 3.4907  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2940/3449]  eta: 0:29:36  lr: 0.000100  loss: 0.0372 (0.0374)  time: 3.4926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2950/3449]  eta: 0:29:01  lr: 0.000100  loss: 0.0367 (0.0374)  time: 3.4902  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2960/3449]  eta: 0:28:26  lr: 0.000100  loss: 0.0358 (0.0374)  time: 3.4892  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2970/3449]  eta: 0:27:51  lr: 0.000100  loss: 0.0360 (0.0374)  time: 3.4892  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2980/3449]  eta: 0:27:16  lr: 0.000100  loss: 0.0379 (0.0374)  time: 3.4895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [2990/3449]  eta: 0:26:41  lr: 0.000100  loss: 0.0354 (0.0374)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [3000/3449]  eta: 0:26:06  lr: 0.000100  loss: 0.0343 (0.0373)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [3010/3449]  eta: 0:25:31  lr: 0.000100  loss: 0.0334 (0.0373)  time: 3.4869  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [3020/3449]  eta: 0:24:57  lr: 0.000100  loss: 0.0355 (0.0373)  time: 3.4869  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3030/3449]  eta: 0:24:22  lr: 0.000100  loss: 0.0355 (0.0373)  time: 3.4860  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3040/3449]  eta: 0:23:47  lr: 0.000100  loss: 0.0348 (0.0373)  time: 3.4847  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3050/3449]  eta: 0:23:12  lr: 0.000100  loss: 0.0346 (0.0373)  time: 3.4848  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3060/3449]  eta: 0:22:37  lr: 0.000100  loss: 0.0340 (0.0373)  time: 3.4839  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3070/3449]  eta: 0:22:02  lr: 0.000100  loss: 0.0339 (0.0373)  time: 3.4823  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3080/3449]  eta: 0:21:27  lr: 0.000100  loss: 0.0341 (0.0373)  time: 3.4815  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3090/3449]  eta: 0:20:52  lr: 0.000100  loss: 0.0341 (0.0373)  time: 3.4813  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3100/3449]  eta: 0:20:17  lr: 0.000100  loss: 0.0347 (0.0373)  time: 3.4835  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3110/3449]  eta: 0:19:42  lr: 0.000100  loss: 0.0373 (0.0373)  time: 3.4854  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3120/3449]  eta: 0:19:08  lr: 0.000100  loss: 0.0385 (0.0373)  time: 3.4863  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3130/3449]  eta: 0:18:33  lr: 0.000100  loss: 0.0339 (0.0373)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3140/3449]  eta: 0:17:58  lr: 0.000100  loss: 0.0339 (0.0373)  time: 3.4903  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3150/3449]  eta: 0:17:23  lr: 0.000100  loss: 0.0358 (0.0373)  time: 3.4889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3160/3449]  eta: 0:16:48  lr: 0.000100  loss: 0.0355 (0.0373)  time: 3.4900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3170/3449]  eta: 0:16:13  lr: 0.000100  loss: 0.0376 (0.0373)  time: 3.4896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3180/3449]  eta: 0:15:38  lr: 0.000100  loss: 0.0344 (0.0373)  time: 3.4890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3190/3449]  eta: 0:15:03  lr: 0.000100  loss: 0.0344 (0.0373)  time: 3.4911  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3200/3449]  eta: 0:14:28  lr: 0.000100  loss: 0.0353 (0.0373)  time: 3.4915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3210/3449]  eta: 0:13:53  lr: 0.000100  loss: 0.0383 (0.0373)  time: 3.4904  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3220/3449]  eta: 0:13:19  lr: 0.000100  loss: 0.0369 (0.0373)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3230/3449]  eta: 0:12:44  lr: 0.000100  loss: 0.0369 (0.0373)  time: 3.4911  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3240/3449]  eta: 0:12:09  lr: 0.000100  loss: 0.0375 (0.0373)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3250/3449]  eta: 0:11:34  lr: 0.000100  loss: 0.0373 (0.0373)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3260/3449]  eta: 0:10:59  lr: 0.000100  loss: 0.0367 (0.0373)  time: 3.4887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3270/3449]  eta: 0:10:24  lr: 0.000100  loss: 0.0333 (0.0373)  time: 3.4889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3280/3449]  eta: 0:09:49  lr: 0.000100  loss: 0.0353 (0.0373)  time: 3.4901  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3290/3449]  eta: 0:09:14  lr: 0.000100  loss: 0.0347 (0.0373)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3300/3449]  eta: 0:08:39  lr: 0.000100  loss: 0.0347 (0.0373)  time: 3.4909  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3310/3449]  eta: 0:08:05  lr: 0.000100  loss: 0.0363 (0.0373)  time: 3.4907  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3320/3449]  eta: 0:07:30  lr: 0.000100  loss: 0.0381 (0.0373)  time: 3.4896  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3330/3449]  eta: 0:06:55  lr: 0.000100  loss: 0.0352 (0.0373)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3340/3449]  eta: 0:06:20  lr: 0.000100  loss: 0.0359 (0.0373)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [3350/3449]  eta: 0:05:45  lr: 0.000100  loss: 0.0369 (0.0373)  time: 3.4891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3360/3449]  eta: 0:05:10  lr: 0.000100  loss: 0.0364 (0.0373)  time: 3.4891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3370/3449]  eta: 0:04:35  lr: 0.000100  loss: 0.0358 (0.0373)  time: 3.4903  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3380/3449]  eta: 0:04:00  lr: 0.000100  loss: 0.0367 (0.0373)  time: 3.4924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3390/3449]  eta: 0:03:25  lr: 0.000100  loss: 0.0372 (0.0373)  time: 3.4928  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3400/3449]  eta: 0:02:50  lr: 0.000100  loss: 0.0388 (0.0373)  time: 3.4918  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [3410/3449]  eta: 0:02:16  lr: 0.000100  loss: 0.0407 (0.0373)  time: 3.4930  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [3420/3449]  eta: 0:01:41  lr: 0.000100  loss: 0.0425 (0.0374)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:34]  [3430/3449]  eta: 0:01:06  lr: 0.000100  loss: 0.0398 (0.0374)  time: 3.4911  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3440/3449]  eta: 0:00:31  lr: 0.000100  loss: 0.0360 (0.0374)  time: 3.4913  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0348 (0.0374)  time: 3.4900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:34] Total time: 3:20:36 (3.4897 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0348 (0.0374)\n",
      "Valid: [epoch:34]  [ 0/14]  eta: 0:04:28  loss: 0.0350 (0.0350)  time: 19.1544  data: 0.9123  max mem: 34968\n",
      "Valid: [epoch:34]  [13/14]  eta: 0:00:18  loss: 0.0316 (0.0320)  time: 18.3223  data: 0.0654  max mem: 34968\n",
      "Valid: [epoch:34] Total time: 0:04:16 (18.3345 s / it)\n",
      "Averaged stats: loss: 0.0316 (0.0320)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_34_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.032%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:35]  [   0/3449]  eta: 5:51:48  lr: 0.000100  loss: 0.0396 (0.0396)  time: 6.1202  data: 2.6871  max mem: 34968\n",
      "Train: [epoch:35]  [  10/3449]  eta: 3:33:03  lr: 0.000100  loss: 0.0396 (0.0398)  time: 3.7173  data: 0.2444  max mem: 34968\n",
      "Train: [epoch:35]  [  20/3449]  eta: 3:26:29  lr: 0.000100  loss: 0.0371 (0.0384)  time: 3.4878  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [  30/3449]  eta: 3:23:47  lr: 0.000100  loss: 0.0353 (0.0367)  time: 3.4989  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [  40/3449]  eta: 3:22:10  lr: 0.000100  loss: 0.0353 (0.0371)  time: 3.5008  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [  50/3449]  eta: 3:20:56  lr: 0.000100  loss: 0.0339 (0.0360)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [  60/3449]  eta: 3:19:53  lr: 0.000100  loss: 0.0347 (0.0368)  time: 3.4993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [  70/3449]  eta: 3:18:59  lr: 0.000100  loss: 0.0353 (0.0363)  time: 3.4984  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:35]  [  80/3449]  eta: 3:18:08  lr: 0.000100  loss: 0.0352 (0.0362)  time: 3.4981  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [  90/3449]  eta: 3:17:20  lr: 0.000100  loss: 0.0358 (0.0369)  time: 3.4959  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 100/3449]  eta: 3:16:36  lr: 0.000100  loss: 0.0364 (0.0372)  time: 3.4968  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 110/3449]  eta: 3:15:53  lr: 0.000100  loss: 0.0360 (0.0376)  time: 3.4980  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 120/3449]  eta: 3:15:12  lr: 0.000100  loss: 0.0346 (0.0379)  time: 3.4970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 130/3449]  eta: 3:14:31  lr: 0.000100  loss: 0.0356 (0.0378)  time: 3.4971  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 140/3449]  eta: 3:13:51  lr: 0.000100  loss: 0.0360 (0.0378)  time: 3.4966  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [ 150/3449]  eta: 3:13:12  lr: 0.000100  loss: 0.0360 (0.0376)  time: 3.4969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 160/3449]  eta: 3:12:34  lr: 0.000100  loss: 0.0337 (0.0375)  time: 3.4982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 170/3449]  eta: 3:11:56  lr: 0.000100  loss: 0.0342 (0.0373)  time: 3.4982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 180/3449]  eta: 3:11:18  lr: 0.000100  loss: 0.0340 (0.0371)  time: 3.4979  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 190/3449]  eta: 3:10:41  lr: 0.000100  loss: 0.0347 (0.0372)  time: 3.4981  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [ 200/3449]  eta: 3:10:04  lr: 0.000100  loss: 0.0375 (0.0371)  time: 3.4980  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [ 210/3449]  eta: 3:09:27  lr: 0.000100  loss: 0.0377 (0.0373)  time: 3.4989  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 220/3449]  eta: 3:08:50  lr: 0.000100  loss: 0.0366 (0.0373)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 230/3449]  eta: 3:08:14  lr: 0.000100  loss: 0.0365 (0.0372)  time: 3.4983  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [ 240/3449]  eta: 3:07:38  lr: 0.000100  loss: 0.0362 (0.0372)  time: 3.4995  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 250/3449]  eta: 3:07:01  lr: 0.000100  loss: 0.0344 (0.0370)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 260/3449]  eta: 3:06:25  lr: 0.000100  loss: 0.0344 (0.0369)  time: 3.4974  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 270/3449]  eta: 3:05:49  lr: 0.000100  loss: 0.0363 (0.0369)  time: 3.4984  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 280/3449]  eta: 3:05:13  lr: 0.000100  loss: 0.0363 (0.0369)  time: 3.4981  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 290/3449]  eta: 3:04:37  lr: 0.000100  loss: 0.0398 (0.0371)  time: 3.5001  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 300/3449]  eta: 3:04:01  lr: 0.000100  loss: 0.0375 (0.0370)  time: 3.5025  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 310/3449]  eta: 3:03:26  lr: 0.000100  loss: 0.0359 (0.0370)  time: 3.5005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 320/3449]  eta: 3:02:50  lr: 0.000100  loss: 0.0379 (0.0371)  time: 3.4988  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 330/3449]  eta: 3:02:14  lr: 0.000100  loss: 0.0381 (0.0371)  time: 3.4993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 340/3449]  eta: 3:01:39  lr: 0.000100  loss: 0.0356 (0.0372)  time: 3.4993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 350/3449]  eta: 3:01:03  lr: 0.000100  loss: 0.0341 (0.0371)  time: 3.4989  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 360/3449]  eta: 3:00:27  lr: 0.000100  loss: 0.0341 (0.0372)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 370/3449]  eta: 2:59:52  lr: 0.000100  loss: 0.0363 (0.0372)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 380/3449]  eta: 2:59:16  lr: 0.000100  loss: 0.0363 (0.0372)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 390/3449]  eta: 2:58:41  lr: 0.000100  loss: 0.0380 (0.0374)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 400/3449]  eta: 2:58:05  lr: 0.000100  loss: 0.0370 (0.0373)  time: 3.4998  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 410/3449]  eta: 2:57:30  lr: 0.000100  loss: 0.0369 (0.0374)  time: 3.4999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 420/3449]  eta: 2:56:55  lr: 0.000100  loss: 0.0359 (0.0373)  time: 3.4994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 430/3449]  eta: 2:56:19  lr: 0.000100  loss: 0.0356 (0.0372)  time: 3.4983  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 440/3449]  eta: 2:55:44  lr: 0.000100  loss: 0.0340 (0.0372)  time: 3.4992  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 450/3449]  eta: 2:55:08  lr: 0.000100  loss: 0.0363 (0.0374)  time: 3.4999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 460/3449]  eta: 2:54:33  lr: 0.000100  loss: 0.0401 (0.0375)  time: 3.5005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 470/3449]  eta: 2:53:58  lr: 0.000100  loss: 0.0370 (0.0375)  time: 3.5021  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 480/3449]  eta: 2:53:23  lr: 0.000100  loss: 0.0370 (0.0376)  time: 3.5018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 490/3449]  eta: 2:52:48  lr: 0.000100  loss: 0.0407 (0.0376)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 500/3449]  eta: 2:52:13  lr: 0.000100  loss: 0.0387 (0.0376)  time: 3.5038  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 510/3449]  eta: 2:51:38  lr: 0.000100  loss: 0.0381 (0.0376)  time: 3.5027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 520/3449]  eta: 2:51:03  lr: 0.000100  loss: 0.0381 (0.0376)  time: 3.5037  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 530/3449]  eta: 2:50:28  lr: 0.000100  loss: 0.0386 (0.0376)  time: 3.5052  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 540/3449]  eta: 2:49:53  lr: 0.000100  loss: 0.0404 (0.0377)  time: 3.5051  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 550/3449]  eta: 2:49:18  lr: 0.000100  loss: 0.0390 (0.0376)  time: 3.5055  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 560/3449]  eta: 2:48:43  lr: 0.000100  loss: 0.0334 (0.0376)  time: 3.5047  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 570/3449]  eta: 2:48:08  lr: 0.000100  loss: 0.0338 (0.0376)  time: 3.5027  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 580/3449]  eta: 2:47:32  lr: 0.000100  loss: 0.0380 (0.0376)  time: 3.5021  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 590/3449]  eta: 2:46:57  lr: 0.000100  loss: 0.0388 (0.0377)  time: 3.5012  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 600/3449]  eta: 2:46:22  lr: 0.000100  loss: 0.0367 (0.0377)  time: 3.5007  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 610/3449]  eta: 2:45:47  lr: 0.000100  loss: 0.0356 (0.0377)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 620/3449]  eta: 2:45:11  lr: 0.000100  loss: 0.0357 (0.0377)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 630/3449]  eta: 2:44:36  lr: 0.000100  loss: 0.0345 (0.0377)  time: 3.4997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 640/3449]  eta: 2:44:01  lr: 0.000100  loss: 0.0343 (0.0378)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 650/3449]  eta: 2:43:26  lr: 0.000100  loss: 0.0384 (0.0379)  time: 3.5013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 660/3449]  eta: 2:42:51  lr: 0.000100  loss: 0.0372 (0.0378)  time: 3.5009  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 670/3449]  eta: 2:42:16  lr: 0.000100  loss: 0.0368 (0.0378)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 680/3449]  eta: 2:41:41  lr: 0.000100  loss: 0.0385 (0.0379)  time: 3.5010  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 690/3449]  eta: 2:41:06  lr: 0.000100  loss: 0.0361 (0.0378)  time: 3.5022  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 700/3449]  eta: 2:40:30  lr: 0.000100  loss: 0.0351 (0.0378)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 710/3449]  eta: 2:39:55  lr: 0.000100  loss: 0.0352 (0.0378)  time: 3.4997  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 720/3449]  eta: 2:39:20  lr: 0.000100  loss: 0.0395 (0.0378)  time: 3.5018  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 730/3449]  eta: 2:38:45  lr: 0.000100  loss: 0.0395 (0.0379)  time: 3.5022  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:35]  [ 740/3449]  eta: 2:38:10  lr: 0.000100  loss: 0.0370 (0.0378)  time: 3.4999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 750/3449]  eta: 2:37:35  lr: 0.000100  loss: 0.0353 (0.0378)  time: 3.4999  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 760/3449]  eta: 2:37:00  lr: 0.000100  loss: 0.0373 (0.0378)  time: 3.4994  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [ 770/3449]  eta: 2:36:24  lr: 0.000100  loss: 0.0374 (0.0378)  time: 3.4989  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 780/3449]  eta: 2:35:49  lr: 0.000100  loss: 0.0359 (0.0378)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 790/3449]  eta: 2:35:14  lr: 0.000100  loss: 0.0355 (0.0378)  time: 3.4990  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 800/3449]  eta: 2:34:39  lr: 0.000100  loss: 0.0355 (0.0377)  time: 3.4993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 810/3449]  eta: 2:34:04  lr: 0.000100  loss: 0.0347 (0.0377)  time: 3.4996  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 820/3449]  eta: 2:33:29  lr: 0.000100  loss: 0.0335 (0.0377)  time: 3.4994  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 830/3449]  eta: 2:32:54  lr: 0.000100  loss: 0.0350 (0.0377)  time: 3.5005  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 840/3449]  eta: 2:32:18  lr: 0.000100  loss: 0.0342 (0.0377)  time: 3.5011  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 850/3449]  eta: 2:31:43  lr: 0.000100  loss: 0.0342 (0.0377)  time: 3.5013  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 860/3449]  eta: 2:31:08  lr: 0.000100  loss: 0.0392 (0.0378)  time: 3.5029  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 870/3449]  eta: 2:30:33  lr: 0.000100  loss: 0.0394 (0.0379)  time: 3.5032  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 880/3449]  eta: 2:29:58  lr: 0.000100  loss: 0.0368 (0.0379)  time: 3.5026  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 890/3449]  eta: 2:29:23  lr: 0.000100  loss: 0.0368 (0.0379)  time: 3.5032  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 900/3449]  eta: 2:28:48  lr: 0.000100  loss: 0.0359 (0.0379)  time: 3.5025  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [ 910/3449]  eta: 2:28:13  lr: 0.000100  loss: 0.0372 (0.0379)  time: 3.5009  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [ 920/3449]  eta: 2:27:38  lr: 0.000100  loss: 0.0375 (0.0379)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 930/3449]  eta: 2:27:03  lr: 0.000100  loss: 0.0356 (0.0379)  time: 3.5024  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 940/3449]  eta: 2:26:28  lr: 0.000100  loss: 0.0362 (0.0380)  time: 3.5030  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 950/3449]  eta: 2:25:53  lr: 0.000100  loss: 0.0404 (0.0380)  time: 3.5028  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 960/3449]  eta: 2:25:18  lr: 0.000100  loss: 0.0388 (0.0380)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 970/3449]  eta: 2:24:43  lr: 0.000100  loss: 0.0351 (0.0380)  time: 3.5014  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 980/3449]  eta: 2:24:08  lr: 0.000100  loss: 0.0354 (0.0380)  time: 3.5019  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [ 990/3449]  eta: 2:23:33  lr: 0.000100  loss: 0.0354 (0.0380)  time: 3.5015  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1000/3449]  eta: 2:22:58  lr: 0.000100  loss: 0.0363 (0.0379)  time: 3.5026  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1010/3449]  eta: 2:22:23  lr: 0.000100  loss: 0.0360 (0.0379)  time: 3.5024  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1020/3449]  eta: 2:21:48  lr: 0.000100  loss: 0.0417 (0.0380)  time: 3.5017  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1030/3449]  eta: 2:21:13  lr: 0.000100  loss: 0.0417 (0.0380)  time: 3.5028  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1040/3449]  eta: 2:20:38  lr: 0.000100  loss: 0.0387 (0.0381)  time: 3.5017  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1050/3449]  eta: 2:20:02  lr: 0.000100  loss: 0.0379 (0.0380)  time: 3.5003  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1060/3449]  eta: 2:19:27  lr: 0.000100  loss: 0.0370 (0.0381)  time: 3.4993  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1070/3449]  eta: 2:18:52  lr: 0.000100  loss: 0.0374 (0.0381)  time: 3.4972  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1080/3449]  eta: 2:18:17  lr: 0.000100  loss: 0.0381 (0.0381)  time: 3.4971  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1090/3449]  eta: 2:17:42  lr: 0.000100  loss: 0.0403 (0.0381)  time: 3.4973  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1100/3449]  eta: 2:17:07  lr: 0.000100  loss: 0.0383 (0.0381)  time: 3.4970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1110/3449]  eta: 2:16:32  lr: 0.000100  loss: 0.0372 (0.0381)  time: 3.4950  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1120/3449]  eta: 2:15:56  lr: 0.000100  loss: 0.0380 (0.0381)  time: 3.4913  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1130/3449]  eta: 2:15:21  lr: 0.000100  loss: 0.0363 (0.0381)  time: 3.4894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1140/3449]  eta: 2:14:46  lr: 0.000100  loss: 0.0370 (0.0381)  time: 3.4892  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1150/3449]  eta: 2:14:10  lr: 0.000100  loss: 0.0370 (0.0381)  time: 3.4892  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1160/3449]  eta: 2:13:35  lr: 0.000100  loss: 0.0359 (0.0381)  time: 3.4892  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1170/3449]  eta: 2:13:00  lr: 0.000100  loss: 0.0354 (0.0381)  time: 3.4894  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1180/3449]  eta: 2:12:25  lr: 0.000100  loss: 0.0357 (0.0381)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1190/3449]  eta: 2:11:49  lr: 0.000100  loss: 0.0368 (0.0382)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1200/3449]  eta: 2:11:14  lr: 0.000100  loss: 0.0347 (0.0382)  time: 3.4900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1210/3449]  eta: 2:10:39  lr: 0.000100  loss: 0.0342 (0.0381)  time: 3.4905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1220/3449]  eta: 2:10:04  lr: 0.000100  loss: 0.0346 (0.0381)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1230/3449]  eta: 2:09:29  lr: 0.000100  loss: 0.0346 (0.0381)  time: 3.4910  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1240/3449]  eta: 2:08:53  lr: 0.000100  loss: 0.0362 (0.0381)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1250/3449]  eta: 2:08:18  lr: 0.000100  loss: 0.0364 (0.0381)  time: 3.4919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1260/3449]  eta: 2:07:43  lr: 0.000100  loss: 0.0357 (0.0381)  time: 3.4916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1270/3449]  eta: 2:07:08  lr: 0.000100  loss: 0.0357 (0.0381)  time: 3.4918  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1280/3449]  eta: 2:06:33  lr: 0.000100  loss: 0.0344 (0.0380)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1290/3449]  eta: 2:05:57  lr: 0.000100  loss: 0.0331 (0.0380)  time: 3.4898  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1300/3449]  eta: 2:05:22  lr: 0.000100  loss: 0.0370 (0.0380)  time: 3.4899  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1310/3449]  eta: 2:04:47  lr: 0.000100  loss: 0.0362 (0.0380)  time: 3.4893  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1320/3449]  eta: 2:04:12  lr: 0.000100  loss: 0.0368 (0.0380)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1330/3449]  eta: 2:03:37  lr: 0.000100  loss: 0.0370 (0.0380)  time: 3.4904  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1340/3449]  eta: 2:03:02  lr: 0.000100  loss: 0.0356 (0.0380)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1350/3449]  eta: 2:02:26  lr: 0.000100  loss: 0.0362 (0.0380)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1360/3449]  eta: 2:01:51  lr: 0.000100  loss: 0.0394 (0.0380)  time: 3.4890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1370/3449]  eta: 2:01:16  lr: 0.000100  loss: 0.0383 (0.0380)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1380/3449]  eta: 2:00:41  lr: 0.000100  loss: 0.0361 (0.0380)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1390/3449]  eta: 2:00:06  lr: 0.000100  loss: 0.0374 (0.0380)  time: 3.4882  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:35]  [1400/3449]  eta: 1:59:31  lr: 0.000100  loss: 0.0364 (0.0380)  time: 3.4880  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1410/3449]  eta: 1:58:55  lr: 0.000100  loss: 0.0367 (0.0380)  time: 3.4869  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1420/3449]  eta: 1:58:20  lr: 0.000100  loss: 0.0365 (0.0380)  time: 3.4862  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1430/3449]  eta: 1:57:45  lr: 0.000100  loss: 0.0354 (0.0380)  time: 3.4864  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1440/3449]  eta: 1:57:10  lr: 0.000100  loss: 0.0345 (0.0379)  time: 3.4866  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1450/3449]  eta: 1:56:35  lr: 0.000100  loss: 0.0372 (0.0379)  time: 3.4863  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1460/3449]  eta: 1:55:59  lr: 0.000100  loss: 0.0345 (0.0379)  time: 3.4869  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1470/3449]  eta: 1:55:24  lr: 0.000100  loss: 0.0308 (0.0379)  time: 3.4882  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1480/3449]  eta: 1:54:49  lr: 0.000100  loss: 0.0331 (0.0379)  time: 3.4880  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1490/3449]  eta: 1:54:14  lr: 0.000100  loss: 0.0331 (0.0378)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1500/3449]  eta: 1:53:39  lr: 0.000100  loss: 0.0373 (0.0378)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1510/3449]  eta: 1:53:04  lr: 0.000100  loss: 0.0410 (0.0379)  time: 3.4887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1520/3449]  eta: 1:52:29  lr: 0.000100  loss: 0.0388 (0.0379)  time: 3.4897  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1530/3449]  eta: 1:51:54  lr: 0.000100  loss: 0.0356 (0.0378)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1540/3449]  eta: 1:51:19  lr: 0.000100  loss: 0.0394 (0.0379)  time: 3.4918  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1550/3449]  eta: 1:50:44  lr: 0.000100  loss: 0.0387 (0.0379)  time: 3.4927  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1560/3449]  eta: 1:50:08  lr: 0.000100  loss: 0.0342 (0.0379)  time: 3.4931  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1570/3449]  eta: 1:49:33  lr: 0.000100  loss: 0.0378 (0.0379)  time: 3.4927  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1580/3449]  eta: 1:48:58  lr: 0.000100  loss: 0.0378 (0.0379)  time: 3.4933  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1590/3449]  eta: 1:48:23  lr: 0.000100  loss: 0.0360 (0.0379)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1600/3449]  eta: 1:47:48  lr: 0.000100  loss: 0.0367 (0.0379)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1610/3449]  eta: 1:47:13  lr: 0.000100  loss: 0.0367 (0.0379)  time: 3.4908  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1620/3449]  eta: 1:46:38  lr: 0.000100  loss: 0.0353 (0.0379)  time: 3.4905  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1630/3449]  eta: 1:46:03  lr: 0.000100  loss: 0.0353 (0.0379)  time: 3.4911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1640/3449]  eta: 1:45:28  lr: 0.000100  loss: 0.0362 (0.0379)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1650/3449]  eta: 1:44:53  lr: 0.000100  loss: 0.0362 (0.0379)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1660/3449]  eta: 1:44:18  lr: 0.000100  loss: 0.0408 (0.0380)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1670/3449]  eta: 1:43:43  lr: 0.000100  loss: 0.0439 (0.0380)  time: 3.4917  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1680/3449]  eta: 1:43:08  lr: 0.000100  loss: 0.0401 (0.0380)  time: 3.4924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1690/3449]  eta: 1:42:33  lr: 0.000100  loss: 0.0400 (0.0380)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1700/3449]  eta: 1:41:58  lr: 0.000100  loss: 0.0370 (0.0380)  time: 3.4914  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1710/3449]  eta: 1:41:23  lr: 0.000100  loss: 0.0349 (0.0380)  time: 3.4924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1720/3449]  eta: 1:40:48  lr: 0.000100  loss: 0.0356 (0.0380)  time: 3.4927  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1730/3449]  eta: 1:40:13  lr: 0.000100  loss: 0.0359 (0.0380)  time: 3.4928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1740/3449]  eta: 1:39:38  lr: 0.000100  loss: 0.0366 (0.0380)  time: 3.4931  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1750/3449]  eta: 1:39:03  lr: 0.000100  loss: 0.0374 (0.0380)  time: 3.4929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1760/3449]  eta: 1:38:27  lr: 0.000100  loss: 0.0351 (0.0380)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1770/3449]  eta: 1:37:52  lr: 0.000100  loss: 0.0344 (0.0379)  time: 3.4919  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1780/3449]  eta: 1:37:17  lr: 0.000100  loss: 0.0357 (0.0379)  time: 3.4916  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1790/3449]  eta: 1:36:42  lr: 0.000100  loss: 0.0346 (0.0379)  time: 3.4919  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1800/3449]  eta: 1:36:07  lr: 0.000100  loss: 0.0345 (0.0379)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1810/3449]  eta: 1:35:32  lr: 0.000100  loss: 0.0361 (0.0379)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1820/3449]  eta: 1:34:57  lr: 0.000100  loss: 0.0353 (0.0379)  time: 3.4935  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1830/3449]  eta: 1:34:22  lr: 0.000100  loss: 0.0372 (0.0379)  time: 3.4947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1840/3449]  eta: 1:33:47  lr: 0.000100  loss: 0.0375 (0.0379)  time: 3.4953  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1850/3449]  eta: 1:33:12  lr: 0.000100  loss: 0.0378 (0.0379)  time: 3.4943  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1860/3449]  eta: 1:32:37  lr: 0.000100  loss: 0.0392 (0.0379)  time: 3.4933  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1870/3449]  eta: 1:32:02  lr: 0.000100  loss: 0.0344 (0.0379)  time: 3.4928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1880/3449]  eta: 1:31:27  lr: 0.000100  loss: 0.0335 (0.0379)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1890/3449]  eta: 1:30:52  lr: 0.000100  loss: 0.0335 (0.0379)  time: 3.4908  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1900/3449]  eta: 1:30:17  lr: 0.000100  loss: 0.0337 (0.0379)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1910/3449]  eta: 1:29:42  lr: 0.000100  loss: 0.0361 (0.0379)  time: 3.4926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [1920/3449]  eta: 1:29:07  lr: 0.000100  loss: 0.0367 (0.0379)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1930/3449]  eta: 1:28:32  lr: 0.000100  loss: 0.0364 (0.0379)  time: 3.4909  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1940/3449]  eta: 1:27:57  lr: 0.000100  loss: 0.0349 (0.0379)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1950/3449]  eta: 1:27:22  lr: 0.000100  loss: 0.0368 (0.0379)  time: 3.4898  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:35]  [1960/3449]  eta: 1:26:47  lr: 0.000100  loss: 0.0350 (0.0379)  time: 3.4891  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:35]  [1970/3449]  eta: 1:26:12  lr: 0.000100  loss: 0.0335 (0.0379)  time: 3.4876  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1980/3449]  eta: 1:25:37  lr: 0.000100  loss: 0.0334 (0.0378)  time: 3.4890  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [1990/3449]  eta: 1:25:02  lr: 0.000100  loss: 0.0346 (0.0379)  time: 3.4886  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2000/3449]  eta: 1:24:27  lr: 0.000100  loss: 0.0346 (0.0379)  time: 3.4863  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2010/3449]  eta: 1:23:52  lr: 0.000100  loss: 0.0344 (0.0379)  time: 3.4856  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2020/3449]  eta: 1:23:17  lr: 0.000100  loss: 0.0341 (0.0379)  time: 3.4852  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2030/3449]  eta: 1:22:42  lr: 0.000100  loss: 0.0386 (0.0379)  time: 3.4840  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2040/3449]  eta: 1:22:07  lr: 0.000100  loss: 0.0387 (0.0379)  time: 3.4843  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2050/3449]  eta: 1:21:32  lr: 0.000100  loss: 0.0362 (0.0379)  time: 3.4864  data: 0.0003  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:35]  [2060/3449]  eta: 1:20:57  lr: 0.000100  loss: 0.0353 (0.0379)  time: 3.4863  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:35]  [2070/3449]  eta: 1:20:21  lr: 0.000100  loss: 0.0354 (0.0379)  time: 3.4855  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2080/3449]  eta: 1:19:46  lr: 0.000100  loss: 0.0408 (0.0379)  time: 3.4855  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2090/3449]  eta: 1:19:11  lr: 0.000100  loss: 0.0376 (0.0379)  time: 3.4858  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2100/3449]  eta: 1:18:36  lr: 0.000100  loss: 0.0363 (0.0379)  time: 3.4865  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2110/3449]  eta: 1:18:01  lr: 0.000100  loss: 0.0332 (0.0379)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2120/3449]  eta: 1:17:26  lr: 0.000100  loss: 0.0350 (0.0379)  time: 3.4868  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2130/3449]  eta: 1:16:51  lr: 0.000100  loss: 0.0350 (0.0379)  time: 3.4870  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2140/3449]  eta: 1:16:16  lr: 0.000100  loss: 0.0343 (0.0379)  time: 3.4878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2150/3449]  eta: 1:15:41  lr: 0.000100  loss: 0.0379 (0.0379)  time: 3.4877  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2160/3449]  eta: 1:15:06  lr: 0.000100  loss: 0.0379 (0.0379)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2170/3449]  eta: 1:14:31  lr: 0.000100  loss: 0.0386 (0.0380)  time: 3.4870  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2180/3449]  eta: 1:13:56  lr: 0.000100  loss: 0.0393 (0.0380)  time: 3.4869  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2190/3449]  eta: 1:13:21  lr: 0.000100  loss: 0.0368 (0.0380)  time: 3.4870  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2200/3449]  eta: 1:12:46  lr: 0.000100  loss: 0.0367 (0.0380)  time: 3.4878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2210/3449]  eta: 1:12:11  lr: 0.000100  loss: 0.0366 (0.0380)  time: 3.4885  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2220/3449]  eta: 1:11:36  lr: 0.000100  loss: 0.0366 (0.0380)  time: 3.4897  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2230/3449]  eta: 1:11:01  lr: 0.000100  loss: 0.0355 (0.0380)  time: 3.4910  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2240/3449]  eta: 1:10:26  lr: 0.000100  loss: 0.0355 (0.0380)  time: 3.4929  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2250/3449]  eta: 1:09:51  lr: 0.000100  loss: 0.0400 (0.0380)  time: 3.4938  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2260/3449]  eta: 1:09:16  lr: 0.000100  loss: 0.0424 (0.0381)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2270/3449]  eta: 1:08:41  lr: 0.000100  loss: 0.0400 (0.0381)  time: 3.4925  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2280/3449]  eta: 1:08:06  lr: 0.000100  loss: 0.0358 (0.0381)  time: 3.4920  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2290/3449]  eta: 1:07:31  lr: 0.000100  loss: 0.0371 (0.0381)  time: 3.4926  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2300/3449]  eta: 1:06:56  lr: 0.000100  loss: 0.0370 (0.0381)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2310/3449]  eta: 1:06:21  lr: 0.000100  loss: 0.0319 (0.0380)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2320/3449]  eta: 1:05:46  lr: 0.000100  loss: 0.0335 (0.0381)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2330/3449]  eta: 1:05:11  lr: 0.000100  loss: 0.0365 (0.0380)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2340/3449]  eta: 1:04:36  lr: 0.000100  loss: 0.0338 (0.0380)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2350/3449]  eta: 1:04:01  lr: 0.000100  loss: 0.0351 (0.0380)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2360/3449]  eta: 1:03:26  lr: 0.000100  loss: 0.0375 (0.0380)  time: 3.4874  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2370/3449]  eta: 1:02:51  lr: 0.000100  loss: 0.0376 (0.0380)  time: 3.4880  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2380/3449]  eta: 1:02:16  lr: 0.000100  loss: 0.0366 (0.0381)  time: 3.4875  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2390/3449]  eta: 1:01:41  lr: 0.000100  loss: 0.0368 (0.0380)  time: 3.4884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2400/3449]  eta: 1:01:06  lr: 0.000100  loss: 0.0368 (0.0380)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2410/3449]  eta: 1:00:31  lr: 0.000100  loss: 0.0376 (0.0381)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2420/3449]  eta: 0:59:56  lr: 0.000100  loss: 0.0379 (0.0381)  time: 3.4891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2430/3449]  eta: 0:59:22  lr: 0.000100  loss: 0.0363 (0.0381)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2440/3449]  eta: 0:58:47  lr: 0.000100  loss: 0.0338 (0.0380)  time: 3.4903  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2450/3449]  eta: 0:58:12  lr: 0.000100  loss: 0.0338 (0.0380)  time: 3.4898  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:35]  [2460/3449]  eta: 0:57:37  lr: 0.000100  loss: 0.0347 (0.0380)  time: 3.4887  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:35]  [2470/3449]  eta: 0:57:02  lr: 0.000100  loss: 0.0339 (0.0380)  time: 3.4873  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2480/3449]  eta: 0:56:27  lr: 0.000100  loss: 0.0353 (0.0380)  time: 3.4875  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2490/3449]  eta: 0:55:52  lr: 0.000100  loss: 0.0370 (0.0380)  time: 3.4874  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2500/3449]  eta: 0:55:17  lr: 0.000100  loss: 0.0355 (0.0380)  time: 3.4860  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2510/3449]  eta: 0:54:42  lr: 0.000100  loss: 0.0353 (0.0380)  time: 3.4868  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2520/3449]  eta: 0:54:07  lr: 0.000100  loss: 0.0342 (0.0380)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2530/3449]  eta: 0:53:32  lr: 0.000100  loss: 0.0347 (0.0380)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2540/3449]  eta: 0:52:57  lr: 0.000100  loss: 0.0355 (0.0380)  time: 3.4916  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2550/3449]  eta: 0:52:22  lr: 0.000100  loss: 0.0365 (0.0380)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2560/3449]  eta: 0:51:47  lr: 0.000100  loss: 0.0364 (0.0379)  time: 3.4891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2570/3449]  eta: 0:51:12  lr: 0.000100  loss: 0.0368 (0.0380)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2580/3449]  eta: 0:50:37  lr: 0.000100  loss: 0.0348 (0.0379)  time: 3.4883  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:35]  [2590/3449]  eta: 0:50:02  lr: 0.000100  loss: 0.0324 (0.0379)  time: 3.4880  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:35]  [2600/3449]  eta: 0:49:27  lr: 0.000100  loss: 0.0343 (0.0379)  time: 3.4879  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2610/3449]  eta: 0:48:52  lr: 0.000100  loss: 0.0341 (0.0379)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2620/3449]  eta: 0:48:17  lr: 0.000100  loss: 0.0358 (0.0379)  time: 3.4879  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2630/3449]  eta: 0:47:42  lr: 0.000100  loss: 0.0354 (0.0379)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2640/3449]  eta: 0:47:07  lr: 0.000100  loss: 0.0336 (0.0379)  time: 3.4911  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2650/3449]  eta: 0:46:32  lr: 0.000100  loss: 0.0339 (0.0379)  time: 3.4924  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2660/3449]  eta: 0:45:57  lr: 0.000100  loss: 0.0378 (0.0379)  time: 3.4931  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2670/3449]  eta: 0:45:22  lr: 0.000100  loss: 0.0388 (0.0379)  time: 3.4929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2680/3449]  eta: 0:44:47  lr: 0.000100  loss: 0.0373 (0.0379)  time: 3.4921  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2690/3449]  eta: 0:44:12  lr: 0.000100  loss: 0.0384 (0.0379)  time: 3.4926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2700/3449]  eta: 0:43:37  lr: 0.000100  loss: 0.0382 (0.0379)  time: 3.4929  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2710/3449]  eta: 0:43:02  lr: 0.000100  loss: 0.0366 (0.0379)  time: 3.4921  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:35]  [2720/3449]  eta: 0:42:27  lr: 0.000100  loss: 0.0366 (0.0379)  time: 3.4912  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2730/3449]  eta: 0:41:52  lr: 0.000100  loss: 0.0349 (0.0379)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2740/3449]  eta: 0:41:17  lr: 0.000100  loss: 0.0349 (0.0379)  time: 3.4905  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2750/3449]  eta: 0:40:42  lr: 0.000100  loss: 0.0371 (0.0379)  time: 3.4906  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2760/3449]  eta: 0:40:08  lr: 0.000100  loss: 0.0371 (0.0380)  time: 3.4914  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2770/3449]  eta: 0:39:33  lr: 0.000100  loss: 0.0370 (0.0380)  time: 3.4935  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2780/3449]  eta: 0:38:58  lr: 0.000100  loss: 0.0364 (0.0380)  time: 3.4937  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2790/3449]  eta: 0:38:23  lr: 0.000100  loss: 0.0385 (0.0380)  time: 3.4915  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2800/3449]  eta: 0:37:48  lr: 0.000100  loss: 0.0351 (0.0380)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2810/3449]  eta: 0:37:13  lr: 0.000100  loss: 0.0339 (0.0380)  time: 3.4900  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2820/3449]  eta: 0:36:38  lr: 0.000100  loss: 0.0362 (0.0380)  time: 3.4914  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2830/3449]  eta: 0:36:03  lr: 0.000100  loss: 0.0362 (0.0380)  time: 3.4917  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2840/3449]  eta: 0:35:28  lr: 0.000100  loss: 0.0345 (0.0379)  time: 3.4923  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2850/3449]  eta: 0:34:53  lr: 0.000100  loss: 0.0370 (0.0380)  time: 3.4939  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [2860/3449]  eta: 0:34:18  lr: 0.000100  loss: 0.0376 (0.0380)  time: 3.4946  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2870/3449]  eta: 0:33:43  lr: 0.000100  loss: 0.0361 (0.0380)  time: 3.4949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2880/3449]  eta: 0:33:08  lr: 0.000100  loss: 0.0361 (0.0380)  time: 3.4958  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2890/3449]  eta: 0:32:33  lr: 0.000100  loss: 0.0362 (0.0380)  time: 3.4961  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2900/3449]  eta: 0:31:58  lr: 0.000100  loss: 0.0362 (0.0380)  time: 3.4957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2910/3449]  eta: 0:31:23  lr: 0.000100  loss: 0.0375 (0.0380)  time: 3.4957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2920/3449]  eta: 0:30:48  lr: 0.000100  loss: 0.0376 (0.0380)  time: 3.4957  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2930/3449]  eta: 0:30:13  lr: 0.000100  loss: 0.0371 (0.0380)  time: 3.4947  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2940/3449]  eta: 0:29:38  lr: 0.000100  loss: 0.0382 (0.0380)  time: 3.4948  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2950/3449]  eta: 0:29:03  lr: 0.000100  loss: 0.0360 (0.0380)  time: 3.4955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2960/3449]  eta: 0:28:28  lr: 0.000100  loss: 0.0358 (0.0380)  time: 3.4959  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2970/3449]  eta: 0:27:54  lr: 0.000100  loss: 0.0376 (0.0380)  time: 3.4967  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2980/3449]  eta: 0:27:19  lr: 0.000100  loss: 0.0376 (0.0380)  time: 3.4969  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [2990/3449]  eta: 0:26:44  lr: 0.000100  loss: 0.0368 (0.0380)  time: 3.4970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3000/3449]  eta: 0:26:09  lr: 0.000100  loss: 0.0367 (0.0380)  time: 3.4970  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3010/3449]  eta: 0:25:34  lr: 0.000100  loss: 0.0353 (0.0380)  time: 3.4965  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3020/3449]  eta: 0:24:59  lr: 0.000100  loss: 0.0353 (0.0380)  time: 3.4961  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [3030/3449]  eta: 0:24:24  lr: 0.000100  loss: 0.0389 (0.0380)  time: 3.4962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3040/3449]  eta: 0:23:49  lr: 0.000100  loss: 0.0389 (0.0380)  time: 3.4964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3050/3449]  eta: 0:23:14  lr: 0.000100  loss: 0.0366 (0.0380)  time: 3.4959  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3060/3449]  eta: 0:22:39  lr: 0.000100  loss: 0.0357 (0.0380)  time: 3.4952  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3070/3449]  eta: 0:22:04  lr: 0.000100  loss: 0.0399 (0.0380)  time: 3.4955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3080/3449]  eta: 0:21:29  lr: 0.000100  loss: 0.0407 (0.0380)  time: 3.4956  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3090/3449]  eta: 0:20:54  lr: 0.000100  loss: 0.0352 (0.0380)  time: 3.4955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3100/3449]  eta: 0:20:19  lr: 0.000100  loss: 0.0336 (0.0380)  time: 3.4963  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3110/3449]  eta: 0:19:44  lr: 0.000100  loss: 0.0395 (0.0380)  time: 3.4958  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3120/3449]  eta: 0:19:09  lr: 0.000100  loss: 0.0407 (0.0380)  time: 3.4945  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3130/3449]  eta: 0:18:34  lr: 0.000100  loss: 0.0370 (0.0380)  time: 3.4938  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3140/3449]  eta: 0:17:59  lr: 0.000100  loss: 0.0366 (0.0380)  time: 3.4939  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3150/3449]  eta: 0:17:24  lr: 0.000100  loss: 0.0361 (0.0380)  time: 3.4935  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3160/3449]  eta: 0:16:50  lr: 0.000100  loss: 0.0352 (0.0380)  time: 3.4924  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3170/3449]  eta: 0:16:15  lr: 0.000100  loss: 0.0385 (0.0380)  time: 3.4922  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3180/3449]  eta: 0:15:40  lr: 0.000100  loss: 0.0381 (0.0380)  time: 3.4930  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3190/3449]  eta: 0:15:05  lr: 0.000100  loss: 0.0354 (0.0380)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [3200/3449]  eta: 0:14:30  lr: 0.000100  loss: 0.0348 (0.0380)  time: 3.4931  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3210/3449]  eta: 0:13:55  lr: 0.000100  loss: 0.0349 (0.0381)  time: 3.4931  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3220/3449]  eta: 0:13:20  lr: 0.000100  loss: 0.0370 (0.0381)  time: 3.4938  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3230/3449]  eta: 0:12:45  lr: 0.000100  loss: 0.0363 (0.0381)  time: 3.4942  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3240/3449]  eta: 0:12:10  lr: 0.000100  loss: 0.0373 (0.0381)  time: 3.4944  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3250/3449]  eta: 0:11:35  lr: 0.000100  loss: 0.0441 (0.0381)  time: 3.4943  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3260/3449]  eta: 0:11:00  lr: 0.000100  loss: 0.0424 (0.0381)  time: 3.4937  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3270/3449]  eta: 0:10:25  lr: 0.000100  loss: 0.0407 (0.0381)  time: 3.4926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3280/3449]  eta: 0:09:50  lr: 0.000100  loss: 0.0408 (0.0381)  time: 3.4928  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [3290/3449]  eta: 0:09:15  lr: 0.000100  loss: 0.0362 (0.0381)  time: 3.4945  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3300/3449]  eta: 0:08:40  lr: 0.000100  loss: 0.0366 (0.0381)  time: 3.4955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3310/3449]  eta: 0:08:05  lr: 0.000100  loss: 0.0376 (0.0382)  time: 3.4955  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3320/3449]  eta: 0:07:30  lr: 0.000100  loss: 0.0378 (0.0382)  time: 3.4949  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3330/3449]  eta: 0:06:55  lr: 0.000100  loss: 0.0372 (0.0382)  time: 3.4936  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [3340/3449]  eta: 0:06:20  lr: 0.000100  loss: 0.0359 (0.0382)  time: 3.4932  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [3350/3449]  eta: 0:05:45  lr: 0.000100  loss: 0.0369 (0.0382)  time: 3.4934  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [3360/3449]  eta: 0:05:11  lr: 0.000100  loss: 0.0357 (0.0382)  time: 3.4927  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [3370/3449]  eta: 0:04:36  lr: 0.000100  loss: 0.0352 (0.0382)  time: 3.4918  data: 0.0001  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:35]  [3380/3449]  eta: 0:04:01  lr: 0.000100  loss: 0.0358 (0.0382)  time: 3.4915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3390/3449]  eta: 0:03:26  lr: 0.000100  loss: 0.0347 (0.0382)  time: 3.4917  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3400/3449]  eta: 0:02:51  lr: 0.000100  loss: 0.0363 (0.0382)  time: 3.4930  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [3410/3449]  eta: 0:02:16  lr: 0.000100  loss: 0.0391 (0.0382)  time: 3.4927  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [3420/3449]  eta: 0:01:41  lr: 0.000100  loss: 0.0404 (0.0382)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:35]  [3430/3449]  eta: 0:01:06  lr: 0.000100  loss: 0.0362 (0.0382)  time: 3.4923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3440/3449]  eta: 0:00:31  lr: 0.000100  loss: 0.0362 (0.0382)  time: 3.4926  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0373 (0.0382)  time: 3.4923  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:35] Total time: 3:20:53 (3.4949 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0373 (0.0382)\n",
      "Valid: [epoch:35]  [ 0/14]  eta: 0:04:23  loss: 0.0344 (0.0344)  time: 18.8248  data: 0.5848  max mem: 34968\n",
      "Valid: [epoch:35]  [13/14]  eta: 0:00:18  loss: 0.0325 (0.0328)  time: 18.2910  data: 0.0420  max mem: 34968\n",
      "Valid: [epoch:35] Total time: 0:04:16 (18.3043 s / it)\n",
      "Averaged stats: loss: 0.0325 (0.0328)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_35_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.033%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:36]  [   0/3449]  eta: 4:31:25  lr: 0.000100  loss: 0.0420 (0.0420)  time: 4.7217  data: 1.2670  max mem: 34968\n",
      "Train: [epoch:36]  [  10/3449]  eta: 3:26:11  lr: 0.000100  loss: 0.0413 (0.0399)  time: 3.5974  data: 0.1153  max mem: 34968\n",
      "Train: [epoch:36]  [  20/3449]  eta: 3:22:35  lr: 0.000100  loss: 0.0371 (0.0375)  time: 3.4861  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [  30/3449]  eta: 3:20:59  lr: 0.000100  loss: 0.0354 (0.0368)  time: 3.4885  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [  40/3449]  eta: 3:19:54  lr: 0.000100  loss: 0.0356 (0.0371)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [  50/3449]  eta: 3:19:02  lr: 0.000100  loss: 0.0363 (0.0374)  time: 3.4927  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [  60/3449]  eta: 3:18:16  lr: 0.000100  loss: 0.0366 (0.0373)  time: 3.4933  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [  70/3449]  eta: 3:17:31  lr: 0.000100  loss: 0.0382 (0.0381)  time: 3.4915  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [  80/3449]  eta: 3:16:48  lr: 0.000100  loss: 0.0368 (0.0383)  time: 3.4895  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [  90/3449]  eta: 3:16:07  lr: 0.000100  loss: 0.0365 (0.0382)  time: 3.4890  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 100/3449]  eta: 3:15:27  lr: 0.000100  loss: 0.0369 (0.0382)  time: 3.4891  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 110/3449]  eta: 3:14:47  lr: 0.000100  loss: 0.0356 (0.0381)  time: 3.4873  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 120/3449]  eta: 3:14:08  lr: 0.000100  loss: 0.0348 (0.0378)  time: 3.4854  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 130/3449]  eta: 3:13:30  lr: 0.000100  loss: 0.0365 (0.0380)  time: 3.4851  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 140/3449]  eta: 3:12:52  lr: 0.000100  loss: 0.0366 (0.0381)  time: 3.4855  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 150/3449]  eta: 3:12:15  lr: 0.000100  loss: 0.0366 (0.0380)  time: 3.4873  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 160/3449]  eta: 3:11:38  lr: 0.000100  loss: 0.0378 (0.0380)  time: 3.4881  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 170/3449]  eta: 3:11:02  lr: 0.000100  loss: 0.0361 (0.0382)  time: 3.4889  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 180/3449]  eta: 3:10:26  lr: 0.000100  loss: 0.0355 (0.0378)  time: 3.4893  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 190/3449]  eta: 3:09:50  lr: 0.000100  loss: 0.0360 (0.0378)  time: 3.4888  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 200/3449]  eta: 3:09:14  lr: 0.000100  loss: 0.0372 (0.0380)  time: 3.4885  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 210/3449]  eta: 3:08:38  lr: 0.000100  loss: 0.0381 (0.0381)  time: 3.4881  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 220/3449]  eta: 3:08:02  lr: 0.000100  loss: 0.0372 (0.0382)  time: 3.4882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 230/3449]  eta: 3:07:26  lr: 0.000100  loss: 0.0372 (0.0384)  time: 3.4882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 240/3449]  eta: 3:06:50  lr: 0.000100  loss: 0.0395 (0.0385)  time: 3.4879  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 250/3449]  eta: 3:06:15  lr: 0.000100  loss: 0.0405 (0.0385)  time: 3.4880  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 260/3449]  eta: 3:05:39  lr: 0.000100  loss: 0.0413 (0.0388)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 270/3449]  eta: 3:05:03  lr: 0.000100  loss: 0.0396 (0.0390)  time: 3.4850  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 280/3449]  eta: 3:04:27  lr: 0.000100  loss: 0.0384 (0.0390)  time: 3.4848  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 290/3449]  eta: 3:03:52  lr: 0.000100  loss: 0.0383 (0.0390)  time: 3.4872  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 300/3449]  eta: 3:03:16  lr: 0.000100  loss: 0.0364 (0.0388)  time: 3.4881  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 310/3449]  eta: 3:02:41  lr: 0.000100  loss: 0.0361 (0.0387)  time: 3.4882  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 320/3449]  eta: 3:02:06  lr: 0.000100  loss: 0.0372 (0.0389)  time: 3.4887  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 330/3449]  eta: 3:01:31  lr: 0.000100  loss: 0.0387 (0.0388)  time: 3.4881  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 340/3449]  eta: 3:00:55  lr: 0.000100  loss: 0.0349 (0.0390)  time: 3.4876  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 350/3449]  eta: 3:00:20  lr: 0.000100  loss: 0.0376 (0.0389)  time: 3.4874  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:36]  [ 360/3449]  eta: 2:59:45  lr: 0.000100  loss: 0.0383 (0.0389)  time: 3.4883  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 370/3449]  eta: 2:59:10  lr: 0.000100  loss: 0.0383 (0.0390)  time: 3.4903  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 380/3449]  eta: 2:58:35  lr: 0.000100  loss: 0.0373 (0.0391)  time: 3.4907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 390/3449]  eta: 2:58:00  lr: 0.000100  loss: 0.0373 (0.0393)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 400/3449]  eta: 2:57:25  lr: 0.000100  loss: 0.0361 (0.0392)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 410/3449]  eta: 2:56:49  lr: 0.000100  loss: 0.0379 (0.0392)  time: 3.4869  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 420/3449]  eta: 2:56:14  lr: 0.000100  loss: 0.0376 (0.0391)  time: 3.4876  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 430/3449]  eta: 2:55:39  lr: 0.000100  loss: 0.0346 (0.0391)  time: 3.4890  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 440/3449]  eta: 2:55:04  lr: 0.000100  loss: 0.0376 (0.0390)  time: 3.4895  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 450/3449]  eta: 2:54:29  lr: 0.000100  loss: 0.0388 (0.0391)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 460/3449]  eta: 2:53:54  lr: 0.000100  loss: 0.0392 (0.0391)  time: 3.4890  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 470/3449]  eta: 2:53:19  lr: 0.000100  loss: 0.0394 (0.0392)  time: 3.4878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 480/3449]  eta: 2:52:44  lr: 0.000100  loss: 0.0392 (0.0392)  time: 3.4879  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 490/3449]  eta: 2:52:09  lr: 0.000100  loss: 0.0360 (0.0390)  time: 3.4884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 500/3449]  eta: 2:51:34  lr: 0.000100  loss: 0.0344 (0.0390)  time: 3.4881  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 510/3449]  eta: 2:50:59  lr: 0.000100  loss: 0.0365 (0.0391)  time: 3.4866  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 520/3449]  eta: 2:50:23  lr: 0.000100  loss: 0.0357 (0.0390)  time: 3.4858  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:36]  [ 530/3449]  eta: 2:49:48  lr: 0.000100  loss: 0.0335 (0.0391)  time: 3.4864  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 540/3449]  eta: 2:49:13  lr: 0.000100  loss: 0.0375 (0.0391)  time: 3.4866  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 550/3449]  eta: 2:48:38  lr: 0.000100  loss: 0.0373 (0.0390)  time: 3.4860  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 560/3449]  eta: 2:48:03  lr: 0.000100  loss: 0.0373 (0.0391)  time: 3.4851  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 570/3449]  eta: 2:47:28  lr: 0.000100  loss: 0.0369 (0.0391)  time: 3.4844  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 580/3449]  eta: 2:46:52  lr: 0.000100  loss: 0.0386 (0.0390)  time: 3.4828  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 590/3449]  eta: 2:46:17  lr: 0.000100  loss: 0.0373 (0.0391)  time: 3.4814  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 600/3449]  eta: 2:45:42  lr: 0.000100  loss: 0.0368 (0.0390)  time: 3.4819  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 610/3449]  eta: 2:45:06  lr: 0.000100  loss: 0.0372 (0.0391)  time: 3.4823  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 620/3449]  eta: 2:44:31  lr: 0.000100  loss: 0.0388 (0.0391)  time: 3.4820  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 630/3449]  eta: 2:43:56  lr: 0.000100  loss: 0.0374 (0.0390)  time: 3.4821  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 640/3449]  eta: 2:43:21  lr: 0.000100  loss: 0.0377 (0.0390)  time: 3.4836  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 650/3449]  eta: 2:42:46  lr: 0.000100  loss: 0.0393 (0.0390)  time: 3.4840  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 660/3449]  eta: 2:42:11  lr: 0.000100  loss: 0.0393 (0.0390)  time: 3.4851  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 670/3449]  eta: 2:41:36  lr: 0.000100  loss: 0.0416 (0.0392)  time: 3.4857  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 680/3449]  eta: 2:41:01  lr: 0.000100  loss: 0.0471 (0.0394)  time: 3.4846  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 690/3449]  eta: 2:40:26  lr: 0.000100  loss: 0.0446 (0.0394)  time: 3.4845  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 700/3449]  eta: 2:39:50  lr: 0.000100  loss: 0.0388 (0.0394)  time: 3.4834  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 710/3449]  eta: 2:39:15  lr: 0.000100  loss: 0.0376 (0.0394)  time: 3.4828  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 720/3449]  eta: 2:38:40  lr: 0.000100  loss: 0.0380 (0.0395)  time: 3.4850  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 730/3449]  eta: 2:38:05  lr: 0.000100  loss: 0.0401 (0.0395)  time: 3.4870  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 740/3449]  eta: 2:37:30  lr: 0.000100  loss: 0.0401 (0.0395)  time: 3.4876  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 750/3449]  eta: 2:36:55  lr: 0.000100  loss: 0.0374 (0.0395)  time: 3.4866  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 760/3449]  eta: 2:36:21  lr: 0.000100  loss: 0.0383 (0.0395)  time: 3.4869  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 770/3449]  eta: 2:35:46  lr: 0.000100  loss: 0.0397 (0.0395)  time: 3.4883  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 780/3449]  eta: 2:35:11  lr: 0.000100  loss: 0.0345 (0.0394)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 790/3449]  eta: 2:34:36  lr: 0.000100  loss: 0.0341 (0.0395)  time: 3.4864  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 800/3449]  eta: 2:34:01  lr: 0.000100  loss: 0.0349 (0.0395)  time: 3.4868  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [ 810/3449]  eta: 2:33:26  lr: 0.000100  loss: 0.0352 (0.0394)  time: 3.4859  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [ 820/3449]  eta: 2:32:51  lr: 0.000100  loss: 0.0362 (0.0394)  time: 3.4848  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 830/3449]  eta: 2:32:16  lr: 0.000100  loss: 0.0403 (0.0394)  time: 3.4846  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [ 840/3449]  eta: 2:31:41  lr: 0.000100  loss: 0.0385 (0.0394)  time: 3.4847  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [ 850/3449]  eta: 2:31:06  lr: 0.000100  loss: 0.0361 (0.0394)  time: 3.4845  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 860/3449]  eta: 2:30:31  lr: 0.000100  loss: 0.0388 (0.0394)  time: 3.4840  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 870/3449]  eta: 2:29:56  lr: 0.000100  loss: 0.0359 (0.0394)  time: 3.4837  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 880/3449]  eta: 2:29:21  lr: 0.000100  loss: 0.0386 (0.0394)  time: 3.4834  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 890/3449]  eta: 2:28:46  lr: 0.000100  loss: 0.0393 (0.0394)  time: 3.4832  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 900/3449]  eta: 2:28:11  lr: 0.000100  loss: 0.0393 (0.0394)  time: 3.4850  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 910/3449]  eta: 2:27:36  lr: 0.000100  loss: 0.0385 (0.0394)  time: 3.4862  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [ 920/3449]  eta: 2:27:01  lr: 0.000100  loss: 0.0384 (0.0395)  time: 3.4855  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 930/3449]  eta: 2:26:26  lr: 0.000100  loss: 0.0380 (0.0394)  time: 3.4836  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 940/3449]  eta: 2:25:51  lr: 0.000100  loss: 0.0375 (0.0395)  time: 3.4826  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [ 950/3449]  eta: 2:25:16  lr: 0.000100  loss: 0.0388 (0.0395)  time: 3.4838  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [ 960/3449]  eta: 2:24:41  lr: 0.000100  loss: 0.0377 (0.0395)  time: 3.4833  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 970/3449]  eta: 2:24:06  lr: 0.000100  loss: 0.0368 (0.0395)  time: 3.4822  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 980/3449]  eta: 2:23:31  lr: 0.000100  loss: 0.0399 (0.0395)  time: 3.4829  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [ 990/3449]  eta: 2:22:56  lr: 0.000100  loss: 0.0369 (0.0395)  time: 3.4851  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1000/3449]  eta: 2:22:21  lr: 0.000100  loss: 0.0376 (0.0395)  time: 3.4868  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1010/3449]  eta: 2:21:46  lr: 0.000100  loss: 0.0356 (0.0395)  time: 3.4875  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1020/3449]  eta: 2:21:11  lr: 0.000100  loss: 0.0348 (0.0394)  time: 3.4877  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1030/3449]  eta: 2:20:36  lr: 0.000100  loss: 0.0362 (0.0394)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1040/3449]  eta: 2:20:01  lr: 0.000100  loss: 0.0379 (0.0394)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1050/3449]  eta: 2:19:26  lr: 0.000100  loss: 0.0383 (0.0393)  time: 3.4860  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1060/3449]  eta: 2:18:52  lr: 0.000100  loss: 0.0359 (0.0393)  time: 3.4847  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1070/3449]  eta: 2:18:17  lr: 0.000100  loss: 0.0358 (0.0393)  time: 3.4837  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1080/3449]  eta: 2:17:42  lr: 0.000100  loss: 0.0358 (0.0393)  time: 3.4829  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1090/3449]  eta: 2:17:07  lr: 0.000100  loss: 0.0370 (0.0393)  time: 3.4850  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [1100/3449]  eta: 2:16:32  lr: 0.000100  loss: 0.0369 (0.0393)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1110/3449]  eta: 2:15:57  lr: 0.000100  loss: 0.0350 (0.0393)  time: 3.4876  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1120/3449]  eta: 2:15:22  lr: 0.000100  loss: 0.0348 (0.0393)  time: 3.4875  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1130/3449]  eta: 2:14:47  lr: 0.000100  loss: 0.0357 (0.0393)  time: 3.4875  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1140/3449]  eta: 2:14:12  lr: 0.000100  loss: 0.0379 (0.0393)  time: 3.4857  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1150/3449]  eta: 2:13:37  lr: 0.000100  loss: 0.0364 (0.0393)  time: 3.4834  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1160/3449]  eta: 2:13:02  lr: 0.000100  loss: 0.0375 (0.0393)  time: 3.4833  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1170/3449]  eta: 2:12:27  lr: 0.000100  loss: 0.0357 (0.0393)  time: 3.4837  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1180/3449]  eta: 2:11:52  lr: 0.000100  loss: 0.0331 (0.0393)  time: 3.4847  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:36]  [1190/3449]  eta: 2:11:18  lr: 0.000100  loss: 0.0361 (0.0393)  time: 3.4859  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1200/3449]  eta: 2:10:43  lr: 0.000100  loss: 0.0351 (0.0392)  time: 3.4862  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1210/3449]  eta: 2:10:08  lr: 0.000100  loss: 0.0345 (0.0392)  time: 3.4863  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1220/3449]  eta: 2:09:33  lr: 0.000100  loss: 0.0360 (0.0392)  time: 3.4863  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1230/3449]  eta: 2:08:58  lr: 0.000100  loss: 0.0372 (0.0392)  time: 3.4861  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1240/3449]  eta: 2:08:23  lr: 0.000100  loss: 0.0387 (0.0392)  time: 3.4868  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1250/3449]  eta: 2:07:48  lr: 0.000100  loss: 0.0401 (0.0393)  time: 3.4878  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1260/3449]  eta: 2:07:13  lr: 0.000100  loss: 0.0391 (0.0393)  time: 3.4880  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1270/3449]  eta: 2:06:38  lr: 0.000100  loss: 0.0379 (0.0393)  time: 3.4866  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1280/3449]  eta: 2:06:04  lr: 0.000100  loss: 0.0364 (0.0392)  time: 3.4849  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1290/3449]  eta: 2:05:29  lr: 0.000100  loss: 0.0358 (0.0392)  time: 3.4848  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1300/3449]  eta: 2:04:54  lr: 0.000100  loss: 0.0356 (0.0392)  time: 3.4850  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1310/3449]  eta: 2:04:19  lr: 0.000100  loss: 0.0386 (0.0393)  time: 3.4856  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1320/3449]  eta: 2:03:44  lr: 0.000100  loss: 0.0429 (0.0393)  time: 3.4850  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1330/3449]  eta: 2:03:09  lr: 0.000100  loss: 0.0418 (0.0393)  time: 3.4846  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1340/3449]  eta: 2:02:34  lr: 0.000100  loss: 0.0388 (0.0393)  time: 3.4843  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1350/3449]  eta: 2:01:59  lr: 0.000100  loss: 0.0361 (0.0393)  time: 3.4825  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1360/3449]  eta: 2:01:24  lr: 0.000100  loss: 0.0362 (0.0393)  time: 3.4816  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1370/3449]  eta: 2:00:49  lr: 0.000100  loss: 0.0379 (0.0393)  time: 3.4829  data: 0.0004  max mem: 34968\n",
      "Train: [epoch:36]  [1380/3449]  eta: 2:00:14  lr: 0.000100  loss: 0.0380 (0.0393)  time: 3.4839  data: 0.0004  max mem: 34968\n",
      "Train: [epoch:36]  [1390/3449]  eta: 1:59:39  lr: 0.000100  loss: 0.0384 (0.0393)  time: 3.4819  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1400/3449]  eta: 1:59:04  lr: 0.000100  loss: 0.0397 (0.0393)  time: 3.4809  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1410/3449]  eta: 1:58:29  lr: 0.000100  loss: 0.0401 (0.0393)  time: 3.4818  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1420/3449]  eta: 1:57:54  lr: 0.000100  loss: 0.0401 (0.0393)  time: 3.4822  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1430/3449]  eta: 1:57:20  lr: 0.000100  loss: 0.0375 (0.0393)  time: 3.4823  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1440/3449]  eta: 1:56:45  lr: 0.000100  loss: 0.0377 (0.0394)  time: 3.4835  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1450/3449]  eta: 1:56:10  lr: 0.000100  loss: 0.0369 (0.0394)  time: 3.4841  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1460/3449]  eta: 1:55:35  lr: 0.000100  loss: 0.0357 (0.0394)  time: 3.4837  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1470/3449]  eta: 1:55:00  lr: 0.000100  loss: 0.0378 (0.0394)  time: 3.4823  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1480/3449]  eta: 1:54:25  lr: 0.000100  loss: 0.0371 (0.0393)  time: 3.4814  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1490/3449]  eta: 1:53:50  lr: 0.000100  loss: 0.0378 (0.0393)  time: 3.4812  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1500/3449]  eta: 1:53:15  lr: 0.000100  loss: 0.0396 (0.0394)  time: 3.4825  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1510/3449]  eta: 1:52:40  lr: 0.000100  loss: 0.0421 (0.0394)  time: 3.4850  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1520/3449]  eta: 1:52:05  lr: 0.000100  loss: 0.0419 (0.0394)  time: 3.4850  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1530/3449]  eta: 1:51:30  lr: 0.000100  loss: 0.0374 (0.0394)  time: 3.4842  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1540/3449]  eta: 1:50:56  lr: 0.000100  loss: 0.0374 (0.0394)  time: 3.4834  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1550/3449]  eta: 1:50:21  lr: 0.000100  loss: 0.0397 (0.0394)  time: 3.4830  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1560/3449]  eta: 1:49:46  lr: 0.000100  loss: 0.0394 (0.0394)  time: 3.4826  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1570/3449]  eta: 1:49:11  lr: 0.000100  loss: 0.0356 (0.0394)  time: 3.4830  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1580/3449]  eta: 1:48:36  lr: 0.000100  loss: 0.0364 (0.0394)  time: 3.4846  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1590/3449]  eta: 1:48:01  lr: 0.000100  loss: 0.0385 (0.0394)  time: 3.4859  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1600/3449]  eta: 1:47:26  lr: 0.000100  loss: 0.0388 (0.0394)  time: 3.4860  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1610/3449]  eta: 1:46:51  lr: 0.000100  loss: 0.0377 (0.0394)  time: 3.4852  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1620/3449]  eta: 1:46:16  lr: 0.000100  loss: 0.0393 (0.0394)  time: 3.4843  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [1630/3449]  eta: 1:45:41  lr: 0.000100  loss: 0.0395 (0.0394)  time: 3.4848  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [1640/3449]  eta: 1:45:07  lr: 0.000100  loss: 0.0369 (0.0394)  time: 3.4847  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1650/3449]  eta: 1:44:32  lr: 0.000100  loss: 0.0369 (0.0394)  time: 3.4840  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1660/3449]  eta: 1:43:57  lr: 0.000100  loss: 0.0373 (0.0394)  time: 3.4847  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1670/3449]  eta: 1:43:22  lr: 0.000100  loss: 0.0344 (0.0394)  time: 3.4851  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1680/3449]  eta: 1:42:47  lr: 0.000100  loss: 0.0396 (0.0394)  time: 3.4854  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1690/3449]  eta: 1:42:12  lr: 0.000100  loss: 0.0382 (0.0394)  time: 3.4852  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1700/3449]  eta: 1:41:37  lr: 0.000100  loss: 0.0372 (0.0394)  time: 3.4852  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1710/3449]  eta: 1:41:02  lr: 0.000100  loss: 0.0357 (0.0394)  time: 3.4847  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1720/3449]  eta: 1:40:28  lr: 0.000100  loss: 0.0352 (0.0394)  time: 3.4847  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1730/3449]  eta: 1:39:53  lr: 0.000100  loss: 0.0382 (0.0394)  time: 3.4862  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1740/3449]  eta: 1:39:18  lr: 0.000100  loss: 0.0387 (0.0394)  time: 3.4862  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1750/3449]  eta: 1:38:43  lr: 0.000100  loss: 0.0374 (0.0394)  time: 3.4838  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1760/3449]  eta: 1:38:08  lr: 0.000100  loss: 0.0366 (0.0394)  time: 3.4823  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1770/3449]  eta: 1:37:33  lr: 0.000100  loss: 0.0366 (0.0394)  time: 3.4833  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1780/3449]  eta: 1:36:58  lr: 0.000100  loss: 0.0362 (0.0393)  time: 3.4837  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1790/3449]  eta: 1:36:23  lr: 0.000100  loss: 0.0362 (0.0393)  time: 3.4830  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1800/3449]  eta: 1:35:48  lr: 0.000100  loss: 0.0379 (0.0394)  time: 3.4835  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1810/3449]  eta: 1:35:14  lr: 0.000100  loss: 0.0379 (0.0394)  time: 3.4846  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1820/3449]  eta: 1:34:39  lr: 0.000100  loss: 0.0369 (0.0394)  time: 3.4845  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [1830/3449]  eta: 1:34:04  lr: 0.000100  loss: 0.0395 (0.0394)  time: 3.4826  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [1840/3449]  eta: 1:33:29  lr: 0.000100  loss: 0.0381 (0.0394)  time: 3.4808  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:36]  [1850/3449]  eta: 1:32:54  lr: 0.000100  loss: 0.0394 (0.0394)  time: 3.4816  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1860/3449]  eta: 1:32:19  lr: 0.000100  loss: 0.0438 (0.0394)  time: 3.4821  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1870/3449]  eta: 1:31:44  lr: 0.000100  loss: 0.0400 (0.0394)  time: 3.4812  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1880/3449]  eta: 1:31:09  lr: 0.000100  loss: 0.0332 (0.0394)  time: 3.4795  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1890/3449]  eta: 1:30:34  lr: 0.000100  loss: 0.0374 (0.0394)  time: 3.4797  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1900/3449]  eta: 1:29:59  lr: 0.000100  loss: 0.0386 (0.0394)  time: 3.4825  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1910/3449]  eta: 1:29:25  lr: 0.000100  loss: 0.0395 (0.0394)  time: 3.4836  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1920/3449]  eta: 1:28:50  lr: 0.000100  loss: 0.0419 (0.0394)  time: 3.4822  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1930/3449]  eta: 1:28:15  lr: 0.000100  loss: 0.0389 (0.0395)  time: 3.4820  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1940/3449]  eta: 1:27:40  lr: 0.000100  loss: 0.0454 (0.0396)  time: 3.4836  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1950/3449]  eta: 1:27:05  lr: 0.000100  loss: 0.0592 (0.0397)  time: 3.4828  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1960/3449]  eta: 1:26:30  lr: 0.000100  loss: 0.0533 (0.0398)  time: 3.4825  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1970/3449]  eta: 1:25:55  lr: 0.000100  loss: 0.0460 (0.0398)  time: 3.4829  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1980/3449]  eta: 1:25:20  lr: 0.000100  loss: 0.0430 (0.0398)  time: 3.4823  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [1990/3449]  eta: 1:24:45  lr: 0.000100  loss: 0.0436 (0.0398)  time: 3.4828  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2000/3449]  eta: 1:24:11  lr: 0.000100  loss: 0.0421 (0.0398)  time: 3.4845  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2010/3449]  eta: 1:23:36  lr: 0.000100  loss: 0.0407 (0.0398)  time: 3.4852  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2020/3449]  eta: 1:23:01  lr: 0.000100  loss: 0.0406 (0.0398)  time: 3.4829  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2030/3449]  eta: 1:22:26  lr: 0.000100  loss: 0.0336 (0.0398)  time: 3.4820  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2040/3449]  eta: 1:21:51  lr: 0.000100  loss: 0.0352 (0.0398)  time: 3.4819  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2050/3449]  eta: 1:21:16  lr: 0.000100  loss: 0.0392 (0.0398)  time: 3.4815  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2060/3449]  eta: 1:20:41  lr: 0.000100  loss: 0.0388 (0.0398)  time: 3.4813  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2070/3449]  eta: 1:20:06  lr: 0.000100  loss: 0.0385 (0.0398)  time: 3.4806  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2080/3449]  eta: 1:19:32  lr: 0.000100  loss: 0.0381 (0.0398)  time: 3.4800  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2090/3449]  eta: 1:18:57  lr: 0.000100  loss: 0.0379 (0.0398)  time: 3.4795  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2100/3449]  eta: 1:18:22  lr: 0.000100  loss: 0.0362 (0.0398)  time: 3.4787  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2110/3449]  eta: 1:17:47  lr: 0.000100  loss: 0.0363 (0.0398)  time: 3.4791  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2120/3449]  eta: 1:17:12  lr: 0.000100  loss: 0.0363 (0.0398)  time: 3.4796  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2130/3449]  eta: 1:16:37  lr: 0.000100  loss: 0.0377 (0.0398)  time: 3.4787  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2140/3449]  eta: 1:16:02  lr: 0.000100  loss: 0.0371 (0.0398)  time: 3.4784  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2150/3449]  eta: 1:15:27  lr: 0.000100  loss: 0.0365 (0.0398)  time: 3.4790  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2160/3449]  eta: 1:14:52  lr: 0.000100  loss: 0.0352 (0.0397)  time: 3.4788  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2170/3449]  eta: 1:14:17  lr: 0.000100  loss: 0.0345 (0.0397)  time: 3.4791  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2180/3449]  eta: 1:13:43  lr: 0.000100  loss: 0.0365 (0.0397)  time: 3.4791  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2190/3449]  eta: 1:13:08  lr: 0.000100  loss: 0.0397 (0.0398)  time: 3.4776  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [2200/3449]  eta: 1:12:33  lr: 0.000100  loss: 0.0389 (0.0398)  time: 3.4770  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [2210/3449]  eta: 1:11:58  lr: 0.000100  loss: 0.0365 (0.0398)  time: 3.4773  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2220/3449]  eta: 1:11:23  lr: 0.000100  loss: 0.0362 (0.0398)  time: 3.4778  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2230/3449]  eta: 1:10:48  lr: 0.000100  loss: 0.0379 (0.0398)  time: 3.4780  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2240/3449]  eta: 1:10:13  lr: 0.000100  loss: 0.0382 (0.0398)  time: 3.4795  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2250/3449]  eta: 1:09:38  lr: 0.000100  loss: 0.0389 (0.0398)  time: 3.4790  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2260/3449]  eta: 1:09:03  lr: 0.000100  loss: 0.0392 (0.0398)  time: 3.4770  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [2270/3449]  eta: 1:08:28  lr: 0.000100  loss: 0.0353 (0.0398)  time: 3.4765  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [2280/3449]  eta: 1:07:54  lr: 0.000100  loss: 0.0366 (0.0398)  time: 3.4763  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2290/3449]  eta: 1:07:19  lr: 0.000100  loss: 0.0380 (0.0397)  time: 3.4776  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2300/3449]  eta: 1:06:44  lr: 0.000100  loss: 0.0378 (0.0398)  time: 3.4805  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2310/3449]  eta: 1:06:09  lr: 0.000100  loss: 0.0359 (0.0398)  time: 3.4803  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2320/3449]  eta: 1:05:34  lr: 0.000100  loss: 0.0343 (0.0397)  time: 3.4782  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2330/3449]  eta: 1:04:59  lr: 0.000100  loss: 0.0361 (0.0397)  time: 3.4773  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2340/3449]  eta: 1:04:24  lr: 0.000100  loss: 0.0381 (0.0397)  time: 3.4770  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2350/3449]  eta: 1:03:49  lr: 0.000100  loss: 0.0383 (0.0397)  time: 3.4769  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2360/3449]  eta: 1:03:15  lr: 0.000100  loss: 0.0368 (0.0397)  time: 3.4765  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2370/3449]  eta: 1:02:40  lr: 0.000100  loss: 0.0361 (0.0397)  time: 3.4765  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2380/3449]  eta: 1:02:05  lr: 0.000100  loss: 0.0361 (0.0397)  time: 3.4762  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2390/3449]  eta: 1:01:30  lr: 0.000100  loss: 0.0335 (0.0397)  time: 3.4764  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2400/3449]  eta: 1:00:55  lr: 0.000100  loss: 0.0353 (0.0397)  time: 3.4778  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2410/3449]  eta: 1:00:20  lr: 0.000100  loss: 0.0359 (0.0397)  time: 3.4778  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2420/3449]  eta: 0:59:45  lr: 0.000100  loss: 0.0365 (0.0397)  time: 3.4792  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2430/3449]  eta: 0:59:10  lr: 0.000100  loss: 0.0361 (0.0396)  time: 3.4806  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2440/3449]  eta: 0:58:36  lr: 0.000100  loss: 0.0349 (0.0396)  time: 3.4790  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2450/3449]  eta: 0:58:01  lr: 0.000100  loss: 0.0368 (0.0396)  time: 3.4793  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2460/3449]  eta: 0:57:26  lr: 0.000100  loss: 0.0377 (0.0396)  time: 3.4793  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [2470/3449]  eta: 0:56:51  lr: 0.000100  loss: 0.0386 (0.0396)  time: 3.4785  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [2480/3449]  eta: 0:56:16  lr: 0.000100  loss: 0.0388 (0.0396)  time: 3.4783  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2490/3449]  eta: 0:55:41  lr: 0.000100  loss: 0.0371 (0.0396)  time: 3.4786  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2500/3449]  eta: 0:55:06  lr: 0.000100  loss: 0.0364 (0.0396)  time: 3.4785  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:36]  [2510/3449]  eta: 0:54:31  lr: 0.000100  loss: 0.0376 (0.0396)  time: 3.4794  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2520/3449]  eta: 0:53:57  lr: 0.000100  loss: 0.0370 (0.0396)  time: 3.4813  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2530/3449]  eta: 0:53:22  lr: 0.000100  loss: 0.0367 (0.0396)  time: 3.4814  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2540/3449]  eta: 0:52:47  lr: 0.000100  loss: 0.0360 (0.0396)  time: 3.4799  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2550/3449]  eta: 0:52:12  lr: 0.000100  loss: 0.0360 (0.0396)  time: 3.4789  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2560/3449]  eta: 0:51:37  lr: 0.000100  loss: 0.0357 (0.0396)  time: 3.4791  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2570/3449]  eta: 0:51:02  lr: 0.000100  loss: 0.0368 (0.0396)  time: 3.4800  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2580/3449]  eta: 0:50:27  lr: 0.000100  loss: 0.0376 (0.0396)  time: 3.4818  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2590/3449]  eta: 0:49:53  lr: 0.000100  loss: 0.0376 (0.0396)  time: 3.4828  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2600/3449]  eta: 0:49:18  lr: 0.000100  loss: 0.0358 (0.0396)  time: 3.4833  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2610/3449]  eta: 0:48:43  lr: 0.000100  loss: 0.0372 (0.0396)  time: 3.4849  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2620/3449]  eta: 0:48:08  lr: 0.000100  loss: 0.0379 (0.0396)  time: 3.4855  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2630/3449]  eta: 0:47:33  lr: 0.000100  loss: 0.0379 (0.0396)  time: 3.4843  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2640/3449]  eta: 0:46:58  lr: 0.000100  loss: 0.0348 (0.0396)  time: 3.4836  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2650/3449]  eta: 0:46:24  lr: 0.000100  loss: 0.0349 (0.0396)  time: 3.4836  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2660/3449]  eta: 0:45:49  lr: 0.000100  loss: 0.0381 (0.0396)  time: 3.4839  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2670/3449]  eta: 0:45:14  lr: 0.000100  loss: 0.0381 (0.0396)  time: 3.4836  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2680/3449]  eta: 0:44:39  lr: 0.000100  loss: 0.0381 (0.0396)  time: 3.4822  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2690/3449]  eta: 0:44:04  lr: 0.000100  loss: 0.0379 (0.0396)  time: 3.4815  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2700/3449]  eta: 0:43:29  lr: 0.000100  loss: 0.0399 (0.0397)  time: 3.4812  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2710/3449]  eta: 0:42:54  lr: 0.000100  loss: 0.0399 (0.0397)  time: 3.4813  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2720/3449]  eta: 0:42:20  lr: 0.000100  loss: 0.0350 (0.0396)  time: 3.4807  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2730/3449]  eta: 0:41:45  lr: 0.000100  loss: 0.0350 (0.0396)  time: 3.4793  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2740/3449]  eta: 0:41:10  lr: 0.000100  loss: 0.0364 (0.0397)  time: 3.4802  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2750/3449]  eta: 0:40:35  lr: 0.000100  loss: 0.0390 (0.0397)  time: 3.4817  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2760/3449]  eta: 0:40:00  lr: 0.000100  loss: 0.0387 (0.0396)  time: 3.4829  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2770/3449]  eta: 0:39:25  lr: 0.000100  loss: 0.0370 (0.0396)  time: 3.4839  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2780/3449]  eta: 0:38:50  lr: 0.000100  loss: 0.0362 (0.0396)  time: 3.4850  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2790/3449]  eta: 0:38:16  lr: 0.000100  loss: 0.0350 (0.0396)  time: 3.4864  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2800/3449]  eta: 0:37:41  lr: 0.000100  loss: 0.0385 (0.0396)  time: 3.4872  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2810/3449]  eta: 0:37:06  lr: 0.000100  loss: 0.0388 (0.0397)  time: 3.4852  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2820/3449]  eta: 0:36:31  lr: 0.000100  loss: 0.0388 (0.0396)  time: 3.4831  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [2830/3449]  eta: 0:35:56  lr: 0.000100  loss: 0.0348 (0.0396)  time: 3.4840  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [2840/3449]  eta: 0:35:21  lr: 0.000100  loss: 0.0341 (0.0396)  time: 3.4851  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2850/3449]  eta: 0:34:47  lr: 0.000100  loss: 0.0365 (0.0396)  time: 3.4844  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2860/3449]  eta: 0:34:12  lr: 0.000100  loss: 0.0382 (0.0396)  time: 3.4838  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2870/3449]  eta: 0:33:37  lr: 0.000100  loss: 0.0370 (0.0396)  time: 3.4838  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2880/3449]  eta: 0:33:02  lr: 0.000100  loss: 0.0358 (0.0396)  time: 3.4841  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2890/3449]  eta: 0:32:27  lr: 0.000100  loss: 0.0351 (0.0396)  time: 3.4841  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2900/3449]  eta: 0:31:52  lr: 0.000100  loss: 0.0351 (0.0396)  time: 3.4835  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2910/3449]  eta: 0:31:18  lr: 0.000100  loss: 0.0363 (0.0396)  time: 3.4839  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2920/3449]  eta: 0:30:43  lr: 0.000100  loss: 0.0357 (0.0396)  time: 3.4845  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2930/3449]  eta: 0:30:08  lr: 0.000100  loss: 0.0365 (0.0396)  time: 3.4851  data: 0.0004  max mem: 34968\n",
      "Train: [epoch:36]  [2940/3449]  eta: 0:29:33  lr: 0.000100  loss: 0.0380 (0.0396)  time: 3.4855  data: 0.0004  max mem: 34968\n",
      "Train: [epoch:36]  [2950/3449]  eta: 0:28:58  lr: 0.000100  loss: 0.0383 (0.0396)  time: 3.4844  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2960/3449]  eta: 0:28:23  lr: 0.000100  loss: 0.0372 (0.0396)  time: 3.4832  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [2970/3449]  eta: 0:27:48  lr: 0.000100  loss: 0.0340 (0.0396)  time: 3.4831  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [2980/3449]  eta: 0:27:14  lr: 0.000100  loss: 0.0355 (0.0396)  time: 3.4825  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [2990/3449]  eta: 0:26:39  lr: 0.000100  loss: 0.0354 (0.0395)  time: 3.4831  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3000/3449]  eta: 0:26:04  lr: 0.000100  loss: 0.0352 (0.0396)  time: 3.4846  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3010/3449]  eta: 0:25:29  lr: 0.000100  loss: 0.0382 (0.0396)  time: 3.4851  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3020/3449]  eta: 0:24:54  lr: 0.000100  loss: 0.0373 (0.0396)  time: 3.4834  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3030/3449]  eta: 0:24:19  lr: 0.000100  loss: 0.0378 (0.0396)  time: 3.4820  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3040/3449]  eta: 0:23:45  lr: 0.000100  loss: 0.0400 (0.0396)  time: 3.4836  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3050/3449]  eta: 0:23:10  lr: 0.000100  loss: 0.0381 (0.0396)  time: 3.4842  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3060/3449]  eta: 0:22:35  lr: 0.000100  loss: 0.0386 (0.0396)  time: 3.4838  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3070/3449]  eta: 0:22:00  lr: 0.000100  loss: 0.0393 (0.0396)  time: 3.4836  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3080/3449]  eta: 0:21:25  lr: 0.000100  loss: 0.0375 (0.0396)  time: 3.4842  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3090/3449]  eta: 0:20:50  lr: 0.000100  loss: 0.0361 (0.0396)  time: 3.4861  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [3100/3449]  eta: 0:20:16  lr: 0.000100  loss: 0.0363 (0.0396)  time: 3.4866  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [3110/3449]  eta: 0:19:41  lr: 0.000100  loss: 0.0363 (0.0396)  time: 3.4856  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3120/3449]  eta: 0:19:06  lr: 0.000100  loss: 0.0377 (0.0396)  time: 3.4837  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3130/3449]  eta: 0:18:31  lr: 0.000100  loss: 0.0371 (0.0396)  time: 3.4824  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3140/3449]  eta: 0:17:56  lr: 0.000100  loss: 0.0354 (0.0396)  time: 3.4837  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3150/3449]  eta: 0:17:21  lr: 0.000100  loss: 0.0362 (0.0395)  time: 3.4864  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:36]  [3160/3449]  eta: 0:16:46  lr: 0.000100  loss: 0.0359 (0.0395)  time: 3.4877  data: 0.0003  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:36]  [3170/3449]  eta: 0:16:12  lr: 0.000100  loss: 0.0371 (0.0395)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3180/3449]  eta: 0:15:37  lr: 0.000100  loss: 0.0365 (0.0395)  time: 3.4913  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3190/3449]  eta: 0:15:02  lr: 0.000100  loss: 0.0341 (0.0395)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3200/3449]  eta: 0:14:27  lr: 0.000100  loss: 0.0360 (0.0395)  time: 3.4891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3210/3449]  eta: 0:13:52  lr: 0.000100  loss: 0.0360 (0.0395)  time: 3.4897  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3220/3449]  eta: 0:13:17  lr: 0.000100  loss: 0.0376 (0.0395)  time: 3.4900  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3230/3449]  eta: 0:12:43  lr: 0.000100  loss: 0.0382 (0.0395)  time: 3.4894  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3240/3449]  eta: 0:12:08  lr: 0.000100  loss: 0.0384 (0.0395)  time: 3.4893  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3250/3449]  eta: 0:11:33  lr: 0.000100  loss: 0.0361 (0.0395)  time: 3.4884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3260/3449]  eta: 0:10:58  lr: 0.000100  loss: 0.0392 (0.0395)  time: 3.4869  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3270/3449]  eta: 0:10:23  lr: 0.000100  loss: 0.0356 (0.0395)  time: 3.4876  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3280/3449]  eta: 0:09:48  lr: 0.000100  loss: 0.0344 (0.0394)  time: 3.4885  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3290/3449]  eta: 0:09:14  lr: 0.000100  loss: 0.0356 (0.0394)  time: 3.4891  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3300/3449]  eta: 0:08:39  lr: 0.000100  loss: 0.0356 (0.0394)  time: 3.4889  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3310/3449]  eta: 0:08:04  lr: 0.000100  loss: 0.0351 (0.0394)  time: 3.4871  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3320/3449]  eta: 0:07:29  lr: 0.000100  loss: 0.0377 (0.0394)  time: 3.4858  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3330/3449]  eta: 0:06:54  lr: 0.000100  loss: 0.0377 (0.0394)  time: 3.4858  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3340/3449]  eta: 0:06:19  lr: 0.000100  loss: 0.0331 (0.0394)  time: 3.4848  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3350/3449]  eta: 0:05:44  lr: 0.000100  loss: 0.0346 (0.0394)  time: 3.4855  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3360/3449]  eta: 0:05:10  lr: 0.000100  loss: 0.0356 (0.0394)  time: 3.4866  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3370/3449]  eta: 0:04:35  lr: 0.000100  loss: 0.0332 (0.0394)  time: 3.4867  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3380/3449]  eta: 0:04:00  lr: 0.000100  loss: 0.0332 (0.0394)  time: 3.4879  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3390/3449]  eta: 0:03:25  lr: 0.000100  loss: 0.0378 (0.0394)  time: 3.4884  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3400/3449]  eta: 0:02:50  lr: 0.000100  loss: 0.0416 (0.0394)  time: 3.4887  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3410/3449]  eta: 0:02:15  lr: 0.000100  loss: 0.0416 (0.0394)  time: 3.4888  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3420/3449]  eta: 0:01:41  lr: 0.000100  loss: 0.0392 (0.0394)  time: 3.4898  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3430/3449]  eta: 0:01:06  lr: 0.000100  loss: 0.0392 (0.0394)  time: 3.4907  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3440/3449]  eta: 0:00:31  lr: 0.000100  loss: 0.0386 (0.0394)  time: 3.4902  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36]  [3448/3449]  eta: 0:00:03  lr: 0.000100  loss: 0.0369 (0.0394)  time: 3.4906  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:36] Total time: 3:20:19 (3.4848 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0369 (0.0394)\n",
      "Valid: [epoch:36]  [ 0/14]  eta: 0:04:26  loss: 0.0374 (0.0374)  time: 19.0019  data: 0.7662  max mem: 34968\n",
      "Valid: [epoch:36]  [13/14]  eta: 0:00:18  loss: 0.0338 (0.0342)  time: 18.3102  data: 0.0550  max mem: 34968\n",
      "Valid: [epoch:36] Total time: 0:04:16 (18.3242 s / it)\n",
      "Averaged stats: loss: 0.0338 (0.0342)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/epoch_36_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.034%\n",
      "Min loss: 0.018\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:37]  [   0/3449]  eta: 5:14:33  lr: 0.000100  loss: 0.0430 (0.0430)  time: 5.4723  data: 2.0365  max mem: 34968\n",
      "Train: [epoch:37]  [  10/3449]  eta: 3:29:33  lr: 0.000100  loss: 0.0422 (0.0403)  time: 3.6563  data: 0.1853  max mem: 34968\n",
      "Train: [epoch:37]  [  20/3449]  eta: 3:24:33  lr: 0.000100  loss: 0.0405 (0.0393)  time: 3.4848  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [  30/3449]  eta: 3:22:30  lr: 0.000100  loss: 0.0375 (0.0405)  time: 3.4973  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [  40/3449]  eta: 3:21:10  lr: 0.000100  loss: 0.0377 (0.0395)  time: 3.5001  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [  50/3449]  eta: 3:20:08  lr: 0.000100  loss: 0.0378 (0.0394)  time: 3.5005  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [  60/3449]  eta: 3:19:13  lr: 0.000100  loss: 0.0386 (0.0394)  time: 3.4995  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [  70/3449]  eta: 3:18:24  lr: 0.000100  loss: 0.0397 (0.0401)  time: 3.4984  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [  80/3449]  eta: 3:17:39  lr: 0.000100  loss: 0.0396 (0.0402)  time: 3.4982  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [  90/3449]  eta: 3:16:55  lr: 0.000100  loss: 0.0385 (0.0401)  time: 3.4983  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 100/3449]  eta: 3:16:14  lr: 0.000100  loss: 0.0392 (0.0402)  time: 3.4984  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 110/3449]  eta: 3:15:34  lr: 0.000100  loss: 0.0379 (0.0402)  time: 3.4995  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 120/3449]  eta: 3:14:55  lr: 0.000100  loss: 0.0365 (0.0402)  time: 3.5009  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 130/3449]  eta: 3:14:17  lr: 0.000100  loss: 0.0401 (0.0405)  time: 3.5004  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 140/3449]  eta: 3:13:39  lr: 0.000100  loss: 0.0401 (0.0408)  time: 3.5007  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 150/3449]  eta: 3:13:02  lr: 0.000100  loss: 0.0390 (0.0407)  time: 3.5010  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 160/3449]  eta: 3:12:24  lr: 0.000100  loss: 0.0412 (0.0409)  time: 3.5004  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 170/3449]  eta: 3:11:48  lr: 0.000100  loss: 0.0428 (0.0412)  time: 3.5007  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 180/3449]  eta: 3:11:11  lr: 0.000100  loss: 0.0371 (0.0409)  time: 3.5013  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 190/3449]  eta: 3:10:35  lr: 0.000100  loss: 0.0371 (0.0409)  time: 3.5022  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 200/3449]  eta: 3:09:59  lr: 0.000100  loss: 0.0421 (0.0412)  time: 3.5033  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 210/3449]  eta: 3:09:23  lr: 0.000100  loss: 0.0379 (0.0409)  time: 3.5025  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 220/3449]  eta: 3:08:46  lr: 0.000100  loss: 0.0375 (0.0411)  time: 3.5008  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 230/3449]  eta: 3:08:11  lr: 0.000100  loss: 0.0391 (0.0410)  time: 3.5013  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 240/3449]  eta: 3:07:35  lr: 0.000100  loss: 0.0391 (0.0409)  time: 3.5023  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 250/3449]  eta: 3:06:59  lr: 0.000100  loss: 0.0406 (0.0410)  time: 3.5022  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 260/3449]  eta: 3:06:23  lr: 0.000100  loss: 0.0403 (0.0410)  time: 3.5015  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 270/3449]  eta: 3:05:47  lr: 0.000100  loss: 0.0372 (0.0409)  time: 3.5011  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 280/3449]  eta: 3:05:12  lr: 0.000100  loss: 0.0358 (0.0407)  time: 3.5012  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 290/3449]  eta: 3:04:36  lr: 0.000100  loss: 0.0387 (0.0408)  time: 3.5012  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:37]  [ 300/3449]  eta: 3:04:01  lr: 0.000100  loss: 0.0397 (0.0409)  time: 3.5012  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 310/3449]  eta: 3:03:25  lr: 0.000100  loss: 0.0381 (0.0407)  time: 3.5014  data: 0.0002  max mem: 34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:37]  [ 320/3449]  eta: 3:02:49  lr: 0.000100  loss: 0.0380 (0.0408)  time: 3.5007  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 330/3449]  eta: 3:02:14  lr: 0.000100  loss: 0.0382 (0.0408)  time: 3.4995  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 340/3449]  eta: 3:01:38  lr: 0.000100  loss: 0.0368 (0.0406)  time: 3.4988  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 350/3449]  eta: 3:01:02  lr: 0.000100  loss: 0.0352 (0.0405)  time: 3.4986  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 360/3449]  eta: 3:00:27  lr: 0.000100  loss: 0.0371 (0.0406)  time: 3.4983  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 370/3449]  eta: 2:59:51  lr: 0.000100  loss: 0.0397 (0.0406)  time: 3.4986  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:37]  [ 380/3449]  eta: 2:59:15  lr: 0.000100  loss: 0.0380 (0.0406)  time: 3.4984  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:37]  [ 390/3449]  eta: 2:58:40  lr: 0.000100  loss: 0.0385 (0.0406)  time: 3.4976  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 400/3449]  eta: 2:58:04  lr: 0.000100  loss: 0.0393 (0.0406)  time: 3.4978  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 410/3449]  eta: 2:57:29  lr: 0.000100  loss: 0.0387 (0.0405)  time: 3.4980  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 420/3449]  eta: 2:56:53  lr: 0.000100  loss: 0.0388 (0.0406)  time: 3.4985  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 430/3449]  eta: 2:56:18  lr: 0.000100  loss: 0.0391 (0.0405)  time: 3.4996  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:37]  [ 440/3449]  eta: 2:55:43  lr: 0.000100  loss: 0.0391 (0.0405)  time: 3.5009  data: 0.0003  max mem: 34968\n",
      "Train: [epoch:37]  [ 450/3449]  eta: 2:55:08  lr: 0.000100  loss: 0.0379 (0.0405)  time: 3.5011  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 460/3449]  eta: 2:54:32  lr: 0.000100  loss: 0.0379 (0.0406)  time: 3.5003  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 470/3449]  eta: 2:53:57  lr: 0.000100  loss: 0.0386 (0.0407)  time: 3.4985  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 480/3449]  eta: 2:53:21  lr: 0.000100  loss: 0.0390 (0.0407)  time: 3.4981  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 490/3449]  eta: 2:52:46  lr: 0.000100  loss: 0.0393 (0.0407)  time: 3.4982  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:37]  [ 500/3449]  eta: 2:52:11  lr: 0.000100  loss: 0.0388 (0.0407)  time: 3.4962  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:37]  [ 510/3449]  eta: 2:51:35  lr: 0.000100  loss: 0.0388 (0.0407)  time: 3.4961  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 520/3449]  eta: 2:51:00  lr: 0.000100  loss: 0.0378 (0.0408)  time: 3.4968  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 530/3449]  eta: 2:50:24  lr: 0.000100  loss: 0.0371 (0.0407)  time: 3.4963  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 540/3449]  eta: 2:49:49  lr: 0.000100  loss: 0.0371 (0.0407)  time: 3.4964  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:37]  [ 550/3449]  eta: 2:49:14  lr: 0.000100  loss: 0.0373 (0.0406)  time: 3.4976  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 560/3449]  eta: 2:48:38  lr: 0.000100  loss: 0.0372 (0.0406)  time: 3.4984  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 570/3449]  eta: 2:48:03  lr: 0.000100  loss: 0.0372 (0.0406)  time: 3.4981  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 580/3449]  eta: 2:47:28  lr: 0.000100  loss: 0.0371 (0.0405)  time: 3.4979  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 590/3449]  eta: 2:46:53  lr: 0.000100  loss: 0.0376 (0.0405)  time: 3.4979  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 600/3449]  eta: 2:46:17  lr: 0.000100  loss: 0.0373 (0.0404)  time: 3.4978  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:37]  [ 610/3449]  eta: 2:45:42  lr: 0.000100  loss: 0.0373 (0.0404)  time: 3.4971  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:37]  [ 620/3449]  eta: 2:45:07  lr: 0.000100  loss: 0.0399 (0.0404)  time: 3.4963  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:37]  [ 630/3449]  eta: 2:44:32  lr: 0.000100  loss: 0.0401 (0.0404)  time: 3.4958  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:37]  [ 640/3449]  eta: 2:43:56  lr: 0.000100  loss: 0.0418 (0.0405)  time: 3.4955  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 650/3449]  eta: 2:43:21  lr: 0.000100  loss: 0.0444 (0.0406)  time: 3.4971  data: 0.0002  max mem: 34968\n",
      "Train: [epoch:37]  [ 660/3449]  eta: 2:42:46  lr: 0.000100  loss: 0.0431 (0.0406)  time: 3.4986  data: 0.0001  max mem: 34968\n",
      "Train: [epoch:37]  [ 670/3449]  eta: 2:42:11  lr: 0.000100  loss: 0.0404 (0.0406)  time: 3.4979  data: 0.0001  max mem: 34968\n",
      "^C\n",
      "Exception in thread Thread-69:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sunggu/.local/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 113, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/sunggu/.local/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 289, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--batch-size 2 \\\n",
    "--epochs 1000 \\\n",
    "--lr_scheduler \"lambda\" \\\n",
    "--lr 1e-4 \\\n",
    "--data-set 'Sinogram_DCM' \\\n",
    "--model-name 'MLPMixer' \\\n",
    "--criterion 'Change L2 L1 Loss' \\\n",
    "--output_dir '/workspace/sunggu/4.Dose_img2img/model/[Ours]MLPMixer_L1_P1' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]MLPMixer_L1_P1/low2high/' \\\n",
    "--validate-every 2 \\\n",
    "--num_workers 4 \\\n",
    "--criterion_mode 'not balance' \\\n",
    "--multiple_GT \"False\" \\\n",
    "--patch_training \"True\" \\\n",
    "--multi-gpu-mode 'Single' \\\n",
    "--resume \"/workspace/sunggu/4.Dose_img2img/model/[Ours]MLPMixer_L1_P1/epoch_2_checkpoint.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "import functools\n",
    "import pydicom\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore') \n",
    "\n",
    "\n",
    "def list_sort_nicely(l):   \n",
    "    def tryint(s):        \n",
    "        try:            \n",
    "            return int(s)        \n",
    "        except:            \n",
    "            return s\n",
    "        \n",
    "    def alphanum_key(s):\n",
    "        return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "    l.sort(key=alphanum_key)    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_20_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/20/*/*/*.dcm')) + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/20/*/*/*.dcm'))\n",
    "n_100_imgs  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/X/*/*/*.dcm'))  + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/X/*/*/*.dcm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixels_hu(path):\n",
    "    # pydicom version...!\n",
    "    # referred from https://www.kaggle.com/gzuidhof/full-preprocessing-tutorial\n",
    "    # ref: pydicom.pixel_data_handlers.util.apply_modality_lut\n",
    "    # '''\n",
    "    # Awesome pydicom lut fuction...!\n",
    "    # ds  = pydicom.dcmread(fname)\n",
    "    # arr = ds.pixel_array\n",
    "    # hu  = apply_modality_lut(arr, ds)\n",
    "    # '''\n",
    "    dcm_image = pydicom.read_file(path)\n",
    "    image = dcm_image.pixel_array\n",
    "    image = image.astype(np.int16)\n",
    "    image[image == -2000] = 0\n",
    "\n",
    "    intercept = dcm_image.RescaleIntercept\n",
    "    slope     = dcm_image.RescaleSlope\n",
    "\n",
    "    if slope != 1:\n",
    "        image = slope * image.astype(np.float64)\n",
    "        image = image.astype(np.int16)\n",
    "\n",
    "    image += np.int16(intercept)\n",
    "    # print(image.shape) # (512, 512)\n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "def dicom_normalize(image, MIN_HU=-1024.0, MAX_HU=3071.0):   # I already check the max value is 3071.0\n",
    "   image = (image - MIN_HU) / (MAX_HU - MIN_HU)   # Range  0.0 ~ 1.0\n",
    "#    image = (image - 0.5) / 0.5                  # Range -1.0 ~ 1.0   @ We do not use -1~1 range becuase there is no Tanh act.\n",
    "   return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from monai.transforms import *\n",
    "from monai.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_20_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/20/*/*/*.dcm')) + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/20/*/*/*.dcm'))\n",
    "n_100_imgs  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/X/*/*/*.dcm'))  + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/X/*/*/*.dcm'))\n",
    "\n",
    "files = [{\"n_20\": n_20, \"n_100\": n_100} for n_20, n_100 in zip(n_20_imgs, n_100_imgs)]            \n",
    "print(\"Train [Total]  number = \", len(n_20_imgs))\n",
    "\n",
    "# CT에 맞는 Augmentation\n",
    "\n",
    "transforms = Compose(\n",
    "    [\n",
    "        Lambdad(keys=[\"n_20\", \"n_100\"], func=get_pixels_hu),\n",
    "        Lambdad(keys=[\"n_20\", \"n_100\"], func=dicom_normalize),\n",
    "        AddChanneld(keys=[\"n_20\", \"n_100\"]),                 \n",
    "\n",
    "        # Crop  \n",
    "        # RandWeightedCropd(keys=[\"image\"], w_key=[\"image\"], spatial_size=(512,512,1), num_samples=1),\n",
    "        # RandSpatialCropd(keys=[\"image\"], roi_size=(512, 512), random_size=False, random_center=True),\n",
    "        # RandSpatialCropd(keys=[\"image\"], roi_size=(512,512,3), random_size=False, random_center=True),\n",
    "#         RandSpatialCropSamplesd(keys=[\"n_20\", \"n_100\"], roi_size=(64, 64), num_samples=8, random_center=True, random_size=False, meta_keys=None, allow_missing_keys=False), \n",
    "            # patch training, next(iter(loader)) output : list로 sample 만큼,,, 그 List 안에 (B, C, H, W)\n",
    "\n",
    "        # (45 degree rotation, vertical & horizontal flip & scaling)\n",
    "#         RandFlipd(keys=[\"n_20\", \"n_100\"], prob=0.1, spatial_axis=[0, 1], allow_missing_keys=False),\n",
    "#         RandRotated(keys=[\"n_20\", \"n_100\"], prob=0.1, range_x=np.pi/4, range_y=np.pi/4, range_z=0.0, keep_size=True, align_corners=False, allow_missing_keys=False),\n",
    "#         RandZoomd(keys=[\"n_20\", \"n_100\"], prob=0.1, min_zoom=0.5, max_zoom=2.0, align_corners=None, keep_size=True, allow_missing_keys=False),\n",
    "        ToTensord(keys=[\"n_20\", \"n_100\"]),\n",
    "    ]\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Dataset(data=files, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_denormalize(image, MIN_HU=-1024.0, MAX_HU=3071.0):\n",
    "    # image = (image - 0.5) / 0.5           # Range -1.0 ~ 1.0   @ We do not use -1~1 range becuase there is no Tanh act.\n",
    "    image = (MAX_HU - MIN_HU)*image + MIN_HU\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(dicom_denormalize(t[470]['n_20'].squeeze()), 'gray', vmin=0, vmax=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dicom_denormalize(t[470]['n_100'].squeeze()), 'gray', vmin=0, vmax=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(t[470]['n_100'], t[470]['n_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCoshLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "#         return torch.mean(torch.log(torch.cosh(torch.pow(ey_t, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LogCoshLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a(t[470]['n_100'], t[470]['n_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.1081e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a(t[470]['n_100'], t[470]['n_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a(t[470]['n_100'], t[470]['n_100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1000*4.1081e-06 - 1000*2.1081e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for i in range(len(lines)):\n",
    "        exec('log_list.append('+lines[i] + ')')\n",
    "    return  log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_list = read_log(path = '/workspace/sunggu/4.Dose_img2img/model/[Privious]ED_CNN/log.txt')\n",
    "\n",
    "train_lr   = [ log_list[i]['train_lr'] for i in range(len(log_list)) ]\n",
    "train_loss = [ log_list[i]['train_loss'] for i in range(len(log_list)) ]\n",
    "valid_loss = [ log_list[i]['valid_loss'] for i in range(len(log_list)) ]\n",
    "epoch      = [ log_list[i]['epoch'] for i in range(len(log_list)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(train_loss)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(valid_loss)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(np.argsort(valid_loss)[:10]) & set(np.argsort(train_loss)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py \\\n",
    "--training-mode 'sinogram' \\\n",
    "--data-set 'TEST_Sinogram_DCM' \\\n",
    "--model-name 'ED_CNN' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Test/png/[Privious]ED_CNN/epoch_999/' \\\n",
    "--num_workers 4 \\\n",
    "--pin-mem \\\n",
    "--range-minus1-plus1 'False' \\\n",
    "--teacher_forcing \"False\" \\\n",
    "--resume '/workspace/sunggu/4.Dose_img2img/model/[Privious]ED_CNN/epoch_999_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 978 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Original === \n",
    "PSNR avg: 54.4628 \n",
    "SSIM avg: 0.9956 \n",
    "RMSE avg: 7.9607\n",
    "\n",
    "\n",
    "Predictions === \n",
    "PSNR avg: 57.6190 \n",
    "SSIM avg: 0.9980 \n",
    "RMSE avg: 5.5423\n",
    "***********************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
