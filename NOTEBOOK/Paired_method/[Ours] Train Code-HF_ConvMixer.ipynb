{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pretrainedmodels==0.7.4\n",
    "# !pip install efficientnet-pytorch==0.6.3\n",
    "# !pip install timm==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CUDA 11.1\n",
    "# !pip install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE -> MAE Loss 꿀팁!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 14 08:13:01 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:18:00.0 Off |                  Off |\r\n",
      "| 30%   27C    P8    17W / 300W |   2204MiB / 48685MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:3B:00.0 Off |                  Off |\r\n",
      "| 30%   28C    P8    18W / 300W |      3MiB / 48685MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:86:00.0 Off |                  Off |\r\n",
      "| 30%   30C    P8    24W / 300W |      3MiB / 48685MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:AF:00.0 Off |                  Off |\r\n",
      "| 31%   49C    P8    21W / 300W |      3MiB / 48685MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sunggu/4.Dose_img2img/scripts study\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/sunggu/4.Dose_img2img/scripts study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 갯수 =  64\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(\"CPU 갯수 = \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc as container_abcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HF_ConvMixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "***********************************************\n",
      "Dataset Name:  Sinogram_DCM\n",
      "---------- Model ----------\n",
      "Resume From:  \n",
      "Output To:  /workspace/sunggu/4.Dose_img2img/model/[Ours]HF_ConvMixer\n",
      "Save   To:  /workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0001\n",
      "Batchsize:  10\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  6899\n",
      "Valid [Total]  number =  14\n",
      "Creating criterion: Change L2 L1 Loss\n",
      "Creating model: HF_ConvMixer\n",
      "Number of Learnable Params: 10352271\n",
      "HF_ConvMixer(\n",
      "  (hf_conv): HF_Conv()\n",
      "  (patch_embed): ConvMixer_Block(\n",
      "    (conv): Conv2d(21, 768, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (gelu): GELU()\n",
      "    (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (mixer_block1): Sequential(\n",
      "    (0): Residual(\n",
      "      (fn): ConvMixer_Block(\n",
      "        (conv): Conv2d(768, 768, kernel_size=(9, 9), stride=(1, 1), padding=same, groups=768)\n",
      "        (gelu): GELU()\n",
      "        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvMixer_Block(\n",
      "      (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (gelu): GELU()\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (mixer_block2): Sequential(\n",
      "    (0): Residual(\n",
      "      (fn): ConvMixer_Block(\n",
      "        (conv): Conv2d(1536, 1536, kernel_size=(9, 9), stride=(1, 1), padding=same, groups=1536)\n",
      "        (gelu): GELU()\n",
      "        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvMixer_Block(\n",
      "      (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (gelu): GELU()\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (mixer_block3): Sequential(\n",
      "    (0): Residual(\n",
      "      (fn): ConvMixer_Block(\n",
      "        (conv): Conv2d(1536, 1536, kernel_size=(9, 9), stride=(1, 1), padding=same, groups=1536)\n",
      "        (gelu): GELU()\n",
      "        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvMixer_Block(\n",
      "      (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (gelu): GELU()\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (mixer_block4): Sequential(\n",
      "    (0): Residual(\n",
      "      (fn): ConvMixer_Block(\n",
      "        (conv): Conv2d(1536, 1536, kernel_size=(9, 9), stride=(1, 1), padding=same, groups=1536)\n",
      "        (gelu): GELU()\n",
      "        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvMixer_Block(\n",
      "      (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (gelu): GELU()\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (mixer_block5): Sequential(\n",
      "    (0): Residual(\n",
      "      (fn): ConvMixer_Block(\n",
      "        (conv): Conv2d(1536, 1536, kernel_size=(9, 9), stride=(1, 1), padding=same, groups=1536)\n",
      "        (gelu): GELU()\n",
      "        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvMixer_Block(\n",
      "      (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (gelu): GELU()\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (mixer_block6): Sequential(\n",
      "    (0): Residual(\n",
      "      (fn): ConvMixer_Block(\n",
      "        (conv): Conv2d(1536, 1536, kernel_size=(9, 9), stride=(1, 1), padding=same, groups=1536)\n",
      "        (gelu): GELU()\n",
      "        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvMixer_Block(\n",
      "      (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (gelu): GELU()\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (mixer_block7): Sequential(\n",
      "    (0): Residual(\n",
      "      (fn): ConvMixer_Block(\n",
      "        (conv): Conv2d(1536, 1536, kernel_size=(9, 9), stride=(1, 1), padding=same, groups=1536)\n",
      "        (gelu): GELU()\n",
      "        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvMixer_Block(\n",
      "      (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (gelu): GELU()\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (mixer_block8): Sequential(\n",
      "    (0): Residual(\n",
      "      (fn): ConvMixer_Block(\n",
      "        (conv): Conv2d(1536, 1536, kernel_size=(9, 9), stride=(1, 1), padding=same, groups=1536)\n",
      "        (gelu): GELU()\n",
      "        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvMixer_Block(\n",
      "      (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (gelu): GELU()\n",
      "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (upsample): Sequential(\n",
      "    (0): Conv2d(768, 336, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): PixelShuffle(upscale_factor=4)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (head): Conv2d(21, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Start training for 1000 epochs\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [  0/689]  eta: 0:14:45  lr: 0.000000  loss: 0.0247 (0.0247)  time: 1.2853  data: 0.6490  max mem: 39645\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [ 10/689]  eta: 0:17:11  lr: 0.000000  loss: 0.0291 (0.0276)  time: 1.5193  data: 0.0591  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [ 20/689]  eta: 0:17:07  lr: 0.000000  loss: 0.0280 (0.0275)  time: 1.5487  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [ 30/689]  eta: 0:16:57  lr: 0.000000  loss: 0.0274 (0.0279)  time: 1.5576  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [ 40/689]  eta: 0:16:45  lr: 0.000000  loss: 0.0276 (0.0277)  time: 1.5624  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [ 50/689]  eta: 0:16:32  lr: 0.000000  loss: 0.0262 (0.0273)  time: 1.5666  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [ 60/689]  eta: 0:16:18  lr: 0.000000  loss: 0.0259 (0.0272)  time: 1.5708  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [ 70/689]  eta: 0:16:04  lr: 0.000000  loss: 0.0259 (0.0270)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [ 80/689]  eta: 0:15:50  lr: 0.000000  loss: 0.0259 (0.0268)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [ 90/689]  eta: 0:15:36  lr: 0.000000  loss: 0.0262 (0.0270)  time: 1.5768  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [100/689]  eta: 0:15:21  lr: 0.000000  loss: 0.0283 (0.0272)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [110/689]  eta: 0:15:06  lr: 0.000000  loss: 0.0291 (0.0274)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [120/689]  eta: 0:14:51  lr: 0.000000  loss: 0.0281 (0.0274)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [130/689]  eta: 0:14:36  lr: 0.000000  loss: 0.0270 (0.0275)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [140/689]  eta: 0:14:20  lr: 0.000000  loss: 0.0286 (0.0276)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [150/689]  eta: 0:14:05  lr: 0.000000  loss: 0.0279 (0.0276)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [160/689]  eta: 0:13:50  lr: 0.000000  loss: 0.0283 (0.0277)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [170/689]  eta: 0:13:34  lr: 0.000000  loss: 0.0287 (0.0277)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [180/689]  eta: 0:13:19  lr: 0.000000  loss: 0.0288 (0.0278)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [190/689]  eta: 0:13:03  lr: 0.000000  loss: 0.0290 (0.0279)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [200/689]  eta: 0:12:48  lr: 0.000000  loss: 0.0275 (0.0278)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [210/689]  eta: 0:12:32  lr: 0.000000  loss: 0.0264 (0.0278)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [220/689]  eta: 0:12:17  lr: 0.000000  loss: 0.0267 (0.0278)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [230/689]  eta: 0:12:01  lr: 0.000000  loss: 0.0276 (0.0278)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [240/689]  eta: 0:11:46  lr: 0.000000  loss: 0.0287 (0.0278)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [250/689]  eta: 0:11:30  lr: 0.000000  loss: 0.0278 (0.0278)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [260/689]  eta: 0:11:14  lr: 0.000000  loss: 0.0275 (0.0278)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [270/689]  eta: 0:10:59  lr: 0.000000  loss: 0.0261 (0.0277)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [280/689]  eta: 0:10:43  lr: 0.000000  loss: 0.0261 (0.0277)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [290/689]  eta: 0:10:27  lr: 0.000000  loss: 0.0267 (0.0277)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [300/689]  eta: 0:10:12  lr: 0.000000  loss: 0.0267 (0.0277)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [310/689]  eta: 0:09:56  lr: 0.000000  loss: 0.0278 (0.0277)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [320/689]  eta: 0:09:40  lr: 0.000000  loss: 0.0278 (0.0277)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [330/689]  eta: 0:09:25  lr: 0.000000  loss: 0.0290 (0.0278)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [340/689]  eta: 0:09:09  lr: 0.000000  loss: 0.0291 (0.0278)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [350/689]  eta: 0:08:53  lr: 0.000000  loss: 0.0282 (0.0278)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [360/689]  eta: 0:08:38  lr: 0.000000  loss: 0.0281 (0.0278)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [370/689]  eta: 0:08:22  lr: 0.000000  loss: 0.0283 (0.0278)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [380/689]  eta: 0:08:06  lr: 0.000000  loss: 0.0286 (0.0278)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [390/689]  eta: 0:07:51  lr: 0.000000  loss: 0.0287 (0.0278)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [400/689]  eta: 0:07:35  lr: 0.000000  loss: 0.0279 (0.0279)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [410/689]  eta: 0:07:19  lr: 0.000000  loss: 0.0275 (0.0279)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [420/689]  eta: 0:07:03  lr: 0.000000  loss: 0.0281 (0.0279)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [430/689]  eta: 0:06:48  lr: 0.000000  loss: 0.0293 (0.0279)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [440/689]  eta: 0:06:32  lr: 0.000000  loss: 0.0268 (0.0279)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [450/689]  eta: 0:06:16  lr: 0.000000  loss: 0.0268 (0.0279)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [460/689]  eta: 0:06:00  lr: 0.000000  loss: 0.0288 (0.0279)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [470/689]  eta: 0:05:45  lr: 0.000000  loss: 0.0306 (0.0280)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [480/689]  eta: 0:05:29  lr: 0.000000  loss: 0.0281 (0.0279)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [490/689]  eta: 0:05:13  lr: 0.000000  loss: 0.0267 (0.0279)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [500/689]  eta: 0:04:57  lr: 0.000000  loss: 0.0280 (0.0279)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [510/689]  eta: 0:04:42  lr: 0.000000  loss: 0.0279 (0.0279)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [520/689]  eta: 0:04:26  lr: 0.000000  loss: 0.0281 (0.0279)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [530/689]  eta: 0:04:10  lr: 0.000000  loss: 0.0283 (0.0280)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [540/689]  eta: 0:03:54  lr: 0.000000  loss: 0.0286 (0.0280)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [550/689]  eta: 0:03:39  lr: 0.000000  loss: 0.0278 (0.0280)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [560/689]  eta: 0:03:23  lr: 0.000000  loss: 0.0278 (0.0280)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [570/689]  eta: 0:03:07  lr: 0.000000  loss: 0.0275 (0.0280)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [580/689]  eta: 0:02:51  lr: 0.000000  loss: 0.0268 (0.0280)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [590/689]  eta: 0:02:36  lr: 0.000000  loss: 0.0266 (0.0280)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [600/689]  eta: 0:02:20  lr: 0.000000  loss: 0.0274 (0.0280)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [610/689]  eta: 0:02:04  lr: 0.000000  loss: 0.0270 (0.0280)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [620/689]  eta: 0:01:48  lr: 0.000000  loss: 0.0272 (0.0280)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [630/689]  eta: 0:01:33  lr: 0.000000  loss: 0.0281 (0.0279)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [640/689]  eta: 0:01:17  lr: 0.000000  loss: 0.0291 (0.0280)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [650/689]  eta: 0:01:01  lr: 0.000000  loss: 0.0287 (0.0280)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [660/689]  eta: 0:00:45  lr: 0.000000  loss: 0.0279 (0.0280)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [670/689]  eta: 0:00:29  lr: 0.000000  loss: 0.0290 (0.0280)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [680/689]  eta: 0:00:14  lr: 0.000000  loss: 0.0290 (0.0280)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:0]  [688/689]  eta: 0:00:01  lr: 0.000000  loss: 0.0277 (0.0280)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:0] Total time: 0:18:06 (1.5772 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.0277 (0.0280)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:0]  [ 0/14]  eta: 0:00:13  loss: 0.0231 (0.0231)  time: 0.9887  data: 0.3744  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:0]  [13/14]  eta: 0:00:00  loss: 0.0268 (0.0279)  time: 0.1127  data: 0.0268  max mem: 39763\n",
      "Valid: [epoch:0] Total time: 0:00:01 (0.1219 s / it)\n",
      "Averaged stats: loss: 0.0268 (0.0279)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_0_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.028%\n",
      "Min loss: 0.028\n",
      "Best Epoch: 0.000\n",
      "/home/sunggu/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [  0/689]  eta: 0:11:22  lr: 0.000000  loss: 0.0237 (0.0237)  time: 0.9906  data: 0.5147  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [ 10/689]  eta: 0:17:14  lr: 0.000000  loss: 0.0265 (0.0272)  time: 1.5229  data: 0.0469  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [ 20/689]  eta: 0:17:16  lr: 0.000000  loss: 0.0265 (0.0269)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [ 30/689]  eta: 0:17:07  lr: 0.000000  loss: 0.0265 (0.0275)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [ 40/689]  eta: 0:16:54  lr: 0.000000  loss: 0.0265 (0.0273)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [ 50/689]  eta: 0:16:40  lr: 0.000000  loss: 0.0257 (0.0268)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [ 60/689]  eta: 0:16:26  lr: 0.000000  loss: 0.0259 (0.0270)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [ 70/689]  eta: 0:16:11  lr: 0.000000  loss: 0.0268 (0.0269)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [ 80/689]  eta: 0:15:56  lr: 0.000000  loss: 0.0263 (0.0267)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [ 90/689]  eta: 0:15:41  lr: 0.000000  loss: 0.0251 (0.0266)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [100/689]  eta: 0:15:25  lr: 0.000000  loss: 0.0261 (0.0267)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [110/689]  eta: 0:15:10  lr: 0.000000  loss: 0.0270 (0.0269)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [120/689]  eta: 0:14:55  lr: 0.000000  loss: 0.0275 (0.0270)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [130/689]  eta: 0:14:39  lr: 0.000000  loss: 0.0281 (0.0272)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [140/689]  eta: 0:14:24  lr: 0.000000  loss: 0.0276 (0.0273)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [150/689]  eta: 0:14:08  lr: 0.000000  loss: 0.0280 (0.0273)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [160/689]  eta: 0:13:52  lr: 0.000000  loss: 0.0280 (0.0273)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [170/689]  eta: 0:13:37  lr: 0.000000  loss: 0.0265 (0.0274)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [180/689]  eta: 0:13:21  lr: 0.000000  loss: 0.0265 (0.0274)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [190/689]  eta: 0:13:05  lr: 0.000000  loss: 0.0266 (0.0274)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [200/689]  eta: 0:12:50  lr: 0.000000  loss: 0.0277 (0.0274)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [210/689]  eta: 0:12:34  lr: 0.000000  loss: 0.0277 (0.0274)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [220/689]  eta: 0:12:18  lr: 0.000000  loss: 0.0278 (0.0275)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [230/689]  eta: 0:12:03  lr: 0.000000  loss: 0.0276 (0.0274)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [240/689]  eta: 0:11:47  lr: 0.000000  loss: 0.0293 (0.0276)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [250/689]  eta: 0:11:31  lr: 0.000000  loss: 0.0293 (0.0275)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [260/689]  eta: 0:11:16  lr: 0.000000  loss: 0.0274 (0.0276)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [270/689]  eta: 0:11:00  lr: 0.000000  loss: 0.0283 (0.0276)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [280/689]  eta: 0:10:44  lr: 0.000000  loss: 0.0279 (0.0276)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [290/689]  eta: 0:10:28  lr: 0.000000  loss: 0.0279 (0.0276)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [300/689]  eta: 0:10:13  lr: 0.000000  loss: 0.0262 (0.0275)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [310/689]  eta: 0:09:57  lr: 0.000000  loss: 0.0251 (0.0274)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [320/689]  eta: 0:09:41  lr: 0.000000  loss: 0.0259 (0.0275)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [330/689]  eta: 0:09:25  lr: 0.000000  loss: 0.0295 (0.0276)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [340/689]  eta: 0:09:10  lr: 0.000000  loss: 0.0295 (0.0276)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [350/689]  eta: 0:08:54  lr: 0.000000  loss: 0.0263 (0.0275)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [360/689]  eta: 0:08:38  lr: 0.000000  loss: 0.0269 (0.0275)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [370/689]  eta: 0:08:22  lr: 0.000000  loss: 0.0270 (0.0275)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [380/689]  eta: 0:08:07  lr: 0.000000  loss: 0.0283 (0.0275)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [390/689]  eta: 0:07:51  lr: 0.000000  loss: 0.0284 (0.0275)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [400/689]  eta: 0:07:35  lr: 0.000000  loss: 0.0267 (0.0275)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [410/689]  eta: 0:07:19  lr: 0.000000  loss: 0.0294 (0.0275)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [420/689]  eta: 0:07:04  lr: 0.000000  loss: 0.0276 (0.0275)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [430/689]  eta: 0:06:48  lr: 0.000000  loss: 0.0281 (0.0275)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [440/689]  eta: 0:06:32  lr: 0.000000  loss: 0.0284 (0.0276)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [450/689]  eta: 0:06:16  lr: 0.000000  loss: 0.0270 (0.0275)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [460/689]  eta: 0:06:01  lr: 0.000000  loss: 0.0276 (0.0276)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [470/689]  eta: 0:05:45  lr: 0.000000  loss: 0.0267 (0.0276)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [480/689]  eta: 0:05:29  lr: 0.000000  loss: 0.0262 (0.0275)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [490/689]  eta: 0:05:13  lr: 0.000000  loss: 0.0273 (0.0275)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [500/689]  eta: 0:04:58  lr: 0.000000  loss: 0.0273 (0.0275)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [510/689]  eta: 0:04:42  lr: 0.000000  loss: 0.0259 (0.0275)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [520/689]  eta: 0:04:26  lr: 0.000000  loss: 0.0273 (0.0275)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [530/689]  eta: 0:04:10  lr: 0.000000  loss: 0.0283 (0.0275)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [540/689]  eta: 0:03:55  lr: 0.000000  loss: 0.0284 (0.0275)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [550/689]  eta: 0:03:39  lr: 0.000000  loss: 0.0274 (0.0275)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [560/689]  eta: 0:03:23  lr: 0.000000  loss: 0.0271 (0.0275)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [570/689]  eta: 0:03:07  lr: 0.000000  loss: 0.0292 (0.0276)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [580/689]  eta: 0:02:51  lr: 0.000000  loss: 0.0297 (0.0276)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [590/689]  eta: 0:02:36  lr: 0.000000  loss: 0.0300 (0.0276)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [600/689]  eta: 0:02:20  lr: 0.000000  loss: 0.0280 (0.0276)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [610/689]  eta: 0:02:04  lr: 0.000000  loss: 0.0272 (0.0276)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [620/689]  eta: 0:01:48  lr: 0.000000  loss: 0.0270 (0.0276)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [630/689]  eta: 0:01:33  lr: 0.000000  loss: 0.0268 (0.0276)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [640/689]  eta: 0:01:17  lr: 0.000000  loss: 0.0283 (0.0276)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [650/689]  eta: 0:01:01  lr: 0.000000  loss: 0.0283 (0.0276)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [660/689]  eta: 0:00:45  lr: 0.000000  loss: 0.0266 (0.0275)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [670/689]  eta: 0:00:29  lr: 0.000000  loss: 0.0258 (0.0275)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [680/689]  eta: 0:00:14  lr: 0.000000  loss: 0.0264 (0.0275)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:1]  [688/689]  eta: 0:00:01  lr: 0.000000  loss: 0.0264 (0.0275)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:1] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.0264 (0.0275)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:1]  [ 0/14]  eta: 0:00:14  loss: 0.0331 (0.0331)  time: 1.0172  data: 0.3609  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:1]  [13/14]  eta: 0:00:00  loss: 0.0261 (0.0272)  time: 0.1146  data: 0.0258  max mem: 39763\n",
      "Valid: [epoch:1] Total time: 0:00:01 (0.1224 s / it)\n",
      "Averaged stats: loss: 0.0261 (0.0272)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_1_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.027%\n",
      "Min loss: 0.027\n",
      "Best Epoch: 1.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [  0/689]  eta: 0:11:31  lr: 0.000010  loss: 0.0293 (0.0293)  time: 1.0036  data: 0.5259  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [ 10/689]  eta: 0:17:16  lr: 0.000010  loss: 0.0221 (0.0225)  time: 1.5260  data: 0.0479  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [ 20/689]  eta: 0:17:17  lr: 0.000010  loss: 0.0198 (0.0206)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [ 30/689]  eta: 0:17:08  lr: 0.000010  loss: 0.0169 (0.0192)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [ 40/689]  eta: 0:16:55  lr: 0.000010  loss: 0.0143 (0.0176)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [ 50/689]  eta: 0:16:41  lr: 0.000010  loss: 0.0116 (0.0163)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [ 60/689]  eta: 0:16:27  lr: 0.000010  loss: 0.0102 (0.0152)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [ 70/689]  eta: 0:16:12  lr: 0.000010  loss: 0.0090 (0.0142)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [ 80/689]  eta: 0:15:57  lr: 0.000010  loss: 0.0083 (0.0135)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [ 90/689]  eta: 0:15:42  lr: 0.000010  loss: 0.0076 (0.0129)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [100/689]  eta: 0:15:26  lr: 0.000010  loss: 0.0075 (0.0124)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [110/689]  eta: 0:15:11  lr: 0.000010  loss: 0.0072 (0.0119)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [120/689]  eta: 0:14:55  lr: 0.000010  loss: 0.0067 (0.0115)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [130/689]  eta: 0:14:40  lr: 0.000010  loss: 0.0062 (0.0111)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [140/689]  eta: 0:14:24  lr: 0.000010  loss: 0.0058 (0.0107)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [150/689]  eta: 0:14:09  lr: 0.000010  loss: 0.0057 (0.0104)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [160/689]  eta: 0:13:53  lr: 0.000010  loss: 0.0053 (0.0100)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [170/689]  eta: 0:13:37  lr: 0.000010  loss: 0.0049 (0.0097)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [180/689]  eta: 0:13:22  lr: 0.000010  loss: 0.0044 (0.0094)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [190/689]  eta: 0:13:06  lr: 0.000010  loss: 0.0040 (0.0092)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [200/689]  eta: 0:12:50  lr: 0.000010  loss: 0.0038 (0.0089)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [210/689]  eta: 0:12:35  lr: 0.000010  loss: 0.0038 (0.0087)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [220/689]  eta: 0:12:19  lr: 0.000010  loss: 0.0036 (0.0084)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [230/689]  eta: 0:12:03  lr: 0.000010  loss: 0.0036 (0.0082)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [240/689]  eta: 0:11:47  lr: 0.000010  loss: 0.0035 (0.0080)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [250/689]  eta: 0:11:32  lr: 0.000010  loss: 0.0032 (0.0079)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [260/689]  eta: 0:11:16  lr: 0.000010  loss: 0.0032 (0.0077)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [270/689]  eta: 0:11:00  lr: 0.000010  loss: 0.0030 (0.0075)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [280/689]  eta: 0:10:45  lr: 0.000010  loss: 0.0029 (0.0074)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [290/689]  eta: 0:10:29  lr: 0.000010  loss: 0.0028 (0.0072)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [300/689]  eta: 0:10:13  lr: 0.000010  loss: 0.0026 (0.0070)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [310/689]  eta: 0:09:57  lr: 0.000010  loss: 0.0025 (0.0069)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [320/689]  eta: 0:09:42  lr: 0.000010  loss: 0.0024 (0.0068)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [330/689]  eta: 0:09:26  lr: 0.000010  loss: 0.0024 (0.0066)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [340/689]  eta: 0:09:10  lr: 0.000010  loss: 0.0023 (0.0065)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [350/689]  eta: 0:08:54  lr: 0.000010  loss: 0.0022 (0.0064)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [360/689]  eta: 0:08:39  lr: 0.000010  loss: 0.0021 (0.0063)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [370/689]  eta: 0:08:23  lr: 0.000010  loss: 0.0021 (0.0062)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [380/689]  eta: 0:08:07  lr: 0.000010  loss: 0.0020 (0.0060)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [390/689]  eta: 0:07:51  lr: 0.000010  loss: 0.0020 (0.0060)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [400/689]  eta: 0:07:35  lr: 0.000010  loss: 0.0020 (0.0059)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [410/689]  eta: 0:07:20  lr: 0.000010  loss: 0.0020 (0.0058)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [420/689]  eta: 0:07:04  lr: 0.000010  loss: 0.0019 (0.0057)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [430/689]  eta: 0:06:48  lr: 0.000010  loss: 0.0017 (0.0056)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [440/689]  eta: 0:06:32  lr: 0.000010  loss: 0.0017 (0.0055)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [450/689]  eta: 0:06:17  lr: 0.000010  loss: 0.0017 (0.0054)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [460/689]  eta: 0:06:01  lr: 0.000010  loss: 0.0017 (0.0053)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [470/689]  eta: 0:05:45  lr: 0.000010  loss: 0.0017 (0.0053)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [480/689]  eta: 0:05:29  lr: 0.000010  loss: 0.0017 (0.0052)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [490/689]  eta: 0:05:13  lr: 0.000010  loss: 0.0016 (0.0051)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [500/689]  eta: 0:04:58  lr: 0.000010  loss: 0.0015 (0.0051)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [510/689]  eta: 0:04:42  lr: 0.000010  loss: 0.0015 (0.0050)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [520/689]  eta: 0:04:26  lr: 0.000010  loss: 0.0015 (0.0049)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [530/689]  eta: 0:04:10  lr: 0.000010  loss: 0.0014 (0.0049)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [540/689]  eta: 0:03:55  lr: 0.000010  loss: 0.0013 (0.0048)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [550/689]  eta: 0:03:39  lr: 0.000010  loss: 0.0013 (0.0047)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [560/689]  eta: 0:03:23  lr: 0.000010  loss: 0.0013 (0.0047)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [570/689]  eta: 0:03:07  lr: 0.000010  loss: 0.0013 (0.0046)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [580/689]  eta: 0:02:51  lr: 0.000010  loss: 0.0012 (0.0046)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [590/689]  eta: 0:02:36  lr: 0.000010  loss: 0.0013 (0.0045)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [600/689]  eta: 0:02:20  lr: 0.000010  loss: 0.0012 (0.0044)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [610/689]  eta: 0:02:04  lr: 0.000010  loss: 0.0012 (0.0044)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [620/689]  eta: 0:01:48  lr: 0.000010  loss: 0.0012 (0.0043)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [630/689]  eta: 0:01:33  lr: 0.000010  loss: 0.0011 (0.0043)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [640/689]  eta: 0:01:17  lr: 0.000010  loss: 0.0011 (0.0042)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [650/689]  eta: 0:01:01  lr: 0.000010  loss: 0.0011 (0.0042)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [660/689]  eta: 0:00:45  lr: 0.000010  loss: 0.0012 (0.0041)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [670/689]  eta: 0:00:29  lr: 0.000010  loss: 0.0011 (0.0041)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [680/689]  eta: 0:00:14  lr: 0.000010  loss: 0.0011 (0.0041)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:2]  [688/689]  eta: 0:00:01  lr: 0.000010  loss: 0.0011 (0.0040)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:2] Total time: 0:18:07 (1.5783 s / it)\n",
      "Averaged stats: lr: 0.000010  loss: 0.0011 (0.0040)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:2]  [ 0/14]  eta: 0:00:14  loss: 0.0012 (0.0012)  time: 1.0440  data: 0.3933  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:2]  [13/14]  eta: 0:00:00  loss: 0.0008 (0.0010)  time: 0.1166  data: 0.0281  max mem: 39763\n",
      "Valid: [epoch:2] Total time: 0:00:01 (0.1253 s / it)\n",
      "Averaged stats: loss: 0.0008 (0.0010)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_2_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.001%\n",
      "Min loss: 0.001\n",
      "Best Epoch: 2.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [  0/689]  eta: 0:11:33  lr: 0.000020  loss: 0.0010 (0.0010)  time: 1.0072  data: 0.5334  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 10/689]  eta: 0:17:15  lr: 0.000020  loss: 0.0010 (0.0010)  time: 1.5256  data: 0.0486  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 20/689]  eta: 0:17:17  lr: 0.000020  loss: 0.0010 (0.0010)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 30/689]  eta: 0:17:07  lr: 0.000020  loss: 0.0010 (0.0011)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 40/689]  eta: 0:16:55  lr: 0.000020  loss: 0.0009 (0.0010)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 50/689]  eta: 0:16:41  lr: 0.000020  loss: 0.0009 (0.0010)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 60/689]  eta: 0:16:26  lr: 0.000020  loss: 0.0009 (0.0010)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 70/689]  eta: 0:16:12  lr: 0.000020  loss: 0.0009 (0.0010)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 80/689]  eta: 0:15:57  lr: 0.000020  loss: 0.0009 (0.0010)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [ 90/689]  eta: 0:15:41  lr: 0.000020  loss: 0.0009 (0.0010)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [100/689]  eta: 0:15:26  lr: 0.000020  loss: 0.0008 (0.0010)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [110/689]  eta: 0:15:11  lr: 0.000020  loss: 0.0009 (0.0010)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [120/689]  eta: 0:14:55  lr: 0.000020  loss: 0.0008 (0.0009)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [130/689]  eta: 0:14:40  lr: 0.000020  loss: 0.0008 (0.0009)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [140/689]  eta: 0:14:24  lr: 0.000020  loss: 0.0008 (0.0009)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [150/689]  eta: 0:14:09  lr: 0.000020  loss: 0.0008 (0.0009)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [160/689]  eta: 0:13:53  lr: 0.000020  loss: 0.0008 (0.0009)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [170/689]  eta: 0:13:37  lr: 0.000020  loss: 0.0008 (0.0009)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [180/689]  eta: 0:13:22  lr: 0.000020  loss: 0.0007 (0.0009)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [190/689]  eta: 0:13:06  lr: 0.000020  loss: 0.0008 (0.0009)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [200/689]  eta: 0:12:50  lr: 0.000020  loss: 0.0008 (0.0009)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [210/689]  eta: 0:12:35  lr: 0.000020  loss: 0.0007 (0.0009)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [220/689]  eta: 0:12:19  lr: 0.000020  loss: 0.0006 (0.0009)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [230/689]  eta: 0:12:03  lr: 0.000020  loss: 0.0006 (0.0009)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [240/689]  eta: 0:11:47  lr: 0.000020  loss: 0.0006 (0.0009)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [250/689]  eta: 0:11:32  lr: 0.000020  loss: 0.0006 (0.0008)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [260/689]  eta: 0:11:16  lr: 0.000020  loss: 0.0006 (0.0008)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [270/689]  eta: 0:11:00  lr: 0.000020  loss: 0.0006 (0.0008)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [280/689]  eta: 0:10:45  lr: 0.000020  loss: 0.0006 (0.0008)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [290/689]  eta: 0:10:29  lr: 0.000020  loss: 0.0006 (0.0008)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [300/689]  eta: 0:10:13  lr: 0.000020  loss: 0.0006 (0.0008)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [310/689]  eta: 0:09:57  lr: 0.000020  loss: 0.0006 (0.0008)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [320/689]  eta: 0:09:42  lr: 0.000020  loss: 0.0006 (0.0008)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [330/689]  eta: 0:09:26  lr: 0.000020  loss: 0.0006 (0.0008)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [340/689]  eta: 0:09:10  lr: 0.000020  loss: 0.0005 (0.0008)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [350/689]  eta: 0:08:54  lr: 0.000020  loss: 0.0005 (0.0008)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [360/689]  eta: 0:08:39  lr: 0.000020  loss: 0.0005 (0.0008)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [370/689]  eta: 0:08:23  lr: 0.000020  loss: 0.0005 (0.0008)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [380/689]  eta: 0:08:07  lr: 0.000020  loss: 0.0005 (0.0008)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [390/689]  eta: 0:07:51  lr: 0.000020  loss: 0.0005 (0.0008)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [400/689]  eta: 0:07:35  lr: 0.000020  loss: 0.0006 (0.0008)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [410/689]  eta: 0:07:20  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [420/689]  eta: 0:07:04  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [430/689]  eta: 0:06:48  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [440/689]  eta: 0:06:32  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [450/689]  eta: 0:06:17  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [460/689]  eta: 0:06:01  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [470/689]  eta: 0:05:45  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [480/689]  eta: 0:05:29  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [490/689]  eta: 0:05:14  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [500/689]  eta: 0:04:58  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [510/689]  eta: 0:04:42  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [520/689]  eta: 0:04:26  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [530/689]  eta: 0:04:10  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [540/689]  eta: 0:03:55  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [550/689]  eta: 0:03:39  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [560/689]  eta: 0:03:23  lr: 0.000020  loss: 0.0004 (0.0007)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [570/689]  eta: 0:03:07  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [580/689]  eta: 0:02:52  lr: 0.000020  loss: 0.0005 (0.0007)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [590/689]  eta: 0:02:36  lr: 0.000020  loss: 0.0004 (0.0007)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [600/689]  eta: 0:02:20  lr: 0.000020  loss: 0.0004 (0.0007)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [610/689]  eta: 0:02:04  lr: 0.000020  loss: 0.0004 (0.0007)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [620/689]  eta: 0:01:48  lr: 0.000020  loss: 0.0004 (0.0007)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [630/689]  eta: 0:01:33  lr: 0.000020  loss: 0.0004 (0.0007)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [640/689]  eta: 0:01:17  lr: 0.000020  loss: 0.0004 (0.0007)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [650/689]  eta: 0:01:01  lr: 0.000020  loss: 0.0004 (0.0006)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [660/689]  eta: 0:00:45  lr: 0.000020  loss: 0.0004 (0.0006)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [670/689]  eta: 0:00:29  lr: 0.000020  loss: 0.0004 (0.0006)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [680/689]  eta: 0:00:14  lr: 0.000020  loss: 0.0004 (0.0006)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:3]  [688/689]  eta: 0:00:01  lr: 0.000020  loss: 0.0004 (0.0006)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:3] Total time: 0:18:07 (1.5787 s / it)\n",
      "Averaged stats: lr: 0.000020  loss: 0.0004 (0.0006)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:3]  [ 0/14]  eta: 0:00:14  loss: 0.0005 (0.0005)  time: 1.0117  data: 0.3702  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:3]  [13/14]  eta: 0:00:00  loss: 0.0004 (0.0004)  time: 0.1142  data: 0.0265  max mem: 39763\n",
      "Valid: [epoch:3] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.0004 (0.0004)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_3_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.000%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 3.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [  0/689]  eta: 0:11:35  lr: 0.000030  loss: 0.0004 (0.0004)  time: 1.0094  data: 0.5297  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 10/689]  eta: 0:17:16  lr: 0.000030  loss: 0.0004 (0.0004)  time: 1.5267  data: 0.0482  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 20/689]  eta: 0:17:18  lr: 0.000030  loss: 0.0004 (0.0004)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 30/689]  eta: 0:17:08  lr: 0.000030  loss: 0.0004 (0.0004)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 40/689]  eta: 0:16:55  lr: 0.000030  loss: 0.0004 (0.0004)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 50/689]  eta: 0:16:41  lr: 0.000030  loss: 0.0004 (0.0004)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 60/689]  eta: 0:16:27  lr: 0.000030  loss: 0.0004 (0.0004)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 70/689]  eta: 0:16:12  lr: 0.000030  loss: 0.0004 (0.0004)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 80/689]  eta: 0:15:57  lr: 0.000030  loss: 0.0004 (0.0004)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [ 90/689]  eta: 0:15:42  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [100/689]  eta: 0:15:26  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [110/689]  eta: 0:15:11  lr: 0.000030  loss: 0.0004 (0.0004)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [120/689]  eta: 0:14:56  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [130/689]  eta: 0:14:40  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [140/689]  eta: 0:14:25  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [150/689]  eta: 0:14:09  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [160/689]  eta: 0:13:53  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [170/689]  eta: 0:13:38  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [180/689]  eta: 0:13:22  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [190/689]  eta: 0:13:06  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [200/689]  eta: 0:12:51  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [210/689]  eta: 0:12:35  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [220/689]  eta: 0:12:19  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [230/689]  eta: 0:12:03  lr: 0.000030  loss: 0.0003 (0.0004)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [240/689]  eta: 0:11:48  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [250/689]  eta: 0:11:32  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [260/689]  eta: 0:11:16  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [270/689]  eta: 0:11:01  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [280/689]  eta: 0:10:45  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [290/689]  eta: 0:10:29  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [300/689]  eta: 0:10:13  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [310/689]  eta: 0:09:58  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [320/689]  eta: 0:09:42  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [330/689]  eta: 0:09:26  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [340/689]  eta: 0:09:10  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [350/689]  eta: 0:08:55  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [360/689]  eta: 0:08:39  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [370/689]  eta: 0:08:23  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [380/689]  eta: 0:08:07  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [390/689]  eta: 0:07:51  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [400/689]  eta: 0:07:36  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [410/689]  eta: 0:07:20  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [420/689]  eta: 0:07:04  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [430/689]  eta: 0:06:48  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [440/689]  eta: 0:06:33  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [450/689]  eta: 0:06:17  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [460/689]  eta: 0:06:01  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [470/689]  eta: 0:05:45  lr: 0.000030  loss: 0.0003 (0.0003)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [480/689]  eta: 0:05:29  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [490/689]  eta: 0:05:14  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [500/689]  eta: 0:04:58  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [510/689]  eta: 0:04:42  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [520/689]  eta: 0:04:26  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [530/689]  eta: 0:04:11  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [540/689]  eta: 0:03:55  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [550/689]  eta: 0:03:39  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [560/689]  eta: 0:03:23  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [570/689]  eta: 0:03:07  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [580/689]  eta: 0:02:52  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [590/689]  eta: 0:02:36  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [600/689]  eta: 0:02:20  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [610/689]  eta: 0:02:04  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [620/689]  eta: 0:01:48  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [630/689]  eta: 0:01:33  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [640/689]  eta: 0:01:17  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [650/689]  eta: 0:01:01  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [660/689]  eta: 0:00:45  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [670/689]  eta: 0:00:29  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [680/689]  eta: 0:00:14  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:4]  [688/689]  eta: 0:00:01  lr: 0.000030  loss: 0.0002 (0.0003)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:4] Total time: 0:18:07 (1.5790 s / it)\n",
      "Averaged stats: lr: 0.000030  loss: 0.0002 (0.0003)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:4]  [ 0/14]  eta: 0:00:14  loss: 0.0003 (0.0003)  time: 1.0164  data: 0.3637  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:4]  [13/14]  eta: 0:00:00  loss: 0.0002 (0.0002)  time: 0.1146  data: 0.0260  max mem: 39763\n",
      "Valid: [epoch:4] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.0002 (0.0002)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_4_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.000%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 4.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [  0/689]  eta: 0:11:44  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.0231  data: 0.5472  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 10/689]  eta: 0:17:16  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5268  data: 0.0498  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 20/689]  eta: 0:17:17  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 30/689]  eta: 0:17:07  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 40/689]  eta: 0:16:55  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 50/689]  eta: 0:16:41  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 60/689]  eta: 0:16:27  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 70/689]  eta: 0:16:12  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 80/689]  eta: 0:15:57  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [ 90/689]  eta: 0:15:42  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [100/689]  eta: 0:15:26  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [110/689]  eta: 0:15:11  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [120/689]  eta: 0:14:55  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [130/689]  eta: 0:14:40  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [140/689]  eta: 0:14:24  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [150/689]  eta: 0:14:09  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [160/689]  eta: 0:13:53  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [170/689]  eta: 0:13:38  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [180/689]  eta: 0:13:22  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [190/689]  eta: 0:13:06  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [200/689]  eta: 0:12:51  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [210/689]  eta: 0:12:35  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [220/689]  eta: 0:12:19  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [230/689]  eta: 0:12:03  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [240/689]  eta: 0:11:48  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [250/689]  eta: 0:11:32  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [260/689]  eta: 0:11:16  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [270/689]  eta: 0:11:01  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [280/689]  eta: 0:10:45  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [290/689]  eta: 0:10:29  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [300/689]  eta: 0:10:13  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [310/689]  eta: 0:09:58  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [320/689]  eta: 0:09:42  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [330/689]  eta: 0:09:26  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [340/689]  eta: 0:09:10  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [350/689]  eta: 0:08:55  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [360/689]  eta: 0:08:39  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [370/689]  eta: 0:08:23  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [380/689]  eta: 0:08:07  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [390/689]  eta: 0:07:51  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [400/689]  eta: 0:07:36  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [410/689]  eta: 0:07:20  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [420/689]  eta: 0:07:04  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [430/689]  eta: 0:06:48  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [440/689]  eta: 0:06:33  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [450/689]  eta: 0:06:17  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [460/689]  eta: 0:06:01  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [470/689]  eta: 0:05:45  lr: 0.000040  loss: 0.0002 (0.0002)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [480/689]  eta: 0:05:29  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [490/689]  eta: 0:05:14  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [500/689]  eta: 0:04:58  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [510/689]  eta: 0:04:42  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [520/689]  eta: 0:04:26  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [530/689]  eta: 0:04:11  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [540/689]  eta: 0:03:55  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [550/689]  eta: 0:03:39  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [560/689]  eta: 0:03:23  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [570/689]  eta: 0:03:07  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [580/689]  eta: 0:02:52  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [590/689]  eta: 0:02:36  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [600/689]  eta: 0:02:20  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [610/689]  eta: 0:02:04  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [620/689]  eta: 0:01:48  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [630/689]  eta: 0:01:33  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [640/689]  eta: 0:01:17  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [650/689]  eta: 0:01:01  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [660/689]  eta: 0:00:45  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [670/689]  eta: 0:00:30  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [680/689]  eta: 0:00:14  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:5]  [688/689]  eta: 0:00:01  lr: 0.000040  loss: 0.0001 (0.0002)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:5] Total time: 0:18:08 (1.5794 s / it)\n",
      "Averaged stats: lr: 0.000040  loss: 0.0001 (0.0002)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:5]  [ 0/14]  eta: 0:00:14  loss: 0.0002 (0.0002)  time: 1.0371  data: 0.3773  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:5]  [13/14]  eta: 0:00:00  loss: 0.0001 (0.0001)  time: 0.1160  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:5] Total time: 0:00:01 (0.1254 s / it)\n",
      "Averaged stats: loss: 0.0001 (0.0001)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_5_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.000%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 5.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [  0/689]  eta: 0:11:27  lr: 0.000050  loss: 0.0001 (0.0001)  time: 0.9982  data: 0.5213  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 10/689]  eta: 0:17:16  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5261  data: 0.0475  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 20/689]  eta: 0:17:18  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 30/689]  eta: 0:17:08  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 40/689]  eta: 0:16:55  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 50/689]  eta: 0:16:41  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 60/689]  eta: 0:16:27  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 70/689]  eta: 0:16:12  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 80/689]  eta: 0:15:57  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [ 90/689]  eta: 0:15:42  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [100/689]  eta: 0:15:26  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [110/689]  eta: 0:15:11  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [120/689]  eta: 0:14:56  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [130/689]  eta: 0:14:40  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [140/689]  eta: 0:14:24  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [150/689]  eta: 0:14:09  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [160/689]  eta: 0:13:53  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [170/689]  eta: 0:13:38  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [180/689]  eta: 0:13:22  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [190/689]  eta: 0:13:06  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [200/689]  eta: 0:12:50  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [210/689]  eta: 0:12:35  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [220/689]  eta: 0:12:19  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [230/689]  eta: 0:12:03  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [240/689]  eta: 0:11:48  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [250/689]  eta: 0:11:32  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [260/689]  eta: 0:11:16  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [270/689]  eta: 0:11:00  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [280/689]  eta: 0:10:45  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [290/689]  eta: 0:10:29  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [300/689]  eta: 0:10:13  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [310/689]  eta: 0:09:57  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [320/689]  eta: 0:09:42  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [330/689]  eta: 0:09:26  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [340/689]  eta: 0:09:10  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [350/689]  eta: 0:08:54  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [360/689]  eta: 0:08:39  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [370/689]  eta: 0:08:23  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [380/689]  eta: 0:08:07  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [390/689]  eta: 0:07:51  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [400/689]  eta: 0:07:36  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [410/689]  eta: 0:07:20  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [420/689]  eta: 0:07:04  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [430/689]  eta: 0:06:48  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [440/689]  eta: 0:06:33  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [450/689]  eta: 0:06:17  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [460/689]  eta: 0:06:01  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [470/689]  eta: 0:05:45  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [480/689]  eta: 0:05:29  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [490/689]  eta: 0:05:14  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [500/689]  eta: 0:04:58  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [510/689]  eta: 0:04:42  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [520/689]  eta: 0:04:26  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [530/689]  eta: 0:04:10  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [540/689]  eta: 0:03:55  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [550/689]  eta: 0:03:39  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [560/689]  eta: 0:03:23  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [570/689]  eta: 0:03:07  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [580/689]  eta: 0:02:52  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [590/689]  eta: 0:02:36  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [600/689]  eta: 0:02:20  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [610/689]  eta: 0:02:04  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [620/689]  eta: 0:01:48  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [630/689]  eta: 0:01:33  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [640/689]  eta: 0:01:17  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [650/689]  eta: 0:01:01  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [660/689]  eta: 0:00:45  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [670/689]  eta: 0:00:29  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [680/689]  eta: 0:00:14  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:6]  [688/689]  eta: 0:00:01  lr: 0.000050  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:6] Total time: 0:18:07 (1.5790 s / it)\n",
      "Averaged stats: lr: 0.000050  loss: 0.0001 (0.0001)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:6]  [ 0/14]  eta: 0:00:14  loss: 0.0001 (0.0001)  time: 1.0149  data: 0.3827  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:6]  [13/14]  eta: 0:00:00  loss: 0.0001 (0.0001)  time: 0.1145  data: 0.0274  max mem: 39763\n",
      "Valid: [epoch:6] Total time: 0:00:01 (0.1238 s / it)\n",
      "Averaged stats: loss: 0.0001 (0.0001)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_6_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.000%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 6.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [  0/689]  eta: 0:11:57  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.0417  data: 0.5665  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 10/689]  eta: 0:17:17  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5287  data: 0.0516  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 20/689]  eta: 0:17:18  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 30/689]  eta: 0:17:08  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 40/689]  eta: 0:16:55  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 50/689]  eta: 0:16:41  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 60/689]  eta: 0:16:27  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 70/689]  eta: 0:16:12  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 80/689]  eta: 0:15:57  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [ 90/689]  eta: 0:15:41  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [100/689]  eta: 0:15:26  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [110/689]  eta: 0:15:11  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [120/689]  eta: 0:14:55  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [130/689]  eta: 0:14:40  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [140/689]  eta: 0:14:24  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [150/689]  eta: 0:14:08  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [160/689]  eta: 0:13:53  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [170/689]  eta: 0:13:37  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [180/689]  eta: 0:13:22  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [190/689]  eta: 0:13:06  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [200/689]  eta: 0:12:50  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [210/689]  eta: 0:12:35  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [220/689]  eta: 0:12:19  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [230/689]  eta: 0:12:03  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [240/689]  eta: 0:11:47  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [250/689]  eta: 0:11:32  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [260/689]  eta: 0:11:16  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [270/689]  eta: 0:11:00  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [280/689]  eta: 0:10:45  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [290/689]  eta: 0:10:29  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [300/689]  eta: 0:10:13  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [310/689]  eta: 0:09:57  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [320/689]  eta: 0:09:42  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [330/689]  eta: 0:09:26  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [340/689]  eta: 0:09:10  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [350/689]  eta: 0:08:54  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [360/689]  eta: 0:08:39  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [370/689]  eta: 0:08:23  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [380/689]  eta: 0:08:07  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [390/689]  eta: 0:07:51  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [400/689]  eta: 0:07:36  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [410/689]  eta: 0:07:20  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [420/689]  eta: 0:07:04  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [430/689]  eta: 0:06:48  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [440/689]  eta: 0:06:32  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [450/689]  eta: 0:06:17  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [460/689]  eta: 0:06:01  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [470/689]  eta: 0:05:45  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [480/689]  eta: 0:05:29  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [490/689]  eta: 0:05:14  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [500/689]  eta: 0:04:58  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [510/689]  eta: 0:04:42  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [520/689]  eta: 0:04:26  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [530/689]  eta: 0:04:10  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [540/689]  eta: 0:03:55  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [550/689]  eta: 0:03:39  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [560/689]  eta: 0:03:23  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [570/689]  eta: 0:03:07  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [580/689]  eta: 0:02:52  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [590/689]  eta: 0:02:36  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [600/689]  eta: 0:02:20  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [610/689]  eta: 0:02:04  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [620/689]  eta: 0:01:48  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [630/689]  eta: 0:01:33  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [640/689]  eta: 0:01:17  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [650/689]  eta: 0:01:01  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [660/689]  eta: 0:00:45  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [670/689]  eta: 0:00:29  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [680/689]  eta: 0:00:14  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:7]  [688/689]  eta: 0:00:01  lr: 0.000060  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:7] Total time: 0:18:08 (1.5791 s / it)\n",
      "Averaged stats: lr: 0.000060  loss: 0.0001 (0.0001)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:7]  [ 0/14]  eta: 0:00:13  loss: 0.0000 (0.0000)  time: 0.9332  data: 0.3772  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:7]  [13/14]  eta: 0:00:00  loss: 0.0000 (0.0001)  time: 0.1086  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:7] Total time: 0:00:01 (0.1179 s / it)\n",
      "Averaged stats: loss: 0.0000 (0.0001)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_7_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.000%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 7.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [  0/689]  eta: 0:11:28  lr: 0.000070  loss: 0.0001 (0.0001)  time: 0.9996  data: 0.5220  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 10/689]  eta: 0:17:16  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5261  data: 0.0475  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 20/689]  eta: 0:17:17  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 30/689]  eta: 0:17:08  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 40/689]  eta: 0:16:55  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 50/689]  eta: 0:16:41  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 60/689]  eta: 0:16:27  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 70/689]  eta: 0:16:12  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 80/689]  eta: 0:15:57  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [ 90/689]  eta: 0:15:42  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [100/689]  eta: 0:15:26  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [110/689]  eta: 0:15:11  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [120/689]  eta: 0:14:55  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [130/689]  eta: 0:14:40  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [140/689]  eta: 0:14:24  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [150/689]  eta: 0:14:09  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [160/689]  eta: 0:13:53  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [170/689]  eta: 0:13:37  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [180/689]  eta: 0:13:22  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [190/689]  eta: 0:13:06  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [200/689]  eta: 0:12:50  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [210/689]  eta: 0:12:35  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [220/689]  eta: 0:12:19  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [230/689]  eta: 0:12:03  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [240/689]  eta: 0:11:48  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [250/689]  eta: 0:11:32  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [260/689]  eta: 0:11:16  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [270/689]  eta: 0:11:00  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [280/689]  eta: 0:10:45  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [290/689]  eta: 0:10:29  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [300/689]  eta: 0:10:13  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [310/689]  eta: 0:09:57  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [320/689]  eta: 0:09:42  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [330/689]  eta: 0:09:26  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [340/689]  eta: 0:09:10  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [350/689]  eta: 0:08:54  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [360/689]  eta: 0:08:39  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [370/689]  eta: 0:08:23  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [380/689]  eta: 0:08:07  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [390/689]  eta: 0:07:51  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [400/689]  eta: 0:07:36  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [410/689]  eta: 0:07:20  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [420/689]  eta: 0:07:04  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [430/689]  eta: 0:06:48  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [440/689]  eta: 0:06:32  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [450/689]  eta: 0:06:17  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [460/689]  eta: 0:06:01  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [470/689]  eta: 0:05:45  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [480/689]  eta: 0:05:29  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [490/689]  eta: 0:05:14  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [500/689]  eta: 0:04:58  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [510/689]  eta: 0:04:42  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [520/689]  eta: 0:04:26  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [530/689]  eta: 0:04:10  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [540/689]  eta: 0:03:55  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [550/689]  eta: 0:03:39  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [560/689]  eta: 0:03:23  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [570/689]  eta: 0:03:07  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [580/689]  eta: 0:02:52  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [590/689]  eta: 0:02:36  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [600/689]  eta: 0:02:20  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [610/689]  eta: 0:02:04  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [620/689]  eta: 0:01:48  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [630/689]  eta: 0:01:33  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [640/689]  eta: 0:01:17  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [650/689]  eta: 0:01:01  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [660/689]  eta: 0:00:45  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [670/689]  eta: 0:00:29  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [680/689]  eta: 0:00:14  lr: 0.000070  loss: 0.0001 (0.0001)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:8]  [688/689]  eta: 0:00:01  lr: 0.000070  loss: 0.0000 (0.0001)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:8] Total time: 0:18:07 (1.5790 s / it)\n",
      "Averaged stats: lr: 0.000070  loss: 0.0000 (0.0001)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:8]  [ 0/14]  eta: 0:00:14  loss: 0.0000 (0.0000)  time: 1.0187  data: 0.3895  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:8]  [13/14]  eta: 0:00:00  loss: 0.0000 (0.0000)  time: 0.1148  data: 0.0279  max mem: 39763\n",
      "Valid: [epoch:8] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_8_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.000%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 8.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [  0/689]  eta: 0:12:02  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.0491  data: 0.5715  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 10/689]  eta: 0:17:18  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5291  data: 0.0520  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 20/689]  eta: 0:17:18  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 30/689]  eta: 0:17:08  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 40/689]  eta: 0:16:55  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 50/689]  eta: 0:16:41  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 60/689]  eta: 0:16:27  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 70/689]  eta: 0:16:12  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 80/689]  eta: 0:15:57  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [ 90/689]  eta: 0:15:42  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [100/689]  eta: 0:15:26  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [110/689]  eta: 0:15:11  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [120/689]  eta: 0:14:56  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [130/689]  eta: 0:14:40  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [140/689]  eta: 0:14:24  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [150/689]  eta: 0:14:09  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [160/689]  eta: 0:13:53  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [170/689]  eta: 0:13:37  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [180/689]  eta: 0:13:22  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [190/689]  eta: 0:13:06  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [200/689]  eta: 0:12:50  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [210/689]  eta: 0:12:35  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [220/689]  eta: 0:12:19  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [230/689]  eta: 0:12:03  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [240/689]  eta: 0:11:48  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [250/689]  eta: 0:11:32  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [260/689]  eta: 0:11:16  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [270/689]  eta: 0:11:00  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [280/689]  eta: 0:10:45  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [290/689]  eta: 0:10:29  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [300/689]  eta: 0:10:13  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [310/689]  eta: 0:09:57  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [320/689]  eta: 0:09:42  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [330/689]  eta: 0:09:26  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [340/689]  eta: 0:09:10  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [350/689]  eta: 0:08:54  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [360/689]  eta: 0:08:39  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [370/689]  eta: 0:08:23  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [380/689]  eta: 0:08:07  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [390/689]  eta: 0:07:51  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [400/689]  eta: 0:07:36  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [410/689]  eta: 0:07:20  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [420/689]  eta: 0:07:04  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [430/689]  eta: 0:06:48  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [440/689]  eta: 0:06:32  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [450/689]  eta: 0:06:17  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [460/689]  eta: 0:06:01  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [470/689]  eta: 0:05:45  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [480/689]  eta: 0:05:29  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [490/689]  eta: 0:05:14  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [500/689]  eta: 0:04:58  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [510/689]  eta: 0:04:42  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [520/689]  eta: 0:04:26  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [530/689]  eta: 0:04:10  lr: 0.000080  loss: 0.0001 (0.0000)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [540/689]  eta: 0:03:55  lr: 0.000080  loss: 0.0001 (0.0000)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [550/689]  eta: 0:03:39  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [560/689]  eta: 0:03:23  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [570/689]  eta: 0:03:07  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [580/689]  eta: 0:02:52  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [590/689]  eta: 0:02:36  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [600/689]  eta: 0:02:20  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [610/689]  eta: 0:02:04  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [620/689]  eta: 0:01:48  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [630/689]  eta: 0:01:33  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [640/689]  eta: 0:01:17  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [650/689]  eta: 0:01:01  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [660/689]  eta: 0:00:45  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [670/689]  eta: 0:00:29  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [680/689]  eta: 0:00:14  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:9]  [688/689]  eta: 0:00:01  lr: 0.000080  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:9] Total time: 0:18:07 (1.5789 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.0000 (0.0000)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:9]  [ 0/14]  eta: 0:00:11  loss: 0.0000 (0.0000)  time: 0.8125  data: 0.3819  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:9]  [13/14]  eta: 0:00:00  loss: 0.0000 (0.0000)  time: 0.1000  data: 0.0273  max mem: 39763\n",
      "Valid: [epoch:9] Total time: 0:00:01 (0.1096 s / it)\n",
      "Averaged stats: loss: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_9_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.000%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 9.000\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [  0/689]  eta: 0:11:55  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.0386  data: 0.5596  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 10/689]  eta: 0:17:18  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5291  data: 0.0510  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 20/689]  eta: 0:17:19  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 30/689]  eta: 0:17:09  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 40/689]  eta: 0:16:56  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 50/689]  eta: 0:16:42  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 60/689]  eta: 0:16:27  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 70/689]  eta: 0:16:12  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 80/689]  eta: 0:15:57  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [ 90/689]  eta: 0:15:42  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [100/689]  eta: 0:15:27  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [110/689]  eta: 0:15:11  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [120/689]  eta: 0:14:56  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [130/689]  eta: 0:14:40  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [140/689]  eta: 0:14:24  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [150/689]  eta: 0:14:09  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [160/689]  eta: 0:13:53  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [170/689]  eta: 0:13:38  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [180/689]  eta: 0:13:22  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [190/689]  eta: 0:13:06  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [200/689]  eta: 0:12:51  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [210/689]  eta: 0:12:35  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [220/689]  eta: 0:12:19  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [230/689]  eta: 0:12:03  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [240/689]  eta: 0:11:48  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [250/689]  eta: 0:11:32  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [260/689]  eta: 0:11:16  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [270/689]  eta: 0:11:00  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [280/689]  eta: 0:10:45  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [290/689]  eta: 0:10:29  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [300/689]  eta: 0:10:13  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [310/689]  eta: 0:09:57  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [320/689]  eta: 0:09:42  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [330/689]  eta: 0:09:26  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [340/689]  eta: 0:09:10  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [350/689]  eta: 0:08:54  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [360/689]  eta: 0:08:39  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [370/689]  eta: 0:08:23  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [380/689]  eta: 0:08:07  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [390/689]  eta: 0:07:51  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [400/689]  eta: 0:07:36  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [410/689]  eta: 0:07:20  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [420/689]  eta: 0:07:04  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [430/689]  eta: 0:06:48  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5787  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [440/689]  eta: 0:06:32  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [450/689]  eta: 0:06:17  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [460/689]  eta: 0:06:01  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [470/689]  eta: 0:05:45  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [480/689]  eta: 0:05:29  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [490/689]  eta: 0:05:14  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [500/689]  eta: 0:04:58  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [510/689]  eta: 0:04:42  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [520/689]  eta: 0:04:26  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [530/689]  eta: 0:04:10  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [540/689]  eta: 0:03:55  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [550/689]  eta: 0:03:39  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [560/689]  eta: 0:03:23  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [570/689]  eta: 0:03:07  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [580/689]  eta: 0:02:52  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [590/689]  eta: 0:02:36  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [600/689]  eta: 0:02:20  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [610/689]  eta: 0:02:04  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [620/689]  eta: 0:01:48  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [630/689]  eta: 0:01:33  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [640/689]  eta: 0:01:17  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [650/689]  eta: 0:01:01  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [660/689]  eta: 0:00:45  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [670/689]  eta: 0:00:29  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [680/689]  eta: 0:00:14  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Train: [epoch:10]  [688/689]  eta: 0:00:01  lr: 0.000090  loss: 0.0000 (0.0000)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:10] Total time: 0:18:07 (1.5788 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.0000 (0.0000)\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:10]  [ 0/14]  eta: 0:00:14  loss: 0.0000 (0.0000)  time: 1.0111  data: 0.3807  max mem: 39763\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Still L2 Loss...!\n",
      "Valid: [epoch:10]  [13/14]  eta: 0:00:00  loss: 0.0000 (0.0000)  time: 0.1142  data: 0.0272  max mem: 39763\n",
      "Valid: [epoch:10] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_10_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.000%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:11]  [  0/689]  eta: 0:11:35  lr: 0.000100  loss: 0.0476 (0.0476)  time: 1.0098  data: 0.5360  max mem: 39763\n",
      "Train: [epoch:11]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.1994 (0.2594)  time: 1.5253  data: 0.0488  max mem: 39763\n",
      "Train: [epoch:11]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.1962 (0.2228)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.1456 (0.1914)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.1118 (0.1704)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0968 (0.1550)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0863 (0.1453)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0914 (0.1379)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0793 (0.1302)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0715 (0.1247)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0784 (0.1210)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0740 (0.1169)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0694 (0.1126)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0674 (0.1097)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0802 (0.1091)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0920 (0.1084)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0900 (0.1071)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0686 (0.1049)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0579 (0.1027)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0683 (0.1015)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [200/689]  eta: 0:12:51  lr: 0.000100  loss: 0.0683 (0.1000)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0576 (0.0980)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0556 (0.0971)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0556 (0.0956)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0576 (0.0941)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0602 (0.0929)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0545 (0.0916)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0535 (0.0903)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0560 (0.0894)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0520 (0.0882)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0590 (0.0879)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0688 (0.0874)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0618 (0.0867)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0764 (0.0868)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0727 (0.0863)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0725 (0.0864)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0855 (0.0869)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0684 (0.0863)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0538 (0.0853)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0499 (0.0845)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0479 (0.0838)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0567 (0.0838)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0565 (0.0830)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0488 (0.0827)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0613 (0.0822)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0613 (0.0819)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0752 (0.0820)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0587 (0.0816)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0524 (0.0810)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0530 (0.0805)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0512 (0.0800)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0554 (0.0796)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0570 (0.0793)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0623 (0.0789)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0490 (0.0784)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0490 (0.0780)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0554 (0.0778)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0577 (0.0774)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0454 (0.0769)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0476 (0.0766)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0449 (0.0760)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0449 (0.0756)  time: 1.5798  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0434 (0.0751)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0394 (0.0747)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0467 (0.0744)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0552 (0.0744)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0559 (0.0743)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0628 (0.0743)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0677 (0.0742)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0552 (0.0739)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:11] Total time: 0:18:07 (1.5789 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0552 (0.0739)\n",
      "Valid: [epoch:11]  [ 0/14]  eta: 0:00:14  loss: 0.0384 (0.0384)  time: 1.0343  data: 0.3395  max mem: 39763\n",
      "Valid: [epoch:11]  [13/14]  eta: 0:00:00  loss: 0.0389 (0.0422)  time: 0.1159  data: 0.0243  max mem: 39763\n",
      "Valid: [epoch:11] Total time: 0:00:01 (0.1249 s / it)\n",
      "Averaged stats: loss: 0.0389 (0.0422)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_11_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.042%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:12]  [  0/689]  eta: 0:11:30  lr: 0.000100  loss: 0.0538 (0.0538)  time: 1.0015  data: 0.5211  max mem: 39763\n",
      "Train: [epoch:12]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0567 (0.0687)  time: 1.5262  data: 0.0475  max mem: 39763\n",
      "Train: [epoch:12]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0668 (0.0757)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0668 (0.0743)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0504 (0.0688)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0478 (0.0672)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0494 (0.0657)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0510 (0.0640)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0531 (0.0631)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0487 (0.0611)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0449 (0.0599)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0471 (0.0596)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0450 (0.0590)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0471 (0.0592)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0521 (0.0590)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0592 (0.0597)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0572 (0.0595)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0544 (0.0595)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0617 (0.0600)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0577 (0.0596)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0501 (0.0592)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0476 (0.0588)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0452 (0.0585)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0452 (0.0585)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0425 (0.0579)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0411 (0.0575)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0476 (0.0576)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0571 (0.0579)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0571 (0.0578)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0485 (0.0575)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0460 (0.0576)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0408 (0.0572)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0403 (0.0570)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0503 (0.0572)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0449 (0.0568)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0421 (0.0565)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0437 (0.0561)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0446 (0.0561)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0465 (0.0560)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0520 (0.0559)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0529 (0.0562)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0673 (0.0565)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0488 (0.0564)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0474 (0.0563)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0498 (0.0562)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0498 (0.0561)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0495 (0.0561)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0507 (0.0562)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0545 (0.0567)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0589 (0.0569)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0605 (0.0570)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0526 (0.0569)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0466 (0.0567)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0429 (0.0565)  time: 1.5793  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0429 (0.0564)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0420 (0.0562)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0490 (0.0561)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0539 (0.0562)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0524 (0.0561)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0466 (0.0560)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0485 (0.0559)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0487 (0.0558)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0476 (0.0557)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0461 (0.0557)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0499 (0.0556)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0497 (0.0555)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0490 (0.0556)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0553 (0.0558)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0500 (0.0558)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0474 (0.0558)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:12] Total time: 0:18:07 (1.5789 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0474 (0.0558)\n",
      "Valid: [epoch:12]  [ 0/14]  eta: 0:00:14  loss: 0.0309 (0.0309)  time: 1.0098  data: 0.3736  max mem: 39763\n",
      "Valid: [epoch:12]  [13/14]  eta: 0:00:00  loss: 0.0309 (0.0311)  time: 0.1141  data: 0.0267  max mem: 39763\n",
      "Valid: [epoch:12] Total time: 0:00:01 (0.1216 s / it)\n",
      "Averaged stats: loss: 0.0309 (0.0311)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_12_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.031%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:13]  [  0/689]  eta: 0:11:45  lr: 0.000100  loss: 0.0385 (0.0385)  time: 1.0246  data: 0.5471  max mem: 39763\n",
      "Train: [epoch:13]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0437 (0.0442)  time: 1.5275  data: 0.0498  max mem: 39763\n",
      "Train: [epoch:13]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0463 (0.0493)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0531 (0.0519)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0477 (0.0513)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0477 (0.0526)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0557 (0.0551)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0647 (0.0569)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0576 (0.0571)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0587 (0.0586)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0603 (0.0589)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0542 (0.0587)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0507 (0.0583)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0486 (0.0586)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0486 (0.0582)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0523 (0.0586)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0531 (0.0582)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0488 (0.0575)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0428 (0.0570)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0472 (0.0569)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0497 (0.0570)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0591 (0.0575)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0591 (0.0574)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0487 (0.0571)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0506 (0.0570)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0565 (0.0574)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0648 (0.0580)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0523 (0.0576)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0492 (0.0574)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0422 (0.0569)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0434 (0.0569)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0416 (0.0564)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0400 (0.0560)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0399 (0.0557)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0412 (0.0555)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0480 (0.0556)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0481 (0.0555)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0489 (0.0554)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0491 (0.0553)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0501 (0.0555)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0451 (0.0552)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0423 (0.0549)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0396 (0.0547)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0421 (0.0545)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [440/689]  eta: 0:06:33  lr: 0.000100  loss: 0.0454 (0.0546)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0447 (0.0543)  time: 1.5805  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0428 (0.0542)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0457 (0.0542)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0409 (0.0539)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0362 (0.0536)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0405 (0.0535)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0447 (0.0533)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0400 (0.0530)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [530/689]  eta: 0:04:11  lr: 0.000100  loss: 0.0361 (0.0527)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0361 (0.0526)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0397 (0.0526)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0481 (0.0526)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0426 (0.0524)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0410 (0.0525)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0572 (0.0528)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0584 (0.0529)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0536 (0.0530)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0519 (0.0530)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0519 (0.0530)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0443 (0.0529)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0452 (0.0530)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0444 (0.0529)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [670/689]  eta: 0:00:30  lr: 0.000100  loss: 0.0409 (0.0527)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0426 (0.0527)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0506 (0.0530)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:13] Total time: 0:18:08 (1.5793 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0506 (0.0530)\n",
      "Valid: [epoch:13]  [ 0/14]  eta: 0:00:14  loss: 0.0469 (0.0469)  time: 1.0279  data: 0.3742  max mem: 39763\n",
      "Valid: [epoch:13]  [13/14]  eta: 0:00:00  loss: 0.0484 (0.0539)  time: 0.1155  data: 0.0268  max mem: 39763\n",
      "Valid: [epoch:13] Total time: 0:00:01 (0.1250 s / it)\n",
      "Averaged stats: loss: 0.0484 (0.0539)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_13_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.054%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:14]  [  0/689]  eta: 0:13:45  lr: 0.000100  loss: 0.0592 (0.0592)  time: 1.1975  data: 0.7176  max mem: 39763\n",
      "Train: [epoch:14]  [ 10/689]  eta: 0:17:28  lr: 0.000100  loss: 0.0635 (0.0643)  time: 1.5440  data: 0.0653  max mem: 39763\n",
      "Train: [epoch:14]  [ 20/689]  eta: 0:17:24  lr: 0.000100  loss: 0.0662 (0.0653)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [ 30/689]  eta: 0:17:12  lr: 0.000100  loss: 0.0603 (0.0629)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [ 40/689]  eta: 0:16:58  lr: 0.000100  loss: 0.0570 (0.0613)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [ 50/689]  eta: 0:16:44  lr: 0.000100  loss: 0.0529 (0.0585)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [ 60/689]  eta: 0:16:29  lr: 0.000100  loss: 0.0428 (0.0566)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [ 70/689]  eta: 0:16:14  lr: 0.000100  loss: 0.0410 (0.0546)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [ 80/689]  eta: 0:15:58  lr: 0.000100  loss: 0.0416 (0.0558)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [ 90/689]  eta: 0:15:43  lr: 0.000100  loss: 0.0459 (0.0550)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [100/689]  eta: 0:15:27  lr: 0.000100  loss: 0.0410 (0.0542)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [110/689]  eta: 0:15:12  lr: 0.000100  loss: 0.0400 (0.0531)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0401 (0.0523)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [130/689]  eta: 0:14:41  lr: 0.000100  loss: 0.0424 (0.0527)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [140/689]  eta: 0:14:25  lr: 0.000100  loss: 0.0537 (0.0531)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0639 (0.0541)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [160/689]  eta: 0:13:54  lr: 0.000100  loss: 0.0510 (0.0536)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [170/689]  eta: 0:13:38  lr: 0.000100  loss: 0.0492 (0.0543)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0561 (0.0543)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [190/689]  eta: 0:13:07  lr: 0.000100  loss: 0.0465 (0.0541)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [200/689]  eta: 0:12:51  lr: 0.000100  loss: 0.0498 (0.0543)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0557 (0.0545)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0590 (0.0549)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [230/689]  eta: 0:12:04  lr: 0.000100  loss: 0.0618 (0.0553)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0618 (0.0556)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0514 (0.0552)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0438 (0.0548)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [270/689]  eta: 0:11:01  lr: 0.000100  loss: 0.0435 (0.0546)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0403 (0.0540)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0392 (0.0539)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0474 (0.0541)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [310/689]  eta: 0:09:58  lr: 0.000100  loss: 0.0526 (0.0540)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0495 (0.0539)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0495 (0.0538)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0415 (0.0534)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [350/689]  eta: 0:08:55  lr: 0.000100  loss: 0.0441 (0.0534)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0508 (0.0534)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0508 (0.0534)  time: 1.5802  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0422 (0.0531)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0411 (0.0530)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0427 (0.0528)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0412 (0.0525)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0408 (0.0524)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0409 (0.0522)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [440/689]  eta: 0:06:33  lr: 0.000100  loss: 0.0522 (0.0524)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0542 (0.0524)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0514 (0.0525)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0453 (0.0524)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0404 (0.0522)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0394 (0.0519)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0379 (0.0518)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0445 (0.0518)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0445 (0.0517)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [530/689]  eta: 0:04:11  lr: 0.000100  loss: 0.0479 (0.0517)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0475 (0.0516)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0414 (0.0514)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0374 (0.0512)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0386 (0.0510)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0390 (0.0510)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0404 (0.0509)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0416 (0.0507)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0399 (0.0505)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0408 (0.0505)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0440 (0.0505)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0422 (0.0504)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0430 (0.0503)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0430 (0.0502)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [670/689]  eta: 0:00:30  lr: 0.000100  loss: 0.0424 (0.0502)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0418 (0.0501)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0451 (0.0501)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:14] Total time: 0:18:08 (1.5793 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0451 (0.0501)\n",
      "Valid: [epoch:14]  [ 0/14]  eta: 0:00:14  loss: 0.0469 (0.0469)  time: 1.0191  data: 0.3730  max mem: 39763\n",
      "Valid: [epoch:14]  [13/14]  eta: 0:00:00  loss: 0.0379 (0.0395)  time: 0.1148  data: 0.0267  max mem: 39763\n",
      "Valid: [epoch:14] Total time: 0:00:01 (0.1243 s / it)\n",
      "Averaged stats: loss: 0.0379 (0.0395)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_14_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.040%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:15]  [  0/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0486 (0.0486)  time: 1.0288  data: 0.5512  max mem: 39763\n",
      "Train: [epoch:15]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0486 (0.0483)  time: 1.5271  data: 0.0502  max mem: 39763\n",
      "Train: [epoch:15]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0433 (0.0487)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0412 (0.0473)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0429 (0.0470)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0439 (0.0475)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0512 (0.0485)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0549 (0.0503)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0514 (0.0499)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0452 (0.0497)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0429 (0.0492)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0418 (0.0495)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0487 (0.0493)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0443 (0.0492)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0434 (0.0489)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0432 (0.0484)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0415 (0.0483)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0506 (0.0492)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0615 (0.0499)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0615 (0.0505)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0548 (0.0505)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0488 (0.0507)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0491 (0.0507)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0497 (0.0507)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0522 (0.0510)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0486 (0.0508)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0422 (0.0505)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0425 (0.0506)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0459 (0.0506)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0448 (0.0504)  time: 1.5801  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0434 (0.0509)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0421 (0.0507)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0417 (0.0506)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0456 (0.0505)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0494 (0.0506)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0493 (0.0505)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0443 (0.0505)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0468 (0.0505)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0445 (0.0503)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0407 (0.0501)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0442 (0.0501)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0513 (0.0503)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0773 (0.0514)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0800 (0.0520)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0737 (0.0523)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0542 (0.0522)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0476 (0.0524)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0518 (0.0527)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0719 (0.0533)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0503 (0.0532)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0452 (0.0530)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0465 (0.0533)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0483 (0.0533)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0457 (0.0532)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0471 (0.0532)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0428 (0.0530)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0416 (0.0528)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0417 (0.0527)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0430 (0.0526)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0456 (0.0526)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0483 (0.0525)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0410 (0.0523)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0409 (0.0522)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0423 (0.0521)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0453 (0.0520)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0500 (0.0521)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0492 (0.0520)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0447 (0.0519)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0408 (0.0518)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0408 (0.0517)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:15] Total time: 0:18:07 (1.5790 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0408 (0.0517)\n",
      "Valid: [epoch:15]  [ 0/14]  eta: 0:00:14  loss: 0.0301 (0.0301)  time: 1.0126  data: 0.3669  max mem: 39763\n",
      "Valid: [epoch:15]  [13/14]  eta: 0:00:00  loss: 0.0310 (0.0314)  time: 0.1143  data: 0.0263  max mem: 39763\n",
      "Valid: [epoch:15] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.0310 (0.0314)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_15_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.031%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:16]  [  0/689]  eta: 0:11:22  lr: 0.000100  loss: 0.0447 (0.0447)  time: 0.9904  data: 0.5136  max mem: 39763\n",
      "Train: [epoch:16]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0471 (0.0507)  time: 1.5265  data: 0.0468  max mem: 39763\n",
      "Train: [epoch:16]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0471 (0.0498)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0460 (0.0514)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0460 (0.0501)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0434 (0.0490)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0456 (0.0520)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0586 (0.0541)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0605 (0.0554)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0633 (0.0558)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0509 (0.0554)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0467 (0.0548)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0457 (0.0550)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0507 (0.0552)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0618 (0.0560)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0601 (0.0564)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0524 (0.0560)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [170/689]  eta: 0:13:38  lr: 0.000100  loss: 0.0448 (0.0555)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0446 (0.0550)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0449 (0.0545)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [200/689]  eta: 0:12:51  lr: 0.000100  loss: 0.0452 (0.0540)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0449 (0.0537)  time: 1.5799  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0473 (0.0536)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0502 (0.0535)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0449 (0.0531)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0426 (0.0531)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0478 (0.0529)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [270/689]  eta: 0:11:01  lr: 0.000100  loss: 0.0485 (0.0530)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0563 (0.0532)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0531 (0.0530)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0485 (0.0532)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [310/689]  eta: 0:09:58  lr: 0.000100  loss: 0.0481 (0.0529)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0419 (0.0525)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0419 (0.0523)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0445 (0.0522)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0481 (0.0521)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0489 (0.0522)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0440 (0.0519)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0411 (0.0518)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0437 (0.0516)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0437 (0.0514)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0411 (0.0512)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0472 (0.0513)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0514 (0.0512)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [440/689]  eta: 0:06:33  lr: 0.000100  loss: 0.0449 (0.0511)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0454 (0.0513)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0516 (0.0512)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0506 (0.0513)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0497 (0.0511)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0404 (0.0509)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0423 (0.0508)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0394 (0.0507)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0394 (0.0505)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0398 (0.0504)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0398 (0.0503)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0385 (0.0501)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0377 (0.0499)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0390 (0.0499)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0433 (0.0499)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0413 (0.0498)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0391 (0.0497)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0425 (0.0497)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0407 (0.0496)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0379 (0.0494)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0391 (0.0494)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0422 (0.0493)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0448 (0.0494)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0499 (0.0494)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0514 (0.0495)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0514 (0.0496)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:16] Total time: 0:18:07 (1.5791 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0514 (0.0496)\n",
      "Valid: [epoch:16]  [ 0/14]  eta: 0:00:14  loss: 0.0373 (0.0373)  time: 1.0159  data: 0.3608  max mem: 39763\n",
      "Valid: [epoch:16]  [13/14]  eta: 0:00:00  loss: 0.0399 (0.0399)  time: 0.1146  data: 0.0258  max mem: 39763\n",
      "Valid: [epoch:16] Total time: 0:00:01 (0.1234 s / it)\n",
      "Averaged stats: loss: 0.0399 (0.0399)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_16_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.040%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:17]  [  0/689]  eta: 0:11:44  lr: 0.000100  loss: 0.0614 (0.0614)  time: 1.0230  data: 0.5459  max mem: 39763\n",
      "Train: [epoch:17]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0479 (0.0477)  time: 1.5268  data: 0.0497  max mem: 39763\n",
      "Train: [epoch:17]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0456 (0.0468)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0456 (0.0474)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0426 (0.0460)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0390 (0.0448)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0404 (0.0445)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0407 (0.0445)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0437 (0.0453)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0468 (0.0461)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0455 (0.0462)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0451 (0.0464)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0471 (0.0470)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0471 (0.0473)  time: 1.5797  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0419 (0.0470)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0438 (0.0471)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0438 (0.0469)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0425 (0.0470)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0502 (0.0474)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0504 (0.0475)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0474 (0.0475)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0453 (0.0474)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0455 (0.0475)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0479 (0.0476)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0477 (0.0476)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0502 (0.0484)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0552 (0.0485)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0521 (0.0488)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0501 (0.0488)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0509 (0.0491)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0577 (0.0493)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0561 (0.0494)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0529 (0.0496)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0520 (0.0497)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0477 (0.0499)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0588 (0.0504)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0666 (0.0508)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0507 (0.0507)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0464 (0.0506)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0419 (0.0504)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0399 (0.0502)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0427 (0.0500)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0430 (0.0500)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0491 (0.0501)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0485 (0.0501)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0448 (0.0500)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0403 (0.0498)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0410 (0.0496)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0379 (0.0494)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0388 (0.0492)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0382 (0.0490)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0382 (0.0490)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0394 (0.0488)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0368 (0.0486)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0351 (0.0484)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0381 (0.0483)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0389 (0.0481)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0394 (0.0480)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0400 (0.0480)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0405 (0.0481)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0408 (0.0479)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0381 (0.0478)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0383 (0.0477)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0383 (0.0475)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0384 (0.0474)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0404 (0.0473)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0406 (0.0473)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0387 (0.0471)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0387 (0.0470)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0404 (0.0470)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:17] Total time: 0:18:07 (1.5788 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0404 (0.0470)\n",
      "Valid: [epoch:17]  [ 0/14]  eta: 0:00:14  loss: 0.0353 (0.0353)  time: 1.0244  data: 0.3755  max mem: 39763\n",
      "Valid: [epoch:17]  [13/14]  eta: 0:00:00  loss: 0.0325 (0.0332)  time: 0.1151  data: 0.0269  max mem: 39763\n",
      "Valid: [epoch:17] Total time: 0:00:01 (0.1240 s / it)\n",
      "Averaged stats: loss: 0.0325 (0.0332)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_17_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.033%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:18]  [  0/689]  eta: 0:11:28  lr: 0.000100  loss: 0.0491 (0.0491)  time: 0.9998  data: 0.5189  max mem: 39763\n",
      "Train: [epoch:18]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0506 (0.0528)  time: 1.5248  data: 0.0472  max mem: 39763\n",
      "Train: [epoch:18]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0432 (0.0481)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0420 (0.0470)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0438 (0.0466)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0421 (0.0463)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0434 (0.0467)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0453 (0.0466)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0445 (0.0463)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0457 (0.0465)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0462 (0.0468)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0493 (0.0472)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0480 (0.0479)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0495 (0.0482)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0499 (0.0483)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0490 (0.0485)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0493 (0.0486)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0494 (0.0492)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0596 (0.0501)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0556 (0.0503)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0476 (0.0502)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0525 (0.0517)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0581 (0.0519)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0508 (0.0518)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0471 (0.0516)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0418 (0.0513)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0420 (0.0511)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0509 (0.0514)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0541 (0.0514)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0441 (0.0512)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0431 (0.0509)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0446 (0.0509)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0541 (0.0512)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0541 (0.0512)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0502 (0.0513)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0514 (0.0515)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0632 (0.0521)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0659 (0.0524)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0456 (0.0521)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0456 (0.0521)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0483 (0.0521)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0548 (0.0523)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0557 (0.0523)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0531 (0.0523)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0531 (0.0527)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0641 (0.0528)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0500 (0.0528)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0466 (0.0526)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0412 (0.0525)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0398 (0.0523)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0397 (0.0521)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0411 (0.0519)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0481 (0.0520)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0516 (0.0521)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0564 (0.0523)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0551 (0.0522)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0458 (0.0521)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0506 (0.0523)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0473 (0.0522)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0453 (0.0522)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0510 (0.0523)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0510 (0.0523)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0469 (0.0522)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0421 (0.0521)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0421 (0.0520)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0415 (0.0518)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0397 (0.0519)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0437 (0.0519)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0452 (0.0518)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0436 (0.0517)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:18] Total time: 0:18:07 (1.5785 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0436 (0.0517)\n",
      "Valid: [epoch:18]  [ 0/14]  eta: 0:00:14  loss: 0.0379 (0.0379)  time: 1.0162  data: 0.3712  max mem: 39763\n",
      "Valid: [epoch:18]  [13/14]  eta: 0:00:00  loss: 0.0367 (0.0374)  time: 0.1145  data: 0.0266  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:18] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.0367 (0.0374)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_18_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.037%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:19]  [  0/689]  eta: 0:11:30  lr: 0.000100  loss: 0.0526 (0.0526)  time: 1.0023  data: 0.5262  max mem: 39763\n",
      "Train: [epoch:19]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0526 (0.0549)  time: 1.5243  data: 0.0479  max mem: 39763\n",
      "Train: [epoch:19]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0441 (0.0527)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0441 (0.0508)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0433 (0.0511)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0442 (0.0505)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0440 (0.0491)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0419 (0.0482)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0425 (0.0477)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0453 (0.0480)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0450 (0.0477)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0450 (0.0477)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0454 (0.0477)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0468 (0.0482)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0539 (0.0493)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0575 (0.0499)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0475 (0.0496)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0433 (0.0493)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0428 (0.0493)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0420 (0.0491)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0446 (0.0493)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0446 (0.0490)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0465 (0.0493)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0498 (0.0494)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0506 (0.0498)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0515 (0.0499)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0501 (0.0504)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0543 (0.0506)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0513 (0.0505)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0451 (0.0504)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0435 (0.0502)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0453 (0.0504)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0468 (0.0503)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0437 (0.0502)  time: 1.5798  data: 0.0002  max mem: 39763\n",
      "Train: [epoch:19]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0408 (0.0500)  time: 1.5798  data: 0.0002  max mem: 39763\n",
      "Train: [epoch:19]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0440 (0.0499)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0475 (0.0499)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0436 (0.0497)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0415 (0.0495)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0411 (0.0494)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0402 (0.0492)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0393 (0.0490)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0391 (0.0488)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0417 (0.0488)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0449 (0.0487)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0456 (0.0487)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0509 (0.0489)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0509 (0.0489)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0496 (0.0490)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0488 (0.0490)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0479 (0.0490)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0493 (0.0493)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0617 (0.0494)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0636 (0.0499)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0545 (0.0499)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0554 (0.0501)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0555 (0.0501)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0472 (0.0500)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0410 (0.0499)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0419 (0.0499)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0442 (0.0501)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0565 (0.0502)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0435 (0.0501)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0443 (0.0503)  time: 1.5794  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0506 (0.0503)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0509 (0.0504)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0444 (0.0503)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0429 (0.0504)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0498 (0.0504)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0498 (0.0503)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:19] Total time: 0:18:07 (1.5785 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0498 (0.0503)\n",
      "Valid: [epoch:19]  [ 0/14]  eta: 0:00:14  loss: 0.0459 (0.0459)  time: 1.0156  data: 0.3901  max mem: 39763\n",
      "Valid: [epoch:19]  [13/14]  eta: 0:00:00  loss: 0.0467 (0.0466)  time: 0.1146  data: 0.0279  max mem: 39763\n",
      "Valid: [epoch:19] Total time: 0:00:01 (0.1241 s / it)\n",
      "Averaged stats: loss: 0.0467 (0.0466)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_19_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.047%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:20]  [  0/689]  eta: 0:11:27  lr: 0.000100  loss: 0.0627 (0.0627)  time: 0.9974  data: 0.5173  max mem: 39763\n",
      "Train: [epoch:20]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0556 (0.0547)  time: 1.5248  data: 0.0471  max mem: 39763\n",
      "Train: [epoch:20]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0524 (0.0554)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0529 (0.0559)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0478 (0.0533)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0436 (0.0520)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0426 (0.0502)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0412 (0.0491)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0451 (0.0491)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0487 (0.0494)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0469 (0.0492)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0421 (0.0492)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0469 (0.0490)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0452 (0.0486)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0485 (0.0490)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0512 (0.0492)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0432 (0.0488)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0417 (0.0484)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0439 (0.0487)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0464 (0.0485)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0446 (0.0483)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0426 (0.0481)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0489 (0.0489)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0669 (0.0497)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0562 (0.0500)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0488 (0.0501)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0446 (0.0500)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0458 (0.0498)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0458 (0.0497)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0413 (0.0495)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0468 (0.0496)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0489 (0.0496)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0429 (0.0496)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0423 (0.0495)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0434 (0.0495)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0434 (0.0493)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0418 (0.0495)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0476 (0.0497)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0474 (0.0497)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0488 (0.0500)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0568 (0.0502)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0569 (0.0503)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0482 (0.0503)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0465 (0.0503)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0485 (0.0503)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0454 (0.0502)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0453 (0.0503)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0449 (0.0502)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0436 (0.0503)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0436 (0.0502)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0417 (0.0501)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0414 (0.0500)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0429 (0.0499)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0481 (0.0499)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0466 (0.0499)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0448 (0.0498)  time: 1.5798  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0469 (0.0498)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0450 (0.0498)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0421 (0.0496)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0428 (0.0496)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0421 (0.0495)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0427 (0.0495)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0441 (0.0494)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0434 (0.0494)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0418 (0.0493)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0415 (0.0492)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0417 (0.0491)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0465 (0.0491)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0518 (0.0492)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0515 (0.0492)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:20] Total time: 0:18:07 (1.5785 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0515 (0.0492)\n",
      "Valid: [epoch:20]  [ 0/14]  eta: 0:00:14  loss: 0.0293 (0.0293)  time: 1.0116  data: 0.3724  max mem: 39763\n",
      "Valid: [epoch:20]  [13/14]  eta: 0:00:00  loss: 0.0271 (0.0279)  time: 0.1142  data: 0.0267  max mem: 39763\n",
      "Valid: [epoch:20] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.0271 (0.0279)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_20_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.028%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:21]  [  0/689]  eta: 0:11:52  lr: 0.000100  loss: 0.0565 (0.0565)  time: 1.0345  data: 0.5559  max mem: 39763\n",
      "Train: [epoch:21]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0565 (0.0566)  time: 1.5278  data: 0.0506  max mem: 39763\n",
      "Train: [epoch:21]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0531 (0.0546)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0525 (0.0541)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [ 40/689]  eta: 0:16:56  lr: 0.000100  loss: 0.0516 (0.0542)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [ 50/689]  eta: 0:16:42  lr: 0.000100  loss: 0.0527 (0.0540)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0474 (0.0527)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0413 (0.0512)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0413 (0.0515)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0487 (0.0513)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [100/689]  eta: 0:15:27  lr: 0.000100  loss: 0.0461 (0.0510)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0479 (0.0511)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0518 (0.0520)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0609 (0.0533)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [140/689]  eta: 0:14:25  lr: 0.000100  loss: 0.0506 (0.0530)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0470 (0.0531)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0550 (0.0538)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [170/689]  eta: 0:13:38  lr: 0.000100  loss: 0.0507 (0.0535)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0477 (0.0534)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0489 (0.0533)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [200/689]  eta: 0:12:51  lr: 0.000100  loss: 0.0451 (0.0528)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0421 (0.0522)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0427 (0.0520)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [230/689]  eta: 0:12:04  lr: 0.000100  loss: 0.0514 (0.0522)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0556 (0.0524)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0509 (0.0526)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0444 (0.0522)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [270/689]  eta: 0:11:01  lr: 0.000100  loss: 0.0444 (0.0524)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0562 (0.0527)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0525 (0.0526)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0460 (0.0523)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [310/689]  eta: 0:09:58  lr: 0.000100  loss: 0.0440 (0.0521)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0450 (0.0520)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0440 (0.0518)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0440 (0.0516)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [350/689]  eta: 0:08:55  lr: 0.000100  loss: 0.0466 (0.0515)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0483 (0.0515)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0523 (0.0517)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0552 (0.0517)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [390/689]  eta: 0:07:52  lr: 0.000100  loss: 0.0467 (0.0516)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0471 (0.0518)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0519 (0.0519)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0463 (0.0518)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0459 (0.0517)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [440/689]  eta: 0:06:33  lr: 0.000100  loss: 0.0493 (0.0517)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0511 (0.0518)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0539 (0.0521)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0505 (0.0521)  time: 1.5798  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [480/689]  eta: 0:05:30  lr: 0.000100  loss: 0.0439 (0.0519)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0438 (0.0519)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0521 (0.0520)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0523 (0.0520)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0485 (0.0520)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [530/689]  eta: 0:04:11  lr: 0.000100  loss: 0.0499 (0.0521)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0450 (0.0519)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0414 (0.0518)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0416 (0.0517)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0425 (0.0516)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0435 (0.0515)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0428 (0.0513)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0429 (0.0515)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0595 (0.0516)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0499 (0.0517)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0570 (0.0520)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0584 (0.0523)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0476 (0.0523)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0469 (0.0522)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [670/689]  eta: 0:00:30  lr: 0.000100  loss: 0.0469 (0.0522)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0542 (0.0523)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0509 (0.0523)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:21] Total time: 0:18:08 (1.5793 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0509 (0.0523)\n",
      "Valid: [epoch:21]  [ 0/14]  eta: 0:00:14  loss: 0.0463 (0.0463)  time: 1.0147  data: 0.3724  max mem: 39763\n",
      "Valid: [epoch:21]  [13/14]  eta: 0:00:00  loss: 0.0453 (0.0457)  time: 0.1145  data: 0.0267  max mem: 39763\n",
      "Valid: [epoch:21] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.0453 (0.0457)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_21_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.046%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:22]  [  0/689]  eta: 0:11:33  lr: 0.000100  loss: 0.0655 (0.0655)  time: 1.0068  data: 0.5261  max mem: 39763\n",
      "Train: [epoch:22]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0509 (0.0531)  time: 1.5253  data: 0.0479  max mem: 39763\n",
      "Train: [epoch:22]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0498 (0.0554)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0495 (0.0535)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0460 (0.0532)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0460 (0.0538)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0464 (0.0528)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0440 (0.0521)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0473 (0.0526)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0468 (0.0518)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0462 (0.0514)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0462 (0.0516)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0482 (0.0515)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0438 (0.0508)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0439 (0.0506)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0455 (0.0502)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0441 (0.0499)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0441 (0.0498)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0490 (0.0499)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0487 (0.0499)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0474 (0.0499)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0451 (0.0499)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0430 (0.0495)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0415 (0.0492)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0431 (0.0491)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0421 (0.0488)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0386 (0.0484)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0401 (0.0483)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0432 (0.0484)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0470 (0.0484)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0476 (0.0484)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0429 (0.0482)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0408 (0.0482)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0434 (0.0481)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0453 (0.0480)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0479 (0.0482)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0504 (0.0482)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0478 (0.0482)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0469 (0.0483)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0476 (0.0483)  time: 1.5788  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:22]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0484 (0.0483)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0479 (0.0483)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0451 (0.0482)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0444 (0.0483)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0482 (0.0484)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0488 (0.0483)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0516 (0.0486)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0508 (0.0487)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0508 (0.0490)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0515 (0.0490)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0460 (0.0490)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0491 (0.0491)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0520 (0.0492)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0498 (0.0493)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0442 (0.0492)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0435 (0.0491)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0437 (0.0490)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0442 (0.0491)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0470 (0.0490)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0441 (0.0490)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0464 (0.0491)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0491 (0.0492)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0521 (0.0493)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0481 (0.0493)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0477 (0.0494)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0493 (0.0494)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0515 (0.0496)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0577 (0.0497)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0591 (0.0499)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0514 (0.0499)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:22] Total time: 0:18:07 (1.5784 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0514 (0.0499)\n",
      "Valid: [epoch:22]  [ 0/14]  eta: 0:00:14  loss: 0.0341 (0.0341)  time: 1.0162  data: 0.4324  max mem: 39763\n",
      "Valid: [epoch:22]  [13/14]  eta: 0:00:00  loss: 0.0367 (0.0369)  time: 0.1145  data: 0.0309  max mem: 39763\n",
      "Valid: [epoch:22] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.0367 (0.0369)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_22_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.037%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:23]  [  0/689]  eta: 0:11:51  lr: 0.000100  loss: 0.0449 (0.0449)  time: 1.0333  data: 0.5577  max mem: 39763\n",
      "Train: [epoch:23]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0428 (0.0437)  time: 1.5274  data: 0.0508  max mem: 39763\n",
      "Train: [epoch:23]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0410 (0.0442)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0448 (0.0490)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0459 (0.0486)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0450 (0.0494)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0450 (0.0486)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0440 (0.0482)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0430 (0.0479)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0419 (0.0475)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0437 (0.0474)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0470 (0.0479)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0517 (0.0479)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0474 (0.0486)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0513 (0.0492)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0542 (0.0498)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0494 (0.0497)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0470 (0.0499)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0475 (0.0499)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0466 (0.0498)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0481 (0.0503)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0564 (0.0508)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0510 (0.0509)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0451 (0.0506)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0458 (0.0505)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0483 (0.0505)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0499 (0.0506)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0540 (0.0509)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0559 (0.0515)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0520 (0.0517)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0488 (0.0515)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0469 (0.0515)  time: 1.5792  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:23]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0494 (0.0515)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0494 (0.0515)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0477 (0.0515)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0436 (0.0513)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0433 (0.0511)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0450 (0.0510)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0458 (0.0509)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0471 (0.0508)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0449 (0.0507)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0448 (0.0506)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0446 (0.0505)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0442 (0.0504)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0421 (0.0502)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0434 (0.0501)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0452 (0.0501)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0460 (0.0500)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0428 (0.0499)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0440 (0.0499)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0540 (0.0501)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0647 (0.0506)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0564 (0.0506)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0524 (0.0508)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0531 (0.0509)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0579 (0.0513)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0606 (0.0515)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0506 (0.0515)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0506 (0.0515)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0509 (0.0515)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0473 (0.0515)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0467 (0.0514)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0458 (0.0513)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0495 (0.0514)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0515 (0.0515)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0491 (0.0514)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0478 (0.0514)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0471 (0.0513)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0475 (0.0514)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0441 (0.0513)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:23] Total time: 0:18:07 (1.5783 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0441 (0.0513)\n",
      "Valid: [epoch:23]  [ 0/14]  eta: 0:00:14  loss: 0.0457 (0.0457)  time: 1.0040  data: 0.6112  max mem: 39763\n",
      "Valid: [epoch:23]  [13/14]  eta: 0:00:00  loss: 0.0404 (0.0417)  time: 0.1137  data: 0.0437  max mem: 39763\n",
      "Valid: [epoch:23] Total time: 0:00:01 (0.1228 s / it)\n",
      "Averaged stats: loss: 0.0404 (0.0417)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_23_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.042%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:24]  [  0/689]  eta: 0:11:57  lr: 0.000100  loss: 0.0410 (0.0410)  time: 1.0420  data: 0.5623  max mem: 39763\n",
      "Train: [epoch:24]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0466 (0.0469)  time: 1.5281  data: 0.0512  max mem: 39763\n",
      "Train: [epoch:24]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0450 (0.0462)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0445 (0.0467)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0454 (0.0463)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0455 (0.0471)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0489 (0.0478)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0510 (0.0484)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0510 (0.0486)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0505 (0.0488)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0515 (0.0513)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0505 (0.0510)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0436 (0.0504)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0446 (0.0502)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0554 (0.0525)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0627 (0.0533)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0559 (0.0536)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0520 (0.0537)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0492 (0.0537)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0471 (0.0534)  time: 1.5789  data: 0.0002  max mem: 39763\n",
      "Train: [epoch:24]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0471 (0.0537)  time: 1.5790  data: 0.0002  max mem: 39763\n",
      "Train: [epoch:24]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0565 (0.0547)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0575 (0.0549)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0522 (0.0547)  time: 1.5786  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:24]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0460 (0.0545)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0476 (0.0542)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0497 (0.0543)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0505 (0.0542)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0495 (0.0541)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0529 (0.0546)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0594 (0.0546)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0538 (0.0546)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0557 (0.0548)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0602 (0.0551)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0662 (0.0556)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0548 (0.0555)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0538 (0.0556)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0509 (0.0556)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0468 (0.0553)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0468 (0.0554)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0591 (0.0554)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0525 (0.0557)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0525 (0.0559)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0525 (0.0557)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0478 (0.0556)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0487 (0.0555)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0481 (0.0554)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0481 (0.0552)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0474 (0.0551)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0450 (0.0549)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0435 (0.0547)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0451 (0.0547)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0476 (0.0547)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0530 (0.0549)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0530 (0.0548)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0518 (0.0548)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0479 (0.0546)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0479 (0.0545)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0507 (0.0545)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0552 (0.0547)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0729 (0.0553)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0869 (0.0558)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0717 (0.0560)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0510 (0.0559)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0493 (0.0560)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0516 (0.0560)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0517 (0.0559)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0473 (0.0558)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0487 (0.0559)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0502 (0.0559)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:24] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0502 (0.0559)\n",
      "Valid: [epoch:24]  [ 0/14]  eta: 0:00:14  loss: 0.0350 (0.0350)  time: 1.0114  data: 0.3900  max mem: 39763\n",
      "Valid: [epoch:24]  [13/14]  eta: 0:00:00  loss: 0.0360 (0.0365)  time: 0.1142  data: 0.0279  max mem: 39763\n",
      "Valid: [epoch:24] Total time: 0:00:01 (0.1234 s / it)\n",
      "Averaged stats: loss: 0.0360 (0.0365)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_24_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.037%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:25]  [  0/689]  eta: 0:14:04  lr: 0.000100  loss: 0.0450 (0.0450)  time: 1.2253  data: 0.7472  max mem: 39763\n",
      "Train: [epoch:25]  [ 10/689]  eta: 0:17:28  lr: 0.000100  loss: 0.0474 (0.0498)  time: 1.5442  data: 0.0680  max mem: 39763\n",
      "Train: [epoch:25]  [ 20/689]  eta: 0:17:23  lr: 0.000100  loss: 0.0482 (0.0514)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [ 30/689]  eta: 0:17:11  lr: 0.000100  loss: 0.0508 (0.0527)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [ 40/689]  eta: 0:16:57  lr: 0.000100  loss: 0.0499 (0.0519)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [ 50/689]  eta: 0:16:43  lr: 0.000100  loss: 0.0494 (0.0523)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [ 60/689]  eta: 0:16:28  lr: 0.000100  loss: 0.0521 (0.0535)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [ 70/689]  eta: 0:16:13  lr: 0.000100  loss: 0.0498 (0.0531)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0480 (0.0529)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0480 (0.0530)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [100/689]  eta: 0:15:27  lr: 0.000100  loss: 0.0492 (0.0533)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0492 (0.0529)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0431 (0.0522)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0420 (0.0516)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0461 (0.0514)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0451 (0.0509)  time: 1.5779  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:25]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0451 (0.0510)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0484 (0.0509)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0475 (0.0508)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0475 (0.0506)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0469 (0.0505)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0474 (0.0506)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0495 (0.0506)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0497 (0.0508)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0533 (0.0512)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0538 (0.0514)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0543 (0.0518)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0498 (0.0517)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0491 (0.0518)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0505 (0.0519)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0500 (0.0519)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0524 (0.0520)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0523 (0.0519)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0461 (0.0518)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0497 (0.0519)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0569 (0.0522)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0508 (0.0522)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0478 (0.0522)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0561 (0.0536)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0763 (0.0538)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0618 (0.0542)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0765 (0.0550)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0829 (0.0558)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0651 (0.0559)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0568 (0.0561)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0538 (0.0561)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0538 (0.0560)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0538 (0.0560)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0532 (0.0559)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0523 (0.0560)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0507 (0.0560)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0487 (0.0559)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0469 (0.0558)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0462 (0.0556)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0482 (0.0555)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0503 (0.0555)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0523 (0.0555)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0527 (0.0554)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0520 (0.0554)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0493 (0.0553)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0496 (0.0553)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0485 (0.0551)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0614 (0.0555)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0610 (0.0555)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0539 (0.0555)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0534 (0.0555)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0557 (0.0557)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0667 (0.0561)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0692 (0.0564)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0659 (0.0564)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:25] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0659 (0.0564)\n",
      "Valid: [epoch:25]  [ 0/14]  eta: 0:00:14  loss: 0.0590 (0.0590)  time: 1.0206  data: 0.3985  max mem: 39763\n",
      "Valid: [epoch:25]  [13/14]  eta: 0:00:00  loss: 0.0514 (0.0541)  time: 0.1149  data: 0.0285  max mem: 39763\n",
      "Valid: [epoch:25] Total time: 0:00:01 (0.1240 s / it)\n",
      "Averaged stats: loss: 0.0514 (0.0541)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_25_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.054%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:26]  [  0/689]  eta: 0:11:37  lr: 0.000100  loss: 0.0696 (0.0696)  time: 1.0124  data: 0.5326  max mem: 39763\n",
      "Train: [epoch:26]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0573 (0.0653)  time: 1.5266  data: 0.0485  max mem: 39763\n",
      "Train: [epoch:26]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0511 (0.0584)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0501 (0.0599)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0594 (0.0599)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0594 (0.0597)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0505 (0.0580)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0535 (0.0594)  time: 1.5790  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:26]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0578 (0.0589)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0510 (0.0581)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0522 (0.0577)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0522 (0.0575)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0493 (0.0567)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0450 (0.0557)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0450 (0.0551)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0450 (0.0544)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0452 (0.0540)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0467 (0.0539)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0468 (0.0534)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0428 (0.0529)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0436 (0.0527)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0504 (0.0531)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0529 (0.0530)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0493 (0.0531)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0539 (0.0532)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0517 (0.0531)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0486 (0.0530)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0468 (0.0528)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0447 (0.0526)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0491 (0.0526)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0503 (0.0526)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0453 (0.0524)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0465 (0.0524)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0505 (0.0523)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0468 (0.0522)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0439 (0.0519)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0437 (0.0520)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0449 (0.0519)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0449 (0.0518)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0482 (0.0517)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0506 (0.0519)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0511 (0.0522)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0520 (0.0523)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0572 (0.0526)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0621 (0.0531)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0690 (0.0534)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0605 (0.0535)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0518 (0.0534)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0469 (0.0533)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0469 (0.0534)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0549 (0.0534)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0604 (0.0538)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0656 (0.0539)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0522 (0.0539)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0522 (0.0541)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0516 (0.0540)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0520 (0.0541)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0623 (0.0544)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0678 (0.0546)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0546 (0.0546)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0766 (0.0556)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0910 (0.0561)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0823 (0.0564)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0553 (0.0564)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0481 (0.0563)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0470 (0.0562)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0512 (0.0562)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0522 (0.0562)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0543 (0.0561)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0482 (0.0560)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:26] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0482 (0.0560)\n",
      "Valid: [epoch:26]  [ 0/14]  eta: 0:00:14  loss: 0.0326 (0.0326)  time: 1.0107  data: 0.3910  max mem: 39763\n",
      "Valid: [epoch:26]  [13/14]  eta: 0:00:00  loss: 0.0345 (0.0350)  time: 0.1142  data: 0.0280  max mem: 39763\n",
      "Valid: [epoch:26] Total time: 0:00:01 (0.1233 s / it)\n",
      "Averaged stats: loss: 0.0345 (0.0350)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_26_input_n_20.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the network on the 14 valid images: 0.035%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:27]  [  0/689]  eta: 0:12:02  lr: 0.000100  loss: 0.0436 (0.0436)  time: 1.0491  data: 0.5716  max mem: 39763\n",
      "Train: [epoch:27]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0503 (0.0521)  time: 1.5279  data: 0.0520  max mem: 39763\n",
      "Train: [epoch:27]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0493 (0.0502)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0485 (0.0513)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0533 (0.0536)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0553 (0.0539)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0514 (0.0536)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0510 (0.0539)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0553 (0.0542)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0523 (0.0538)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0509 (0.0540)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0509 (0.0537)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0524 (0.0541)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0536 (0.0546)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0641 (0.0558)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0666 (0.0566)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0652 (0.0572)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0629 (0.0578)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0565 (0.0578)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0497 (0.0576)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0481 (0.0572)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0478 (0.0568)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0480 (0.0564)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0463 (0.0560)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0476 (0.0558)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0581 (0.0561)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0609 (0.0561)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0529 (0.0560)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0482 (0.0557)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0459 (0.0555)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0480 (0.0553)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0457 (0.0549)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0468 (0.0549)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0501 (0.0548)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0474 (0.0549)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0523 (0.0548)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0510 (0.0547)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0475 (0.0546)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0475 (0.0545)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0479 (0.0543)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0511 (0.0544)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0527 (0.0543)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0476 (0.0543)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0499 (0.0543)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0496 (0.0541)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0475 (0.0540)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0472 (0.0539)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0538 (0.0543)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0631 (0.0544)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0507 (0.0543)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0503 (0.0543)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0449 (0.0542)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0449 (0.0541)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0490 (0.0540)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0551 (0.0541)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0572 (0.0542)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0566 (0.0543)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0609 (0.0545)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0557 (0.0545)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0541 (0.0545)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0495 (0.0545)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0495 (0.0545)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0562 (0.0545)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0557 (0.0547)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0613 (0.0549)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0570 (0.0549)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:27]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0527 (0.0549)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0579 (0.0550)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0585 (0.0552)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0620 (0.0552)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:27] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0620 (0.0552)\n",
      "Valid: [epoch:27]  [ 0/14]  eta: 0:00:14  loss: 0.0383 (0.0383)  time: 1.0128  data: 0.3845  max mem: 39763\n",
      "Valid: [epoch:27]  [13/14]  eta: 0:00:00  loss: 0.0326 (0.0347)  time: 0.1143  data: 0.0275  max mem: 39763\n",
      "Valid: [epoch:27] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.0326 (0.0347)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_27_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.035%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:28]  [  0/689]  eta: 0:12:24  lr: 0.000100  loss: 0.0498 (0.0498)  time: 1.0811  data: 0.6014  max mem: 39763\n",
      "Train: [epoch:28]  [ 10/689]  eta: 0:17:20  lr: 0.000100  loss: 0.0556 (0.0580)  time: 1.5319  data: 0.0548  max mem: 39763\n",
      "Train: [epoch:28]  [ 20/689]  eta: 0:17:19  lr: 0.000100  loss: 0.0565 (0.0587)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [ 30/689]  eta: 0:17:09  lr: 0.000100  loss: 0.0557 (0.0577)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0497 (0.0564)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0511 (0.0579)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0484 (0.0560)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0461 (0.0547)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0471 (0.0542)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0497 (0.0555)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0510 (0.0552)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0498 (0.0547)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0507 (0.0555)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0578 (0.0559)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0520 (0.0555)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0477 (0.0553)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0484 (0.0550)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0508 (0.0551)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0525 (0.0553)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0550 (0.0554)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0577 (0.0557)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0526 (0.0558)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0508 (0.0556)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0508 (0.0553)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0505 (0.0551)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0488 (0.0549)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0499 (0.0547)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0500 (0.0551)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0547 (0.0551)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0557 (0.0554)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0555 (0.0553)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0518 (0.0553)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0509 (0.0554)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0546 (0.0554)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0513 (0.0552)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0479 (0.0552)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0508 (0.0551)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0513 (0.0551)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0513 (0.0550)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0505 (0.0551)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0486 (0.0549)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0465 (0.0547)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0577 (0.0552)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0589 (0.0552)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0515 (0.0551)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0477 (0.0549)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0467 (0.0549)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0503 (0.0548)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0503 (0.0548)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0497 (0.0547)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0526 (0.0548)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0500 (0.0547)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0474 (0.0546)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0495 (0.0545)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0511 (0.0545)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0504 (0.0544)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0576 (0.0548)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0642 (0.0549)  time: 1.5797  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:28]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0557 (0.0549)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0524 (0.0549)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0508 (0.0549)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0508 (0.0548)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0487 (0.0551)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0509 (0.0550)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0559 (0.0551)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0561 (0.0551)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0558 (0.0552)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0588 (0.0553)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0543 (0.0553)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0534 (0.0553)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:28] Total time: 0:18:07 (1.5789 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0534 (0.0553)\n",
      "Valid: [epoch:28]  [ 0/14]  eta: 0:00:14  loss: 0.0462 (0.0462)  time: 1.0152  data: 0.3818  max mem: 39763\n",
      "Valid: [epoch:28]  [13/14]  eta: 0:00:00  loss: 0.0425 (0.0448)  time: 0.1145  data: 0.0273  max mem: 39763\n",
      "Valid: [epoch:28] Total time: 0:00:01 (0.1234 s / it)\n",
      "Averaged stats: loss: 0.0425 (0.0448)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_28_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.045%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:29]  [  0/689]  eta: 0:11:27  lr: 0.000100  loss: 0.0553 (0.0553)  time: 0.9976  data: 0.5191  max mem: 39763\n",
      "Train: [epoch:29]  [ 10/689]  eta: 0:17:14  lr: 0.000100  loss: 0.0553 (0.0569)  time: 1.5237  data: 0.0473  max mem: 39763\n",
      "Train: [epoch:29]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0549 (0.0562)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0549 (0.0565)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0559 (0.0570)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0544 (0.0571)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0606 (0.0593)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0736 (0.0621)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0617 (0.0618)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0579 (0.0625)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0709 (0.0633)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0603 (0.0632)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0600 (0.0630)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0585 (0.0626)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0654 (0.0640)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0785 (0.0657)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0785 (0.0662)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0625 (0.0662)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0577 (0.0657)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0535 (0.0651)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0557 (0.0650)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0586 (0.0652)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0562 (0.0647)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0519 (0.0642)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0501 (0.0637)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0548 (0.0636)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0586 (0.0634)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0575 (0.0633)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0580 (0.0635)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0580 (0.0633)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0524 (0.0632)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0519 (0.0628)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0519 (0.0624)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0522 (0.0622)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0500 (0.0619)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0511 (0.0618)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0480 (0.0616)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0467 (0.0612)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0467 (0.0608)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0480 (0.0606)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0495 (0.0604)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0568 (0.0610)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0642 (0.0611)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0582 (0.0610)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0613 (0.0611)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0550 (0.0610)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0547 (0.0610)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0607 (0.0615)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0697 (0.0618)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0598 (0.0617)  time: 1.5813  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:29]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0578 (0.0617)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0516 (0.0615)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0535 (0.0613)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0540 (0.0612)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0550 (0.0613)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0550 (0.0612)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0486 (0.0610)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0502 (0.0610)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0501 (0.0608)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0503 (0.0608)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0568 (0.0607)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0568 (0.0608)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0552 (0.0608)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0550 (0.0607)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0539 (0.0607)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0529 (0.0605)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0472 (0.0603)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0482 (0.0603)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0498 (0.0602)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0485 (0.0600)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:29] Total time: 0:18:07 (1.5790 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0485 (0.0600)\n",
      "Valid: [epoch:29]  [ 0/14]  eta: 0:00:14  loss: 0.0451 (0.0451)  time: 1.0057  data: 0.4024  max mem: 39763\n",
      "Valid: [epoch:29]  [13/14]  eta: 0:00:00  loss: 0.0424 (0.0433)  time: 0.1138  data: 0.0288  max mem: 39763\n",
      "Valid: [epoch:29] Total time: 0:00:01 (0.1274 s / it)\n",
      "Averaged stats: loss: 0.0424 (0.0433)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_29_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.043%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:30]  [  0/689]  eta: 0:11:19  lr: 0.000100  loss: 0.0638 (0.0638)  time: 0.9863  data: 0.4976  max mem: 39763\n",
      "Train: [epoch:30]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0527 (0.0537)  time: 1.5248  data: 0.0453  max mem: 39763\n",
      "Train: [epoch:30]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0491 (0.0525)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0522 (0.0565)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0600 (0.0574)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0630 (0.0609)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0681 (0.0611)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0527 (0.0603)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0543 (0.0603)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0576 (0.0610)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0633 (0.0616)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0627 (0.0620)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0690 (0.0626)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0591 (0.0623)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0502 (0.0616)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0578 (0.0619)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0630 (0.0619)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0582 (0.0617)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0545 (0.0614)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0512 (0.0608)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0498 (0.0604)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0490 (0.0598)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0495 (0.0597)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0571 (0.0602)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0671 (0.0612)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0620 (0.0614)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0674 (0.0620)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0703 (0.0622)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0658 (0.0623)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0604 (0.0621)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0507 (0.0617)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0478 (0.0613)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0473 (0.0610)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0494 (0.0607)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0498 (0.0606)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0542 (0.0603)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0534 (0.0601)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0540 (0.0600)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0567 (0.0600)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0565 (0.0600)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0534 (0.0599)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0532 (0.0598)  time: 1.5786  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:30]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0539 (0.0598)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0585 (0.0599)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0589 (0.0601)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0545 (0.0603)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0539 (0.0603)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0546 (0.0603)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0582 (0.0604)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0599 (0.0605)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0596 (0.0606)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0596 (0.0608)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0644 (0.0613)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0627 (0.0613)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0536 (0.0611)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0536 (0.0610)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0540 (0.0609)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0549 (0.0609)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0574 (0.0610)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0627 (0.0611)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0600 (0.0610)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0554 (0.0610)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0526 (0.0609)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0506 (0.0608)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0538 (0.0607)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0572 (0.0608)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0616 (0.0609)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0682 (0.0610)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0630 (0.0610)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0619 (0.0610)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:30] Total time: 0:18:07 (1.5787 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0619 (0.0610)\n",
      "Valid: [epoch:30]  [ 0/14]  eta: 0:00:11  loss: 0.0449 (0.0449)  time: 0.8055  data: 0.3612  max mem: 39763\n",
      "Valid: [epoch:30]  [13/14]  eta: 0:00:00  loss: 0.0376 (0.0391)  time: 0.0995  data: 0.0259  max mem: 39763\n",
      "Valid: [epoch:30] Total time: 0:00:01 (0.1090 s / it)\n",
      "Averaged stats: loss: 0.0376 (0.0391)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_30_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.039%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:31]  [  0/689]  eta: 0:14:14  lr: 0.000100  loss: 0.0463 (0.0463)  time: 1.2406  data: 0.7613  max mem: 39763\n",
      "Train: [epoch:31]  [ 10/689]  eta: 0:17:30  lr: 0.000100  loss: 0.0605 (0.0614)  time: 1.5476  data: 0.0693  max mem: 39763\n",
      "Train: [epoch:31]  [ 20/689]  eta: 0:17:25  lr: 0.000100  loss: 0.0635 (0.0674)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [ 30/689]  eta: 0:17:13  lr: 0.000100  loss: 0.0641 (0.0667)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [ 40/689]  eta: 0:16:59  lr: 0.000100  loss: 0.0630 (0.0654)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [ 50/689]  eta: 0:16:44  lr: 0.000100  loss: 0.0542 (0.0631)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [ 60/689]  eta: 0:16:29  lr: 0.000100  loss: 0.0523 (0.0619)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [ 70/689]  eta: 0:16:14  lr: 0.000100  loss: 0.0488 (0.0604)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [ 80/689]  eta: 0:15:59  lr: 0.000100  loss: 0.0489 (0.0593)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [ 90/689]  eta: 0:15:43  lr: 0.000100  loss: 0.0489 (0.0584)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [100/689]  eta: 0:15:28  lr: 0.000100  loss: 0.0497 (0.0582)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [110/689]  eta: 0:15:12  lr: 0.000100  loss: 0.0555 (0.0584)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [120/689]  eta: 0:14:57  lr: 0.000100  loss: 0.0592 (0.0591)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [130/689]  eta: 0:14:41  lr: 0.000100  loss: 0.0627 (0.0595)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [140/689]  eta: 0:14:25  lr: 0.000100  loss: 0.0598 (0.0592)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [150/689]  eta: 0:14:10  lr: 0.000100  loss: 0.0538 (0.0588)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [160/689]  eta: 0:13:54  lr: 0.000100  loss: 0.0500 (0.0587)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [170/689]  eta: 0:13:38  lr: 0.000100  loss: 0.0500 (0.0582)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [180/689]  eta: 0:13:23  lr: 0.000100  loss: 0.0520 (0.0585)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [190/689]  eta: 0:13:07  lr: 0.000100  loss: 0.0619 (0.0588)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [200/689]  eta: 0:12:51  lr: 0.000100  loss: 0.0603 (0.0591)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0573 (0.0595)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [220/689]  eta: 0:12:20  lr: 0.000100  loss: 0.0707 (0.0608)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [230/689]  eta: 0:12:04  lr: 0.000100  loss: 0.0748 (0.0611)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0547 (0.0609)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0547 (0.0609)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [260/689]  eta: 0:11:17  lr: 0.000100  loss: 0.0567 (0.0607)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [270/689]  eta: 0:11:01  lr: 0.000100  loss: 0.0561 (0.0606)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0548 (0.0605)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0567 (0.0607)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [300/689]  eta: 0:10:14  lr: 0.000100  loss: 0.0627 (0.0608)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [310/689]  eta: 0:09:58  lr: 0.000100  loss: 0.0550 (0.0608)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0593 (0.0609)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0599 (0.0609)  time: 1.5798  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:31]  [340/689]  eta: 0:09:11  lr: 0.000100  loss: 0.0542 (0.0607)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [350/689]  eta: 0:08:55  lr: 0.000100  loss: 0.0540 (0.0605)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0529 (0.0604)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0564 (0.0604)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0587 (0.0603)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [390/689]  eta: 0:07:52  lr: 0.000100  loss: 0.0558 (0.0603)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0558 (0.0602)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0579 (0.0602)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0541 (0.0600)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0501 (0.0600)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [440/689]  eta: 0:06:33  lr: 0.000100  loss: 0.0662 (0.0604)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0682 (0.0606)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0619 (0.0607)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0556 (0.0606)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [480/689]  eta: 0:05:30  lr: 0.000100  loss: 0.0513 (0.0604)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0523 (0.0603)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0536 (0.0603)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0520 (0.0601)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0510 (0.0600)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [530/689]  eta: 0:04:11  lr: 0.000100  loss: 0.0510 (0.0599)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0525 (0.0598)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0517 (0.0597)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0500 (0.0595)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0542 (0.0596)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0553 (0.0595)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0548 (0.0594)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0522 (0.0593)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0504 (0.0592)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0524 (0.0591)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0546 (0.0591)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0509 (0.0590)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0496 (0.0589)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0503 (0.0587)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [670/689]  eta: 0:00:30  lr: 0.000100  loss: 0.0508 (0.0586)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0508 (0.0585)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0506 (0.0585)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:31] Total time: 0:18:08 (1.5797 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0506 (0.0585)\n",
      "Valid: [epoch:31]  [ 0/14]  eta: 0:00:14  loss: 0.0464 (0.0464)  time: 1.0210  data: 0.3892  max mem: 39763\n",
      "Valid: [epoch:31]  [13/14]  eta: 0:00:00  loss: 0.0372 (0.0392)  time: 0.1149  data: 0.0279  max mem: 39763\n",
      "Valid: [epoch:31] Total time: 0:00:01 (0.1244 s / it)\n",
      "Averaged stats: loss: 0.0372 (0.0392)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_31_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.039%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:32]  [  0/689]  eta: 0:11:22  lr: 0.000100  loss: 0.0527 (0.0527)  time: 0.9911  data: 0.5128  max mem: 39763\n",
      "Train: [epoch:32]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0508 (0.0514)  time: 1.5269  data: 0.0467  max mem: 39763\n",
      "Train: [epoch:32]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0544 (0.0539)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [ 30/689]  eta: 0:17:09  lr: 0.000100  loss: 0.0550 (0.0537)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [ 40/689]  eta: 0:16:56  lr: 0.000100  loss: 0.0568 (0.0557)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [ 50/689]  eta: 0:16:42  lr: 0.000100  loss: 0.0580 (0.0556)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [ 60/689]  eta: 0:16:28  lr: 0.000100  loss: 0.0522 (0.0551)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [ 70/689]  eta: 0:16:13  lr: 0.000100  loss: 0.0487 (0.0544)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [ 80/689]  eta: 0:15:58  lr: 0.000100  loss: 0.0500 (0.0548)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0537 (0.0547)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [100/689]  eta: 0:15:27  lr: 0.000100  loss: 0.0538 (0.0547)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [110/689]  eta: 0:15:12  lr: 0.000100  loss: 0.0539 (0.0547)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0546 (0.0548)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [130/689]  eta: 0:14:41  lr: 0.000100  loss: 0.0567 (0.0551)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [140/689]  eta: 0:14:25  lr: 0.000100  loss: 0.0585 (0.0559)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [150/689]  eta: 0:14:10  lr: 0.000100  loss: 0.0611 (0.0571)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [160/689]  eta: 0:13:54  lr: 0.000100  loss: 0.0749 (0.0584)  time: 1.5839  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [170/689]  eta: 0:13:39  lr: 0.000100  loss: 0.0616 (0.0584)  time: 1.5845  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [180/689]  eta: 0:13:23  lr: 0.000100  loss: 0.0591 (0.0585)  time: 1.5838  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [190/689]  eta: 0:13:07  lr: 0.000100  loss: 0.0615 (0.0590)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [200/689]  eta: 0:12:52  lr: 0.000100  loss: 0.0703 (0.0603)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [210/689]  eta: 0:12:36  lr: 0.000100  loss: 0.0677 (0.0604)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [220/689]  eta: 0:12:20  lr: 0.000100  loss: 0.0552 (0.0601)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [230/689]  eta: 0:12:05  lr: 0.000100  loss: 0.0509 (0.0598)  time: 1.5840  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [240/689]  eta: 0:11:49  lr: 0.000100  loss: 0.0512 (0.0596)  time: 1.5841  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [250/689]  eta: 0:11:33  lr: 0.000100  loss: 0.0512 (0.0595)  time: 1.5839  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:32]  [260/689]  eta: 0:11:17  lr: 0.000100  loss: 0.0538 (0.0597)  time: 1.5840  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [270/689]  eta: 0:11:02  lr: 0.000100  loss: 0.0558 (0.0602)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [280/689]  eta: 0:10:46  lr: 0.000100  loss: 0.0566 (0.0602)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [290/689]  eta: 0:10:30  lr: 0.000100  loss: 0.0575 (0.0604)  time: 1.5839  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [300/689]  eta: 0:10:14  lr: 0.000100  loss: 0.0548 (0.0601)  time: 1.5837  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [310/689]  eta: 0:09:59  lr: 0.000100  loss: 0.0524 (0.0599)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [320/689]  eta: 0:09:43  lr: 0.000100  loss: 0.0579 (0.0604)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [330/689]  eta: 0:09:27  lr: 0.000100  loss: 0.0627 (0.0602)  time: 1.5837  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [340/689]  eta: 0:09:11  lr: 0.000100  loss: 0.0519 (0.0600)  time: 1.5837  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [350/689]  eta: 0:08:55  lr: 0.000100  loss: 0.0521 (0.0598)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [360/689]  eta: 0:08:40  lr: 0.000100  loss: 0.0569 (0.0601)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [370/689]  eta: 0:08:24  lr: 0.000100  loss: 0.0573 (0.0600)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [380/689]  eta: 0:08:08  lr: 0.000100  loss: 0.0538 (0.0598)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [390/689]  eta: 0:07:52  lr: 0.000100  loss: 0.0553 (0.0598)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0561 (0.0598)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [410/689]  eta: 0:07:21  lr: 0.000100  loss: 0.0563 (0.0599)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [420/689]  eta: 0:07:05  lr: 0.000100  loss: 0.0673 (0.0602)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [430/689]  eta: 0:06:49  lr: 0.000100  loss: 0.0833 (0.0609)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [440/689]  eta: 0:06:33  lr: 0.000100  loss: 0.0788 (0.0610)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0562 (0.0608)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [460/689]  eta: 0:06:02  lr: 0.000100  loss: 0.0527 (0.0607)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [470/689]  eta: 0:05:46  lr: 0.000100  loss: 0.0571 (0.0606)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [480/689]  eta: 0:05:30  lr: 0.000100  loss: 0.0582 (0.0607)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0539 (0.0607)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0521 (0.0605)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [510/689]  eta: 0:04:43  lr: 0.000100  loss: 0.0530 (0.0604)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [520/689]  eta: 0:04:27  lr: 0.000100  loss: 0.0533 (0.0603)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [530/689]  eta: 0:04:11  lr: 0.000100  loss: 0.0533 (0.0605)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0568 (0.0606)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0560 (0.0605)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [560/689]  eta: 0:03:24  lr: 0.000100  loss: 0.0560 (0.0605)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [570/689]  eta: 0:03:08  lr: 0.000100  loss: 0.0553 (0.0604)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0581 (0.0607)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0647 (0.0607)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0526 (0.0606)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0518 (0.0606)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [620/689]  eta: 0:01:49  lr: 0.000100  loss: 0.0600 (0.0608)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0703 (0.0610)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0630 (0.0611)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0572 (0.0611)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0601 (0.0612)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [670/689]  eta: 0:00:30  lr: 0.000100  loss: 0.0557 (0.0611)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0531 (0.0611)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0596 (0.0611)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:32] Total time: 0:18:10 (1.5821 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0596 (0.0611)\n",
      "Valid: [epoch:32]  [ 0/14]  eta: 0:00:14  loss: 0.0670 (0.0670)  time: 1.0208  data: 0.3732  max mem: 39763\n",
      "Valid: [epoch:32]  [13/14]  eta: 0:00:00  loss: 0.0625 (0.0641)  time: 0.1150  data: 0.0267  max mem: 39763\n",
      "Valid: [epoch:32] Total time: 0:00:01 (0.1257 s / it)\n",
      "Averaged stats: loss: 0.0625 (0.0641)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_32_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.064%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:33]  [  0/689]  eta: 0:14:03  lr: 0.000100  loss: 0.0664 (0.0664)  time: 1.2247  data: 0.7473  max mem: 39763\n",
      "Train: [epoch:33]  [ 10/689]  eta: 0:17:29  lr: 0.000100  loss: 0.0645 (0.0662)  time: 1.5451  data: 0.0680  max mem: 39763\n",
      "Train: [epoch:33]  [ 20/689]  eta: 0:17:24  lr: 0.000100  loss: 0.0690 (0.0713)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [ 30/689]  eta: 0:17:12  lr: 0.000100  loss: 0.0713 (0.0704)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [ 40/689]  eta: 0:16:58  lr: 0.000100  loss: 0.0691 (0.0712)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [ 50/689]  eta: 0:16:44  lr: 0.000100  loss: 0.0715 (0.0731)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [ 60/689]  eta: 0:16:29  lr: 0.000100  loss: 0.0598 (0.0704)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [ 70/689]  eta: 0:16:14  lr: 0.000100  loss: 0.0541 (0.0687)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [ 80/689]  eta: 0:15:58  lr: 0.000100  loss: 0.0555 (0.0676)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [ 90/689]  eta: 0:15:43  lr: 0.000100  loss: 0.0526 (0.0663)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [100/689]  eta: 0:15:27  lr: 0.000100  loss: 0.0519 (0.0651)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [110/689]  eta: 0:15:12  lr: 0.000100  loss: 0.0529 (0.0643)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0568 (0.0644)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [130/689]  eta: 0:14:41  lr: 0.000100  loss: 0.0544 (0.0637)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [140/689]  eta: 0:14:25  lr: 0.000100  loss: 0.0558 (0.0636)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0582 (0.0633)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [160/689]  eta: 0:13:54  lr: 0.000100  loss: 0.0576 (0.0631)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [170/689]  eta: 0:13:38  lr: 0.000100  loss: 0.0576 (0.0629)  time: 1.5801  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:33]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0545 (0.0626)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [190/689]  eta: 0:13:07  lr: 0.000100  loss: 0.0559 (0.0631)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [200/689]  eta: 0:12:51  lr: 0.000100  loss: 0.0614 (0.0628)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0549 (0.0623)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0505 (0.0619)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [230/689]  eta: 0:12:04  lr: 0.000100  loss: 0.0529 (0.0616)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0542 (0.0616)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0579 (0.0614)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0579 (0.0614)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [270/689]  eta: 0:11:01  lr: 0.000100  loss: 0.0559 (0.0612)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0542 (0.0612)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0573 (0.0612)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0573 (0.0611)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [310/689]  eta: 0:09:58  lr: 0.000100  loss: 0.0518 (0.0608)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0533 (0.0607)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0543 (0.0606)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0580 (0.0607)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [350/689]  eta: 0:08:55  lr: 0.000100  loss: 0.0649 (0.0610)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0609 (0.0610)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0560 (0.0611)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0560 (0.0612)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0572 (0.0612)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0572 (0.0611)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0548 (0.0613)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0509 (0.0612)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0524 (0.0610)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [440/689]  eta: 0:06:33  lr: 0.000100  loss: 0.0529 (0.0609)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0521 (0.0608)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0539 (0.0608)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0642 (0.0610)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0599 (0.0610)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0571 (0.0609)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0595 (0.0609)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0594 (0.0609)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0562 (0.0608)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0541 (0.0607)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0551 (0.0607)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0622 (0.0608)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0641 (0.0611)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0620 (0.0611)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0575 (0.0612)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0555 (0.0611)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0514 (0.0609)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0503 (0.0609)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0538 (0.0610)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0570 (0.0609)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0580 (0.0610)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0605 (0.0610)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0561 (0.0610)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0538 (0.0610)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0551 (0.0609)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0571 (0.0610)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:33] Total time: 0:18:07 (1.5790 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0571 (0.0610)\n",
      "Valid: [epoch:33]  [ 0/14]  eta: 0:00:14  loss: 0.0443 (0.0443)  time: 1.0105  data: 0.3701  max mem: 39763\n",
      "Valid: [epoch:33]  [13/14]  eta: 0:00:00  loss: 0.0399 (0.0419)  time: 0.1142  data: 0.0265  max mem: 39763\n",
      "Valid: [epoch:33] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.0399 (0.0419)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_33_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.042%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:34]  [  0/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0641 (0.0641)  time: 1.0046  data: 0.5256  max mem: 39763\n",
      "Train: [epoch:34]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0767 (0.0831)  time: 1.5270  data: 0.0479  max mem: 39763\n",
      "Train: [epoch:34]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0754 (0.0768)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0687 (0.0767)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [ 40/689]  eta: 0:16:56  lr: 0.000100  loss: 0.0716 (0.0758)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [ 50/689]  eta: 0:16:42  lr: 0.000100  loss: 0.0607 (0.0732)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0690 (0.0770)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0661 (0.0744)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0583 (0.0727)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0529 (0.0705)  time: 1.5798  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:34]  [100/689]  eta: 0:15:27  lr: 0.000100  loss: 0.0524 (0.0690)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0545 (0.0683)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0545 (0.0674)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0559 (0.0667)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [140/689]  eta: 0:14:25  lr: 0.000100  loss: 0.0589 (0.0669)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0683 (0.0673)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0684 (0.0683)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [170/689]  eta: 0:13:38  lr: 0.000100  loss: 0.0672 (0.0690)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0783 (0.0700)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0711 (0.0699)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [200/689]  eta: 0:12:51  lr: 0.000100  loss: 0.0581 (0.0696)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0556 (0.0689)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0568 (0.0687)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [230/689]  eta: 0:12:04  lr: 0.000100  loss: 0.0605 (0.0689)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0652 (0.0688)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0603 (0.0688)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0597 (0.0685)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [270/689]  eta: 0:11:01  lr: 0.000100  loss: 0.0595 (0.0684)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0583 (0.0679)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0583 (0.0679)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0645 (0.0685)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [310/689]  eta: 0:09:58  lr: 0.000100  loss: 0.0645 (0.0687)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0669 (0.0690)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0652 (0.0690)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0602 (0.0688)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [350/689]  eta: 0:08:55  lr: 0.000100  loss: 0.0609 (0.0690)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0698 (0.0689)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0593 (0.0687)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0546 (0.0683)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0527 (0.0680)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0527 (0.0677)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0517 (0.0674)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0574 (0.0672)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0584 (0.0671)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [440/689]  eta: 0:06:33  lr: 0.000100  loss: 0.0560 (0.0668)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0570 (0.0667)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0617 (0.0667)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0610 (0.0665)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0565 (0.0664)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0589 (0.0663)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0608 (0.0663)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0662 (0.0668)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0787 (0.0672)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0713 (0.0672)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0626 (0.0671)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0632 (0.0671)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0605 (0.0669)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0552 (0.0667)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0538 (0.0665)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0563 (0.0665)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0593 (0.0664)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0624 (0.0665)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0632 (0.0666)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0686 (0.0667)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0645 (0.0667)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0629 (0.0668)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0596 (0.0667)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0551 (0.0665)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0551 (0.0664)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0604 (0.0663)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:34] Total time: 0:18:07 (1.5790 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0604 (0.0663)\n",
      "Valid: [epoch:34]  [ 0/14]  eta: 0:00:14  loss: 0.0445 (0.0445)  time: 1.0107  data: 0.3777  max mem: 39763\n",
      "Valid: [epoch:34]  [13/14]  eta: 0:00:00  loss: 0.0445 (0.0451)  time: 0.1141  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:34] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.0445 (0.0451)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_34_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.045%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:35]  [  0/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0496 (0.0496)  time: 1.0044  data: 0.5279  max mem: 39763\n",
      "Train: [epoch:35]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0620 (0.0614)  time: 1.5258  data: 0.0481  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:35]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0588 (0.0586)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0547 (0.0575)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0553 (0.0575)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0558 (0.0581)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0580 (0.0584)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0580 (0.0580)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0560 (0.0579)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0553 (0.0579)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0615 (0.0589)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0728 (0.0611)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0753 (0.0619)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0659 (0.0626)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0674 (0.0636)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0813 (0.0663)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0751 (0.0666)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0669 (0.0679)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0660 (0.0679)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0726 (0.0682)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0746 (0.0685)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0632 (0.0683)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0590 (0.0679)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0567 (0.0681)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0655 (0.0678)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0576 (0.0674)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0559 (0.0671)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0661 (0.0675)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0642 (0.0673)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0569 (0.0671)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0607 (0.0682)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0719 (0.0685)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0649 (0.0685)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0703 (0.0691)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0731 (0.0692)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0644 (0.0691)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0635 (0.0691)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0627 (0.0688)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0551 (0.0685)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0565 (0.0684)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0607 (0.0684)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0606 (0.0682)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0579 (0.0680)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0588 (0.0679)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0588 (0.0677)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0611 (0.0677)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0632 (0.0675)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0580 (0.0673)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0552 (0.0671)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0542 (0.0669)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0558 (0.0667)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0587 (0.0666)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0602 (0.0664)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0547 (0.0663)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0551 (0.0661)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0552 (0.0659)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0565 (0.0658)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0602 (0.0656)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0559 (0.0654)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0554 (0.0653)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0562 (0.0652)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0567 (0.0652)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0604 (0.0652)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0605 (0.0651)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0570 (0.0650)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0558 (0.0649)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0577 (0.0649)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0629 (0.0649)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0657 (0.0653)  time: 1.5797  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:35]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0767 (0.0654)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:35] Total time: 0:18:07 (1.5786 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0767 (0.0654)\n",
      "Valid: [epoch:35]  [ 0/14]  eta: 0:00:14  loss: 0.0500 (0.0500)  time: 1.0109  data: 0.3786  max mem: 39763\n",
      "Valid: [epoch:35]  [13/14]  eta: 0:00:00  loss: 0.0416 (0.0441)  time: 0.1143  data: 0.0271  max mem: 39763\n",
      "Valid: [epoch:35] Total time: 0:00:01 (0.1234 s / it)\n",
      "Averaged stats: loss: 0.0416 (0.0441)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_35_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.044%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:36]  [  0/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0717 (0.0717)  time: 1.0043  data: 0.5242  max mem: 39763\n",
      "Train: [epoch:36]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0573 (0.0605)  time: 1.5267  data: 0.0477  max mem: 39763\n",
      "Train: [epoch:36]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0592 (0.0650)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0650 (0.0643)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0622 (0.0644)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0586 (0.0634)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0567 (0.0624)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0525 (0.0611)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0541 (0.0615)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0678 (0.0635)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0739 (0.0645)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0637 (0.0643)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0589 (0.0641)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0572 (0.0636)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0559 (0.0633)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0559 (0.0629)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0583 (0.0632)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0640 (0.0631)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0591 (0.0630)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0580 (0.0629)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0580 (0.0629)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0561 (0.0627)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0573 (0.0625)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0578 (0.0626)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0637 (0.0626)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0677 (0.0634)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0767 (0.0643)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0680 (0.0645)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0680 (0.0650)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0797 (0.0656)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0797 (0.0661)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0708 (0.0666)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0678 (0.0666)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0640 (0.0666)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0640 (0.0667)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0609 (0.0666)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0617 (0.0668)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0715 (0.0669)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0652 (0.0669)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0566 (0.0667)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0559 (0.0665)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0580 (0.0664)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0581 (0.0664)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0623 (0.0665)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0596 (0.0663)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0583 (0.0664)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0663 (0.0666)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0672 (0.0670)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0724 (0.0671)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0633 (0.0671)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0646 (0.0674)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0646 (0.0673)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0573 (0.0671)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0549 (0.0669)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0549 (0.0669)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0636 (0.0670)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0646 (0.0669)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0626 (0.0668)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0578 (0.0666)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0545 (0.0664)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0541 (0.0663)  time: 1.5794  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:36]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0562 (0.0661)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0571 (0.0665)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0742 (0.0667)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0617 (0.0666)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0597 (0.0665)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0586 (0.0664)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0567 (0.0663)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0578 (0.0662)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0558 (0.0660)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:36] Total time: 0:18:07 (1.5788 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0558 (0.0660)\n",
      "Valid: [epoch:36]  [ 0/14]  eta: 0:00:14  loss: 0.0403 (0.0403)  time: 1.0109  data: 0.3473  max mem: 39763\n",
      "Valid: [epoch:36]  [13/14]  eta: 0:00:00  loss: 0.0411 (0.0424)  time: 0.1142  data: 0.0249  max mem: 39763\n",
      "Valid: [epoch:36] Total time: 0:00:01 (0.1233 s / it)\n",
      "Averaged stats: loss: 0.0411 (0.0424)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_36_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.042%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:37]  [  0/689]  eta: 0:12:31  lr: 0.000100  loss: 0.0570 (0.0570)  time: 1.0913  data: 0.6123  max mem: 39763\n",
      "Train: [epoch:37]  [ 10/689]  eta: 0:17:20  lr: 0.000100  loss: 0.0580 (0.0594)  time: 1.5325  data: 0.0557  max mem: 39763\n",
      "Train: [epoch:37]  [ 20/689]  eta: 0:17:19  lr: 0.000100  loss: 0.0580 (0.0610)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [ 30/689]  eta: 0:17:09  lr: 0.000100  loss: 0.0568 (0.0609)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [ 40/689]  eta: 0:16:56  lr: 0.000100  loss: 0.0618 (0.0622)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [ 50/689]  eta: 0:16:42  lr: 0.000100  loss: 0.0573 (0.0612)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0556 (0.0611)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0546 (0.0604)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0545 (0.0597)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0547 (0.0597)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [100/689]  eta: 0:15:27  lr: 0.000100  loss: 0.0604 (0.0597)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0581 (0.0592)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0555 (0.0591)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0539 (0.0588)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0530 (0.0587)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0573 (0.0589)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0606 (0.0589)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0551 (0.0589)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0536 (0.0586)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0526 (0.0584)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0543 (0.0582)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0548 (0.0580)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0561 (0.0582)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0552 (0.0580)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0547 (0.0579)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0547 (0.0578)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0544 (0.0578)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0583 (0.0579)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0567 (0.0579)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0565 (0.0579)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0569 (0.0579)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0596 (0.0583)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0601 (0.0584)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0638 (0.0594)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0799 (0.0600)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0692 (0.0602)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0615 (0.0602)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0574 (0.0602)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0551 (0.0601)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0564 (0.0603)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0694 (0.0610)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0862 (0.0614)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0731 (0.0617)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0689 (0.0619)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0689 (0.0621)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0658 (0.0622)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0648 (0.0623)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0602 (0.0623)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0583 (0.0622)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0603 (0.0623)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0695 (0.0627)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0814 (0.0631)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0714 (0.0633)  time: 1.5801  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:37]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0674 (0.0635)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0661 (0.0636)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0703 (0.0638)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0744 (0.0640)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0675 (0.0640)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0624 (0.0639)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0648 (0.0641)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0723 (0.0645)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0841 (0.0653)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0841 (0.0657)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0717 (0.0659)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0693 (0.0660)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0687 (0.0660)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0675 (0.0660)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0623 (0.0661)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0713 (0.0666)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0713 (0.0666)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:37] Total time: 0:18:07 (1.5790 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0713 (0.0666)\n",
      "Valid: [epoch:37]  [ 0/14]  eta: 0:00:14  loss: 0.0504 (0.0504)  time: 1.0199  data: 0.3937  max mem: 39763\n",
      "Valid: [epoch:37]  [13/14]  eta: 0:00:00  loss: 0.0438 (0.0459)  time: 0.1148  data: 0.0282  max mem: 39763\n",
      "Valid: [epoch:37] Total time: 0:00:01 (0.1240 s / it)\n",
      "Averaged stats: loss: 0.0438 (0.0459)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_37_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.046%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:38]  [  0/689]  eta: 0:11:43  lr: 0.000100  loss: 0.0641 (0.0641)  time: 1.0203  data: 0.5378  max mem: 39763\n",
      "Train: [epoch:38]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0641 (0.0644)  time: 1.5279  data: 0.0490  max mem: 39763\n",
      "Train: [epoch:38]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0613 (0.0648)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0673 (0.0685)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [ 40/689]  eta: 0:16:56  lr: 0.000100  loss: 0.0682 (0.0683)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [ 50/689]  eta: 0:16:42  lr: 0.000100  loss: 0.0683 (0.0709)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0722 (0.0711)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0658 (0.0694)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0613 (0.0691)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0736 (0.0718)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0744 (0.0717)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0759 (0.0736)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0859 (0.0747)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0798 (0.0753)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0655 (0.0745)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0614 (0.0736)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0591 (0.0729)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0637 (0.0727)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0678 (0.0727)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0642 (0.0723)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0577 (0.0716)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0569 (0.0710)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0574 (0.0705)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0626 (0.0708)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0716 (0.0709)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0636 (0.0705)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0553 (0.0700)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0542 (0.0696)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0597 (0.0696)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0676 (0.0696)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0698 (0.0696)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0598 (0.0692)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0593 (0.0690)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0593 (0.0687)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0596 (0.0687)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0625 (0.0686)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0597 (0.0684)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0567 (0.0681)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0557 (0.0678)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0549 (0.0675)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0583 (0.0673)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0608 (0.0673)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0606 (0.0673)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0592 (0.0671)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0613 (0.0670)  time: 1.5790  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:38]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0552 (0.0668)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0555 (0.0667)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0648 (0.0667)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0648 (0.0666)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0609 (0.0666)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0637 (0.0666)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0698 (0.0667)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0679 (0.0667)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0644 (0.0667)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0631 (0.0670)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0849 (0.0684)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0805 (0.0685)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0670 (0.0684)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0632 (0.0688)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0663 (0.0688)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0617 (0.0687)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0578 (0.0688)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0609 (0.0688)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0589 (0.0686)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0555 (0.0685)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0609 (0.0684)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0612 (0.0684)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0625 (0.0683)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0635 (0.0683)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0630 (0.0683)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:38] Total time: 0:18:07 (1.5787 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0630 (0.0683)\n",
      "Valid: [epoch:38]  [ 0/14]  eta: 0:00:11  loss: 0.0451 (0.0451)  time: 0.8126  data: 0.3873  max mem: 39763\n",
      "Valid: [epoch:38]  [13/14]  eta: 0:00:00  loss: 0.0451 (0.0471)  time: 0.1000  data: 0.0277  max mem: 39763\n",
      "Valid: [epoch:38] Total time: 0:00:01 (0.1090 s / it)\n",
      "Averaged stats: loss: 0.0451 (0.0471)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_38_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.047%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:39]  [  0/689]  eta: 0:11:55  lr: 0.000100  loss: 0.0572 (0.0572)  time: 1.0379  data: 0.5589  max mem: 39763\n",
      "Train: [epoch:39]  [ 10/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0724 (0.0705)  time: 1.5292  data: 0.0509  max mem: 39763\n",
      "Train: [epoch:39]  [ 20/689]  eta: 0:17:19  lr: 0.000100  loss: 0.0643 (0.0651)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0596 (0.0656)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [ 40/689]  eta: 0:16:56  lr: 0.000100  loss: 0.0674 (0.0677)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [ 50/689]  eta: 0:16:42  lr: 0.000100  loss: 0.0688 (0.0683)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0655 (0.0679)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0640 (0.0685)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0625 (0.0677)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0578 (0.0666)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [100/689]  eta: 0:15:27  lr: 0.000100  loss: 0.0577 (0.0662)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0631 (0.0659)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0649 (0.0664)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0653 (0.0662)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0653 (0.0676)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0780 (0.0685)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0700 (0.0689)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [170/689]  eta: 0:13:38  lr: 0.000100  loss: 0.0675 (0.0688)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0626 (0.0688)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0618 (0.0688)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [200/689]  eta: 0:12:51  lr: 0.000100  loss: 0.0600 (0.0683)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0615 (0.0681)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0620 (0.0683)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0615 (0.0684)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0655 (0.0684)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0636 (0.0684)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0645 (0.0683)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0594 (0.0679)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0584 (0.0676)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0614 (0.0677)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0633 (0.0676)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0633 (0.0675)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0648 (0.0675)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0658 (0.0676)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0752 (0.0681)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0745 (0.0683)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0619 (0.0681)  time: 1.5803  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:39]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0595 (0.0680)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0552 (0.0678)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0552 (0.0676)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0587 (0.0675)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0653 (0.0676)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0656 (0.0676)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0702 (0.0678)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [440/689]  eta: 0:06:33  lr: 0.000100  loss: 0.0803 (0.0683)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0879 (0.0690)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0699 (0.0689)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0614 (0.0691)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0593 (0.0691)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0633 (0.0691)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0626 (0.0689)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0552 (0.0686)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0597 (0.0686)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0626 (0.0684)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0626 (0.0685)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0624 (0.0683)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0612 (0.0682)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0573 (0.0680)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0573 (0.0679)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0568 (0.0677)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0582 (0.0676)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0600 (0.0675)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0590 (0.0673)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0592 (0.0674)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0643 (0.0673)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0658 (0.0674)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0649 (0.0675)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0649 (0.0675)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0671 (0.0676)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0672 (0.0675)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:39] Total time: 0:18:07 (1.5790 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0672 (0.0675)\n",
      "Valid: [epoch:39]  [ 0/14]  eta: 0:00:14  loss: 0.0493 (0.0493)  time: 1.0149  data: 0.3496  max mem: 39763\n",
      "Valid: [epoch:39]  [13/14]  eta: 0:00:00  loss: 0.0456 (0.0470)  time: 0.1145  data: 0.0250  max mem: 39763\n",
      "Valid: [epoch:39] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.0456 (0.0470)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_39_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.047%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:40]  [  0/689]  eta: 0:11:26  lr: 0.000100  loss: 0.0695 (0.0695)  time: 0.9960  data: 0.5100  max mem: 39763\n",
      "Train: [epoch:40]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0598 (0.0607)  time: 1.5257  data: 0.0465  max mem: 39763\n",
      "Train: [epoch:40]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0653 (0.0672)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0665 (0.0657)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0632 (0.0660)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0686 (0.0681)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0771 (0.0707)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0699 (0.0702)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0652 (0.0702)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0652 (0.0698)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0669 (0.0697)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0669 (0.0697)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0779 (0.0707)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0856 (0.0724)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0978 (0.0746)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.1083 (0.0768)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0915 (0.0771)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0608 (0.0760)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0608 (0.0757)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0677 (0.0758)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0614 (0.0751)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0613 (0.0746)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0599 (0.0739)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0574 (0.0733)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0589 (0.0726)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0602 (0.0722)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0621 (0.0721)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0620 (0.0716)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0620 (0.0714)  time: 1.5787  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:40]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0620 (0.0710)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0620 (0.0709)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0704 (0.0710)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0726 (0.0711)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0744 (0.0711)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0697 (0.0711)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0701 (0.0712)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0701 (0.0711)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0651 (0.0709)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0625 (0.0707)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0665 (0.0708)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0710 (0.0708)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0686 (0.0708)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0654 (0.0707)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0614 (0.0704)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0608 (0.0703)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0608 (0.0701)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0659 (0.0702)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0707 (0.0702)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0614 (0.0701)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0595 (0.0699)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0624 (0.0698)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0635 (0.0697)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0665 (0.0701)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0908 (0.0706)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.1006 (0.0711)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0693 (0.0711)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0697 (0.0713)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0747 (0.0713)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0720 (0.0716)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0777 (0.0716)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0778 (0.0718)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0861 (0.0721)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0751 (0.0720)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0647 (0.0720)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0629 (0.0719)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0648 (0.0718)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0609 (0.0716)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0611 (0.0715)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0651 (0.0715)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0709 (0.0719)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:40] Total time: 0:18:07 (1.5784 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0709 (0.0719)\n",
      "Valid: [epoch:40]  [ 0/14]  eta: 0:00:14  loss: 0.0522 (0.0522)  time: 1.0112  data: 0.3846  max mem: 39763\n",
      "Valid: [epoch:40]  [13/14]  eta: 0:00:00  loss: 0.0514 (0.0523)  time: 0.1143  data: 0.0275  max mem: 39763\n",
      "Valid: [epoch:40] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.0514 (0.0523)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_40_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.052%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:41]  [  0/689]  eta: 0:11:53  lr: 0.000100  loss: 0.0563 (0.0563)  time: 1.0350  data: 0.5569  max mem: 39763\n",
      "Train: [epoch:41]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0776 (0.0799)  time: 1.5277  data: 0.0507  max mem: 39763\n",
      "Train: [epoch:41]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0696 (0.0746)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0656 (0.0737)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0743 (0.0765)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0901 (0.0853)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0901 (0.0836)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0686 (0.0822)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0760 (0.0830)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0874 (0.0875)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.1080 (0.0910)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1033 (0.0923)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.1033 (0.0958)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0892 (0.0957)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0812 (0.0946)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0861 (0.0950)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0940 (0.0949)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0827 (0.0945)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0772 (0.0937)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0696 (0.0924)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0615 (0.0909)  time: 1.5786  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:41]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0633 (0.0897)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0635 (0.0886)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0630 (0.0876)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0684 (0.0876)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0718 (0.0868)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0607 (0.0860)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0600 (0.0852)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0596 (0.0843)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0597 (0.0836)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0594 (0.0828)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0566 (0.0820)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0560 (0.0813)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0603 (0.0808)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0651 (0.0805)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0668 (0.0801)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0646 (0.0797)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0665 (0.0796)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0794 (0.0798)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0709 (0.0796)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0670 (0.0795)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0670 (0.0791)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0652 (0.0789)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0613 (0.0787)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0711 (0.0786)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0681 (0.0783)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0665 (0.0783)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0639 (0.0779)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0614 (0.0777)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0639 (0.0775)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0623 (0.0772)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0610 (0.0769)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0619 (0.0766)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0619 (0.0763)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0623 (0.0761)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0623 (0.0759)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0648 (0.0758)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0648 (0.0756)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0629 (0.0753)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0615 (0.0751)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0605 (0.0750)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0654 (0.0749)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0666 (0.0747)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0716 (0.0749)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0791 (0.0750)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0682 (0.0748)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0653 (0.0747)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0676 (0.0746)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0640 (0.0744)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0623 (0.0745)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:41] Total time: 0:18:07 (1.5781 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0623 (0.0745)\n",
      "Valid: [epoch:41]  [ 0/14]  eta: 0:00:14  loss: 0.0529 (0.0529)  time: 1.0113  data: 0.3825  max mem: 39763\n",
      "Valid: [epoch:41]  [13/14]  eta: 0:00:00  loss: 0.0500 (0.0508)  time: 0.1142  data: 0.0274  max mem: 39763\n",
      "Valid: [epoch:41] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.0500 (0.0508)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_41_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.051%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:42]  [  0/689]  eta: 0:11:38  lr: 0.000100  loss: 0.0498 (0.0498)  time: 1.0139  data: 0.5351  max mem: 39763\n",
      "Train: [epoch:42]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0789 (0.0746)  time: 1.5260  data: 0.0487  max mem: 39763\n",
      "Train: [epoch:42]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0635 (0.0679)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0605 (0.0665)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0600 (0.0649)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0588 (0.0651)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0598 (0.0655)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0636 (0.0668)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0735 (0.0682)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0705 (0.0680)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0707 (0.0693)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0728 (0.0702)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0728 (0.0711)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:42]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0606 (0.0707)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0597 (0.0699)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0609 (0.0694)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0628 (0.0689)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0602 (0.0683)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0599 (0.0680)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0607 (0.0677)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0608 (0.0673)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0593 (0.0670)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0637 (0.0669)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0649 (0.0669)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0635 (0.0671)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0763 (0.0680)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0747 (0.0685)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0666 (0.0683)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0639 (0.0689)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0689 (0.0692)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0689 (0.0696)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0949 (0.0707)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0891 (0.0711)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0846 (0.0718)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0876 (0.0723)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0713 (0.0725)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0713 (0.0727)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0842 (0.0733)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.1069 (0.0744)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0728 (0.0743)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0668 (0.0741)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0717 (0.0742)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0679 (0.0739)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0621 (0.0737)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0647 (0.0738)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0647 (0.0736)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0713 (0.0736)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0726 (0.0735)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0683 (0.0736)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0769 (0.0737)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0820 (0.0740)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0791 (0.0740)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0791 (0.0744)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0740 (0.0743)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0695 (0.0743)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0644 (0.0742)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0683 (0.0741)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0611 (0.0739)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0614 (0.0739)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0626 (0.0737)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0615 (0.0735)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0622 (0.0734)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0641 (0.0732)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0612 (0.0732)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0661 (0.0731)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0672 (0.0731)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0678 (0.0732)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0803 (0.0734)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0718 (0.0733)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0664 (0.0732)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:42] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0664 (0.0732)\n",
      "Valid: [epoch:42]  [ 0/14]  eta: 0:00:14  loss: 0.0711 (0.0711)  time: 1.0404  data: 0.3604  max mem: 39763\n",
      "Valid: [epoch:42]  [13/14]  eta: 0:00:00  loss: 0.0591 (0.0608)  time: 0.1164  data: 0.0258  max mem: 39763\n",
      "Valid: [epoch:42] Total time: 0:00:01 (0.1257 s / it)\n",
      "Averaged stats: loss: 0.0591 (0.0608)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_42_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.061%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:43]  [  0/689]  eta: 0:11:37  lr: 0.000100  loss: 0.0714 (0.0714)  time: 1.0120  data: 0.5371  max mem: 39763\n",
      "Train: [epoch:43]  [ 10/689]  eta: 0:17:14  lr: 0.000100  loss: 0.0684 (0.0683)  time: 1.5243  data: 0.0489  max mem: 39763\n",
      "Train: [epoch:43]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0631 (0.0644)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0607 (0.0651)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0607 (0.0645)  time: 1.5765  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:43]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0639 (0.0657)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0773 (0.0695)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0796 (0.0703)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0728 (0.0723)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.0701 (0.0722)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0698 (0.0729)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0747 (0.0737)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0696 (0.0733)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0657 (0.0727)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0654 (0.0725)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0686 (0.0726)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0668 (0.0724)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.0648 (0.0722)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0632 (0.0715)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0612 (0.0726)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0646 (0.0723)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0703 (0.0727)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0890 (0.0738)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.0748 (0.0735)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0687 (0.0737)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0687 (0.0734)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0644 (0.0734)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0689 (0.0739)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0912 (0.0748)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0852 (0.0753)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0852 (0.0765)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0869 (0.0768)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0737 (0.0768)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0781 (0.0772)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0724 (0.0769)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0718 (0.0768)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0724 (0.0768)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0711 (0.0768)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0714 (0.0767)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0671 (0.0765)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0655 (0.0764)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0655 (0.0762)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0687 (0.0763)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0687 (0.0762)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0689 (0.0761)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0680 (0.0759)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0669 (0.0758)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0677 (0.0756)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0623 (0.0754)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0593 (0.0752)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.0589 (0.0748)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0599 (0.0747)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0695 (0.0747)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0683 (0.0746)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0667 (0.0744)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0644 (0.0742)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0652 (0.0742)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0633 (0.0739)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0587 (0.0737)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0604 (0.0735)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0645 (0.0735)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0686 (0.0735)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0693 (0.0735)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0717 (0.0737)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0711 (0.0737)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0697 (0.0736)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0704 (0.0737)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0710 (0.0737)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0649 (0.0736)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0667 (0.0736)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:43] Total time: 0:18:06 (1.5772 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0667 (0.0736)\n",
      "Valid: [epoch:43]  [ 0/14]  eta: 0:00:14  loss: 0.0526 (0.0526)  time: 1.0099  data: 0.3793  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:43]  [13/14]  eta: 0:00:00  loss: 0.0495 (0.0507)  time: 0.1141  data: 0.0271  max mem: 39763\n",
      "Valid: [epoch:43] Total time: 0:00:01 (0.1245 s / it)\n",
      "Averaged stats: loss: 0.0495 (0.0507)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_43_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.051%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:44]  [  0/689]  eta: 0:11:33  lr: 0.000100  loss: 0.0581 (0.0581)  time: 1.0060  data: 0.5268  max mem: 39763\n",
      "Train: [epoch:44]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0661 (0.0675)  time: 1.5246  data: 0.0480  max mem: 39763\n",
      "Train: [epoch:44]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0638 (0.0656)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0656 (0.0671)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0693 (0.0689)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0781 (0.0740)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0846 (0.0757)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0791 (0.0766)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0730 (0.0760)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0729 (0.0755)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0707 (0.0751)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0711 (0.0755)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0778 (0.0779)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0969 (0.0796)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0852 (0.0791)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0661 (0.0786)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0704 (0.0780)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0682 (0.0772)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0639 (0.0770)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0690 (0.0770)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0666 (0.0767)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0679 (0.0767)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0702 (0.0766)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0628 (0.0760)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0628 (0.0757)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0670 (0.0756)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0651 (0.0753)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0651 (0.0748)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0633 (0.0745)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0616 (0.0740)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.0635 (0.0737)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0635 (0.0735)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0664 (0.0734)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0664 (0.0733)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0681 (0.0732)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0626 (0.0729)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0650 (0.0728)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0667 (0.0727)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0628 (0.0725)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0654 (0.0726)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0687 (0.0727)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0654 (0.0725)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0638 (0.0725)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0708 (0.0725)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0660 (0.0723)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0646 (0.0721)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0651 (0.0720)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0657 (0.0721)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0797 (0.0725)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0797 (0.0726)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.0688 (0.0728)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0632 (0.0727)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0620 (0.0728)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0687 (0.0730)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0765 (0.0731)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0709 (0.0729)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0654 (0.0729)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0663 (0.0727)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0638 (0.0726)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0638 (0.0726)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0791 (0.0729)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0713 (0.0729)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0641 (0.0727)  time: 1.5774  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:44]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0647 (0.0726)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0652 (0.0725)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0622 (0.0724)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0625 (0.0723)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0625 (0.0721)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0639 (0.0721)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0664 (0.0721)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:44] Total time: 0:18:06 (1.5770 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0664 (0.0721)\n",
      "Valid: [epoch:44]  [ 0/14]  eta: 0:00:14  loss: 0.0555 (0.0555)  time: 1.0118  data: 0.3901  max mem: 39763\n",
      "Valid: [epoch:44]  [13/14]  eta: 0:00:00  loss: 0.0526 (0.0534)  time: 0.1143  data: 0.0279  max mem: 39763\n",
      "Valid: [epoch:44] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.0526 (0.0534)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_44_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.053%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:45]  [  0/689]  eta: 0:11:40  lr: 0.000100  loss: 0.0814 (0.0814)  time: 1.0165  data: 0.5380  max mem: 39763\n",
      "Train: [epoch:45]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0695 (0.0686)  time: 1.5245  data: 0.0490  max mem: 39763\n",
      "Train: [epoch:45]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0665 (0.0670)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [ 30/689]  eta: 0:17:06  lr: 0.000100  loss: 0.0663 (0.0680)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0663 (0.0674)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0604 (0.0667)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [ 60/689]  eta: 0:16:25  lr: 0.000100  loss: 0.0613 (0.0667)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0672 (0.0667)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0655 (0.0668)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.0664 (0.0670)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0643 (0.0668)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0637 (0.0671)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0647 (0.0668)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0635 (0.0667)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0646 (0.0671)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0706 (0.0674)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0673 (0.0671)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0619 (0.0671)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0725 (0.0674)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0727 (0.0677)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0696 (0.0677)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0645 (0.0676)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0650 (0.0676)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0657 (0.0676)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0643 (0.0675)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0614 (0.0674)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0623 (0.0675)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0616 (0.0672)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0594 (0.0670)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0594 (0.0668)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0625 (0.0668)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0642 (0.0667)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0623 (0.0666)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0662 (0.0668)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0662 (0.0667)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0646 (0.0667)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0660 (0.0668)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0660 (0.0668)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0639 (0.0668)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0643 (0.0668)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0705 (0.0671)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0738 (0.0677)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0732 (0.0680)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0709 (0.0681)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0784 (0.0686)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0896 (0.0690)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0719 (0.0691)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0706 (0.0691)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0680 (0.0691)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0710 (0.0692)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0719 (0.0694)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0629 (0.0692)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0616 (0.0693)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0655 (0.0693)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0654 (0.0692)  time: 1.5793  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:45]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0634 (0.0691)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0641 (0.0691)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0641 (0.0690)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0638 (0.0690)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0724 (0.0691)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0724 (0.0691)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0608 (0.0690)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0632 (0.0690)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0632 (0.0689)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0614 (0.0688)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0611 (0.0688)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0642 (0.0688)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0762 (0.0690)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0693 (0.0690)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0693 (0.0691)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:45] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0693 (0.0691)\n",
      "Valid: [epoch:45]  [ 0/14]  eta: 0:00:14  loss: 0.0495 (0.0495)  time: 1.0025  data: 0.3927  max mem: 39763\n",
      "Valid: [epoch:45]  [13/14]  eta: 0:00:00  loss: 0.0520 (0.0525)  time: 0.1136  data: 0.0281  max mem: 39763\n",
      "Valid: [epoch:45] Total time: 0:00:01 (0.1226 s / it)\n",
      "Averaged stats: loss: 0.0520 (0.0525)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_45_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.053%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:46]  [  0/689]  eta: 0:11:30  lr: 0.000100  loss: 0.0839 (0.0839)  time: 1.0021  data: 0.5229  max mem: 39763\n",
      "Train: [epoch:46]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0839 (0.0866)  time: 1.5250  data: 0.0476  max mem: 39763\n",
      "Train: [epoch:46]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0772 (0.0821)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0683 (0.0766)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0683 (0.0784)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0703 (0.0779)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0732 (0.0765)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0739 (0.0779)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0932 (0.0824)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.1026 (0.0842)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0992 (0.0870)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0826 (0.0861)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0646 (0.0846)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0667 (0.0835)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0707 (0.0838)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0798 (0.0839)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0838 (0.0838)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0852 (0.0851)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0821 (0.0847)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0787 (0.0845)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0713 (0.0844)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0674 (0.0840)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0683 (0.0837)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0815 (0.0837)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0813 (0.0838)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0783 (0.0836)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0768 (0.0834)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0814 (0.0840)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1008 (0.0855)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1106 (0.0868)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.1144 (0.0876)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0866 (0.0876)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0709 (0.0871)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0709 (0.0871)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0856 (0.0874)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0880 (0.0872)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0681 (0.0875)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0681 (0.0871)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0649 (0.0866)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0661 (0.0861)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0693 (0.0858)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0669 (0.0853)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0650 (0.0849)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0666 (0.0846)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0689 (0.0843)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0689 (0.0839)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0666 (0.0836)  time: 1.5781  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:46]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0678 (0.0834)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0725 (0.0836)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0725 (0.0836)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.0693 (0.0836)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0790 (0.0836)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0756 (0.0833)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0675 (0.0831)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0675 (0.0829)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0731 (0.0827)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0706 (0.0825)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0691 (0.0824)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0691 (0.0822)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0710 (0.0820)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0711 (0.0820)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0660 (0.0817)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0610 (0.0815)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0773 (0.0815)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0800 (0.0817)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0886 (0.0819)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0792 (0.0818)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0709 (0.0816)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0709 (0.0814)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0720 (0.0813)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:46] Total time: 0:18:06 (1.5773 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0720 (0.0813)\n",
      "Valid: [epoch:46]  [ 0/14]  eta: 0:00:14  loss: 0.0546 (0.0546)  time: 1.0201  data: 0.3436  max mem: 39763\n",
      "Valid: [epoch:46]  [13/14]  eta: 0:00:00  loss: 0.0508 (0.0526)  time: 0.1148  data: 0.0246  max mem: 39763\n",
      "Valid: [epoch:46] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.0508 (0.0526)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_46_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.053%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:47]  [  0/689]  eta: 0:11:49  lr: 0.000100  loss: 0.0588 (0.0588)  time: 1.0295  data: 0.5524  max mem: 39763\n",
      "Train: [epoch:47]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0710 (0.0775)  time: 1.5250  data: 0.0503  max mem: 39763\n",
      "Train: [epoch:47]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0823 (0.0854)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [ 30/689]  eta: 0:17:06  lr: 0.000100  loss: 0.0891 (0.0864)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [ 40/689]  eta: 0:16:53  lr: 0.000100  loss: 0.0705 (0.0820)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0692 (0.0797)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [ 60/689]  eta: 0:16:25  lr: 0.000100  loss: 0.0674 (0.0772)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [ 70/689]  eta: 0:16:10  lr: 0.000100  loss: 0.0615 (0.0756)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [ 80/689]  eta: 0:15:55  lr: 0.000100  loss: 0.0659 (0.0751)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.0665 (0.0738)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0666 (0.0740)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [110/689]  eta: 0:15:09  lr: 0.000100  loss: 0.0703 (0.0736)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0621 (0.0725)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0598 (0.0743)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.1035 (0.0779)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0891 (0.0782)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0722 (0.0777)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.0783 (0.0794)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0948 (0.0796)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0800 (0.0801)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.0797 (0.0800)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0778 (0.0800)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0675 (0.0795)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.0666 (0.0790)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0659 (0.0786)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0650 (0.0781)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0634 (0.0777)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0660 (0.0773)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0691 (0.0772)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0719 (0.0769)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.0683 (0.0767)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0668 (0.0765)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0688 (0.0762)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0701 (0.0762)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [340/689]  eta: 0:09:09  lr: 0.000100  loss: 0.0682 (0.0761)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0682 (0.0761)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0676 (0.0759)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0664 (0.0758)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [380/689]  eta: 0:08:06  lr: 0.000100  loss: 0.0657 (0.0755)  time: 1.5770  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:47]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0645 (0.0753)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0690 (0.0752)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0656 (0.0750)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [420/689]  eta: 0:07:03  lr: 0.000100  loss: 0.0622 (0.0748)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0631 (0.0746)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0680 (0.0745)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0706 (0.0743)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [460/689]  eta: 0:06:00  lr: 0.000100  loss: 0.0695 (0.0743)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0695 (0.0743)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0637 (0.0741)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0606 (0.0738)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.0666 (0.0737)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0679 (0.0736)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0695 (0.0737)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0678 (0.0736)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0649 (0.0735)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0649 (0.0734)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0634 (0.0733)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0636 (0.0732)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0652 (0.0730)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0651 (0.0729)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0664 (0.0728)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0655 (0.0727)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0655 (0.0726)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0696 (0.0728)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0770 (0.0728)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0632 (0.0727)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0638 (0.0727)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0701 (0.0727)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0738 (0.0727)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0738 (0.0727)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:47] Total time: 0:18:06 (1.5769 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0738 (0.0727)\n",
      "Valid: [epoch:47]  [ 0/14]  eta: 0:00:14  loss: 0.0511 (0.0511)  time: 1.0034  data: 0.3669  max mem: 39763\n",
      "Valid: [epoch:47]  [13/14]  eta: 0:00:00  loss: 0.0541 (0.0565)  time: 0.1137  data: 0.0263  max mem: 39763\n",
      "Valid: [epoch:47] Total time: 0:00:01 (0.1229 s / it)\n",
      "Averaged stats: loss: 0.0541 (0.0565)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_47_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.056%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:48]  [  0/689]  eta: 0:11:38  lr: 0.000100  loss: 0.0672 (0.0672)  time: 1.0132  data: 0.5308  max mem: 39763\n",
      "Train: [epoch:48]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0682 (0.0759)  time: 1.5253  data: 0.0483  max mem: 39763\n",
      "Train: [epoch:48]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0710 (0.0746)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0698 (0.0735)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0676 (0.0763)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0777 (0.0772)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [ 60/689]  eta: 0:16:25  lr: 0.000100  loss: 0.0766 (0.0765)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0695 (0.0759)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0685 (0.0755)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.0707 (0.0756)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0763 (0.0761)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0787 (0.0766)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0751 (0.0764)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0730 (0.0764)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0674 (0.0756)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [150/689]  eta: 0:14:07  lr: 0.000100  loss: 0.0669 (0.0755)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0674 (0.0755)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.0667 (0.0751)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0703 (0.0749)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0693 (0.0745)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.0685 (0.0749)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0695 (0.0746)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0699 (0.0745)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.0706 (0.0744)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0720 (0.0744)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0741 (0.0745)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0684 (0.0744)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [270/689]  eta: 0:10:59  lr: 0.000100  loss: 0.0656 (0.0741)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0643 (0.0741)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0772 (0.0746)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.0788 (0.0747)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:48]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0695 (0.0744)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0695 (0.0748)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0812 (0.0753)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [340/689]  eta: 0:09:09  lr: 0.000100  loss: 0.0791 (0.0754)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0702 (0.0753)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0672 (0.0752)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0709 (0.0750)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [380/689]  eta: 0:08:06  lr: 0.000100  loss: 0.0649 (0.0747)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0670 (0.0745)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0704 (0.0746)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0756 (0.0745)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0719 (0.0744)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0686 (0.0743)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0713 (0.0745)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0746 (0.0745)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0706 (0.0745)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0704 (0.0745)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0660 (0.0743)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0631 (0.0741)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.0667 (0.0741)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0739 (0.0744)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0737 (0.0743)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0697 (0.0742)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0701 (0.0743)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0685 (0.0742)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0683 (0.0742)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0677 (0.0741)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0685 (0.0741)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0677 (0.0740)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0671 (0.0740)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0719 (0.0740)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0673 (0.0739)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0653 (0.0738)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0684 (0.0738)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0655 (0.0736)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0655 (0.0736)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0681 (0.0735)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0710 (0.0736)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0710 (0.0735)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:48] Total time: 0:18:06 (1.5772 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0710 (0.0735)\n",
      "Valid: [epoch:48]  [ 0/14]  eta: 0:00:14  loss: 0.0547 (0.0547)  time: 1.0038  data: 0.3820  max mem: 39763\n",
      "Valid: [epoch:48]  [13/14]  eta: 0:00:00  loss: 0.0595 (0.0623)  time: 0.1137  data: 0.0274  max mem: 39763\n",
      "Valid: [epoch:48] Total time: 0:00:01 (0.1228 s / it)\n",
      "Averaged stats: loss: 0.0595 (0.0623)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_48_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.062%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:49]  [  0/689]  eta: 0:11:44  lr: 0.000100  loss: 0.0721 (0.0721)  time: 1.0230  data: 0.5421  max mem: 39763\n",
      "Train: [epoch:49]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0853 (0.0870)  time: 1.5246  data: 0.0494  max mem: 39763\n",
      "Train: [epoch:49]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0850 (0.0854)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [ 30/689]  eta: 0:17:06  lr: 0.000100  loss: 0.0827 (0.0848)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0771 (0.0815)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0659 (0.0785)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [ 60/689]  eta: 0:16:25  lr: 0.000100  loss: 0.0679 (0.0776)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0764 (0.0792)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [ 80/689]  eta: 0:15:55  lr: 0.000100  loss: 0.0767 (0.0784)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.0695 (0.0770)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0721 (0.0781)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0792 (0.0784)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0721 (0.0787)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0831 (0.0798)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0879 (0.0806)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [150/689]  eta: 0:14:07  lr: 0.000100  loss: 0.0901 (0.0835)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0869 (0.0832)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.0813 (0.0836)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0804 (0.0833)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0804 (0.0845)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.0977 (0.0857)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0909 (0.0856)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0738 (0.0855)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:49]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.0721 (0.0848)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0692 (0.0847)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0824 (0.0850)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0839 (0.0856)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0809 (0.0853)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0802 (0.0853)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0780 (0.0854)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.0715 (0.0850)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0679 (0.0845)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0714 (0.0843)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0681 (0.0837)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0646 (0.0833)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0688 (0.0830)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0707 (0.0828)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0695 (0.0825)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0683 (0.0822)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0705 (0.0819)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0747 (0.0819)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0790 (0.0819)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0758 (0.0816)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0680 (0.0813)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0660 (0.0812)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0731 (0.0811)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0790 (0.0814)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0738 (0.0811)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0669 (0.0809)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0669 (0.0807)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0670 (0.0806)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0660 (0.0803)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0688 (0.0801)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0688 (0.0799)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0687 (0.0798)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0658 (0.0795)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0710 (0.0795)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0755 (0.0795)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0873 (0.0798)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0938 (0.0799)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0884 (0.0802)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0843 (0.0801)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0833 (0.0803)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0833 (0.0803)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0806 (0.0803)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0775 (0.0805)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0840 (0.0807)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0805 (0.0806)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0772 (0.0807)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0765 (0.0807)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:49] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0765 (0.0807)\n",
      "Valid: [epoch:49]  [ 0/14]  eta: 0:00:14  loss: 0.0613 (0.0613)  time: 1.0064  data: 0.3727  max mem: 39763\n",
      "Valid: [epoch:49]  [13/14]  eta: 0:00:00  loss: 0.0613 (0.0635)  time: 0.1140  data: 0.0267  max mem: 39763\n",
      "Valid: [epoch:49] Total time: 0:00:01 (0.1228 s / it)\n",
      "Averaged stats: loss: 0.0613 (0.0635)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_49_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.063%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:50]  [  0/689]  eta: 0:11:42  lr: 0.000100  loss: 0.0584 (0.0584)  time: 1.0194  data: 0.5369  max mem: 39763\n",
      "Train: [epoch:50]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0687 (0.0703)  time: 1.5266  data: 0.0489  max mem: 39763\n",
      "Train: [epoch:50]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0691 (0.0709)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0727 (0.0733)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0727 (0.0729)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0704 (0.0729)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0722 (0.0725)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0723 (0.0741)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0774 (0.0745)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0763 (0.0748)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0756 (0.0752)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0737 (0.0756)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0740 (0.0757)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0740 (0.0760)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0762 (0.0763)  time: 1.5779  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:50]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0736 (0.0759)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0703 (0.0758)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0697 (0.0755)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0653 (0.0749)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0653 (0.0746)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0696 (0.0747)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0696 (0.0743)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0714 (0.0747)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0732 (0.0746)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0707 (0.0746)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0752 (0.0750)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0760 (0.0749)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0756 (0.0752)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0804 (0.0757)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0827 (0.0758)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0750 (0.0757)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0710 (0.0755)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0648 (0.0752)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0657 (0.0752)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0694 (0.0750)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0719 (0.0752)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0709 (0.0749)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0657 (0.0748)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0704 (0.0747)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0751 (0.0749)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0774 (0.0750)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0775 (0.0751)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0686 (0.0749)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0678 (0.0748)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0736 (0.0751)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0812 (0.0751)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0714 (0.0753)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0779 (0.0753)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0769 (0.0754)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0729 (0.0755)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0704 (0.0755)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0697 (0.0755)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0690 (0.0755)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0737 (0.0756)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0890 (0.0761)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0847 (0.0763)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0827 (0.0765)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0784 (0.0766)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0702 (0.0764)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0669 (0.0764)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0660 (0.0763)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0727 (0.0766)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0751 (0.0767)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0713 (0.0766)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0712 (0.0765)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0755 (0.0768)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0773 (0.0769)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0799 (0.0770)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0832 (0.0771)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0836 (0.0771)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:50] Total time: 0:18:07 (1.5782 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0836 (0.0771)\n",
      "Valid: [epoch:50]  [ 0/14]  eta: 0:00:14  loss: 0.0706 (0.0706)  time: 1.0343  data: 0.3645  max mem: 39763\n",
      "Valid: [epoch:50]  [13/14]  eta: 0:00:00  loss: 0.0694 (0.0704)  time: 0.1159  data: 0.0261  max mem: 39763\n",
      "Valid: [epoch:50] Total time: 0:00:01 (0.1238 s / it)\n",
      "Averaged stats: loss: 0.0694 (0.0704)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_50_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.070%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:51]  [  0/689]  eta: 0:11:56  lr: 0.000100  loss: 0.0626 (0.0626)  time: 1.0395  data: 0.5605  max mem: 39763\n",
      "Train: [epoch:51]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0749 (0.0747)  time: 1.5278  data: 0.0510  max mem: 39763\n",
      "Train: [epoch:51]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0749 (0.0803)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0998 (0.0912)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0900 (0.0900)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0816 (0.0891)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0798 (0.0876)  time: 1.5787  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:51]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0747 (0.0858)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0733 (0.0849)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0755 (0.0845)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0834 (0.0851)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0891 (0.0859)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0763 (0.0853)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0746 (0.0848)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0770 (0.0848)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0768 (0.0846)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0734 (0.0841)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0726 (0.0837)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0686 (0.0830)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0694 (0.0828)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0769 (0.0831)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0787 (0.0829)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0750 (0.0825)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0750 (0.0827)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0860 (0.0835)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.1010 (0.0850)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0911 (0.0851)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0798 (0.0848)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0782 (0.0846)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0813 (0.0845)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0779 (0.0847)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0707 (0.0844)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0711 (0.0845)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0742 (0.0843)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0718 (0.0840)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0786 (0.0842)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0740 (0.0839)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0738 (0.0839)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0823 (0.0841)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0858 (0.0845)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0773 (0.0845)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0839 (0.0851)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1048 (0.0859)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1078 (0.0875)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1187 (0.0885)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.1181 (0.0891)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.1178 (0.0899)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0868 (0.0896)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0742 (0.0893)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0749 (0.0892)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0914 (0.0897)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1070 (0.0903)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0973 (0.0905)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0832 (0.0904)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0785 (0.0902)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0738 (0.0899)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0716 (0.0898)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0746 (0.0896)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0683 (0.0893)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0712 (0.0891)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0760 (0.0890)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0794 (0.0890)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0758 (0.0889)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0758 (0.0887)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0679 (0.0883)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0659 (0.0880)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0680 (0.0878)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0711 (0.0875)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0730 (0.0875)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0741 (0.0874)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:51] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0741 (0.0874)\n",
      "Valid: [epoch:51]  [ 0/14]  eta: 0:00:14  loss: 0.0650 (0.0650)  time: 1.0161  data: 0.3434  max mem: 39763\n",
      "Valid: [epoch:51]  [13/14]  eta: 0:00:00  loss: 0.0678 (0.0702)  time: 0.1145  data: 0.0246  max mem: 39763\n",
      "Valid: [epoch:51] Total time: 0:00:01 (0.1225 s / it)\n",
      "Averaged stats: loss: 0.0678 (0.0702)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_51_input_n_20.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the network on the 14 valid images: 0.070%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:52]  [  0/689]  eta: 0:11:23  lr: 0.000100  loss: 0.0786 (0.0786)  time: 0.9917  data: 0.5119  max mem: 39763\n",
      "Train: [epoch:52]  [ 10/689]  eta: 0:17:14  lr: 0.000100  loss: 0.0721 (0.0751)  time: 1.5238  data: 0.0466  max mem: 39763\n",
      "Train: [epoch:52]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0760 (0.0835)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0834 (0.0837)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0744 (0.0815)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0744 (0.0812)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0744 (0.0804)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0725 (0.0792)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0716 (0.0801)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0790 (0.0819)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0790 (0.0819)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0754 (0.0816)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0750 (0.0815)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0757 (0.0810)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0776 (0.0811)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0776 (0.0806)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0744 (0.0804)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0742 (0.0801)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0727 (0.0800)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0714 (0.0796)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0714 (0.0792)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0765 (0.0794)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0846 (0.0798)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0880 (0.0797)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0759 (0.0796)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0736 (0.0793)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0752 (0.0792)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0722 (0.0789)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0692 (0.0786)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0692 (0.0783)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0699 (0.0782)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0703 (0.0778)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0735 (0.0779)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0796 (0.0780)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0715 (0.0777)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0705 (0.0775)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0710 (0.0773)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0695 (0.0772)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0695 (0.0771)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0704 (0.0771)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0727 (0.0769)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0689 (0.0768)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0655 (0.0765)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0655 (0.0764)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0702 (0.0764)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0761 (0.0764)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0759 (0.0764)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0700 (0.0762)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0673 (0.0761)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0670 (0.0760)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0700 (0.0760)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0676 (0.0758)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0676 (0.0757)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0689 (0.0756)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0705 (0.0756)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0726 (0.0755)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0726 (0.0754)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0734 (0.0755)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0742 (0.0754)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0710 (0.0753)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0695 (0.0752)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0693 (0.0751)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0693 (0.0751)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0733 (0.0752)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0765 (0.0753)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0753 (0.0753)  time: 1.5790  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:52]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0729 (0.0753)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0746 (0.0757)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0718 (0.0756)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0680 (0.0755)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:52] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0680 (0.0755)\n",
      "Valid: [epoch:52]  [ 0/14]  eta: 0:00:14  loss: 0.0618 (0.0618)  time: 1.0447  data: 0.4079  max mem: 39763\n",
      "Valid: [epoch:52]  [13/14]  eta: 0:00:00  loss: 0.0701 (0.0700)  time: 0.1166  data: 0.0292  max mem: 39763\n",
      "Valid: [epoch:52] Total time: 0:00:01 (0.1254 s / it)\n",
      "Averaged stats: loss: 0.0701 (0.0700)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_52_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.070%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:53]  [  0/689]  eta: 0:12:08  lr: 0.000100  loss: 0.0726 (0.0726)  time: 1.0575  data: 0.5790  max mem: 39763\n",
      "Train: [epoch:53]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0729 (0.0772)  time: 1.5284  data: 0.0527  max mem: 39763\n",
      "Train: [epoch:53]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0734 (0.0772)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0734 (0.0766)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0716 (0.0755)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0755 (0.0771)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0755 (0.0761)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0706 (0.0758)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0738 (0.0761)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0728 (0.0756)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0717 (0.0753)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0719 (0.0754)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0714 (0.0751)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0714 (0.0754)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0721 (0.0752)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0711 (0.0753)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0713 (0.0753)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0713 (0.0752)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0765 (0.0755)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0726 (0.0753)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0697 (0.0752)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0717 (0.0751)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0718 (0.0750)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0712 (0.0749)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0717 (0.0749)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0683 (0.0748)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0674 (0.0747)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0674 (0.0745)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0671 (0.0743)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0674 (0.0742)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0717 (0.0742)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0711 (0.0741)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0671 (0.0739)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0734 (0.0739)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0739 (0.0739)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0700 (0.0739)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0718 (0.0740)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0728 (0.0740)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0774 (0.0742)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0858 (0.0749)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0997 (0.0755)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0891 (0.0758)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0749 (0.0756)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0710 (0.0757)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0722 (0.0756)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0748 (0.0757)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0778 (0.0763)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0857 (0.0765)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0748 (0.0764)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0721 (0.0763)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0721 (0.0763)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0745 (0.0764)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0802 (0.0768)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1006 (0.0774)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0867 (0.0774)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0711 (0.0772)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0717 (0.0773)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0763 (0.0773)  time: 1.5792  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:53]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0791 (0.0775)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0796 (0.0775)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0713 (0.0773)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0689 (0.0773)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0715 (0.0772)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0687 (0.0771)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0688 (0.0771)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0832 (0.0775)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0904 (0.0776)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0706 (0.0775)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0693 (0.0774)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0690 (0.0773)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:53] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0690 (0.0773)\n",
      "Valid: [epoch:53]  [ 0/14]  eta: 0:00:14  loss: 0.0688 (0.0688)  time: 1.0217  data: 0.3675  max mem: 39763\n",
      "Valid: [epoch:53]  [13/14]  eta: 0:00:00  loss: 0.0688 (0.0690)  time: 0.1151  data: 0.0263  max mem: 39763\n",
      "Valid: [epoch:53] Total time: 0:00:01 (0.1241 s / it)\n",
      "Averaged stats: loss: 0.0688 (0.0690)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_53_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.069%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:54]  [  0/689]  eta: 0:11:27  lr: 0.000100  loss: 0.0780 (0.0780)  time: 0.9982  data: 0.5189  max mem: 39763\n",
      "Train: [epoch:54]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0749 (0.0736)  time: 1.5252  data: 0.0472  max mem: 39763\n",
      "Train: [epoch:54]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0709 (0.0711)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0679 (0.0715)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0679 (0.0711)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0680 (0.0719)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0721 (0.0726)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0716 (0.0722)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0695 (0.0728)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0748 (0.0743)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0850 (0.0754)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0793 (0.0756)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0734 (0.0753)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0737 (0.0754)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0795 (0.0760)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0763 (0.0756)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0688 (0.0758)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0814 (0.0762)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0755 (0.0760)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0720 (0.0760)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0708 (0.0757)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0740 (0.0764)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0733 (0.0761)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0704 (0.0760)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0761 (0.0763)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0825 (0.0766)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0764 (0.0763)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0684 (0.0763)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0731 (0.0763)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0720 (0.0762)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0757 (0.0764)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0743 (0.0762)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0732 (0.0762)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0774 (0.0763)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0777 (0.0763)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0710 (0.0763)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0691 (0.0762)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0683 (0.0760)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0677 (0.0759)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0692 (0.0758)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0692 (0.0757)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0723 (0.0758)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0812 (0.0761)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0860 (0.0765)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0842 (0.0766)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0754 (0.0767)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0802 (0.0770)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0844 (0.0773)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0816 (0.0774)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0815 (0.0774)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:54]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0825 (0.0776)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0845 (0.0776)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0759 (0.0776)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0713 (0.0775)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0734 (0.0775)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0734 (0.0774)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0707 (0.0773)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0690 (0.0772)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0737 (0.0772)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0748 (0.0772)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0749 (0.0773)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0775 (0.0774)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0741 (0.0774)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0778 (0.0774)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0737 (0.0773)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0739 (0.0773)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0739 (0.0774)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0676 (0.0773)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0742 (0.0776)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0835 (0.0777)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:54] Total time: 0:18:07 (1.5782 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0835 (0.0777)\n",
      "Valid: [epoch:54]  [ 0/14]  eta: 0:00:14  loss: 0.0759 (0.0759)  time: 1.0135  data: 0.3801  max mem: 39763\n",
      "Valid: [epoch:54]  [13/14]  eta: 0:00:00  loss: 0.0720 (0.0711)  time: 0.1144  data: 0.0272  max mem: 39763\n",
      "Valid: [epoch:54] Total time: 0:00:01 (0.1234 s / it)\n",
      "Averaged stats: loss: 0.0720 (0.0711)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_54_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.071%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:55]  [  0/689]  eta: 0:11:43  lr: 0.000100  loss: 0.0650 (0.0650)  time: 1.0217  data: 0.5455  max mem: 39763\n",
      "Train: [epoch:55]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0811 (0.0825)  time: 1.5256  data: 0.0497  max mem: 39763\n",
      "Train: [epoch:55]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0803 (0.0825)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0804 (0.0845)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0833 (0.0834)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0752 (0.0821)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0729 (0.0824)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0926 (0.0858)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0977 (0.0870)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0921 (0.0883)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0921 (0.0886)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0904 (0.0884)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0837 (0.0884)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0825 (0.0883)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0818 (0.0879)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0832 (0.0883)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0766 (0.0877)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0761 (0.0875)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0733 (0.0868)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0694 (0.0860)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0705 (0.0853)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0752 (0.0850)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0788 (0.0846)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0791 (0.0843)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0730 (0.0839)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0727 (0.0835)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0705 (0.0831)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0682 (0.0828)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0701 (0.0823)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0645 (0.0817)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0707 (0.0815)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0722 (0.0811)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0690 (0.0808)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0669 (0.0805)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0664 (0.0801)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0677 (0.0799)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0711 (0.0797)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0678 (0.0795)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0708 (0.0793)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0729 (0.0791)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0709 (0.0790)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0708 (0.0789)  time: 1.5786  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:55]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0714 (0.0787)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0741 (0.0788)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0748 (0.0787)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0712 (0.0785)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0747 (0.0789)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0928 (0.0792)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0838 (0.0793)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0822 (0.0794)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0748 (0.0793)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0736 (0.0792)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0718 (0.0791)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0718 (0.0790)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0678 (0.0788)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0725 (0.0788)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0725 (0.0787)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0709 (0.0786)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0679 (0.0784)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0693 (0.0783)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0706 (0.0782)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0706 (0.0781)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0687 (0.0779)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0663 (0.0778)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0690 (0.0778)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0742 (0.0777)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0711 (0.0777)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0664 (0.0775)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0669 (0.0774)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0656 (0.0773)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:55] Total time: 0:18:07 (1.5782 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0656 (0.0773)\n",
      "Valid: [epoch:55]  [ 0/14]  eta: 0:00:13  loss: 0.0654 (0.0654)  time: 0.9858  data: 0.5683  max mem: 39763\n",
      "Valid: [epoch:55]  [13/14]  eta: 0:00:00  loss: 0.0686 (0.0688)  time: 0.1124  data: 0.0406  max mem: 39763\n",
      "Valid: [epoch:55] Total time: 0:00:01 (0.1215 s / it)\n",
      "Averaged stats: loss: 0.0686 (0.0688)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_55_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.069%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:56]  [  0/689]  eta: 0:11:51  lr: 0.000100  loss: 0.0825 (0.0825)  time: 1.0331  data: 0.5536  max mem: 39763\n",
      "Train: [epoch:56]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0743 (0.0770)  time: 1.5277  data: 0.0504  max mem: 39763\n",
      "Train: [epoch:56]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0705 (0.0735)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0705 (0.0742)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0737 (0.0740)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0708 (0.0737)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0705 (0.0736)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0730 (0.0738)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0730 (0.0738)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0709 (0.0736)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0701 (0.0736)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0765 (0.0749)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0765 (0.0750)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0748 (0.0751)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0744 (0.0750)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0754 (0.0755)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0804 (0.0762)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0830 (0.0765)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0797 (0.0764)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0733 (0.0764)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0687 (0.0761)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0713 (0.0760)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0768 (0.0764)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0796 (0.0764)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0753 (0.0763)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0737 (0.0763)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0737 (0.0762)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0738 (0.0762)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0750 (0.0763)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0800 (0.0766)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0756 (0.0763)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0677 (0.0761)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0677 (0.0759)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0746 (0.0760)  time: 1.5789  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:56]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0794 (0.0762)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0762 (0.0761)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0747 (0.0760)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0759 (0.0761)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0770 (0.0760)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0736 (0.0760)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0797 (0.0763)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0791 (0.0763)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0725 (0.0761)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0705 (0.0760)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0705 (0.0759)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0700 (0.0759)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0734 (0.0760)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0770 (0.0761)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0741 (0.0760)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0673 (0.0758)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0680 (0.0758)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0705 (0.0757)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0724 (0.0758)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0784 (0.0760)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0847 (0.0762)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0781 (0.0762)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0759 (0.0761)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0688 (0.0760)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0712 (0.0760)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0684 (0.0759)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0656 (0.0758)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0711 (0.0759)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0777 (0.0760)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0777 (0.0760)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0822 (0.0762)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0806 (0.0763)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0746 (0.0763)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0768 (0.0764)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0836 (0.0765)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0822 (0.0766)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:56] Total time: 0:18:07 (1.5782 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0822 (0.0766)\n",
      "Valid: [epoch:56]  [ 0/14]  eta: 0:00:14  loss: 0.0719 (0.0719)  time: 1.0216  data: 0.3485  max mem: 39763\n",
      "Valid: [epoch:56]  [13/14]  eta: 0:00:00  loss: 0.0698 (0.0693)  time: 0.1150  data: 0.0249  max mem: 39763\n",
      "Valid: [epoch:56] Total time: 0:00:01 (0.1225 s / it)\n",
      "Averaged stats: loss: 0.0698 (0.0693)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_56_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.069%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:57]  [  0/689]  eta: 0:12:44  lr: 0.000100  loss: 0.0793 (0.0793)  time: 1.1090  data: 0.6299  max mem: 39763\n",
      "Train: [epoch:57]  [ 10/689]  eta: 0:17:21  lr: 0.000100  loss: 0.0757 (0.0775)  time: 1.5338  data: 0.0573  max mem: 39763\n",
      "Train: [epoch:57]  [ 20/689]  eta: 0:17:20  lr: 0.000100  loss: 0.0729 (0.0745)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [ 30/689]  eta: 0:17:09  lr: 0.000100  loss: 0.0736 (0.0769)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [ 40/689]  eta: 0:16:56  lr: 0.000100  loss: 0.0777 (0.0773)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [ 50/689]  eta: 0:16:42  lr: 0.000100  loss: 0.0734 (0.0772)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0699 (0.0755)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0701 (0.0758)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0787 (0.0761)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0749 (0.0759)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0712 (0.0753)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0712 (0.0753)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0758 (0.0758)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0761 (0.0761)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0761 (0.0770)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0782 (0.0776)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0851 (0.0780)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0794 (0.0781)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0786 (0.0787)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0796 (0.0794)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0792 (0.0793)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0734 (0.0790)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0759 (0.0796)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0808 (0.0795)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0748 (0.0792)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0676 (0.0791)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:57]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0752 (0.0792)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0754 (0.0791)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0700 (0.0789)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0718 (0.0788)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0718 (0.0784)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0725 (0.0784)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0835 (0.0788)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0803 (0.0788)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0803 (0.0790)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0792 (0.0789)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0700 (0.0786)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0699 (0.0784)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0699 (0.0782)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0687 (0.0782)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0689 (0.0780)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0689 (0.0780)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0738 (0.0778)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0689 (0.0777)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0689 (0.0775)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0710 (0.0775)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0729 (0.0773)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0745 (0.0774)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0783 (0.0774)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0739 (0.0774)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0735 (0.0773)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0733 (0.0773)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0748 (0.0773)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0710 (0.0772)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0685 (0.0771)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0706 (0.0771)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0711 (0.0772)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0723 (0.0771)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0723 (0.0771)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0739 (0.0770)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0721 (0.0770)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0757 (0.0771)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0730 (0.0770)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0730 (0.0770)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0768 (0.0769)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0735 (0.0769)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0690 (0.0768)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0705 (0.0767)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0708 (0.0767)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0722 (0.0767)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:57] Total time: 0:18:07 (1.5783 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0722 (0.0767)\n",
      "Valid: [epoch:57]  [ 0/14]  eta: 0:00:14  loss: 0.0763 (0.0763)  time: 1.0186  data: 0.3883  max mem: 39763\n",
      "Valid: [epoch:57]  [13/14]  eta: 0:00:00  loss: 0.0713 (0.0718)  time: 0.1147  data: 0.0278  max mem: 39763\n",
      "Valid: [epoch:57] Total time: 0:00:01 (0.1240 s / it)\n",
      "Averaged stats: loss: 0.0713 (0.0718)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_57_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.072%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:58]  [  0/689]  eta: 0:11:39  lr: 0.000100  loss: 0.0840 (0.0840)  time: 1.0151  data: 0.5357  max mem: 39763\n",
      "Train: [epoch:58]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0927 (0.0969)  time: 1.5266  data: 0.0488  max mem: 39763\n",
      "Train: [epoch:58]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0891 (0.0957)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0860 (0.0917)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0796 (0.0891)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0732 (0.0864)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0732 (0.0845)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0718 (0.0829)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0767 (0.0826)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0777 (0.0820)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0747 (0.0814)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0719 (0.0811)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0737 (0.0807)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0712 (0.0803)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0701 (0.0795)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0732 (0.0793)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0732 (0.0789)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0716 (0.0787)  time: 1.5787  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:58]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0731 (0.0784)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0712 (0.0783)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0712 (0.0780)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0748 (0.0782)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0753 (0.0779)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0729 (0.0777)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0729 (0.0776)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0799 (0.0779)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0867 (0.0783)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0830 (0.0784)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0805 (0.0786)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0809 (0.0787)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0815 (0.0790)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0771 (0.0787)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0696 (0.0786)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0722 (0.0784)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0701 (0.0783)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0677 (0.0781)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0687 (0.0779)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0692 (0.0778)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0722 (0.0777)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0722 (0.0778)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0764 (0.0778)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0757 (0.0778)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0751 (0.0779)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0810 (0.0779)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0778 (0.0779)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0792 (0.0780)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0880 (0.0784)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0872 (0.0784)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0742 (0.0784)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0758 (0.0783)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0730 (0.0783)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0716 (0.0783)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0795 (0.0783)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0795 (0.0783)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0787 (0.0784)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0822 (0.0787)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0875 (0.0788)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0821 (0.0788)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0748 (0.0788)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0787 (0.0789)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0776 (0.0788)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0711 (0.0787)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0696 (0.0785)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0717 (0.0784)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0733 (0.0784)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0784 (0.0786)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0782 (0.0784)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0693 (0.0783)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0705 (0.0783)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0765 (0.0783)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:58] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0765 (0.0783)\n",
      "Valid: [epoch:58]  [ 0/14]  eta: 0:00:14  loss: 0.0612 (0.0612)  time: 1.0220  data: 0.3913  max mem: 39763\n",
      "Valid: [epoch:58]  [13/14]  eta: 0:00:00  loss: 0.0693 (0.0685)  time: 0.1150  data: 0.0280  max mem: 39763\n",
      "Valid: [epoch:58] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.0693 (0.0685)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_58_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.068%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:59]  [  0/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0801 (0.0801)  time: 1.0263  data: 0.5517  max mem: 39763\n",
      "Train: [epoch:59]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0760 (0.0747)  time: 1.5262  data: 0.0502  max mem: 39763\n",
      "Train: [epoch:59]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0705 (0.0721)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0678 (0.0727)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0697 (0.0726)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0693 (0.0723)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0695 (0.0723)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0702 (0.0722)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0722 (0.0727)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0699 (0.0723)  time: 1.5789  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:59]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0686 (0.0721)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0706 (0.0728)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0745 (0.0731)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0765 (0.0732)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0769 (0.0738)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0767 (0.0740)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0732 (0.0745)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0762 (0.0747)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0790 (0.0752)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0811 (0.0756)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0775 (0.0756)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0775 (0.0760)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0831 (0.0763)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0825 (0.0766)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0801 (0.0769)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0836 (0.0771)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0755 (0.0770)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0724 (0.0769)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0760 (0.0769)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0748 (0.0768)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0729 (0.0767)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0701 (0.0765)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0682 (0.0764)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0714 (0.0763)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0739 (0.0764)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0766 (0.0765)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0767 (0.0764)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0716 (0.0764)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0680 (0.0763)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0683 (0.0761)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0731 (0.0762)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0778 (0.0762)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0686 (0.0761)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0685 (0.0760)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0694 (0.0759)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0694 (0.0757)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0712 (0.0757)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0737 (0.0758)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0733 (0.0757)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0727 (0.0757)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0717 (0.0757)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0710 (0.0757)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0675 (0.0756)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0699 (0.0757)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0756 (0.0757)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0763 (0.0758)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0778 (0.0757)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0700 (0.0757)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0700 (0.0757)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0710 (0.0756)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0705 (0.0756)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0705 (0.0755)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0723 (0.0755)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0736 (0.0755)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0783 (0.0755)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0773 (0.0756)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0811 (0.0757)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0772 (0.0757)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0722 (0.0757)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0722 (0.0756)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:59] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0722 (0.0756)\n",
      "Valid: [epoch:59]  [ 0/14]  eta: 0:00:14  loss: 0.0644 (0.0644)  time: 1.0073  data: 0.6043  max mem: 39763\n",
      "Valid: [epoch:59]  [13/14]  eta: 0:00:00  loss: 0.0677 (0.0671)  time: 0.1139  data: 0.0432  max mem: 39763\n",
      "Valid: [epoch:59] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.0677 (0.0671)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_59_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.067%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:60]  [  0/689]  eta: 0:11:44  lr: 0.000100  loss: 0.0713 (0.0713)  time: 1.0219  data: 0.5420  max mem: 39763\n",
      "Train: [epoch:60]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0775 (0.0789)  time: 1.5266  data: 0.0494  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:60]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0775 (0.0797)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0775 (0.0790)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0775 (0.0785)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0707 (0.0774)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0748 (0.0777)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0774 (0.0772)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0769 (0.0779)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0782 (0.0780)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0762 (0.0782)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0791 (0.0786)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0728 (0.0782)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0723 (0.0778)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0743 (0.0777)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0761 (0.0776)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0724 (0.0776)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0724 (0.0773)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0697 (0.0770)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0726 (0.0768)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0747 (0.0768)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0706 (0.0764)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0690 (0.0762)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0730 (0.0761)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0794 (0.0767)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0779 (0.0767)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0724 (0.0766)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0743 (0.0767)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0790 (0.0771)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0759 (0.0769)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0708 (0.0767)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0708 (0.0765)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0719 (0.0766)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0743 (0.0767)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0710 (0.0765)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0717 (0.0767)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0803 (0.0767)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0720 (0.0766)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0710 (0.0765)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0714 (0.0764)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0744 (0.0763)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0744 (0.0764)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0760 (0.0764)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0760 (0.0765)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0753 (0.0765)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0738 (0.0765)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0788 (0.0765)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0799 (0.0765)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0730 (0.0765)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0696 (0.0764)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0674 (0.0763)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0699 (0.0762)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0717 (0.0761)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0725 (0.0761)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0778 (0.0762)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0784 (0.0763)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0730 (0.0763)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0718 (0.0763)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0688 (0.0762)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0692 (0.0762)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0810 (0.0763)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0816 (0.0764)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0730 (0.0763)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0730 (0.0763)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0768 (0.0763)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0763 (0.0763)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0769 (0.0763)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0789 (0.0765)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0789 (0.0766)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:60]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0782 (0.0766)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:60] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0782 (0.0766)\n",
      "Valid: [epoch:60]  [ 0/14]  eta: 0:00:14  loss: 0.0647 (0.0647)  time: 1.0194  data: 0.3467  max mem: 39763\n",
      "Valid: [epoch:60]  [13/14]  eta: 0:00:00  loss: 0.0686 (0.0687)  time: 0.1148  data: 0.0248  max mem: 39763\n",
      "Valid: [epoch:60] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.0686 (0.0687)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_60_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.069%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:61]  [  0/689]  eta: 0:12:10  lr: 0.000100  loss: 0.0666 (0.0666)  time: 1.0609  data: 0.5845  max mem: 39763\n",
      "Train: [epoch:61]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0800 (0.0799)  time: 1.5282  data: 0.0532  max mem: 39763\n",
      "Train: [epoch:61]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0805 (0.0825)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0805 (0.0897)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0847 (0.0888)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0849 (0.0880)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0808 (0.0875)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0801 (0.0860)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0744 (0.0846)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0700 (0.0833)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0745 (0.0829)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0754 (0.0821)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0770 (0.0825)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0885 (0.0836)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0897 (0.0843)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0844 (0.0847)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0876 (0.0851)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0847 (0.0853)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0883 (0.0864)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0951 (0.0867)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0860 (0.0874)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0860 (0.0877)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0920 (0.0885)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0845 (0.0881)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0840 (0.0881)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0858 (0.0884)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0852 (0.0881)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0822 (0.0877)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0752 (0.0872)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0755 (0.0869)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0756 (0.0864)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0739 (0.0861)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0766 (0.0860)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0792 (0.0859)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0792 (0.0857)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0777 (0.0855)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0751 (0.0852)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0741 (0.0849)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0741 (0.0846)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0709 (0.0844)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0753 (0.0843)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0774 (0.0841)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0786 (0.0840)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0826 (0.0840)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0801 (0.0840)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0789 (0.0839)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0772 (0.0838)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0758 (0.0837)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0753 (0.0836)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0716 (0.0833)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0716 (0.0831)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0715 (0.0829)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0747 (0.0829)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0795 (0.0828)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0775 (0.0827)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0756 (0.0827)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0789 (0.0826)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0801 (0.0829)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0826 (0.0830)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0869 (0.0831)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0869 (0.0833)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:61]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0914 (0.0836)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0977 (0.0838)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0936 (0.0840)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0836 (0.0840)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0760 (0.0839)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0791 (0.0839)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0844 (0.0840)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0864 (0.0842)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0864 (0.0842)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:61] Total time: 0:18:06 (1.5775 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0864 (0.0842)\n",
      "Valid: [epoch:61]  [ 0/14]  eta: 0:00:14  loss: 0.0765 (0.0765)  time: 1.0211  data: 0.3728  max mem: 39763\n",
      "Valid: [epoch:61]  [13/14]  eta: 0:00:00  loss: 0.0748 (0.0736)  time: 0.1149  data: 0.0267  max mem: 39763\n",
      "Valid: [epoch:61] Total time: 0:00:01 (0.1242 s / it)\n",
      "Averaged stats: loss: 0.0748 (0.0736)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_61_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.074%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:62]  [  0/689]  eta: 0:11:52  lr: 0.000100  loss: 0.0701 (0.0701)  time: 1.0335  data: 0.5539  max mem: 39763\n",
      "Train: [epoch:62]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0734 (0.0786)  time: 1.5271  data: 0.0504  max mem: 39763\n",
      "Train: [epoch:62]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0734 (0.0777)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0776 (0.0804)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0797 (0.0803)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0783 (0.0794)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0744 (0.0786)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0740 (0.0779)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0728 (0.0774)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0760 (0.0779)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0824 (0.0786)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0815 (0.0786)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0769 (0.0783)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0769 (0.0784)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0783 (0.0785)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0812 (0.0786)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0790 (0.0787)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0767 (0.0787)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0774 (0.0792)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0811 (0.0793)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0806 (0.0796)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0778 (0.0794)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0721 (0.0790)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0750 (0.0789)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0820 (0.0794)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0836 (0.0793)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0786 (0.0796)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0819 (0.0798)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0819 (0.0802)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0892 (0.0807)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0887 (0.0807)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0726 (0.0805)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0731 (0.0806)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0815 (0.0808)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0797 (0.0807)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0769 (0.0806)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0794 (0.0805)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0757 (0.0803)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0742 (0.0802)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0742 (0.0802)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0775 (0.0802)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0755 (0.0801)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0761 (0.0803)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0817 (0.0804)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0765 (0.0804)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0721 (0.0803)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0736 (0.0803)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0778 (0.0803)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0778 (0.0803)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0796 (0.0804)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0801 (0.0804)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0809 (0.0804)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0809 (0.0806)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:62]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0775 (0.0804)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0752 (0.0805)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0784 (0.0804)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0772 (0.0804)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0780 (0.0804)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0781 (0.0803)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0733 (0.0804)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0841 (0.0804)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0817 (0.0804)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0744 (0.0804)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0759 (0.0803)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0780 (0.0804)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0803 (0.0805)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0807 (0.0805)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0796 (0.0805)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0767 (0.0805)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0767 (0.0804)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:62] Total time: 0:18:06 (1.5775 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0767 (0.0804)\n",
      "Valid: [epoch:62]  [ 0/14]  eta: 0:00:14  loss: 0.0729 (0.0729)  time: 1.0130  data: 0.3871  max mem: 39763\n",
      "Valid: [epoch:62]  [13/14]  eta: 0:00:00  loss: 0.0729 (0.0718)  time: 0.1144  data: 0.0277  max mem: 39763\n",
      "Valid: [epoch:62] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.0729 (0.0718)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_62_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.072%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:63]  [  0/689]  eta: 0:12:06  lr: 0.000100  loss: 0.0762 (0.0762)  time: 1.0551  data: 0.5787  max mem: 39763\n",
      "Train: [epoch:63]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0762 (0.0842)  time: 1.5272  data: 0.0527  max mem: 39763\n",
      "Train: [epoch:63]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0752 (0.0808)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0773 (0.0832)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0813 (0.0826)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0803 (0.0820)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0744 (0.0809)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0750 (0.0813)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0765 (0.0807)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.0778 (0.0813)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0830 (0.0816)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0845 (0.0822)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0864 (0.0826)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0880 (0.0832)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0838 (0.0832)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0813 (0.0831)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0821 (0.0834)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.0871 (0.0838)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0883 (0.0843)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0906 (0.0847)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.0850 (0.0846)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0836 (0.0847)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0810 (0.0844)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.0810 (0.0845)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0795 (0.0842)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0737 (0.0839)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0716 (0.0837)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0766 (0.0837)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0793 (0.0835)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0769 (0.0835)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.0852 (0.0837)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0821 (0.0836)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0770 (0.0834)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0797 (0.0834)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0782 (0.0831)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0739 (0.0830)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0759 (0.0827)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0759 (0.0827)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0798 (0.0826)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0773 (0.0825)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0775 (0.0826)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0809 (0.0824)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [420/689]  eta: 0:07:03  lr: 0.000100  loss: 0.0749 (0.0824)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0870 (0.0826)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0775 (0.0824)  time: 1.5779  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:63]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0709 (0.0821)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [460/689]  eta: 0:06:00  lr: 0.000100  loss: 0.0737 (0.0821)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0800 (0.0819)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0756 (0.0818)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0756 (0.0817)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.0743 (0.0815)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0730 (0.0814)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0727 (0.0813)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0727 (0.0811)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0724 (0.0810)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0743 (0.0809)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0743 (0.0808)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0798 (0.0809)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0766 (0.0808)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0731 (0.0806)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0755 (0.0807)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0768 (0.0806)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0752 (0.0805)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0773 (0.0806)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0762 (0.0805)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0762 (0.0805)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0794 (0.0805)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0785 (0.0804)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0785 (0.0805)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0751 (0.0804)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:63] Total time: 0:18:06 (1.5769 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0751 (0.0804)\n",
      "Valid: [epoch:63]  [ 0/14]  eta: 0:00:14  loss: 0.0730 (0.0730)  time: 1.0111  data: 0.3480  max mem: 39763\n",
      "Valid: [epoch:63]  [13/14]  eta: 0:00:00  loss: 0.0730 (0.0719)  time: 0.1142  data: 0.0249  max mem: 39763\n",
      "Valid: [epoch:63] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.0730 (0.0719)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_63_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.072%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:64]  [  0/689]  eta: 0:11:54  lr: 0.000100  loss: 0.0825 (0.0825)  time: 1.0374  data: 0.5577  max mem: 39763\n",
      "Train: [epoch:64]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0842 (0.0838)  time: 1.5278  data: 0.0508  max mem: 39763\n",
      "Train: [epoch:64]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0882 (0.0878)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0823 (0.0856)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0811 (0.0844)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0752 (0.0827)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0752 (0.0817)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0712 (0.0804)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0737 (0.0806)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0784 (0.0810)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0772 (0.0811)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0772 (0.0811)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0752 (0.0810)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0749 (0.0811)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0838 (0.0811)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0824 (0.0816)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0889 (0.0826)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.0820 (0.0826)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0772 (0.0825)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0775 (0.0825)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.0775 (0.0822)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0764 (0.0822)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0832 (0.0824)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.0850 (0.0827)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0802 (0.0828)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0808 (0.0832)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0939 (0.0841)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0955 (0.0849)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0900 (0.0850)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0827 (0.0852)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.0910 (0.0854)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0910 (0.0857)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0908 (0.0859)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0890 (0.0859)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [340/689]  eta: 0:09:09  lr: 0.000100  loss: 0.0788 (0.0857)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0758 (0.0855)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0796 (0.0856)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:64]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0819 (0.0856)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0819 (0.0856)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0796 (0.0856)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0796 (0.0854)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0764 (0.0852)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0740 (0.0850)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0759 (0.0849)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0764 (0.0847)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0770 (0.0846)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0749 (0.0845)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0749 (0.0844)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0738 (0.0841)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0753 (0.0842)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.0790 (0.0840)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0780 (0.0839)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0773 (0.0839)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0793 (0.0838)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0815 (0.0838)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0756 (0.0836)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0724 (0.0835)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0759 (0.0835)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0788 (0.0834)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0767 (0.0833)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0776 (0.0832)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0798 (0.0832)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0781 (0.0832)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0777 (0.0832)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0779 (0.0831)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0779 (0.0831)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0777 (0.0830)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0770 (0.0829)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0745 (0.0829)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0736 (0.0828)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:64] Total time: 0:18:06 (1.5771 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0736 (0.0828)\n",
      "Valid: [epoch:64]  [ 0/14]  eta: 0:00:14  loss: 0.0610 (0.0610)  time: 1.0127  data: 0.3866  max mem: 39763\n",
      "Valid: [epoch:64]  [13/14]  eta: 0:00:00  loss: 0.0706 (0.0701)  time: 0.1143  data: 0.0277  max mem: 39763\n",
      "Valid: [epoch:64] Total time: 0:00:01 (0.1242 s / it)\n",
      "Averaged stats: loss: 0.0706 (0.0701)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_64_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.070%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:65]  [  0/689]  eta: 0:11:39  lr: 0.000100  loss: 0.0672 (0.0672)  time: 1.0152  data: 0.5382  max mem: 39763\n",
      "Train: [epoch:65]  [ 10/689]  eta: 0:17:14  lr: 0.000100  loss: 0.0791 (0.0794)  time: 1.5234  data: 0.0490  max mem: 39763\n",
      "Train: [epoch:65]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0768 (0.0770)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [ 30/689]  eta: 0:17:06  lr: 0.000100  loss: 0.0769 (0.0777)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [ 40/689]  eta: 0:16:53  lr: 0.000100  loss: 0.0783 (0.0788)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0815 (0.0783)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [ 60/689]  eta: 0:16:25  lr: 0.000100  loss: 0.0755 (0.0781)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [ 70/689]  eta: 0:16:10  lr: 0.000100  loss: 0.0799 (0.0791)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [ 80/689]  eta: 0:15:55  lr: 0.000100  loss: 0.0799 (0.0790)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.0759 (0.0790)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0795 (0.0793)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [110/689]  eta: 0:15:09  lr: 0.000100  loss: 0.0786 (0.0791)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0737 (0.0788)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0723 (0.0785)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0744 (0.0786)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [150/689]  eta: 0:14:07  lr: 0.000100  loss: 0.0778 (0.0787)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0787 (0.0789)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.0799 (0.0790)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0753 (0.0790)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0774 (0.0791)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.0794 (0.0792)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0775 (0.0791)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0744 (0.0789)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.0747 (0.0789)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0747 (0.0789)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0739 (0.0787)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0761 (0.0787)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [270/689]  eta: 0:10:59  lr: 0.000100  loss: 0.0763 (0.0787)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0797 (0.0791)  time: 1.5787  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:65]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0889 (0.0794)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.0846 (0.0796)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0762 (0.0796)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0852 (0.0799)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0824 (0.0798)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [340/689]  eta: 0:09:09  lr: 0.000100  loss: 0.0737 (0.0796)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0736 (0.0795)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0727 (0.0794)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0727 (0.0793)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0765 (0.0792)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0790 (0.0794)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0873 (0.0796)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0812 (0.0796)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0751 (0.0795)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0780 (0.0796)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0790 (0.0796)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0782 (0.0796)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0837 (0.0799)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0836 (0.0799)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0771 (0.0798)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0789 (0.0799)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0861 (0.0802)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0920 (0.0804)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0856 (0.0806)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0838 (0.0807)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0837 (0.0808)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0833 (0.0808)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0814 (0.0809)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0765 (0.0807)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0764 (0.0807)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0801 (0.0807)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0801 (0.0807)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0792 (0.0807)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0770 (0.0808)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0808 (0.0808)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0800 (0.0808)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0770 (0.0808)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0793 (0.0808)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0796 (0.0808)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0796 (0.0808)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0829 (0.0809)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:65] Total time: 0:18:06 (1.5774 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0829 (0.0809)\n",
      "Valid: [epoch:65]  [ 0/14]  eta: 0:00:13  loss: 0.0801 (0.0801)  time: 0.9862  data: 0.3686  max mem: 39763\n",
      "Valid: [epoch:65]  [13/14]  eta: 0:00:00  loss: 0.0801 (0.0792)  time: 0.1124  data: 0.0264  max mem: 39763\n",
      "Valid: [epoch:65] Total time: 0:00:01 (0.1208 s / it)\n",
      "Averaged stats: loss: 0.0801 (0.0792)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_65_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.079%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:66]  [  0/689]  eta: 0:14:27  lr: 0.000100  loss: 0.0901 (0.0901)  time: 1.2590  data: 0.7808  max mem: 39763\n",
      "Train: [epoch:66]  [ 10/689]  eta: 0:17:30  lr: 0.000100  loss: 0.0909 (0.0966)  time: 1.5478  data: 0.0711  max mem: 39763\n",
      "Train: [epoch:66]  [ 20/689]  eta: 0:17:25  lr: 0.000100  loss: 0.0819 (0.0884)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [ 30/689]  eta: 0:17:13  lr: 0.000100  loss: 0.0809 (0.0855)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [ 40/689]  eta: 0:16:59  lr: 0.000100  loss: 0.0758 (0.0834)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [ 50/689]  eta: 0:16:44  lr: 0.000100  loss: 0.0771 (0.0825)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [ 60/689]  eta: 0:16:29  lr: 0.000100  loss: 0.0776 (0.0819)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [ 70/689]  eta: 0:16:14  lr: 0.000100  loss: 0.0800 (0.0825)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [ 80/689]  eta: 0:15:58  lr: 0.000100  loss: 0.0816 (0.0826)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [ 90/689]  eta: 0:15:43  lr: 0.000100  loss: 0.0909 (0.0847)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [100/689]  eta: 0:15:27  lr: 0.000100  loss: 0.0882 (0.0844)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [110/689]  eta: 0:15:12  lr: 0.000100  loss: 0.0829 (0.0847)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0833 (0.0845)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0822 (0.0842)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [140/689]  eta: 0:14:25  lr: 0.000100  loss: 0.0741 (0.0834)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0735 (0.0831)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0786 (0.0832)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [170/689]  eta: 0:13:38  lr: 0.000100  loss: 0.0837 (0.0836)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0837 (0.0837)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0799 (0.0836)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0791 (0.0833)  time: 1.5778  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:66]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0797 (0.0832)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0777 (0.0829)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0769 (0.0827)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0785 (0.0828)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0895 (0.0831)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0857 (0.0831)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0756 (0.0828)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0743 (0.0828)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0819 (0.0828)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0763 (0.0827)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0743 (0.0826)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0782 (0.0826)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0782 (0.0825)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0767 (0.0824)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0791 (0.0824)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0784 (0.0823)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0763 (0.0822)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0757 (0.0821)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0765 (0.0821)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0793 (0.0822)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0814 (0.0821)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0790 (0.0822)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0819 (0.0823)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0807 (0.0822)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0778 (0.0821)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0817 (0.0821)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0813 (0.0821)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0745 (0.0819)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0789 (0.0820)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0804 (0.0819)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0773 (0.0819)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0777 (0.0819)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0783 (0.0819)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0754 (0.0817)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0762 (0.0819)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0861 (0.0819)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0782 (0.0819)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0760 (0.0818)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0714 (0.0817)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0797 (0.0817)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0802 (0.0817)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0853 (0.0817)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0782 (0.0817)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0782 (0.0817)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0783 (0.0816)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0790 (0.0816)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0758 (0.0815)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0763 (0.0815)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0813 (0.0815)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:66] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0813 (0.0815)\n",
      "Valid: [epoch:66]  [ 0/14]  eta: 0:00:14  loss: 0.0639 (0.0639)  time: 1.0116  data: 0.4020  max mem: 39763\n",
      "Valid: [epoch:66]  [13/14]  eta: 0:00:00  loss: 0.0739 (0.0733)  time: 0.1142  data: 0.0288  max mem: 39763\n",
      "Valid: [epoch:66] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.0739 (0.0733)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_66_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.073%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:67]  [  0/689]  eta: 0:12:01  lr: 0.000100  loss: 0.0743 (0.0743)  time: 1.0471  data: 0.5735  max mem: 39763\n",
      "Train: [epoch:67]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0748 (0.0813)  time: 1.5269  data: 0.0522  max mem: 39763\n",
      "Train: [epoch:67]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0748 (0.0798)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0793 (0.0818)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0786 (0.0813)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0790 (0.0819)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0821 (0.0820)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0776 (0.0816)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0804 (0.0818)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.0838 (0.0824)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0861 (0.0831)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0828 (0.0830)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0828 (0.0831)  time: 1.5781  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:67]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0837 (0.0835)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0863 (0.0836)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0827 (0.0835)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0831 (0.0838)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.0885 (0.0847)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0917 (0.0853)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0832 (0.0851)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0802 (0.0849)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0777 (0.0846)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0762 (0.0843)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.0809 (0.0844)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0827 (0.0843)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0771 (0.0840)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0771 (0.0839)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0784 (0.0837)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0765 (0.0835)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0767 (0.0835)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.0805 (0.0835)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0801 (0.0834)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0765 (0.0833)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0770 (0.0833)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0770 (0.0832)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0765 (0.0830)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0838 (0.0832)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0798 (0.0831)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0782 (0.0831)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0818 (0.0830)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0798 (0.0829)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0807 (0.0832)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0835 (0.0831)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0789 (0.0830)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0821 (0.0831)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0803 (0.0830)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0784 (0.0831)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0828 (0.0831)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0781 (0.0830)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0780 (0.0829)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0824 (0.0829)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0773 (0.0828)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0758 (0.0827)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0758 (0.0826)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0779 (0.0825)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0811 (0.0826)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0835 (0.0827)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0771 (0.0825)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0738 (0.0824)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0752 (0.0824)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0762 (0.0822)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0776 (0.0822)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0757 (0.0821)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0757 (0.0821)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0824 (0.0821)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0801 (0.0821)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0803 (0.0820)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0816 (0.0822)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0853 (0.0822)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0850 (0.0823)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:67] Total time: 0:18:06 (1.5773 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0850 (0.0823)\n",
      "Valid: [epoch:67]  [ 0/14]  eta: 0:00:14  loss: 0.0669 (0.0669)  time: 1.0068  data: 0.3906  max mem: 39763\n",
      "Valid: [epoch:67]  [13/14]  eta: 0:00:00  loss: 0.0737 (0.0746)  time: 0.1139  data: 0.0280  max mem: 39763\n",
      "Valid: [epoch:67] Total time: 0:00:01 (0.1211 s / it)\n",
      "Averaged stats: loss: 0.0737 (0.0746)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_67_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.075%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:68]  [  0/689]  eta: 0:11:58  lr: 0.000100  loss: 0.0886 (0.0886)  time: 1.0422  data: 0.5637  max mem: 39763\n",
      "Train: [epoch:68]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0766 (0.0771)  time: 1.5283  data: 0.0513  max mem: 39763\n",
      "Train: [epoch:68]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0766 (0.0802)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0814 (0.0844)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0878 (0.0848)  time: 1.5777  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:68]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0864 (0.0852)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0879 (0.0859)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0826 (0.0850)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0791 (0.0850)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0863 (0.0856)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0903 (0.0867)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0879 (0.0865)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0789 (0.0861)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0821 (0.0860)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0847 (0.0858)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0827 (0.0857)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0827 (0.0856)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0784 (0.0852)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0799 (0.0855)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0953 (0.0858)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0847 (0.0859)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0951 (0.0867)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0962 (0.0870)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0893 (0.0870)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0820 (0.0869)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0830 (0.0868)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0868 (0.0869)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0856 (0.0868)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0815 (0.0866)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0756 (0.0862)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0773 (0.0862)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0773 (0.0859)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0766 (0.0857)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0800 (0.0857)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0798 (0.0855)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0789 (0.0855)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0860 (0.0856)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0852 (0.0855)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0850 (0.0856)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0911 (0.0861)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0940 (0.0865)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1043 (0.0874)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1043 (0.0879)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1057 (0.0885)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1127 (0.0894)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1089 (0.0896)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0930 (0.0897)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0890 (0.0902)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1029 (0.0907)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1029 (0.0909)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0833 (0.0907)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0782 (0.0905)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0809 (0.0906)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0846 (0.0904)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0820 (0.0904)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0820 (0.0903)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0790 (0.0903)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0837 (0.0901)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0830 (0.0901)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0830 (0.0902)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0878 (0.0902)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0854 (0.0902)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0831 (0.0901)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0800 (0.0899)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0786 (0.0898)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0786 (0.0896)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0760 (0.0894)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0756 (0.0893)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0760 (0.0892)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0741 (0.0889)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:68] Total time: 0:18:06 (1.5775 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0741 (0.0889)\n",
      "Valid: [epoch:68]  [ 0/14]  eta: 0:00:14  loss: 0.0801 (0.0801)  time: 1.0142  data: 0.4149  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:68]  [13/14]  eta: 0:00:00  loss: 0.0766 (0.0766)  time: 0.1144  data: 0.0297  max mem: 39763\n",
      "Valid: [epoch:68] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.0766 (0.0766)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_68_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.077%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:69]  [  0/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0993 (0.0993)  time: 1.0502  data: 0.5689  max mem: 39763\n",
      "Train: [epoch:69]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0785 (0.0805)  time: 1.5277  data: 0.0518  max mem: 39763\n",
      "Train: [epoch:69]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0785 (0.0803)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0820 (0.0811)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0781 (0.0809)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0781 (0.0808)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0804 (0.0816)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0856 (0.0825)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0850 (0.0826)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0802 (0.0825)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0872 (0.0839)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0918 (0.0845)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0918 (0.0854)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0896 (0.0855)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0835 (0.0855)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0838 (0.0854)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0838 (0.0851)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0798 (0.0849)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0802 (0.0849)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0843 (0.0850)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0843 (0.0848)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0792 (0.0849)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0762 (0.0846)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0772 (0.0845)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0830 (0.0845)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0818 (0.0842)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0777 (0.0842)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0804 (0.0841)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0804 (0.0841)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0781 (0.0840)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0781 (0.0839)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0780 (0.0839)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0780 (0.0839)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0837 (0.0839)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0820 (0.0838)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0803 (0.0837)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0787 (0.0836)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0788 (0.0835)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0778 (0.0834)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0762 (0.0832)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0768 (0.0833)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0864 (0.0835)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0844 (0.0835)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0822 (0.0837)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0827 (0.0837)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0793 (0.0837)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0826 (0.0838)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0924 (0.0840)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0901 (0.0840)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0799 (0.0840)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0783 (0.0839)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0785 (0.0838)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0785 (0.0837)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0771 (0.0836)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0771 (0.0836)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0761 (0.0834)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0766 (0.0834)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0776 (0.0833)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0776 (0.0832)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0810 (0.0833)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0812 (0.0833)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0795 (0.0832)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0792 (0.0832)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:69]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0800 (0.0831)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0808 (0.0831)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0791 (0.0830)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0776 (0.0829)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0768 (0.0829)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0778 (0.0828)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0791 (0.0828)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:69] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0791 (0.0828)\n",
      "Valid: [epoch:69]  [ 0/14]  eta: 0:00:13  loss: 0.0658 (0.0658)  time: 0.9835  data: 0.3853  max mem: 39763\n",
      "Valid: [epoch:69]  [13/14]  eta: 0:00:00  loss: 0.0748 (0.0751)  time: 0.1123  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:69] Total time: 0:00:01 (0.1217 s / it)\n",
      "Averaged stats: loss: 0.0748 (0.0751)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_69_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.075%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:70]  [  0/689]  eta: 0:12:30  lr: 0.000100  loss: 0.0745 (0.0745)  time: 1.0894  data: 0.6082  max mem: 39763\n",
      "Train: [epoch:70]  [ 10/689]  eta: 0:17:21  lr: 0.000100  loss: 0.0754 (0.0802)  time: 1.5335  data: 0.0554  max mem: 39763\n",
      "Train: [epoch:70]  [ 20/689]  eta: 0:17:20  lr: 0.000100  loss: 0.0771 (0.0798)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [ 30/689]  eta: 0:17:10  lr: 0.000100  loss: 0.0771 (0.0802)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [ 40/689]  eta: 0:16:56  lr: 0.000100  loss: 0.0799 (0.0820)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [ 50/689]  eta: 0:16:42  lr: 0.000100  loss: 0.0799 (0.0817)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0812 (0.0827)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0800 (0.0820)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0787 (0.0823)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0809 (0.0827)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [100/689]  eta: 0:15:27  lr: 0.000100  loss: 0.0870 (0.0835)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0895 (0.0845)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0887 (0.0844)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0851 (0.0854)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0894 (0.0860)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0891 (0.0863)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0908 (0.0880)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0869 (0.0878)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0804 (0.0875)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0817 (0.0875)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0805 (0.0871)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0805 (0.0870)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0836 (0.0869)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0790 (0.0867)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0809 (0.0864)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0797 (0.0862)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0783 (0.0862)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0782 (0.0860)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0774 (0.0858)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0769 (0.0857)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0855 (0.0859)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0870 (0.0858)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0830 (0.0858)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0811 (0.0856)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0805 (0.0855)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0811 (0.0853)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0780 (0.0852)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0793 (0.0851)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0826 (0.0851)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0858 (0.0853)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0875 (0.0854)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0820 (0.0853)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0768 (0.0853)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0796 (0.0853)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0796 (0.0852)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0780 (0.0850)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0779 (0.0849)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0783 (0.0848)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0763 (0.0847)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0810 (0.0849)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0937 (0.0849)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0823 (0.0849)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0823 (0.0849)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0823 (0.0849)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0851 (0.0849)  time: 1.5779  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:70]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0873 (0.0851)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0965 (0.0854)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0908 (0.0855)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0834 (0.0854)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0842 (0.0857)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0862 (0.0857)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0815 (0.0856)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0817 (0.0857)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0816 (0.0856)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0835 (0.0857)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0835 (0.0857)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0812 (0.0856)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0866 (0.0857)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0866 (0.0858)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0882 (0.0859)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:70] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0882 (0.0859)\n",
      "Valid: [epoch:70]  [ 0/14]  eta: 0:00:14  loss: 0.0783 (0.0783)  time: 1.0080  data: 0.3843  max mem: 39763\n",
      "Valid: [epoch:70]  [13/14]  eta: 0:00:00  loss: 0.0829 (0.0821)  time: 0.1140  data: 0.0275  max mem: 39763\n",
      "Valid: [epoch:70] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.0829 (0.0821)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_70_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.082%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:71]  [  0/689]  eta: 0:12:13  lr: 0.000100  loss: 0.1132 (0.1132)  time: 1.0646  data: 0.5873  max mem: 39763\n",
      "Train: [epoch:71]  [ 10/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0786 (0.0870)  time: 1.5293  data: 0.0535  max mem: 39763\n",
      "Train: [epoch:71]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0786 (0.0853)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0878 (0.0862)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0887 (0.0854)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0789 (0.0842)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0791 (0.0851)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0938 (0.0873)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0993 (0.0905)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0973 (0.0910)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0931 (0.0912)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0896 (0.0912)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0850 (0.0906)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0845 (0.0901)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0853 (0.0901)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0834 (0.0896)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0830 (0.0896)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0850 (0.0892)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0850 (0.0891)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0900 (0.0893)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0856 (0.0889)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0815 (0.0887)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0845 (0.0885)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0802 (0.0881)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0796 (0.0879)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0843 (0.0878)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0830 (0.0876)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0836 (0.0876)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0855 (0.0875)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0836 (0.0876)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0892 (0.0879)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0932 (0.0882)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0885 (0.0883)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0863 (0.0882)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0834 (0.0880)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0767 (0.0877)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0767 (0.0876)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0814 (0.0874)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0757 (0.0871)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0780 (0.0872)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0877 (0.0874)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0916 (0.0876)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1011 (0.0881)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1011 (0.0883)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0845 (0.0885)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0845 (0.0886)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0883 (0.0889)  time: 1.5791  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:71]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0935 (0.0891)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0954 (0.0894)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0929 (0.0894)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0839 (0.0893)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0833 (0.0892)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0866 (0.0892)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0866 (0.0892)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0855 (0.0893)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0901 (0.0892)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0840 (0.0891)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0792 (0.0890)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0815 (0.0890)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0861 (0.0889)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0861 (0.0889)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0825 (0.0888)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0790 (0.0888)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0816 (0.0886)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0831 (0.0888)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0932 (0.0889)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0861 (0.0889)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0845 (0.0888)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0836 (0.0887)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0833 (0.0887)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:71] Total time: 0:18:07 (1.5785 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0833 (0.0887)\n",
      "Valid: [epoch:71]  [ 0/14]  eta: 0:00:14  loss: 0.0796 (0.0796)  time: 1.0084  data: 0.3889  max mem: 39763\n",
      "Valid: [epoch:71]  [13/14]  eta: 0:00:00  loss: 0.0796 (0.0780)  time: 0.1140  data: 0.0278  max mem: 39763\n",
      "Valid: [epoch:71] Total time: 0:00:01 (0.1233 s / it)\n",
      "Averaged stats: loss: 0.0796 (0.0780)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_71_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.078%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:72]  [  0/689]  eta: 0:11:42  lr: 0.000100  loss: 0.0742 (0.0742)  time: 1.0199  data: 0.5405  max mem: 39763\n",
      "Train: [epoch:72]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0763 (0.0764)  time: 1.5283  data: 0.0492  max mem: 39763\n",
      "Train: [epoch:72]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0779 (0.0784)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0803 (0.0820)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0809 (0.0828)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0851 (0.0843)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0900 (0.0850)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0874 (0.0857)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0856 (0.0864)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0869 (0.0864)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0874 (0.0873)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0886 (0.0872)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [120/689]  eta: 0:14:56  lr: 0.000100  loss: 0.0834 (0.0871)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0850 (0.0870)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0818 (0.0867)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0813 (0.0864)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0819 (0.0863)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [170/689]  eta: 0:13:38  lr: 0.000100  loss: 0.0861 (0.0864)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0838 (0.0860)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0798 (0.0861)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0793 (0.0859)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0785 (0.0856)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0827 (0.0858)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0855 (0.0858)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0834 (0.0858)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0811 (0.0856)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0797 (0.0854)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0817 (0.0854)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0859 (0.0855)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0848 (0.0856)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0799 (0.0854)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0768 (0.0853)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0828 (0.0852)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0828 (0.0853)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0770 (0.0851)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0769 (0.0850)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0814 (0.0850)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0835 (0.0850)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0800 (0.0848)  time: 1.5791  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:72]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0796 (0.0849)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [400/689]  eta: 0:07:36  lr: 0.000100  loss: 0.0909 (0.0852)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0947 (0.0854)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0890 (0.0857)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0890 (0.0857)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0880 (0.0858)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0914 (0.0859)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0914 (0.0859)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0833 (0.0859)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0809 (0.0859)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0835 (0.0859)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0812 (0.0858)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0799 (0.0858)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0834 (0.0858)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0796 (0.0856)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0786 (0.0856)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0809 (0.0856)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0795 (0.0855)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0795 (0.0854)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0821 (0.0853)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0834 (0.0854)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0868 (0.0856)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0892 (0.0856)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0821 (0.0856)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0789 (0.0855)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0843 (0.0857)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0928 (0.0858)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0860 (0.0859)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0861 (0.0860)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0997 (0.0862)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1023 (0.0865)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:72] Total time: 0:18:07 (1.5786 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1023 (0.0865)\n",
      "Valid: [epoch:72]  [ 0/14]  eta: 0:00:14  loss: 0.0779 (0.0779)  time: 1.0099  data: 0.4263  max mem: 39763\n",
      "Valid: [epoch:72]  [13/14]  eta: 0:00:00  loss: 0.0836 (0.0819)  time: 0.1141  data: 0.0305  max mem: 39763\n",
      "Valid: [epoch:72] Total time: 0:00:01 (0.1234 s / it)\n",
      "Averaged stats: loss: 0.0836 (0.0819)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_72_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.082%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:73]  [  0/689]  eta: 0:12:00  lr: 0.000100  loss: 0.0814 (0.0814)  time: 1.0463  data: 0.5716  max mem: 39763\n",
      "Train: [epoch:73]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0872 (0.0882)  time: 1.5269  data: 0.0520  max mem: 39763\n",
      "Train: [epoch:73]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0863 (0.0879)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0834 (0.0865)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0834 (0.0860)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0850 (0.0863)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0834 (0.0856)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0794 (0.0848)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0792 (0.0846)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0847 (0.0855)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0877 (0.0861)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0868 (0.0863)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0814 (0.0865)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0910 (0.0875)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0904 (0.0875)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0864 (0.0877)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0881 (0.0882)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1002 (0.0908)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1044 (0.0915)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0995 (0.0918)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0939 (0.0920)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0892 (0.0921)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0813 (0.0915)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0817 (0.0914)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0816 (0.0910)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0816 (0.0909)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0865 (0.0908)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0831 (0.0905)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0838 (0.0903)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0838 (0.0900)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0796 (0.0898)  time: 1.5782  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:73]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0786 (0.0896)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0776 (0.0894)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0822 (0.0892)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0860 (0.0892)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0863 (0.0891)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0838 (0.0889)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0834 (0.0888)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0780 (0.0885)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0787 (0.0885)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0882 (0.0884)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0846 (0.0884)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0791 (0.0881)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0780 (0.0881)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0910 (0.0884)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0961 (0.0885)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0960 (0.0887)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0942 (0.0889)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0941 (0.0890)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0873 (0.0890)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0873 (0.0893)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1003 (0.0896)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1017 (0.0902)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1103 (0.0905)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0942 (0.0905)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0886 (0.0905)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0886 (0.0905)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0850 (0.0904)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0812 (0.0903)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0847 (0.0902)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0836 (0.0901)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0860 (0.0900)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0863 (0.0900)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0868 (0.0899)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0878 (0.0900)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0894 (0.0900)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0880 (0.0900)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0861 (0.0900)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0879 (0.0901)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0982 (0.0903)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:73] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0982 (0.0903)\n",
      "Valid: [epoch:73]  [ 0/14]  eta: 0:00:13  loss: 0.0915 (0.0915)  time: 0.9971  data: 0.4049  max mem: 39763\n",
      "Valid: [epoch:73]  [13/14]  eta: 0:00:00  loss: 0.0850 (0.0874)  time: 0.1132  data: 0.0290  max mem: 39763\n",
      "Valid: [epoch:73] Total time: 0:00:01 (0.1225 s / it)\n",
      "Averaged stats: loss: 0.0850 (0.0874)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_73_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.087%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:74]  [  0/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0996 (0.0996)  time: 1.0054  data: 0.5272  max mem: 39763\n",
      "Train: [epoch:74]  [ 10/689]  eta: 0:17:14  lr: 0.000100  loss: 0.0869 (0.0882)  time: 1.5241  data: 0.0480  max mem: 39763\n",
      "Train: [epoch:74]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0830 (0.0871)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0878 (0.0889)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0826 (0.0880)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0818 (0.0879)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0845 (0.0875)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0861 (0.0878)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0840 (0.0871)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0814 (0.0867)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0839 (0.0869)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0897 (0.0875)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.1016 (0.0886)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0971 (0.0889)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0875 (0.0884)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0901 (0.0894)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0953 (0.0895)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0906 (0.0895)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0851 (0.0892)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0851 (0.0892)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0863 (0.0891)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0888 (0.0891)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0873 (0.0889)  time: 1.5779  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:74]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0873 (0.0891)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0912 (0.0892)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0852 (0.0890)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0852 (0.0890)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0861 (0.0889)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0843 (0.0888)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0797 (0.0886)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0797 (0.0883)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0805 (0.0882)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0835 (0.0882)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0858 (0.0883)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0841 (0.0883)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0790 (0.0881)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0790 (0.0879)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0779 (0.0876)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0774 (0.0874)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0804 (0.0874)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0814 (0.0874)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0857 (0.0874)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0852 (0.0873)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0792 (0.0871)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0812 (0.0872)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0868 (0.0872)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0865 (0.0873)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0857 (0.0872)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0842 (0.0874)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0863 (0.0874)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0989 (0.0879)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1073 (0.0882)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0954 (0.0884)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0878 (0.0884)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0866 (0.0884)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0854 (0.0883)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0846 (0.0883)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0859 (0.0883)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0858 (0.0882)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0796 (0.0882)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0890 (0.0883)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0880 (0.0883)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0832 (0.0882)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0816 (0.0882)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0802 (0.0881)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0866 (0.0882)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0866 (0.0882)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0858 (0.0882)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0831 (0.0881)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0806 (0.0880)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:74] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0806 (0.0880)\n",
      "Valid: [epoch:74]  [ 0/14]  eta: 0:00:14  loss: 0.0840 (0.0840)  time: 1.0165  data: 0.3603  max mem: 39763\n",
      "Valid: [epoch:74]  [13/14]  eta: 0:00:00  loss: 0.0798 (0.0798)  time: 0.1146  data: 0.0258  max mem: 39763\n",
      "Valid: [epoch:74] Total time: 0:00:01 (0.1249 s / it)\n",
      "Averaged stats: loss: 0.0798 (0.0798)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_74_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.080%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:75]  [  0/689]  eta: 0:12:04  lr: 0.000100  loss: 0.0758 (0.0758)  time: 1.0517  data: 0.5749  max mem: 39763\n",
      "Train: [epoch:75]  [ 10/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0888 (0.0896)  time: 1.5291  data: 0.0524  max mem: 39763\n",
      "Train: [epoch:75]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0874 (0.0884)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0863 (0.0890)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0860 (0.0877)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0800 (0.0865)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0811 (0.0869)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0907 (0.0882)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0947 (0.0886)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0885 (0.0883)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0844 (0.0888)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0864 (0.0890)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0889 (0.0891)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0839 (0.0889)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0839 (0.0890)  time: 1.5777  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:75]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0888 (0.0894)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0932 (0.0896)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0922 (0.0898)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0914 (0.0901)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0926 (0.0902)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0908 (0.0902)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0877 (0.0905)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0877 (0.0905)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0860 (0.0906)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0834 (0.0903)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0834 (0.0903)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0888 (0.0903)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0915 (0.0904)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0902 (0.0904)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0866 (0.0902)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0828 (0.0902)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0844 (0.0901)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0875 (0.0901)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0825 (0.0899)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0821 (0.0898)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0822 (0.0897)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0846 (0.0897)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0865 (0.0896)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0870 (0.0898)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0974 (0.0900)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0924 (0.0900)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0917 (0.0900)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0857 (0.0898)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0800 (0.0897)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0806 (0.0896)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0824 (0.0895)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0850 (0.0895)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0873 (0.0895)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0847 (0.0894)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0866 (0.0895)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0906 (0.0895)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0913 (0.0896)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0926 (0.0898)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0947 (0.0900)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0880 (0.0899)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0875 (0.0899)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0850 (0.0900)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0904 (0.0901)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0871 (0.0901)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0859 (0.0900)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0855 (0.0900)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0849 (0.0899)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0893 (0.0900)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0945 (0.0901)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0890 (0.0901)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0821 (0.0899)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0854 (0.0900)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0872 (0.0899)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0851 (0.0899)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0800 (0.0898)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:75] Total time: 0:18:07 (1.5784 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0800 (0.0898)\n",
      "Valid: [epoch:75]  [ 0/14]  eta: 0:00:11  loss: 0.0848 (0.0848)  time: 0.8039  data: 0.3857  max mem: 39763\n",
      "Valid: [epoch:75]  [13/14]  eta: 0:00:00  loss: 0.0784 (0.0788)  time: 0.0995  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:75] Total time: 0:00:01 (0.1086 s / it)\n",
      "Averaged stats: loss: 0.0784 (0.0788)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_75_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.079%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:76]  [  0/689]  eta: 0:11:49  lr: 0.000100  loss: 0.0796 (0.0796)  time: 1.0294  data: 0.5499  max mem: 39763\n",
      "Train: [epoch:76]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0858 (0.0870)  time: 1.5283  data: 0.0501  max mem: 39763\n",
      "Train: [epoch:76]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0867 (0.0865)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0876 (0.0872)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0876 (0.0876)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0864 (0.0869)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0818 (0.0871)  time: 1.5790  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:76]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0842 (0.0866)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0827 (0.0862)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0827 (0.0866)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0917 (0.0876)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0932 (0.0881)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0913 (0.0885)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0865 (0.0886)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0840 (0.0885)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0844 (0.0883)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0855 (0.0885)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0837 (0.0885)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0836 (0.0885)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0864 (0.0884)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0869 (0.0883)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0869 (0.0885)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0861 (0.0886)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0890 (0.0889)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0890 (0.0888)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0835 (0.0887)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0848 (0.0887)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0868 (0.0888)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0868 (0.0888)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0862 (0.0890)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0964 (0.0892)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0915 (0.0894)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0890 (0.0895)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0955 (0.0897)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0953 (0.0897)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0825 (0.0896)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0827 (0.0895)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0814 (0.0892)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0812 (0.0891)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0835 (0.0891)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0892 (0.0892)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0892 (0.0891)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0844 (0.0890)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0871 (0.0890)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0882 (0.0890)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0902 (0.0890)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0903 (0.0890)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0939 (0.0891)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0845 (0.0889)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0847 (0.0890)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0847 (0.0889)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0832 (0.0889)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0830 (0.0888)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0809 (0.0887)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0834 (0.0887)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0842 (0.0887)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0839 (0.0887)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0849 (0.0887)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0896 (0.0888)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0872 (0.0888)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0854 (0.0888)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0821 (0.0887)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0841 (0.0886)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0863 (0.0887)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0870 (0.0887)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0860 (0.0886)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0903 (0.0886)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0927 (0.0888)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0965 (0.0889)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0965 (0.0889)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:76] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0965 (0.0889)\n",
      "Valid: [epoch:76]  [ 0/14]  eta: 0:00:13  loss: 0.0884 (0.0884)  time: 0.9998  data: 0.4241  max mem: 39763\n",
      "Valid: [epoch:76]  [13/14]  eta: 0:00:00  loss: 0.0780 (0.0796)  time: 0.1134  data: 0.0303  max mem: 39763\n",
      "Valid: [epoch:76] Total time: 0:00:01 (0.1224 s / it)\n",
      "Averaged stats: loss: 0.0780 (0.0796)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_76_input_n_20.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the network on the 14 valid images: 0.080%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:77]  [  0/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0777 (0.0777)  time: 1.0717  data: 0.5958  max mem: 39763\n",
      "Train: [epoch:77]  [ 10/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0853 (0.0865)  time: 1.5296  data: 0.0542  max mem: 39763\n",
      "Train: [epoch:77]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0886 (0.0880)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0892 (0.0893)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0981 (0.0926)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0960 (0.0930)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0909 (0.0922)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0862 (0.0916)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0863 (0.0914)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0859 (0.0911)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0865 (0.0909)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0857 (0.0904)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0854 (0.0900)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0870 (0.0900)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0858 (0.0895)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0858 (0.0898)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0868 (0.0899)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0916 (0.0903)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0928 (0.0907)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0928 (0.0907)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0827 (0.0902)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0827 (0.0900)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0843 (0.0900)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0855 (0.0899)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0875 (0.0899)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0875 (0.0898)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0867 (0.0897)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0861 (0.0899)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0898 (0.0899)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0885 (0.0899)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0852 (0.0897)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0834 (0.0896)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0823 (0.0895)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0827 (0.0895)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0903 (0.0896)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0908 (0.0896)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0904 (0.0897)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0916 (0.0899)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0946 (0.0899)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0965 (0.0902)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0981 (0.0909)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1022 (0.0916)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0929 (0.0915)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0902 (0.0916)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0937 (0.0917)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0901 (0.0915)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0843 (0.0915)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0843 (0.0913)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0875 (0.0914)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0965 (0.0915)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0960 (0.0916)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0919 (0.0916)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0869 (0.0916)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0868 (0.0915)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0855 (0.0914)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0847 (0.0913)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0831 (0.0913)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0888 (0.0912)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0888 (0.0912)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0888 (0.0911)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0888 (0.0912)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0909 (0.0912)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0885 (0.0911)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0933 (0.0913)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0963 (0.0915)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0961 (0.0916)  time: 1.5777  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:77]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0910 (0.0916)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0910 (0.0917)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0867 (0.0916)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0828 (0.0916)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:77] Total time: 0:18:06 (1.5774 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0828 (0.0916)\n",
      "Valid: [epoch:77]  [ 0/14]  eta: 0:00:13  loss: 0.0844 (0.0844)  time: 0.9647  data: 0.4581  max mem: 39763\n",
      "Valid: [epoch:77]  [13/14]  eta: 0:00:00  loss: 0.0772 (0.0791)  time: 0.1109  data: 0.0328  max mem: 39763\n",
      "Valid: [epoch:77] Total time: 0:00:01 (0.1202 s / it)\n",
      "Averaged stats: loss: 0.0772 (0.0791)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_77_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.079%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:78]  [  0/689]  eta: 0:11:37  lr: 0.000100  loss: 0.1000 (0.1000)  time: 1.0120  data: 0.5325  max mem: 39763\n",
      "Train: [epoch:78]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0915 (0.0907)  time: 1.5251  data: 0.0485  max mem: 39763\n",
      "Train: [epoch:78]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0867 (0.0874)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0867 (0.0890)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0860 (0.0882)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0795 (0.0872)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0798 (0.0870)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0832 (0.0866)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0860 (0.0873)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0865 (0.0871)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0854 (0.0872)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0968 (0.0883)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0992 (0.0889)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0939 (0.0896)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0889 (0.0897)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0916 (0.0904)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0962 (0.0912)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.0909 (0.0913)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0900 (0.0916)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0888 (0.0916)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0887 (0.0916)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0883 (0.0916)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0906 (0.0917)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0876 (0.0914)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0827 (0.0913)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0849 (0.0915)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0934 (0.0925)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.1170 (0.0942)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1397 (0.0966)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1215 (0.0971)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0903 (0.0970)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0970 (0.0972)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1023 (0.0977)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0990 (0.0978)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0971 (0.0978)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0911 (0.0976)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0911 (0.0975)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0855 (0.0972)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0855 (0.0972)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0941 (0.0972)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0923 (0.0971)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0921 (0.0971)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0877 (0.0968)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0902 (0.0969)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0952 (0.0968)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0944 (0.0968)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0900 (0.0967)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0857 (0.0965)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0830 (0.0963)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0909 (0.0963)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0917 (0.0961)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0926 (0.0962)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0956 (0.0962)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0871 (0.0960)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0829 (0.0958)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0864 (0.0957)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0891 (0.0956)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0878 (0.0954)  time: 1.5792  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:78]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0853 (0.0953)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0884 (0.0952)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0884 (0.0952)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0838 (0.0950)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0828 (0.0949)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0851 (0.0947)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0851 (0.0946)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0892 (0.0946)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0894 (0.0945)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0833 (0.0943)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0853 (0.0943)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0859 (0.0942)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:78] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0859 (0.0942)\n",
      "Valid: [epoch:78]  [ 0/14]  eta: 0:00:14  loss: 0.0717 (0.0717)  time: 1.0174  data: 0.3853  max mem: 39763\n",
      "Valid: [epoch:78]  [13/14]  eta: 0:00:00  loss: 0.0781 (0.0802)  time: 0.1146  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:78] Total time: 0:00:01 (0.1243 s / it)\n",
      "Averaged stats: loss: 0.0781 (0.0802)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_78_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.080%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:79]  [  0/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0822 (0.0822)  time: 1.0495  data: 0.5713  max mem: 39763\n",
      "Train: [epoch:79]  [ 10/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0873 (0.0914)  time: 1.5291  data: 0.0520  max mem: 39763\n",
      "Train: [epoch:79]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0873 (0.0898)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0857 (0.0890)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0859 (0.0895)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0896 (0.0897)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0897 (0.0899)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0852 (0.0893)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0840 (0.0894)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0884 (0.0896)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0884 (0.0897)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0908 (0.0899)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0929 (0.0900)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0936 (0.0908)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0944 (0.0906)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0953 (0.0911)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0917 (0.0910)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0917 (0.0914)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0970 (0.0915)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0942 (0.0917)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0900 (0.0918)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0870 (0.0916)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0863 (0.0915)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0921 (0.0917)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0944 (0.0920)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0996 (0.0926)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.1030 (0.0932)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0959 (0.0931)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0928 (0.0935)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.1011 (0.0936)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0935 (0.0938)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0903 (0.0937)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0903 (0.0938)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0990 (0.0940)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0942 (0.0942)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1019 (0.0948)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1033 (0.0950)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.1016 (0.0955)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0987 (0.0953)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0846 (0.0953)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0870 (0.0953)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0949 (0.0955)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1014 (0.0960)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0918 (0.0959)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0893 (0.0959)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0893 (0.0958)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0945 (0.0961)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1011 (0.0960)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0919 (0.0960)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0915 (0.0959)  time: 1.5792  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:79]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0919 (0.0960)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0945 (0.0959)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0905 (0.0958)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0894 (0.0957)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0894 (0.0957)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0925 (0.0956)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0900 (0.0955)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0918 (0.0956)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0975 (0.0956)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0975 (0.0959)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0978 (0.0960)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0885 (0.0959)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0878 (0.0958)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0878 (0.0956)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0921 (0.0957)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0982 (0.0959)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1000 (0.0961)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0968 (0.0961)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0985 (0.0965)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1165 (0.0966)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:79] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1165 (0.0966)\n",
      "Valid: [epoch:79]  [ 0/14]  eta: 0:00:14  loss: 0.0956 (0.0956)  time: 1.0132  data: 0.3770  max mem: 39763\n",
      "Valid: [epoch:79]  [13/14]  eta: 0:00:00  loss: 0.0840 (0.0878)  time: 0.1144  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:79] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.0840 (0.0878)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_79_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.088%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:80]  [  0/689]  eta: 0:11:39  lr: 0.000100  loss: 0.1179 (0.1179)  time: 1.0154  data: 0.5345  max mem: 39763\n",
      "Train: [epoch:80]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0872 (0.0959)  time: 1.5264  data: 0.0487  max mem: 39763\n",
      "Train: [epoch:80]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0911 (0.0951)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0950 (0.0942)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0942 (0.0942)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0938 (0.0941)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0890 (0.0931)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0893 (0.0929)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0947 (0.0936)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.1022 (0.0952)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.1008 (0.0952)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0956 (0.0956)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0927 (0.0954)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0908 (0.0959)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0972 (0.0958)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0923 (0.0954)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0893 (0.0954)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0918 (0.0955)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1019 (0.0962)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0977 (0.0960)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0896 (0.0956)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0909 (0.0955)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0933 (0.0954)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0933 (0.0954)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0920 (0.0955)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0981 (0.0959)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.1006 (0.0961)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0938 (0.0960)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0904 (0.0959)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0904 (0.0956)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0871 (0.0955)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0912 (0.0956)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0980 (0.0957)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0989 (0.0957)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0963 (0.0959)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0960 (0.0959)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0931 (0.0958)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0894 (0.0956)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0875 (0.0954)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0868 (0.0953)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0929 (0.0954)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0925 (0.0952)  time: 1.5786  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:80]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0861 (0.0951)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0868 (0.0950)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0900 (0.0951)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0917 (0.0950)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0917 (0.0950)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0867 (0.0949)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0843 (0.0947)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0876 (0.0947)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0871 (0.0945)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0871 (0.0944)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0892 (0.0943)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0878 (0.0943)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0907 (0.0942)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0891 (0.0942)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0882 (0.0941)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0865 (0.0941)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0857 (0.0939)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0851 (0.0939)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0947 (0.0939)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0914 (0.0939)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0899 (0.0939)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0910 (0.0939)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0918 (0.0938)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0930 (0.0938)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0942 (0.0938)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0942 (0.0939)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0931 (0.0939)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0934 (0.0939)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:80] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0934 (0.0939)\n",
      "Valid: [epoch:80]  [ 0/14]  eta: 0:00:14  loss: 0.0772 (0.0772)  time: 1.0114  data: 0.4084  max mem: 39763\n",
      "Valid: [epoch:80]  [13/14]  eta: 0:00:00  loss: 0.0827 (0.0861)  time: 0.1142  data: 0.0292  max mem: 39763\n",
      "Valid: [epoch:80] Total time: 0:00:01 (0.1233 s / it)\n",
      "Averaged stats: loss: 0.0827 (0.0861)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_80_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.086%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:81]  [  0/689]  eta: 0:11:54  lr: 0.000100  loss: 0.1146 (0.1146)  time: 1.0369  data: 0.5591  max mem: 39763\n",
      "Train: [epoch:81]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0918 (0.0948)  time: 1.5274  data: 0.0509  max mem: 39763\n",
      "Train: [epoch:81]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0891 (0.0917)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0895 (0.0905)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0917 (0.0918)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0908 (0.0908)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0881 (0.0906)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0929 (0.0916)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0889 (0.0911)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0858 (0.0909)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0864 (0.0910)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0975 (0.0920)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0964 (0.0922)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0894 (0.0921)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0896 (0.0922)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0896 (0.0921)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0915 (0.0926)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0952 (0.0927)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0950 (0.0931)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0986 (0.0936)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0945 (0.0935)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0907 (0.0934)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0904 (0.0934)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0973 (0.0938)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [240/689]  eta: 0:11:48  lr: 0.000100  loss: 0.0992 (0.0941)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.1008 (0.0946)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.1008 (0.0948)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0958 (0.0948)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0912 (0.0947)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0919 (0.0947)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0919 (0.0947)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0889 (0.0945)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0889 (0.0945)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0949 (0.0945)  time: 1.5794  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:81]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0948 (0.0946)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0930 (0.0945)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0905 (0.0943)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0902 (0.0942)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0847 (0.0940)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0891 (0.0941)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0923 (0.0940)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0874 (0.0938)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0843 (0.0936)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0857 (0.0937)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0943 (0.0938)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0923 (0.0938)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0904 (0.0938)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0924 (0.0938)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0934 (0.0938)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0922 (0.0937)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0879 (0.0937)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0874 (0.0936)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0908 (0.0936)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0940 (0.0937)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0888 (0.0936)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0866 (0.0935)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0867 (0.0934)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0888 (0.0934)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0880 (0.0933)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0865 (0.0932)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0865 (0.0931)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0902 (0.0931)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0922 (0.0932)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0987 (0.0932)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0931 (0.0933)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0951 (0.0934)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0966 (0.0936)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0961 (0.0936)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0882 (0.0936)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0851 (0.0935)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:81] Total time: 0:18:07 (1.5785 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0851 (0.0935)\n",
      "Valid: [epoch:81]  [ 0/14]  eta: 0:00:14  loss: 0.0743 (0.0743)  time: 1.0080  data: 0.3887  max mem: 39763\n",
      "Valid: [epoch:81]  [13/14]  eta: 0:00:00  loss: 0.0825 (0.0842)  time: 0.1139  data: 0.0278  max mem: 39763\n",
      "Valid: [epoch:81] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.0825 (0.0842)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_81_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.084%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:82]  [  0/689]  eta: 0:11:29  lr: 0.000100  loss: 0.1009 (0.1009)  time: 1.0010  data: 0.5217  max mem: 39763\n",
      "Train: [epoch:82]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0938 (0.0956)  time: 1.5258  data: 0.0475  max mem: 39763\n",
      "Train: [epoch:82]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0920 (0.0934)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0897 (0.0922)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0894 (0.0915)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0852 (0.0902)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0852 (0.0902)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0886 (0.0901)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0886 (0.0902)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0935 (0.0910)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0960 (0.0922)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1023 (0.0931)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0932 (0.0929)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0975 (0.0942)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.1054 (0.0947)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0949 (0.0949)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0985 (0.0955)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0952 (0.0954)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1010 (0.0960)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.1063 (0.0969)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0998 (0.0973)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0958 (0.0973)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0999 (0.0977)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0989 (0.0977)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0973 (0.0978)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0947 (0.0977)  time: 1.5781  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:82]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0964 (0.0978)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.1005 (0.0979)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0915 (0.0977)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0891 (0.0975)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0888 (0.0972)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0910 (0.0972)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0950 (0.0974)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0973 (0.0975)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0956 (0.0975)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0895 (0.0973)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0884 (0.0970)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0871 (0.0969)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0871 (0.0966)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0882 (0.0966)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0882 (0.0964)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0919 (0.0964)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0919 (0.0963)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0875 (0.0962)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0871 (0.0960)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0856 (0.0959)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0854 (0.0958)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0877 (0.0957)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0890 (0.0957)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0904 (0.0956)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0856 (0.0954)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0867 (0.0954)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0916 (0.0954)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0973 (0.0954)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0952 (0.0954)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0872 (0.0953)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0855 (0.0952)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0860 (0.0951)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0894 (0.0950)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0918 (0.0950)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0918 (0.0949)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0849 (0.0948)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0849 (0.0947)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0897 (0.0947)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0903 (0.0947)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0907 (0.0947)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0904 (0.0946)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0888 (0.0946)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0872 (0.0945)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0861 (0.0945)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:82] Total time: 0:18:07 (1.5784 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0861 (0.0945)\n",
      "Valid: [epoch:82]  [ 0/14]  eta: 0:00:14  loss: 0.0790 (0.0790)  time: 1.0239  data: 0.4246  max mem: 39763\n",
      "Valid: [epoch:82]  [13/14]  eta: 0:00:00  loss: 0.0813 (0.0842)  time: 0.1151  data: 0.0304  max mem: 39763\n",
      "Valid: [epoch:82] Total time: 0:00:01 (0.1240 s / it)\n",
      "Averaged stats: loss: 0.0813 (0.0842)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_82_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.084%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:83]  [  0/689]  eta: 0:11:42  lr: 0.000100  loss: 0.0834 (0.0834)  time: 1.0200  data: 0.5431  max mem: 39763\n",
      "Train: [epoch:83]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0845 (0.0844)  time: 1.5258  data: 0.0494  max mem: 39763\n",
      "Train: [epoch:83]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0884 (0.0927)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0966 (0.0947)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0962 (0.0944)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0955 (0.0962)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.0955 (0.0957)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0882 (0.0949)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0864 (0.0940)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0892 (0.0939)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0922 (0.0939)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0946 (0.0940)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0893 (0.0938)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0886 (0.0937)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0878 (0.0938)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [150/689]  eta: 0:14:09  lr: 0.000100  loss: 0.0930 (0.0940)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0918 (0.0941)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0946 (0.0945)  time: 1.5798  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:83]  [180/689]  eta: 0:13:22  lr: 0.000100  loss: 0.0999 (0.0947)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0926 (0.0945)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0885 (0.0943)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [210/689]  eta: 0:12:35  lr: 0.000100  loss: 0.0904 (0.0945)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0968 (0.0946)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0958 (0.0946)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0992 (0.0950)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0985 (0.0951)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0978 (0.0953)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.1026 (0.0956)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [280/689]  eta: 0:10:45  lr: 0.000100  loss: 0.0918 (0.0956)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0916 (0.0955)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0926 (0.0954)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0934 (0.0957)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [320/689]  eta: 0:09:42  lr: 0.000100  loss: 0.0933 (0.0955)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0889 (0.0955)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0919 (0.0955)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0948 (0.0955)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [360/689]  eta: 0:08:39  lr: 0.000100  loss: 0.0880 (0.0954)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0867 (0.0952)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0868 (0.0951)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0892 (0.0950)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0933 (0.0951)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0933 (0.0950)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0944 (0.0953)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0951 (0.0952)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0925 (0.0951)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.0898 (0.0949)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0914 (0.0952)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1046 (0.0954)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0989 (0.0954)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0887 (0.0953)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0941 (0.0954)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0960 (0.0953)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0882 (0.0954)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0922 (0.0953)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0923 (0.0953)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0947 (0.0954)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0927 (0.0953)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0916 (0.0953)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0918 (0.0952)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0918 (0.0951)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0918 (0.0952)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0944 (0.0953)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0944 (0.0953)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0926 (0.0952)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0941 (0.0952)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0960 (0.0953)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0977 (0.0953)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0932 (0.0953)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0946 (0.0953)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0981 (0.0954)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:83] Total time: 0:18:07 (1.5783 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0981 (0.0954)\n",
      "Valid: [epoch:83]  [ 0/14]  eta: 0:00:14  loss: 0.0981 (0.0981)  time: 1.0208  data: 0.3865  max mem: 39763\n",
      "Valid: [epoch:83]  [13/14]  eta: 0:00:00  loss: 0.0835 (0.0871)  time: 0.1149  data: 0.0277  max mem: 39763\n",
      "Valid: [epoch:83] Total time: 0:00:01 (0.1240 s / it)\n",
      "Averaged stats: loss: 0.0835 (0.0871)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_83_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.087%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:84]  [  0/689]  eta: 0:11:30  lr: 0.000100  loss: 0.0784 (0.0784)  time: 1.0021  data: 0.5212  max mem: 39763\n",
      "Train: [epoch:84]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0926 (0.0927)  time: 1.5250  data: 0.0475  max mem: 39763\n",
      "Train: [epoch:84]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0956 (0.0983)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.1132 (0.1039)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.1147 (0.1057)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.1025 (0.1053)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0983 (0.1040)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1025 (0.1045)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1030 (0.1038)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0952 (0.1032)  time: 1.5781  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:84]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0952 (0.1023)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0974 (0.1020)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0982 (0.1013)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0958 (0.1009)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0988 (0.1010)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0991 (0.1012)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0987 (0.1012)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1037 (0.1028)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1290 (0.1040)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.1224 (0.1049)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.1055 (0.1052)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0995 (0.1048)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0965 (0.1046)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0940 (0.1041)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0950 (0.1039)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0963 (0.1034)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0956 (0.1033)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0942 (0.1027)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0865 (0.1024)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0888 (0.1021)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0926 (0.1021)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0926 (0.1018)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0907 (0.1015)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0991 (0.1017)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1021 (0.1017)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0958 (0.1015)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0968 (0.1016)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.1001 (0.1013)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0929 (0.1011)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0940 (0.1010)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0946 (0.1010)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0946 (0.1009)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0997 (0.1010)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0982 (0.1009)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0920 (0.1007)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0897 (0.1006)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0972 (0.1006)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0955 (0.1005)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0915 (0.1002)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0924 (0.1001)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0930 (0.1000)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0860 (0.0997)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0892 (0.0996)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0907 (0.0994)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0882 (0.0993)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0897 (0.0992)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0913 (0.0991)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0896 (0.0990)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0892 (0.0988)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0874 (0.0987)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0875 (0.0985)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0860 (0.0983)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0909 (0.0985)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0975 (0.0984)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0915 (0.0983)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0915 (0.0983)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0925 (0.0982)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0932 (0.0983)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0996 (0.0983)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0975 (0.0982)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:84] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0975 (0.0982)\n",
      "Valid: [epoch:84]  [ 0/14]  eta: 0:00:13  loss: 0.0770 (0.0770)  time: 0.9915  data: 0.4553  max mem: 39763\n",
      "Valid: [epoch:84]  [13/14]  eta: 0:00:00  loss: 0.0834 (0.0868)  time: 0.1128  data: 0.0326  max mem: 39763\n",
      "Valid: [epoch:84] Total time: 0:00:01 (0.1222 s / it)\n",
      "Averaged stats: loss: 0.0834 (0.0868)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_84_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.087%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:85]  [  0/689]  eta: 0:12:04  lr: 0.000100  loss: 0.1008 (0.1008)  time: 1.0516  data: 0.5739  max mem: 39763\n",
      "Train: [epoch:85]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.1034 (0.1014)  time: 1.5274  data: 0.0522  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:85]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0959 (0.0982)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.0922 (0.0963)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0896 (0.0951)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0924 (0.0957)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0935 (0.0955)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0913 (0.0952)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0913 (0.0950)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0936 (0.0944)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0936 (0.0950)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0948 (0.0955)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0930 (0.0955)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0922 (0.0957)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0934 (0.0961)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1012 (0.0970)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1012 (0.0972)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0925 (0.0970)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0904 (0.0968)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0898 (0.0966)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0903 (0.0962)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0904 (0.0962)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0928 (0.0962)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0928 (0.0961)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0936 (0.0961)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0936 (0.0961)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0936 (0.0962)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0911 (0.0959)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0873 (0.0958)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0909 (0.0957)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0910 (0.0957)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0891 (0.0955)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0881 (0.0953)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0896 (0.0952)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0905 (0.0951)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0927 (0.0952)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0909 (0.0952)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0873 (0.0951)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0874 (0.0950)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0943 (0.0951)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0943 (0.0952)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0947 (0.0954)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0964 (0.0954)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0994 (0.0955)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0972 (0.0955)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0913 (0.0955)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0914 (0.0955)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0979 (0.0957)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0986 (0.0958)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0940 (0.0957)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0928 (0.0958)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0947 (0.0958)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0952 (0.0959)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0952 (0.0959)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0937 (0.0959)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0958 (0.0960)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0965 (0.0960)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0927 (0.0960)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0911 (0.0959)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0926 (0.0959)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0934 (0.0959)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0915 (0.0958)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0902 (0.0957)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0952 (0.0958)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0978 (0.0958)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0938 (0.0959)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0912 (0.0958)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0918 (0.0958)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0954 (0.0958)  time: 1.5795  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:85]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0969 (0.0959)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:85] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0969 (0.0959)\n",
      "Valid: [epoch:85]  [ 0/14]  eta: 0:00:14  loss: 0.0950 (0.0950)  time: 1.0154  data: 0.3775  max mem: 39763\n",
      "Valid: [epoch:85]  [13/14]  eta: 0:00:00  loss: 0.0884 (0.0905)  time: 0.1146  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:85] Total time: 0:00:01 (0.1241 s / it)\n",
      "Averaged stats: loss: 0.0884 (0.0905)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_85_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.091%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:86]  [  0/689]  eta: 0:11:29  lr: 0.000100  loss: 0.1007 (0.1007)  time: 1.0008  data: 0.5210  max mem: 39763\n",
      "Train: [epoch:86]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.1007 (0.0992)  time: 1.5261  data: 0.0474  max mem: 39763\n",
      "Train: [epoch:86]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0973 (0.0970)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0927 (0.1003)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.1093 (0.1026)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0966 (0.1013)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0896 (0.0996)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0899 (0.1004)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0977 (0.0997)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0946 (0.0991)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.1010 (0.0995)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0961 (0.0991)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0939 (0.0991)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0968 (0.0990)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0997 (0.0994)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0945 (0.0993)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0971 (0.0995)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0941 (0.0990)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0902 (0.0985)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0953 (0.0986)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0930 (0.0984)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0930 (0.0984)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0943 (0.0983)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0942 (0.0983)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0955 (0.0981)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0912 (0.0978)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0932 (0.0981)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0972 (0.0981)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0960 (0.0983)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0960 (0.0981)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0897 (0.0980)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0900 (0.0978)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0943 (0.0979)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0956 (0.0978)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0906 (0.0975)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0896 (0.0974)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0925 (0.0974)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0991 (0.0976)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.1008 (0.0977)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0949 (0.0976)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0932 (0.0977)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0978 (0.0977)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0956 (0.0977)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0903 (0.0976)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0892 (0.0976)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0911 (0.0975)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0969 (0.0976)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0992 (0.0977)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0920 (0.0975)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0924 (0.0975)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0971 (0.0976)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0987 (0.0978)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1107 (0.0983)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1107 (0.0984)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.1001 (0.0984)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1043 (0.0986)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1004 (0.0986)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0966 (0.0988)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1054 (0.0989)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1020 (0.0991)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1020 (0.0992)  time: 1.5790  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:86]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0973 (0.0992)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0993 (0.0993)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0985 (0.0992)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0985 (0.0993)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1042 (0.0995)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1040 (0.0996)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1072 (0.0998)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1076 (0.0999)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1076 (0.1000)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:86] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1076 (0.1000)\n",
      "Valid: [epoch:86]  [ 0/14]  eta: 0:00:14  loss: 0.0821 (0.0821)  time: 1.0055  data: 0.3905  max mem: 39763\n",
      "Valid: [epoch:86]  [13/14]  eta: 0:00:00  loss: 0.0881 (0.0890)  time: 0.1138  data: 0.0279  max mem: 39763\n",
      "Valid: [epoch:86] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.0881 (0.0890)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_86_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.089%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:87]  [  0/689]  eta: 0:12:16  lr: 0.000100  loss: 0.1089 (0.1089)  time: 1.0684  data: 0.5947  max mem: 39763\n",
      "Train: [epoch:87]  [ 10/689]  eta: 0:17:19  lr: 0.000100  loss: 0.0960 (0.0978)  time: 1.5304  data: 0.0541  max mem: 39763\n",
      "Train: [epoch:87]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0956 (0.0972)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0990 (0.0986)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.1038 (0.1027)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.1116 (0.1067)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.1087 (0.1055)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0990 (0.1045)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0993 (0.1047)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.1062 (0.1050)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.1062 (0.1054)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.1084 (0.1054)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0980 (0.1048)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0933 (0.1041)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0972 (0.1038)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1016 (0.1035)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0984 (0.1032)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0949 (0.1029)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0916 (0.1024)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0901 (0.1017)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0900 (0.1015)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0946 (0.1016)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.1031 (0.1017)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0983 (0.1014)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0946 (0.1011)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0920 (0.1008)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0917 (0.1007)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0937 (0.1007)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1044 (0.1008)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0982 (0.1006)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0955 (0.1005)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0937 (0.1005)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0941 (0.1004)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0980 (0.1003)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0936 (0.1001)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0894 (0.0999)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0907 (0.0999)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0888 (0.0996)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0859 (0.0994)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0913 (0.0994)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0957 (0.0993)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.0971 (0.0994)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0972 (0.0994)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0941 (0.0992)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0939 (0.0992)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0948 (0.0991)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0949 (0.0992)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1037 (0.0995)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0982 (0.0994)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0952 (0.0993)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0985 (0.0993)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1003 (0.0993)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0947 (0.0993)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:87]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0926 (0.0992)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0967 (0.0994)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1000 (0.0994)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1012 (0.0995)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1068 (0.0996)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0988 (0.0996)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0941 (0.0995)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0936 (0.0995)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0942 (0.0994)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0937 (0.0993)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0983 (0.0994)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0983 (0.0994)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0956 (0.0994)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1019 (0.0995)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0980 (0.0994)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0925 (0.0993)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0935 (0.0993)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:87] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0935 (0.0993)\n",
      "Valid: [epoch:87]  [ 0/14]  eta: 0:00:14  loss: 0.0843 (0.0843)  time: 1.0186  data: 0.3945  max mem: 39763\n",
      "Valid: [epoch:87]  [13/14]  eta: 0:00:00  loss: 0.0860 (0.0885)  time: 0.1148  data: 0.0282  max mem: 39763\n",
      "Valid: [epoch:87] Total time: 0:00:01 (0.1238 s / it)\n",
      "Averaged stats: loss: 0.0860 (0.0885)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_87_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.089%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:88]  [  0/689]  eta: 0:12:10  lr: 0.000100  loss: 0.0892 (0.0892)  time: 1.0601  data: 0.5797  max mem: 39763\n",
      "Train: [epoch:88]  [ 10/689]  eta: 0:17:19  lr: 0.000100  loss: 0.0892 (0.0914)  time: 1.5302  data: 0.0528  max mem: 39763\n",
      "Train: [epoch:88]  [ 20/689]  eta: 0:17:19  lr: 0.000100  loss: 0.0893 (0.0922)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [ 30/689]  eta: 0:17:09  lr: 0.000100  loss: 0.0915 (0.0929)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0915 (0.0934)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0973 (0.0965)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [ 60/689]  eta: 0:16:27  lr: 0.000100  loss: 0.1039 (0.0974)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.0977 (0.0973)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.0967 (0.0975)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [ 90/689]  eta: 0:15:42  lr: 0.000100  loss: 0.0967 (0.0975)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.0945 (0.0979)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [110/689]  eta: 0:15:11  lr: 0.000100  loss: 0.0945 (0.0977)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0998 (0.0983)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [130/689]  eta: 0:14:40  lr: 0.000100  loss: 0.0998 (0.0984)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0994 (0.0987)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0990 (0.0987)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.0933 (0.0987)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.0913 (0.0987)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0913 (0.0986)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0937 (0.0987)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0989 (0.0988)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0924 (0.0984)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0907 (0.0982)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0985 (0.0986)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0969 (0.0985)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.0957 (0.0986)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0961 (0.0985)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0985 (0.0986)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0995 (0.0987)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0946 (0.0985)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0919 (0.0984)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0925 (0.0983)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0937 (0.0983)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.0971 (0.0988)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1097 (0.0992)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1015 (0.0992)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0998 (0.0992)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0965 (0.0991)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0947 (0.0990)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0967 (0.0991)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1002 (0.0992)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.1001 (0.0994)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0982 (0.0993)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0948 (0.0993)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0974 (0.0993)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:88]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0939 (0.0992)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.1002 (0.0993)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1029 (0.0993)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0972 (0.0992)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0975 (0.0992)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0957 (0.0991)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0990 (0.0993)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0997 (0.0993)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0941 (0.0992)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0898 (0.0991)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0952 (0.0990)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0955 (0.0990)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0990 (0.0991)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1010 (0.0992)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0972 (0.0992)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0945 (0.0991)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0968 (0.0991)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0972 (0.0992)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0992 (0.0992)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1059 (0.0994)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0971 (0.0993)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0969 (0.0994)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0984 (0.0994)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0984 (0.0995)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0984 (0.0995)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:88] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0984 (0.0995)\n",
      "Valid: [epoch:88]  [ 0/14]  eta: 0:00:13  loss: 0.0868 (0.0868)  time: 0.9693  data: 0.3593  max mem: 39763\n",
      "Valid: [epoch:88]  [13/14]  eta: 0:00:00  loss: 0.0938 (0.0951)  time: 0.1112  data: 0.0257  max mem: 39763\n",
      "Valid: [epoch:88] Total time: 0:00:01 (0.1195 s / it)\n",
      "Averaged stats: loss: 0.0938 (0.0951)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_88_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.095%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:89]  [  0/689]  eta: 0:11:40  lr: 0.000100  loss: 0.0979 (0.0979)  time: 1.0170  data: 0.5390  max mem: 39763\n",
      "Train: [epoch:89]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0979 (0.0991)  time: 1.5246  data: 0.0491  max mem: 39763\n",
      "Train: [epoch:89]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0950 (0.0987)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [ 30/689]  eta: 0:17:06  lr: 0.000100  loss: 0.0976 (0.1010)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0972 (0.0998)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.0956 (0.0992)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0915 (0.0990)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0968 (0.1003)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0993 (0.1001)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0993 (0.1004)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.1000 (0.1007)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1049 (0.1014)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0992 (0.1013)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0992 (0.1019)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.1085 (0.1027)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1057 (0.1036)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1057 (0.1038)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1062 (0.1039)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1083 (0.1043)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1061 (0.1044)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.1067 (0.1053)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.1153 (0.1057)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.1072 (0.1058)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.1036 (0.1058)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1025 (0.1057)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0988 (0.1054)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.0968 (0.1053)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.1046 (0.1054)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1076 (0.1055)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1014 (0.1053)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.1014 (0.1053)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0997 (0.1051)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0953 (0.1048)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0948 (0.1046)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0946 (0.1043)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0946 (0.1043)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1004 (0.1041)  time: 1.5777  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:89]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0995 (0.1041)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0969 (0.1038)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0968 (0.1037)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1024 (0.1036)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0988 (0.1035)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0940 (0.1032)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.0936 (0.1030)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.0942 (0.1030)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0917 (0.1027)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0906 (0.1025)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0924 (0.1024)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0902 (0.1023)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.0942 (0.1024)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.1011 (0.1024)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0995 (0.1024)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1011 (0.1023)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1011 (0.1024)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.0968 (0.1023)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1018 (0.1024)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0973 (0.1023)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0937 (0.1021)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0984 (0.1021)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0962 (0.1020)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0925 (0.1018)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0926 (0.1018)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0937 (0.1017)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0960 (0.1016)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0976 (0.1017)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0949 (0.1016)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0942 (0.1015)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0910 (0.1014)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0910 (0.1013)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0910 (0.1012)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:89] Total time: 0:18:06 (1.5772 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0910 (0.1012)\n",
      "Valid: [epoch:89]  [ 0/14]  eta: 0:00:14  loss: 0.0789 (0.0789)  time: 1.0083  data: 0.3698  max mem: 39763\n",
      "Valid: [epoch:89]  [13/14]  eta: 0:00:00  loss: 0.0887 (0.0909)  time: 0.1140  data: 0.0265  max mem: 39763\n",
      "Valid: [epoch:89] Total time: 0:00:01 (0.1224 s / it)\n",
      "Averaged stats: loss: 0.0887 (0.0909)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_89_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.091%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:90]  [  0/689]  eta: 0:11:49  lr: 0.000100  loss: 0.1156 (0.1156)  time: 1.0304  data: 0.5517  max mem: 39763\n",
      "Train: [epoch:90]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0935 (0.0958)  time: 1.5270  data: 0.0502  max mem: 39763\n",
      "Train: [epoch:90]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0935 (0.0956)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.1012 (0.0982)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.1012 (0.0983)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0947 (0.0990)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0963 (0.0990)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0963 (0.0990)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0957 (0.0986)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0951 (0.0982)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0978 (0.0988)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.0993 (0.0991)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0972 (0.0997)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1004 (0.0999)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0948 (0.0995)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0963 (0.0996)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1015 (0.0998)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1017 (0.1000)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1006 (0.1000)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0965 (0.1000)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.0958 (0.0997)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0937 (0.0995)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0944 (0.0995)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0969 (0.0997)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0976 (0.0997)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0944 (0.0997)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0930 (0.0997)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0968 (0.0998)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1037 (0.1001)  time: 1.5794  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:90]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0986 (0.0999)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0937 (0.0999)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0916 (0.0997)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0964 (0.0998)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.0964 (0.0997)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.0989 (0.0998)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.0996 (0.0998)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0943 (0.0999)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.0936 (0.0997)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0919 (0.0995)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0926 (0.0995)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.0944 (0.0994)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.0956 (0.0995)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.0935 (0.0995)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1013 (0.0996)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1008 (0.0996)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0997 (0.0998)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.1082 (0.1000)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1099 (0.1002)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1047 (0.1004)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1047 (0.1005)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.1042 (0.1006)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0999 (0.1006)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0998 (0.1007)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0979 (0.1007)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0945 (0.1006)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0940 (0.1005)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0996 (0.1005)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1040 (0.1006)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0957 (0.1005)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0970 (0.1006)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0970 (0.1006)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0952 (0.1005)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0952 (0.1005)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0959 (0.1005)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.0954 (0.1004)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.0951 (0.1003)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.0906 (0.1003)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0964 (0.1004)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0985 (0.1003)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0985 (0.1004)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:90] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0985 (0.1004)\n",
      "Valid: [epoch:90]  [ 0/14]  eta: 0:00:13  loss: 0.0979 (0.0979)  time: 0.9657  data: 0.3947  max mem: 39763\n",
      "Valid: [epoch:90]  [13/14]  eta: 0:00:00  loss: 0.0926 (0.0935)  time: 0.1109  data: 0.0282  max mem: 39763\n",
      "Valid: [epoch:90] Total time: 0:00:01 (0.1201 s / it)\n",
      "Averaged stats: loss: 0.0926 (0.0935)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_90_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.093%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:91]  [  0/689]  eta: 0:11:43  lr: 0.000100  loss: 0.1077 (0.1077)  time: 1.0213  data: 0.5456  max mem: 39763\n",
      "Train: [epoch:91]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.0928 (0.0976)  time: 1.5247  data: 0.0497  max mem: 39763\n",
      "Train: [epoch:91]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0930 (0.0967)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [ 30/689]  eta: 0:17:06  lr: 0.000100  loss: 0.0956 (0.0963)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [ 40/689]  eta: 0:16:53  lr: 0.000100  loss: 0.0992 (0.0976)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.1016 (0.0980)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [ 60/689]  eta: 0:16:25  lr: 0.000100  loss: 0.0933 (0.0974)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [ 70/689]  eta: 0:16:10  lr: 0.000100  loss: 0.0937 (0.0973)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [ 80/689]  eta: 0:15:55  lr: 0.000100  loss: 0.0937 (0.0974)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.0931 (0.0972)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.0949 (0.0978)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1006 (0.0985)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.0966 (0.0981)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.0936 (0.0980)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.0958 (0.0983)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0977 (0.0983)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.0977 (0.0986)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.0988 (0.0988)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.0988 (0.0989)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.0983 (0.0991)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.0948 (0.0990)  time: 1.5787  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:91]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0934 (0.0989)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.1010 (0.0991)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.1000 (0.0990)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1023 (0.0996)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1085 (0.0999)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.1086 (0.1003)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.1144 (0.1007)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1149 (0.1012)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1036 (0.1011)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0976 (0.1014)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1033 (0.1017)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1052 (0.1021)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.1106 (0.1024)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1080 (0.1025)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1059 (0.1026)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1012 (0.1026)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.1017 (0.1028)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.1060 (0.1029)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1122 (0.1032)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1058 (0.1033)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1055 (0.1033)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1028 (0.1034)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1022 (0.1035)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1002 (0.1035)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0990 (0.1036)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.0958 (0.1035)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.0979 (0.1037)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1030 (0.1035)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1013 (0.1035)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.1087 (0.1037)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1074 (0.1037)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1013 (0.1036)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.0990 (0.1036)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0980 (0.1035)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.0981 (0.1035)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0981 (0.1034)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.0999 (0.1034)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1074 (0.1036)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1167 (0.1039)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1203 (0.1040)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1116 (0.1045)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1156 (0.1047)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1120 (0.1048)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1148 (0.1050)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1186 (0.1053)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1081 (0.1054)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1041 (0.1054)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1062 (0.1055)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1017 (0.1054)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:91] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1017 (0.1054)\n",
      "Valid: [epoch:91]  [ 0/14]  eta: 0:00:14  loss: 0.1025 (0.1025)  time: 1.0154  data: 0.3474  max mem: 39763\n",
      "Valid: [epoch:91]  [13/14]  eta: 0:00:00  loss: 0.0947 (0.0955)  time: 0.1145  data: 0.0249  max mem: 39763\n",
      "Valid: [epoch:91] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.0947 (0.0955)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_91_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.096%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:92]  [  0/689]  eta: 0:11:38  lr: 0.000100  loss: 0.1116 (0.1116)  time: 1.0142  data: 0.5338  max mem: 39763\n",
      "Train: [epoch:92]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0990 (0.0997)  time: 1.5259  data: 0.0486  max mem: 39763\n",
      "Train: [epoch:92]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0990 (0.1000)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.1014 (0.0993)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.1014 (0.1003)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.0984 (0.1001)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0961 (0.0998)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.0994 (0.1004)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.0969 (0.1000)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0966 (0.1000)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.1024 (0.1007)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1045 (0.1022)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.1069 (0.1021)  time: 1.5781  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:92]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1026 (0.1022)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.1026 (0.1025)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1019 (0.1022)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1023 (0.1023)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1021 (0.1021)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1021 (0.1028)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1101 (0.1030)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.1032 (0.1027)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0963 (0.1027)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0972 (0.1026)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0981 (0.1025)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.0976 (0.1023)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.0965 (0.1021)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0979 (0.1021)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0979 (0.1020)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1007 (0.1023)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1030 (0.1022)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0986 (0.1021)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0973 (0.1020)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0992 (0.1019)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.1016 (0.1020)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1040 (0.1022)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1022 (0.1022)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0956 (0.1020)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.0965 (0.1021)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.0945 (0.1018)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.0957 (0.1019)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1024 (0.1020)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.1053 (0.1026)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1147 (0.1029)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1125 (0.1032)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1097 (0.1034)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.1054 (0.1033)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.1033 (0.1035)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1026 (0.1035)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0984 (0.1035)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [490/689]  eta: 0:05:14  lr: 0.000100  loss: 0.0984 (0.1034)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.0990 (0.1033)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0990 (0.1034)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0994 (0.1034)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [530/689]  eta: 0:04:11  lr: 0.000100  loss: 0.0978 (0.1034)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.0996 (0.1034)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1014 (0.1035)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1056 (0.1036)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1053 (0.1036)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.1001 (0.1035)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0968 (0.1035)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0989 (0.1035)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0996 (0.1034)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0983 (0.1034)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0983 (0.1033)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1046 (0.1034)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1065 (0.1034)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1068 (0.1035)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [670/689]  eta: 0:00:30  lr: 0.000100  loss: 0.1052 (0.1034)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1052 (0.1035)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1024 (0.1035)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:92] Total time: 0:18:08 (1.5800 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1024 (0.1035)\n",
      "Valid: [epoch:92]  [ 0/14]  eta: 0:00:14  loss: 0.1053 (0.1053)  time: 1.0467  data: 0.3755  max mem: 39763\n",
      "Valid: [epoch:92]  [13/14]  eta: 0:00:00  loss: 0.0953 (0.0965)  time: 0.1168  data: 0.0269  max mem: 39763\n",
      "Valid: [epoch:92] Total time: 0:00:01 (0.1260 s / it)\n",
      "Averaged stats: loss: 0.0953 (0.0965)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_92_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.097%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:93]  [  0/689]  eta: 0:11:50  lr: 0.000100  loss: 0.1275 (0.1275)  time: 1.0312  data: 0.5534  max mem: 39763\n",
      "Train: [epoch:93]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.0965 (0.1026)  time: 1.5265  data: 0.0504  max mem: 39763\n",
      "Train: [epoch:93]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0992 (0.1025)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0992 (0.1036)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.0965 (0.1031)  time: 1.5784  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:93]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.1032 (0.1048)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.1039 (0.1042)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.1012 (0.1050)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [ 80/689]  eta: 0:15:57  lr: 0.000100  loss: 0.1079 (0.1053)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.1037 (0.1048)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.1001 (0.1047)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1031 (0.1048)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.1033 (0.1047)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1030 (0.1045)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.0957 (0.1041)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.0976 (0.1039)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.1078 (0.1044)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1078 (0.1045)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1024 (0.1043)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.0992 (0.1042)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.1023 (0.1043)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.1023 (0.1040)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.1003 (0.1039)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.0997 (0.1040)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1017 (0.1040)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1049 (0.1042)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.1042 (0.1043)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0986 (0.1043)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0979 (0.1043)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.0937 (0.1039)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.0920 (0.1036)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1068 (0.1062)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1406 (0.1077)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.1330 (0.1086)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1317 (0.1093)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1287 (0.1099)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1242 (0.1103)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.1135 (0.1104)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.1086 (0.1104)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1081 (0.1102)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1016 (0.1102)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1016 (0.1102)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1005 (0.1100)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1025 (0.1101)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1078 (0.1101)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1055 (0.1099)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.1021 (0.1099)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1027 (0.1098)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1013 (0.1095)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1020 (0.1095)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.1054 (0.1094)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1049 (0.1093)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1042 (0.1092)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1085 (0.1094)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.1175 (0.1095)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1093 (0.1095)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1040 (0.1094)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1033 (0.1093)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.0963 (0.1092)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0970 (0.1090)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0994 (0.1089)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1012 (0.1089)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0992 (0.1087)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.0973 (0.1086)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1029 (0.1085)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1014 (0.1084)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1037 (0.1085)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1018 (0.1083)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.0978 (0.1082)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1019 (0.1081)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:93] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1019 (0.1081)\n",
      "Valid: [epoch:93]  [ 0/14]  eta: 0:00:14  loss: 0.1024 (0.1024)  time: 1.0136  data: 0.4037  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:93]  [13/14]  eta: 0:00:00  loss: 0.0976 (0.0986)  time: 0.1143  data: 0.0289  max mem: 39763\n",
      "Valid: [epoch:93] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.0976 (0.0986)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_93_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.099%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:94]  [  0/689]  eta: 0:11:46  lr: 0.000100  loss: 0.0965 (0.0965)  time: 1.0252  data: 0.5453  max mem: 39763\n",
      "Train: [epoch:94]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.1048 (0.1072)  time: 1.5273  data: 0.0497  max mem: 39763\n",
      "Train: [epoch:94]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.1081 (0.1090)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.1039 (0.1067)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.1057 (0.1083)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.1123 (0.1086)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.1022 (0.1076)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1005 (0.1069)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1005 (0.1063)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0984 (0.1063)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.1072 (0.1066)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1128 (0.1068)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.1149 (0.1074)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1110 (0.1085)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.1089 (0.1084)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1047 (0.1081)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.1027 (0.1079)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1027 (0.1076)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1056 (0.1076)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.1028 (0.1072)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.1009 (0.1069)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.1056 (0.1069)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.1110 (0.1078)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.1178 (0.1080)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1106 (0.1083)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [250/689]  eta: 0:11:32  lr: 0.000100  loss: 0.1069 (0.1079)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0988 (0.1077)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0988 (0.1075)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1012 (0.1073)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.1005 (0.1073)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.1005 (0.1072)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0991 (0.1070)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0999 (0.1070)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.1046 (0.1070)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1046 (0.1070)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1003 (0.1069)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.0986 (0.1068)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.1064 (0.1068)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.1069 (0.1068)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1015 (0.1067)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1013 (0.1066)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [410/689]  eta: 0:07:20  lr: 0.000100  loss: 0.1013 (0.1065)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1011 (0.1064)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1011 (0.1063)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1039 (0.1064)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [450/689]  eta: 0:06:17  lr: 0.000100  loss: 0.1135 (0.1065)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.1132 (0.1067)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1029 (0.1066)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.0992 (0.1066)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1041 (0.1067)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.1066 (0.1066)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.0996 (0.1066)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.0991 (0.1065)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1031 (0.1065)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.1043 (0.1065)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1075 (0.1066)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1053 (0.1066)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1021 (0.1065)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [580/689]  eta: 0:02:52  lr: 0.000100  loss: 0.0988 (0.1064)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.0978 (0.1063)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0978 (0.1062)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0957 (0.1061)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1028 (0.1060)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:94]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1028 (0.1060)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1060 (0.1061)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1091 (0.1062)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1026 (0.1061)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1015 (0.1060)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1025 (0.1060)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0974 (0.1059)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:94] Total time: 0:18:07 (1.5783 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0974 (0.1059)\n",
      "Valid: [epoch:94]  [ 0/14]  eta: 0:00:14  loss: 0.0916 (0.0916)  time: 1.0171  data: 0.3998  max mem: 39763\n",
      "Valid: [epoch:94]  [13/14]  eta: 0:00:00  loss: 0.0966 (0.0975)  time: 0.1147  data: 0.0286  max mem: 39763\n",
      "Valid: [epoch:94] Total time: 0:00:01 (0.1240 s / it)\n",
      "Averaged stats: loss: 0.0966 (0.0975)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_94_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.097%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:95]  [  0/689]  eta: 0:11:58  lr: 0.000100  loss: 0.0996 (0.0996)  time: 1.0434  data: 0.5685  max mem: 39763\n",
      "Train: [epoch:95]  [ 10/689]  eta: 0:17:17  lr: 0.000100  loss: 0.0991 (0.1071)  time: 1.5283  data: 0.0518  max mem: 39763\n",
      "Train: [epoch:95]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.0983 (0.1017)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.0974 (0.1028)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.1015 (0.1037)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.1015 (0.1037)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.1007 (0.1036)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1035 (0.1039)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1018 (0.1036)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.0967 (0.1034)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.1019 (0.1040)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1042 (0.1042)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.0989 (0.1039)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1033 (0.1049)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.1115 (0.1052)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1055 (0.1053)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [160/689]  eta: 0:13:53  lr: 0.000100  loss: 0.1052 (0.1053)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1023 (0.1053)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1015 (0.1051)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [190/689]  eta: 0:13:06  lr: 0.000100  loss: 0.1099 (0.1060)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.1148 (0.1062)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0991 (0.1062)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [220/689]  eta: 0:12:19  lr: 0.000100  loss: 0.0981 (0.1059)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.1006 (0.1061)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1065 (0.1062)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1071 (0.1063)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0967 (0.1059)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.0956 (0.1057)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1010 (0.1059)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [290/689]  eta: 0:10:29  lr: 0.000100  loss: 0.1073 (0.1058)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.1035 (0.1058)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1006 (0.1056)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1008 (0.1057)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [330/689]  eta: 0:09:26  lr: 0.000100  loss: 0.1054 (0.1057)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1052 (0.1058)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1073 (0.1059)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1045 (0.1059)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [370/689]  eta: 0:08:23  lr: 0.000100  loss: 0.1006 (0.1059)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.1006 (0.1058)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1053 (0.1061)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1078 (0.1061)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1051 (0.1060)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1005 (0.1058)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1013 (0.1059)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1094 (0.1060)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1020 (0.1059)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.1081 (0.1062)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1125 (0.1063)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1051 (0.1063)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1107 (0.1065)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.1143 (0.1066)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1090 (0.1065)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1065 (0.1067)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1122 (0.1069)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [540/689]  eta: 0:03:55  lr: 0.000100  loss: 0.1088 (0.1069)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:95]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1010 (0.1067)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.0989 (0.1067)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1028 (0.1067)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1028 (0.1067)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1024 (0.1067)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1014 (0.1066)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1014 (0.1065)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.0981 (0.1064)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1036 (0.1065)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1032 (0.1064)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1025 (0.1064)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1027 (0.1064)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1010 (0.1064)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1041 (0.1066)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1189 (0.1070)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:95] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1189 (0.1070)\n",
      "Valid: [epoch:95]  [ 0/14]  eta: 0:00:14  loss: 0.1015 (0.1015)  time: 1.0115  data: 0.3993  max mem: 39763\n",
      "Valid: [epoch:95]  [13/14]  eta: 0:00:00  loss: 0.1118 (0.1119)  time: 0.1142  data: 0.0286  max mem: 39763\n",
      "Valid: [epoch:95] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.1118 (0.1119)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_95_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.112%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:96]  [  0/689]  eta: 0:11:41  lr: 0.000100  loss: 0.1548 (0.1548)  time: 1.0184  data: 0.5386  max mem: 39763\n",
      "Train: [epoch:96]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.1260 (0.1298)  time: 1.5256  data: 0.0490  max mem: 39763\n",
      "Train: [epoch:96]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.1222 (0.1254)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.1228 (0.1256)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.1107 (0.1217)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.1087 (0.1202)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.1157 (0.1195)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1088 (0.1175)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1080 (0.1168)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.1039 (0.1152)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.1039 (0.1148)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1095 (0.1150)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.1084 (0.1141)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1019 (0.1133)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.1018 (0.1127)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1007 (0.1123)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1049 (0.1121)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1054 (0.1116)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1056 (0.1114)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1055 (0.1112)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.1001 (0.1105)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0998 (0.1103)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.1031 (0.1101)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.1024 (0.1097)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1024 (0.1096)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1033 (0.1094)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.1033 (0.1091)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.1037 (0.1091)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1099 (0.1092)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1045 (0.1090)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.1023 (0.1089)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1044 (0.1089)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1059 (0.1090)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.1119 (0.1092)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1096 (0.1091)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1027 (0.1090)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1047 (0.1090)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.1119 (0.1092)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.1137 (0.1092)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1137 (0.1095)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1120 (0.1096)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1049 (0.1096)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1042 (0.1096)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1042 (0.1096)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1055 (0.1096)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1060 (0.1096)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.1111 (0.1101)  time: 1.5778  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:96]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1111 (0.1103)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1133 (0.1105)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1102 (0.1104)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.1043 (0.1103)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1076 (0.1104)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1096 (0.1104)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1040 (0.1103)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.1079 (0.1105)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1168 (0.1106)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1098 (0.1105)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1049 (0.1104)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1098 (0.1105)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1101 (0.1106)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1065 (0.1105)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1065 (0.1105)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1038 (0.1103)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1049 (0.1103)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1067 (0.1103)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1048 (0.1102)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1052 (0.1102)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1086 (0.1103)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1084 (0.1104)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1000 (0.1102)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:96] Total time: 0:18:06 (1.5773 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1000 (0.1102)\n",
      "Valid: [epoch:96]  [ 0/14]  eta: 0:00:14  loss: 0.0933 (0.0933)  time: 1.0117  data: 0.3980  max mem: 39763\n",
      "Valid: [epoch:96]  [13/14]  eta: 0:00:00  loss: 0.0988 (0.0999)  time: 0.1142  data: 0.0285  max mem: 39763\n",
      "Valid: [epoch:96] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.0988 (0.0999)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_96_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.100%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:97]  [  0/689]  eta: 0:12:26  lr: 0.000100  loss: 0.1022 (0.1022)  time: 1.0834  data: 0.6075  max mem: 39763\n",
      "Train: [epoch:97]  [ 10/689]  eta: 0:17:19  lr: 0.000100  loss: 0.1083 (0.1052)  time: 1.5311  data: 0.0553  max mem: 39763\n",
      "Train: [epoch:97]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.1004 (0.1020)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.1009 (0.1037)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.1060 (0.1051)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.1141 (0.1069)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.1141 (0.1091)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [ 70/689]  eta: 0:16:12  lr: 0.000100  loss: 0.1043 (0.1083)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1011 (0.1077)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.1023 (0.1073)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.1023 (0.1070)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1047 (0.1072)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.1111 (0.1079)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1083 (0.1080)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.1094 (0.1088)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1143 (0.1092)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1163 (0.1106)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1299 (0.1117)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1249 (0.1124)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1101 (0.1124)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.1081 (0.1122)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.0976 (0.1115)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.0967 (0.1113)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.1015 (0.1110)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1031 (0.1109)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1013 (0.1105)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [260/689]  eta: 0:11:16  lr: 0.000100  loss: 0.0974 (0.1103)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.1108 (0.1103)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.0988 (0.1097)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0909 (0.1093)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.1027 (0.1095)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1123 (0.1095)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1087 (0.1096)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.1104 (0.1098)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1038 (0.1095)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1006 (0.1094)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1046 (0.1093)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.1015 (0.1092)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.1015 (0.1092)  time: 1.5774  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:97]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1130 (0.1094)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1177 (0.1095)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1101 (0.1095)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1101 (0.1097)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1116 (0.1097)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1042 (0.1096)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1030 (0.1095)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.1074 (0.1095)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1122 (0.1097)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1124 (0.1097)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1140 (0.1098)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.1236 (0.1103)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1189 (0.1104)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1141 (0.1105)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1073 (0.1104)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.1037 (0.1103)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1048 (0.1102)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1012 (0.1101)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1043 (0.1100)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1039 (0.1100)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1001 (0.1098)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.0974 (0.1097)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.0968 (0.1096)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1030 (0.1095)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1033 (0.1094)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1033 (0.1093)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1046 (0.1094)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1090 (0.1094)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1070 (0.1094)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1023 (0.1093)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1026 (0.1093)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:97] Total time: 0:18:06 (1.5770 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1026 (0.1093)\n",
      "Valid: [epoch:97]  [ 0/14]  eta: 0:00:14  loss: 0.1091 (0.1091)  time: 1.0176  data: 0.3931  max mem: 39763\n",
      "Valid: [epoch:97]  [13/14]  eta: 0:00:00  loss: 0.0962 (0.0987)  time: 0.1146  data: 0.0281  max mem: 39763\n",
      "Valid: [epoch:97] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.0962 (0.0987)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_97_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.099%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:98]  [  0/689]  eta: 0:11:40  lr: 0.000100  loss: 0.1239 (0.1239)  time: 1.0166  data: 0.5381  max mem: 39763\n",
      "Train: [epoch:98]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.1045 (0.1075)  time: 1.5256  data: 0.0490  max mem: 39763\n",
      "Train: [epoch:98]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.1038 (0.1049)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.1028 (0.1050)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.0973 (0.1039)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.1029 (0.1064)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.1060 (0.1069)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1060 (0.1074)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1056 (0.1075)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.1057 (0.1080)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.1041 (0.1077)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1058 (0.1080)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.1136 (0.1094)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1171 (0.1106)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.1158 (0.1111)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [150/689]  eta: 0:14:07  lr: 0.000100  loss: 0.1073 (0.1107)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1038 (0.1105)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.1035 (0.1102)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [180/689]  eta: 0:13:20  lr: 0.000100  loss: 0.1053 (0.1104)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1068 (0.1104)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.1060 (0.1103)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [210/689]  eta: 0:12:33  lr: 0.000100  loss: 0.1045 (0.1103)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.1066 (0.1103)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.1106 (0.1104)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [240/689]  eta: 0:11:46  lr: 0.000100  loss: 0.1094 (0.1103)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1065 (0.1102)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.1059 (0.1101)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [270/689]  eta: 0:10:59  lr: 0.000100  loss: 0.1031 (0.1099)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1020 (0.1097)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.0995 (0.1095)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.1033 (0.1094)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:98]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1025 (0.1092)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.0984 (0.1089)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.1007 (0.1087)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [340/689]  eta: 0:09:09  lr: 0.000100  loss: 0.1033 (0.1086)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1079 (0.1089)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1116 (0.1090)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.1006 (0.1088)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [380/689]  eta: 0:08:06  lr: 0.000100  loss: 0.0986 (0.1086)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1018 (0.1086)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1073 (0.1087)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1088 (0.1088)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [420/689]  eta: 0:07:03  lr: 0.000100  loss: 0.1046 (0.1088)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1049 (0.1089)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1064 (0.1088)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1026 (0.1087)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [460/689]  eta: 0:06:00  lr: 0.000100  loss: 0.1029 (0.1087)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1032 (0.1087)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1032 (0.1087)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1011 (0.1087)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.1038 (0.1087)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1060 (0.1086)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1060 (0.1086)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1090 (0.1087)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.1101 (0.1087)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1034 (0.1086)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1034 (0.1086)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1054 (0.1086)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1064 (0.1086)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1029 (0.1086)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1062 (0.1086)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1103 (0.1086)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1127 (0.1088)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1141 (0.1089)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1110 (0.1089)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1098 (0.1089)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1074 (0.1089)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1063 (0.1089)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1035 (0.1088)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.0988 (0.1087)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:98] Total time: 0:18:06 (1.5771 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.0988 (0.1087)\n",
      "Valid: [epoch:98]  [ 0/14]  eta: 0:00:13  loss: 0.0938 (0.0938)  time: 0.9993  data: 0.3862  max mem: 39763\n",
      "Valid: [epoch:98]  [13/14]  eta: 0:00:00  loss: 0.0972 (0.0994)  time: 0.1133  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:98] Total time: 0:00:01 (0.1210 s / it)\n",
      "Averaged stats: loss: 0.0972 (0.0994)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_98_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.099%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:99]  [  0/689]  eta: 0:11:54  lr: 0.000100  loss: 0.1002 (0.1002)  time: 1.0375  data: 0.5578  max mem: 39763\n",
      "Train: [epoch:99]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.1042 (0.1086)  time: 1.5258  data: 0.0508  max mem: 39763\n",
      "Train: [epoch:99]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.1029 (0.1077)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.1023 (0.1080)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.1119 (0.1091)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.1048 (0.1073)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.0997 (0.1063)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1013 (0.1065)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1019 (0.1064)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.1002 (0.1058)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.1023 (0.1064)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1110 (0.1065)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.1103 (0.1071)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1088 (0.1075)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.1116 (0.1081)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1109 (0.1083)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1097 (0.1086)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.1097 (0.1088)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1065 (0.1087)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1070 (0.1088)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.1101 (0.1089)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.1072 (0.1087)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.1093 (0.1093)  time: 1.5771  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:99]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.1190 (0.1096)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1122 (0.1097)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1085 (0.1097)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.1053 (0.1096)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [270/689]  eta: 0:10:59  lr: 0.000100  loss: 0.0998 (0.1092)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1001 (0.1093)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1046 (0.1091)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.0986 (0.1090)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.0986 (0.1088)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1028 (0.1089)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.1102 (0.1091)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [340/689]  eta: 0:09:09  lr: 0.000100  loss: 0.1094 (0.1091)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1080 (0.1093)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1121 (0.1094)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.1043 (0.1092)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [380/689]  eta: 0:08:06  lr: 0.000100  loss: 0.1042 (0.1093)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1093 (0.1095)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1189 (0.1099)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1157 (0.1101)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [420/689]  eta: 0:07:03  lr: 0.000100  loss: 0.1144 (0.1101)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1033 (0.1101)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1025 (0.1099)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.0989 (0.1099)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [460/689]  eta: 0:06:00  lr: 0.000100  loss: 0.1128 (0.1100)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1101 (0.1099)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1033 (0.1098)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1091 (0.1098)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.1041 (0.1097)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1067 (0.1096)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1067 (0.1096)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1063 (0.1097)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.1101 (0.1099)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1170 (0.1100)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1218 (0.1105)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1286 (0.1107)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1163 (0.1108)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1106 (0.1112)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1151 (0.1113)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1068 (0.1111)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1060 (0.1111)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1116 (0.1112)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1205 (0.1114)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1059 (0.1113)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1031 (0.1113)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1117 (0.1113)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1168 (0.1114)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1104 (0.1114)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:99] Total time: 0:18:06 (1.5769 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1104 (0.1114)\n",
      "Valid: [epoch:99]  [ 0/14]  eta: 0:00:14  loss: 0.0983 (0.0983)  time: 1.0093  data: 0.3947  max mem: 39763\n",
      "Valid: [epoch:99]  [13/14]  eta: 0:00:00  loss: 0.1014 (0.1023)  time: 0.1141  data: 0.0283  max mem: 39763\n",
      "Valid: [epoch:99] Total time: 0:00:01 (0.1215 s / it)\n",
      "Averaged stats: loss: 0.1014 (0.1023)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_99_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.102%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:100]  [  0/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1026 (0.1026)  time: 1.0262  data: 0.5436  max mem: 39763\n",
      "Train: [epoch:100]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.1138 (0.1090)  time: 1.5263  data: 0.0495  max mem: 39763\n",
      "Train: [epoch:100]  [ 20/689]  eta: 0:17:17  lr: 0.000100  loss: 0.1070 (0.1071)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.1061 (0.1067)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.1058 (0.1062)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.1036 (0.1051)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.1037 (0.1059)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1047 (0.1057)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1040 (0.1062)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.1073 (0.1068)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.1109 (0.1077)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1192 (0.1094)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.1183 (0.1094)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1084 (0.1105)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.1202 (0.1123)  time: 1.5775  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:100]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1323 (0.1134)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1320 (0.1151)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.1290 (0.1160)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1202 (0.1164)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1132 (0.1167)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.1122 (0.1167)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.1150 (0.1172)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.1095 (0.1167)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.1055 (0.1167)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1148 (0.1166)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1020 (0.1160)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.1020 (0.1158)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.1052 (0.1158)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1092 (0.1156)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1092 (0.1155)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.1079 (0.1152)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1072 (0.1150)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1103 (0.1149)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.1090 (0.1149)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1057 (0.1145)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1019 (0.1142)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1061 (0.1141)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.1031 (0.1138)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.1029 (0.1138)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1029 (0.1136)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1024 (0.1134)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1051 (0.1133)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1051 (0.1131)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1068 (0.1132)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1161 (0.1132)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1078 (0.1131)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [460/689]  eta: 0:06:00  lr: 0.000100  loss: 0.1087 (0.1130)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1095 (0.1129)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1031 (0.1127)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1014 (0.1126)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.1141 (0.1127)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1167 (0.1128)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1178 (0.1130)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1268 (0.1136)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.1307 (0.1138)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1122 (0.1140)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1120 (0.1141)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1142 (0.1141)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1076 (0.1139)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1019 (0.1138)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1078 (0.1137)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1086 (0.1137)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1072 (0.1136)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1083 (0.1135)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1085 (0.1136)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1085 (0.1135)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1119 (0.1136)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1158 (0.1136)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1141 (0.1136)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1045 (0.1134)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:100] Total time: 0:18:06 (1.5770 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1045 (0.1134)\n",
      "Valid: [epoch:100]  [ 0/14]  eta: 0:00:14  loss: 0.0956 (0.0956)  time: 1.0057  data: 0.4193  max mem: 39763\n",
      "Valid: [epoch:100]  [13/14]  eta: 0:00:00  loss: 0.1004 (0.1022)  time: 0.1138  data: 0.0300  max mem: 39763\n",
      "Valid: [epoch:100] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.1004 (0.1022)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_100_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.102%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:101]  [  0/689]  eta: 0:11:55  lr: 0.000100  loss: 0.1143 (0.1143)  time: 1.0391  data: 0.5586  max mem: 39763\n",
      "Train: [epoch:101]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.1073 (0.1106)  time: 1.5256  data: 0.0509  max mem: 39763\n",
      "Train: [epoch:101]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.1049 (0.1074)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [ 30/689]  eta: 0:17:06  lr: 0.000100  loss: 0.1101 (0.1100)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.1116 (0.1096)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.1113 (0.1118)  time: 1.5763  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:101]  [ 60/689]  eta: 0:16:25  lr: 0.000100  loss: 0.1142 (0.1117)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1117 (0.1117)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1153 (0.1130)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.1212 (0.1142)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.1157 (0.1145)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1143 (0.1143)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.1143 (0.1147)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1176 (0.1148)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.1176 (0.1151)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1155 (0.1148)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1084 (0.1144)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.1048 (0.1143)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1109 (0.1142)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1104 (0.1143)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.1104 (0.1143)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.1085 (0.1141)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.1048 (0.1140)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.1062 (0.1137)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1058 (0.1134)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1058 (0.1131)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.1057 (0.1130)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.1081 (0.1130)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1101 (0.1128)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1100 (0.1128)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.1100 (0.1127)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1126 (0.1128)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1146 (0.1129)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.1186 (0.1132)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [340/689]  eta: 0:09:09  lr: 0.000100  loss: 0.1060 (0.1131)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1059 (0.1130)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1086 (0.1128)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.1046 (0.1127)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [380/689]  eta: 0:08:06  lr: 0.000100  loss: 0.1088 (0.1126)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1088 (0.1125)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1038 (0.1126)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1035 (0.1124)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [420/689]  eta: 0:07:03  lr: 0.000100  loss: 0.1021 (0.1123)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1037 (0.1123)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1085 (0.1124)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1085 (0.1123)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [460/689]  eta: 0:06:00  lr: 0.000100  loss: 0.1071 (0.1122)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1071 (0.1122)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1103 (0.1122)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1113 (0.1123)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.1092 (0.1123)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1118 (0.1124)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1097 (0.1123)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1079 (0.1124)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.1065 (0.1122)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1047 (0.1122)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1140 (0.1122)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1136 (0.1122)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1121 (0.1122)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1170 (0.1123)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1198 (0.1124)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1161 (0.1124)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1053 (0.1123)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1053 (0.1123)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1092 (0.1123)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1071 (0.1123)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1071 (0.1122)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1065 (0.1121)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1083 (0.1121)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1065 (0.1121)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:101] Total time: 0:18:06 (1.5766 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1065 (0.1121)\n",
      "Valid: [epoch:101]  [ 0/14]  eta: 0:00:14  loss: 0.0920 (0.0920)  time: 1.0296  data: 0.3855  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:101]  [13/14]  eta: 0:00:00  loss: 0.1009 (0.1033)  time: 0.1155  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:101] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.1009 (0.1033)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_101_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.103%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:102]  [  0/689]  eta: 0:11:36  lr: 0.000100  loss: 0.1367 (0.1367)  time: 1.0114  data: 0.5307  max mem: 39763\n",
      "Train: [epoch:102]  [ 10/689]  eta: 0:17:15  lr: 0.000100  loss: 0.1045 (0.1042)  time: 1.5250  data: 0.0483  max mem: 39763\n",
      "Train: [epoch:102]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.1050 (0.1071)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [ 30/689]  eta: 0:17:06  lr: 0.000100  loss: 0.1129 (0.1101)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.1135 (0.1113)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.1064 (0.1104)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [ 60/689]  eta: 0:16:25  lr: 0.000100  loss: 0.1025 (0.1096)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1015 (0.1087)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [ 80/689]  eta: 0:15:55  lr: 0.000100  loss: 0.1019 (0.1085)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.1054 (0.1088)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.1053 (0.1087)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [110/689]  eta: 0:15:09  lr: 0.000100  loss: 0.1053 (0.1087)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.1123 (0.1094)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1131 (0.1098)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.1088 (0.1096)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [150/689]  eta: 0:14:07  lr: 0.000100  loss: 0.1089 (0.1097)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1096 (0.1097)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.1087 (0.1097)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1089 (0.1101)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1073 (0.1100)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.1061 (0.1101)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.1065 (0.1100)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.1058 (0.1100)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.1138 (0.1106)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [240/689]  eta: 0:11:46  lr: 0.000100  loss: 0.1182 (0.1113)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1150 (0.1113)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.1043 (0.1111)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [270/689]  eta: 0:10:59  lr: 0.000100  loss: 0.1080 (0.1115)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1181 (0.1120)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1220 (0.1127)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.1170 (0.1129)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1170 (0.1132)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1136 (0.1132)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.1083 (0.1133)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [340/689]  eta: 0:09:09  lr: 0.000100  loss: 0.1106 (0.1133)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1114 (0.1135)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1114 (0.1135)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.1069 (0.1133)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [380/689]  eta: 0:08:06  lr: 0.000100  loss: 0.1097 (0.1133)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1131 (0.1133)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1167 (0.1136)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1134 (0.1135)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [420/689]  eta: 0:07:03  lr: 0.000100  loss: 0.1096 (0.1134)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1096 (0.1134)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1120 (0.1134)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1159 (0.1135)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [460/689]  eta: 0:06:00  lr: 0.000100  loss: 0.1159 (0.1137)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1141 (0.1137)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1128 (0.1137)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1117 (0.1137)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.1130 (0.1138)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1110 (0.1137)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1078 (0.1136)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1124 (0.1137)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.1167 (0.1139)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1195 (0.1141)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1127 (0.1142)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1127 (0.1142)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1086 (0.1142)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1072 (0.1141)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1063 (0.1140)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1078 (0.1140)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1171 (0.1141)  time: 1.5784  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:102]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1147 (0.1141)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1113 (0.1140)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1079 (0.1139)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1042 (0.1138)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.0987 (0.1137)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1030 (0.1137)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1096 (0.1137)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:102] Total time: 0:18:06 (1.5771 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1096 (0.1137)\n",
      "Valid: [epoch:102]  [ 0/14]  eta: 0:00:13  loss: 0.1098 (0.1098)  time: 0.9954  data: 0.4313  max mem: 39763\n",
      "Valid: [epoch:102]  [13/14]  eta: 0:00:00  loss: 0.1031 (0.1044)  time: 0.1132  data: 0.0309  max mem: 39763\n",
      "Valid: [epoch:102] Total time: 0:00:01 (0.1225 s / it)\n",
      "Averaged stats: loss: 0.1031 (0.1044)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_102_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.104%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:103]  [  0/689]  eta: 0:12:20  lr: 0.000100  loss: 0.1062 (0.1062)  time: 1.0749  data: 0.5995  max mem: 39763\n",
      "Train: [epoch:103]  [ 10/689]  eta: 0:17:19  lr: 0.000100  loss: 0.1103 (0.1115)  time: 1.5303  data: 0.0546  max mem: 39763\n",
      "Train: [epoch:103]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.1103 (0.1118)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.1101 (0.1126)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.1101 (0.1119)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.1066 (0.1107)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.1026 (0.1095)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1030 (0.1092)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1003 (0.1081)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.1023 (0.1086)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.1101 (0.1092)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1120 (0.1094)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.1118 (0.1097)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1062 (0.1092)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [140/689]  eta: 0:14:24  lr: 0.000100  loss: 0.1024 (0.1089)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1044 (0.1088)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1059 (0.1095)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1075 (0.1096)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1087 (0.1097)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1111 (0.1101)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.1114 (0.1104)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.1114 (0.1106)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.1094 (0.1107)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.1183 (0.1111)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1152 (0.1112)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1083 (0.1110)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.1067 (0.1111)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.1140 (0.1112)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1152 (0.1114)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1070 (0.1114)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.1098 (0.1114)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1106 (0.1114)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1102 (0.1116)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.1102 (0.1115)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1040 (0.1116)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1137 (0.1117)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1137 (0.1119)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.1132 (0.1121)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.1094 (0.1121)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1100 (0.1121)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1100 (0.1122)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1091 (0.1122)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1090 (0.1122)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1063 (0.1121)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1065 (0.1121)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1109 (0.1120)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.1140 (0.1123)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1085 (0.1123)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1034 (0.1122)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1135 (0.1124)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.1161 (0.1124)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1107 (0.1125)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1136 (0.1127)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1141 (0.1126)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:103]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.1064 (0.1126)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1092 (0.1126)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1132 (0.1127)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1153 (0.1127)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1098 (0.1127)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1148 (0.1127)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1158 (0.1128)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1083 (0.1127)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1092 (0.1128)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1065 (0.1127)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1060 (0.1126)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1095 (0.1127)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1123 (0.1126)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1123 (0.1127)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1064 (0.1127)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1085 (0.1126)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:103] Total time: 0:18:06 (1.5774 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1085 (0.1126)\n",
      "Valid: [epoch:103]  [ 0/14]  eta: 0:00:14  loss: 0.1128 (0.1128)  time: 1.0436  data: 0.3785  max mem: 39763\n",
      "Valid: [epoch:103]  [13/14]  eta: 0:00:00  loss: 0.1033 (0.1045)  time: 0.1165  data: 0.0271  max mem: 39763\n",
      "Valid: [epoch:103] Total time: 0:00:01 (0.1258 s / it)\n",
      "Averaged stats: loss: 0.1033 (0.1045)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_103_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.105%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:104]  [  0/689]  eta: 0:12:15  lr: 0.000100  loss: 0.1292 (0.1292)  time: 1.0675  data: 0.5924  max mem: 39763\n",
      "Train: [epoch:104]  [ 10/689]  eta: 0:17:19  lr: 0.000100  loss: 0.1124 (0.1148)  time: 1.5308  data: 0.0539  max mem: 39763\n",
      "Train: [epoch:104]  [ 20/689]  eta: 0:17:18  lr: 0.000100  loss: 0.1094 (0.1144)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [ 30/689]  eta: 0:17:08  lr: 0.000100  loss: 0.1115 (0.1158)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [ 40/689]  eta: 0:16:55  lr: 0.000100  loss: 0.1145 (0.1162)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [ 50/689]  eta: 0:16:41  lr: 0.000100  loss: 0.1148 (0.1160)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.1083 (0.1146)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1071 (0.1135)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1050 (0.1130)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [ 90/689]  eta: 0:15:41  lr: 0.000100  loss: 0.1056 (0.1129)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [100/689]  eta: 0:15:26  lr: 0.000100  loss: 0.1086 (0.1132)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1063 (0.1128)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [120/689]  eta: 0:14:55  lr: 0.000100  loss: 0.1036 (0.1122)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1060 (0.1129)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.1130 (0.1133)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [150/689]  eta: 0:14:08  lr: 0.000100  loss: 0.1115 (0.1131)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1086 (0.1135)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [170/689]  eta: 0:13:37  lr: 0.000100  loss: 0.1083 (0.1132)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1077 (0.1131)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1101 (0.1132)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [200/689]  eta: 0:12:50  lr: 0.000100  loss: 0.1113 (0.1133)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.1136 (0.1136)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.1136 (0.1136)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [230/689]  eta: 0:12:03  lr: 0.000100  loss: 0.1076 (0.1133)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1054 (0.1132)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1044 (0.1129)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.1044 (0.1129)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [270/689]  eta: 0:11:00  lr: 0.000100  loss: 0.1077 (0.1126)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1075 (0.1125)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1105 (0.1126)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [300/689]  eta: 0:10:13  lr: 0.000100  loss: 0.1133 (0.1126)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1091 (0.1125)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1078 (0.1126)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.1095 (0.1127)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [340/689]  eta: 0:09:10  lr: 0.000100  loss: 0.1146 (0.1129)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1210 (0.1132)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1211 (0.1133)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.1100 (0.1134)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [380/689]  eta: 0:08:07  lr: 0.000100  loss: 0.1089 (0.1132)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1077 (0.1131)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1195 (0.1133)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1195 (0.1133)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [420/689]  eta: 0:07:04  lr: 0.000100  loss: 0.1129 (0.1132)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1135 (0.1133)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1113 (0.1132)  time: 1.5788  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:104]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1050 (0.1132)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [460/689]  eta: 0:06:01  lr: 0.000100  loss: 0.1091 (0.1134)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1139 (0.1135)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1212 (0.1138)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1239 (0.1144)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [500/689]  eta: 0:04:58  lr: 0.000100  loss: 0.1500 (0.1156)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1394 (0.1159)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1261 (0.1164)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1411 (0.1171)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.1411 (0.1174)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1311 (0.1176)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1195 (0.1176)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1217 (0.1180)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1303 (0.1182)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1211 (0.1183)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1131 (0.1183)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1112 (0.1182)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1112 (0.1182)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1135 (0.1183)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1169 (0.1184)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1166 (0.1184)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1129 (0.1185)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1097 (0.1184)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1078 (0.1183)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1083 (0.1183)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:104] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1083 (0.1183)\n",
      "Valid: [epoch:104]  [ 0/14]  eta: 0:00:14  loss: 0.1015 (0.1015)  time: 1.0081  data: 0.4251  max mem: 39763\n",
      "Valid: [epoch:104]  [13/14]  eta: 0:00:00  loss: 0.1064 (0.1078)  time: 0.1139  data: 0.0304  max mem: 39763\n",
      "Valid: [epoch:104] Total time: 0:00:01 (0.1233 s / it)\n",
      "Averaged stats: loss: 0.1064 (0.1078)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_104_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.108%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:105]  [  0/689]  eta: 0:11:51  lr: 0.000100  loss: 0.1053 (0.1053)  time: 1.0333  data: 0.5547  max mem: 39763\n",
      "Train: [epoch:105]  [ 10/689]  eta: 0:17:16  lr: 0.000100  loss: 0.1104 (0.1160)  time: 1.5259  data: 0.0505  max mem: 39763\n",
      "Train: [epoch:105]  [ 20/689]  eta: 0:17:16  lr: 0.000100  loss: 0.1132 (0.1168)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [ 30/689]  eta: 0:17:07  lr: 0.000100  loss: 0.1202 (0.1193)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [ 40/689]  eta: 0:16:54  lr: 0.000100  loss: 0.1173 (0.1188)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [ 50/689]  eta: 0:16:40  lr: 0.000100  loss: 0.1136 (0.1177)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [ 60/689]  eta: 0:16:26  lr: 0.000100  loss: 0.1103 (0.1174)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [ 70/689]  eta: 0:16:11  lr: 0.000100  loss: 0.1115 (0.1174)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [ 80/689]  eta: 0:15:56  lr: 0.000100  loss: 0.1115 (0.1175)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [ 90/689]  eta: 0:15:40  lr: 0.000100  loss: 0.1139 (0.1174)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [100/689]  eta: 0:15:25  lr: 0.000100  loss: 0.1121 (0.1174)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [110/689]  eta: 0:15:10  lr: 0.000100  loss: 0.1131 (0.1177)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [120/689]  eta: 0:14:54  lr: 0.000100  loss: 0.1174 (0.1177)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [130/689]  eta: 0:14:39  lr: 0.000100  loss: 0.1159 (0.1177)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [140/689]  eta: 0:14:23  lr: 0.000100  loss: 0.1109 (0.1172)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [150/689]  eta: 0:14:07  lr: 0.000100  loss: 0.1140 (0.1177)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [160/689]  eta: 0:13:52  lr: 0.000100  loss: 0.1154 (0.1174)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [170/689]  eta: 0:13:36  lr: 0.000100  loss: 0.1146 (0.1174)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [180/689]  eta: 0:13:21  lr: 0.000100  loss: 0.1146 (0.1172)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [190/689]  eta: 0:13:05  lr: 0.000100  loss: 0.1108 (0.1171)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [200/689]  eta: 0:12:49  lr: 0.000100  loss: 0.1070 (0.1165)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [210/689]  eta: 0:12:34  lr: 0.000100  loss: 0.1080 (0.1165)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [220/689]  eta: 0:12:18  lr: 0.000100  loss: 0.1080 (0.1162)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [230/689]  eta: 0:12:02  lr: 0.000100  loss: 0.1086 (0.1160)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [240/689]  eta: 0:11:47  lr: 0.000100  loss: 0.1096 (0.1157)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [250/689]  eta: 0:11:31  lr: 0.000100  loss: 0.1116 (0.1157)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [260/689]  eta: 0:11:15  lr: 0.000100  loss: 0.1116 (0.1155)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [270/689]  eta: 0:10:59  lr: 0.000100  loss: 0.1083 (0.1153)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [280/689]  eta: 0:10:44  lr: 0.000100  loss: 0.1084 (0.1151)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [290/689]  eta: 0:10:28  lr: 0.000100  loss: 0.1086 (0.1151)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [300/689]  eta: 0:10:12  lr: 0.000100  loss: 0.1057 (0.1150)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [310/689]  eta: 0:09:57  lr: 0.000100  loss: 0.1083 (0.1149)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [320/689]  eta: 0:09:41  lr: 0.000100  loss: 0.1098 (0.1149)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [330/689]  eta: 0:09:25  lr: 0.000100  loss: 0.1121 (0.1152)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [340/689]  eta: 0:09:09  lr: 0.000100  loss: 0.1129 (0.1152)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [350/689]  eta: 0:08:54  lr: 0.000100  loss: 0.1128 (0.1152)  time: 1.5784  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:105]  [360/689]  eta: 0:08:38  lr: 0.000100  loss: 0.1123 (0.1152)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [370/689]  eta: 0:08:22  lr: 0.000100  loss: 0.1123 (0.1152)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [380/689]  eta: 0:08:06  lr: 0.000100  loss: 0.1107 (0.1151)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [390/689]  eta: 0:07:51  lr: 0.000100  loss: 0.1085 (0.1151)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [400/689]  eta: 0:07:35  lr: 0.000100  loss: 0.1147 (0.1153)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [410/689]  eta: 0:07:19  lr: 0.000100  loss: 0.1141 (0.1151)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [420/689]  eta: 0:07:03  lr: 0.000100  loss: 0.1082 (0.1149)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [430/689]  eta: 0:06:48  lr: 0.000100  loss: 0.1114 (0.1149)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [440/689]  eta: 0:06:32  lr: 0.000100  loss: 0.1134 (0.1149)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [450/689]  eta: 0:06:16  lr: 0.000100  loss: 0.1052 (0.1147)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [460/689]  eta: 0:06:00  lr: 0.000100  loss: 0.1116 (0.1148)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [470/689]  eta: 0:05:45  lr: 0.000100  loss: 0.1121 (0.1147)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [480/689]  eta: 0:05:29  lr: 0.000100  loss: 0.1110 (0.1147)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [490/689]  eta: 0:05:13  lr: 0.000100  loss: 0.1149 (0.1148)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [500/689]  eta: 0:04:57  lr: 0.000100  loss: 0.1162 (0.1149)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [510/689]  eta: 0:04:42  lr: 0.000100  loss: 0.1078 (0.1148)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [520/689]  eta: 0:04:26  lr: 0.000100  loss: 0.1056 (0.1147)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [530/689]  eta: 0:04:10  lr: 0.000100  loss: 0.1108 (0.1148)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [540/689]  eta: 0:03:54  lr: 0.000100  loss: 0.1127 (0.1147)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [550/689]  eta: 0:03:39  lr: 0.000100  loss: 0.1076 (0.1146)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [560/689]  eta: 0:03:23  lr: 0.000100  loss: 0.1058 (0.1145)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [570/689]  eta: 0:03:07  lr: 0.000100  loss: 0.1115 (0.1146)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [580/689]  eta: 0:02:51  lr: 0.000100  loss: 0.1122 (0.1146)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [590/689]  eta: 0:02:36  lr: 0.000100  loss: 0.1115 (0.1147)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [600/689]  eta: 0:02:20  lr: 0.000100  loss: 0.1148 (0.1147)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [610/689]  eta: 0:02:04  lr: 0.000100  loss: 0.1166 (0.1148)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [620/689]  eta: 0:01:48  lr: 0.000100  loss: 0.1237 (0.1151)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [630/689]  eta: 0:01:33  lr: 0.000100  loss: 0.1256 (0.1152)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [640/689]  eta: 0:01:17  lr: 0.000100  loss: 0.1145 (0.1152)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [650/689]  eta: 0:01:01  lr: 0.000100  loss: 0.1145 (0.1153)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [660/689]  eta: 0:00:45  lr: 0.000100  loss: 0.1141 (0.1153)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [670/689]  eta: 0:00:29  lr: 0.000100  loss: 0.1122 (0.1153)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [680/689]  eta: 0:00:14  lr: 0.000100  loss: 0.1159 (0.1156)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105]  [688/689]  eta: 0:00:01  lr: 0.000100  loss: 0.1188 (0.1156)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:105] Total time: 0:18:06 (1.5772 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 0.1188 (0.1156)\n",
      "Valid: [epoch:105]  [ 0/14]  eta: 0:00:14  loss: 0.1008 (0.1008)  time: 1.0206  data: 0.3970  max mem: 39763\n",
      "Valid: [epoch:105]  [13/14]  eta: 0:00:00  loss: 0.1073 (0.1087)  time: 0.1149  data: 0.0284  max mem: 39763\n",
      "Valid: [epoch:105] Total time: 0:00:01 (0.1253 s / it)\n",
      "Averaged stats: loss: 0.1073 (0.1087)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_105_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.109%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:106]  [  0/689]  eta: 0:12:06  lr: 0.000099  loss: 0.1302 (0.1302)  time: 1.0551  data: 0.5694  max mem: 39763\n",
      "Train: [epoch:106]  [ 10/689]  eta: 0:17:18  lr: 0.000099  loss: 0.1164 (0.1170)  time: 1.5294  data: 0.0519  max mem: 39763\n",
      "Train: [epoch:106]  [ 20/689]  eta: 0:17:18  lr: 0.000099  loss: 0.1136 (0.1181)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [ 30/689]  eta: 0:17:08  lr: 0.000099  loss: 0.1140 (0.1171)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [ 40/689]  eta: 0:16:55  lr: 0.000099  loss: 0.1153 (0.1187)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [ 50/689]  eta: 0:16:41  lr: 0.000099  loss: 0.1225 (0.1199)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [ 60/689]  eta: 0:16:26  lr: 0.000099  loss: 0.1264 (0.1234)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [ 70/689]  eta: 0:16:11  lr: 0.000099  loss: 0.1264 (0.1240)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [ 80/689]  eta: 0:15:56  lr: 0.000099  loss: 0.1193 (0.1230)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [ 90/689]  eta: 0:15:41  lr: 0.000099  loss: 0.1071 (0.1215)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [100/689]  eta: 0:15:26  lr: 0.000099  loss: 0.1090 (0.1206)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [110/689]  eta: 0:15:10  lr: 0.000099  loss: 0.1084 (0.1194)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [120/689]  eta: 0:14:55  lr: 0.000099  loss: 0.1084 (0.1191)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [130/689]  eta: 0:14:39  lr: 0.000099  loss: 0.1182 (0.1193)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [140/689]  eta: 0:14:23  lr: 0.000099  loss: 0.1221 (0.1192)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [150/689]  eta: 0:14:08  lr: 0.000099  loss: 0.1142 (0.1190)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [160/689]  eta: 0:13:52  lr: 0.000099  loss: 0.1141 (0.1190)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [170/689]  eta: 0:13:37  lr: 0.000099  loss: 0.1140 (0.1190)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [180/689]  eta: 0:13:21  lr: 0.000099  loss: 0.1165 (0.1190)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [190/689]  eta: 0:13:05  lr: 0.000099  loss: 0.1186 (0.1192)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [200/689]  eta: 0:12:50  lr: 0.000099  loss: 0.1100 (0.1188)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [210/689]  eta: 0:12:34  lr: 0.000099  loss: 0.1074 (0.1184)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [220/689]  eta: 0:12:18  lr: 0.000099  loss: 0.1076 (0.1184)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [230/689]  eta: 0:12:03  lr: 0.000099  loss: 0.1149 (0.1182)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [240/689]  eta: 0:11:47  lr: 0.000099  loss: 0.1090 (0.1178)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [250/689]  eta: 0:11:31  lr: 0.000099  loss: 0.1088 (0.1176)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [260/689]  eta: 0:11:15  lr: 0.000099  loss: 0.1108 (0.1175)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:106]  [270/689]  eta: 0:11:00  lr: 0.000099  loss: 0.1108 (0.1173)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [280/689]  eta: 0:10:44  lr: 0.000099  loss: 0.1102 (0.1172)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [290/689]  eta: 0:10:28  lr: 0.000099  loss: 0.1115 (0.1173)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [300/689]  eta: 0:10:13  lr: 0.000099  loss: 0.1180 (0.1174)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [310/689]  eta: 0:09:57  lr: 0.000099  loss: 0.1088 (0.1171)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [320/689]  eta: 0:09:41  lr: 0.000099  loss: 0.1093 (0.1173)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [330/689]  eta: 0:09:25  lr: 0.000099  loss: 0.1096 (0.1172)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [340/689]  eta: 0:09:10  lr: 0.000099  loss: 0.1092 (0.1172)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [350/689]  eta: 0:08:54  lr: 0.000099  loss: 0.1092 (0.1170)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [360/689]  eta: 0:08:38  lr: 0.000099  loss: 0.1107 (0.1170)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [370/689]  eta: 0:08:22  lr: 0.000099  loss: 0.1076 (0.1168)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [380/689]  eta: 0:08:07  lr: 0.000099  loss: 0.1071 (0.1166)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [390/689]  eta: 0:07:51  lr: 0.000099  loss: 0.1140 (0.1166)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [400/689]  eta: 0:07:35  lr: 0.000099  loss: 0.1144 (0.1165)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [410/689]  eta: 0:07:19  lr: 0.000099  loss: 0.1161 (0.1167)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [420/689]  eta: 0:07:04  lr: 0.000099  loss: 0.1105 (0.1165)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [430/689]  eta: 0:06:48  lr: 0.000099  loss: 0.1135 (0.1167)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [440/689]  eta: 0:06:32  lr: 0.000099  loss: 0.1191 (0.1167)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [450/689]  eta: 0:06:16  lr: 0.000099  loss: 0.1191 (0.1168)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [460/689]  eta: 0:06:01  lr: 0.000099  loss: 0.1218 (0.1171)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [470/689]  eta: 0:05:45  lr: 0.000099  loss: 0.1229 (0.1173)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [480/689]  eta: 0:05:29  lr: 0.000099  loss: 0.1191 (0.1172)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [490/689]  eta: 0:05:13  lr: 0.000099  loss: 0.1106 (0.1170)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [500/689]  eta: 0:04:58  lr: 0.000099  loss: 0.1097 (0.1169)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [510/689]  eta: 0:04:42  lr: 0.000099  loss: 0.1106 (0.1169)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [520/689]  eta: 0:04:26  lr: 0.000099  loss: 0.1122 (0.1169)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [530/689]  eta: 0:04:10  lr: 0.000099  loss: 0.1106 (0.1168)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [540/689]  eta: 0:03:54  lr: 0.000099  loss: 0.1094 (0.1168)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [550/689]  eta: 0:03:39  lr: 0.000099  loss: 0.1122 (0.1168)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [560/689]  eta: 0:03:23  lr: 0.000099  loss: 0.1187 (0.1169)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [570/689]  eta: 0:03:07  lr: 0.000099  loss: 0.1159 (0.1170)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [580/689]  eta: 0:02:51  lr: 0.000099  loss: 0.1080 (0.1169)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [590/689]  eta: 0:02:36  lr: 0.000099  loss: 0.1066 (0.1168)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [600/689]  eta: 0:02:20  lr: 0.000099  loss: 0.1084 (0.1168)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [610/689]  eta: 0:02:04  lr: 0.000099  loss: 0.1141 (0.1168)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [620/689]  eta: 0:01:48  lr: 0.000099  loss: 0.1139 (0.1167)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [630/689]  eta: 0:01:33  lr: 0.000099  loss: 0.1093 (0.1167)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [640/689]  eta: 0:01:17  lr: 0.000099  loss: 0.1093 (0.1167)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [650/689]  eta: 0:01:01  lr: 0.000099  loss: 0.1085 (0.1166)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [660/689]  eta: 0:00:45  lr: 0.000099  loss: 0.1081 (0.1166)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [670/689]  eta: 0:00:29  lr: 0.000099  loss: 0.1059 (0.1165)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [680/689]  eta: 0:00:14  lr: 0.000099  loss: 0.1104 (0.1165)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106]  [688/689]  eta: 0:00:01  lr: 0.000099  loss: 0.1162 (0.1165)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:106] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.1162 (0.1165)\n",
      "Valid: [epoch:106]  [ 0/14]  eta: 0:00:14  loss: 0.1225 (0.1225)  time: 1.0245  data: 0.3643  max mem: 39763\n",
      "Valid: [epoch:106]  [13/14]  eta: 0:00:00  loss: 0.1055 (0.1083)  time: 0.1152  data: 0.0261  max mem: 39763\n",
      "Valid: [epoch:106] Total time: 0:00:01 (0.1272 s / it)\n",
      "Averaged stats: loss: 0.1055 (0.1083)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_106_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.108%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:107]  [  0/689]  eta: 0:11:49  lr: 0.000099  loss: 0.1334 (0.1334)  time: 1.0301  data: 0.5543  max mem: 39763\n",
      "Train: [epoch:107]  [ 10/689]  eta: 0:17:16  lr: 0.000099  loss: 0.1103 (0.1128)  time: 1.5258  data: 0.0505  max mem: 39763\n",
      "Train: [epoch:107]  [ 20/689]  eta: 0:17:16  lr: 0.000099  loss: 0.1124 (0.1131)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [ 30/689]  eta: 0:17:07  lr: 0.000099  loss: 0.1269 (0.1229)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [ 40/689]  eta: 0:16:54  lr: 0.000099  loss: 0.1313 (0.1249)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [ 50/689]  eta: 0:16:40  lr: 0.000099  loss: 0.1264 (0.1234)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [ 60/689]  eta: 0:16:26  lr: 0.000099  loss: 0.1191 (0.1228)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [ 70/689]  eta: 0:16:11  lr: 0.000099  loss: 0.1175 (0.1216)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [ 80/689]  eta: 0:15:56  lr: 0.000099  loss: 0.1116 (0.1199)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [ 90/689]  eta: 0:15:41  lr: 0.000099  loss: 0.1066 (0.1188)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [100/689]  eta: 0:15:25  lr: 0.000099  loss: 0.1080 (0.1186)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [110/689]  eta: 0:15:10  lr: 0.000099  loss: 0.1105 (0.1185)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [120/689]  eta: 0:14:54  lr: 0.000099  loss: 0.1078 (0.1176)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [130/689]  eta: 0:14:39  lr: 0.000099  loss: 0.1088 (0.1177)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [140/689]  eta: 0:14:23  lr: 0.000099  loss: 0.1157 (0.1175)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [150/689]  eta: 0:14:08  lr: 0.000099  loss: 0.1157 (0.1175)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [160/689]  eta: 0:13:52  lr: 0.000099  loss: 0.1130 (0.1171)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [170/689]  eta: 0:13:36  lr: 0.000099  loss: 0.1120 (0.1169)  time: 1.5775  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:107]  [180/689]  eta: 0:13:21  lr: 0.000099  loss: 0.1171 (0.1174)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [190/689]  eta: 0:13:05  lr: 0.000099  loss: 0.1270 (0.1178)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [200/689]  eta: 0:12:49  lr: 0.000099  loss: 0.1228 (0.1180)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [210/689]  eta: 0:12:34  lr: 0.000099  loss: 0.1190 (0.1181)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [220/689]  eta: 0:12:18  lr: 0.000099  loss: 0.1183 (0.1181)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [230/689]  eta: 0:12:02  lr: 0.000099  loss: 0.1130 (0.1179)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [240/689]  eta: 0:11:47  lr: 0.000099  loss: 0.1116 (0.1177)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [250/689]  eta: 0:11:31  lr: 0.000099  loss: 0.1116 (0.1173)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [260/689]  eta: 0:11:15  lr: 0.000099  loss: 0.1126 (0.1173)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [270/689]  eta: 0:11:00  lr: 0.000099  loss: 0.1077 (0.1170)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [280/689]  eta: 0:10:44  lr: 0.000099  loss: 0.1165 (0.1176)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [290/689]  eta: 0:10:28  lr: 0.000099  loss: 0.1276 (0.1178)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [300/689]  eta: 0:10:12  lr: 0.000099  loss: 0.1173 (0.1178)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [310/689]  eta: 0:09:57  lr: 0.000099  loss: 0.1118 (0.1175)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [320/689]  eta: 0:09:41  lr: 0.000099  loss: 0.1152 (0.1176)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [330/689]  eta: 0:09:25  lr: 0.000099  loss: 0.1156 (0.1177)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [340/689]  eta: 0:09:10  lr: 0.000099  loss: 0.1110 (0.1174)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [350/689]  eta: 0:08:54  lr: 0.000099  loss: 0.1110 (0.1173)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [360/689]  eta: 0:08:38  lr: 0.000099  loss: 0.1117 (0.1171)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [370/689]  eta: 0:08:22  lr: 0.000099  loss: 0.1082 (0.1168)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [380/689]  eta: 0:08:07  lr: 0.000099  loss: 0.1094 (0.1168)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [390/689]  eta: 0:07:51  lr: 0.000099  loss: 0.1130 (0.1167)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [400/689]  eta: 0:07:35  lr: 0.000099  loss: 0.1138 (0.1167)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [410/689]  eta: 0:07:19  lr: 0.000099  loss: 0.1162 (0.1167)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [420/689]  eta: 0:07:04  lr: 0.000099  loss: 0.1095 (0.1165)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [430/689]  eta: 0:06:48  lr: 0.000099  loss: 0.1090 (0.1165)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [440/689]  eta: 0:06:32  lr: 0.000099  loss: 0.1147 (0.1166)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [450/689]  eta: 0:06:16  lr: 0.000099  loss: 0.1147 (0.1164)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [460/689]  eta: 0:06:01  lr: 0.000099  loss: 0.1132 (0.1165)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [470/689]  eta: 0:05:45  lr: 0.000099  loss: 0.1132 (0.1165)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [480/689]  eta: 0:05:29  lr: 0.000099  loss: 0.1075 (0.1163)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [490/689]  eta: 0:05:13  lr: 0.000099  loss: 0.1075 (0.1163)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [500/689]  eta: 0:04:58  lr: 0.000099  loss: 0.1144 (0.1162)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [510/689]  eta: 0:04:42  lr: 0.000099  loss: 0.1065 (0.1160)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [520/689]  eta: 0:04:26  lr: 0.000099  loss: 0.1071 (0.1161)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [530/689]  eta: 0:04:10  lr: 0.000099  loss: 0.1107 (0.1160)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [540/689]  eta: 0:03:54  lr: 0.000099  loss: 0.1129 (0.1161)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [550/689]  eta: 0:03:39  lr: 0.000099  loss: 0.1152 (0.1161)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [560/689]  eta: 0:03:23  lr: 0.000099  loss: 0.1209 (0.1162)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [570/689]  eta: 0:03:07  lr: 0.000099  loss: 0.1253 (0.1163)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [580/689]  eta: 0:02:51  lr: 0.000099  loss: 0.1216 (0.1163)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [590/689]  eta: 0:02:36  lr: 0.000099  loss: 0.1131 (0.1164)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [600/689]  eta: 0:02:20  lr: 0.000099  loss: 0.1137 (0.1164)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [610/689]  eta: 0:02:04  lr: 0.000099  loss: 0.1133 (0.1163)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [620/689]  eta: 0:01:48  lr: 0.000099  loss: 0.1104 (0.1162)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [630/689]  eta: 0:01:33  lr: 0.000099  loss: 0.1084 (0.1162)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [640/689]  eta: 0:01:17  lr: 0.000099  loss: 0.1109 (0.1162)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [650/689]  eta: 0:01:01  lr: 0.000099  loss: 0.1112 (0.1162)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [660/689]  eta: 0:00:45  lr: 0.000099  loss: 0.1104 (0.1162)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [670/689]  eta: 0:00:29  lr: 0.000099  loss: 0.1128 (0.1163)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [680/689]  eta: 0:00:14  lr: 0.000099  loss: 0.1164 (0.1163)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107]  [688/689]  eta: 0:00:01  lr: 0.000099  loss: 0.1185 (0.1163)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:107] Total time: 0:18:06 (1.5775 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.1185 (0.1163)\n",
      "Valid: [epoch:107]  [ 0/14]  eta: 0:00:13  loss: 0.0970 (0.0970)  time: 0.9930  data: 0.3726  max mem: 39763\n",
      "Valid: [epoch:107]  [13/14]  eta: 0:00:00  loss: 0.1067 (0.1088)  time: 0.1129  data: 0.0267  max mem: 39763\n",
      "Valid: [epoch:107] Total time: 0:00:01 (0.1223 s / it)\n",
      "Averaged stats: loss: 0.1067 (0.1088)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_107_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.109%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:108]  [  0/689]  eta: 0:12:12  lr: 0.000099  loss: 0.1109 (0.1109)  time: 1.0634  data: 0.5812  max mem: 39763\n",
      "Train: [epoch:108]  [ 10/689]  eta: 0:17:18  lr: 0.000099  loss: 0.1192 (0.1205)  time: 1.5297  data: 0.0529  max mem: 39763\n",
      "Train: [epoch:108]  [ 20/689]  eta: 0:17:18  lr: 0.000099  loss: 0.1253 (0.1252)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [ 30/689]  eta: 0:17:08  lr: 0.000099  loss: 0.1349 (0.1320)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [ 40/689]  eta: 0:16:55  lr: 0.000099  loss: 0.1303 (0.1304)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [ 50/689]  eta: 0:16:41  lr: 0.000099  loss: 0.1139 (0.1266)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [ 60/689]  eta: 0:16:26  lr: 0.000099  loss: 0.1052 (0.1246)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [ 70/689]  eta: 0:16:11  lr: 0.000099  loss: 0.1150 (0.1234)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [ 80/689]  eta: 0:15:56  lr: 0.000099  loss: 0.1137 (0.1222)  time: 1.5776  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:108]  [ 90/689]  eta: 0:15:41  lr: 0.000099  loss: 0.1076 (0.1205)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [100/689]  eta: 0:15:25  lr: 0.000099  loss: 0.1083 (0.1199)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [110/689]  eta: 0:15:10  lr: 0.000099  loss: 0.1155 (0.1200)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [120/689]  eta: 0:14:55  lr: 0.000099  loss: 0.1157 (0.1197)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [130/689]  eta: 0:14:39  lr: 0.000099  loss: 0.1132 (0.1193)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [140/689]  eta: 0:14:23  lr: 0.000099  loss: 0.1130 (0.1190)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [150/689]  eta: 0:14:08  lr: 0.000099  loss: 0.1147 (0.1190)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [160/689]  eta: 0:13:52  lr: 0.000099  loss: 0.1147 (0.1190)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [170/689]  eta: 0:13:37  lr: 0.000099  loss: 0.1119 (0.1190)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [180/689]  eta: 0:13:21  lr: 0.000099  loss: 0.1148 (0.1192)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [190/689]  eta: 0:13:05  lr: 0.000099  loss: 0.1151 (0.1192)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [200/689]  eta: 0:12:50  lr: 0.000099  loss: 0.1147 (0.1190)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [210/689]  eta: 0:12:34  lr: 0.000099  loss: 0.1128 (0.1185)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [220/689]  eta: 0:12:18  lr: 0.000099  loss: 0.1159 (0.1189)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [230/689]  eta: 0:12:03  lr: 0.000099  loss: 0.1291 (0.1194)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [240/689]  eta: 0:11:47  lr: 0.000099  loss: 0.1233 (0.1193)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [250/689]  eta: 0:11:31  lr: 0.000099  loss: 0.1159 (0.1191)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [260/689]  eta: 0:11:15  lr: 0.000099  loss: 0.1123 (0.1189)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [270/689]  eta: 0:11:00  lr: 0.000099  loss: 0.1123 (0.1188)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [280/689]  eta: 0:10:44  lr: 0.000099  loss: 0.1136 (0.1188)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [290/689]  eta: 0:10:28  lr: 0.000099  loss: 0.1172 (0.1187)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [300/689]  eta: 0:10:13  lr: 0.000099  loss: 0.1196 (0.1187)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [310/689]  eta: 0:09:57  lr: 0.000099  loss: 0.1135 (0.1186)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [320/689]  eta: 0:09:41  lr: 0.000099  loss: 0.1122 (0.1186)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [330/689]  eta: 0:09:25  lr: 0.000099  loss: 0.1182 (0.1188)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [340/689]  eta: 0:09:10  lr: 0.000099  loss: 0.1212 (0.1189)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [350/689]  eta: 0:08:54  lr: 0.000099  loss: 0.1172 (0.1188)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [360/689]  eta: 0:08:38  lr: 0.000099  loss: 0.1140 (0.1189)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [370/689]  eta: 0:08:22  lr: 0.000099  loss: 0.1092 (0.1186)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [380/689]  eta: 0:08:07  lr: 0.000099  loss: 0.1133 (0.1189)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [390/689]  eta: 0:07:51  lr: 0.000099  loss: 0.1240 (0.1191)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [400/689]  eta: 0:07:35  lr: 0.000099  loss: 0.1200 (0.1192)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [410/689]  eta: 0:07:19  lr: 0.000099  loss: 0.1132 (0.1191)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [420/689]  eta: 0:07:04  lr: 0.000099  loss: 0.1101 (0.1188)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [430/689]  eta: 0:06:48  lr: 0.000099  loss: 0.1101 (0.1187)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [440/689]  eta: 0:06:32  lr: 0.000099  loss: 0.1148 (0.1186)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [450/689]  eta: 0:06:16  lr: 0.000099  loss: 0.1137 (0.1186)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [460/689]  eta: 0:06:01  lr: 0.000099  loss: 0.1137 (0.1186)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [470/689]  eta: 0:05:45  lr: 0.000099  loss: 0.1179 (0.1187)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [480/689]  eta: 0:05:29  lr: 0.000099  loss: 0.1156 (0.1186)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [490/689]  eta: 0:05:13  lr: 0.000099  loss: 0.1168 (0.1186)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [500/689]  eta: 0:04:58  lr: 0.000099  loss: 0.1157 (0.1185)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [510/689]  eta: 0:04:42  lr: 0.000099  loss: 0.1139 (0.1187)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [520/689]  eta: 0:04:26  lr: 0.000099  loss: 0.1131 (0.1186)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [530/689]  eta: 0:04:10  lr: 0.000099  loss: 0.1123 (0.1185)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [540/689]  eta: 0:03:55  lr: 0.000099  loss: 0.1129 (0.1184)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [550/689]  eta: 0:03:39  lr: 0.000099  loss: 0.1111 (0.1184)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [560/689]  eta: 0:03:23  lr: 0.000099  loss: 0.1114 (0.1183)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [570/689]  eta: 0:03:07  lr: 0.000099  loss: 0.1184 (0.1184)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [580/689]  eta: 0:02:51  lr: 0.000099  loss: 0.1126 (0.1183)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [590/689]  eta: 0:02:36  lr: 0.000099  loss: 0.1116 (0.1183)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [600/689]  eta: 0:02:20  lr: 0.000099  loss: 0.1184 (0.1184)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [610/689]  eta: 0:02:04  lr: 0.000099  loss: 0.1164 (0.1184)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [620/689]  eta: 0:01:48  lr: 0.000099  loss: 0.1152 (0.1183)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [630/689]  eta: 0:01:33  lr: 0.000099  loss: 0.1178 (0.1184)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [640/689]  eta: 0:01:17  lr: 0.000099  loss: 0.1234 (0.1184)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [650/689]  eta: 0:01:01  lr: 0.000099  loss: 0.1149 (0.1184)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [660/689]  eta: 0:00:45  lr: 0.000099  loss: 0.1149 (0.1183)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [670/689]  eta: 0:00:29  lr: 0.000099  loss: 0.1099 (0.1182)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [680/689]  eta: 0:00:14  lr: 0.000099  loss: 0.1099 (0.1182)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108]  [688/689]  eta: 0:00:01  lr: 0.000099  loss: 0.1097 (0.1181)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:108] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.1097 (0.1181)\n",
      "Valid: [epoch:108]  [ 0/14]  eta: 0:00:14  loss: 0.1205 (0.1205)  time: 1.0075  data: 0.4271  max mem: 39763\n",
      "Valid: [epoch:108]  [13/14]  eta: 0:00:00  loss: 0.1062 (0.1097)  time: 0.1139  data: 0.0306  max mem: 39763\n",
      "Valid: [epoch:108] Total time: 0:00:01 (0.1217 s / it)\n",
      "Averaged stats: loss: 0.1062 (0.1097)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_108_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.110%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:109]  [  0/689]  eta: 0:12:19  lr: 0.000099  loss: 0.1253 (0.1253)  time: 1.0728  data: 0.5952  max mem: 39763\n",
      "Train: [epoch:109]  [ 10/689]  eta: 0:17:18  lr: 0.000099  loss: 0.1149 (0.1133)  time: 1.5294  data: 0.0542  max mem: 39763\n",
      "Train: [epoch:109]  [ 20/689]  eta: 0:17:18  lr: 0.000099  loss: 0.1151 (0.1143)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [ 30/689]  eta: 0:17:08  lr: 0.000099  loss: 0.1169 (0.1163)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [ 40/689]  eta: 0:16:55  lr: 0.000099  loss: 0.1201 (0.1174)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [ 50/689]  eta: 0:16:41  lr: 0.000099  loss: 0.1157 (0.1168)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [ 60/689]  eta: 0:16:26  lr: 0.000099  loss: 0.1135 (0.1162)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [ 70/689]  eta: 0:16:11  lr: 0.000099  loss: 0.1184 (0.1165)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [ 80/689]  eta: 0:15:56  lr: 0.000099  loss: 0.1267 (0.1186)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [ 90/689]  eta: 0:15:41  lr: 0.000099  loss: 0.1267 (0.1193)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [100/689]  eta: 0:15:26  lr: 0.000099  loss: 0.1188 (0.1192)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [110/689]  eta: 0:15:10  lr: 0.000099  loss: 0.1155 (0.1188)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [120/689]  eta: 0:14:55  lr: 0.000099  loss: 0.1167 (0.1189)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [130/689]  eta: 0:14:39  lr: 0.000099  loss: 0.1219 (0.1199)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [140/689]  eta: 0:14:24  lr: 0.000099  loss: 0.1173 (0.1196)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [150/689]  eta: 0:14:08  lr: 0.000099  loss: 0.1144 (0.1195)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [160/689]  eta: 0:13:52  lr: 0.000099  loss: 0.1212 (0.1199)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [170/689]  eta: 0:13:37  lr: 0.000099  loss: 0.1274 (0.1209)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [180/689]  eta: 0:13:21  lr: 0.000099  loss: 0.1315 (0.1213)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [190/689]  eta: 0:13:05  lr: 0.000099  loss: 0.1284 (0.1220)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [200/689]  eta: 0:12:50  lr: 0.000099  loss: 0.1249 (0.1218)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [210/689]  eta: 0:12:34  lr: 0.000099  loss: 0.1249 (0.1225)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [220/689]  eta: 0:12:18  lr: 0.000099  loss: 0.1266 (0.1224)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [230/689]  eta: 0:12:03  lr: 0.000099  loss: 0.1212 (0.1227)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [240/689]  eta: 0:11:47  lr: 0.000099  loss: 0.1225 (0.1226)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [250/689]  eta: 0:11:31  lr: 0.000099  loss: 0.1221 (0.1226)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [260/689]  eta: 0:11:16  lr: 0.000099  loss: 0.1203 (0.1228)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [270/689]  eta: 0:11:00  lr: 0.000099  loss: 0.1159 (0.1223)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [280/689]  eta: 0:10:44  lr: 0.000099  loss: 0.1078 (0.1220)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [290/689]  eta: 0:10:28  lr: 0.000099  loss: 0.1160 (0.1221)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [300/689]  eta: 0:10:13  lr: 0.000099  loss: 0.1186 (0.1221)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [310/689]  eta: 0:09:57  lr: 0.000099  loss: 0.1190 (0.1219)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [320/689]  eta: 0:09:41  lr: 0.000099  loss: 0.1169 (0.1218)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [330/689]  eta: 0:09:25  lr: 0.000099  loss: 0.1114 (0.1216)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [340/689]  eta: 0:09:10  lr: 0.000099  loss: 0.1108 (0.1213)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [350/689]  eta: 0:08:54  lr: 0.000099  loss: 0.1144 (0.1214)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [360/689]  eta: 0:08:38  lr: 0.000099  loss: 0.1178 (0.1216)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [370/689]  eta: 0:08:22  lr: 0.000099  loss: 0.1178 (0.1222)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [380/689]  eta: 0:08:07  lr: 0.000099  loss: 0.1287 (0.1227)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [390/689]  eta: 0:07:51  lr: 0.000099  loss: 0.1300 (0.1228)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [400/689]  eta: 0:07:35  lr: 0.000099  loss: 0.1277 (0.1230)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [410/689]  eta: 0:07:19  lr: 0.000099  loss: 0.1277 (0.1232)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [420/689]  eta: 0:07:04  lr: 0.000099  loss: 0.1175 (0.1230)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [430/689]  eta: 0:06:48  lr: 0.000099  loss: 0.1175 (0.1230)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [440/689]  eta: 0:06:32  lr: 0.000099  loss: 0.1230 (0.1229)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [450/689]  eta: 0:06:16  lr: 0.000099  loss: 0.1133 (0.1227)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [460/689]  eta: 0:06:01  lr: 0.000099  loss: 0.1145 (0.1227)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [470/689]  eta: 0:05:45  lr: 0.000099  loss: 0.1177 (0.1227)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [480/689]  eta: 0:05:29  lr: 0.000099  loss: 0.1216 (0.1227)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [490/689]  eta: 0:05:13  lr: 0.000099  loss: 0.1216 (0.1227)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [500/689]  eta: 0:04:58  lr: 0.000099  loss: 0.1158 (0.1225)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [510/689]  eta: 0:04:42  lr: 0.000099  loss: 0.1145 (0.1224)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [520/689]  eta: 0:04:26  lr: 0.000099  loss: 0.1146 (0.1224)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [530/689]  eta: 0:04:10  lr: 0.000099  loss: 0.1133 (0.1222)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [540/689]  eta: 0:03:54  lr: 0.000099  loss: 0.1149 (0.1222)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [550/689]  eta: 0:03:39  lr: 0.000099  loss: 0.1207 (0.1221)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [560/689]  eta: 0:03:23  lr: 0.000099  loss: 0.1220 (0.1221)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [570/689]  eta: 0:03:07  lr: 0.000099  loss: 0.1130 (0.1219)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [580/689]  eta: 0:02:51  lr: 0.000099  loss: 0.1130 (0.1219)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [590/689]  eta: 0:02:36  lr: 0.000099  loss: 0.1137 (0.1218)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [600/689]  eta: 0:02:20  lr: 0.000099  loss: 0.1119 (0.1217)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [610/689]  eta: 0:02:04  lr: 0.000099  loss: 0.1119 (0.1216)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [620/689]  eta: 0:01:48  lr: 0.000099  loss: 0.1119 (0.1215)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [630/689]  eta: 0:01:33  lr: 0.000099  loss: 0.1130 (0.1214)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [640/689]  eta: 0:01:17  lr: 0.000099  loss: 0.1140 (0.1214)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [650/689]  eta: 0:01:01  lr: 0.000099  loss: 0.1129 (0.1212)  time: 1.5777  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:109]  [660/689]  eta: 0:00:45  lr: 0.000099  loss: 0.1129 (0.1211)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [670/689]  eta: 0:00:29  lr: 0.000099  loss: 0.1117 (0.1211)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [680/689]  eta: 0:00:14  lr: 0.000099  loss: 0.1136 (0.1210)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109]  [688/689]  eta: 0:00:01  lr: 0.000099  loss: 0.1147 (0.1209)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:109] Total time: 0:18:06 (1.5773 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.1147 (0.1209)\n",
      "Valid: [epoch:109]  [ 0/14]  eta: 0:00:14  loss: 0.1155 (0.1155)  time: 1.0389  data: 0.3985  max mem: 39763\n",
      "Valid: [epoch:109]  [13/14]  eta: 0:00:00  loss: 0.1086 (0.1124)  time: 0.1161  data: 0.0285  max mem: 39763\n",
      "Valid: [epoch:109] Total time: 0:00:01 (0.1267 s / it)\n",
      "Averaged stats: loss: 0.1086 (0.1124)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_109_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.112%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:110]  [  0/689]  eta: 0:11:51  lr: 0.000099  loss: 0.1093 (0.1093)  time: 1.0331  data: 0.5564  max mem: 39763\n",
      "Train: [epoch:110]  [ 10/689]  eta: 0:17:16  lr: 0.000099  loss: 0.1093 (0.1098)  time: 1.5271  data: 0.0507  max mem: 39763\n",
      "Train: [epoch:110]  [ 20/689]  eta: 0:17:17  lr: 0.000099  loss: 0.1097 (0.1129)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [ 30/689]  eta: 0:17:07  lr: 0.000099  loss: 0.1115 (0.1136)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [ 40/689]  eta: 0:16:55  lr: 0.000099  loss: 0.1117 (0.1135)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [ 50/689]  eta: 0:16:41  lr: 0.000099  loss: 0.1130 (0.1144)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [ 60/689]  eta: 0:16:26  lr: 0.000099  loss: 0.1172 (0.1167)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [ 70/689]  eta: 0:16:11  lr: 0.000099  loss: 0.1177 (0.1164)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [ 80/689]  eta: 0:15:56  lr: 0.000099  loss: 0.1167 (0.1162)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [ 90/689]  eta: 0:15:41  lr: 0.000099  loss: 0.1126 (0.1163)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [100/689]  eta: 0:15:26  lr: 0.000099  loss: 0.1143 (0.1164)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [110/689]  eta: 0:15:10  lr: 0.000099  loss: 0.1193 (0.1173)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [120/689]  eta: 0:14:55  lr: 0.000099  loss: 0.1217 (0.1174)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [130/689]  eta: 0:14:39  lr: 0.000099  loss: 0.1173 (0.1182)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [140/689]  eta: 0:14:24  lr: 0.000099  loss: 0.1198 (0.1183)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [150/689]  eta: 0:14:08  lr: 0.000099  loss: 0.1209 (0.1187)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [160/689]  eta: 0:13:52  lr: 0.000099  loss: 0.1184 (0.1186)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [170/689]  eta: 0:13:37  lr: 0.000099  loss: 0.1152 (0.1185)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [180/689]  eta: 0:13:21  lr: 0.000099  loss: 0.1186 (0.1188)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [190/689]  eta: 0:13:06  lr: 0.000099  loss: 0.1226 (0.1189)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [200/689]  eta: 0:12:50  lr: 0.000099  loss: 0.1166 (0.1187)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [210/689]  eta: 0:12:34  lr: 0.000099  loss: 0.1183 (0.1191)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [220/689]  eta: 0:12:19  lr: 0.000099  loss: 0.1280 (0.1198)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [230/689]  eta: 0:12:03  lr: 0.000099  loss: 0.1265 (0.1197)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [240/689]  eta: 0:11:47  lr: 0.000099  loss: 0.1163 (0.1198)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [250/689]  eta: 0:11:31  lr: 0.000099  loss: 0.1177 (0.1197)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [260/689]  eta: 0:11:16  lr: 0.000099  loss: 0.1122 (0.1195)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [270/689]  eta: 0:11:00  lr: 0.000099  loss: 0.1110 (0.1193)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [280/689]  eta: 0:10:44  lr: 0.000099  loss: 0.1128 (0.1199)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [290/689]  eta: 0:10:29  lr: 0.000099  loss: 0.1171 (0.1198)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [300/689]  eta: 0:10:13  lr: 0.000099  loss: 0.1143 (0.1200)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [310/689]  eta: 0:09:57  lr: 0.000099  loss: 0.1148 (0.1199)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [320/689]  eta: 0:09:41  lr: 0.000099  loss: 0.1148 (0.1199)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [330/689]  eta: 0:09:26  lr: 0.000099  loss: 0.1175 (0.1199)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [340/689]  eta: 0:09:10  lr: 0.000099  loss: 0.1127 (0.1197)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [350/689]  eta: 0:08:54  lr: 0.000099  loss: 0.1088 (0.1195)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [360/689]  eta: 0:08:38  lr: 0.000099  loss: 0.1129 (0.1195)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [370/689]  eta: 0:08:23  lr: 0.000099  loss: 0.1156 (0.1195)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [380/689]  eta: 0:08:07  lr: 0.000099  loss: 0.1126 (0.1194)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [390/689]  eta: 0:07:51  lr: 0.000099  loss: 0.1126 (0.1194)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [400/689]  eta: 0:07:35  lr: 0.000099  loss: 0.1194 (0.1194)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [410/689]  eta: 0:07:19  lr: 0.000099  loss: 0.1153 (0.1194)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [420/689]  eta: 0:07:04  lr: 0.000099  loss: 0.1118 (0.1192)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [430/689]  eta: 0:06:48  lr: 0.000099  loss: 0.1135 (0.1192)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [440/689]  eta: 0:06:32  lr: 0.000099  loss: 0.1132 (0.1191)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [450/689]  eta: 0:06:16  lr: 0.000099  loss: 0.1132 (0.1191)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [460/689]  eta: 0:06:01  lr: 0.000099  loss: 0.1172 (0.1193)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [470/689]  eta: 0:05:45  lr: 0.000099  loss: 0.1176 (0.1194)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [480/689]  eta: 0:05:29  lr: 0.000099  loss: 0.1142 (0.1193)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [490/689]  eta: 0:05:13  lr: 0.000099  loss: 0.1126 (0.1193)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [500/689]  eta: 0:04:58  lr: 0.000099  loss: 0.1086 (0.1193)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [510/689]  eta: 0:04:42  lr: 0.000099  loss: 0.1091 (0.1191)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [520/689]  eta: 0:04:26  lr: 0.000099  loss: 0.1150 (0.1192)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [530/689]  eta: 0:04:10  lr: 0.000099  loss: 0.1163 (0.1191)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [540/689]  eta: 0:03:55  lr: 0.000099  loss: 0.1156 (0.1192)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [550/689]  eta: 0:03:39  lr: 0.000099  loss: 0.1164 (0.1192)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [560/689]  eta: 0:03:23  lr: 0.000099  loss: 0.1166 (0.1192)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:110]  [570/689]  eta: 0:03:07  lr: 0.000099  loss: 0.1174 (0.1192)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [580/689]  eta: 0:02:51  lr: 0.000099  loss: 0.1202 (0.1193)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [590/689]  eta: 0:02:36  lr: 0.000099  loss: 0.1148 (0.1192)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [600/689]  eta: 0:02:20  lr: 0.000099  loss: 0.1112 (0.1192)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [610/689]  eta: 0:02:04  lr: 0.000099  loss: 0.1167 (0.1193)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [620/689]  eta: 0:01:48  lr: 0.000099  loss: 0.1166 (0.1192)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [630/689]  eta: 0:01:33  lr: 0.000099  loss: 0.1106 (0.1191)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [640/689]  eta: 0:01:17  lr: 0.000099  loss: 0.1162 (0.1191)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [650/689]  eta: 0:01:01  lr: 0.000099  loss: 0.1166 (0.1191)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [660/689]  eta: 0:00:45  lr: 0.000099  loss: 0.1166 (0.1191)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [670/689]  eta: 0:00:29  lr: 0.000099  loss: 0.1195 (0.1192)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [680/689]  eta: 0:00:14  lr: 0.000099  loss: 0.1172 (0.1192)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110]  [688/689]  eta: 0:00:01  lr: 0.000099  loss: 0.1149 (0.1192)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:110] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.1149 (0.1192)\n",
      "Valid: [epoch:110]  [ 0/14]  eta: 0:00:14  loss: 0.1248 (0.1248)  time: 1.0109  data: 0.3854  max mem: 39763\n",
      "Valid: [epoch:110]  [13/14]  eta: 0:00:00  loss: 0.1080 (0.1108)  time: 0.1142  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:110] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.1080 (0.1108)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_110_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.111%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:111]  [  0/689]  eta: 0:11:58  lr: 0.000099  loss: 0.1188 (0.1188)  time: 1.0433  data: 0.5655  max mem: 39763\n",
      "Train: [epoch:111]  [ 10/689]  eta: 0:17:17  lr: 0.000099  loss: 0.1155 (0.1161)  time: 1.5277  data: 0.0515  max mem: 39763\n",
      "Train: [epoch:111]  [ 20/689]  eta: 0:17:18  lr: 0.000099  loss: 0.1156 (0.1191)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [ 30/689]  eta: 0:17:07  lr: 0.000099  loss: 0.1174 (0.1192)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [ 40/689]  eta: 0:16:55  lr: 0.000099  loss: 0.1180 (0.1212)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [ 50/689]  eta: 0:16:41  lr: 0.000099  loss: 0.1163 (0.1192)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [ 60/689]  eta: 0:16:26  lr: 0.000099  loss: 0.1084 (0.1179)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [ 70/689]  eta: 0:16:11  lr: 0.000099  loss: 0.1084 (0.1165)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [ 80/689]  eta: 0:15:56  lr: 0.000099  loss: 0.1127 (0.1172)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [ 90/689]  eta: 0:15:41  lr: 0.000099  loss: 0.1208 (0.1182)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [100/689]  eta: 0:15:26  lr: 0.000099  loss: 0.1208 (0.1191)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [110/689]  eta: 0:15:10  lr: 0.000099  loss: 0.1196 (0.1193)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [120/689]  eta: 0:14:55  lr: 0.000099  loss: 0.1223 (0.1204)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [130/689]  eta: 0:14:39  lr: 0.000099  loss: 0.1177 (0.1200)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [140/689]  eta: 0:14:24  lr: 0.000099  loss: 0.1170 (0.1203)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [150/689]  eta: 0:14:08  lr: 0.000099  loss: 0.1207 (0.1202)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [160/689]  eta: 0:13:53  lr: 0.000099  loss: 0.1260 (0.1209)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [170/689]  eta: 0:13:37  lr: 0.000099  loss: 0.1220 (0.1208)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [180/689]  eta: 0:13:21  lr: 0.000099  loss: 0.1165 (0.1207)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [190/689]  eta: 0:13:06  lr: 0.000099  loss: 0.1149 (0.1205)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [200/689]  eta: 0:12:50  lr: 0.000099  loss: 0.1115 (0.1200)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [210/689]  eta: 0:12:34  lr: 0.000099  loss: 0.1104 (0.1199)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [220/689]  eta: 0:12:19  lr: 0.000099  loss: 0.1137 (0.1197)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [230/689]  eta: 0:12:03  lr: 0.000099  loss: 0.1118 (0.1194)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [240/689]  eta: 0:11:47  lr: 0.000099  loss: 0.1120 (0.1192)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [250/689]  eta: 0:11:32  lr: 0.000099  loss: 0.1139 (0.1193)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [260/689]  eta: 0:11:16  lr: 0.000099  loss: 0.1187 (0.1196)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [270/689]  eta: 0:11:00  lr: 0.000099  loss: 0.1187 (0.1196)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [280/689]  eta: 0:10:44  lr: 0.000099  loss: 0.1185 (0.1197)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [290/689]  eta: 0:10:29  lr: 0.000099  loss: 0.1185 (0.1196)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [300/689]  eta: 0:10:13  lr: 0.000099  loss: 0.1164 (0.1198)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [310/689]  eta: 0:09:57  lr: 0.000099  loss: 0.1160 (0.1195)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [320/689]  eta: 0:09:41  lr: 0.000099  loss: 0.1095 (0.1195)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [330/689]  eta: 0:09:26  lr: 0.000099  loss: 0.1185 (0.1196)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [340/689]  eta: 0:09:10  lr: 0.000099  loss: 0.1197 (0.1197)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [350/689]  eta: 0:08:54  lr: 0.000099  loss: 0.1159 (0.1195)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [360/689]  eta: 0:08:38  lr: 0.000099  loss: 0.1180 (0.1197)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [370/689]  eta: 0:08:23  lr: 0.000099  loss: 0.1231 (0.1197)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [380/689]  eta: 0:08:07  lr: 0.000099  loss: 0.1206 (0.1197)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [390/689]  eta: 0:07:51  lr: 0.000099  loss: 0.1191 (0.1198)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [400/689]  eta: 0:07:35  lr: 0.000099  loss: 0.1221 (0.1198)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [410/689]  eta: 0:07:19  lr: 0.000099  loss: 0.1259 (0.1200)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [420/689]  eta: 0:07:04  lr: 0.000099  loss: 0.1187 (0.1199)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [430/689]  eta: 0:06:48  lr: 0.000099  loss: 0.1174 (0.1200)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [440/689]  eta: 0:06:32  lr: 0.000099  loss: 0.1153 (0.1199)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [450/689]  eta: 0:06:16  lr: 0.000099  loss: 0.1165 (0.1198)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [460/689]  eta: 0:06:01  lr: 0.000099  loss: 0.1206 (0.1200)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [470/689]  eta: 0:05:45  lr: 0.000099  loss: 0.1190 (0.1200)  time: 1.5777  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:111]  [480/689]  eta: 0:05:29  lr: 0.000099  loss: 0.1184 (0.1202)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [490/689]  eta: 0:05:13  lr: 0.000099  loss: 0.1246 (0.1203)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [500/689]  eta: 0:04:58  lr: 0.000099  loss: 0.1238 (0.1204)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [510/689]  eta: 0:04:42  lr: 0.000099  loss: 0.1215 (0.1204)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [520/689]  eta: 0:04:26  lr: 0.000099  loss: 0.1146 (0.1204)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [530/689]  eta: 0:04:10  lr: 0.000099  loss: 0.1150 (0.1204)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [540/689]  eta: 0:03:55  lr: 0.000099  loss: 0.1180 (0.1204)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [550/689]  eta: 0:03:39  lr: 0.000099  loss: 0.1166 (0.1203)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [560/689]  eta: 0:03:23  lr: 0.000099  loss: 0.1121 (0.1202)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [570/689]  eta: 0:03:07  lr: 0.000099  loss: 0.1141 (0.1201)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [580/689]  eta: 0:02:51  lr: 0.000099  loss: 0.1085 (0.1200)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [590/689]  eta: 0:02:36  lr: 0.000099  loss: 0.1132 (0.1200)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [600/689]  eta: 0:02:20  lr: 0.000099  loss: 0.1119 (0.1198)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [610/689]  eta: 0:02:04  lr: 0.000099  loss: 0.1089 (0.1197)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [620/689]  eta: 0:01:48  lr: 0.000099  loss: 0.1175 (0.1198)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [630/689]  eta: 0:01:33  lr: 0.000099  loss: 0.1260 (0.1199)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [640/689]  eta: 0:01:17  lr: 0.000099  loss: 0.1144 (0.1200)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [650/689]  eta: 0:01:01  lr: 0.000099  loss: 0.1143 (0.1200)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [660/689]  eta: 0:00:45  lr: 0.000099  loss: 0.1315 (0.1202)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [670/689]  eta: 0:00:29  lr: 0.000099  loss: 0.1234 (0.1202)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [680/689]  eta: 0:00:14  lr: 0.000099  loss: 0.1131 (0.1201)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111]  [688/689]  eta: 0:00:01  lr: 0.000099  loss: 0.1145 (0.1201)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:111] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.1145 (0.1201)\n",
      "Valid: [epoch:111]  [ 0/14]  eta: 0:00:14  loss: 0.1104 (0.1104)  time: 1.0135  data: 0.3890  max mem: 39763\n",
      "Valid: [epoch:111]  [13/14]  eta: 0:00:00  loss: 0.1104 (0.1152)  time: 0.1143  data: 0.0278  max mem: 39763\n",
      "Valid: [epoch:111] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.1104 (0.1152)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_111_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.115%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:112]  [  0/689]  eta: 0:11:38  lr: 0.000099  loss: 0.1082 (0.1082)  time: 1.0134  data: 0.5335  max mem: 39763\n",
      "Train: [epoch:112]  [ 10/689]  eta: 0:17:16  lr: 0.000099  loss: 0.1173 (0.1200)  time: 1.5260  data: 0.0486  max mem: 39763\n",
      "Train: [epoch:112]  [ 20/689]  eta: 0:17:17  lr: 0.000099  loss: 0.1123 (0.1168)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [ 30/689]  eta: 0:17:07  lr: 0.000099  loss: 0.1123 (0.1179)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [ 40/689]  eta: 0:16:55  lr: 0.000099  loss: 0.1151 (0.1179)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [ 50/689]  eta: 0:16:41  lr: 0.000099  loss: 0.1115 (0.1167)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [ 60/689]  eta: 0:16:26  lr: 0.000099  loss: 0.1117 (0.1176)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [ 70/689]  eta: 0:16:12  lr: 0.000099  loss: 0.1158 (0.1174)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [ 80/689]  eta: 0:15:57  lr: 0.000099  loss: 0.1160 (0.1174)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [ 90/689]  eta: 0:15:41  lr: 0.000099  loss: 0.1190 (0.1175)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [100/689]  eta: 0:15:26  lr: 0.000099  loss: 0.1246 (0.1195)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [110/689]  eta: 0:15:11  lr: 0.000099  loss: 0.1332 (0.1209)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [120/689]  eta: 0:14:55  lr: 0.000099  loss: 0.1228 (0.1209)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [130/689]  eta: 0:14:40  lr: 0.000099  loss: 0.1183 (0.1212)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [140/689]  eta: 0:14:24  lr: 0.000099  loss: 0.1265 (0.1215)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [150/689]  eta: 0:14:08  lr: 0.000099  loss: 0.1237 (0.1216)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [160/689]  eta: 0:13:53  lr: 0.000099  loss: 0.1291 (0.1223)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [170/689]  eta: 0:13:37  lr: 0.000099  loss: 0.1454 (0.1237)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [180/689]  eta: 0:13:22  lr: 0.000099  loss: 0.1305 (0.1236)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [190/689]  eta: 0:13:06  lr: 0.000099  loss: 0.1256 (0.1243)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [200/689]  eta: 0:12:50  lr: 0.000099  loss: 0.1313 (0.1244)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [210/689]  eta: 0:12:34  lr: 0.000099  loss: 0.1184 (0.1241)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [220/689]  eta: 0:12:19  lr: 0.000099  loss: 0.1178 (0.1241)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [230/689]  eta: 0:12:03  lr: 0.000099  loss: 0.1219 (0.1244)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [240/689]  eta: 0:11:47  lr: 0.000099  loss: 0.1327 (0.1252)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [250/689]  eta: 0:11:32  lr: 0.000099  loss: 0.1281 (0.1250)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [260/689]  eta: 0:11:16  lr: 0.000099  loss: 0.1183 (0.1249)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [270/689]  eta: 0:11:00  lr: 0.000099  loss: 0.1172 (0.1246)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [280/689]  eta: 0:10:44  lr: 0.000099  loss: 0.1161 (0.1243)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [290/689]  eta: 0:10:29  lr: 0.000099  loss: 0.1231 (0.1242)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [300/689]  eta: 0:10:13  lr: 0.000099  loss: 0.1173 (0.1239)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [310/689]  eta: 0:09:57  lr: 0.000099  loss: 0.1085 (0.1235)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [320/689]  eta: 0:09:41  lr: 0.000099  loss: 0.1140 (0.1236)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [330/689]  eta: 0:09:26  lr: 0.000099  loss: 0.1199 (0.1236)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [340/689]  eta: 0:09:10  lr: 0.000099  loss: 0.1198 (0.1236)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [350/689]  eta: 0:08:54  lr: 0.000099  loss: 0.1170 (0.1235)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [360/689]  eta: 0:08:38  lr: 0.000099  loss: 0.1119 (0.1233)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [370/689]  eta: 0:08:23  lr: 0.000099  loss: 0.1178 (0.1233)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [380/689]  eta: 0:08:07  lr: 0.000099  loss: 0.1222 (0.1232)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:112]  [390/689]  eta: 0:07:51  lr: 0.000099  loss: 0.1184 (0.1231)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [400/689]  eta: 0:07:35  lr: 0.000099  loss: 0.1216 (0.1231)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [410/689]  eta: 0:07:20  lr: 0.000099  loss: 0.1268 (0.1233)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [420/689]  eta: 0:07:04  lr: 0.000099  loss: 0.1197 (0.1231)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [430/689]  eta: 0:06:48  lr: 0.000099  loss: 0.1135 (0.1231)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [440/689]  eta: 0:06:32  lr: 0.000099  loss: 0.1145 (0.1232)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [450/689]  eta: 0:06:17  lr: 0.000099  loss: 0.1177 (0.1231)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [460/689]  eta: 0:06:01  lr: 0.000099  loss: 0.1177 (0.1230)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [470/689]  eta: 0:05:45  lr: 0.000099  loss: 0.1123 (0.1229)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [480/689]  eta: 0:05:29  lr: 0.000099  loss: 0.1153 (0.1229)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [490/689]  eta: 0:05:13  lr: 0.000099  loss: 0.1278 (0.1230)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [500/689]  eta: 0:04:58  lr: 0.000099  loss: 0.1255 (0.1229)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [510/689]  eta: 0:04:42  lr: 0.000099  loss: 0.1138 (0.1228)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [520/689]  eta: 0:04:26  lr: 0.000099  loss: 0.1140 (0.1228)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [530/689]  eta: 0:04:10  lr: 0.000099  loss: 0.1203 (0.1228)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [540/689]  eta: 0:03:55  lr: 0.000099  loss: 0.1183 (0.1227)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [550/689]  eta: 0:03:39  lr: 0.000099  loss: 0.1201 (0.1227)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [560/689]  eta: 0:03:23  lr: 0.000099  loss: 0.1179 (0.1226)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [570/689]  eta: 0:03:07  lr: 0.000099  loss: 0.1171 (0.1226)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [580/689]  eta: 0:02:51  lr: 0.000099  loss: 0.1171 (0.1226)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [590/689]  eta: 0:02:36  lr: 0.000099  loss: 0.1167 (0.1226)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [600/689]  eta: 0:02:20  lr: 0.000099  loss: 0.1178 (0.1226)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [610/689]  eta: 0:02:04  lr: 0.000099  loss: 0.1235 (0.1226)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [620/689]  eta: 0:01:48  lr: 0.000099  loss: 0.1185 (0.1226)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [630/689]  eta: 0:01:33  lr: 0.000099  loss: 0.1190 (0.1226)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [640/689]  eta: 0:01:17  lr: 0.000099  loss: 0.1237 (0.1227)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [650/689]  eta: 0:01:01  lr: 0.000099  loss: 0.1237 (0.1227)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [660/689]  eta: 0:00:45  lr: 0.000099  loss: 0.1230 (0.1228)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [670/689]  eta: 0:00:29  lr: 0.000099  loss: 0.1241 (0.1227)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [680/689]  eta: 0:00:14  lr: 0.000099  loss: 0.1162 (0.1227)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112]  [688/689]  eta: 0:00:01  lr: 0.000099  loss: 0.1169 (0.1227)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:112] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.1169 (0.1227)\n",
      "Valid: [epoch:112]  [ 0/14]  eta: 0:00:13  loss: 0.1181 (0.1181)  time: 0.9952  data: 0.3626  max mem: 39763\n",
      "Valid: [epoch:112]  [13/14]  eta: 0:00:00  loss: 0.1093 (0.1167)  time: 0.1130  data: 0.0260  max mem: 39763\n",
      "Valid: [epoch:112] Total time: 0:00:01 (0.1222 s / it)\n",
      "Averaged stats: loss: 0.1093 (0.1167)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_112_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.117%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:113]  [  0/689]  eta: 0:11:52  lr: 0.000099  loss: 0.1151 (0.1151)  time: 1.0336  data: 0.5573  max mem: 39763\n",
      "Train: [epoch:113]  [ 10/689]  eta: 0:17:16  lr: 0.000099  loss: 0.1151 (0.1164)  time: 1.5266  data: 0.0507  max mem: 39763\n",
      "Train: [epoch:113]  [ 20/689]  eta: 0:17:17  lr: 0.000099  loss: 0.1169 (0.1246)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [ 30/689]  eta: 0:17:08  lr: 0.000099  loss: 0.1169 (0.1238)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [ 40/689]  eta: 0:16:55  lr: 0.000099  loss: 0.1255 (0.1247)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [ 50/689]  eta: 0:16:41  lr: 0.000099  loss: 0.1171 (0.1225)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [ 60/689]  eta: 0:16:26  lr: 0.000099  loss: 0.1164 (0.1228)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [ 70/689]  eta: 0:16:12  lr: 0.000099  loss: 0.1191 (0.1220)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [ 80/689]  eta: 0:15:56  lr: 0.000099  loss: 0.1185 (0.1218)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [ 90/689]  eta: 0:15:41  lr: 0.000099  loss: 0.1189 (0.1227)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [100/689]  eta: 0:15:26  lr: 0.000099  loss: 0.1205 (0.1224)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [110/689]  eta: 0:15:10  lr: 0.000099  loss: 0.1205 (0.1222)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [120/689]  eta: 0:14:55  lr: 0.000099  loss: 0.1215 (0.1222)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [130/689]  eta: 0:14:39  lr: 0.000099  loss: 0.1223 (0.1225)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [140/689]  eta: 0:14:24  lr: 0.000099  loss: 0.1202 (0.1223)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [150/689]  eta: 0:14:08  lr: 0.000099  loss: 0.1172 (0.1224)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [160/689]  eta: 0:13:53  lr: 0.000099  loss: 0.1180 (0.1226)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [170/689]  eta: 0:13:37  lr: 0.000099  loss: 0.1168 (0.1225)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [180/689]  eta: 0:13:21  lr: 0.000099  loss: 0.1169 (0.1226)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [190/689]  eta: 0:13:06  lr: 0.000099  loss: 0.1189 (0.1227)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [200/689]  eta: 0:12:50  lr: 0.000099  loss: 0.1214 (0.1225)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [210/689]  eta: 0:12:34  lr: 0.000099  loss: 0.1170 (0.1223)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [220/689]  eta: 0:12:19  lr: 0.000099  loss: 0.1170 (0.1223)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [230/689]  eta: 0:12:03  lr: 0.000099  loss: 0.1255 (0.1226)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [240/689]  eta: 0:11:47  lr: 0.000099  loss: 0.1293 (0.1231)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [250/689]  eta: 0:11:32  lr: 0.000099  loss: 0.1193 (0.1225)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [260/689]  eta: 0:11:16  lr: 0.000099  loss: 0.1142 (0.1225)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [270/689]  eta: 0:11:00  lr: 0.000099  loss: 0.1176 (0.1225)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [280/689]  eta: 0:10:44  lr: 0.000099  loss: 0.1188 (0.1225)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [290/689]  eta: 0:10:29  lr: 0.000099  loss: 0.1167 (0.1224)  time: 1.5792  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:113]  [300/689]  eta: 0:10:13  lr: 0.000099  loss: 0.1150 (0.1224)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [310/689]  eta: 0:09:57  lr: 0.000099  loss: 0.1189 (0.1224)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [320/689]  eta: 0:09:41  lr: 0.000099  loss: 0.1229 (0.1224)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [330/689]  eta: 0:09:26  lr: 0.000099  loss: 0.1162 (0.1223)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [340/689]  eta: 0:09:10  lr: 0.000099  loss: 0.1160 (0.1222)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [350/689]  eta: 0:08:54  lr: 0.000099  loss: 0.1240 (0.1223)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [360/689]  eta: 0:08:38  lr: 0.000099  loss: 0.1193 (0.1221)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [370/689]  eta: 0:08:23  lr: 0.000099  loss: 0.1141 (0.1221)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [380/689]  eta: 0:08:07  lr: 0.000099  loss: 0.1177 (0.1221)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [390/689]  eta: 0:07:51  lr: 0.000099  loss: 0.1277 (0.1222)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [400/689]  eta: 0:07:35  lr: 0.000099  loss: 0.1333 (0.1226)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [410/689]  eta: 0:07:20  lr: 0.000099  loss: 0.1333 (0.1230)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [420/689]  eta: 0:07:04  lr: 0.000099  loss: 0.1329 (0.1234)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [430/689]  eta: 0:06:48  lr: 0.000099  loss: 0.1312 (0.1234)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [440/689]  eta: 0:06:32  lr: 0.000099  loss: 0.1166 (0.1233)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [450/689]  eta: 0:06:17  lr: 0.000099  loss: 0.1146 (0.1232)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [460/689]  eta: 0:06:01  lr: 0.000099  loss: 0.1146 (0.1232)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [470/689]  eta: 0:05:45  lr: 0.000099  loss: 0.1175 (0.1232)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [480/689]  eta: 0:05:29  lr: 0.000099  loss: 0.1175 (0.1231)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [490/689]  eta: 0:05:13  lr: 0.000099  loss: 0.1192 (0.1231)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [500/689]  eta: 0:04:58  lr: 0.000099  loss: 0.1202 (0.1231)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [510/689]  eta: 0:04:42  lr: 0.000099  loss: 0.1176 (0.1230)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [520/689]  eta: 0:04:26  lr: 0.000099  loss: 0.1136 (0.1230)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [530/689]  eta: 0:04:10  lr: 0.000099  loss: 0.1225 (0.1231)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [540/689]  eta: 0:03:55  lr: 0.000099  loss: 0.1231 (0.1231)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [550/689]  eta: 0:03:39  lr: 0.000099  loss: 0.1179 (0.1231)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [560/689]  eta: 0:03:23  lr: 0.000099  loss: 0.1190 (0.1232)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [570/689]  eta: 0:03:07  lr: 0.000099  loss: 0.1437 (0.1238)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [580/689]  eta: 0:02:51  lr: 0.000099  loss: 0.1437 (0.1241)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [590/689]  eta: 0:02:36  lr: 0.000099  loss: 0.1296 (0.1241)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [600/689]  eta: 0:02:20  lr: 0.000099  loss: 0.1251 (0.1242)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [610/689]  eta: 0:02:04  lr: 0.000099  loss: 0.1328 (0.1245)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [620/689]  eta: 0:01:48  lr: 0.000099  loss: 0.1257 (0.1246)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [630/689]  eta: 0:01:33  lr: 0.000099  loss: 0.1244 (0.1247)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [640/689]  eta: 0:01:17  lr: 0.000099  loss: 0.1230 (0.1245)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [650/689]  eta: 0:01:01  lr: 0.000099  loss: 0.1113 (0.1244)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [660/689]  eta: 0:00:45  lr: 0.000099  loss: 0.1198 (0.1245)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [670/689]  eta: 0:00:29  lr: 0.000099  loss: 0.1225 (0.1245)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [680/689]  eta: 0:00:14  lr: 0.000099  loss: 0.1225 (0.1245)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113]  [688/689]  eta: 0:00:01  lr: 0.000099  loss: 0.1159 (0.1243)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:113] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.1159 (0.1243)\n",
      "Valid: [epoch:113]  [ 0/14]  eta: 0:00:14  loss: 0.1090 (0.1090)  time: 1.0203  data: 0.3715  max mem: 39763\n",
      "Valid: [epoch:113]  [13/14]  eta: 0:00:00  loss: 0.1096 (0.1158)  time: 0.1148  data: 0.0266  max mem: 39763\n",
      "Valid: [epoch:113] Total time: 0:00:01 (0.1223 s / it)\n",
      "Averaged stats: loss: 0.1096 (0.1158)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_113_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.116%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:114]  [  0/689]  eta: 0:11:49  lr: 0.000099  loss: 0.1221 (0.1221)  time: 1.0300  data: 0.5529  max mem: 39763\n",
      "Train: [epoch:114]  [ 10/689]  eta: 0:17:17  lr: 0.000099  loss: 0.1176 (0.1184)  time: 1.5276  data: 0.0503  max mem: 39763\n",
      "Train: [epoch:114]  [ 20/689]  eta: 0:17:18  lr: 0.000099  loss: 0.1153 (0.1198)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [ 30/689]  eta: 0:17:08  lr: 0.000099  loss: 0.1245 (0.1226)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [ 40/689]  eta: 0:16:55  lr: 0.000099  loss: 0.1279 (0.1246)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [ 50/689]  eta: 0:16:41  lr: 0.000099  loss: 0.1234 (0.1243)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [ 60/689]  eta: 0:16:27  lr: 0.000099  loss: 0.1168 (0.1225)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [ 70/689]  eta: 0:16:12  lr: 0.000099  loss: 0.1139 (0.1221)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [ 80/689]  eta: 0:15:57  lr: 0.000099  loss: 0.1166 (0.1219)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [ 90/689]  eta: 0:15:41  lr: 0.000099  loss: 0.1268 (0.1228)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [100/689]  eta: 0:15:26  lr: 0.000099  loss: 0.1218 (0.1225)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [110/689]  eta: 0:15:11  lr: 0.000099  loss: 0.1187 (0.1226)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [120/689]  eta: 0:14:55  lr: 0.000099  loss: 0.1240 (0.1234)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [130/689]  eta: 0:14:40  lr: 0.000099  loss: 0.1265 (0.1237)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [140/689]  eta: 0:14:24  lr: 0.000099  loss: 0.1198 (0.1233)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [150/689]  eta: 0:14:08  lr: 0.000099  loss: 0.1173 (0.1232)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [160/689]  eta: 0:13:53  lr: 0.000099  loss: 0.1205 (0.1235)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [170/689]  eta: 0:13:37  lr: 0.000099  loss: 0.1170 (0.1235)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [180/689]  eta: 0:13:21  lr: 0.000099  loss: 0.1217 (0.1235)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [190/689]  eta: 0:13:06  lr: 0.000099  loss: 0.1256 (0.1237)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [200/689]  eta: 0:12:50  lr: 0.000099  loss: 0.1193 (0.1235)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:114]  [210/689]  eta: 0:12:34  lr: 0.000099  loss: 0.1193 (0.1240)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [220/689]  eta: 0:12:19  lr: 0.000099  loss: 0.1279 (0.1241)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [230/689]  eta: 0:12:03  lr: 0.000099  loss: 0.1202 (0.1240)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [240/689]  eta: 0:11:47  lr: 0.000099  loss: 0.1199 (0.1243)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [250/689]  eta: 0:11:31  lr: 0.000099  loss: 0.1244 (0.1245)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [260/689]  eta: 0:11:16  lr: 0.000099  loss: 0.1283 (0.1246)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [270/689]  eta: 0:11:00  lr: 0.000099  loss: 0.1242 (0.1245)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [280/689]  eta: 0:10:44  lr: 0.000099  loss: 0.1180 (0.1243)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [290/689]  eta: 0:10:29  lr: 0.000099  loss: 0.1182 (0.1241)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [300/689]  eta: 0:10:13  lr: 0.000099  loss: 0.1174 (0.1240)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [310/689]  eta: 0:09:57  lr: 0.000099  loss: 0.1167 (0.1239)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [320/689]  eta: 0:09:41  lr: 0.000099  loss: 0.1201 (0.1239)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [330/689]  eta: 0:09:26  lr: 0.000099  loss: 0.1201 (0.1240)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [340/689]  eta: 0:09:10  lr: 0.000099  loss: 0.1273 (0.1242)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [350/689]  eta: 0:08:54  lr: 0.000099  loss: 0.1197 (0.1242)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [360/689]  eta: 0:08:38  lr: 0.000099  loss: 0.1156 (0.1241)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [370/689]  eta: 0:08:23  lr: 0.000099  loss: 0.1209 (0.1240)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [380/689]  eta: 0:08:07  lr: 0.000099  loss: 0.1209 (0.1240)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [390/689]  eta: 0:07:51  lr: 0.000099  loss: 0.1218 (0.1242)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [400/689]  eta: 0:07:35  lr: 0.000099  loss: 0.1238 (0.1242)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [410/689]  eta: 0:07:19  lr: 0.000099  loss: 0.1238 (0.1243)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [420/689]  eta: 0:07:04  lr: 0.000099  loss: 0.1249 (0.1244)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [430/689]  eta: 0:06:48  lr: 0.000099  loss: 0.1291 (0.1246)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [440/689]  eta: 0:06:32  lr: 0.000099  loss: 0.1272 (0.1247)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [450/689]  eta: 0:06:16  lr: 0.000099  loss: 0.1230 (0.1246)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [460/689]  eta: 0:06:01  lr: 0.000099  loss: 0.1245 (0.1247)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [470/689]  eta: 0:05:45  lr: 0.000099  loss: 0.1253 (0.1248)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [480/689]  eta: 0:05:29  lr: 0.000099  loss: 0.1306 (0.1250)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [490/689]  eta: 0:05:13  lr: 0.000099  loss: 0.1272 (0.1250)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [500/689]  eta: 0:04:58  lr: 0.000099  loss: 0.1222 (0.1248)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [510/689]  eta: 0:04:42  lr: 0.000099  loss: 0.1216 (0.1249)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [520/689]  eta: 0:04:26  lr: 0.000099  loss: 0.1211 (0.1249)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [530/689]  eta: 0:04:10  lr: 0.000099  loss: 0.1192 (0.1249)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [540/689]  eta: 0:03:54  lr: 0.000099  loss: 0.1192 (0.1248)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [550/689]  eta: 0:03:39  lr: 0.000099  loss: 0.1192 (0.1246)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [560/689]  eta: 0:03:23  lr: 0.000099  loss: 0.1186 (0.1248)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [570/689]  eta: 0:03:07  lr: 0.000099  loss: 0.1267 (0.1249)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [580/689]  eta: 0:02:51  lr: 0.000099  loss: 0.1251 (0.1249)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [590/689]  eta: 0:02:36  lr: 0.000099  loss: 0.1223 (0.1249)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [600/689]  eta: 0:02:20  lr: 0.000099  loss: 0.1187 (0.1249)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [610/689]  eta: 0:02:04  lr: 0.000099  loss: 0.1211 (0.1249)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [620/689]  eta: 0:01:48  lr: 0.000099  loss: 0.1244 (0.1250)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [630/689]  eta: 0:01:33  lr: 0.000099  loss: 0.1221 (0.1250)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [640/689]  eta: 0:01:17  lr: 0.000099  loss: 0.1277 (0.1251)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [650/689]  eta: 0:01:01  lr: 0.000099  loss: 0.1284 (0.1250)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [660/689]  eta: 0:00:45  lr: 0.000099  loss: 0.1176 (0.1249)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [670/689]  eta: 0:00:29  lr: 0.000099  loss: 0.1182 (0.1250)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [680/689]  eta: 0:00:14  lr: 0.000099  loss: 0.1237 (0.1250)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114]  [688/689]  eta: 0:00:01  lr: 0.000099  loss: 0.1262 (0.1250)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:114] Total time: 0:18:06 (1.5774 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.1262 (0.1250)\n",
      "Valid: [epoch:114]  [ 0/14]  eta: 0:00:14  loss: 0.1088 (0.1088)  time: 1.0212  data: 0.3843  max mem: 39763\n",
      "Valid: [epoch:114]  [13/14]  eta: 0:00:00  loss: 0.1126 (0.1141)  time: 0.1149  data: 0.0275  max mem: 39763\n",
      "Valid: [epoch:114] Total time: 0:00:01 (0.1238 s / it)\n",
      "Averaged stats: loss: 0.1126 (0.1141)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_114_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.114%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:115]  [  0/689]  eta: 0:12:35  lr: 0.000098  loss: 0.1509 (0.1509)  time: 1.0969  data: 0.6200  max mem: 39763\n",
      "Train: [epoch:115]  [ 10/689]  eta: 0:17:19  lr: 0.000098  loss: 0.1270 (0.1315)  time: 1.5308  data: 0.0564  max mem: 39763\n",
      "Train: [epoch:115]  [ 20/689]  eta: 0:17:18  lr: 0.000098  loss: 0.1199 (0.1277)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [ 30/689]  eta: 0:17:08  lr: 0.000098  loss: 0.1152 (0.1253)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [ 40/689]  eta: 0:16:55  lr: 0.000098  loss: 0.1148 (0.1223)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [ 50/689]  eta: 0:16:40  lr: 0.000098  loss: 0.1145 (0.1221)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [ 60/689]  eta: 0:16:26  lr: 0.000098  loss: 0.1146 (0.1211)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [ 70/689]  eta: 0:16:11  lr: 0.000098  loss: 0.1174 (0.1215)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [ 80/689]  eta: 0:15:56  lr: 0.000098  loss: 0.1152 (0.1210)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [ 90/689]  eta: 0:15:40  lr: 0.000098  loss: 0.1152 (0.1209)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [100/689]  eta: 0:15:25  lr: 0.000098  loss: 0.1245 (0.1215)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [110/689]  eta: 0:15:10  lr: 0.000098  loss: 0.1266 (0.1222)  time: 1.5772  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:115]  [120/689]  eta: 0:14:54  lr: 0.000098  loss: 0.1204 (0.1223)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [130/689]  eta: 0:14:39  lr: 0.000098  loss: 0.1142 (0.1216)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [140/689]  eta: 0:14:23  lr: 0.000098  loss: 0.1142 (0.1217)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [150/689]  eta: 0:14:08  lr: 0.000098  loss: 0.1222 (0.1220)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [160/689]  eta: 0:13:52  lr: 0.000098  loss: 0.1245 (0.1221)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [170/689]  eta: 0:13:36  lr: 0.000098  loss: 0.1242 (0.1224)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [180/689]  eta: 0:13:21  lr: 0.000098  loss: 0.1229 (0.1224)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [190/689]  eta: 0:13:05  lr: 0.000098  loss: 0.1214 (0.1225)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [200/689]  eta: 0:12:49  lr: 0.000098  loss: 0.1191 (0.1222)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [210/689]  eta: 0:12:34  lr: 0.000098  loss: 0.1173 (0.1222)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [220/689]  eta: 0:12:18  lr: 0.000098  loss: 0.1180 (0.1222)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [230/689]  eta: 0:12:02  lr: 0.000098  loss: 0.1182 (0.1224)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [240/689]  eta: 0:11:47  lr: 0.000098  loss: 0.1282 (0.1228)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [250/689]  eta: 0:11:31  lr: 0.000098  loss: 0.1259 (0.1227)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [260/689]  eta: 0:11:15  lr: 0.000098  loss: 0.1280 (0.1233)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [270/689]  eta: 0:10:59  lr: 0.000098  loss: 0.1329 (0.1236)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [280/689]  eta: 0:10:44  lr: 0.000098  loss: 0.1209 (0.1236)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [290/689]  eta: 0:10:28  lr: 0.000098  loss: 0.1209 (0.1238)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [300/689]  eta: 0:10:12  lr: 0.000098  loss: 0.1284 (0.1240)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [310/689]  eta: 0:09:57  lr: 0.000098  loss: 0.1240 (0.1240)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [320/689]  eta: 0:09:41  lr: 0.000098  loss: 0.1192 (0.1238)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [330/689]  eta: 0:09:25  lr: 0.000098  loss: 0.1198 (0.1238)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [340/689]  eta: 0:09:09  lr: 0.000098  loss: 0.1209 (0.1237)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [350/689]  eta: 0:08:54  lr: 0.000098  loss: 0.1143 (0.1237)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [360/689]  eta: 0:08:38  lr: 0.000098  loss: 0.1193 (0.1236)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [370/689]  eta: 0:08:22  lr: 0.000098  loss: 0.1194 (0.1236)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [380/689]  eta: 0:08:06  lr: 0.000098  loss: 0.1259 (0.1237)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [390/689]  eta: 0:07:51  lr: 0.000098  loss: 0.1283 (0.1239)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [400/689]  eta: 0:07:35  lr: 0.000098  loss: 0.1264 (0.1239)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [410/689]  eta: 0:07:19  lr: 0.000098  loss: 0.1272 (0.1240)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [420/689]  eta: 0:07:03  lr: 0.000098  loss: 0.1283 (0.1242)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [430/689]  eta: 0:06:48  lr: 0.000098  loss: 0.1228 (0.1242)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [440/689]  eta: 0:06:32  lr: 0.000098  loss: 0.1197 (0.1241)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [450/689]  eta: 0:06:16  lr: 0.000098  loss: 0.1193 (0.1240)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [460/689]  eta: 0:06:00  lr: 0.000098  loss: 0.1195 (0.1240)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [470/689]  eta: 0:05:45  lr: 0.000098  loss: 0.1205 (0.1241)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [480/689]  eta: 0:05:29  lr: 0.000098  loss: 0.1198 (0.1239)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [490/689]  eta: 0:05:13  lr: 0.000098  loss: 0.1157 (0.1239)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [500/689]  eta: 0:04:57  lr: 0.000098  loss: 0.1210 (0.1240)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [510/689]  eta: 0:04:42  lr: 0.000098  loss: 0.1276 (0.1241)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [520/689]  eta: 0:04:26  lr: 0.000098  loss: 0.1346 (0.1244)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [530/689]  eta: 0:04:10  lr: 0.000098  loss: 0.1332 (0.1244)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [540/689]  eta: 0:03:54  lr: 0.000098  loss: 0.1191 (0.1244)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [550/689]  eta: 0:03:39  lr: 0.000098  loss: 0.1173 (0.1245)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [560/689]  eta: 0:03:23  lr: 0.000098  loss: 0.1230 (0.1245)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [570/689]  eta: 0:03:07  lr: 0.000098  loss: 0.1262 (0.1246)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [580/689]  eta: 0:02:51  lr: 0.000098  loss: 0.1137 (0.1245)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [590/689]  eta: 0:02:36  lr: 0.000098  loss: 0.1132 (0.1245)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [600/689]  eta: 0:02:20  lr: 0.000098  loss: 0.1182 (0.1245)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [610/689]  eta: 0:02:04  lr: 0.000098  loss: 0.1237 (0.1247)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [620/689]  eta: 0:01:48  lr: 0.000098  loss: 0.1259 (0.1247)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [630/689]  eta: 0:01:33  lr: 0.000098  loss: 0.1205 (0.1247)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [640/689]  eta: 0:01:17  lr: 0.000098  loss: 0.1185 (0.1246)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [650/689]  eta: 0:01:01  lr: 0.000098  loss: 0.1159 (0.1245)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [660/689]  eta: 0:00:45  lr: 0.000098  loss: 0.1179 (0.1245)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [670/689]  eta: 0:00:29  lr: 0.000098  loss: 0.1165 (0.1244)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [680/689]  eta: 0:00:14  lr: 0.000098  loss: 0.1187 (0.1244)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115]  [688/689]  eta: 0:00:01  lr: 0.000098  loss: 0.1187 (0.1243)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:115] Total time: 0:18:06 (1.5770 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.1187 (0.1243)\n",
      "Valid: [epoch:115]  [ 0/14]  eta: 0:00:14  loss: 0.1059 (0.1059)  time: 1.0016  data: 0.3967  max mem: 39763\n",
      "Valid: [epoch:115]  [13/14]  eta: 0:00:00  loss: 0.1130 (0.1145)  time: 0.1135  data: 0.0284  max mem: 39763\n",
      "Valid: [epoch:115] Total time: 0:00:01 (0.1227 s / it)\n",
      "Averaged stats: loss: 0.1130 (0.1145)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_115_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.114%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:116]  [  0/689]  eta: 0:11:46  lr: 0.000098  loss: 0.1044 (0.1044)  time: 1.0250  data: 0.5443  max mem: 39763\n",
      "Train: [epoch:116]  [ 10/689]  eta: 0:17:16  lr: 0.000098  loss: 0.1246 (0.1292)  time: 1.5265  data: 0.0496  max mem: 39763\n",
      "Train: [epoch:116]  [ 20/689]  eta: 0:17:17  lr: 0.000098  loss: 0.1242 (0.1284)  time: 1.5770  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:116]  [ 30/689]  eta: 0:17:07  lr: 0.000098  loss: 0.1238 (0.1294)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [ 40/689]  eta: 0:16:54  lr: 0.000098  loss: 0.1291 (0.1298)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [ 50/689]  eta: 0:16:40  lr: 0.000098  loss: 0.1291 (0.1292)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [ 60/689]  eta: 0:16:26  lr: 0.000098  loss: 0.1181 (0.1284)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [ 70/689]  eta: 0:16:11  lr: 0.000098  loss: 0.1169 (0.1266)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [ 80/689]  eta: 0:15:56  lr: 0.000098  loss: 0.1158 (0.1255)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [ 90/689]  eta: 0:15:41  lr: 0.000098  loss: 0.1158 (0.1254)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [100/689]  eta: 0:15:25  lr: 0.000098  loss: 0.1242 (0.1255)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [110/689]  eta: 0:15:10  lr: 0.000098  loss: 0.1290 (0.1265)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [120/689]  eta: 0:14:55  lr: 0.000098  loss: 0.1275 (0.1264)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [130/689]  eta: 0:14:39  lr: 0.000098  loss: 0.1206 (0.1257)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [140/689]  eta: 0:14:23  lr: 0.000098  loss: 0.1222 (0.1258)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [150/689]  eta: 0:14:08  lr: 0.000098  loss: 0.1232 (0.1259)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [160/689]  eta: 0:13:52  lr: 0.000098  loss: 0.1242 (0.1267)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [170/689]  eta: 0:13:37  lr: 0.000098  loss: 0.1265 (0.1266)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [180/689]  eta: 0:13:21  lr: 0.000098  loss: 0.1191 (0.1262)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [190/689]  eta: 0:13:05  lr: 0.000098  loss: 0.1243 (0.1266)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [200/689]  eta: 0:12:50  lr: 0.000098  loss: 0.1209 (0.1261)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [210/689]  eta: 0:12:34  lr: 0.000098  loss: 0.1197 (0.1268)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [220/689]  eta: 0:12:18  lr: 0.000098  loss: 0.1447 (0.1278)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [230/689]  eta: 0:12:02  lr: 0.000098  loss: 0.1448 (0.1291)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [240/689]  eta: 0:11:47  lr: 0.000098  loss: 0.1518 (0.1297)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [250/689]  eta: 0:11:31  lr: 0.000098  loss: 0.1415 (0.1303)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [260/689]  eta: 0:11:15  lr: 0.000098  loss: 0.1377 (0.1306)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [270/689]  eta: 0:11:00  lr: 0.000098  loss: 0.1298 (0.1304)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [280/689]  eta: 0:10:44  lr: 0.000098  loss: 0.1246 (0.1307)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [290/689]  eta: 0:10:28  lr: 0.000098  loss: 0.1238 (0.1305)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [300/689]  eta: 0:10:12  lr: 0.000098  loss: 0.1232 (0.1302)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [310/689]  eta: 0:09:57  lr: 0.000098  loss: 0.1232 (0.1300)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [320/689]  eta: 0:09:41  lr: 0.000098  loss: 0.1236 (0.1299)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [330/689]  eta: 0:09:25  lr: 0.000098  loss: 0.1179 (0.1295)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [340/689]  eta: 0:09:09  lr: 0.000098  loss: 0.1198 (0.1296)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [350/689]  eta: 0:08:54  lr: 0.000098  loss: 0.1230 (0.1294)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [360/689]  eta: 0:08:38  lr: 0.000098  loss: 0.1204 (0.1292)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [370/689]  eta: 0:08:22  lr: 0.000098  loss: 0.1180 (0.1289)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [380/689]  eta: 0:08:06  lr: 0.000098  loss: 0.1165 (0.1288)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [390/689]  eta: 0:07:51  lr: 0.000098  loss: 0.1212 (0.1286)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [400/689]  eta: 0:07:35  lr: 0.000098  loss: 0.1256 (0.1288)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [410/689]  eta: 0:07:19  lr: 0.000098  loss: 0.1295 (0.1289)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [420/689]  eta: 0:07:03  lr: 0.000098  loss: 0.1239 (0.1288)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [430/689]  eta: 0:06:48  lr: 0.000098  loss: 0.1224 (0.1287)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [440/689]  eta: 0:06:32  lr: 0.000098  loss: 0.1249 (0.1286)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [450/689]  eta: 0:06:16  lr: 0.000098  loss: 0.1255 (0.1284)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [460/689]  eta: 0:06:00  lr: 0.000098  loss: 0.1272 (0.1285)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [470/689]  eta: 0:05:45  lr: 0.000098  loss: 0.1272 (0.1286)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [480/689]  eta: 0:05:29  lr: 0.000098  loss: 0.1228 (0.1284)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [490/689]  eta: 0:05:13  lr: 0.000098  loss: 0.1219 (0.1283)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [500/689]  eta: 0:04:57  lr: 0.000098  loss: 0.1212 (0.1281)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [510/689]  eta: 0:04:42  lr: 0.000098  loss: 0.1219 (0.1280)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [520/689]  eta: 0:04:26  lr: 0.000098  loss: 0.1224 (0.1279)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [530/689]  eta: 0:04:10  lr: 0.000098  loss: 0.1217 (0.1278)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [540/689]  eta: 0:03:54  lr: 0.000098  loss: 0.1236 (0.1278)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [550/689]  eta: 0:03:39  lr: 0.000098  loss: 0.1210 (0.1277)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [560/689]  eta: 0:03:23  lr: 0.000098  loss: 0.1109 (0.1276)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [570/689]  eta: 0:03:07  lr: 0.000098  loss: 0.1218 (0.1276)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [580/689]  eta: 0:02:51  lr: 0.000098  loss: 0.1218 (0.1275)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [590/689]  eta: 0:02:36  lr: 0.000098  loss: 0.1222 (0.1275)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [600/689]  eta: 0:02:20  lr: 0.000098  loss: 0.1151 (0.1273)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [610/689]  eta: 0:02:04  lr: 0.000098  loss: 0.1125 (0.1271)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [620/689]  eta: 0:01:48  lr: 0.000098  loss: 0.1158 (0.1271)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [630/689]  eta: 0:01:33  lr: 0.000098  loss: 0.1223 (0.1270)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [640/689]  eta: 0:01:17  lr: 0.000098  loss: 0.1216 (0.1270)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [650/689]  eta: 0:01:01  lr: 0.000098  loss: 0.1250 (0.1270)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [660/689]  eta: 0:00:45  lr: 0.000098  loss: 0.1250 (0.1270)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [670/689]  eta: 0:00:29  lr: 0.000098  loss: 0.1259 (0.1270)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116]  [680/689]  eta: 0:00:14  lr: 0.000098  loss: 0.1320 (0.1272)  time: 1.5768  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:116]  [688/689]  eta: 0:00:01  lr: 0.000098  loss: 0.1287 (0.1271)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:116] Total time: 0:18:06 (1.5767 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.1287 (0.1271)\n",
      "Valid: [epoch:116]  [ 0/14]  eta: 0:00:14  loss: 0.1111 (0.1111)  time: 1.0228  data: 0.3928  max mem: 39763\n",
      "Valid: [epoch:116]  [13/14]  eta: 0:00:00  loss: 0.1165 (0.1177)  time: 0.1151  data: 0.0281  max mem: 39763\n",
      "Valid: [epoch:116] Total time: 0:00:01 (0.1244 s / it)\n",
      "Averaged stats: loss: 0.1165 (0.1177)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_116_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.118%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:117]  [  0/689]  eta: 0:11:52  lr: 0.000098  loss: 0.1005 (0.1005)  time: 1.0344  data: 0.5588  max mem: 39763\n",
      "Train: [epoch:117]  [ 10/689]  eta: 0:17:15  lr: 0.000098  loss: 0.1237 (0.1250)  time: 1.5246  data: 0.0509  max mem: 39763\n",
      "Train: [epoch:117]  [ 20/689]  eta: 0:17:16  lr: 0.000098  loss: 0.1174 (0.1230)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [ 30/689]  eta: 0:17:06  lr: 0.000098  loss: 0.1191 (0.1240)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [ 40/689]  eta: 0:16:53  lr: 0.000098  loss: 0.1221 (0.1240)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [ 50/689]  eta: 0:16:40  lr: 0.000098  loss: 0.1293 (0.1258)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [ 60/689]  eta: 0:16:25  lr: 0.000098  loss: 0.1247 (0.1250)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [ 70/689]  eta: 0:16:10  lr: 0.000098  loss: 0.1199 (0.1258)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [ 80/689]  eta: 0:15:55  lr: 0.000098  loss: 0.1199 (0.1255)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [ 90/689]  eta: 0:15:40  lr: 0.000098  loss: 0.1225 (0.1257)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [100/689]  eta: 0:15:25  lr: 0.000098  loss: 0.1237 (0.1259)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [110/689]  eta: 0:15:09  lr: 0.000098  loss: 0.1221 (0.1265)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [120/689]  eta: 0:14:54  lr: 0.000098  loss: 0.1196 (0.1259)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [130/689]  eta: 0:14:38  lr: 0.000098  loss: 0.1194 (0.1257)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [140/689]  eta: 0:14:23  lr: 0.000098  loss: 0.1194 (0.1255)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [150/689]  eta: 0:14:07  lr: 0.000098  loss: 0.1218 (0.1261)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [160/689]  eta: 0:13:52  lr: 0.000098  loss: 0.1268 (0.1261)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [170/689]  eta: 0:13:36  lr: 0.000098  loss: 0.1258 (0.1260)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [180/689]  eta: 0:13:21  lr: 0.000098  loss: 0.1202 (0.1260)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [190/689]  eta: 0:13:05  lr: 0.000098  loss: 0.1202 (0.1263)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [200/689]  eta: 0:12:49  lr: 0.000098  loss: 0.1213 (0.1262)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [210/689]  eta: 0:12:34  lr: 0.000098  loss: 0.1163 (0.1260)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [220/689]  eta: 0:12:18  lr: 0.000098  loss: 0.1226 (0.1261)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [230/689]  eta: 0:12:02  lr: 0.000098  loss: 0.1252 (0.1262)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [240/689]  eta: 0:11:47  lr: 0.000098  loss: 0.1306 (0.1265)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [250/689]  eta: 0:11:31  lr: 0.000098  loss: 0.1215 (0.1261)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [260/689]  eta: 0:11:15  lr: 0.000098  loss: 0.1192 (0.1261)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [270/689]  eta: 0:10:59  lr: 0.000098  loss: 0.1141 (0.1258)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [280/689]  eta: 0:10:44  lr: 0.000098  loss: 0.1249 (0.1262)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [290/689]  eta: 0:10:28  lr: 0.000098  loss: 0.1249 (0.1261)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [300/689]  eta: 0:10:12  lr: 0.000098  loss: 0.1173 (0.1258)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [310/689]  eta: 0:09:57  lr: 0.000098  loss: 0.1175 (0.1257)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [320/689]  eta: 0:09:41  lr: 0.000098  loss: 0.1207 (0.1257)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [330/689]  eta: 0:09:25  lr: 0.000098  loss: 0.1211 (0.1258)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [340/689]  eta: 0:09:09  lr: 0.000098  loss: 0.1281 (0.1258)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [350/689]  eta: 0:08:54  lr: 0.000098  loss: 0.1195 (0.1257)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [360/689]  eta: 0:08:38  lr: 0.000098  loss: 0.1195 (0.1258)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [370/689]  eta: 0:08:22  lr: 0.000098  loss: 0.1227 (0.1256)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [380/689]  eta: 0:08:06  lr: 0.000098  loss: 0.1227 (0.1255)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [390/689]  eta: 0:07:51  lr: 0.000098  loss: 0.1224 (0.1255)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [400/689]  eta: 0:07:35  lr: 0.000098  loss: 0.1224 (0.1256)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [410/689]  eta: 0:07:19  lr: 0.000098  loss: 0.1286 (0.1257)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [420/689]  eta: 0:07:03  lr: 0.000098  loss: 0.1285 (0.1259)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [430/689]  eta: 0:06:48  lr: 0.000098  loss: 0.1261 (0.1260)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [440/689]  eta: 0:06:32  lr: 0.000098  loss: 0.1249 (0.1260)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [450/689]  eta: 0:06:16  lr: 0.000098  loss: 0.1232 (0.1259)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [460/689]  eta: 0:06:00  lr: 0.000098  loss: 0.1273 (0.1260)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [470/689]  eta: 0:05:45  lr: 0.000098  loss: 0.1280 (0.1261)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [480/689]  eta: 0:05:29  lr: 0.000098  loss: 0.1280 (0.1262)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [490/689]  eta: 0:05:13  lr: 0.000098  loss: 0.1264 (0.1262)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [500/689]  eta: 0:04:57  lr: 0.000098  loss: 0.1226 (0.1262)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [510/689]  eta: 0:04:42  lr: 0.000098  loss: 0.1239 (0.1262)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [520/689]  eta: 0:04:26  lr: 0.000098  loss: 0.1242 (0.1262)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [530/689]  eta: 0:04:10  lr: 0.000098  loss: 0.1242 (0.1262)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [540/689]  eta: 0:03:54  lr: 0.000098  loss: 0.1219 (0.1260)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [550/689]  eta: 0:03:39  lr: 0.000098  loss: 0.1231 (0.1261)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [560/689]  eta: 0:03:23  lr: 0.000098  loss: 0.1274 (0.1263)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [570/689]  eta: 0:03:07  lr: 0.000098  loss: 0.1279 (0.1263)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [580/689]  eta: 0:02:51  lr: 0.000098  loss: 0.1253 (0.1263)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [590/689]  eta: 0:02:36  lr: 0.000098  loss: 0.1253 (0.1263)  time: 1.5775  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:117]  [600/689]  eta: 0:02:20  lr: 0.000098  loss: 0.1205 (0.1262)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [610/689]  eta: 0:02:04  lr: 0.000098  loss: 0.1242 (0.1264)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [620/689]  eta: 0:01:48  lr: 0.000098  loss: 0.1264 (0.1264)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [630/689]  eta: 0:01:33  lr: 0.000098  loss: 0.1230 (0.1264)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [640/689]  eta: 0:01:17  lr: 0.000098  loss: 0.1254 (0.1265)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [650/689]  eta: 0:01:01  lr: 0.000098  loss: 0.1287 (0.1265)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [660/689]  eta: 0:00:45  lr: 0.000098  loss: 0.1208 (0.1264)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [670/689]  eta: 0:00:29  lr: 0.000098  loss: 0.1208 (0.1264)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [680/689]  eta: 0:00:14  lr: 0.000098  loss: 0.1281 (0.1265)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117]  [688/689]  eta: 0:00:01  lr: 0.000098  loss: 0.1281 (0.1265)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:117] Total time: 0:18:06 (1.5768 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.1281 (0.1265)\n",
      "Valid: [epoch:117]  [ 0/14]  eta: 0:00:14  loss: 0.1265 (0.1265)  time: 1.0147  data: 0.3750  max mem: 39763\n",
      "Valid: [epoch:117]  [13/14]  eta: 0:00:00  loss: 0.1164 (0.1180)  time: 0.1144  data: 0.0268  max mem: 39763\n",
      "Valid: [epoch:117] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.1164 (0.1180)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_117_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.118%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:118]  [  0/689]  eta: 0:11:34  lr: 0.000098  loss: 0.1308 (0.1308)  time: 1.0076  data: 0.5355  max mem: 39763\n",
      "Train: [epoch:118]  [ 10/689]  eta: 0:17:15  lr: 0.000098  loss: 0.1164 (0.1204)  time: 1.5249  data: 0.0488  max mem: 39763\n",
      "Train: [epoch:118]  [ 20/689]  eta: 0:17:16  lr: 0.000098  loss: 0.1200 (0.1272)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [ 30/689]  eta: 0:17:06  lr: 0.000098  loss: 0.1298 (0.1284)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [ 40/689]  eta: 0:16:53  lr: 0.000098  loss: 0.1216 (0.1275)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [ 50/689]  eta: 0:16:40  lr: 0.000098  loss: 0.1201 (0.1266)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [ 60/689]  eta: 0:16:25  lr: 0.000098  loss: 0.1245 (0.1268)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [ 70/689]  eta: 0:16:10  lr: 0.000098  loss: 0.1283 (0.1278)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [ 80/689]  eta: 0:15:55  lr: 0.000098  loss: 0.1181 (0.1268)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [ 90/689]  eta: 0:15:40  lr: 0.000098  loss: 0.1193 (0.1273)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [100/689]  eta: 0:15:25  lr: 0.000098  loss: 0.1300 (0.1283)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [110/689]  eta: 0:15:09  lr: 0.000098  loss: 0.1363 (0.1294)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [120/689]  eta: 0:14:54  lr: 0.000098  loss: 0.1269 (0.1290)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [130/689]  eta: 0:14:38  lr: 0.000098  loss: 0.1226 (0.1284)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [140/689]  eta: 0:14:23  lr: 0.000098  loss: 0.1226 (0.1286)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [150/689]  eta: 0:14:07  lr: 0.000098  loss: 0.1206 (0.1281)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [160/689]  eta: 0:13:52  lr: 0.000098  loss: 0.1230 (0.1283)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [170/689]  eta: 0:13:36  lr: 0.000098  loss: 0.1352 (0.1293)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [180/689]  eta: 0:13:20  lr: 0.000098  loss: 0.1392 (0.1299)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [190/689]  eta: 0:13:05  lr: 0.000098  loss: 0.1408 (0.1304)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [200/689]  eta: 0:12:49  lr: 0.000098  loss: 0.1312 (0.1306)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [210/689]  eta: 0:12:33  lr: 0.000098  loss: 0.1282 (0.1303)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [220/689]  eta: 0:12:18  lr: 0.000098  loss: 0.1239 (0.1300)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [230/689]  eta: 0:12:02  lr: 0.000098  loss: 0.1218 (0.1297)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [240/689]  eta: 0:11:46  lr: 0.000098  loss: 0.1207 (0.1295)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [250/689]  eta: 0:11:31  lr: 0.000098  loss: 0.1241 (0.1294)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [260/689]  eta: 0:11:15  lr: 0.000098  loss: 0.1269 (0.1293)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [270/689]  eta: 0:10:59  lr: 0.000098  loss: 0.1282 (0.1293)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [280/689]  eta: 0:10:43  lr: 0.000098  loss: 0.1256 (0.1293)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [290/689]  eta: 0:10:28  lr: 0.000098  loss: 0.1224 (0.1289)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [300/689]  eta: 0:10:12  lr: 0.000098  loss: 0.1226 (0.1289)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [310/689]  eta: 0:09:56  lr: 0.000098  loss: 0.1232 (0.1288)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [320/689]  eta: 0:09:41  lr: 0.000098  loss: 0.1246 (0.1287)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [330/689]  eta: 0:09:25  lr: 0.000098  loss: 0.1204 (0.1287)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [340/689]  eta: 0:09:09  lr: 0.000098  loss: 0.1237 (0.1287)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [350/689]  eta: 0:08:53  lr: 0.000098  loss: 0.1212 (0.1284)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [360/689]  eta: 0:08:38  lr: 0.000098  loss: 0.1213 (0.1284)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [370/689]  eta: 0:08:22  lr: 0.000098  loss: 0.1213 (0.1283)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [380/689]  eta: 0:08:06  lr: 0.000098  loss: 0.1172 (0.1282)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [390/689]  eta: 0:07:50  lr: 0.000098  loss: 0.1294 (0.1283)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [400/689]  eta: 0:07:35  lr: 0.000098  loss: 0.1294 (0.1283)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [410/689]  eta: 0:07:19  lr: 0.000098  loss: 0.1212 (0.1283)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [420/689]  eta: 0:07:03  lr: 0.000098  loss: 0.1183 (0.1280)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [430/689]  eta: 0:06:48  lr: 0.000098  loss: 0.1188 (0.1280)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [440/689]  eta: 0:06:32  lr: 0.000098  loss: 0.1286 (0.1282)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [450/689]  eta: 0:06:16  lr: 0.000098  loss: 0.1286 (0.1280)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [460/689]  eta: 0:06:00  lr: 0.000098  loss: 0.1293 (0.1282)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [470/689]  eta: 0:05:45  lr: 0.000098  loss: 0.1307 (0.1281)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [480/689]  eta: 0:05:29  lr: 0.000098  loss: 0.1209 (0.1279)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [490/689]  eta: 0:05:13  lr: 0.000098  loss: 0.1217 (0.1280)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [500/689]  eta: 0:04:57  lr: 0.000098  loss: 0.1250 (0.1279)  time: 1.5782  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:118]  [510/689]  eta: 0:04:42  lr: 0.000098  loss: 0.1225 (0.1280)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [520/689]  eta: 0:04:26  lr: 0.000098  loss: 0.1284 (0.1280)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [530/689]  eta: 0:04:10  lr: 0.000098  loss: 0.1202 (0.1279)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [540/689]  eta: 0:03:54  lr: 0.000098  loss: 0.1219 (0.1279)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [550/689]  eta: 0:03:39  lr: 0.000098  loss: 0.1236 (0.1278)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [560/689]  eta: 0:03:23  lr: 0.000098  loss: 0.1206 (0.1278)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [570/689]  eta: 0:03:07  lr: 0.000098  loss: 0.1206 (0.1279)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [580/689]  eta: 0:02:51  lr: 0.000098  loss: 0.1167 (0.1277)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [590/689]  eta: 0:02:36  lr: 0.000098  loss: 0.1180 (0.1277)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [600/689]  eta: 0:02:20  lr: 0.000098  loss: 0.1184 (0.1276)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [610/689]  eta: 0:02:04  lr: 0.000098  loss: 0.1227 (0.1277)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [620/689]  eta: 0:01:48  lr: 0.000098  loss: 0.1292 (0.1277)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [630/689]  eta: 0:01:33  lr: 0.000098  loss: 0.1177 (0.1276)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [640/689]  eta: 0:01:17  lr: 0.000098  loss: 0.1176 (0.1274)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [650/689]  eta: 0:01:01  lr: 0.000098  loss: 0.1201 (0.1275)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [660/689]  eta: 0:00:45  lr: 0.000098  loss: 0.1217 (0.1276)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [670/689]  eta: 0:00:29  lr: 0.000098  loss: 0.1217 (0.1276)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [680/689]  eta: 0:00:14  lr: 0.000098  loss: 0.1242 (0.1275)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118]  [688/689]  eta: 0:00:01  lr: 0.000098  loss: 0.1237 (0.1275)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:118] Total time: 0:18:06 (1.5767 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.1237 (0.1275)\n",
      "Valid: [epoch:118]  [ 0/14]  eta: 0:00:14  loss: 0.1155 (0.1155)  time: 1.0131  data: 0.3692  max mem: 39763\n",
      "Valid: [epoch:118]  [13/14]  eta: 0:00:00  loss: 0.1181 (0.1194)  time: 0.1143  data: 0.0264  max mem: 39763\n",
      "Valid: [epoch:118] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.1181 (0.1194)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_118_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.119%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:119]  [  0/689]  eta: 0:12:08  lr: 0.000098  loss: 0.1283 (0.1283)  time: 1.0579  data: 0.5808  max mem: 39763\n",
      "Train: [epoch:119]  [ 10/689]  eta: 0:17:17  lr: 0.000098  loss: 0.1302 (0.1314)  time: 1.5282  data: 0.0529  max mem: 39763\n",
      "Train: [epoch:119]  [ 20/689]  eta: 0:17:17  lr: 0.000098  loss: 0.1389 (0.1354)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [ 30/689]  eta: 0:17:07  lr: 0.000098  loss: 0.1357 (0.1325)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [ 40/689]  eta: 0:16:54  lr: 0.000098  loss: 0.1240 (0.1313)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [ 50/689]  eta: 0:16:41  lr: 0.000098  loss: 0.1252 (0.1303)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [ 60/689]  eta: 0:16:26  lr: 0.000098  loss: 0.1252 (0.1296)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [ 70/689]  eta: 0:16:11  lr: 0.000098  loss: 0.1245 (0.1290)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [ 80/689]  eta: 0:15:56  lr: 0.000098  loss: 0.1168 (0.1281)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [ 90/689]  eta: 0:15:41  lr: 0.000098  loss: 0.1259 (0.1284)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [100/689]  eta: 0:15:25  lr: 0.000098  loss: 0.1267 (0.1286)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [110/689]  eta: 0:15:10  lr: 0.000098  loss: 0.1254 (0.1285)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [120/689]  eta: 0:14:55  lr: 0.000098  loss: 0.1241 (0.1283)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [130/689]  eta: 0:14:39  lr: 0.000098  loss: 0.1186 (0.1274)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [140/689]  eta: 0:14:23  lr: 0.000098  loss: 0.1200 (0.1285)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [150/689]  eta: 0:14:08  lr: 0.000098  loss: 0.1362 (0.1289)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [160/689]  eta: 0:13:52  lr: 0.000098  loss: 0.1295 (0.1292)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [170/689]  eta: 0:13:37  lr: 0.000098  loss: 0.1333 (0.1297)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [180/689]  eta: 0:13:21  lr: 0.000098  loss: 0.1347 (0.1298)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [190/689]  eta: 0:13:05  lr: 0.000098  loss: 0.1332 (0.1297)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [200/689]  eta: 0:12:50  lr: 0.000098  loss: 0.1223 (0.1295)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [210/689]  eta: 0:12:34  lr: 0.000098  loss: 0.1219 (0.1293)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [220/689]  eta: 0:12:18  lr: 0.000098  loss: 0.1230 (0.1293)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [230/689]  eta: 0:12:02  lr: 0.000098  loss: 0.1299 (0.1293)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [240/689]  eta: 0:11:47  lr: 0.000098  loss: 0.1222 (0.1294)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [250/689]  eta: 0:11:31  lr: 0.000098  loss: 0.1259 (0.1296)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [260/689]  eta: 0:11:15  lr: 0.000098  loss: 0.1310 (0.1301)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [270/689]  eta: 0:11:00  lr: 0.000098  loss: 0.1248 (0.1298)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [280/689]  eta: 0:10:44  lr: 0.000098  loss: 0.1248 (0.1301)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [290/689]  eta: 0:10:28  lr: 0.000098  loss: 0.1308 (0.1301)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [300/689]  eta: 0:10:12  lr: 0.000098  loss: 0.1249 (0.1300)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [310/689]  eta: 0:09:57  lr: 0.000098  loss: 0.1201 (0.1298)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [320/689]  eta: 0:09:41  lr: 0.000098  loss: 0.1250 (0.1297)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [330/689]  eta: 0:09:25  lr: 0.000098  loss: 0.1249 (0.1296)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [340/689]  eta: 0:09:10  lr: 0.000098  loss: 0.1232 (0.1296)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [350/689]  eta: 0:08:54  lr: 0.000098  loss: 0.1270 (0.1295)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [360/689]  eta: 0:08:38  lr: 0.000098  loss: 0.1274 (0.1295)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [370/689]  eta: 0:08:22  lr: 0.000098  loss: 0.1245 (0.1293)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [380/689]  eta: 0:08:07  lr: 0.000098  loss: 0.1197 (0.1291)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [390/689]  eta: 0:07:51  lr: 0.000098  loss: 0.1213 (0.1291)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [400/689]  eta: 0:07:35  lr: 0.000098  loss: 0.1300 (0.1292)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [410/689]  eta: 0:07:19  lr: 0.000098  loss: 0.1300 (0.1293)  time: 1.5784  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:119]  [420/689]  eta: 0:07:04  lr: 0.000098  loss: 0.1349 (0.1294)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [430/689]  eta: 0:06:48  lr: 0.000098  loss: 0.1310 (0.1295)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [440/689]  eta: 0:06:32  lr: 0.000098  loss: 0.1301 (0.1295)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [450/689]  eta: 0:06:16  lr: 0.000098  loss: 0.1279 (0.1294)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [460/689]  eta: 0:06:01  lr: 0.000098  loss: 0.1279 (0.1295)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [470/689]  eta: 0:05:45  lr: 0.000098  loss: 0.1262 (0.1294)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [480/689]  eta: 0:05:29  lr: 0.000098  loss: 0.1217 (0.1293)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [490/689]  eta: 0:05:13  lr: 0.000098  loss: 0.1214 (0.1291)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [500/689]  eta: 0:04:58  lr: 0.000098  loss: 0.1240 (0.1292)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [510/689]  eta: 0:04:42  lr: 0.000098  loss: 0.1244 (0.1291)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [520/689]  eta: 0:04:26  lr: 0.000098  loss: 0.1230 (0.1291)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [530/689]  eta: 0:04:10  lr: 0.000098  loss: 0.1230 (0.1289)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [540/689]  eta: 0:03:54  lr: 0.000098  loss: 0.1237 (0.1289)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [550/689]  eta: 0:03:39  lr: 0.000098  loss: 0.1289 (0.1290)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [560/689]  eta: 0:03:23  lr: 0.000098  loss: 0.1396 (0.1292)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [570/689]  eta: 0:03:07  lr: 0.000098  loss: 0.1265 (0.1290)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [580/689]  eta: 0:02:51  lr: 0.000098  loss: 0.1178 (0.1289)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [590/689]  eta: 0:02:36  lr: 0.000098  loss: 0.1180 (0.1289)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [600/689]  eta: 0:02:20  lr: 0.000098  loss: 0.1258 (0.1290)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [610/689]  eta: 0:02:04  lr: 0.000098  loss: 0.1251 (0.1288)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [620/689]  eta: 0:01:48  lr: 0.000098  loss: 0.1178 (0.1288)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [630/689]  eta: 0:01:33  lr: 0.000098  loss: 0.1314 (0.1289)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [640/689]  eta: 0:01:17  lr: 0.000098  loss: 0.1330 (0.1290)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [650/689]  eta: 0:01:01  lr: 0.000098  loss: 0.1283 (0.1290)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [660/689]  eta: 0:00:45  lr: 0.000098  loss: 0.1257 (0.1290)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [670/689]  eta: 0:00:29  lr: 0.000098  loss: 0.1236 (0.1289)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [680/689]  eta: 0:00:14  lr: 0.000098  loss: 0.1257 (0.1288)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119]  [688/689]  eta: 0:00:01  lr: 0.000098  loss: 0.1195 (0.1287)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:119] Total time: 0:18:06 (1.5775 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.1195 (0.1287)\n",
      "Valid: [epoch:119]  [ 0/14]  eta: 0:00:14  loss: 0.1127 (0.1127)  time: 1.0148  data: 0.3832  max mem: 39763\n",
      "Valid: [epoch:119]  [13/14]  eta: 0:00:00  loss: 0.1199 (0.1191)  time: 0.1144  data: 0.0274  max mem: 39763\n",
      "Valid: [epoch:119] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.1199 (0.1191)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_119_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.119%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:120]  [  0/689]  eta: 0:11:47  lr: 0.000098  loss: 0.1143 (0.1143)  time: 1.0267  data: 0.5480  max mem: 39763\n",
      "Train: [epoch:120]  [ 10/689]  eta: 0:17:16  lr: 0.000098  loss: 0.1236 (0.1268)  time: 1.5268  data: 0.0499  max mem: 39763\n",
      "Train: [epoch:120]  [ 20/689]  eta: 0:17:17  lr: 0.000098  loss: 0.1215 (0.1229)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [ 30/689]  eta: 0:17:07  lr: 0.000098  loss: 0.1186 (0.1257)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [ 40/689]  eta: 0:16:54  lr: 0.000098  loss: 0.1268 (0.1271)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [ 50/689]  eta: 0:16:40  lr: 0.000098  loss: 0.1274 (0.1275)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [ 60/689]  eta: 0:16:26  lr: 0.000098  loss: 0.1274 (0.1272)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [ 70/689]  eta: 0:16:11  lr: 0.000098  loss: 0.1238 (0.1280)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [ 80/689]  eta: 0:15:56  lr: 0.000098  loss: 0.1232 (0.1277)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [ 90/689]  eta: 0:15:41  lr: 0.000098  loss: 0.1197 (0.1267)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [100/689]  eta: 0:15:25  lr: 0.000098  loss: 0.1207 (0.1271)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [110/689]  eta: 0:15:10  lr: 0.000098  loss: 0.1250 (0.1278)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [120/689]  eta: 0:14:55  lr: 0.000098  loss: 0.1273 (0.1281)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [130/689]  eta: 0:14:39  lr: 0.000098  loss: 0.1262 (0.1279)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [140/689]  eta: 0:14:23  lr: 0.000098  loss: 0.1207 (0.1277)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [150/689]  eta: 0:14:08  lr: 0.000098  loss: 0.1195 (0.1281)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [160/689]  eta: 0:13:52  lr: 0.000098  loss: 0.1195 (0.1282)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [170/689]  eta: 0:13:37  lr: 0.000098  loss: 0.1233 (0.1286)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [180/689]  eta: 0:13:21  lr: 0.000098  loss: 0.1244 (0.1286)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [190/689]  eta: 0:13:05  lr: 0.000098  loss: 0.1288 (0.1289)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [200/689]  eta: 0:12:50  lr: 0.000098  loss: 0.1340 (0.1293)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [210/689]  eta: 0:12:34  lr: 0.000098  loss: 0.1290 (0.1295)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [220/689]  eta: 0:12:18  lr: 0.000098  loss: 0.1349 (0.1299)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [230/689]  eta: 0:12:02  lr: 0.000098  loss: 0.1383 (0.1304)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [240/689]  eta: 0:11:47  lr: 0.000098  loss: 0.1406 (0.1308)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [250/689]  eta: 0:11:31  lr: 0.000098  loss: 0.1391 (0.1312)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [260/689]  eta: 0:11:15  lr: 0.000098  loss: 0.1348 (0.1312)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [270/689]  eta: 0:11:00  lr: 0.000098  loss: 0.1289 (0.1311)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [280/689]  eta: 0:10:44  lr: 0.000098  loss: 0.1317 (0.1313)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [290/689]  eta: 0:10:28  lr: 0.000098  loss: 0.1322 (0.1313)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [300/689]  eta: 0:10:13  lr: 0.000098  loss: 0.1322 (0.1315)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [310/689]  eta: 0:09:57  lr: 0.000098  loss: 0.1340 (0.1318)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [320/689]  eta: 0:09:41  lr: 0.000098  loss: 0.1398 (0.1321)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:120]  [330/689]  eta: 0:09:25  lr: 0.000098  loss: 0.1396 (0.1323)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [340/689]  eta: 0:09:10  lr: 0.000098  loss: 0.1396 (0.1325)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [350/689]  eta: 0:08:54  lr: 0.000098  loss: 0.1346 (0.1325)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [360/689]  eta: 0:08:38  lr: 0.000098  loss: 0.1213 (0.1321)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [370/689]  eta: 0:08:22  lr: 0.000098  loss: 0.1190 (0.1318)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [380/689]  eta: 0:08:07  lr: 0.000098  loss: 0.1208 (0.1318)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [390/689]  eta: 0:07:51  lr: 0.000098  loss: 0.1319 (0.1319)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [400/689]  eta: 0:07:35  lr: 0.000098  loss: 0.1326 (0.1320)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [410/689]  eta: 0:07:19  lr: 0.000098  loss: 0.1269 (0.1320)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [420/689]  eta: 0:07:04  lr: 0.000098  loss: 0.1223 (0.1317)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [430/689]  eta: 0:06:48  lr: 0.000098  loss: 0.1220 (0.1314)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [440/689]  eta: 0:06:32  lr: 0.000098  loss: 0.1216 (0.1312)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [450/689]  eta: 0:06:16  lr: 0.000098  loss: 0.1247 (0.1311)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [460/689]  eta: 0:06:01  lr: 0.000098  loss: 0.1298 (0.1312)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [470/689]  eta: 0:05:45  lr: 0.000098  loss: 0.1342 (0.1311)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [480/689]  eta: 0:05:29  lr: 0.000098  loss: 0.1269 (0.1311)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [490/689]  eta: 0:05:13  lr: 0.000098  loss: 0.1280 (0.1310)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [500/689]  eta: 0:04:58  lr: 0.000098  loss: 0.1232 (0.1309)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [510/689]  eta: 0:04:42  lr: 0.000098  loss: 0.1189 (0.1308)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [520/689]  eta: 0:04:26  lr: 0.000098  loss: 0.1243 (0.1307)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [530/689]  eta: 0:04:10  lr: 0.000098  loss: 0.1268 (0.1308)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [540/689]  eta: 0:03:54  lr: 0.000098  loss: 0.1284 (0.1308)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [550/689]  eta: 0:03:39  lr: 0.000098  loss: 0.1304 (0.1310)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [560/689]  eta: 0:03:23  lr: 0.000098  loss: 0.1294 (0.1309)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [570/689]  eta: 0:03:07  lr: 0.000098  loss: 0.1232 (0.1307)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [580/689]  eta: 0:02:51  lr: 0.000098  loss: 0.1224 (0.1306)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [590/689]  eta: 0:02:36  lr: 0.000098  loss: 0.1226 (0.1306)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [600/689]  eta: 0:02:20  lr: 0.000098  loss: 0.1278 (0.1307)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [610/689]  eta: 0:02:04  lr: 0.000098  loss: 0.1358 (0.1307)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [620/689]  eta: 0:01:48  lr: 0.000098  loss: 0.1316 (0.1308)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [630/689]  eta: 0:01:33  lr: 0.000098  loss: 0.1296 (0.1308)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [640/689]  eta: 0:01:17  lr: 0.000098  loss: 0.1296 (0.1310)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [650/689]  eta: 0:01:01  lr: 0.000098  loss: 0.1297 (0.1309)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [660/689]  eta: 0:00:45  lr: 0.000098  loss: 0.1243 (0.1308)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [670/689]  eta: 0:00:29  lr: 0.000098  loss: 0.1234 (0.1308)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [680/689]  eta: 0:00:14  lr: 0.000098  loss: 0.1222 (0.1306)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120]  [688/689]  eta: 0:00:01  lr: 0.000098  loss: 0.1183 (0.1306)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:120] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.1183 (0.1306)\n",
      "Valid: [epoch:120]  [ 0/14]  eta: 0:00:14  loss: 0.1104 (0.1104)  time: 1.0121  data: 0.4043  max mem: 39763\n",
      "Valid: [epoch:120]  [13/14]  eta: 0:00:00  loss: 0.1216 (0.1205)  time: 0.1143  data: 0.0289  max mem: 39763\n",
      "Valid: [epoch:120] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.1216 (0.1205)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_120_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.120%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:121]  [  0/689]  eta: 0:11:49  lr: 0.000098  loss: 0.1573 (0.1573)  time: 1.0304  data: 0.5526  max mem: 39763\n",
      "Train: [epoch:121]  [ 10/689]  eta: 0:17:16  lr: 0.000098  loss: 0.1247 (0.1311)  time: 1.5260  data: 0.0503  max mem: 39763\n",
      "Train: [epoch:121]  [ 20/689]  eta: 0:17:17  lr: 0.000098  loss: 0.1247 (0.1283)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [ 30/689]  eta: 0:17:07  lr: 0.000098  loss: 0.1264 (0.1288)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [ 40/689]  eta: 0:16:54  lr: 0.000098  loss: 0.1264 (0.1304)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [ 50/689]  eta: 0:16:40  lr: 0.000098  loss: 0.1278 (0.1298)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [ 60/689]  eta: 0:16:26  lr: 0.000098  loss: 0.1257 (0.1294)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [ 70/689]  eta: 0:16:11  lr: 0.000098  loss: 0.1271 (0.1304)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [ 80/689]  eta: 0:15:56  lr: 0.000098  loss: 0.1293 (0.1304)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [ 90/689]  eta: 0:15:41  lr: 0.000098  loss: 0.1223 (0.1300)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [100/689]  eta: 0:15:25  lr: 0.000098  loss: 0.1262 (0.1306)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [110/689]  eta: 0:15:10  lr: 0.000098  loss: 0.1258 (0.1300)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [120/689]  eta: 0:14:55  lr: 0.000098  loss: 0.1200 (0.1293)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [130/689]  eta: 0:14:39  lr: 0.000098  loss: 0.1223 (0.1289)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [140/689]  eta: 0:14:23  lr: 0.000098  loss: 0.1245 (0.1290)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [150/689]  eta: 0:14:08  lr: 0.000098  loss: 0.1195 (0.1287)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [160/689]  eta: 0:13:52  lr: 0.000098  loss: 0.1188 (0.1288)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [170/689]  eta: 0:13:37  lr: 0.000098  loss: 0.1276 (0.1287)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [180/689]  eta: 0:13:21  lr: 0.000098  loss: 0.1237 (0.1282)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [190/689]  eta: 0:13:05  lr: 0.000098  loss: 0.1237 (0.1284)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [200/689]  eta: 0:12:50  lr: 0.000098  loss: 0.1262 (0.1282)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [210/689]  eta: 0:12:34  lr: 0.000098  loss: 0.1227 (0.1279)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [220/689]  eta: 0:12:18  lr: 0.000098  loss: 0.1210 (0.1276)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [230/689]  eta: 0:12:03  lr: 0.000098  loss: 0.1194 (0.1276)  time: 1.5788  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:121]  [240/689]  eta: 0:11:47  lr: 0.000098  loss: 0.1273 (0.1278)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [250/689]  eta: 0:11:31  lr: 0.000098  loss: 0.1303 (0.1279)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [260/689]  eta: 0:11:16  lr: 0.000098  loss: 0.1296 (0.1279)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [270/689]  eta: 0:11:00  lr: 0.000098  loss: 0.1286 (0.1279)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [280/689]  eta: 0:10:44  lr: 0.000098  loss: 0.1320 (0.1281)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [290/689]  eta: 0:10:28  lr: 0.000098  loss: 0.1320 (0.1283)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [300/689]  eta: 0:10:13  lr: 0.000098  loss: 0.1286 (0.1284)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [310/689]  eta: 0:09:57  lr: 0.000098  loss: 0.1226 (0.1282)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [320/689]  eta: 0:09:41  lr: 0.000098  loss: 0.1200 (0.1281)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [330/689]  eta: 0:09:25  lr: 0.000098  loss: 0.1263 (0.1281)  time: 1.5797  data: 0.0002  max mem: 39763\n",
      "Train: [epoch:121]  [340/689]  eta: 0:09:10  lr: 0.000098  loss: 0.1282 (0.1281)  time: 1.5795  data: 0.0002  max mem: 39763\n",
      "Train: [epoch:121]  [350/689]  eta: 0:08:54  lr: 0.000098  loss: 0.1299 (0.1282)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [360/689]  eta: 0:08:38  lr: 0.000098  loss: 0.1307 (0.1284)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [370/689]  eta: 0:08:23  lr: 0.000098  loss: 0.1307 (0.1285)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [380/689]  eta: 0:08:07  lr: 0.000098  loss: 0.1305 (0.1284)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [390/689]  eta: 0:07:51  lr: 0.000098  loss: 0.1305 (0.1286)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [400/689]  eta: 0:07:35  lr: 0.000098  loss: 0.1367 (0.1290)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [410/689]  eta: 0:07:19  lr: 0.000098  loss: 0.1354 (0.1292)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [420/689]  eta: 0:07:04  lr: 0.000098  loss: 0.1296 (0.1293)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [430/689]  eta: 0:06:48  lr: 0.000098  loss: 0.1246 (0.1292)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [440/689]  eta: 0:06:32  lr: 0.000098  loss: 0.1282 (0.1293)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [450/689]  eta: 0:06:16  lr: 0.000098  loss: 0.1282 (0.1293)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [460/689]  eta: 0:06:01  lr: 0.000098  loss: 0.1286 (0.1295)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [470/689]  eta: 0:05:45  lr: 0.000098  loss: 0.1296 (0.1296)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [480/689]  eta: 0:05:29  lr: 0.000098  loss: 0.1296 (0.1297)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [490/689]  eta: 0:05:13  lr: 0.000098  loss: 0.1304 (0.1297)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [500/689]  eta: 0:04:58  lr: 0.000098  loss: 0.1270 (0.1297)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [510/689]  eta: 0:04:42  lr: 0.000098  loss: 0.1270 (0.1298)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [520/689]  eta: 0:04:26  lr: 0.000098  loss: 0.1272 (0.1298)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [530/689]  eta: 0:04:10  lr: 0.000098  loss: 0.1266 (0.1297)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [540/689]  eta: 0:03:55  lr: 0.000098  loss: 0.1330 (0.1299)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [550/689]  eta: 0:03:39  lr: 0.000098  loss: 0.1339 (0.1300)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [560/689]  eta: 0:03:23  lr: 0.000098  loss: 0.1294 (0.1300)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [570/689]  eta: 0:03:07  lr: 0.000098  loss: 0.1265 (0.1300)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [580/689]  eta: 0:02:51  lr: 0.000098  loss: 0.1239 (0.1299)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [590/689]  eta: 0:02:36  lr: 0.000098  loss: 0.1202 (0.1298)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [600/689]  eta: 0:02:20  lr: 0.000098  loss: 0.1226 (0.1298)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [610/689]  eta: 0:02:04  lr: 0.000098  loss: 0.1240 (0.1298)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [620/689]  eta: 0:01:48  lr: 0.000098  loss: 0.1379 (0.1301)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [630/689]  eta: 0:01:33  lr: 0.000098  loss: 0.1379 (0.1302)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [640/689]  eta: 0:01:17  lr: 0.000098  loss: 0.1386 (0.1304)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [650/689]  eta: 0:01:01  lr: 0.000098  loss: 0.1273 (0.1302)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [660/689]  eta: 0:00:45  lr: 0.000098  loss: 0.1273 (0.1304)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [670/689]  eta: 0:00:29  lr: 0.000098  loss: 0.1414 (0.1306)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [680/689]  eta: 0:00:14  lr: 0.000098  loss: 0.1351 (0.1306)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121]  [688/689]  eta: 0:00:01  lr: 0.000098  loss: 0.1321 (0.1306)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:121] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.1321 (0.1306)\n",
      "Valid: [epoch:121]  [ 0/14]  eta: 0:00:14  loss: 0.1135 (0.1135)  time: 1.0142  data: 0.3991  max mem: 39763\n",
      "Valid: [epoch:121]  [13/14]  eta: 0:00:00  loss: 0.1216 (0.1228)  time: 0.1144  data: 0.0286  max mem: 39763\n",
      "Valid: [epoch:121] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.1216 (0.1228)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_121_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.123%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:122]  [  0/689]  eta: 0:11:28  lr: 0.000098  loss: 0.1174 (0.1174)  time: 0.9996  data: 0.5201  max mem: 39763\n",
      "Train: [epoch:122]  [ 10/689]  eta: 0:17:15  lr: 0.000098  loss: 0.1301 (0.1361)  time: 1.5247  data: 0.0474  max mem: 39763\n",
      "Train: [epoch:122]  [ 20/689]  eta: 0:17:16  lr: 0.000098  loss: 0.1246 (0.1295)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [ 30/689]  eta: 0:17:07  lr: 0.000098  loss: 0.1250 (0.1308)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [ 40/689]  eta: 0:16:54  lr: 0.000098  loss: 0.1317 (0.1321)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [ 50/689]  eta: 0:16:40  lr: 0.000098  loss: 0.1304 (0.1320)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [ 60/689]  eta: 0:16:26  lr: 0.000098  loss: 0.1304 (0.1326)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [ 70/689]  eta: 0:16:11  lr: 0.000098  loss: 0.1268 (0.1321)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [ 80/689]  eta: 0:15:56  lr: 0.000098  loss: 0.1267 (0.1329)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [ 90/689]  eta: 0:15:41  lr: 0.000098  loss: 0.1356 (0.1329)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [100/689]  eta: 0:15:25  lr: 0.000098  loss: 0.1356 (0.1334)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [110/689]  eta: 0:15:10  lr: 0.000098  loss: 0.1332 (0.1329)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [120/689]  eta: 0:14:54  lr: 0.000098  loss: 0.1265 (0.1331)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [130/689]  eta: 0:14:39  lr: 0.000098  loss: 0.1287 (0.1335)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [140/689]  eta: 0:14:23  lr: 0.000098  loss: 0.1400 (0.1339)  time: 1.5770  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:122]  [150/689]  eta: 0:14:08  lr: 0.000098  loss: 0.1400 (0.1345)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [160/689]  eta: 0:13:52  lr: 0.000098  loss: 0.1514 (0.1355)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [170/689]  eta: 0:13:36  lr: 0.000098  loss: 0.1497 (0.1358)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [180/689]  eta: 0:13:21  lr: 0.000098  loss: 0.1406 (0.1363)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [190/689]  eta: 0:13:05  lr: 0.000098  loss: 0.1441 (0.1374)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [200/689]  eta: 0:12:49  lr: 0.000098  loss: 0.1477 (0.1379)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [210/689]  eta: 0:12:34  lr: 0.000098  loss: 0.1375 (0.1378)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [220/689]  eta: 0:12:18  lr: 0.000098  loss: 0.1328 (0.1375)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [230/689]  eta: 0:12:02  lr: 0.000098  loss: 0.1338 (0.1374)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [240/689]  eta: 0:11:47  lr: 0.000098  loss: 0.1388 (0.1375)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [250/689]  eta: 0:11:31  lr: 0.000098  loss: 0.1317 (0.1371)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [260/689]  eta: 0:11:15  lr: 0.000098  loss: 0.1254 (0.1368)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [270/689]  eta: 0:11:00  lr: 0.000098  loss: 0.1260 (0.1366)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [280/689]  eta: 0:10:44  lr: 0.000098  loss: 0.1260 (0.1361)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [290/689]  eta: 0:10:28  lr: 0.000098  loss: 0.1274 (0.1358)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [300/689]  eta: 0:10:12  lr: 0.000098  loss: 0.1242 (0.1355)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [310/689]  eta: 0:09:57  lr: 0.000098  loss: 0.1242 (0.1354)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [320/689]  eta: 0:09:41  lr: 0.000098  loss: 0.1199 (0.1350)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [330/689]  eta: 0:09:25  lr: 0.000098  loss: 0.1190 (0.1346)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [340/689]  eta: 0:09:09  lr: 0.000098  loss: 0.1245 (0.1345)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [350/689]  eta: 0:08:54  lr: 0.000098  loss: 0.1275 (0.1341)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [360/689]  eta: 0:08:38  lr: 0.000098  loss: 0.1275 (0.1342)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [370/689]  eta: 0:08:22  lr: 0.000098  loss: 0.1176 (0.1337)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [380/689]  eta: 0:08:06  lr: 0.000098  loss: 0.1178 (0.1334)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [390/689]  eta: 0:07:51  lr: 0.000098  loss: 0.1212 (0.1335)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [400/689]  eta: 0:07:35  lr: 0.000098  loss: 0.1220 (0.1334)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [410/689]  eta: 0:07:19  lr: 0.000098  loss: 0.1220 (0.1333)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [420/689]  eta: 0:07:03  lr: 0.000098  loss: 0.1307 (0.1333)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [430/689]  eta: 0:06:48  lr: 0.000098  loss: 0.1327 (0.1333)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [440/689]  eta: 0:06:32  lr: 0.000098  loss: 0.1263 (0.1335)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [450/689]  eta: 0:06:16  lr: 0.000098  loss: 0.1248 (0.1333)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [460/689]  eta: 0:06:00  lr: 0.000098  loss: 0.1253 (0.1333)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [470/689]  eta: 0:05:45  lr: 0.000098  loss: 0.1270 (0.1332)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [480/689]  eta: 0:05:29  lr: 0.000098  loss: 0.1258 (0.1331)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [490/689]  eta: 0:05:13  lr: 0.000098  loss: 0.1331 (0.1333)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [500/689]  eta: 0:04:57  lr: 0.000098  loss: 0.1325 (0.1333)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [510/689]  eta: 0:04:42  lr: 0.000098  loss: 0.1285 (0.1332)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [520/689]  eta: 0:04:26  lr: 0.000098  loss: 0.1285 (0.1333)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [530/689]  eta: 0:04:10  lr: 0.000098  loss: 0.1305 (0.1334)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [540/689]  eta: 0:03:54  lr: 0.000098  loss: 0.1395 (0.1336)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [550/689]  eta: 0:03:39  lr: 0.000098  loss: 0.1379 (0.1335)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [560/689]  eta: 0:03:23  lr: 0.000098  loss: 0.1305 (0.1335)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [570/689]  eta: 0:03:07  lr: 0.000098  loss: 0.1368 (0.1336)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [580/689]  eta: 0:02:51  lr: 0.000098  loss: 0.1301 (0.1335)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [590/689]  eta: 0:02:36  lr: 0.000098  loss: 0.1247 (0.1334)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [600/689]  eta: 0:02:20  lr: 0.000098  loss: 0.1271 (0.1333)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [610/689]  eta: 0:02:04  lr: 0.000098  loss: 0.1271 (0.1332)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [620/689]  eta: 0:01:48  lr: 0.000098  loss: 0.1234 (0.1332)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [630/689]  eta: 0:01:33  lr: 0.000098  loss: 0.1235 (0.1331)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [640/689]  eta: 0:01:17  lr: 0.000098  loss: 0.1249 (0.1332)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [650/689]  eta: 0:01:01  lr: 0.000098  loss: 0.1312 (0.1332)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [660/689]  eta: 0:00:45  lr: 0.000098  loss: 0.1293 (0.1331)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [670/689]  eta: 0:00:29  lr: 0.000098  loss: 0.1250 (0.1332)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [680/689]  eta: 0:00:14  lr: 0.000098  loss: 0.1301 (0.1332)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122]  [688/689]  eta: 0:00:01  lr: 0.000098  loss: 0.1301 (0.1332)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:122] Total time: 0:18:06 (1.5768 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.1301 (0.1332)\n",
      "Valid: [epoch:122]  [ 0/14]  eta: 0:00:14  loss: 0.1155 (0.1155)  time: 1.0276  data: 0.3769  max mem: 39763\n",
      "Valid: [epoch:122]  [13/14]  eta: 0:00:00  loss: 0.1213 (0.1214)  time: 0.1154  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:122] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.1213 (0.1214)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_122_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.121%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:123]  [  0/689]  eta: 0:11:49  lr: 0.000098  loss: 0.1152 (0.1152)  time: 1.0294  data: 0.5514  max mem: 39763\n",
      "Train: [epoch:123]  [ 10/689]  eta: 0:17:15  lr: 0.000098  loss: 0.1242 (0.1273)  time: 1.5243  data: 0.0502  max mem: 39763\n",
      "Train: [epoch:123]  [ 20/689]  eta: 0:17:15  lr: 0.000098  loss: 0.1242 (0.1291)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [ 30/689]  eta: 0:17:06  lr: 0.000098  loss: 0.1270 (0.1305)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [ 40/689]  eta: 0:16:53  lr: 0.000098  loss: 0.1258 (0.1288)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [ 50/689]  eta: 0:16:39  lr: 0.000098  loss: 0.1227 (0.1284)  time: 1.5759  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:123]  [ 60/689]  eta: 0:16:25  lr: 0.000098  loss: 0.1246 (0.1287)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [ 70/689]  eta: 0:16:10  lr: 0.000098  loss: 0.1316 (0.1294)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [ 80/689]  eta: 0:15:55  lr: 0.000098  loss: 0.1221 (0.1288)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [ 90/689]  eta: 0:15:40  lr: 0.000098  loss: 0.1205 (0.1281)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [100/689]  eta: 0:15:25  lr: 0.000098  loss: 0.1233 (0.1290)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [110/689]  eta: 0:15:09  lr: 0.000098  loss: 0.1360 (0.1293)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [120/689]  eta: 0:14:54  lr: 0.000098  loss: 0.1350 (0.1306)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [130/689]  eta: 0:14:38  lr: 0.000098  loss: 0.1342 (0.1308)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [140/689]  eta: 0:14:23  lr: 0.000098  loss: 0.1342 (0.1315)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [150/689]  eta: 0:14:07  lr: 0.000098  loss: 0.1319 (0.1315)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [160/689]  eta: 0:13:52  lr: 0.000098  loss: 0.1259 (0.1312)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [170/689]  eta: 0:13:36  lr: 0.000098  loss: 0.1252 (0.1306)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [180/689]  eta: 0:13:21  lr: 0.000098  loss: 0.1301 (0.1309)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [190/689]  eta: 0:13:05  lr: 0.000098  loss: 0.1280 (0.1306)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [200/689]  eta: 0:12:49  lr: 0.000098  loss: 0.1260 (0.1303)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [210/689]  eta: 0:12:34  lr: 0.000098  loss: 0.1269 (0.1302)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [220/689]  eta: 0:12:18  lr: 0.000098  loss: 0.1208 (0.1301)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [230/689]  eta: 0:12:02  lr: 0.000098  loss: 0.1269 (0.1300)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [240/689]  eta: 0:11:47  lr: 0.000098  loss: 0.1307 (0.1302)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [250/689]  eta: 0:11:31  lr: 0.000098  loss: 0.1296 (0.1303)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [260/689]  eta: 0:11:15  lr: 0.000098  loss: 0.1282 (0.1303)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [270/689]  eta: 0:11:00  lr: 0.000098  loss: 0.1215 (0.1300)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [280/689]  eta: 0:10:44  lr: 0.000098  loss: 0.1181 (0.1299)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [290/689]  eta: 0:10:28  lr: 0.000098  loss: 0.1145 (0.1293)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [300/689]  eta: 0:10:12  lr: 0.000098  loss: 0.1178 (0.1295)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [310/689]  eta: 0:09:57  lr: 0.000098  loss: 0.1342 (0.1296)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [320/689]  eta: 0:09:41  lr: 0.000098  loss: 0.1313 (0.1296)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [330/689]  eta: 0:09:25  lr: 0.000098  loss: 0.1320 (0.1299)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [340/689]  eta: 0:09:10  lr: 0.000098  loss: 0.1369 (0.1300)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [350/689]  eta: 0:08:54  lr: 0.000098  loss: 0.1331 (0.1301)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [360/689]  eta: 0:08:38  lr: 0.000098  loss: 0.1267 (0.1300)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [370/689]  eta: 0:08:22  lr: 0.000098  loss: 0.1248 (0.1301)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [380/689]  eta: 0:08:07  lr: 0.000098  loss: 0.1353 (0.1305)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [390/689]  eta: 0:07:51  lr: 0.000098  loss: 0.1338 (0.1305)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [400/689]  eta: 0:07:35  lr: 0.000098  loss: 0.1285 (0.1304)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [410/689]  eta: 0:07:19  lr: 0.000098  loss: 0.1306 (0.1305)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [420/689]  eta: 0:07:04  lr: 0.000098  loss: 0.1214 (0.1303)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [430/689]  eta: 0:06:48  lr: 0.000098  loss: 0.1324 (0.1305)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [440/689]  eta: 0:06:32  lr: 0.000098  loss: 0.1354 (0.1305)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [450/689]  eta: 0:06:16  lr: 0.000098  loss: 0.1289 (0.1305)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [460/689]  eta: 0:06:01  lr: 0.000098  loss: 0.1246 (0.1305)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [470/689]  eta: 0:05:45  lr: 0.000098  loss: 0.1239 (0.1304)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [480/689]  eta: 0:05:29  lr: 0.000098  loss: 0.1247 (0.1303)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [490/689]  eta: 0:05:13  lr: 0.000098  loss: 0.1247 (0.1302)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [500/689]  eta: 0:04:58  lr: 0.000098  loss: 0.1293 (0.1302)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [510/689]  eta: 0:04:42  lr: 0.000098  loss: 0.1286 (0.1301)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [520/689]  eta: 0:04:26  lr: 0.000098  loss: 0.1291 (0.1302)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [530/689]  eta: 0:04:10  lr: 0.000098  loss: 0.1291 (0.1301)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [540/689]  eta: 0:03:54  lr: 0.000098  loss: 0.1219 (0.1300)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [550/689]  eta: 0:03:39  lr: 0.000098  loss: 0.1220 (0.1300)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [560/689]  eta: 0:03:23  lr: 0.000098  loss: 0.1347 (0.1302)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [570/689]  eta: 0:03:07  lr: 0.000098  loss: 0.1358 (0.1303)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [580/689]  eta: 0:02:51  lr: 0.000098  loss: 0.1333 (0.1304)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [590/689]  eta: 0:02:36  lr: 0.000098  loss: 0.1314 (0.1305)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [600/689]  eta: 0:02:20  lr: 0.000098  loss: 0.1269 (0.1305)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [610/689]  eta: 0:02:04  lr: 0.000098  loss: 0.1269 (0.1304)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [620/689]  eta: 0:01:48  lr: 0.000098  loss: 0.1280 (0.1305)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [630/689]  eta: 0:01:33  lr: 0.000098  loss: 0.1280 (0.1305)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [640/689]  eta: 0:01:17  lr: 0.000098  loss: 0.1297 (0.1305)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [650/689]  eta: 0:01:01  lr: 0.000098  loss: 0.1297 (0.1304)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [660/689]  eta: 0:00:45  lr: 0.000098  loss: 0.1262 (0.1304)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [670/689]  eta: 0:00:29  lr: 0.000098  loss: 0.1264 (0.1304)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [680/689]  eta: 0:00:14  lr: 0.000098  loss: 0.1264 (0.1304)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123]  [688/689]  eta: 0:00:01  lr: 0.000098  loss: 0.1268 (0.1304)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:123] Total time: 0:18:06 (1.5774 s / it)\n",
      "Averaged stats: lr: 0.000098  loss: 0.1268 (0.1304)\n",
      "Valid: [epoch:123]  [ 0/14]  eta: 0:00:13  loss: 0.1281 (0.1281)  time: 0.9884  data: 0.4062  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:123]  [13/14]  eta: 0:00:00  loss: 0.1213 (0.1214)  time: 0.1125  data: 0.0291  max mem: 39763\n",
      "Valid: [epoch:123] Total time: 0:00:01 (0.1219 s / it)\n",
      "Averaged stats: loss: 0.1213 (0.1214)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_123_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.121%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:124]  [  0/689]  eta: 0:11:47  lr: 0.000097  loss: 0.1119 (0.1119)  time: 1.0273  data: 0.5520  max mem: 39763\n",
      "Train: [epoch:124]  [ 10/689]  eta: 0:17:16  lr: 0.000097  loss: 0.1251 (0.1254)  time: 1.5270  data: 0.0503  max mem: 39763\n",
      "Train: [epoch:124]  [ 20/689]  eta: 0:17:17  lr: 0.000097  loss: 0.1216 (0.1242)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [ 30/689]  eta: 0:17:07  lr: 0.000097  loss: 0.1235 (0.1287)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [ 40/689]  eta: 0:16:54  lr: 0.000097  loss: 0.1279 (0.1284)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [ 50/689]  eta: 0:16:40  lr: 0.000097  loss: 0.1261 (0.1302)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [ 60/689]  eta: 0:16:26  lr: 0.000097  loss: 0.1255 (0.1298)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [ 70/689]  eta: 0:16:11  lr: 0.000097  loss: 0.1288 (0.1304)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [ 80/689]  eta: 0:15:56  lr: 0.000097  loss: 0.1301 (0.1305)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [ 90/689]  eta: 0:15:41  lr: 0.000097  loss: 0.1301 (0.1306)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [100/689]  eta: 0:15:25  lr: 0.000097  loss: 0.1323 (0.1310)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [110/689]  eta: 0:15:10  lr: 0.000097  loss: 0.1325 (0.1319)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [120/689]  eta: 0:14:55  lr: 0.000097  loss: 0.1376 (0.1328)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [130/689]  eta: 0:14:39  lr: 0.000097  loss: 0.1376 (0.1329)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [140/689]  eta: 0:14:24  lr: 0.000097  loss: 0.1312 (0.1329)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [150/689]  eta: 0:14:08  lr: 0.000097  loss: 0.1293 (0.1331)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [160/689]  eta: 0:13:52  lr: 0.000097  loss: 0.1298 (0.1332)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [170/689]  eta: 0:13:37  lr: 0.000097  loss: 0.1298 (0.1331)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [180/689]  eta: 0:13:21  lr: 0.000097  loss: 0.1258 (0.1334)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [190/689]  eta: 0:13:05  lr: 0.000097  loss: 0.1282 (0.1335)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [200/689]  eta: 0:12:50  lr: 0.000097  loss: 0.1286 (0.1334)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [210/689]  eta: 0:12:34  lr: 0.000097  loss: 0.1278 (0.1333)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [220/689]  eta: 0:12:18  lr: 0.000097  loss: 0.1335 (0.1335)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [230/689]  eta: 0:12:03  lr: 0.000097  loss: 0.1339 (0.1336)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [240/689]  eta: 0:11:47  lr: 0.000097  loss: 0.1359 (0.1336)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [250/689]  eta: 0:11:31  lr: 0.000097  loss: 0.1319 (0.1336)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [260/689]  eta: 0:11:16  lr: 0.000097  loss: 0.1334 (0.1341)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [270/689]  eta: 0:11:00  lr: 0.000097  loss: 0.1330 (0.1338)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [280/689]  eta: 0:10:44  lr: 0.000097  loss: 0.1269 (0.1339)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [290/689]  eta: 0:10:29  lr: 0.000097  loss: 0.1349 (0.1339)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [300/689]  eta: 0:10:13  lr: 0.000097  loss: 0.1324 (0.1338)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [310/689]  eta: 0:09:57  lr: 0.000097  loss: 0.1236 (0.1335)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [320/689]  eta: 0:09:41  lr: 0.000097  loss: 0.1227 (0.1335)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [330/689]  eta: 0:09:26  lr: 0.000097  loss: 0.1302 (0.1335)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [340/689]  eta: 0:09:10  lr: 0.000097  loss: 0.1294 (0.1334)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [350/689]  eta: 0:08:54  lr: 0.000097  loss: 0.1252 (0.1331)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [360/689]  eta: 0:08:38  lr: 0.000097  loss: 0.1230 (0.1331)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [370/689]  eta: 0:08:23  lr: 0.000097  loss: 0.1301 (0.1330)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [380/689]  eta: 0:08:07  lr: 0.000097  loss: 0.1310 (0.1328)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [390/689]  eta: 0:07:51  lr: 0.000097  loss: 0.1291 (0.1328)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [400/689]  eta: 0:07:35  lr: 0.000097  loss: 0.1314 (0.1329)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [410/689]  eta: 0:07:20  lr: 0.000097  loss: 0.1314 (0.1328)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [420/689]  eta: 0:07:04  lr: 0.000097  loss: 0.1286 (0.1327)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [430/689]  eta: 0:06:48  lr: 0.000097  loss: 0.1306 (0.1330)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [440/689]  eta: 0:06:32  lr: 0.000097  loss: 0.1342 (0.1331)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [450/689]  eta: 0:06:17  lr: 0.000097  loss: 0.1312 (0.1331)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [460/689]  eta: 0:06:01  lr: 0.000097  loss: 0.1303 (0.1331)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [470/689]  eta: 0:05:45  lr: 0.000097  loss: 0.1237 (0.1330)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [480/689]  eta: 0:05:29  lr: 0.000097  loss: 0.1298 (0.1330)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [490/689]  eta: 0:05:13  lr: 0.000097  loss: 0.1255 (0.1328)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [500/689]  eta: 0:04:58  lr: 0.000097  loss: 0.1252 (0.1328)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [510/689]  eta: 0:04:42  lr: 0.000097  loss: 0.1255 (0.1327)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [520/689]  eta: 0:04:26  lr: 0.000097  loss: 0.1251 (0.1327)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [530/689]  eta: 0:04:10  lr: 0.000097  loss: 0.1251 (0.1325)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [540/689]  eta: 0:03:55  lr: 0.000097  loss: 0.1263 (0.1324)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [550/689]  eta: 0:03:39  lr: 0.000097  loss: 0.1263 (0.1323)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [560/689]  eta: 0:03:23  lr: 0.000097  loss: 0.1208 (0.1321)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [570/689]  eta: 0:03:07  lr: 0.000097  loss: 0.1188 (0.1321)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [580/689]  eta: 0:02:51  lr: 0.000097  loss: 0.1238 (0.1321)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [590/689]  eta: 0:02:36  lr: 0.000097  loss: 0.1231 (0.1321)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [600/689]  eta: 0:02:20  lr: 0.000097  loss: 0.1206 (0.1320)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [610/689]  eta: 0:02:04  lr: 0.000097  loss: 0.1355 (0.1321)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [620/689]  eta: 0:01:48  lr: 0.000097  loss: 0.1407 (0.1322)  time: 1.5778  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:124]  [630/689]  eta: 0:01:33  lr: 0.000097  loss: 0.1247 (0.1321)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [640/689]  eta: 0:01:17  lr: 0.000097  loss: 0.1248 (0.1321)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [650/689]  eta: 0:01:01  lr: 0.000097  loss: 0.1293 (0.1320)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [660/689]  eta: 0:00:45  lr: 0.000097  loss: 0.1349 (0.1322)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [670/689]  eta: 0:00:29  lr: 0.000097  loss: 0.1357 (0.1322)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [680/689]  eta: 0:00:14  lr: 0.000097  loss: 0.1252 (0.1320)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124]  [688/689]  eta: 0:00:01  lr: 0.000097  loss: 0.1214 (0.1320)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:124] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.1214 (0.1320)\n",
      "Valid: [epoch:124]  [ 0/14]  eta: 0:00:14  loss: 0.1280 (0.1280)  time: 1.0179  data: 0.3635  max mem: 39763\n",
      "Valid: [epoch:124]  [13/14]  eta: 0:00:00  loss: 0.1204 (0.1218)  time: 0.1147  data: 0.0260  max mem: 39763\n",
      "Valid: [epoch:124] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.1204 (0.1218)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_124_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.122%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:125]  [  0/689]  eta: 0:12:12  lr: 0.000097  loss: 0.1125 (0.1125)  time: 1.0637  data: 0.5899  max mem: 39763\n",
      "Train: [epoch:125]  [ 10/689]  eta: 0:17:17  lr: 0.000097  loss: 0.1281 (0.1318)  time: 1.5284  data: 0.0537  max mem: 39763\n",
      "Train: [epoch:125]  [ 20/689]  eta: 0:17:17  lr: 0.000097  loss: 0.1319 (0.1328)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [ 30/689]  eta: 0:17:07  lr: 0.000097  loss: 0.1387 (0.1371)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [ 40/689]  eta: 0:16:54  lr: 0.000097  loss: 0.1304 (0.1348)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [ 50/689]  eta: 0:16:40  lr: 0.000097  loss: 0.1259 (0.1338)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [ 60/689]  eta: 0:16:26  lr: 0.000097  loss: 0.1329 (0.1347)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [ 70/689]  eta: 0:16:11  lr: 0.000097  loss: 0.1353 (0.1349)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [ 80/689]  eta: 0:15:56  lr: 0.000097  loss: 0.1258 (0.1338)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [ 90/689]  eta: 0:15:41  lr: 0.000097  loss: 0.1244 (0.1337)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [100/689]  eta: 0:15:25  lr: 0.000097  loss: 0.1284 (0.1340)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [110/689]  eta: 0:15:10  lr: 0.000097  loss: 0.1348 (0.1344)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [120/689]  eta: 0:14:54  lr: 0.000097  loss: 0.1348 (0.1351)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [130/689]  eta: 0:14:39  lr: 0.000097  loss: 0.1412 (0.1367)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [140/689]  eta: 0:14:23  lr: 0.000097  loss: 0.1408 (0.1366)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [150/689]  eta: 0:14:08  lr: 0.000097  loss: 0.1308 (0.1365)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [160/689]  eta: 0:13:52  lr: 0.000097  loss: 0.1279 (0.1363)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [170/689]  eta: 0:13:37  lr: 0.000097  loss: 0.1293 (0.1367)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [180/689]  eta: 0:13:21  lr: 0.000097  loss: 0.1418 (0.1369)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [190/689]  eta: 0:13:05  lr: 0.000097  loss: 0.1367 (0.1372)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [200/689]  eta: 0:12:50  lr: 0.000097  loss: 0.1367 (0.1373)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [210/689]  eta: 0:12:34  lr: 0.000097  loss: 0.1304 (0.1369)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [220/689]  eta: 0:12:18  lr: 0.000097  loss: 0.1331 (0.1374)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [230/689]  eta: 0:12:02  lr: 0.000097  loss: 0.1363 (0.1373)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [240/689]  eta: 0:11:47  lr: 0.000097  loss: 0.1358 (0.1372)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [250/689]  eta: 0:11:31  lr: 0.000097  loss: 0.1346 (0.1371)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [260/689]  eta: 0:11:15  lr: 0.000097  loss: 0.1346 (0.1375)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [270/689]  eta: 0:11:00  lr: 0.000097  loss: 0.1388 (0.1374)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [280/689]  eta: 0:10:44  lr: 0.000097  loss: 0.1281 (0.1373)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [290/689]  eta: 0:10:28  lr: 0.000097  loss: 0.1287 (0.1371)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [300/689]  eta: 0:10:12  lr: 0.000097  loss: 0.1348 (0.1371)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [310/689]  eta: 0:09:57  lr: 0.000097  loss: 0.1328 (0.1370)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [320/689]  eta: 0:09:41  lr: 0.000097  loss: 0.1312 (0.1372)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [330/689]  eta: 0:09:25  lr: 0.000097  loss: 0.1477 (0.1377)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [340/689]  eta: 0:09:10  lr: 0.000097  loss: 0.1466 (0.1381)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [350/689]  eta: 0:08:54  lr: 0.000097  loss: 0.1414 (0.1382)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [360/689]  eta: 0:08:38  lr: 0.000097  loss: 0.1297 (0.1378)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [370/689]  eta: 0:08:22  lr: 0.000097  loss: 0.1221 (0.1375)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [380/689]  eta: 0:08:07  lr: 0.000097  loss: 0.1240 (0.1375)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [390/689]  eta: 0:07:51  lr: 0.000097  loss: 0.1317 (0.1374)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [400/689]  eta: 0:07:35  lr: 0.000097  loss: 0.1329 (0.1375)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [410/689]  eta: 0:07:19  lr: 0.000097  loss: 0.1449 (0.1377)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [420/689]  eta: 0:07:04  lr: 0.000097  loss: 0.1343 (0.1374)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [430/689]  eta: 0:06:48  lr: 0.000097  loss: 0.1265 (0.1372)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [440/689]  eta: 0:06:32  lr: 0.000097  loss: 0.1328 (0.1371)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [450/689]  eta: 0:06:16  lr: 0.000097  loss: 0.1306 (0.1369)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [460/689]  eta: 0:06:00  lr: 0.000097  loss: 0.1289 (0.1369)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [470/689]  eta: 0:05:45  lr: 0.000097  loss: 0.1337 (0.1368)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [480/689]  eta: 0:05:29  lr: 0.000097  loss: 0.1362 (0.1369)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [490/689]  eta: 0:05:13  lr: 0.000097  loss: 0.1303 (0.1367)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [500/689]  eta: 0:04:57  lr: 0.000097  loss: 0.1293 (0.1368)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [510/689]  eta: 0:04:42  lr: 0.000097  loss: 0.1246 (0.1365)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [520/689]  eta: 0:04:26  lr: 0.000097  loss: 0.1267 (0.1365)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [530/689]  eta: 0:04:10  lr: 0.000097  loss: 0.1401 (0.1366)  time: 1.5775  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:125]  [540/689]  eta: 0:03:54  lr: 0.000097  loss: 0.1401 (0.1366)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [550/689]  eta: 0:03:39  lr: 0.000097  loss: 0.1278 (0.1365)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [560/689]  eta: 0:03:23  lr: 0.000097  loss: 0.1341 (0.1366)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [570/689]  eta: 0:03:07  lr: 0.000097  loss: 0.1423 (0.1367)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [580/689]  eta: 0:02:51  lr: 0.000097  loss: 0.1432 (0.1369)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [590/689]  eta: 0:02:36  lr: 0.000097  loss: 0.1402 (0.1368)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [600/689]  eta: 0:02:20  lr: 0.000097  loss: 0.1315 (0.1367)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [610/689]  eta: 0:02:04  lr: 0.000097  loss: 0.1276 (0.1366)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [620/689]  eta: 0:01:48  lr: 0.000097  loss: 0.1254 (0.1364)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [630/689]  eta: 0:01:33  lr: 0.000097  loss: 0.1246 (0.1362)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [640/689]  eta: 0:01:17  lr: 0.000097  loss: 0.1223 (0.1361)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [650/689]  eta: 0:01:01  lr: 0.000097  loss: 0.1325 (0.1362)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [660/689]  eta: 0:00:45  lr: 0.000097  loss: 0.1470 (0.1363)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [670/689]  eta: 0:00:29  lr: 0.000097  loss: 0.1456 (0.1364)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [680/689]  eta: 0:00:14  lr: 0.000097  loss: 0.1364 (0.1364)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125]  [688/689]  eta: 0:00:01  lr: 0.000097  loss: 0.1295 (0.1362)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:125] Total time: 0:18:06 (1.5770 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.1295 (0.1362)\n",
      "Valid: [epoch:125]  [ 0/14]  eta: 0:00:14  loss: 0.1309 (0.1309)  time: 1.0156  data: 0.3938  max mem: 39763\n",
      "Valid: [epoch:125]  [13/14]  eta: 0:00:00  loss: 0.1227 (0.1233)  time: 0.1145  data: 0.0282  max mem: 39763\n",
      "Valid: [epoch:125] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.1227 (0.1233)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_125_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.123%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:126]  [  0/689]  eta: 0:11:43  lr: 0.000097  loss: 0.1200 (0.1200)  time: 1.0212  data: 0.5423  max mem: 39763\n",
      "Train: [epoch:126]  [ 10/689]  eta: 0:17:15  lr: 0.000097  loss: 0.1322 (0.1386)  time: 1.5252  data: 0.0494  max mem: 39763\n",
      "Train: [epoch:126]  [ 20/689]  eta: 0:17:16  lr: 0.000097  loss: 0.1248 (0.1332)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [ 30/689]  eta: 0:17:07  lr: 0.000097  loss: 0.1239 (0.1341)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [ 40/689]  eta: 0:16:54  lr: 0.000097  loss: 0.1306 (0.1333)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [ 50/689]  eta: 0:16:40  lr: 0.000097  loss: 0.1306 (0.1336)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [ 60/689]  eta: 0:16:25  lr: 0.000097  loss: 0.1284 (0.1326)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [ 70/689]  eta: 0:16:11  lr: 0.000097  loss: 0.1248 (0.1320)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [ 80/689]  eta: 0:15:56  lr: 0.000097  loss: 0.1246 (0.1311)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [ 90/689]  eta: 0:15:40  lr: 0.000097  loss: 0.1249 (0.1314)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [100/689]  eta: 0:15:25  lr: 0.000097  loss: 0.1280 (0.1320)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [110/689]  eta: 0:15:10  lr: 0.000097  loss: 0.1308 (0.1325)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [120/689]  eta: 0:14:54  lr: 0.000097  loss: 0.1306 (0.1328)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [130/689]  eta: 0:14:39  lr: 0.000097  loss: 0.1306 (0.1330)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [140/689]  eta: 0:14:23  lr: 0.000097  loss: 0.1337 (0.1331)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [150/689]  eta: 0:14:08  lr: 0.000097  loss: 0.1347 (0.1336)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [160/689]  eta: 0:13:52  lr: 0.000097  loss: 0.1267 (0.1330)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [170/689]  eta: 0:13:36  lr: 0.000097  loss: 0.1247 (0.1329)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [180/689]  eta: 0:13:21  lr: 0.000097  loss: 0.1264 (0.1329)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [190/689]  eta: 0:13:05  lr: 0.000097  loss: 0.1281 (0.1327)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [200/689]  eta: 0:12:50  lr: 0.000097  loss: 0.1253 (0.1324)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [210/689]  eta: 0:12:34  lr: 0.000097  loss: 0.1235 (0.1323)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [220/689]  eta: 0:12:18  lr: 0.000097  loss: 0.1227 (0.1322)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [230/689]  eta: 0:12:03  lr: 0.000097  loss: 0.1259 (0.1323)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [240/689]  eta: 0:11:47  lr: 0.000097  loss: 0.1338 (0.1325)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [250/689]  eta: 0:11:31  lr: 0.000097  loss: 0.1326 (0.1326)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [260/689]  eta: 0:11:15  lr: 0.000097  loss: 0.1318 (0.1326)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [270/689]  eta: 0:11:00  lr: 0.000097  loss: 0.1332 (0.1326)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [280/689]  eta: 0:10:44  lr: 0.000097  loss: 0.1305 (0.1325)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [290/689]  eta: 0:10:28  lr: 0.000097  loss: 0.1309 (0.1325)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [300/689]  eta: 0:10:13  lr: 0.000097  loss: 0.1328 (0.1327)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [310/689]  eta: 0:09:57  lr: 0.000097  loss: 0.1260 (0.1326)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [320/689]  eta: 0:09:41  lr: 0.000097  loss: 0.1248 (0.1324)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [330/689]  eta: 0:09:25  lr: 0.000097  loss: 0.1369 (0.1331)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [340/689]  eta: 0:09:10  lr: 0.000097  loss: 0.1378 (0.1331)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [350/689]  eta: 0:08:54  lr: 0.000097  loss: 0.1290 (0.1331)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [360/689]  eta: 0:08:38  lr: 0.000097  loss: 0.1320 (0.1333)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [370/689]  eta: 0:08:22  lr: 0.000097  loss: 0.1295 (0.1332)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [380/689]  eta: 0:08:07  lr: 0.000097  loss: 0.1263 (0.1332)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [390/689]  eta: 0:07:51  lr: 0.000097  loss: 0.1305 (0.1333)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [400/689]  eta: 0:07:35  lr: 0.000097  loss: 0.1309 (0.1334)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [410/689]  eta: 0:07:19  lr: 0.000097  loss: 0.1335 (0.1334)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [420/689]  eta: 0:07:04  lr: 0.000097  loss: 0.1295 (0.1333)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [430/689]  eta: 0:06:48  lr: 0.000097  loss: 0.1267 (0.1332)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [440/689]  eta: 0:06:32  lr: 0.000097  loss: 0.1293 (0.1332)  time: 1.5789  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:126]  [450/689]  eta: 0:06:16  lr: 0.000097  loss: 0.1249 (0.1330)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [460/689]  eta: 0:06:01  lr: 0.000097  loss: 0.1332 (0.1333)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [470/689]  eta: 0:05:45  lr: 0.000097  loss: 0.1422 (0.1334)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [480/689]  eta: 0:05:29  lr: 0.000097  loss: 0.1269 (0.1333)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [490/689]  eta: 0:05:13  lr: 0.000097  loss: 0.1342 (0.1335)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [500/689]  eta: 0:04:58  lr: 0.000097  loss: 0.1313 (0.1335)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [510/689]  eta: 0:04:42  lr: 0.000097  loss: 0.1300 (0.1336)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [520/689]  eta: 0:04:26  lr: 0.000097  loss: 0.1322 (0.1336)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [530/689]  eta: 0:04:10  lr: 0.000097  loss: 0.1268 (0.1334)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [540/689]  eta: 0:03:55  lr: 0.000097  loss: 0.1259 (0.1333)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [550/689]  eta: 0:03:39  lr: 0.000097  loss: 0.1278 (0.1333)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [560/689]  eta: 0:03:23  lr: 0.000097  loss: 0.1352 (0.1334)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [570/689]  eta: 0:03:07  lr: 0.000097  loss: 0.1352 (0.1334)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [580/689]  eta: 0:02:51  lr: 0.000097  loss: 0.1313 (0.1335)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [590/689]  eta: 0:02:36  lr: 0.000097  loss: 0.1343 (0.1335)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [600/689]  eta: 0:02:20  lr: 0.000097  loss: 0.1340 (0.1336)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [610/689]  eta: 0:02:04  lr: 0.000097  loss: 0.1354 (0.1337)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [620/689]  eta: 0:01:48  lr: 0.000097  loss: 0.1354 (0.1337)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [630/689]  eta: 0:01:33  lr: 0.000097  loss: 0.1330 (0.1338)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [640/689]  eta: 0:01:17  lr: 0.000097  loss: 0.1330 (0.1338)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [650/689]  eta: 0:01:01  lr: 0.000097  loss: 0.1342 (0.1339)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [660/689]  eta: 0:00:45  lr: 0.000097  loss: 0.1297 (0.1339)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [670/689]  eta: 0:00:29  lr: 0.000097  loss: 0.1294 (0.1340)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [680/689]  eta: 0:00:14  lr: 0.000097  loss: 0.1332 (0.1340)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126]  [688/689]  eta: 0:00:01  lr: 0.000097  loss: 0.1332 (0.1340)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:126] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.1332 (0.1340)\n",
      "Valid: [epoch:126]  [ 0/14]  eta: 0:00:14  loss: 0.1293 (0.1293)  time: 1.0165  data: 0.3842  max mem: 39763\n",
      "Valid: [epoch:126]  [13/14]  eta: 0:00:00  loss: 0.1225 (0.1240)  time: 0.1146  data: 0.0275  max mem: 39763\n",
      "Valid: [epoch:126] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.1225 (0.1240)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_126_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.124%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:127]  [  0/689]  eta: 0:11:47  lr: 0.000097  loss: 0.1252 (0.1252)  time: 1.0275  data: 0.5590  max mem: 39763\n",
      "Train: [epoch:127]  [ 10/689]  eta: 0:17:15  lr: 0.000097  loss: 0.1269 (0.1267)  time: 1.5255  data: 0.0509  max mem: 39763\n",
      "Train: [epoch:127]  [ 20/689]  eta: 0:17:16  lr: 0.000097  loss: 0.1312 (0.1317)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [ 30/689]  eta: 0:17:06  lr: 0.000097  loss: 0.1349 (0.1354)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [ 40/689]  eta: 0:16:54  lr: 0.000097  loss: 0.1306 (0.1347)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [ 50/689]  eta: 0:16:40  lr: 0.000097  loss: 0.1266 (0.1341)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [ 60/689]  eta: 0:16:25  lr: 0.000097  loss: 0.1271 (0.1361)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [ 70/689]  eta: 0:16:11  lr: 0.000097  loss: 0.1480 (0.1372)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [ 80/689]  eta: 0:15:55  lr: 0.000097  loss: 0.1296 (0.1361)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [ 90/689]  eta: 0:15:40  lr: 0.000097  loss: 0.1296 (0.1369)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [100/689]  eta: 0:15:25  lr: 0.000097  loss: 0.1383 (0.1367)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [110/689]  eta: 0:15:10  lr: 0.000097  loss: 0.1331 (0.1366)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [120/689]  eta: 0:14:54  lr: 0.000097  loss: 0.1347 (0.1367)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [130/689]  eta: 0:14:39  lr: 0.000097  loss: 0.1310 (0.1370)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [140/689]  eta: 0:14:23  lr: 0.000097  loss: 0.1310 (0.1369)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [150/689]  eta: 0:14:08  lr: 0.000097  loss: 0.1405 (0.1370)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [160/689]  eta: 0:13:52  lr: 0.000097  loss: 0.1328 (0.1367)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [170/689]  eta: 0:13:36  lr: 0.000097  loss: 0.1312 (0.1367)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [180/689]  eta: 0:13:21  lr: 0.000097  loss: 0.1333 (0.1368)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [190/689]  eta: 0:13:05  lr: 0.000097  loss: 0.1372 (0.1370)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [200/689]  eta: 0:12:50  lr: 0.000097  loss: 0.1416 (0.1372)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [210/689]  eta: 0:12:34  lr: 0.000097  loss: 0.1312 (0.1369)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [220/689]  eta: 0:12:18  lr: 0.000097  loss: 0.1333 (0.1370)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [230/689]  eta: 0:12:03  lr: 0.000097  loss: 0.1370 (0.1370)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [240/689]  eta: 0:11:47  lr: 0.000097  loss: 0.1346 (0.1371)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [250/689]  eta: 0:11:31  lr: 0.000097  loss: 0.1340 (0.1370)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [260/689]  eta: 0:11:15  lr: 0.000097  loss: 0.1340 (0.1371)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [270/689]  eta: 0:11:00  lr: 0.000097  loss: 0.1452 (0.1378)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [280/689]  eta: 0:10:44  lr: 0.000097  loss: 0.1476 (0.1379)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [290/689]  eta: 0:10:28  lr: 0.000097  loss: 0.1293 (0.1374)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [300/689]  eta: 0:10:13  lr: 0.000097  loss: 0.1238 (0.1374)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [310/689]  eta: 0:09:57  lr: 0.000097  loss: 0.1378 (0.1375)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [320/689]  eta: 0:09:41  lr: 0.000097  loss: 0.1341 (0.1373)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [330/689]  eta: 0:09:25  lr: 0.000097  loss: 0.1306 (0.1372)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [340/689]  eta: 0:09:10  lr: 0.000097  loss: 0.1294 (0.1371)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [350/689]  eta: 0:08:54  lr: 0.000097  loss: 0.1299 (0.1370)  time: 1.5790  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:127]  [360/689]  eta: 0:08:38  lr: 0.000097  loss: 0.1299 (0.1368)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [370/689]  eta: 0:08:22  lr: 0.000097  loss: 0.1295 (0.1366)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [380/689]  eta: 0:08:07  lr: 0.000097  loss: 0.1306 (0.1365)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [390/689]  eta: 0:07:51  lr: 0.000097  loss: 0.1320 (0.1365)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [400/689]  eta: 0:07:35  lr: 0.000097  loss: 0.1386 (0.1369)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [410/689]  eta: 0:07:19  lr: 0.000097  loss: 0.1364 (0.1367)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [420/689]  eta: 0:07:04  lr: 0.000097  loss: 0.1280 (0.1365)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [430/689]  eta: 0:06:48  lr: 0.000097  loss: 0.1311 (0.1366)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [440/689]  eta: 0:06:32  lr: 0.000097  loss: 0.1356 (0.1367)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [450/689]  eta: 0:06:16  lr: 0.000097  loss: 0.1386 (0.1368)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [460/689]  eta: 0:06:01  lr: 0.000097  loss: 0.1315 (0.1368)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [470/689]  eta: 0:05:45  lr: 0.000097  loss: 0.1351 (0.1370)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [480/689]  eta: 0:05:29  lr: 0.000097  loss: 0.1393 (0.1369)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [490/689]  eta: 0:05:13  lr: 0.000097  loss: 0.1349 (0.1368)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [500/689]  eta: 0:04:58  lr: 0.000097  loss: 0.1350 (0.1368)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [510/689]  eta: 0:04:42  lr: 0.000097  loss: 0.1345 (0.1368)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [520/689]  eta: 0:04:26  lr: 0.000097  loss: 0.1279 (0.1367)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [530/689]  eta: 0:04:10  lr: 0.000097  loss: 0.1275 (0.1366)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [540/689]  eta: 0:03:55  lr: 0.000097  loss: 0.1365 (0.1367)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [550/689]  eta: 0:03:39  lr: 0.000097  loss: 0.1387 (0.1367)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [560/689]  eta: 0:03:23  lr: 0.000097  loss: 0.1422 (0.1368)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [570/689]  eta: 0:03:07  lr: 0.000097  loss: 0.1353 (0.1369)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [580/689]  eta: 0:02:51  lr: 0.000097  loss: 0.1303 (0.1368)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [590/689]  eta: 0:02:36  lr: 0.000097  loss: 0.1338 (0.1369)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [600/689]  eta: 0:02:20  lr: 0.000097  loss: 0.1415 (0.1371)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [610/689]  eta: 0:02:04  lr: 0.000097  loss: 0.1358 (0.1369)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [620/689]  eta: 0:01:48  lr: 0.000097  loss: 0.1361 (0.1370)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [630/689]  eta: 0:01:33  lr: 0.000097  loss: 0.1375 (0.1370)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [640/689]  eta: 0:01:17  lr: 0.000097  loss: 0.1342 (0.1370)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [650/689]  eta: 0:01:01  lr: 0.000097  loss: 0.1308 (0.1369)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [660/689]  eta: 0:00:45  lr: 0.000097  loss: 0.1309 (0.1369)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [670/689]  eta: 0:00:29  lr: 0.000097  loss: 0.1364 (0.1370)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [680/689]  eta: 0:00:14  lr: 0.000097  loss: 0.1447 (0.1370)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127]  [688/689]  eta: 0:00:01  lr: 0.000097  loss: 0.1296 (0.1369)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:127] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.1296 (0.1369)\n",
      "Valid: [epoch:127]  [ 0/14]  eta: 0:00:14  loss: 0.1291 (0.1291)  time: 1.0102  data: 0.3694  max mem: 39763\n",
      "Valid: [epoch:127]  [13/14]  eta: 0:00:00  loss: 0.1291 (0.1287)  time: 0.1141  data: 0.0264  max mem: 39763\n",
      "Valid: [epoch:127] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.1291 (0.1287)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_127_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.129%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:128]  [  0/689]  eta: 0:11:44  lr: 0.000097  loss: 0.1351 (0.1351)  time: 1.0226  data: 0.5430  max mem: 39763\n",
      "Train: [epoch:128]  [ 10/689]  eta: 0:17:16  lr: 0.000097  loss: 0.1511 (0.1445)  time: 1.5270  data: 0.0494  max mem: 39763\n",
      "Train: [epoch:128]  [ 20/689]  eta: 0:17:17  lr: 0.000097  loss: 0.1448 (0.1458)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [ 30/689]  eta: 0:17:07  lr: 0.000097  loss: 0.1410 (0.1435)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [ 40/689]  eta: 0:16:54  lr: 0.000097  loss: 0.1382 (0.1422)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [ 50/689]  eta: 0:16:40  lr: 0.000097  loss: 0.1302 (0.1409)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [ 60/689]  eta: 0:16:26  lr: 0.000097  loss: 0.1292 (0.1396)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [ 70/689]  eta: 0:16:11  lr: 0.000097  loss: 0.1313 (0.1394)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [ 80/689]  eta: 0:15:56  lr: 0.000097  loss: 0.1375 (0.1402)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [ 90/689]  eta: 0:15:41  lr: 0.000097  loss: 0.1375 (0.1394)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [100/689]  eta: 0:15:25  lr: 0.000097  loss: 0.1306 (0.1388)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [110/689]  eta: 0:15:10  lr: 0.000097  loss: 0.1306 (0.1385)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [120/689]  eta: 0:14:54  lr: 0.000097  loss: 0.1310 (0.1386)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [130/689]  eta: 0:14:39  lr: 0.000097  loss: 0.1343 (0.1384)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [140/689]  eta: 0:14:23  lr: 0.000097  loss: 0.1292 (0.1381)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [150/689]  eta: 0:14:08  lr: 0.000097  loss: 0.1340 (0.1386)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [160/689]  eta: 0:13:52  lr: 0.000097  loss: 0.1341 (0.1382)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [170/689]  eta: 0:13:37  lr: 0.000097  loss: 0.1392 (0.1384)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [180/689]  eta: 0:13:21  lr: 0.000097  loss: 0.1324 (0.1382)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [190/689]  eta: 0:13:05  lr: 0.000097  loss: 0.1324 (0.1387)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [200/689]  eta: 0:12:50  lr: 0.000097  loss: 0.1372 (0.1384)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [210/689]  eta: 0:12:34  lr: 0.000097  loss: 0.1299 (0.1383)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [220/689]  eta: 0:12:18  lr: 0.000097  loss: 0.1300 (0.1383)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [230/689]  eta: 0:12:03  lr: 0.000097  loss: 0.1318 (0.1384)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [240/689]  eta: 0:11:47  lr: 0.000097  loss: 0.1326 (0.1384)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [250/689]  eta: 0:11:31  lr: 0.000097  loss: 0.1282 (0.1381)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [260/689]  eta: 0:11:16  lr: 0.000097  loss: 0.1309 (0.1381)  time: 1.5795  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:128]  [270/689]  eta: 0:11:00  lr: 0.000097  loss: 0.1299 (0.1376)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [280/689]  eta: 0:10:44  lr: 0.000097  loss: 0.1271 (0.1376)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [290/689]  eta: 0:10:29  lr: 0.000097  loss: 0.1290 (0.1374)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [300/689]  eta: 0:10:13  lr: 0.000097  loss: 0.1270 (0.1373)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [310/689]  eta: 0:09:57  lr: 0.000097  loss: 0.1351 (0.1373)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [320/689]  eta: 0:09:41  lr: 0.000097  loss: 0.1372 (0.1373)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [330/689]  eta: 0:09:26  lr: 0.000097  loss: 0.1324 (0.1372)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [340/689]  eta: 0:09:10  lr: 0.000097  loss: 0.1311 (0.1370)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [350/689]  eta: 0:08:54  lr: 0.000097  loss: 0.1322 (0.1370)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [360/689]  eta: 0:08:38  lr: 0.000097  loss: 0.1361 (0.1370)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [370/689]  eta: 0:08:23  lr: 0.000097  loss: 0.1291 (0.1367)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [380/689]  eta: 0:08:07  lr: 0.000097  loss: 0.1327 (0.1368)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [390/689]  eta: 0:07:51  lr: 0.000097  loss: 0.1391 (0.1367)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [400/689]  eta: 0:07:35  lr: 0.000097  loss: 0.1283 (0.1366)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [410/689]  eta: 0:07:20  lr: 0.000097  loss: 0.1267 (0.1364)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [420/689]  eta: 0:07:04  lr: 0.000097  loss: 0.1351 (0.1365)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [430/689]  eta: 0:06:48  lr: 0.000097  loss: 0.1393 (0.1367)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [440/689]  eta: 0:06:32  lr: 0.000097  loss: 0.1403 (0.1370)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [450/689]  eta: 0:06:16  lr: 0.000097  loss: 0.1423 (0.1368)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [460/689]  eta: 0:06:01  lr: 0.000097  loss: 0.1350 (0.1369)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [470/689]  eta: 0:05:45  lr: 0.000097  loss: 0.1360 (0.1369)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [480/689]  eta: 0:05:29  lr: 0.000097  loss: 0.1385 (0.1370)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [490/689]  eta: 0:05:13  lr: 0.000097  loss: 0.1297 (0.1370)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [500/689]  eta: 0:04:58  lr: 0.000097  loss: 0.1275 (0.1369)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [510/689]  eta: 0:04:42  lr: 0.000097  loss: 0.1355 (0.1371)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [520/689]  eta: 0:04:26  lr: 0.000097  loss: 0.1355 (0.1372)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [530/689]  eta: 0:04:10  lr: 0.000097  loss: 0.1355 (0.1370)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [540/689]  eta: 0:03:55  lr: 0.000097  loss: 0.1269 (0.1369)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [550/689]  eta: 0:03:39  lr: 0.000097  loss: 0.1283 (0.1369)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [560/689]  eta: 0:03:23  lr: 0.000097  loss: 0.1298 (0.1368)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [570/689]  eta: 0:03:07  lr: 0.000097  loss: 0.1327 (0.1368)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [580/689]  eta: 0:02:51  lr: 0.000097  loss: 0.1299 (0.1367)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [590/689]  eta: 0:02:36  lr: 0.000097  loss: 0.1335 (0.1367)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [600/689]  eta: 0:02:20  lr: 0.000097  loss: 0.1300 (0.1367)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [610/689]  eta: 0:02:04  lr: 0.000097  loss: 0.1274 (0.1365)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [620/689]  eta: 0:01:48  lr: 0.000097  loss: 0.1264 (0.1363)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [630/689]  eta: 0:01:33  lr: 0.000097  loss: 0.1292 (0.1363)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [640/689]  eta: 0:01:17  lr: 0.000097  loss: 0.1334 (0.1364)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [650/689]  eta: 0:01:01  lr: 0.000097  loss: 0.1358 (0.1363)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [660/689]  eta: 0:00:45  lr: 0.000097  loss: 0.1309 (0.1362)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [670/689]  eta: 0:00:29  lr: 0.000097  loss: 0.1318 (0.1362)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [680/689]  eta: 0:00:14  lr: 0.000097  loss: 0.1394 (0.1362)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128]  [688/689]  eta: 0:00:01  lr: 0.000097  loss: 0.1394 (0.1362)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:128] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.1394 (0.1362)\n",
      "Valid: [epoch:128]  [ 0/14]  eta: 0:00:14  loss: 0.1332 (0.1332)  time: 1.0172  data: 0.3769  max mem: 39763\n",
      "Valid: [epoch:128]  [13/14]  eta: 0:00:00  loss: 0.1266 (0.1264)  time: 0.1146  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:128] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.1266 (0.1264)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_128_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.126%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:129]  [  0/689]  eta: 0:11:56  lr: 0.000097  loss: 0.1372 (0.1372)  time: 1.0396  data: 0.5636  max mem: 39763\n",
      "Train: [epoch:129]  [ 10/689]  eta: 0:17:15  lr: 0.000097  loss: 0.1410 (0.1412)  time: 1.5256  data: 0.0513  max mem: 39763\n",
      "Train: [epoch:129]  [ 20/689]  eta: 0:17:16  lr: 0.000097  loss: 0.1357 (0.1382)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [ 30/689]  eta: 0:17:06  lr: 0.000097  loss: 0.1383 (0.1422)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [ 40/689]  eta: 0:16:54  lr: 0.000097  loss: 0.1456 (0.1433)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [ 50/689]  eta: 0:16:40  lr: 0.000097  loss: 0.1417 (0.1436)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [ 60/689]  eta: 0:16:25  lr: 0.000097  loss: 0.1360 (0.1417)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [ 70/689]  eta: 0:16:11  lr: 0.000097  loss: 0.1341 (0.1412)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [ 80/689]  eta: 0:15:56  lr: 0.000097  loss: 0.1328 (0.1401)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [ 90/689]  eta: 0:15:40  lr: 0.000097  loss: 0.1250 (0.1391)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [100/689]  eta: 0:15:25  lr: 0.000097  loss: 0.1323 (0.1393)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [110/689]  eta: 0:15:10  lr: 0.000097  loss: 0.1347 (0.1392)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [120/689]  eta: 0:14:54  lr: 0.000097  loss: 0.1375 (0.1395)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [130/689]  eta: 0:14:39  lr: 0.000097  loss: 0.1326 (0.1390)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [140/689]  eta: 0:14:23  lr: 0.000097  loss: 0.1313 (0.1388)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [150/689]  eta: 0:14:07  lr: 0.000097  loss: 0.1326 (0.1390)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [160/689]  eta: 0:13:52  lr: 0.000097  loss: 0.1317 (0.1384)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [170/689]  eta: 0:13:36  lr: 0.000097  loss: 0.1312 (0.1382)  time: 1.5776  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:129]  [180/689]  eta: 0:13:21  lr: 0.000097  loss: 0.1433 (0.1387)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [190/689]  eta: 0:13:05  lr: 0.000097  loss: 0.1433 (0.1388)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [200/689]  eta: 0:12:49  lr: 0.000097  loss: 0.1411 (0.1391)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [210/689]  eta: 0:12:34  lr: 0.000097  loss: 0.1370 (0.1390)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [220/689]  eta: 0:12:18  lr: 0.000097  loss: 0.1282 (0.1386)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [230/689]  eta: 0:12:02  lr: 0.000097  loss: 0.1284 (0.1382)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [240/689]  eta: 0:11:47  lr: 0.000097  loss: 0.1297 (0.1380)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [250/689]  eta: 0:11:31  lr: 0.000097  loss: 0.1297 (0.1379)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [260/689]  eta: 0:11:15  lr: 0.000097  loss: 0.1299 (0.1377)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [270/689]  eta: 0:11:00  lr: 0.000097  loss: 0.1314 (0.1378)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [280/689]  eta: 0:10:44  lr: 0.000097  loss: 0.1377 (0.1379)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [290/689]  eta: 0:10:28  lr: 0.000097  loss: 0.1368 (0.1382)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [300/689]  eta: 0:10:12  lr: 0.000097  loss: 0.1334 (0.1383)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [310/689]  eta: 0:09:57  lr: 0.000097  loss: 0.1356 (0.1382)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [320/689]  eta: 0:09:41  lr: 0.000097  loss: 0.1335 (0.1382)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [330/689]  eta: 0:09:25  lr: 0.000097  loss: 0.1311 (0.1382)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [340/689]  eta: 0:09:09  lr: 0.000097  loss: 0.1359 (0.1382)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [350/689]  eta: 0:08:54  lr: 0.000097  loss: 0.1414 (0.1384)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [360/689]  eta: 0:08:38  lr: 0.000097  loss: 0.1435 (0.1386)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [370/689]  eta: 0:08:22  lr: 0.000097  loss: 0.1391 (0.1384)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [380/689]  eta: 0:08:06  lr: 0.000097  loss: 0.1292 (0.1384)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [390/689]  eta: 0:07:51  lr: 0.000097  loss: 0.1320 (0.1384)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [400/689]  eta: 0:07:35  lr: 0.000097  loss: 0.1396 (0.1387)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [410/689]  eta: 0:07:19  lr: 0.000097  loss: 0.1396 (0.1387)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [420/689]  eta: 0:07:03  lr: 0.000097  loss: 0.1373 (0.1387)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [430/689]  eta: 0:06:48  lr: 0.000097  loss: 0.1311 (0.1386)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [440/689]  eta: 0:06:32  lr: 0.000097  loss: 0.1359 (0.1387)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [450/689]  eta: 0:06:16  lr: 0.000097  loss: 0.1426 (0.1387)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [460/689]  eta: 0:06:00  lr: 0.000097  loss: 0.1317 (0.1388)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [470/689]  eta: 0:05:45  lr: 0.000097  loss: 0.1393 (0.1388)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [480/689]  eta: 0:05:29  lr: 0.000097  loss: 0.1343 (0.1387)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [490/689]  eta: 0:05:13  lr: 0.000097  loss: 0.1306 (0.1386)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [500/689]  eta: 0:04:57  lr: 0.000097  loss: 0.1347 (0.1385)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [510/689]  eta: 0:04:42  lr: 0.000097  loss: 0.1347 (0.1386)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [520/689]  eta: 0:04:26  lr: 0.000097  loss: 0.1317 (0.1385)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [530/689]  eta: 0:04:10  lr: 0.000097  loss: 0.1317 (0.1385)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [540/689]  eta: 0:03:54  lr: 0.000097  loss: 0.1366 (0.1386)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [550/689]  eta: 0:03:39  lr: 0.000097  loss: 0.1416 (0.1386)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [560/689]  eta: 0:03:23  lr: 0.000097  loss: 0.1369 (0.1387)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [570/689]  eta: 0:03:07  lr: 0.000097  loss: 0.1331 (0.1387)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [580/689]  eta: 0:02:51  lr: 0.000097  loss: 0.1331 (0.1386)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [590/689]  eta: 0:02:36  lr: 0.000097  loss: 0.1309 (0.1386)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [600/689]  eta: 0:02:20  lr: 0.000097  loss: 0.1297 (0.1386)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [610/689]  eta: 0:02:04  lr: 0.000097  loss: 0.1302 (0.1385)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [620/689]  eta: 0:01:48  lr: 0.000097  loss: 0.1316 (0.1384)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [630/689]  eta: 0:01:33  lr: 0.000097  loss: 0.1298 (0.1383)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [640/689]  eta: 0:01:17  lr: 0.000097  loss: 0.1297 (0.1382)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [650/689]  eta: 0:01:01  lr: 0.000097  loss: 0.1297 (0.1382)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [660/689]  eta: 0:00:45  lr: 0.000097  loss: 0.1335 (0.1382)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [670/689]  eta: 0:00:29  lr: 0.000097  loss: 0.1291 (0.1382)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [680/689]  eta: 0:00:14  lr: 0.000097  loss: 0.1291 (0.1381)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129]  [688/689]  eta: 0:00:01  lr: 0.000097  loss: 0.1274 (0.1380)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:129] Total time: 0:18:06 (1.5768 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.1274 (0.1380)\n",
      "Valid: [epoch:129]  [ 0/14]  eta: 0:00:14  loss: 0.1218 (0.1218)  time: 1.0201  data: 0.3971  max mem: 39763\n",
      "Valid: [epoch:129]  [13/14]  eta: 0:00:00  loss: 0.1293 (0.1277)  time: 0.1148  data: 0.0284  max mem: 39763\n",
      "Valid: [epoch:129] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.1293 (0.1277)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_129_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.128%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:130]  [  0/689]  eta: 0:11:48  lr: 0.000097  loss: 0.1603 (0.1603)  time: 1.0276  data: 0.5395  max mem: 39763\n",
      "Train: [epoch:130]  [ 10/689]  eta: 0:17:16  lr: 0.000097  loss: 0.1447 (0.1455)  time: 1.5259  data: 0.0491  max mem: 39763\n",
      "Train: [epoch:130]  [ 20/689]  eta: 0:17:17  lr: 0.000097  loss: 0.1334 (0.1378)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [ 30/689]  eta: 0:17:07  lr: 0.000097  loss: 0.1334 (0.1371)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [ 40/689]  eta: 0:16:54  lr: 0.000097  loss: 0.1343 (0.1376)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [ 50/689]  eta: 0:16:40  lr: 0.000097  loss: 0.1273 (0.1354)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [ 60/689]  eta: 0:16:26  lr: 0.000097  loss: 0.1254 (0.1345)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [ 70/689]  eta: 0:16:11  lr: 0.000097  loss: 0.1319 (0.1345)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [ 80/689]  eta: 0:15:56  lr: 0.000097  loss: 0.1347 (0.1352)  time: 1.5767  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:130]  [ 90/689]  eta: 0:15:40  lr: 0.000097  loss: 0.1324 (0.1353)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [100/689]  eta: 0:15:25  lr: 0.000097  loss: 0.1302 (0.1356)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [110/689]  eta: 0:15:10  lr: 0.000097  loss: 0.1329 (0.1360)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [120/689]  eta: 0:14:54  lr: 0.000097  loss: 0.1329 (0.1360)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [130/689]  eta: 0:14:39  lr: 0.000097  loss: 0.1318 (0.1355)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [140/689]  eta: 0:14:23  lr: 0.000097  loss: 0.1334 (0.1364)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [150/689]  eta: 0:14:07  lr: 0.000097  loss: 0.1394 (0.1366)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [160/689]  eta: 0:13:52  lr: 0.000097  loss: 0.1394 (0.1373)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [170/689]  eta: 0:13:36  lr: 0.000097  loss: 0.1451 (0.1377)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [180/689]  eta: 0:13:21  lr: 0.000097  loss: 0.1451 (0.1384)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [190/689]  eta: 0:13:05  lr: 0.000097  loss: 0.1439 (0.1385)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [200/689]  eta: 0:12:49  lr: 0.000097  loss: 0.1333 (0.1382)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [210/689]  eta: 0:12:34  lr: 0.000097  loss: 0.1330 (0.1381)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [220/689]  eta: 0:12:18  lr: 0.000097  loss: 0.1311 (0.1381)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [230/689]  eta: 0:12:02  lr: 0.000097  loss: 0.1311 (0.1384)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [240/689]  eta: 0:11:47  lr: 0.000097  loss: 0.1329 (0.1382)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [250/689]  eta: 0:11:31  lr: 0.000097  loss: 0.1330 (0.1385)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [260/689]  eta: 0:11:15  lr: 0.000097  loss: 0.1398 (0.1387)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [270/689]  eta: 0:10:59  lr: 0.000097  loss: 0.1436 (0.1390)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [280/689]  eta: 0:10:44  lr: 0.000097  loss: 0.1361 (0.1390)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [290/689]  eta: 0:10:28  lr: 0.000097  loss: 0.1373 (0.1389)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [300/689]  eta: 0:10:12  lr: 0.000097  loss: 0.1389 (0.1389)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [310/689]  eta: 0:09:57  lr: 0.000097  loss: 0.1304 (0.1386)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [320/689]  eta: 0:09:41  lr: 0.000097  loss: 0.1260 (0.1383)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [330/689]  eta: 0:09:25  lr: 0.000097  loss: 0.1323 (0.1384)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [340/689]  eta: 0:09:09  lr: 0.000097  loss: 0.1355 (0.1385)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [350/689]  eta: 0:08:54  lr: 0.000097  loss: 0.1338 (0.1384)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [360/689]  eta: 0:08:38  lr: 0.000097  loss: 0.1306 (0.1383)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [370/689]  eta: 0:08:22  lr: 0.000097  loss: 0.1300 (0.1381)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [380/689]  eta: 0:08:06  lr: 0.000097  loss: 0.1274 (0.1380)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [390/689]  eta: 0:07:51  lr: 0.000097  loss: 0.1387 (0.1382)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [400/689]  eta: 0:07:35  lr: 0.000097  loss: 0.1392 (0.1383)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [410/689]  eta: 0:07:19  lr: 0.000097  loss: 0.1380 (0.1383)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [420/689]  eta: 0:07:03  lr: 0.000097  loss: 0.1348 (0.1382)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [430/689]  eta: 0:06:48  lr: 0.000097  loss: 0.1300 (0.1381)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [440/689]  eta: 0:06:32  lr: 0.000097  loss: 0.1358 (0.1380)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [450/689]  eta: 0:06:16  lr: 0.000097  loss: 0.1383 (0.1383)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [460/689]  eta: 0:06:00  lr: 0.000097  loss: 0.1379 (0.1383)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [470/689]  eta: 0:05:45  lr: 0.000097  loss: 0.1368 (0.1383)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [480/689]  eta: 0:05:29  lr: 0.000097  loss: 0.1368 (0.1383)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [490/689]  eta: 0:05:13  lr: 0.000097  loss: 0.1368 (0.1384)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [500/689]  eta: 0:04:57  lr: 0.000097  loss: 0.1391 (0.1385)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [510/689]  eta: 0:04:42  lr: 0.000097  loss: 0.1397 (0.1385)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [520/689]  eta: 0:04:26  lr: 0.000097  loss: 0.1429 (0.1386)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [530/689]  eta: 0:04:10  lr: 0.000097  loss: 0.1429 (0.1387)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [540/689]  eta: 0:03:54  lr: 0.000097  loss: 0.1352 (0.1387)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [550/689]  eta: 0:03:39  lr: 0.000097  loss: 0.1344 (0.1388)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [560/689]  eta: 0:03:23  lr: 0.000097  loss: 0.1410 (0.1389)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [570/689]  eta: 0:03:07  lr: 0.000097  loss: 0.1426 (0.1391)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [580/689]  eta: 0:02:51  lr: 0.000097  loss: 0.1328 (0.1390)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [590/689]  eta: 0:02:36  lr: 0.000097  loss: 0.1300 (0.1390)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [600/689]  eta: 0:02:20  lr: 0.000097  loss: 0.1352 (0.1389)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [610/689]  eta: 0:02:04  lr: 0.000097  loss: 0.1364 (0.1389)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [620/689]  eta: 0:01:48  lr: 0.000097  loss: 0.1364 (0.1389)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [630/689]  eta: 0:01:33  lr: 0.000097  loss: 0.1360 (0.1389)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [640/689]  eta: 0:01:17  lr: 0.000097  loss: 0.1349 (0.1389)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [650/689]  eta: 0:01:01  lr: 0.000097  loss: 0.1373 (0.1389)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [660/689]  eta: 0:00:45  lr: 0.000097  loss: 0.1374 (0.1390)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [670/689]  eta: 0:00:29  lr: 0.000097  loss: 0.1370 (0.1390)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [680/689]  eta: 0:00:14  lr: 0.000097  loss: 0.1370 (0.1391)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130]  [688/689]  eta: 0:00:01  lr: 0.000097  loss: 0.1331 (0.1390)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:130] Total time: 0:18:06 (1.5769 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.1331 (0.1390)\n",
      "Valid: [epoch:130]  [ 0/14]  eta: 0:00:14  loss: 0.1203 (0.1203)  time: 1.0164  data: 0.4516  max mem: 39763\n",
      "Valid: [epoch:130]  [13/14]  eta: 0:00:00  loss: 0.1302 (0.1310)  time: 0.1145  data: 0.0323  max mem: 39763\n",
      "Valid: [epoch:130] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.1302 (0.1310)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_130_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.131%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:131]  [  0/689]  eta: 0:11:52  lr: 0.000097  loss: 0.1206 (0.1206)  time: 1.0343  data: 0.5569  max mem: 39763\n",
      "Train: [epoch:131]  [ 10/689]  eta: 0:17:15  lr: 0.000097  loss: 0.1244 (0.1322)  time: 1.5254  data: 0.0507  max mem: 39763\n",
      "Train: [epoch:131]  [ 20/689]  eta: 0:17:16  lr: 0.000097  loss: 0.1280 (0.1316)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [ 30/689]  eta: 0:17:07  lr: 0.000097  loss: 0.1288 (0.1328)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [ 40/689]  eta: 0:16:54  lr: 0.000097  loss: 0.1302 (0.1330)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [ 50/689]  eta: 0:16:40  lr: 0.000097  loss: 0.1323 (0.1342)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [ 60/689]  eta: 0:16:25  lr: 0.000097  loss: 0.1386 (0.1351)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [ 70/689]  eta: 0:16:11  lr: 0.000097  loss: 0.1430 (0.1364)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [ 80/689]  eta: 0:15:56  lr: 0.000097  loss: 0.1433 (0.1375)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [ 90/689]  eta: 0:15:40  lr: 0.000097  loss: 0.1419 (0.1383)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [100/689]  eta: 0:15:25  lr: 0.000097  loss: 0.1390 (0.1383)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [110/689]  eta: 0:15:10  lr: 0.000097  loss: 0.1378 (0.1385)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [120/689]  eta: 0:14:54  lr: 0.000097  loss: 0.1405 (0.1386)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [130/689]  eta: 0:14:39  lr: 0.000097  loss: 0.1320 (0.1385)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [140/689]  eta: 0:14:23  lr: 0.000097  loss: 0.1427 (0.1405)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [150/689]  eta: 0:14:08  lr: 0.000097  loss: 0.1476 (0.1407)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [160/689]  eta: 0:13:52  lr: 0.000097  loss: 0.1435 (0.1413)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [170/689]  eta: 0:13:36  lr: 0.000097  loss: 0.1410 (0.1419)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [180/689]  eta: 0:13:21  lr: 0.000097  loss: 0.1500 (0.1422)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [190/689]  eta: 0:13:05  lr: 0.000097  loss: 0.1442 (0.1424)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [200/689]  eta: 0:12:49  lr: 0.000097  loss: 0.1354 (0.1420)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [210/689]  eta: 0:12:34  lr: 0.000097  loss: 0.1282 (0.1415)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [220/689]  eta: 0:12:18  lr: 0.000097  loss: 0.1296 (0.1416)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [230/689]  eta: 0:12:02  lr: 0.000097  loss: 0.1349 (0.1412)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [240/689]  eta: 0:11:47  lr: 0.000097  loss: 0.1349 (0.1413)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [250/689]  eta: 0:11:31  lr: 0.000097  loss: 0.1325 (0.1411)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [260/689]  eta: 0:11:15  lr: 0.000097  loss: 0.1304 (0.1406)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [270/689]  eta: 0:10:59  lr: 0.000097  loss: 0.1270 (0.1401)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [280/689]  eta: 0:10:44  lr: 0.000097  loss: 0.1293 (0.1401)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [290/689]  eta: 0:10:28  lr: 0.000097  loss: 0.1293 (0.1398)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [300/689]  eta: 0:10:12  lr: 0.000097  loss: 0.1336 (0.1396)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [310/689]  eta: 0:09:57  lr: 0.000097  loss: 0.1274 (0.1391)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [320/689]  eta: 0:09:41  lr: 0.000097  loss: 0.1255 (0.1393)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [330/689]  eta: 0:09:25  lr: 0.000097  loss: 0.1413 (0.1393)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [340/689]  eta: 0:09:09  lr: 0.000097  loss: 0.1368 (0.1392)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [350/689]  eta: 0:08:54  lr: 0.000097  loss: 0.1394 (0.1394)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [360/689]  eta: 0:08:38  lr: 0.000097  loss: 0.1410 (0.1393)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [370/689]  eta: 0:08:22  lr: 0.000097  loss: 0.1371 (0.1394)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [380/689]  eta: 0:08:06  lr: 0.000097  loss: 0.1426 (0.1395)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [390/689]  eta: 0:07:51  lr: 0.000097  loss: 0.1347 (0.1395)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [400/689]  eta: 0:07:35  lr: 0.000097  loss: 0.1388 (0.1400)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [410/689]  eta: 0:07:19  lr: 0.000097  loss: 0.1544 (0.1402)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [420/689]  eta: 0:07:03  lr: 0.000097  loss: 0.1374 (0.1401)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [430/689]  eta: 0:06:48  lr: 0.000097  loss: 0.1374 (0.1402)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [440/689]  eta: 0:06:32  lr: 0.000097  loss: 0.1427 (0.1403)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [450/689]  eta: 0:06:16  lr: 0.000097  loss: 0.1302 (0.1400)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [460/689]  eta: 0:06:00  lr: 0.000097  loss: 0.1375 (0.1401)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [470/689]  eta: 0:05:45  lr: 0.000097  loss: 0.1381 (0.1399)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [480/689]  eta: 0:05:29  lr: 0.000097  loss: 0.1355 (0.1398)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [490/689]  eta: 0:05:13  lr: 0.000097  loss: 0.1316 (0.1397)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [500/689]  eta: 0:04:57  lr: 0.000097  loss: 0.1310 (0.1396)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [510/689]  eta: 0:04:42  lr: 0.000097  loss: 0.1340 (0.1396)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [520/689]  eta: 0:04:26  lr: 0.000097  loss: 0.1385 (0.1398)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [530/689]  eta: 0:04:10  lr: 0.000097  loss: 0.1337 (0.1397)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [540/689]  eta: 0:03:54  lr: 0.000097  loss: 0.1344 (0.1396)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [550/689]  eta: 0:03:39  lr: 0.000097  loss: 0.1278 (0.1394)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [560/689]  eta: 0:03:23  lr: 0.000097  loss: 0.1295 (0.1395)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [570/689]  eta: 0:03:07  lr: 0.000097  loss: 0.1412 (0.1395)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [580/689]  eta: 0:02:51  lr: 0.000097  loss: 0.1451 (0.1397)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [590/689]  eta: 0:02:36  lr: 0.000097  loss: 0.1463 (0.1399)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [600/689]  eta: 0:02:20  lr: 0.000097  loss: 0.1397 (0.1399)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [610/689]  eta: 0:02:04  lr: 0.000097  loss: 0.1389 (0.1398)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [620/689]  eta: 0:01:48  lr: 0.000097  loss: 0.1421 (0.1403)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [630/689]  eta: 0:01:33  lr: 0.000097  loss: 0.1568 (0.1404)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [640/689]  eta: 0:01:17  lr: 0.000097  loss: 0.1515 (0.1407)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [650/689]  eta: 0:01:01  lr: 0.000097  loss: 0.1412 (0.1408)  time: 1.5788  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:131]  [660/689]  eta: 0:00:45  lr: 0.000097  loss: 0.1379 (0.1408)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [670/689]  eta: 0:00:29  lr: 0.000097  loss: 0.1376 (0.1407)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [680/689]  eta: 0:00:14  lr: 0.000097  loss: 0.1311 (0.1406)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131]  [688/689]  eta: 0:00:01  lr: 0.000097  loss: 0.1333 (0.1406)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:131] Total time: 0:18:06 (1.5772 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.1333 (0.1406)\n",
      "Valid: [epoch:131]  [ 0/14]  eta: 0:00:14  loss: 0.1207 (0.1207)  time: 1.0385  data: 0.3948  max mem: 39763\n",
      "Valid: [epoch:131]  [13/14]  eta: 0:00:00  loss: 0.1286 (0.1300)  time: 0.1161  data: 0.0283  max mem: 39763\n",
      "Valid: [epoch:131] Total time: 0:00:01 (0.1257 s / it)\n",
      "Averaged stats: loss: 0.1286 (0.1300)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_131_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.130%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:132]  [  0/689]  eta: 0:11:33  lr: 0.000097  loss: 0.1485 (0.1485)  time: 1.0066  data: 0.5264  max mem: 39763\n",
      "Train: [epoch:132]  [ 10/689]  eta: 0:17:15  lr: 0.000097  loss: 0.1485 (0.1424)  time: 1.5258  data: 0.0479  max mem: 39763\n",
      "Train: [epoch:132]  [ 20/689]  eta: 0:17:17  lr: 0.000097  loss: 0.1412 (0.1427)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [ 30/689]  eta: 0:17:07  lr: 0.000097  loss: 0.1412 (0.1409)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [ 40/689]  eta: 0:16:55  lr: 0.000097  loss: 0.1334 (0.1378)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [ 50/689]  eta: 0:16:41  lr: 0.000097  loss: 0.1325 (0.1373)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [ 60/689]  eta: 0:16:26  lr: 0.000097  loss: 0.1328 (0.1377)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [ 70/689]  eta: 0:16:12  lr: 0.000097  loss: 0.1353 (0.1376)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [ 80/689]  eta: 0:15:56  lr: 0.000097  loss: 0.1356 (0.1377)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [ 90/689]  eta: 0:15:41  lr: 0.000097  loss: 0.1408 (0.1385)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [100/689]  eta: 0:15:26  lr: 0.000097  loss: 0.1457 (0.1395)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [110/689]  eta: 0:15:10  lr: 0.000097  loss: 0.1491 (0.1399)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [120/689]  eta: 0:14:55  lr: 0.000097  loss: 0.1307 (0.1394)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [130/689]  eta: 0:14:39  lr: 0.000097  loss: 0.1374 (0.1400)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [140/689]  eta: 0:14:24  lr: 0.000097  loss: 0.1405 (0.1396)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [150/689]  eta: 0:14:08  lr: 0.000097  loss: 0.1345 (0.1398)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [160/689]  eta: 0:13:52  lr: 0.000097  loss: 0.1348 (0.1396)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [170/689]  eta: 0:13:37  lr: 0.000097  loss: 0.1342 (0.1392)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [180/689]  eta: 0:13:21  lr: 0.000097  loss: 0.1326 (0.1390)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [190/689]  eta: 0:13:06  lr: 0.000097  loss: 0.1304 (0.1389)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [200/689]  eta: 0:12:50  lr: 0.000097  loss: 0.1260 (0.1384)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [210/689]  eta: 0:12:34  lr: 0.000097  loss: 0.1271 (0.1381)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [220/689]  eta: 0:12:18  lr: 0.000097  loss: 0.1299 (0.1384)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [230/689]  eta: 0:12:03  lr: 0.000097  loss: 0.1353 (0.1384)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [240/689]  eta: 0:11:47  lr: 0.000097  loss: 0.1415 (0.1389)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [250/689]  eta: 0:11:31  lr: 0.000097  loss: 0.1415 (0.1387)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [260/689]  eta: 0:11:16  lr: 0.000097  loss: 0.1360 (0.1386)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [270/689]  eta: 0:11:00  lr: 0.000097  loss: 0.1401 (0.1388)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [280/689]  eta: 0:10:44  lr: 0.000097  loss: 0.1409 (0.1389)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [290/689]  eta: 0:10:28  lr: 0.000097  loss: 0.1318 (0.1386)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [300/689]  eta: 0:10:13  lr: 0.000097  loss: 0.1318 (0.1385)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [310/689]  eta: 0:09:57  lr: 0.000097  loss: 0.1364 (0.1388)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [320/689]  eta: 0:09:41  lr: 0.000097  loss: 0.1364 (0.1387)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [330/689]  eta: 0:09:25  lr: 0.000097  loss: 0.1338 (0.1387)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [340/689]  eta: 0:09:10  lr: 0.000097  loss: 0.1379 (0.1388)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [350/689]  eta: 0:08:54  lr: 0.000097  loss: 0.1379 (0.1388)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [360/689]  eta: 0:08:38  lr: 0.000097  loss: 0.1336 (0.1390)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [370/689]  eta: 0:08:22  lr: 0.000097  loss: 0.1377 (0.1390)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [380/689]  eta: 0:08:07  lr: 0.000097  loss: 0.1369 (0.1390)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [390/689]  eta: 0:07:51  lr: 0.000097  loss: 0.1317 (0.1387)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [400/689]  eta: 0:07:35  lr: 0.000097  loss: 0.1338 (0.1389)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [410/689]  eta: 0:07:19  lr: 0.000097  loss: 0.1538 (0.1393)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [420/689]  eta: 0:07:04  lr: 0.000097  loss: 0.1501 (0.1394)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [430/689]  eta: 0:06:48  lr: 0.000097  loss: 0.1421 (0.1397)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [440/689]  eta: 0:06:32  lr: 0.000097  loss: 0.1433 (0.1399)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [450/689]  eta: 0:06:16  lr: 0.000097  loss: 0.1424 (0.1399)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [460/689]  eta: 0:06:01  lr: 0.000097  loss: 0.1398 (0.1400)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [470/689]  eta: 0:05:45  lr: 0.000097  loss: 0.1402 (0.1402)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [480/689]  eta: 0:05:29  lr: 0.000097  loss: 0.1346 (0.1399)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [490/689]  eta: 0:05:13  lr: 0.000097  loss: 0.1377 (0.1402)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [500/689]  eta: 0:04:58  lr: 0.000097  loss: 0.1449 (0.1402)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [510/689]  eta: 0:04:42  lr: 0.000097  loss: 0.1328 (0.1401)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [520/689]  eta: 0:04:26  lr: 0.000097  loss: 0.1369 (0.1402)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [530/689]  eta: 0:04:10  lr: 0.000097  loss: 0.1396 (0.1401)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [540/689]  eta: 0:03:54  lr: 0.000097  loss: 0.1349 (0.1400)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [550/689]  eta: 0:03:39  lr: 0.000097  loss: 0.1322 (0.1400)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [560/689]  eta: 0:03:23  lr: 0.000097  loss: 0.1401 (0.1401)  time: 1.5795  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:132]  [570/689]  eta: 0:03:07  lr: 0.000097  loss: 0.1432 (0.1401)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [580/689]  eta: 0:02:51  lr: 0.000097  loss: 0.1381 (0.1401)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [590/689]  eta: 0:02:36  lr: 0.000097  loss: 0.1381 (0.1401)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [600/689]  eta: 0:02:20  lr: 0.000097  loss: 0.1364 (0.1401)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [610/689]  eta: 0:02:04  lr: 0.000097  loss: 0.1324 (0.1400)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [620/689]  eta: 0:01:48  lr: 0.000097  loss: 0.1341 (0.1400)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [630/689]  eta: 0:01:33  lr: 0.000097  loss: 0.1410 (0.1401)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [640/689]  eta: 0:01:17  lr: 0.000097  loss: 0.1417 (0.1401)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [650/689]  eta: 0:01:01  lr: 0.000097  loss: 0.1421 (0.1402)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [660/689]  eta: 0:00:45  lr: 0.000097  loss: 0.1401 (0.1402)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [670/689]  eta: 0:00:29  lr: 0.000097  loss: 0.1338 (0.1401)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [680/689]  eta: 0:00:14  lr: 0.000097  loss: 0.1324 (0.1400)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132]  [688/689]  eta: 0:00:01  lr: 0.000097  loss: 0.1330 (0.1399)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:132] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000097  loss: 0.1330 (0.1399)\n",
      "Valid: [epoch:132]  [ 0/14]  eta: 0:00:13  loss: 0.1208 (0.1208)  time: 0.9819  data: 0.4510  max mem: 39763\n",
      "Valid: [epoch:132]  [13/14]  eta: 0:00:00  loss: 0.1305 (0.1306)  time: 0.1123  data: 0.0323  max mem: 39763\n",
      "Valid: [epoch:132] Total time: 0:00:01 (0.1197 s / it)\n",
      "Averaged stats: loss: 0.1305 (0.1306)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_132_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.131%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:133]  [  0/689]  eta: 0:12:01  lr: 0.000096  loss: 0.1464 (0.1464)  time: 1.0469  data: 0.5689  max mem: 39763\n",
      "Train: [epoch:133]  [ 10/689]  eta: 0:17:18  lr: 0.000096  loss: 0.1418 (0.1410)  time: 1.5290  data: 0.0518  max mem: 39763\n",
      "Train: [epoch:133]  [ 20/689]  eta: 0:17:18  lr: 0.000096  loss: 0.1372 (0.1381)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [ 30/689]  eta: 0:17:08  lr: 0.000096  loss: 0.1376 (0.1419)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [ 40/689]  eta: 0:16:55  lr: 0.000096  loss: 0.1358 (0.1396)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [ 50/689]  eta: 0:16:41  lr: 0.000096  loss: 0.1358 (0.1407)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [ 60/689]  eta: 0:16:27  lr: 0.000096  loss: 0.1356 (0.1400)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [ 70/689]  eta: 0:16:12  lr: 0.000096  loss: 0.1343 (0.1410)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [ 80/689]  eta: 0:15:57  lr: 0.000096  loss: 0.1424 (0.1417)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [ 90/689]  eta: 0:15:41  lr: 0.000096  loss: 0.1306 (0.1403)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [100/689]  eta: 0:15:26  lr: 0.000096  loss: 0.1306 (0.1409)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [110/689]  eta: 0:15:11  lr: 0.000096  loss: 0.1390 (0.1413)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [120/689]  eta: 0:14:55  lr: 0.000096  loss: 0.1358 (0.1412)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [130/689]  eta: 0:14:40  lr: 0.000096  loss: 0.1346 (0.1406)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [140/689]  eta: 0:14:24  lr: 0.000096  loss: 0.1356 (0.1408)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [150/689]  eta: 0:14:08  lr: 0.000096  loss: 0.1371 (0.1410)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [160/689]  eta: 0:13:53  lr: 0.000096  loss: 0.1398 (0.1414)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [170/689]  eta: 0:13:37  lr: 0.000096  loss: 0.1427 (0.1415)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [180/689]  eta: 0:13:21  lr: 0.000096  loss: 0.1345 (0.1409)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [190/689]  eta: 0:13:06  lr: 0.000096  loss: 0.1299 (0.1407)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [200/689]  eta: 0:12:50  lr: 0.000096  loss: 0.1324 (0.1404)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [210/689]  eta: 0:12:34  lr: 0.000096  loss: 0.1320 (0.1403)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [220/689]  eta: 0:12:19  lr: 0.000096  loss: 0.1408 (0.1408)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [230/689]  eta: 0:12:03  lr: 0.000096  loss: 0.1370 (0.1405)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [240/689]  eta: 0:11:47  lr: 0.000096  loss: 0.1370 (0.1411)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [250/689]  eta: 0:11:32  lr: 0.000096  loss: 0.1392 (0.1409)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [260/689]  eta: 0:11:16  lr: 0.000096  loss: 0.1368 (0.1412)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [270/689]  eta: 0:11:00  lr: 0.000096  loss: 0.1368 (0.1412)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [280/689]  eta: 0:10:44  lr: 0.000096  loss: 0.1350 (0.1413)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [290/689]  eta: 0:10:29  lr: 0.000096  loss: 0.1411 (0.1415)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [300/689]  eta: 0:10:13  lr: 0.000096  loss: 0.1405 (0.1415)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [310/689]  eta: 0:09:57  lr: 0.000096  loss: 0.1344 (0.1413)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [320/689]  eta: 0:09:41  lr: 0.000096  loss: 0.1345 (0.1415)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [330/689]  eta: 0:09:26  lr: 0.000096  loss: 0.1371 (0.1414)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [340/689]  eta: 0:09:10  lr: 0.000096  loss: 0.1355 (0.1412)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [350/689]  eta: 0:08:54  lr: 0.000096  loss: 0.1341 (0.1410)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [360/689]  eta: 0:08:38  lr: 0.000096  loss: 0.1329 (0.1410)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [370/689]  eta: 0:08:23  lr: 0.000096  loss: 0.1318 (0.1408)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [380/689]  eta: 0:08:07  lr: 0.000096  loss: 0.1318 (0.1407)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [390/689]  eta: 0:07:51  lr: 0.000096  loss: 0.1353 (0.1406)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [400/689]  eta: 0:07:35  lr: 0.000096  loss: 0.1370 (0.1408)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [410/689]  eta: 0:07:20  lr: 0.000096  loss: 0.1416 (0.1409)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [420/689]  eta: 0:07:04  lr: 0.000096  loss: 0.1410 (0.1409)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [430/689]  eta: 0:06:48  lr: 0.000096  loss: 0.1446 (0.1413)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [440/689]  eta: 0:06:32  lr: 0.000096  loss: 0.1521 (0.1416)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [450/689]  eta: 0:06:16  lr: 0.000096  loss: 0.1497 (0.1418)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [460/689]  eta: 0:06:01  lr: 0.000096  loss: 0.1460 (0.1419)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [470/689]  eta: 0:05:45  lr: 0.000096  loss: 0.1421 (0.1419)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:133]  [480/689]  eta: 0:05:29  lr: 0.000096  loss: 0.1343 (0.1417)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [490/689]  eta: 0:05:13  lr: 0.000096  loss: 0.1337 (0.1418)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [500/689]  eta: 0:04:58  lr: 0.000096  loss: 0.1409 (0.1420)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [510/689]  eta: 0:04:42  lr: 0.000096  loss: 0.1468 (0.1421)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [520/689]  eta: 0:04:26  lr: 0.000096  loss: 0.1401 (0.1421)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [530/689]  eta: 0:04:10  lr: 0.000096  loss: 0.1326 (0.1420)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [540/689]  eta: 0:03:55  lr: 0.000096  loss: 0.1353 (0.1420)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [550/689]  eta: 0:03:39  lr: 0.000096  loss: 0.1353 (0.1419)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [560/689]  eta: 0:03:23  lr: 0.000096  loss: 0.1347 (0.1418)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [570/689]  eta: 0:03:07  lr: 0.000096  loss: 0.1402 (0.1419)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [580/689]  eta: 0:02:51  lr: 0.000096  loss: 0.1404 (0.1419)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [590/689]  eta: 0:02:36  lr: 0.000096  loss: 0.1365 (0.1419)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [600/689]  eta: 0:02:20  lr: 0.000096  loss: 0.1381 (0.1419)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [610/689]  eta: 0:02:04  lr: 0.000096  loss: 0.1371 (0.1419)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [620/689]  eta: 0:01:48  lr: 0.000096  loss: 0.1364 (0.1419)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [630/689]  eta: 0:01:33  lr: 0.000096  loss: 0.1393 (0.1418)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [640/689]  eta: 0:01:17  lr: 0.000096  loss: 0.1351 (0.1417)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [650/689]  eta: 0:01:01  lr: 0.000096  loss: 0.1317 (0.1416)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [660/689]  eta: 0:00:45  lr: 0.000096  loss: 0.1367 (0.1417)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [670/689]  eta: 0:00:29  lr: 0.000096  loss: 0.1449 (0.1418)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [680/689]  eta: 0:00:14  lr: 0.000096  loss: 0.1428 (0.1418)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133]  [688/689]  eta: 0:00:01  lr: 0.000096  loss: 0.1375 (0.1418)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:133] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: 0.1375 (0.1418)\n",
      "Valid: [epoch:133]  [ 0/14]  eta: 0:00:14  loss: 0.1321 (0.1321)  time: 1.0073  data: 0.3831  max mem: 39763\n",
      "Valid: [epoch:133]  [13/14]  eta: 0:00:00  loss: 0.1321 (0.1320)  time: 0.1141  data: 0.0274  max mem: 39763\n",
      "Valid: [epoch:133] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.1321 (0.1320)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_133_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.132%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:134]  [  0/689]  eta: 0:11:27  lr: 0.000096  loss: 0.1258 (0.1258)  time: 0.9984  data: 0.5188  max mem: 39763\n",
      "Train: [epoch:134]  [ 10/689]  eta: 0:17:15  lr: 0.000096  loss: 0.1412 (0.1468)  time: 1.5253  data: 0.0472  max mem: 39763\n",
      "Train: [epoch:134]  [ 20/689]  eta: 0:17:17  lr: 0.000096  loss: 0.1401 (0.1417)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [ 30/689]  eta: 0:17:07  lr: 0.000096  loss: 0.1360 (0.1415)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [ 40/689]  eta: 0:16:54  lr: 0.000096  loss: 0.1347 (0.1402)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [ 50/689]  eta: 0:16:41  lr: 0.000096  loss: 0.1350 (0.1403)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [ 60/689]  eta: 0:16:26  lr: 0.000096  loss: 0.1364 (0.1411)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [ 70/689]  eta: 0:16:11  lr: 0.000096  loss: 0.1345 (0.1399)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [ 80/689]  eta: 0:15:56  lr: 0.000096  loss: 0.1336 (0.1394)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [ 90/689]  eta: 0:15:41  lr: 0.000096  loss: 0.1383 (0.1398)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [100/689]  eta: 0:15:26  lr: 0.000096  loss: 0.1396 (0.1409)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [110/689]  eta: 0:15:10  lr: 0.000096  loss: 0.1412 (0.1413)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [120/689]  eta: 0:14:55  lr: 0.000096  loss: 0.1392 (0.1417)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [130/689]  eta: 0:14:39  lr: 0.000096  loss: 0.1385 (0.1418)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [140/689]  eta: 0:14:24  lr: 0.000096  loss: 0.1385 (0.1415)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [150/689]  eta: 0:14:08  lr: 0.000096  loss: 0.1371 (0.1414)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [160/689]  eta: 0:13:52  lr: 0.000096  loss: 0.1411 (0.1418)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [170/689]  eta: 0:13:37  lr: 0.000096  loss: 0.1382 (0.1417)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [180/689]  eta: 0:13:21  lr: 0.000096  loss: 0.1381 (0.1419)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [190/689]  eta: 0:13:05  lr: 0.000096  loss: 0.1408 (0.1421)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [200/689]  eta: 0:12:50  lr: 0.000096  loss: 0.1424 (0.1420)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [210/689]  eta: 0:12:34  lr: 0.000096  loss: 0.1365 (0.1416)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [220/689]  eta: 0:12:18  lr: 0.000096  loss: 0.1336 (0.1416)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [230/689]  eta: 0:12:03  lr: 0.000096  loss: 0.1450 (0.1418)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [240/689]  eta: 0:11:47  lr: 0.000096  loss: 0.1485 (0.1422)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [250/689]  eta: 0:11:31  lr: 0.000096  loss: 0.1404 (0.1418)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [260/689]  eta: 0:11:16  lr: 0.000096  loss: 0.1369 (0.1416)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [270/689]  eta: 0:11:00  lr: 0.000096  loss: 0.1307 (0.1412)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [280/689]  eta: 0:10:44  lr: 0.000096  loss: 0.1304 (0.1413)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [290/689]  eta: 0:10:28  lr: 0.000096  loss: 0.1406 (0.1412)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [300/689]  eta: 0:10:13  lr: 0.000096  loss: 0.1406 (0.1413)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [310/689]  eta: 0:09:57  lr: 0.000096  loss: 0.1414 (0.1412)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [320/689]  eta: 0:09:41  lr: 0.000096  loss: 0.1428 (0.1414)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [330/689]  eta: 0:09:25  lr: 0.000096  loss: 0.1475 (0.1417)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [340/689]  eta: 0:09:10  lr: 0.000096  loss: 0.1482 (0.1421)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [350/689]  eta: 0:08:54  lr: 0.000096  loss: 0.1566 (0.1428)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [360/689]  eta: 0:08:38  lr: 0.000096  loss: 0.1558 (0.1431)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [370/689]  eta: 0:08:22  lr: 0.000096  loss: 0.1417 (0.1432)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [380/689]  eta: 0:08:07  lr: 0.000096  loss: 0.1353 (0.1430)  time: 1.5777  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:134]  [390/689]  eta: 0:07:51  lr: 0.000096  loss: 0.1422 (0.1433)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [400/689]  eta: 0:07:35  lr: 0.000096  loss: 0.1433 (0.1433)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [410/689]  eta: 0:07:19  lr: 0.000096  loss: 0.1401 (0.1432)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [420/689]  eta: 0:07:04  lr: 0.000096  loss: 0.1356 (0.1430)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [430/689]  eta: 0:06:48  lr: 0.000096  loss: 0.1325 (0.1429)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [440/689]  eta: 0:06:32  lr: 0.000096  loss: 0.1287 (0.1426)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [450/689]  eta: 0:06:16  lr: 0.000096  loss: 0.1351 (0.1427)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [460/689]  eta: 0:06:01  lr: 0.000096  loss: 0.1408 (0.1426)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [470/689]  eta: 0:05:45  lr: 0.000096  loss: 0.1384 (0.1426)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [480/689]  eta: 0:05:29  lr: 0.000096  loss: 0.1370 (0.1424)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [490/689]  eta: 0:05:13  lr: 0.000096  loss: 0.1374 (0.1423)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [500/689]  eta: 0:04:58  lr: 0.000096  loss: 0.1377 (0.1423)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [510/689]  eta: 0:04:42  lr: 0.000096  loss: 0.1417 (0.1425)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [520/689]  eta: 0:04:26  lr: 0.000096  loss: 0.1476 (0.1425)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [530/689]  eta: 0:04:10  lr: 0.000096  loss: 0.1412 (0.1425)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [540/689]  eta: 0:03:54  lr: 0.000096  loss: 0.1392 (0.1426)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [550/689]  eta: 0:03:39  lr: 0.000096  loss: 0.1393 (0.1425)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [560/689]  eta: 0:03:23  lr: 0.000096  loss: 0.1425 (0.1427)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [570/689]  eta: 0:03:07  lr: 0.000096  loss: 0.1472 (0.1428)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [580/689]  eta: 0:02:51  lr: 0.000096  loss: 0.1331 (0.1426)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [590/689]  eta: 0:02:36  lr: 0.000096  loss: 0.1332 (0.1426)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [600/689]  eta: 0:02:20  lr: 0.000096  loss: 0.1394 (0.1426)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [610/689]  eta: 0:02:04  lr: 0.000096  loss: 0.1394 (0.1427)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [620/689]  eta: 0:01:48  lr: 0.000096  loss: 0.1399 (0.1427)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [630/689]  eta: 0:01:33  lr: 0.000096  loss: 0.1386 (0.1427)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [640/689]  eta: 0:01:17  lr: 0.000096  loss: 0.1431 (0.1428)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [650/689]  eta: 0:01:01  lr: 0.000096  loss: 0.1363 (0.1427)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [660/689]  eta: 0:00:45  lr: 0.000096  loss: 0.1395 (0.1427)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [670/689]  eta: 0:00:29  lr: 0.000096  loss: 0.1408 (0.1426)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [680/689]  eta: 0:00:14  lr: 0.000096  loss: 0.1401 (0.1427)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134]  [688/689]  eta: 0:00:01  lr: 0.000096  loss: 0.1408 (0.1427)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:134] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: 0.1408 (0.1427)\n",
      "Valid: [epoch:134]  [ 0/14]  eta: 0:00:14  loss: 0.1222 (0.1222)  time: 1.0085  data: 0.3682  max mem: 39763\n",
      "Valid: [epoch:134]  [13/14]  eta: 0:00:00  loss: 0.1319 (0.1333)  time: 0.1140  data: 0.0264  max mem: 39763\n",
      "Valid: [epoch:134] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.1319 (0.1333)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_134_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.133%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:135]  [  0/689]  eta: 0:12:25  lr: 0.000096  loss: 0.1356 (0.1356)  time: 1.0819  data: 0.6053  max mem: 39763\n",
      "Train: [epoch:135]  [ 10/689]  eta: 0:17:19  lr: 0.000096  loss: 0.1474 (0.1492)  time: 1.5307  data: 0.0551  max mem: 39763\n",
      "Train: [epoch:135]  [ 20/689]  eta: 0:17:18  lr: 0.000096  loss: 0.1472 (0.1449)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [ 30/689]  eta: 0:17:08  lr: 0.000096  loss: 0.1470 (0.1470)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [ 40/689]  eta: 0:16:55  lr: 0.000096  loss: 0.1416 (0.1453)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [ 50/689]  eta: 0:16:41  lr: 0.000096  loss: 0.1385 (0.1455)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [ 60/689]  eta: 0:16:27  lr: 0.000096  loss: 0.1381 (0.1443)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [ 70/689]  eta: 0:16:12  lr: 0.000096  loss: 0.1366 (0.1429)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [ 80/689]  eta: 0:15:57  lr: 0.000096  loss: 0.1322 (0.1427)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [ 90/689]  eta: 0:15:42  lr: 0.000096  loss: 0.1409 (0.1430)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [100/689]  eta: 0:15:26  lr: 0.000096  loss: 0.1409 (0.1428)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [110/689]  eta: 0:15:11  lr: 0.000096  loss: 0.1369 (0.1431)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [120/689]  eta: 0:14:55  lr: 0.000096  loss: 0.1396 (0.1430)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [130/689]  eta: 0:14:40  lr: 0.000096  loss: 0.1396 (0.1430)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [140/689]  eta: 0:14:24  lr: 0.000096  loss: 0.1498 (0.1432)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [150/689]  eta: 0:14:09  lr: 0.000096  loss: 0.1421 (0.1431)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [160/689]  eta: 0:13:53  lr: 0.000096  loss: 0.1326 (0.1423)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [170/689]  eta: 0:13:37  lr: 0.000096  loss: 0.1316 (0.1417)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [180/689]  eta: 0:13:22  lr: 0.000096  loss: 0.1351 (0.1417)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [190/689]  eta: 0:13:06  lr: 0.000096  loss: 0.1388 (0.1419)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [200/689]  eta: 0:12:50  lr: 0.000096  loss: 0.1363 (0.1417)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [210/689]  eta: 0:12:35  lr: 0.000096  loss: 0.1336 (0.1414)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [220/689]  eta: 0:12:19  lr: 0.000096  loss: 0.1345 (0.1416)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [230/689]  eta: 0:12:03  lr: 0.000096  loss: 0.1373 (0.1413)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [240/689]  eta: 0:11:47  lr: 0.000096  loss: 0.1383 (0.1416)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [250/689]  eta: 0:11:32  lr: 0.000096  loss: 0.1380 (0.1418)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [260/689]  eta: 0:11:16  lr: 0.000096  loss: 0.1340 (0.1415)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [270/689]  eta: 0:11:00  lr: 0.000096  loss: 0.1290 (0.1414)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [280/689]  eta: 0:10:44  lr: 0.000096  loss: 0.1317 (0.1412)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [290/689]  eta: 0:10:29  lr: 0.000096  loss: 0.1371 (0.1413)  time: 1.5793  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:135]  [300/689]  eta: 0:10:13  lr: 0.000096  loss: 0.1398 (0.1412)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [310/689]  eta: 0:09:57  lr: 0.000096  loss: 0.1346 (0.1411)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [320/689]  eta: 0:09:41  lr: 0.000096  loss: 0.1331 (0.1408)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [330/689]  eta: 0:09:26  lr: 0.000096  loss: 0.1338 (0.1410)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [340/689]  eta: 0:09:10  lr: 0.000096  loss: 0.1442 (0.1413)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [350/689]  eta: 0:08:54  lr: 0.000096  loss: 0.1460 (0.1415)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [360/689]  eta: 0:08:38  lr: 0.000096  loss: 0.1491 (0.1418)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [370/689]  eta: 0:08:23  lr: 0.000096  loss: 0.1470 (0.1418)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [380/689]  eta: 0:08:07  lr: 0.000096  loss: 0.1373 (0.1419)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [390/689]  eta: 0:07:51  lr: 0.000096  loss: 0.1495 (0.1426)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [400/689]  eta: 0:07:35  lr: 0.000096  loss: 0.1595 (0.1430)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [410/689]  eta: 0:07:20  lr: 0.000096  loss: 0.1612 (0.1434)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [420/689]  eta: 0:07:04  lr: 0.000096  loss: 0.1536 (0.1434)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [430/689]  eta: 0:06:48  lr: 0.000096  loss: 0.1380 (0.1436)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [440/689]  eta: 0:06:32  lr: 0.000096  loss: 0.1437 (0.1437)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [450/689]  eta: 0:06:16  lr: 0.000096  loss: 0.1482 (0.1438)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [460/689]  eta: 0:06:01  lr: 0.000096  loss: 0.1487 (0.1440)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [470/689]  eta: 0:05:45  lr: 0.000096  loss: 0.1470 (0.1441)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [480/689]  eta: 0:05:29  lr: 0.000096  loss: 0.1424 (0.1443)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [490/689]  eta: 0:05:13  lr: 0.000096  loss: 0.1424 (0.1443)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [500/689]  eta: 0:04:58  lr: 0.000096  loss: 0.1413 (0.1443)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [510/689]  eta: 0:04:42  lr: 0.000096  loss: 0.1405 (0.1442)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [520/689]  eta: 0:04:26  lr: 0.000096  loss: 0.1324 (0.1441)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [530/689]  eta: 0:04:10  lr: 0.000096  loss: 0.1459 (0.1442)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [540/689]  eta: 0:03:55  lr: 0.000096  loss: 0.1482 (0.1441)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [550/689]  eta: 0:03:39  lr: 0.000096  loss: 0.1406 (0.1442)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [560/689]  eta: 0:03:23  lr: 0.000096  loss: 0.1454 (0.1442)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [570/689]  eta: 0:03:07  lr: 0.000096  loss: 0.1376 (0.1440)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [580/689]  eta: 0:02:51  lr: 0.000096  loss: 0.1372 (0.1441)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [590/689]  eta: 0:02:36  lr: 0.000096  loss: 0.1378 (0.1442)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [600/689]  eta: 0:02:20  lr: 0.000096  loss: 0.1421 (0.1442)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [610/689]  eta: 0:02:04  lr: 0.000096  loss: 0.1421 (0.1443)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [620/689]  eta: 0:01:48  lr: 0.000096  loss: 0.1408 (0.1444)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [630/689]  eta: 0:01:33  lr: 0.000096  loss: 0.1425 (0.1444)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [640/689]  eta: 0:01:17  lr: 0.000096  loss: 0.1425 (0.1444)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [650/689]  eta: 0:01:01  lr: 0.000096  loss: 0.1344 (0.1442)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [660/689]  eta: 0:00:45  lr: 0.000096  loss: 0.1350 (0.1441)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [670/689]  eta: 0:00:29  lr: 0.000096  loss: 0.1392 (0.1440)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [680/689]  eta: 0:00:14  lr: 0.000096  loss: 0.1320 (0.1439)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135]  [688/689]  eta: 0:00:01  lr: 0.000096  loss: 0.1339 (0.1438)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:135] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: 0.1339 (0.1438)\n",
      "Valid: [epoch:135]  [ 0/14]  eta: 0:00:14  loss: 0.1192 (0.1192)  time: 1.0055  data: 0.3852  max mem: 39763\n",
      "Valid: [epoch:135]  [13/14]  eta: 0:00:00  loss: 0.1323 (0.1344)  time: 0.1138  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:135] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.1323 (0.1344)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_135_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.134%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:136]  [  0/689]  eta: 0:11:56  lr: 0.000096  loss: 0.1478 (0.1478)  time: 1.0403  data: 0.5598  max mem: 39763\n",
      "Train: [epoch:136]  [ 10/689]  eta: 0:17:18  lr: 0.000096  loss: 0.1370 (0.1390)  time: 1.5291  data: 0.0510  max mem: 39763\n",
      "Train: [epoch:136]  [ 20/689]  eta: 0:17:18  lr: 0.000096  loss: 0.1370 (0.1435)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [ 30/689]  eta: 0:17:08  lr: 0.000096  loss: 0.1463 (0.1444)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [ 40/689]  eta: 0:16:55  lr: 0.000096  loss: 0.1344 (0.1423)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [ 50/689]  eta: 0:16:41  lr: 0.000096  loss: 0.1334 (0.1413)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [ 60/689]  eta: 0:16:27  lr: 0.000096  loss: 0.1360 (0.1416)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [ 70/689]  eta: 0:16:12  lr: 0.000096  loss: 0.1410 (0.1412)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [ 80/689]  eta: 0:15:57  lr: 0.000096  loss: 0.1395 (0.1414)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [ 90/689]  eta: 0:15:41  lr: 0.000096  loss: 0.1378 (0.1419)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [100/689]  eta: 0:15:26  lr: 0.000096  loss: 0.1512 (0.1432)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [110/689]  eta: 0:15:11  lr: 0.000096  loss: 0.1512 (0.1435)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [120/689]  eta: 0:14:55  lr: 0.000096  loss: 0.1402 (0.1432)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [130/689]  eta: 0:14:40  lr: 0.000096  loss: 0.1373 (0.1430)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [140/689]  eta: 0:14:24  lr: 0.000096  loss: 0.1446 (0.1438)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [150/689]  eta: 0:14:08  lr: 0.000096  loss: 0.1414 (0.1433)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [160/689]  eta: 0:13:53  lr: 0.000096  loss: 0.1359 (0.1432)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [170/689]  eta: 0:13:37  lr: 0.000096  loss: 0.1481 (0.1439)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [180/689]  eta: 0:13:21  lr: 0.000096  loss: 0.1488 (0.1437)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [190/689]  eta: 0:13:06  lr: 0.000096  loss: 0.1461 (0.1441)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [200/689]  eta: 0:12:50  lr: 0.000096  loss: 0.1464 (0.1441)  time: 1.5784  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:136]  [210/689]  eta: 0:12:34  lr: 0.000096  loss: 0.1464 (0.1445)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [220/689]  eta: 0:12:19  lr: 0.000096  loss: 0.1399 (0.1444)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [230/689]  eta: 0:12:03  lr: 0.000096  loss: 0.1476 (0.1448)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [240/689]  eta: 0:11:47  lr: 0.000096  loss: 0.1495 (0.1451)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [250/689]  eta: 0:11:32  lr: 0.000096  loss: 0.1480 (0.1452)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [260/689]  eta: 0:11:16  lr: 0.000096  loss: 0.1472 (0.1455)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [270/689]  eta: 0:11:00  lr: 0.000096  loss: 0.1397 (0.1452)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [280/689]  eta: 0:10:44  lr: 0.000096  loss: 0.1386 (0.1454)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [290/689]  eta: 0:10:29  lr: 0.000096  loss: 0.1424 (0.1453)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [300/689]  eta: 0:10:13  lr: 0.000096  loss: 0.1331 (0.1450)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [310/689]  eta: 0:09:57  lr: 0.000096  loss: 0.1314 (0.1446)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [320/689]  eta: 0:09:41  lr: 0.000096  loss: 0.1361 (0.1446)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [330/689]  eta: 0:09:26  lr: 0.000096  loss: 0.1361 (0.1443)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [340/689]  eta: 0:09:10  lr: 0.000096  loss: 0.1343 (0.1444)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [350/689]  eta: 0:08:54  lr: 0.000096  loss: 0.1460 (0.1445)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [360/689]  eta: 0:08:38  lr: 0.000096  loss: 0.1388 (0.1443)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [370/689]  eta: 0:08:23  lr: 0.000096  loss: 0.1394 (0.1443)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [380/689]  eta: 0:08:07  lr: 0.000096  loss: 0.1436 (0.1444)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [390/689]  eta: 0:07:51  lr: 0.000096  loss: 0.1443 (0.1447)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [400/689]  eta: 0:07:35  lr: 0.000096  loss: 0.1419 (0.1447)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [410/689]  eta: 0:07:20  lr: 0.000096  loss: 0.1393 (0.1445)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [420/689]  eta: 0:07:04  lr: 0.000096  loss: 0.1304 (0.1444)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [430/689]  eta: 0:06:48  lr: 0.000096  loss: 0.1389 (0.1444)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [440/689]  eta: 0:06:32  lr: 0.000096  loss: 0.1395 (0.1443)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [450/689]  eta: 0:06:16  lr: 0.000096  loss: 0.1375 (0.1442)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [460/689]  eta: 0:06:01  lr: 0.000096  loss: 0.1328 (0.1440)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [470/689]  eta: 0:05:45  lr: 0.000096  loss: 0.1368 (0.1438)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [480/689]  eta: 0:05:29  lr: 0.000096  loss: 0.1326 (0.1437)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [490/689]  eta: 0:05:13  lr: 0.000096  loss: 0.1326 (0.1437)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [500/689]  eta: 0:04:58  lr: 0.000096  loss: 0.1415 (0.1437)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [510/689]  eta: 0:04:42  lr: 0.000096  loss: 0.1415 (0.1438)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [520/689]  eta: 0:04:26  lr: 0.000096  loss: 0.1374 (0.1438)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [530/689]  eta: 0:04:10  lr: 0.000096  loss: 0.1387 (0.1438)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [540/689]  eta: 0:03:55  lr: 0.000096  loss: 0.1406 (0.1440)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [550/689]  eta: 0:03:39  lr: 0.000096  loss: 0.1432 (0.1439)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [560/689]  eta: 0:03:23  lr: 0.000096  loss: 0.1432 (0.1440)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [570/689]  eta: 0:03:07  lr: 0.000096  loss: 0.1462 (0.1441)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [580/689]  eta: 0:02:51  lr: 0.000096  loss: 0.1446 (0.1442)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [590/689]  eta: 0:02:36  lr: 0.000096  loss: 0.1446 (0.1442)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [600/689]  eta: 0:02:20  lr: 0.000096  loss: 0.1412 (0.1441)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [610/689]  eta: 0:02:04  lr: 0.000096  loss: 0.1329 (0.1440)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [620/689]  eta: 0:01:48  lr: 0.000096  loss: 0.1355 (0.1440)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [630/689]  eta: 0:01:33  lr: 0.000096  loss: 0.1410 (0.1439)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [640/689]  eta: 0:01:17  lr: 0.000096  loss: 0.1417 (0.1440)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [650/689]  eta: 0:01:01  lr: 0.000096  loss: 0.1404 (0.1440)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [660/689]  eta: 0:00:45  lr: 0.000096  loss: 0.1444 (0.1440)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [670/689]  eta: 0:00:29  lr: 0.000096  loss: 0.1444 (0.1440)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [680/689]  eta: 0:00:14  lr: 0.000096  loss: 0.1419 (0.1440)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136]  [688/689]  eta: 0:00:01  lr: 0.000096  loss: 0.1462 (0.1440)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:136] Total time: 0:18:07 (1.5782 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: 0.1462 (0.1440)\n",
      "Valid: [epoch:136]  [ 0/14]  eta: 0:00:14  loss: 0.1330 (0.1330)  time: 1.0095  data: 0.3762  max mem: 39763\n",
      "Valid: [epoch:136]  [13/14]  eta: 0:00:00  loss: 0.1356 (0.1377)  time: 0.1141  data: 0.0269  max mem: 39763\n",
      "Valid: [epoch:136] Total time: 0:00:01 (0.1222 s / it)\n",
      "Averaged stats: loss: 0.1356 (0.1377)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_136_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.138%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:137]  [  0/689]  eta: 0:12:11  lr: 0.000096  loss: 0.1508 (0.1508)  time: 1.0619  data: 0.5861  max mem: 39763\n",
      "Train: [epoch:137]  [ 10/689]  eta: 0:17:17  lr: 0.000096  loss: 0.1646 (0.1646)  time: 1.5284  data: 0.0534  max mem: 39763\n",
      "Train: [epoch:137]  [ 20/689]  eta: 0:17:18  lr: 0.000096  loss: 0.1695 (0.1750)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [ 30/689]  eta: 0:17:08  lr: 0.000096  loss: 0.1689 (0.1713)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [ 40/689]  eta: 0:16:55  lr: 0.000096  loss: 0.1515 (0.1656)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [ 50/689]  eta: 0:16:41  lr: 0.000096  loss: 0.1429 (0.1629)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [ 60/689]  eta: 0:16:26  lr: 0.000096  loss: 0.1545 (0.1615)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [ 70/689]  eta: 0:16:11  lr: 0.000096  loss: 0.1545 (0.1598)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [ 80/689]  eta: 0:15:56  lr: 0.000096  loss: 0.1405 (0.1576)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [ 90/689]  eta: 0:15:41  lr: 0.000096  loss: 0.1405 (0.1565)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [100/689]  eta: 0:15:26  lr: 0.000096  loss: 0.1398 (0.1550)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [110/689]  eta: 0:15:10  lr: 0.000096  loss: 0.1371 (0.1533)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:137]  [120/689]  eta: 0:14:55  lr: 0.000096  loss: 0.1350 (0.1514)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [130/689]  eta: 0:14:39  lr: 0.000096  loss: 0.1350 (0.1511)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [140/689]  eta: 0:14:24  lr: 0.000096  loss: 0.1449 (0.1509)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [150/689]  eta: 0:14:08  lr: 0.000096  loss: 0.1426 (0.1504)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [160/689]  eta: 0:13:53  lr: 0.000096  loss: 0.1426 (0.1508)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [170/689]  eta: 0:13:37  lr: 0.000096  loss: 0.1428 (0.1507)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [180/689]  eta: 0:13:21  lr: 0.000096  loss: 0.1421 (0.1507)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [190/689]  eta: 0:13:06  lr: 0.000096  loss: 0.1434 (0.1505)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [200/689]  eta: 0:12:50  lr: 0.000096  loss: 0.1372 (0.1499)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [210/689]  eta: 0:12:34  lr: 0.000096  loss: 0.1336 (0.1491)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [220/689]  eta: 0:12:19  lr: 0.000096  loss: 0.1332 (0.1488)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [230/689]  eta: 0:12:03  lr: 0.000096  loss: 0.1501 (0.1490)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [240/689]  eta: 0:11:47  lr: 0.000096  loss: 0.1527 (0.1492)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [250/689]  eta: 0:11:31  lr: 0.000096  loss: 0.1441 (0.1490)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [260/689]  eta: 0:11:16  lr: 0.000096  loss: 0.1414 (0.1491)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [270/689]  eta: 0:11:00  lr: 0.000096  loss: 0.1426 (0.1488)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [280/689]  eta: 0:10:44  lr: 0.000096  loss: 0.1419 (0.1486)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [290/689]  eta: 0:10:29  lr: 0.000096  loss: 0.1412 (0.1484)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [300/689]  eta: 0:10:13  lr: 0.000096  loss: 0.1412 (0.1481)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [310/689]  eta: 0:09:57  lr: 0.000096  loss: 0.1419 (0.1481)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [320/689]  eta: 0:09:41  lr: 0.000096  loss: 0.1419 (0.1478)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [330/689]  eta: 0:09:26  lr: 0.000096  loss: 0.1389 (0.1475)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [340/689]  eta: 0:09:10  lr: 0.000096  loss: 0.1375 (0.1475)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [350/689]  eta: 0:08:54  lr: 0.000096  loss: 0.1375 (0.1472)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [360/689]  eta: 0:08:38  lr: 0.000096  loss: 0.1399 (0.1475)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [370/689]  eta: 0:08:23  lr: 0.000096  loss: 0.1482 (0.1475)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [380/689]  eta: 0:08:07  lr: 0.000096  loss: 0.1403 (0.1474)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [390/689]  eta: 0:07:51  lr: 0.000096  loss: 0.1415 (0.1474)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [400/689]  eta: 0:07:35  lr: 0.000096  loss: 0.1415 (0.1473)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [410/689]  eta: 0:07:19  lr: 0.000096  loss: 0.1412 (0.1473)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [420/689]  eta: 0:07:04  lr: 0.000096  loss: 0.1392 (0.1470)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [430/689]  eta: 0:06:48  lr: 0.000096  loss: 0.1390 (0.1470)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [440/689]  eta: 0:06:32  lr: 0.000096  loss: 0.1471 (0.1472)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [450/689]  eta: 0:06:16  lr: 0.000096  loss: 0.1418 (0.1470)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [460/689]  eta: 0:06:01  lr: 0.000096  loss: 0.1429 (0.1472)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [470/689]  eta: 0:05:45  lr: 0.000096  loss: 0.1399 (0.1470)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [480/689]  eta: 0:05:29  lr: 0.000096  loss: 0.1377 (0.1469)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [490/689]  eta: 0:05:13  lr: 0.000096  loss: 0.1386 (0.1467)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [500/689]  eta: 0:04:58  lr: 0.000096  loss: 0.1366 (0.1466)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [510/689]  eta: 0:04:42  lr: 0.000096  loss: 0.1393 (0.1467)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [520/689]  eta: 0:04:26  lr: 0.000096  loss: 0.1509 (0.1469)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [530/689]  eta: 0:04:10  lr: 0.000096  loss: 0.1504 (0.1468)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [540/689]  eta: 0:03:55  lr: 0.000096  loss: 0.1415 (0.1467)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [550/689]  eta: 0:03:39  lr: 0.000096  loss: 0.1380 (0.1468)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [560/689]  eta: 0:03:23  lr: 0.000096  loss: 0.1416 (0.1467)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [570/689]  eta: 0:03:07  lr: 0.000096  loss: 0.1357 (0.1466)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [580/689]  eta: 0:02:51  lr: 0.000096  loss: 0.1385 (0.1466)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [590/689]  eta: 0:02:36  lr: 0.000096  loss: 0.1404 (0.1464)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [600/689]  eta: 0:02:20  lr: 0.000096  loss: 0.1404 (0.1464)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [610/689]  eta: 0:02:04  lr: 0.000096  loss: 0.1400 (0.1462)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [620/689]  eta: 0:01:48  lr: 0.000096  loss: 0.1366 (0.1461)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [630/689]  eta: 0:01:33  lr: 0.000096  loss: 0.1358 (0.1460)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [640/689]  eta: 0:01:17  lr: 0.000096  loss: 0.1337 (0.1459)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [650/689]  eta: 0:01:01  lr: 0.000096  loss: 0.1356 (0.1460)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [660/689]  eta: 0:00:45  lr: 0.000096  loss: 0.1451 (0.1461)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [670/689]  eta: 0:00:29  lr: 0.000096  loss: 0.1392 (0.1459)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [680/689]  eta: 0:00:14  lr: 0.000096  loss: 0.1358 (0.1458)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137]  [688/689]  eta: 0:00:01  lr: 0.000096  loss: 0.1346 (0.1458)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:137] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: 0.1346 (0.1458)\n",
      "Valid: [epoch:137]  [ 0/14]  eta: 0:00:14  loss: 0.1298 (0.1298)  time: 1.0235  data: 0.4099  max mem: 39763\n",
      "Valid: [epoch:137]  [13/14]  eta: 0:00:00  loss: 0.1334 (0.1357)  time: 0.1150  data: 0.0293  max mem: 39763\n",
      "Valid: [epoch:137] Total time: 0:00:01 (0.1244 s / it)\n",
      "Averaged stats: loss: 0.1334 (0.1357)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_137_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.136%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:138]  [  0/689]  eta: 0:12:03  lr: 0.000096  loss: 0.1450 (0.1450)  time: 1.0501  data: 0.5724  max mem: 39763\n",
      "Train: [epoch:138]  [ 10/689]  eta: 0:17:17  lr: 0.000096  loss: 0.1450 (0.1450)  time: 1.5282  data: 0.0521  max mem: 39763\n",
      "Train: [epoch:138]  [ 20/689]  eta: 0:17:17  lr: 0.000096  loss: 0.1430 (0.1423)  time: 1.5766  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:138]  [ 30/689]  eta: 0:17:07  lr: 0.000096  loss: 0.1406 (0.1438)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [ 40/689]  eta: 0:16:54  lr: 0.000096  loss: 0.1397 (0.1417)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [ 50/689]  eta: 0:16:40  lr: 0.000096  loss: 0.1375 (0.1433)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [ 60/689]  eta: 0:16:26  lr: 0.000096  loss: 0.1407 (0.1432)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [ 70/689]  eta: 0:16:11  lr: 0.000096  loss: 0.1376 (0.1434)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [ 80/689]  eta: 0:15:56  lr: 0.000096  loss: 0.1391 (0.1435)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [ 90/689]  eta: 0:15:41  lr: 0.000096  loss: 0.1400 (0.1429)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [100/689]  eta: 0:15:25  lr: 0.000096  loss: 0.1400 (0.1436)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [110/689]  eta: 0:15:10  lr: 0.000096  loss: 0.1442 (0.1448)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [120/689]  eta: 0:14:55  lr: 0.000096  loss: 0.1443 (0.1449)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [130/689]  eta: 0:14:39  lr: 0.000096  loss: 0.1498 (0.1459)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [140/689]  eta: 0:14:24  lr: 0.000096  loss: 0.1533 (0.1463)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [150/689]  eta: 0:14:08  lr: 0.000096  loss: 0.1516 (0.1467)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [160/689]  eta: 0:13:52  lr: 0.000096  loss: 0.1372 (0.1460)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [170/689]  eta: 0:13:37  lr: 0.000096  loss: 0.1352 (0.1458)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [180/689]  eta: 0:13:21  lr: 0.000096  loss: 0.1374 (0.1454)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [190/689]  eta: 0:13:05  lr: 0.000096  loss: 0.1418 (0.1454)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [200/689]  eta: 0:12:50  lr: 0.000096  loss: 0.1449 (0.1454)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [210/689]  eta: 0:12:34  lr: 0.000096  loss: 0.1379 (0.1451)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [220/689]  eta: 0:12:18  lr: 0.000096  loss: 0.1389 (0.1457)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [230/689]  eta: 0:12:03  lr: 0.000096  loss: 0.1516 (0.1464)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [240/689]  eta: 0:11:47  lr: 0.000096  loss: 0.1592 (0.1472)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [250/689]  eta: 0:11:31  lr: 0.000096  loss: 0.1504 (0.1473)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [260/689]  eta: 0:11:16  lr: 0.000096  loss: 0.1382 (0.1470)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [270/689]  eta: 0:11:00  lr: 0.000096  loss: 0.1357 (0.1467)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [280/689]  eta: 0:10:44  lr: 0.000096  loss: 0.1387 (0.1467)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [290/689]  eta: 0:10:28  lr: 0.000096  loss: 0.1425 (0.1465)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [300/689]  eta: 0:10:13  lr: 0.000096  loss: 0.1480 (0.1466)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [310/689]  eta: 0:09:57  lr: 0.000096  loss: 0.1489 (0.1466)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [320/689]  eta: 0:09:41  lr: 0.000096  loss: 0.1429 (0.1467)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [330/689]  eta: 0:09:25  lr: 0.000096  loss: 0.1516 (0.1472)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [340/689]  eta: 0:09:10  lr: 0.000096  loss: 0.1585 (0.1475)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [350/689]  eta: 0:08:54  lr: 0.000096  loss: 0.1523 (0.1475)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [360/689]  eta: 0:08:38  lr: 0.000096  loss: 0.1523 (0.1478)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [370/689]  eta: 0:08:22  lr: 0.000096  loss: 0.1445 (0.1476)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [380/689]  eta: 0:08:07  lr: 0.000096  loss: 0.1395 (0.1475)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [390/689]  eta: 0:07:51  lr: 0.000096  loss: 0.1417 (0.1475)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [400/689]  eta: 0:07:35  lr: 0.000096  loss: 0.1544 (0.1478)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [410/689]  eta: 0:07:19  lr: 0.000096  loss: 0.1504 (0.1476)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [420/689]  eta: 0:07:04  lr: 0.000096  loss: 0.1424 (0.1474)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [430/689]  eta: 0:06:48  lr: 0.000096  loss: 0.1416 (0.1472)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [440/689]  eta: 0:06:32  lr: 0.000096  loss: 0.1381 (0.1472)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [450/689]  eta: 0:06:16  lr: 0.000096  loss: 0.1380 (0.1471)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [460/689]  eta: 0:06:01  lr: 0.000096  loss: 0.1430 (0.1474)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [470/689]  eta: 0:05:45  lr: 0.000096  loss: 0.1556 (0.1475)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [480/689]  eta: 0:05:29  lr: 0.000096  loss: 0.1407 (0.1473)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [490/689]  eta: 0:05:13  lr: 0.000096  loss: 0.1324 (0.1472)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [500/689]  eta: 0:04:58  lr: 0.000096  loss: 0.1307 (0.1469)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [510/689]  eta: 0:04:42  lr: 0.000096  loss: 0.1344 (0.1468)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [520/689]  eta: 0:04:26  lr: 0.000096  loss: 0.1395 (0.1468)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [530/689]  eta: 0:04:10  lr: 0.000096  loss: 0.1334 (0.1466)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [540/689]  eta: 0:03:54  lr: 0.000096  loss: 0.1358 (0.1465)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [550/689]  eta: 0:03:39  lr: 0.000096  loss: 0.1372 (0.1463)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [560/689]  eta: 0:03:23  lr: 0.000096  loss: 0.1402 (0.1463)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [570/689]  eta: 0:03:07  lr: 0.000096  loss: 0.1401 (0.1462)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [580/689]  eta: 0:02:51  lr: 0.000096  loss: 0.1409 (0.1463)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [590/689]  eta: 0:02:36  lr: 0.000096  loss: 0.1458 (0.1463)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [600/689]  eta: 0:02:20  lr: 0.000096  loss: 0.1372 (0.1461)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [610/689]  eta: 0:02:04  lr: 0.000096  loss: 0.1360 (0.1460)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [620/689]  eta: 0:01:48  lr: 0.000096  loss: 0.1430 (0.1459)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [630/689]  eta: 0:01:33  lr: 0.000096  loss: 0.1438 (0.1460)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [640/689]  eta: 0:01:17  lr: 0.000096  loss: 0.1398 (0.1460)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [650/689]  eta: 0:01:01  lr: 0.000096  loss: 0.1421 (0.1461)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [660/689]  eta: 0:00:45  lr: 0.000096  loss: 0.1421 (0.1459)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [670/689]  eta: 0:00:29  lr: 0.000096  loss: 0.1393 (0.1459)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138]  [680/689]  eta: 0:00:14  lr: 0.000096  loss: 0.1383 (0.1458)  time: 1.5781  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:138]  [688/689]  eta: 0:00:01  lr: 0.000096  loss: 0.1349 (0.1458)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:138] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: 0.1349 (0.1458)\n",
      "Valid: [epoch:138]  [ 0/14]  eta: 0:00:14  loss: 0.1514 (0.1514)  time: 1.0118  data: 0.3802  max mem: 39763\n",
      "Valid: [epoch:138]  [13/14]  eta: 0:00:00  loss: 0.1355 (0.1382)  time: 0.1142  data: 0.0272  max mem: 39763\n",
      "Valid: [epoch:138] Total time: 0:00:01 (0.1223 s / it)\n",
      "Averaged stats: loss: 0.1355 (0.1382)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_138_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.138%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:139]  [  0/689]  eta: 0:12:11  lr: 0.000096  loss: 0.1690 (0.1690)  time: 1.0623  data: 0.5857  max mem: 39763\n",
      "Train: [epoch:139]  [ 10/689]  eta: 0:17:17  lr: 0.000096  loss: 0.1366 (0.1397)  time: 1.5283  data: 0.0533  max mem: 39763\n",
      "Train: [epoch:139]  [ 20/689]  eta: 0:17:17  lr: 0.000096  loss: 0.1366 (0.1437)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [ 30/689]  eta: 0:17:07  lr: 0.000096  loss: 0.1427 (0.1447)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [ 40/689]  eta: 0:16:54  lr: 0.000096  loss: 0.1379 (0.1429)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [ 50/689]  eta: 0:16:40  lr: 0.000096  loss: 0.1395 (0.1442)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [ 60/689]  eta: 0:16:26  lr: 0.000096  loss: 0.1338 (0.1430)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [ 70/689]  eta: 0:16:11  lr: 0.000096  loss: 0.1322 (0.1423)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [ 80/689]  eta: 0:15:56  lr: 0.000096  loss: 0.1341 (0.1421)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [ 90/689]  eta: 0:15:41  lr: 0.000096  loss: 0.1371 (0.1428)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [100/689]  eta: 0:15:25  lr: 0.000096  loss: 0.1478 (0.1434)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [110/689]  eta: 0:15:10  lr: 0.000096  loss: 0.1411 (0.1436)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [120/689]  eta: 0:14:55  lr: 0.000096  loss: 0.1423 (0.1433)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [130/689]  eta: 0:14:39  lr: 0.000096  loss: 0.1380 (0.1427)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [140/689]  eta: 0:14:24  lr: 0.000096  loss: 0.1374 (0.1427)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [150/689]  eta: 0:14:08  lr: 0.000096  loss: 0.1374 (0.1425)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [160/689]  eta: 0:13:52  lr: 0.000096  loss: 0.1402 (0.1422)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [170/689]  eta: 0:13:37  lr: 0.000096  loss: 0.1430 (0.1425)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [180/689]  eta: 0:13:21  lr: 0.000096  loss: 0.1500 (0.1432)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [190/689]  eta: 0:13:06  lr: 0.000096  loss: 0.1534 (0.1437)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [200/689]  eta: 0:12:50  lr: 0.000096  loss: 0.1534 (0.1442)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [210/689]  eta: 0:12:34  lr: 0.000096  loss: 0.1531 (0.1444)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [220/689]  eta: 0:12:19  lr: 0.000096  loss: 0.1407 (0.1444)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [230/689]  eta: 0:12:03  lr: 0.000096  loss: 0.1424 (0.1446)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [240/689]  eta: 0:11:47  lr: 0.000096  loss: 0.1484 (0.1446)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [250/689]  eta: 0:11:32  lr: 0.000096  loss: 0.1441 (0.1446)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [260/689]  eta: 0:11:16  lr: 0.000096  loss: 0.1522 (0.1454)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [270/689]  eta: 0:11:00  lr: 0.000096  loss: 0.1503 (0.1452)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [280/689]  eta: 0:10:44  lr: 0.000096  loss: 0.1418 (0.1457)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [290/689]  eta: 0:10:29  lr: 0.000096  loss: 0.1508 (0.1458)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [300/689]  eta: 0:10:13  lr: 0.000096  loss: 0.1494 (0.1457)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [310/689]  eta: 0:09:57  lr: 0.000096  loss: 0.1465 (0.1458)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [320/689]  eta: 0:09:42  lr: 0.000096  loss: 0.1452 (0.1457)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [330/689]  eta: 0:09:26  lr: 0.000096  loss: 0.1437 (0.1456)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [340/689]  eta: 0:09:10  lr: 0.000096  loss: 0.1407 (0.1457)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [350/689]  eta: 0:08:54  lr: 0.000096  loss: 0.1449 (0.1458)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [360/689]  eta: 0:08:39  lr: 0.000096  loss: 0.1511 (0.1459)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [370/689]  eta: 0:08:23  lr: 0.000096  loss: 0.1495 (0.1461)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [380/689]  eta: 0:08:07  lr: 0.000096  loss: 0.1544 (0.1463)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [390/689]  eta: 0:07:51  lr: 0.000096  loss: 0.1507 (0.1464)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [400/689]  eta: 0:07:35  lr: 0.000096  loss: 0.1503 (0.1466)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [410/689]  eta: 0:07:20  lr: 0.000096  loss: 0.1503 (0.1466)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [420/689]  eta: 0:07:04  lr: 0.000096  loss: 0.1476 (0.1466)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [430/689]  eta: 0:06:48  lr: 0.000096  loss: 0.1430 (0.1466)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [440/689]  eta: 0:06:32  lr: 0.000096  loss: 0.1450 (0.1466)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [450/689]  eta: 0:06:17  lr: 0.000096  loss: 0.1443 (0.1464)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [460/689]  eta: 0:06:01  lr: 0.000096  loss: 0.1391 (0.1464)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [470/689]  eta: 0:05:45  lr: 0.000096  loss: 0.1511 (0.1465)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [480/689]  eta: 0:05:29  lr: 0.000096  loss: 0.1430 (0.1465)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [490/689]  eta: 0:05:14  lr: 0.000096  loss: 0.1391 (0.1464)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [500/689]  eta: 0:04:58  lr: 0.000096  loss: 0.1404 (0.1464)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [510/689]  eta: 0:04:42  lr: 0.000096  loss: 0.1449 (0.1464)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [520/689]  eta: 0:04:26  lr: 0.000096  loss: 0.1425 (0.1464)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [530/689]  eta: 0:04:10  lr: 0.000096  loss: 0.1383 (0.1463)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [540/689]  eta: 0:03:55  lr: 0.000096  loss: 0.1400 (0.1463)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [550/689]  eta: 0:03:39  lr: 0.000096  loss: 0.1537 (0.1465)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [560/689]  eta: 0:03:23  lr: 0.000096  loss: 0.1459 (0.1464)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [570/689]  eta: 0:03:07  lr: 0.000096  loss: 0.1368 (0.1464)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [580/689]  eta: 0:02:52  lr: 0.000096  loss: 0.1435 (0.1463)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [590/689]  eta: 0:02:36  lr: 0.000096  loss: 0.1403 (0.1462)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:139]  [600/689]  eta: 0:02:20  lr: 0.000096  loss: 0.1386 (0.1464)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [610/689]  eta: 0:02:04  lr: 0.000096  loss: 0.1444 (0.1463)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [620/689]  eta: 0:01:48  lr: 0.000096  loss: 0.1447 (0.1463)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [630/689]  eta: 0:01:33  lr: 0.000096  loss: 0.1434 (0.1462)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [640/689]  eta: 0:01:17  lr: 0.000096  loss: 0.1418 (0.1463)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [650/689]  eta: 0:01:01  lr: 0.000096  loss: 0.1437 (0.1463)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [660/689]  eta: 0:00:45  lr: 0.000096  loss: 0.1349 (0.1464)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [670/689]  eta: 0:00:29  lr: 0.000096  loss: 0.1481 (0.1465)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [680/689]  eta: 0:00:14  lr: 0.000096  loss: 0.1425 (0.1464)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139]  [688/689]  eta: 0:00:01  lr: 0.000096  loss: 0.1425 (0.1464)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:139] Total time: 0:18:07 (1.5786 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: 0.1425 (0.1464)\n",
      "Valid: [epoch:139]  [ 0/14]  eta: 0:00:14  loss: 0.1197 (0.1197)  time: 1.0124  data: 0.3974  max mem: 39763\n",
      "Valid: [epoch:139]  [13/14]  eta: 0:00:00  loss: 0.1334 (0.1357)  time: 0.1143  data: 0.0284  max mem: 39763\n",
      "Valid: [epoch:139] Total time: 0:00:01 (0.1234 s / it)\n",
      "Averaged stats: loss: 0.1334 (0.1357)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_139_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.136%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:140]  [  0/689]  eta: 0:11:31  lr: 0.000096  loss: 0.1258 (0.1258)  time: 1.0034  data: 0.5237  max mem: 39763\n",
      "Train: [epoch:140]  [ 10/689]  eta: 0:17:15  lr: 0.000096  loss: 0.1410 (0.1424)  time: 1.5251  data: 0.0477  max mem: 39763\n",
      "Train: [epoch:140]  [ 20/689]  eta: 0:17:17  lr: 0.000096  loss: 0.1411 (0.1439)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [ 30/689]  eta: 0:17:07  lr: 0.000096  loss: 0.1425 (0.1463)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [ 40/689]  eta: 0:16:54  lr: 0.000096  loss: 0.1425 (0.1463)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [ 50/689]  eta: 0:16:41  lr: 0.000096  loss: 0.1460 (0.1468)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [ 60/689]  eta: 0:16:26  lr: 0.000096  loss: 0.1463 (0.1465)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [ 70/689]  eta: 0:16:11  lr: 0.000096  loss: 0.1442 (0.1466)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [ 80/689]  eta: 0:15:56  lr: 0.000096  loss: 0.1485 (0.1466)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [ 90/689]  eta: 0:15:41  lr: 0.000096  loss: 0.1513 (0.1473)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [100/689]  eta: 0:15:26  lr: 0.000096  loss: 0.1540 (0.1485)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [110/689]  eta: 0:15:10  lr: 0.000096  loss: 0.1580 (0.1493)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [120/689]  eta: 0:14:55  lr: 0.000096  loss: 0.1479 (0.1489)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [130/689]  eta: 0:14:39  lr: 0.000096  loss: 0.1461 (0.1489)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [140/689]  eta: 0:14:24  lr: 0.000096  loss: 0.1460 (0.1490)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [150/689]  eta: 0:14:08  lr: 0.000096  loss: 0.1483 (0.1492)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [160/689]  eta: 0:13:52  lr: 0.000096  loss: 0.1496 (0.1495)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [170/689]  eta: 0:13:37  lr: 0.000096  loss: 0.1568 (0.1501)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [180/689]  eta: 0:13:21  lr: 0.000096  loss: 0.1459 (0.1497)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [190/689]  eta: 0:13:05  lr: 0.000096  loss: 0.1414 (0.1501)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [200/689]  eta: 0:12:50  lr: 0.000096  loss: 0.1398 (0.1493)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [210/689]  eta: 0:12:34  lr: 0.000096  loss: 0.1398 (0.1496)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [220/689]  eta: 0:12:18  lr: 0.000096  loss: 0.1433 (0.1491)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [230/689]  eta: 0:12:03  lr: 0.000096  loss: 0.1355 (0.1490)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [240/689]  eta: 0:11:47  lr: 0.000096  loss: 0.1428 (0.1490)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [250/689]  eta: 0:11:31  lr: 0.000096  loss: 0.1407 (0.1485)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [260/689]  eta: 0:11:16  lr: 0.000096  loss: 0.1472 (0.1487)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [270/689]  eta: 0:11:00  lr: 0.000096  loss: 0.1486 (0.1486)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [280/689]  eta: 0:10:44  lr: 0.000096  loss: 0.1433 (0.1484)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [290/689]  eta: 0:10:28  lr: 0.000096  loss: 0.1429 (0.1481)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [300/689]  eta: 0:10:13  lr: 0.000096  loss: 0.1428 (0.1479)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [310/689]  eta: 0:09:57  lr: 0.000096  loss: 0.1368 (0.1476)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [320/689]  eta: 0:09:41  lr: 0.000096  loss: 0.1368 (0.1476)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [330/689]  eta: 0:09:26  lr: 0.000096  loss: 0.1473 (0.1476)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [340/689]  eta: 0:09:10  lr: 0.000096  loss: 0.1473 (0.1477)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [350/689]  eta: 0:08:54  lr: 0.000096  loss: 0.1465 (0.1477)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [360/689]  eta: 0:08:38  lr: 0.000096  loss: 0.1450 (0.1478)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [370/689]  eta: 0:08:23  lr: 0.000096  loss: 0.1478 (0.1480)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [380/689]  eta: 0:08:07  lr: 0.000096  loss: 0.1484 (0.1480)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [390/689]  eta: 0:07:51  lr: 0.000096  loss: 0.1443 (0.1479)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [400/689]  eta: 0:07:35  lr: 0.000096  loss: 0.1399 (0.1479)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [410/689]  eta: 0:07:19  lr: 0.000096  loss: 0.1427 (0.1479)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [420/689]  eta: 0:07:04  lr: 0.000096  loss: 0.1429 (0.1478)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [430/689]  eta: 0:06:48  lr: 0.000096  loss: 0.1409 (0.1479)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [440/689]  eta: 0:06:32  lr: 0.000096  loss: 0.1432 (0.1480)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [450/689]  eta: 0:06:16  lr: 0.000096  loss: 0.1432 (0.1479)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [460/689]  eta: 0:06:01  lr: 0.000096  loss: 0.1451 (0.1480)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [470/689]  eta: 0:05:45  lr: 0.000096  loss: 0.1426 (0.1480)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [480/689]  eta: 0:05:29  lr: 0.000096  loss: 0.1420 (0.1479)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [490/689]  eta: 0:05:13  lr: 0.000096  loss: 0.1411 (0.1479)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [500/689]  eta: 0:04:58  lr: 0.000096  loss: 0.1457 (0.1480)  time: 1.5776  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:140]  [510/689]  eta: 0:04:42  lr: 0.000096  loss: 0.1486 (0.1481)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [520/689]  eta: 0:04:26  lr: 0.000096  loss: 0.1486 (0.1482)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [530/689]  eta: 0:04:10  lr: 0.000096  loss: 0.1440 (0.1481)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [540/689]  eta: 0:03:55  lr: 0.000096  loss: 0.1461 (0.1482)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [550/689]  eta: 0:03:39  lr: 0.000096  loss: 0.1464 (0.1482)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [560/689]  eta: 0:03:23  lr: 0.000096  loss: 0.1388 (0.1481)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [570/689]  eta: 0:03:07  lr: 0.000096  loss: 0.1404 (0.1480)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [580/689]  eta: 0:02:51  lr: 0.000096  loss: 0.1439 (0.1480)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [590/689]  eta: 0:02:36  lr: 0.000096  loss: 0.1483 (0.1480)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [600/689]  eta: 0:02:20  lr: 0.000096  loss: 0.1419 (0.1479)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [610/689]  eta: 0:02:04  lr: 0.000096  loss: 0.1419 (0.1479)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [620/689]  eta: 0:01:48  lr: 0.000096  loss: 0.1391 (0.1479)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [630/689]  eta: 0:01:33  lr: 0.000096  loss: 0.1429 (0.1479)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [640/689]  eta: 0:01:17  lr: 0.000096  loss: 0.1429 (0.1479)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [650/689]  eta: 0:01:01  lr: 0.000096  loss: 0.1383 (0.1477)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [660/689]  eta: 0:00:45  lr: 0.000096  loss: 0.1366 (0.1476)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [670/689]  eta: 0:00:29  lr: 0.000096  loss: 0.1436 (0.1477)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [680/689]  eta: 0:00:14  lr: 0.000096  loss: 0.1506 (0.1477)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140]  [688/689]  eta: 0:00:01  lr: 0.000096  loss: 0.1554 (0.1479)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:140] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: 0.1554 (0.1479)\n",
      "Valid: [epoch:140]  [ 0/14]  eta: 0:00:14  loss: 0.1290 (0.1290)  time: 1.0213  data: 0.3710  max mem: 39763\n",
      "Valid: [epoch:140]  [13/14]  eta: 0:00:00  loss: 0.1361 (0.1382)  time: 0.1149  data: 0.0266  max mem: 39763\n",
      "Valid: [epoch:140] Total time: 0:00:01 (0.1241 s / it)\n",
      "Averaged stats: loss: 0.1361 (0.1382)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_140_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.138%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:141]  [  0/689]  eta: 0:11:48  lr: 0.000096  loss: 0.1581 (0.1581)  time: 1.0289  data: 0.5541  max mem: 39763\n",
      "Train: [epoch:141]  [ 10/689]  eta: 0:17:15  lr: 0.000096  loss: 0.1442 (0.1435)  time: 1.5254  data: 0.0504  max mem: 39763\n",
      "Train: [epoch:141]  [ 20/689]  eta: 0:17:16  lr: 0.000096  loss: 0.1451 (0.1463)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [ 30/689]  eta: 0:17:07  lr: 0.000096  loss: 0.1468 (0.1455)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [ 40/689]  eta: 0:16:54  lr: 0.000096  loss: 0.1408 (0.1430)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [ 50/689]  eta: 0:16:40  lr: 0.000096  loss: 0.1343 (0.1434)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [ 60/689]  eta: 0:16:26  lr: 0.000096  loss: 0.1417 (0.1440)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [ 70/689]  eta: 0:16:11  lr: 0.000096  loss: 0.1481 (0.1440)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [ 80/689]  eta: 0:15:56  lr: 0.000096  loss: 0.1433 (0.1435)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [ 90/689]  eta: 0:15:41  lr: 0.000096  loss: 0.1439 (0.1448)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [100/689]  eta: 0:15:25  lr: 0.000096  loss: 0.1592 (0.1468)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [110/689]  eta: 0:15:10  lr: 0.000096  loss: 0.1592 (0.1474)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [120/689]  eta: 0:14:54  lr: 0.000096  loss: 0.1420 (0.1471)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [130/689]  eta: 0:14:39  lr: 0.000096  loss: 0.1422 (0.1477)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [140/689]  eta: 0:14:23  lr: 0.000096  loss: 0.1586 (0.1491)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [150/689]  eta: 0:14:08  lr: 0.000096  loss: 0.1645 (0.1503)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [160/689]  eta: 0:13:52  lr: 0.000096  loss: 0.1529 (0.1501)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [170/689]  eta: 0:13:37  lr: 0.000096  loss: 0.1529 (0.1510)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [180/689]  eta: 0:13:21  lr: 0.000096  loss: 0.1582 (0.1514)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [190/689]  eta: 0:13:05  lr: 0.000096  loss: 0.1560 (0.1522)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [200/689]  eta: 0:12:50  lr: 0.000096  loss: 0.1560 (0.1523)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [210/689]  eta: 0:12:34  lr: 0.000096  loss: 0.1484 (0.1521)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [220/689]  eta: 0:12:18  lr: 0.000096  loss: 0.1467 (0.1523)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [230/689]  eta: 0:12:03  lr: 0.000096  loss: 0.1472 (0.1523)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [240/689]  eta: 0:11:47  lr: 0.000096  loss: 0.1455 (0.1523)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [250/689]  eta: 0:11:31  lr: 0.000096  loss: 0.1455 (0.1519)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [260/689]  eta: 0:11:16  lr: 0.000096  loss: 0.1462 (0.1520)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [270/689]  eta: 0:11:00  lr: 0.000096  loss: 0.1462 (0.1519)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [280/689]  eta: 0:10:44  lr: 0.000096  loss: 0.1405 (0.1515)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [290/689]  eta: 0:10:28  lr: 0.000096  loss: 0.1405 (0.1514)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [300/689]  eta: 0:10:13  lr: 0.000096  loss: 0.1449 (0.1515)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [310/689]  eta: 0:09:57  lr: 0.000096  loss: 0.1449 (0.1513)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [320/689]  eta: 0:09:41  lr: 0.000096  loss: 0.1412 (0.1514)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [330/689]  eta: 0:09:26  lr: 0.000096  loss: 0.1461 (0.1513)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [340/689]  eta: 0:09:10  lr: 0.000096  loss: 0.1432 (0.1511)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [350/689]  eta: 0:08:54  lr: 0.000096  loss: 0.1432 (0.1509)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [360/689]  eta: 0:08:38  lr: 0.000096  loss: 0.1436 (0.1510)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [370/689]  eta: 0:08:23  lr: 0.000096  loss: 0.1403 (0.1508)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [380/689]  eta: 0:08:07  lr: 0.000096  loss: 0.1363 (0.1505)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [390/689]  eta: 0:07:51  lr: 0.000096  loss: 0.1475 (0.1508)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [400/689]  eta: 0:07:35  lr: 0.000096  loss: 0.1532 (0.1508)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [410/689]  eta: 0:07:19  lr: 0.000096  loss: 0.1624 (0.1511)  time: 1.5782  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:141]  [420/689]  eta: 0:07:04  lr: 0.000096  loss: 0.1573 (0.1510)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [430/689]  eta: 0:06:48  lr: 0.000096  loss: 0.1458 (0.1510)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [440/689]  eta: 0:06:32  lr: 0.000096  loss: 0.1466 (0.1510)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [450/689]  eta: 0:06:16  lr: 0.000096  loss: 0.1461 (0.1509)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [460/689]  eta: 0:06:01  lr: 0.000096  loss: 0.1479 (0.1509)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [470/689]  eta: 0:05:45  lr: 0.000096  loss: 0.1578 (0.1511)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [480/689]  eta: 0:05:29  lr: 0.000096  loss: 0.1474 (0.1510)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [490/689]  eta: 0:05:13  lr: 0.000096  loss: 0.1406 (0.1509)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [500/689]  eta: 0:04:58  lr: 0.000096  loss: 0.1453 (0.1509)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [510/689]  eta: 0:04:42  lr: 0.000096  loss: 0.1462 (0.1508)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [520/689]  eta: 0:04:26  lr: 0.000096  loss: 0.1450 (0.1507)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [530/689]  eta: 0:04:10  lr: 0.000096  loss: 0.1355 (0.1504)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [540/689]  eta: 0:03:55  lr: 0.000096  loss: 0.1384 (0.1504)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [550/689]  eta: 0:03:39  lr: 0.000096  loss: 0.1428 (0.1502)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [560/689]  eta: 0:03:23  lr: 0.000096  loss: 0.1502 (0.1504)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [570/689]  eta: 0:03:07  lr: 0.000096  loss: 0.1528 (0.1502)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [580/689]  eta: 0:02:51  lr: 0.000096  loss: 0.1446 (0.1502)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [590/689]  eta: 0:02:36  lr: 0.000096  loss: 0.1461 (0.1503)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [600/689]  eta: 0:02:20  lr: 0.000096  loss: 0.1457 (0.1502)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [610/689]  eta: 0:02:04  lr: 0.000096  loss: 0.1400 (0.1501)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [620/689]  eta: 0:01:48  lr: 0.000096  loss: 0.1421 (0.1500)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [630/689]  eta: 0:01:33  lr: 0.000096  loss: 0.1447 (0.1499)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [640/689]  eta: 0:01:17  lr: 0.000096  loss: 0.1458 (0.1500)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [650/689]  eta: 0:01:01  lr: 0.000096  loss: 0.1405 (0.1500)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [660/689]  eta: 0:00:45  lr: 0.000096  loss: 0.1392 (0.1500)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [670/689]  eta: 0:00:29  lr: 0.000096  loss: 0.1445 (0.1500)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [680/689]  eta: 0:00:14  lr: 0.000096  loss: 0.1445 (0.1501)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141]  [688/689]  eta: 0:00:01  lr: 0.000096  loss: 0.1437 (0.1500)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:141] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: 0.1437 (0.1500)\n",
      "Valid: [epoch:141]  [ 0/14]  eta: 0:00:14  loss: 0.1335 (0.1335)  time: 1.0174  data: 0.3889  max mem: 39763\n",
      "Valid: [epoch:141]  [13/14]  eta: 0:00:00  loss: 0.1370 (0.1394)  time: 0.1146  data: 0.0278  max mem: 39763\n",
      "Valid: [epoch:141] Total time: 0:00:01 (0.1238 s / it)\n",
      "Averaged stats: loss: 0.1370 (0.1394)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_141_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.139%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:142]  [  0/689]  eta: 0:12:22  lr: 0.000095  loss: 0.1698 (0.1698)  time: 1.0782  data: 0.5991  max mem: 39763\n",
      "Train: [epoch:142]  [ 10/689]  eta: 0:17:20  lr: 0.000095  loss: 0.1557 (0.1577)  time: 1.5321  data: 0.0545  max mem: 39763\n",
      "Train: [epoch:142]  [ 20/689]  eta: 0:17:20  lr: 0.000095  loss: 0.1520 (0.1539)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [ 30/689]  eta: 0:17:09  lr: 0.000095  loss: 0.1456 (0.1500)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [ 40/689]  eta: 0:16:56  lr: 0.000095  loss: 0.1440 (0.1485)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [ 50/689]  eta: 0:16:42  lr: 0.000095  loss: 0.1440 (0.1494)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [ 60/689]  eta: 0:16:27  lr: 0.000095  loss: 0.1479 (0.1499)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [ 70/689]  eta: 0:16:12  lr: 0.000095  loss: 0.1428 (0.1493)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [ 80/689]  eta: 0:15:57  lr: 0.000095  loss: 0.1404 (0.1486)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [ 90/689]  eta: 0:15:42  lr: 0.000095  loss: 0.1411 (0.1485)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [100/689]  eta: 0:15:26  lr: 0.000095  loss: 0.1485 (0.1497)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [110/689]  eta: 0:15:11  lr: 0.000095  loss: 0.1485 (0.1492)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [120/689]  eta: 0:14:55  lr: 0.000095  loss: 0.1477 (0.1498)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [130/689]  eta: 0:14:40  lr: 0.000095  loss: 0.1452 (0.1503)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [140/689]  eta: 0:14:24  lr: 0.000095  loss: 0.1414 (0.1499)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [150/689]  eta: 0:14:09  lr: 0.000095  loss: 0.1414 (0.1498)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [160/689]  eta: 0:13:53  lr: 0.000095  loss: 0.1400 (0.1492)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [170/689]  eta: 0:13:37  lr: 0.000095  loss: 0.1410 (0.1491)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [180/689]  eta: 0:13:22  lr: 0.000095  loss: 0.1450 (0.1490)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [190/689]  eta: 0:13:06  lr: 0.000095  loss: 0.1437 (0.1489)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [200/689]  eta: 0:12:50  lr: 0.000095  loss: 0.1428 (0.1491)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [210/689]  eta: 0:12:35  lr: 0.000095  loss: 0.1428 (0.1487)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [220/689]  eta: 0:12:19  lr: 0.000095  loss: 0.1473 (0.1495)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [230/689]  eta: 0:12:03  lr: 0.000095  loss: 0.1557 (0.1494)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [240/689]  eta: 0:11:47  lr: 0.000095  loss: 0.1486 (0.1494)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [250/689]  eta: 0:11:32  lr: 0.000095  loss: 0.1438 (0.1491)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [260/689]  eta: 0:11:16  lr: 0.000095  loss: 0.1382 (0.1493)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [270/689]  eta: 0:11:00  lr: 0.000095  loss: 0.1378 (0.1489)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [280/689]  eta: 0:10:45  lr: 0.000095  loss: 0.1362 (0.1487)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [290/689]  eta: 0:10:29  lr: 0.000095  loss: 0.1479 (0.1490)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [300/689]  eta: 0:10:13  lr: 0.000095  loss: 0.1507 (0.1493)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [310/689]  eta: 0:09:57  lr: 0.000095  loss: 0.1403 (0.1491)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [320/689]  eta: 0:09:42  lr: 0.000095  loss: 0.1442 (0.1491)  time: 1.5782  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:142]  [330/689]  eta: 0:09:26  lr: 0.000095  loss: 0.1557 (0.1495)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [340/689]  eta: 0:09:10  lr: 0.000095  loss: 0.1446 (0.1493)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [350/689]  eta: 0:08:54  lr: 0.000095  loss: 0.1483 (0.1496)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [360/689]  eta: 0:08:38  lr: 0.000095  loss: 0.1497 (0.1495)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [370/689]  eta: 0:08:23  lr: 0.000095  loss: 0.1426 (0.1494)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [380/689]  eta: 0:08:07  lr: 0.000095  loss: 0.1390 (0.1494)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [390/689]  eta: 0:07:51  lr: 0.000095  loss: 0.1390 (0.1491)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [400/689]  eta: 0:07:35  lr: 0.000095  loss: 0.1452 (0.1494)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [410/689]  eta: 0:07:20  lr: 0.000095  loss: 0.1559 (0.1493)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [420/689]  eta: 0:07:04  lr: 0.000095  loss: 0.1473 (0.1492)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [430/689]  eta: 0:06:48  lr: 0.000095  loss: 0.1511 (0.1493)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [440/689]  eta: 0:06:32  lr: 0.000095  loss: 0.1470 (0.1493)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [450/689]  eta: 0:06:17  lr: 0.000095  loss: 0.1441 (0.1493)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [460/689]  eta: 0:06:01  lr: 0.000095  loss: 0.1539 (0.1494)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [470/689]  eta: 0:05:45  lr: 0.000095  loss: 0.1601 (0.1497)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [480/689]  eta: 0:05:29  lr: 0.000095  loss: 0.1474 (0.1496)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [490/689]  eta: 0:05:13  lr: 0.000095  loss: 0.1391 (0.1495)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [500/689]  eta: 0:04:58  lr: 0.000095  loss: 0.1425 (0.1495)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [510/689]  eta: 0:04:42  lr: 0.000095  loss: 0.1588 (0.1499)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [520/689]  eta: 0:04:26  lr: 0.000095  loss: 0.1568 (0.1499)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [530/689]  eta: 0:04:10  lr: 0.000095  loss: 0.1421 (0.1500)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [540/689]  eta: 0:03:55  lr: 0.000095  loss: 0.1439 (0.1500)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [550/689]  eta: 0:03:39  lr: 0.000095  loss: 0.1462 (0.1500)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [560/689]  eta: 0:03:23  lr: 0.000095  loss: 0.1496 (0.1502)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [570/689]  eta: 0:03:07  lr: 0.000095  loss: 0.1482 (0.1500)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [580/689]  eta: 0:02:51  lr: 0.000095  loss: 0.1387 (0.1500)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [590/689]  eta: 0:02:36  lr: 0.000095  loss: 0.1492 (0.1500)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [600/689]  eta: 0:02:20  lr: 0.000095  loss: 0.1430 (0.1497)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [610/689]  eta: 0:02:04  lr: 0.000095  loss: 0.1375 (0.1496)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [620/689]  eta: 0:01:48  lr: 0.000095  loss: 0.1399 (0.1496)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [630/689]  eta: 0:01:33  lr: 0.000095  loss: 0.1383 (0.1496)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [640/689]  eta: 0:01:17  lr: 0.000095  loss: 0.1401 (0.1496)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [650/689]  eta: 0:01:01  lr: 0.000095  loss: 0.1409 (0.1496)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [660/689]  eta: 0:00:45  lr: 0.000095  loss: 0.1409 (0.1495)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [670/689]  eta: 0:00:29  lr: 0.000095  loss: 0.1442 (0.1496)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [680/689]  eta: 0:00:14  lr: 0.000095  loss: 0.1442 (0.1495)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142]  [688/689]  eta: 0:00:01  lr: 0.000095  loss: 0.1532 (0.1496)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:142] Total time: 0:18:07 (1.5782 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: 0.1532 (0.1496)\n",
      "Valid: [epoch:142]  [ 0/14]  eta: 0:00:14  loss: 0.1450 (0.1450)  time: 1.0065  data: 0.5229  max mem: 39763\n",
      "Valid: [epoch:142]  [13/14]  eta: 0:00:00  loss: 0.1365 (0.1388)  time: 0.1139  data: 0.0374  max mem: 39763\n",
      "Valid: [epoch:142] Total time: 0:00:01 (0.1212 s / it)\n",
      "Averaged stats: loss: 0.1365 (0.1388)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_142_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.139%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:143]  [  0/689]  eta: 0:11:53  lr: 0.000095  loss: 0.1381 (0.1381)  time: 1.0362  data: 0.5563  max mem: 39763\n",
      "Train: [epoch:143]  [ 10/689]  eta: 0:17:16  lr: 0.000095  loss: 0.1408 (0.1481)  time: 1.5263  data: 0.0507  max mem: 39763\n",
      "Train: [epoch:143]  [ 20/689]  eta: 0:17:17  lr: 0.000095  loss: 0.1416 (0.1460)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [ 30/689]  eta: 0:17:07  lr: 0.000095  loss: 0.1464 (0.1509)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [ 40/689]  eta: 0:16:54  lr: 0.000095  loss: 0.1536 (0.1500)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [ 50/689]  eta: 0:16:40  lr: 0.000095  loss: 0.1497 (0.1518)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [ 60/689]  eta: 0:16:26  lr: 0.000095  loss: 0.1471 (0.1504)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [ 70/689]  eta: 0:16:11  lr: 0.000095  loss: 0.1335 (0.1494)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [ 80/689]  eta: 0:15:56  lr: 0.000095  loss: 0.1376 (0.1486)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [ 90/689]  eta: 0:15:41  lr: 0.000095  loss: 0.1453 (0.1490)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [100/689]  eta: 0:15:25  lr: 0.000095  loss: 0.1486 (0.1489)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [110/689]  eta: 0:15:10  lr: 0.000095  loss: 0.1463 (0.1488)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [120/689]  eta: 0:14:54  lr: 0.000095  loss: 0.1463 (0.1488)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [130/689]  eta: 0:14:39  lr: 0.000095  loss: 0.1500 (0.1494)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [140/689]  eta: 0:14:23  lr: 0.000095  loss: 0.1489 (0.1496)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [150/689]  eta: 0:14:08  lr: 0.000095  loss: 0.1494 (0.1500)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [160/689]  eta: 0:13:52  lr: 0.000095  loss: 0.1526 (0.1502)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [170/689]  eta: 0:13:37  lr: 0.000095  loss: 0.1474 (0.1498)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [180/689]  eta: 0:13:21  lr: 0.000095  loss: 0.1450 (0.1496)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [190/689]  eta: 0:13:05  lr: 0.000095  loss: 0.1450 (0.1493)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [200/689]  eta: 0:12:50  lr: 0.000095  loss: 0.1402 (0.1492)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [210/689]  eta: 0:12:34  lr: 0.000095  loss: 0.1412 (0.1490)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [220/689]  eta: 0:12:18  lr: 0.000095  loss: 0.1430 (0.1489)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [230/689]  eta: 0:12:03  lr: 0.000095  loss: 0.1461 (0.1487)  time: 1.5782  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:143]  [240/689]  eta: 0:11:47  lr: 0.000095  loss: 0.1471 (0.1491)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [250/689]  eta: 0:11:31  lr: 0.000095  loss: 0.1468 (0.1488)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [260/689]  eta: 0:11:15  lr: 0.000095  loss: 0.1446 (0.1488)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [270/689]  eta: 0:11:00  lr: 0.000095  loss: 0.1424 (0.1487)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [280/689]  eta: 0:10:44  lr: 0.000095  loss: 0.1417 (0.1484)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [290/689]  eta: 0:10:28  lr: 0.000095  loss: 0.1448 (0.1485)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [300/689]  eta: 0:10:13  lr: 0.000095  loss: 0.1452 (0.1486)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [310/689]  eta: 0:09:57  lr: 0.000095  loss: 0.1452 (0.1488)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [320/689]  eta: 0:09:41  lr: 0.000095  loss: 0.1363 (0.1483)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [330/689]  eta: 0:09:25  lr: 0.000095  loss: 0.1352 (0.1485)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [340/689]  eta: 0:09:10  lr: 0.000095  loss: 0.1443 (0.1485)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [350/689]  eta: 0:08:54  lr: 0.000095  loss: 0.1467 (0.1488)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [360/689]  eta: 0:08:38  lr: 0.000095  loss: 0.1441 (0.1486)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [370/689]  eta: 0:08:22  lr: 0.000095  loss: 0.1439 (0.1486)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [380/689]  eta: 0:08:07  lr: 0.000095  loss: 0.1464 (0.1487)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [390/689]  eta: 0:07:51  lr: 0.000095  loss: 0.1659 (0.1490)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [400/689]  eta: 0:07:35  lr: 0.000095  loss: 0.1603 (0.1492)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [410/689]  eta: 0:07:19  lr: 0.000095  loss: 0.1460 (0.1492)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [420/689]  eta: 0:07:04  lr: 0.000095  loss: 0.1370 (0.1490)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [430/689]  eta: 0:06:48  lr: 0.000095  loss: 0.1433 (0.1489)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [440/689]  eta: 0:06:32  lr: 0.000095  loss: 0.1518 (0.1492)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [450/689]  eta: 0:06:16  lr: 0.000095  loss: 0.1486 (0.1490)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [460/689]  eta: 0:06:01  lr: 0.000095  loss: 0.1413 (0.1492)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [470/689]  eta: 0:05:45  lr: 0.000095  loss: 0.1529 (0.1493)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [480/689]  eta: 0:05:29  lr: 0.000095  loss: 0.1423 (0.1492)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [490/689]  eta: 0:05:13  lr: 0.000095  loss: 0.1394 (0.1492)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [500/689]  eta: 0:04:58  lr: 0.000095  loss: 0.1394 (0.1491)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [510/689]  eta: 0:04:42  lr: 0.000095  loss: 0.1527 (0.1494)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [520/689]  eta: 0:04:26  lr: 0.000095  loss: 0.1529 (0.1494)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [530/689]  eta: 0:04:10  lr: 0.000095  loss: 0.1460 (0.1493)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [540/689]  eta: 0:03:54  lr: 0.000095  loss: 0.1467 (0.1493)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [550/689]  eta: 0:03:39  lr: 0.000095  loss: 0.1522 (0.1495)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [560/689]  eta: 0:03:23  lr: 0.000095  loss: 0.1567 (0.1497)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [570/689]  eta: 0:03:07  lr: 0.000095  loss: 0.1462 (0.1496)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [580/689]  eta: 0:02:51  lr: 0.000095  loss: 0.1452 (0.1496)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [590/689]  eta: 0:02:36  lr: 0.000095  loss: 0.1565 (0.1498)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [600/689]  eta: 0:02:20  lr: 0.000095  loss: 0.1535 (0.1497)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [610/689]  eta: 0:02:04  lr: 0.000095  loss: 0.1519 (0.1498)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [620/689]  eta: 0:01:48  lr: 0.000095  loss: 0.1519 (0.1497)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [630/689]  eta: 0:01:33  lr: 0.000095  loss: 0.1404 (0.1497)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [640/689]  eta: 0:01:17  lr: 0.000095  loss: 0.1450 (0.1497)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [650/689]  eta: 0:01:01  lr: 0.000095  loss: 0.1461 (0.1498)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [660/689]  eta: 0:00:45  lr: 0.000095  loss: 0.1496 (0.1497)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [670/689]  eta: 0:00:29  lr: 0.000095  loss: 0.1436 (0.1497)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [680/689]  eta: 0:00:14  lr: 0.000095  loss: 0.1423 (0.1496)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143]  [688/689]  eta: 0:00:01  lr: 0.000095  loss: 0.1374 (0.1494)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:143] Total time: 0:18:06 (1.5775 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: 0.1374 (0.1494)\n",
      "Valid: [epoch:143]  [ 0/14]  eta: 0:00:14  loss: 0.1451 (0.1451)  time: 1.0214  data: 0.3741  max mem: 39763\n",
      "Valid: [epoch:143]  [13/14]  eta: 0:00:00  loss: 0.1368 (0.1390)  time: 0.1149  data: 0.0268  max mem: 39763\n",
      "Valid: [epoch:143] Total time: 0:00:01 (0.1241 s / it)\n",
      "Averaged stats: loss: 0.1368 (0.1390)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_143_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.139%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:144]  [  0/689]  eta: 0:11:38  lr: 0.000095  loss: 0.1343 (0.1343)  time: 1.0137  data: 0.5336  max mem: 39763\n",
      "Train: [epoch:144]  [ 10/689]  eta: 0:17:16  lr: 0.000095  loss: 0.1351 (0.1385)  time: 1.5261  data: 0.0486  max mem: 39763\n",
      "Train: [epoch:144]  [ 20/689]  eta: 0:17:17  lr: 0.000095  loss: 0.1387 (0.1418)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [ 30/689]  eta: 0:17:07  lr: 0.000095  loss: 0.1488 (0.1479)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [ 40/689]  eta: 0:16:54  lr: 0.000095  loss: 0.1581 (0.1491)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [ 50/689]  eta: 0:16:40  lr: 0.000095  loss: 0.1497 (0.1499)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [ 60/689]  eta: 0:16:26  lr: 0.000095  loss: 0.1447 (0.1496)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [ 70/689]  eta: 0:16:11  lr: 0.000095  loss: 0.1442 (0.1511)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [ 80/689]  eta: 0:15:56  lr: 0.000095  loss: 0.1597 (0.1517)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [ 90/689]  eta: 0:15:41  lr: 0.000095  loss: 0.1564 (0.1529)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [100/689]  eta: 0:15:25  lr: 0.000095  loss: 0.1648 (0.1553)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [110/689]  eta: 0:15:10  lr: 0.000095  loss: 0.1533 (0.1547)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [120/689]  eta: 0:14:55  lr: 0.000095  loss: 0.1462 (0.1540)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [130/689]  eta: 0:14:39  lr: 0.000095  loss: 0.1524 (0.1551)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [140/689]  eta: 0:14:23  lr: 0.000095  loss: 0.1507 (0.1545)  time: 1.5778  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:144]  [150/689]  eta: 0:14:08  lr: 0.000095  loss: 0.1504 (0.1547)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [160/689]  eta: 0:13:52  lr: 0.000095  loss: 0.1551 (0.1549)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [170/689]  eta: 0:13:37  lr: 0.000095  loss: 0.1563 (0.1552)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [180/689]  eta: 0:13:21  lr: 0.000095  loss: 0.1477 (0.1549)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [190/689]  eta: 0:13:05  lr: 0.000095  loss: 0.1477 (0.1552)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [200/689]  eta: 0:12:50  lr: 0.000095  loss: 0.1612 (0.1557)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [210/689]  eta: 0:12:34  lr: 0.000095  loss: 0.1540 (0.1551)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [220/689]  eta: 0:12:18  lr: 0.000095  loss: 0.1487 (0.1551)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [230/689]  eta: 0:12:03  lr: 0.000095  loss: 0.1502 (0.1550)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [240/689]  eta: 0:11:47  lr: 0.000095  loss: 0.1439 (0.1549)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [250/689]  eta: 0:11:31  lr: 0.000095  loss: 0.1488 (0.1549)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [260/689]  eta: 0:11:16  lr: 0.000095  loss: 0.1510 (0.1549)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [270/689]  eta: 0:11:00  lr: 0.000095  loss: 0.1511 (0.1550)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [280/689]  eta: 0:10:44  lr: 0.000095  loss: 0.1501 (0.1550)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [290/689]  eta: 0:10:28  lr: 0.000095  loss: 0.1467 (0.1544)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [300/689]  eta: 0:10:13  lr: 0.000095  loss: 0.1437 (0.1544)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [310/689]  eta: 0:09:57  lr: 0.000095  loss: 0.1437 (0.1540)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [320/689]  eta: 0:09:41  lr: 0.000095  loss: 0.1384 (0.1537)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [330/689]  eta: 0:09:26  lr: 0.000095  loss: 0.1504 (0.1540)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [340/689]  eta: 0:09:10  lr: 0.000095  loss: 0.1544 (0.1539)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [350/689]  eta: 0:08:54  lr: 0.000095  loss: 0.1430 (0.1536)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [360/689]  eta: 0:08:38  lr: 0.000095  loss: 0.1487 (0.1537)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [370/689]  eta: 0:08:23  lr: 0.000095  loss: 0.1487 (0.1536)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [380/689]  eta: 0:08:07  lr: 0.000095  loss: 0.1458 (0.1533)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [390/689]  eta: 0:07:51  lr: 0.000095  loss: 0.1435 (0.1532)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [400/689]  eta: 0:07:35  lr: 0.000095  loss: 0.1469 (0.1532)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [410/689]  eta: 0:07:20  lr: 0.000095  loss: 0.1465 (0.1530)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [420/689]  eta: 0:07:04  lr: 0.000095  loss: 0.1412 (0.1527)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [430/689]  eta: 0:06:48  lr: 0.000095  loss: 0.1475 (0.1528)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [440/689]  eta: 0:06:32  lr: 0.000095  loss: 0.1560 (0.1530)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [450/689]  eta: 0:06:17  lr: 0.000095  loss: 0.1522 (0.1530)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [460/689]  eta: 0:06:01  lr: 0.000095  loss: 0.1490 (0.1530)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [470/689]  eta: 0:05:45  lr: 0.000095  loss: 0.1490 (0.1530)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [480/689]  eta: 0:05:29  lr: 0.000095  loss: 0.1494 (0.1530)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [490/689]  eta: 0:05:13  lr: 0.000095  loss: 0.1497 (0.1531)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [500/689]  eta: 0:04:58  lr: 0.000095  loss: 0.1541 (0.1531)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [510/689]  eta: 0:04:42  lr: 0.000095  loss: 0.1558 (0.1531)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [520/689]  eta: 0:04:26  lr: 0.000095  loss: 0.1482 (0.1531)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [530/689]  eta: 0:04:10  lr: 0.000095  loss: 0.1496 (0.1532)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [540/689]  eta: 0:03:55  lr: 0.000095  loss: 0.1488 (0.1531)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [550/689]  eta: 0:03:39  lr: 0.000095  loss: 0.1464 (0.1532)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [560/689]  eta: 0:03:23  lr: 0.000095  loss: 0.1454 (0.1531)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [570/689]  eta: 0:03:07  lr: 0.000095  loss: 0.1467 (0.1530)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [580/689]  eta: 0:02:51  lr: 0.000095  loss: 0.1489 (0.1530)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [590/689]  eta: 0:02:36  lr: 0.000095  loss: 0.1530 (0.1531)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [600/689]  eta: 0:02:20  lr: 0.000095  loss: 0.1498 (0.1530)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [610/689]  eta: 0:02:04  lr: 0.000095  loss: 0.1422 (0.1529)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [620/689]  eta: 0:01:48  lr: 0.000095  loss: 0.1429 (0.1529)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [630/689]  eta: 0:01:33  lr: 0.000095  loss: 0.1524 (0.1529)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [640/689]  eta: 0:01:17  lr: 0.000095  loss: 0.1480 (0.1529)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [650/689]  eta: 0:01:01  lr: 0.000095  loss: 0.1427 (0.1527)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [660/689]  eta: 0:00:45  lr: 0.000095  loss: 0.1436 (0.1527)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [670/689]  eta: 0:00:29  lr: 0.000095  loss: 0.1542 (0.1528)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [680/689]  eta: 0:00:14  lr: 0.000095  loss: 0.1507 (0.1528)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144]  [688/689]  eta: 0:00:01  lr: 0.000095  loss: 0.1433 (0.1526)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:144] Total time: 0:18:07 (1.5784 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: 0.1433 (0.1526)\n",
      "Valid: [epoch:144]  [ 0/14]  eta: 0:00:14  loss: 0.1321 (0.1321)  time: 1.0152  data: 0.4074  max mem: 39763\n",
      "Valid: [epoch:144]  [13/14]  eta: 0:00:00  loss: 0.1397 (0.1416)  time: 0.1145  data: 0.0292  max mem: 39763\n",
      "Valid: [epoch:144] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.1397 (0.1416)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_144_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.142%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:145]  [  0/689]  eta: 0:12:02  lr: 0.000095  loss: 0.1492 (0.1492)  time: 1.0486  data: 0.5699  max mem: 39763\n",
      "Train: [epoch:145]  [ 10/689]  eta: 0:17:18  lr: 0.000095  loss: 0.1473 (0.1525)  time: 1.5289  data: 0.0519  max mem: 39763\n",
      "Train: [epoch:145]  [ 20/689]  eta: 0:17:18  lr: 0.000095  loss: 0.1468 (0.1526)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [ 30/689]  eta: 0:17:08  lr: 0.000095  loss: 0.1533 (0.1560)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [ 40/689]  eta: 0:16:55  lr: 0.000095  loss: 0.1500 (0.1547)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [ 50/689]  eta: 0:16:41  lr: 0.000095  loss: 0.1483 (0.1539)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:145]  [ 60/689]  eta: 0:16:27  lr: 0.000095  loss: 0.1484 (0.1538)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [ 70/689]  eta: 0:16:12  lr: 0.000095  loss: 0.1475 (0.1539)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [ 80/689]  eta: 0:15:57  lr: 0.000095  loss: 0.1475 (0.1532)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [ 90/689]  eta: 0:15:42  lr: 0.000095  loss: 0.1470 (0.1530)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [100/689]  eta: 0:15:26  lr: 0.000095  loss: 0.1470 (0.1533)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [110/689]  eta: 0:15:11  lr: 0.000095  loss: 0.1537 (0.1535)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [120/689]  eta: 0:14:55  lr: 0.000095  loss: 0.1463 (0.1533)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [130/689]  eta: 0:14:40  lr: 0.000095  loss: 0.1432 (0.1524)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [140/689]  eta: 0:14:24  lr: 0.000095  loss: 0.1427 (0.1517)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [150/689]  eta: 0:14:09  lr: 0.000095  loss: 0.1458 (0.1522)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [160/689]  eta: 0:13:53  lr: 0.000095  loss: 0.1513 (0.1521)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [170/689]  eta: 0:13:37  lr: 0.000095  loss: 0.1499 (0.1522)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [180/689]  eta: 0:13:22  lr: 0.000095  loss: 0.1490 (0.1519)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [190/689]  eta: 0:13:06  lr: 0.000095  loss: 0.1477 (0.1525)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [200/689]  eta: 0:12:50  lr: 0.000095  loss: 0.1488 (0.1523)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [210/689]  eta: 0:12:34  lr: 0.000095  loss: 0.1383 (0.1517)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [220/689]  eta: 0:12:19  lr: 0.000095  loss: 0.1457 (0.1518)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [230/689]  eta: 0:12:03  lr: 0.000095  loss: 0.1534 (0.1519)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [240/689]  eta: 0:11:47  lr: 0.000095  loss: 0.1534 (0.1521)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [250/689]  eta: 0:11:32  lr: 0.000095  loss: 0.1484 (0.1523)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [260/689]  eta: 0:11:16  lr: 0.000095  loss: 0.1506 (0.1524)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [270/689]  eta: 0:11:00  lr: 0.000095  loss: 0.1506 (0.1522)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [280/689]  eta: 0:10:44  lr: 0.000095  loss: 0.1520 (0.1523)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [290/689]  eta: 0:10:29  lr: 0.000095  loss: 0.1530 (0.1522)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [300/689]  eta: 0:10:13  lr: 0.000095  loss: 0.1525 (0.1523)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [310/689]  eta: 0:09:57  lr: 0.000095  loss: 0.1508 (0.1521)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [320/689]  eta: 0:09:41  lr: 0.000095  loss: 0.1483 (0.1522)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [330/689]  eta: 0:09:26  lr: 0.000095  loss: 0.1524 (0.1524)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [340/689]  eta: 0:09:10  lr: 0.000095  loss: 0.1464 (0.1523)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [350/689]  eta: 0:08:54  lr: 0.000095  loss: 0.1445 (0.1526)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [360/689]  eta: 0:08:38  lr: 0.000095  loss: 0.1531 (0.1526)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [370/689]  eta: 0:08:23  lr: 0.000095  loss: 0.1502 (0.1527)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [380/689]  eta: 0:08:07  lr: 0.000095  loss: 0.1502 (0.1526)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [390/689]  eta: 0:07:51  lr: 0.000095  loss: 0.1522 (0.1527)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [400/689]  eta: 0:07:35  lr: 0.000095  loss: 0.1564 (0.1529)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [410/689]  eta: 0:07:20  lr: 0.000095  loss: 0.1605 (0.1530)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [420/689]  eta: 0:07:04  lr: 0.000095  loss: 0.1496 (0.1529)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [430/689]  eta: 0:06:48  lr: 0.000095  loss: 0.1440 (0.1529)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [440/689]  eta: 0:06:32  lr: 0.000095  loss: 0.1418 (0.1529)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [450/689]  eta: 0:06:17  lr: 0.000095  loss: 0.1461 (0.1530)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [460/689]  eta: 0:06:01  lr: 0.000095  loss: 0.1547 (0.1532)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [470/689]  eta: 0:05:45  lr: 0.000095  loss: 0.1509 (0.1530)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [480/689]  eta: 0:05:29  lr: 0.000095  loss: 0.1418 (0.1528)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [490/689]  eta: 0:05:13  lr: 0.000095  loss: 0.1445 (0.1529)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [500/689]  eta: 0:04:58  lr: 0.000095  loss: 0.1455 (0.1529)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [510/689]  eta: 0:04:42  lr: 0.000095  loss: 0.1581 (0.1531)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [520/689]  eta: 0:04:26  lr: 0.000095  loss: 0.1605 (0.1532)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [530/689]  eta: 0:04:10  lr: 0.000095  loss: 0.1492 (0.1530)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [540/689]  eta: 0:03:55  lr: 0.000095  loss: 0.1417 (0.1530)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [550/689]  eta: 0:03:39  lr: 0.000095  loss: 0.1439 (0.1530)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [560/689]  eta: 0:03:23  lr: 0.000095  loss: 0.1523 (0.1531)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [570/689]  eta: 0:03:07  lr: 0.000095  loss: 0.1434 (0.1530)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [580/689]  eta: 0:02:51  lr: 0.000095  loss: 0.1434 (0.1530)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [590/689]  eta: 0:02:36  lr: 0.000095  loss: 0.1533 (0.1531)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [600/689]  eta: 0:02:20  lr: 0.000095  loss: 0.1511 (0.1532)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [610/689]  eta: 0:02:04  lr: 0.000095  loss: 0.1473 (0.1531)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [620/689]  eta: 0:01:48  lr: 0.000095  loss: 0.1489 (0.1531)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [630/689]  eta: 0:01:33  lr: 0.000095  loss: 0.1522 (0.1531)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [640/689]  eta: 0:01:17  lr: 0.000095  loss: 0.1506 (0.1531)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [650/689]  eta: 0:01:01  lr: 0.000095  loss: 0.1473 (0.1530)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [660/689]  eta: 0:00:45  lr: 0.000095  loss: 0.1416 (0.1531)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [670/689]  eta: 0:00:29  lr: 0.000095  loss: 0.1478 (0.1530)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [680/689]  eta: 0:00:14  lr: 0.000095  loss: 0.1491 (0.1531)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145]  [688/689]  eta: 0:00:01  lr: 0.000095  loss: 0.1415 (0.1529)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:145] Total time: 0:18:07 (1.5783 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: 0.1415 (0.1529)\n",
      "Valid: [epoch:145]  [ 0/14]  eta: 0:00:14  loss: 0.1483 (0.1483)  time: 1.0158  data: 0.3492  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:145]  [13/14]  eta: 0:00:00  loss: 0.1409 (0.1418)  time: 0.1145  data: 0.0250  max mem: 39763\n",
      "Valid: [epoch:145] Total time: 0:00:01 (0.1228 s / it)\n",
      "Averaged stats: loss: 0.1409 (0.1418)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_145_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.142%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:146]  [  0/689]  eta: 0:11:39  lr: 0.000095  loss: 0.1498 (0.1498)  time: 1.0151  data: 0.5342  max mem: 39763\n",
      "Train: [epoch:146]  [ 10/689]  eta: 0:17:16  lr: 0.000095  loss: 0.1468 (0.1524)  time: 1.5269  data: 0.0486  max mem: 39763\n",
      "Train: [epoch:146]  [ 20/689]  eta: 0:17:17  lr: 0.000095  loss: 0.1473 (0.1550)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [ 30/689]  eta: 0:17:08  lr: 0.000095  loss: 0.1521 (0.1543)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [ 40/689]  eta: 0:16:55  lr: 0.000095  loss: 0.1422 (0.1523)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [ 50/689]  eta: 0:16:41  lr: 0.000095  loss: 0.1423 (0.1514)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [ 60/689]  eta: 0:16:26  lr: 0.000095  loss: 0.1430 (0.1505)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [ 70/689]  eta: 0:16:12  lr: 0.000095  loss: 0.1473 (0.1511)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [ 80/689]  eta: 0:15:56  lr: 0.000095  loss: 0.1484 (0.1515)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [ 90/689]  eta: 0:15:41  lr: 0.000095  loss: 0.1484 (0.1516)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [100/689]  eta: 0:15:26  lr: 0.000095  loss: 0.1499 (0.1520)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [110/689]  eta: 0:15:10  lr: 0.000095  loss: 0.1499 (0.1524)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [120/689]  eta: 0:14:55  lr: 0.000095  loss: 0.1497 (0.1522)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [130/689]  eta: 0:14:39  lr: 0.000095  loss: 0.1454 (0.1518)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [140/689]  eta: 0:14:24  lr: 0.000095  loss: 0.1454 (0.1520)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [150/689]  eta: 0:14:08  lr: 0.000095  loss: 0.1509 (0.1520)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [160/689]  eta: 0:13:53  lr: 0.000095  loss: 0.1529 (0.1523)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [170/689]  eta: 0:13:37  lr: 0.000095  loss: 0.1497 (0.1517)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [180/689]  eta: 0:13:21  lr: 0.000095  loss: 0.1422 (0.1514)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [190/689]  eta: 0:13:06  lr: 0.000095  loss: 0.1496 (0.1517)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [200/689]  eta: 0:12:50  lr: 0.000095  loss: 0.1507 (0.1517)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [210/689]  eta: 0:12:34  lr: 0.000095  loss: 0.1501 (0.1518)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [220/689]  eta: 0:12:19  lr: 0.000095  loss: 0.1477 (0.1518)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [230/689]  eta: 0:12:03  lr: 0.000095  loss: 0.1508 (0.1519)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [240/689]  eta: 0:11:47  lr: 0.000095  loss: 0.1578 (0.1521)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [250/689]  eta: 0:11:32  lr: 0.000095  loss: 0.1522 (0.1518)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [260/689]  eta: 0:11:16  lr: 0.000095  loss: 0.1474 (0.1519)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [270/689]  eta: 0:11:00  lr: 0.000095  loss: 0.1481 (0.1518)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [280/689]  eta: 0:10:44  lr: 0.000095  loss: 0.1543 (0.1520)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [290/689]  eta: 0:10:29  lr: 0.000095  loss: 0.1498 (0.1519)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [300/689]  eta: 0:10:13  lr: 0.000095  loss: 0.1423 (0.1517)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [310/689]  eta: 0:09:57  lr: 0.000095  loss: 0.1395 (0.1516)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [320/689]  eta: 0:09:41  lr: 0.000095  loss: 0.1411 (0.1516)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [330/689]  eta: 0:09:26  lr: 0.000095  loss: 0.1562 (0.1517)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [340/689]  eta: 0:09:10  lr: 0.000095  loss: 0.1555 (0.1519)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [350/689]  eta: 0:08:54  lr: 0.000095  loss: 0.1435 (0.1518)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [360/689]  eta: 0:08:38  lr: 0.000095  loss: 0.1434 (0.1517)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [370/689]  eta: 0:08:23  lr: 0.000095  loss: 0.1465 (0.1518)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [380/689]  eta: 0:08:07  lr: 0.000095  loss: 0.1533 (0.1521)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [390/689]  eta: 0:07:51  lr: 0.000095  loss: 0.1553 (0.1522)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [400/689]  eta: 0:07:35  lr: 0.000095  loss: 0.1497 (0.1523)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [410/689]  eta: 0:07:20  lr: 0.000095  loss: 0.1569 (0.1524)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [420/689]  eta: 0:07:04  lr: 0.000095  loss: 0.1429 (0.1522)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [430/689]  eta: 0:06:48  lr: 0.000095  loss: 0.1422 (0.1522)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [440/689]  eta: 0:06:32  lr: 0.000095  loss: 0.1575 (0.1527)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [450/689]  eta: 0:06:17  lr: 0.000095  loss: 0.1533 (0.1526)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [460/689]  eta: 0:06:01  lr: 0.000095  loss: 0.1453 (0.1526)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [470/689]  eta: 0:05:45  lr: 0.000095  loss: 0.1473 (0.1526)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [480/689]  eta: 0:05:29  lr: 0.000095  loss: 0.1492 (0.1527)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [490/689]  eta: 0:05:13  lr: 0.000095  loss: 0.1598 (0.1528)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [500/689]  eta: 0:04:58  lr: 0.000095  loss: 0.1620 (0.1534)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [510/689]  eta: 0:04:42  lr: 0.000095  loss: 0.1581 (0.1534)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [520/689]  eta: 0:04:26  lr: 0.000095  loss: 0.1511 (0.1534)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [530/689]  eta: 0:04:10  lr: 0.000095  loss: 0.1485 (0.1534)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [540/689]  eta: 0:03:55  lr: 0.000095  loss: 0.1433 (0.1534)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [550/689]  eta: 0:03:39  lr: 0.000095  loss: 0.1403 (0.1532)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [560/689]  eta: 0:03:23  lr: 0.000095  loss: 0.1424 (0.1532)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [570/689]  eta: 0:03:07  lr: 0.000095  loss: 0.1499 (0.1532)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [580/689]  eta: 0:02:52  lr: 0.000095  loss: 0.1471 (0.1531)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [590/689]  eta: 0:02:36  lr: 0.000095  loss: 0.1432 (0.1532)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [600/689]  eta: 0:02:20  lr: 0.000095  loss: 0.1432 (0.1532)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [610/689]  eta: 0:02:04  lr: 0.000095  loss: 0.1531 (0.1532)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [620/689]  eta: 0:01:48  lr: 0.000095  loss: 0.1543 (0.1531)  time: 1.5788  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:146]  [630/689]  eta: 0:01:33  lr: 0.000095  loss: 0.1534 (0.1532)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [640/689]  eta: 0:01:17  lr: 0.000095  loss: 0.1507 (0.1531)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [650/689]  eta: 0:01:01  lr: 0.000095  loss: 0.1504 (0.1531)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [660/689]  eta: 0:00:45  lr: 0.000095  loss: 0.1528 (0.1533)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [670/689]  eta: 0:00:29  lr: 0.000095  loss: 0.1503 (0.1533)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [680/689]  eta: 0:00:14  lr: 0.000095  loss: 0.1473 (0.1532)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146]  [688/689]  eta: 0:00:01  lr: 0.000095  loss: 0.1424 (0.1530)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:146] Total time: 0:18:07 (1.5784 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: 0.1424 (0.1530)\n",
      "Valid: [epoch:146]  [ 0/14]  eta: 0:00:14  loss: 0.1398 (0.1398)  time: 1.0187  data: 0.3596  max mem: 39763\n",
      "Valid: [epoch:146]  [13/14]  eta: 0:00:00  loss: 0.1410 (0.1428)  time: 0.1148  data: 0.0257  max mem: 39763\n",
      "Valid: [epoch:146] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.1410 (0.1428)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_146_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.143%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:147]  [  0/689]  eta: 0:12:04  lr: 0.000095  loss: 0.1273 (0.1273)  time: 1.0516  data: 0.5744  max mem: 39763\n",
      "Train: [epoch:147]  [ 10/689]  eta: 0:17:17  lr: 0.000095  loss: 0.1486 (0.1538)  time: 1.5278  data: 0.0523  max mem: 39763\n",
      "Train: [epoch:147]  [ 20/689]  eta: 0:17:17  lr: 0.000095  loss: 0.1486 (0.1544)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [ 30/689]  eta: 0:17:08  lr: 0.000095  loss: 0.1594 (0.1561)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [ 40/689]  eta: 0:16:55  lr: 0.000095  loss: 0.1571 (0.1563)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [ 50/689]  eta: 0:16:41  lr: 0.000095  loss: 0.1502 (0.1551)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [ 60/689]  eta: 0:16:26  lr: 0.000095  loss: 0.1406 (0.1531)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [ 70/689]  eta: 0:16:11  lr: 0.000095  loss: 0.1382 (0.1513)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [ 80/689]  eta: 0:15:56  lr: 0.000095  loss: 0.1402 (0.1506)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [ 90/689]  eta: 0:15:41  lr: 0.000095  loss: 0.1427 (0.1505)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [100/689]  eta: 0:15:26  lr: 0.000095  loss: 0.1467 (0.1508)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [110/689]  eta: 0:15:10  lr: 0.000095  loss: 0.1498 (0.1515)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [120/689]  eta: 0:14:55  lr: 0.000095  loss: 0.1516 (0.1521)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [130/689]  eta: 0:14:39  lr: 0.000095  loss: 0.1516 (0.1529)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [140/689]  eta: 0:14:24  lr: 0.000095  loss: 0.1560 (0.1531)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [150/689]  eta: 0:14:08  lr: 0.000095  loss: 0.1469 (0.1526)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [160/689]  eta: 0:13:53  lr: 0.000095  loss: 0.1425 (0.1524)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [170/689]  eta: 0:13:37  lr: 0.000095  loss: 0.1473 (0.1526)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [180/689]  eta: 0:13:21  lr: 0.000095  loss: 0.1495 (0.1527)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [190/689]  eta: 0:13:06  lr: 0.000095  loss: 0.1485 (0.1524)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [200/689]  eta: 0:12:50  lr: 0.000095  loss: 0.1488 (0.1523)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [210/689]  eta: 0:12:34  lr: 0.000095  loss: 0.1442 (0.1516)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [220/689]  eta: 0:12:19  lr: 0.000095  loss: 0.1388 (0.1513)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [230/689]  eta: 0:12:03  lr: 0.000095  loss: 0.1422 (0.1512)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [240/689]  eta: 0:11:47  lr: 0.000095  loss: 0.1490 (0.1513)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [250/689]  eta: 0:11:32  lr: 0.000095  loss: 0.1541 (0.1516)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [260/689]  eta: 0:11:16  lr: 0.000095  loss: 0.1516 (0.1516)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [270/689]  eta: 0:11:00  lr: 0.000095  loss: 0.1448 (0.1513)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [280/689]  eta: 0:10:44  lr: 0.000095  loss: 0.1409 (0.1513)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [290/689]  eta: 0:10:29  lr: 0.000095  loss: 0.1491 (0.1513)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [300/689]  eta: 0:10:13  lr: 0.000095  loss: 0.1489 (0.1515)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [310/689]  eta: 0:09:57  lr: 0.000095  loss: 0.1426 (0.1512)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [320/689]  eta: 0:09:41  lr: 0.000095  loss: 0.1426 (0.1511)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [330/689]  eta: 0:09:26  lr: 0.000095  loss: 0.1496 (0.1513)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [340/689]  eta: 0:09:10  lr: 0.000095  loss: 0.1533 (0.1514)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [350/689]  eta: 0:08:54  lr: 0.000095  loss: 0.1505 (0.1513)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [360/689]  eta: 0:08:38  lr: 0.000095  loss: 0.1460 (0.1511)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [370/689]  eta: 0:08:23  lr: 0.000095  loss: 0.1460 (0.1511)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [380/689]  eta: 0:08:07  lr: 0.000095  loss: 0.1435 (0.1509)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [390/689]  eta: 0:07:51  lr: 0.000095  loss: 0.1435 (0.1510)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [400/689]  eta: 0:07:35  lr: 0.000095  loss: 0.1548 (0.1512)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [410/689]  eta: 0:07:20  lr: 0.000095  loss: 0.1479 (0.1511)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [420/689]  eta: 0:07:04  lr: 0.000095  loss: 0.1448 (0.1510)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [430/689]  eta: 0:06:48  lr: 0.000095  loss: 0.1470 (0.1512)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [440/689]  eta: 0:06:32  lr: 0.000095  loss: 0.1528 (0.1514)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [450/689]  eta: 0:06:17  lr: 0.000095  loss: 0.1534 (0.1515)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [460/689]  eta: 0:06:01  lr: 0.000095  loss: 0.1531 (0.1515)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [470/689]  eta: 0:05:45  lr: 0.000095  loss: 0.1483 (0.1515)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [480/689]  eta: 0:05:29  lr: 0.000095  loss: 0.1503 (0.1516)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [490/689]  eta: 0:05:13  lr: 0.000095  loss: 0.1484 (0.1515)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [500/689]  eta: 0:04:58  lr: 0.000095  loss: 0.1401 (0.1515)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [510/689]  eta: 0:04:42  lr: 0.000095  loss: 0.1446 (0.1515)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [520/689]  eta: 0:04:26  lr: 0.000095  loss: 0.1476 (0.1515)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [530/689]  eta: 0:04:10  lr: 0.000095  loss: 0.1517 (0.1517)  time: 1.5793  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:147]  [540/689]  eta: 0:03:55  lr: 0.000095  loss: 0.1537 (0.1519)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [550/689]  eta: 0:03:39  lr: 0.000095  loss: 0.1484 (0.1518)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [560/689]  eta: 0:03:23  lr: 0.000095  loss: 0.1492 (0.1520)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [570/689]  eta: 0:03:07  lr: 0.000095  loss: 0.1532 (0.1520)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [580/689]  eta: 0:02:52  lr: 0.000095  loss: 0.1466 (0.1520)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [590/689]  eta: 0:02:36  lr: 0.000095  loss: 0.1482 (0.1520)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [600/689]  eta: 0:02:20  lr: 0.000095  loss: 0.1504 (0.1520)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [610/689]  eta: 0:02:04  lr: 0.000095  loss: 0.1493 (0.1520)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [620/689]  eta: 0:01:48  lr: 0.000095  loss: 0.1572 (0.1522)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [630/689]  eta: 0:01:33  lr: 0.000095  loss: 0.1585 (0.1523)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [640/689]  eta: 0:01:17  lr: 0.000095  loss: 0.1552 (0.1524)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [650/689]  eta: 0:01:01  lr: 0.000095  loss: 0.1513 (0.1524)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [660/689]  eta: 0:00:45  lr: 0.000095  loss: 0.1512 (0.1525)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [670/689]  eta: 0:00:29  lr: 0.000095  loss: 0.1508 (0.1524)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [680/689]  eta: 0:00:14  lr: 0.000095  loss: 0.1490 (0.1525)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147]  [688/689]  eta: 0:00:01  lr: 0.000095  loss: 0.1508 (0.1525)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:147] Total time: 0:18:07 (1.5784 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: 0.1508 (0.1525)\n",
      "Valid: [epoch:147]  [ 0/14]  eta: 0:00:14  loss: 0.1436 (0.1436)  time: 1.0141  data: 0.3464  max mem: 39763\n",
      "Valid: [epoch:147]  [13/14]  eta: 0:00:00  loss: 0.1453 (0.1473)  time: 0.1144  data: 0.0248  max mem: 39763\n",
      "Valid: [epoch:147] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.1453 (0.1473)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_147_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.147%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:148]  [  0/689]  eta: 0:11:09  lr: 0.000095  loss: 0.1934 (0.1934)  time: 0.9710  data: 0.4912  max mem: 39763\n",
      "Train: [epoch:148]  [ 10/689]  eta: 0:17:13  lr: 0.000095  loss: 0.1580 (0.1591)  time: 1.5223  data: 0.0447  max mem: 39763\n",
      "Train: [epoch:148]  [ 20/689]  eta: 0:17:16  lr: 0.000095  loss: 0.1473 (0.1516)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [ 30/689]  eta: 0:17:07  lr: 0.000095  loss: 0.1516 (0.1591)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [ 40/689]  eta: 0:16:54  lr: 0.000095  loss: 0.1626 (0.1595)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [ 50/689]  eta: 0:16:41  lr: 0.000095  loss: 0.1550 (0.1573)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [ 60/689]  eta: 0:16:26  lr: 0.000095  loss: 0.1503 (0.1564)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [ 70/689]  eta: 0:16:11  lr: 0.000095  loss: 0.1500 (0.1562)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [ 80/689]  eta: 0:15:56  lr: 0.000095  loss: 0.1495 (0.1551)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [ 90/689]  eta: 0:15:41  lr: 0.000095  loss: 0.1491 (0.1556)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [100/689]  eta: 0:15:26  lr: 0.000095  loss: 0.1513 (0.1563)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [110/689]  eta: 0:15:11  lr: 0.000095  loss: 0.1562 (0.1562)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [120/689]  eta: 0:14:55  lr: 0.000095  loss: 0.1555 (0.1565)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [130/689]  eta: 0:14:40  lr: 0.000095  loss: 0.1453 (0.1557)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [140/689]  eta: 0:14:24  lr: 0.000095  loss: 0.1453 (0.1557)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [150/689]  eta: 0:14:08  lr: 0.000095  loss: 0.1579 (0.1560)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [160/689]  eta: 0:13:53  lr: 0.000095  loss: 0.1486 (0.1555)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [170/689]  eta: 0:13:37  lr: 0.000095  loss: 0.1424 (0.1554)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [180/689]  eta: 0:13:22  lr: 0.000095  loss: 0.1488 (0.1551)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [190/689]  eta: 0:13:06  lr: 0.000095  loss: 0.1406 (0.1545)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [200/689]  eta: 0:12:50  lr: 0.000095  loss: 0.1462 (0.1551)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [210/689]  eta: 0:12:35  lr: 0.000095  loss: 0.1546 (0.1546)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [220/689]  eta: 0:12:19  lr: 0.000095  loss: 0.1520 (0.1547)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [230/689]  eta: 0:12:03  lr: 0.000095  loss: 0.1494 (0.1545)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [240/689]  eta: 0:11:47  lr: 0.000095  loss: 0.1502 (0.1545)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [250/689]  eta: 0:11:32  lr: 0.000095  loss: 0.1528 (0.1549)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [260/689]  eta: 0:11:16  lr: 0.000095  loss: 0.1512 (0.1547)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [270/689]  eta: 0:11:00  lr: 0.000095  loss: 0.1468 (0.1548)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [280/689]  eta: 0:10:44  lr: 0.000095  loss: 0.1492 (0.1549)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [290/689]  eta: 0:10:29  lr: 0.000095  loss: 0.1549 (0.1550)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [300/689]  eta: 0:10:13  lr: 0.000095  loss: 0.1565 (0.1553)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [310/689]  eta: 0:09:57  lr: 0.000095  loss: 0.1594 (0.1556)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [320/689]  eta: 0:09:41  lr: 0.000095  loss: 0.1632 (0.1558)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [330/689]  eta: 0:09:26  lr: 0.000095  loss: 0.1536 (0.1559)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [340/689]  eta: 0:09:10  lr: 0.000095  loss: 0.1475 (0.1558)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [350/689]  eta: 0:08:54  lr: 0.000095  loss: 0.1483 (0.1558)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [360/689]  eta: 0:08:38  lr: 0.000095  loss: 0.1588 (0.1558)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [370/689]  eta: 0:08:23  lr: 0.000095  loss: 0.1442 (0.1558)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [380/689]  eta: 0:08:07  lr: 0.000095  loss: 0.1450 (0.1562)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [390/689]  eta: 0:07:51  lr: 0.000095  loss: 0.1521 (0.1561)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [400/689]  eta: 0:07:35  lr: 0.000095  loss: 0.1521 (0.1563)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [410/689]  eta: 0:07:20  lr: 0.000095  loss: 0.1634 (0.1567)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [420/689]  eta: 0:07:04  lr: 0.000095  loss: 0.1554 (0.1566)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [430/689]  eta: 0:06:48  lr: 0.000095  loss: 0.1518 (0.1567)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [440/689]  eta: 0:06:32  lr: 0.000095  loss: 0.1557 (0.1567)  time: 1.5800  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:148]  [450/689]  eta: 0:06:17  lr: 0.000095  loss: 0.1517 (0.1567)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [460/689]  eta: 0:06:01  lr: 0.000095  loss: 0.1493 (0.1567)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [470/689]  eta: 0:05:45  lr: 0.000095  loss: 0.1489 (0.1567)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [480/689]  eta: 0:05:29  lr: 0.000095  loss: 0.1491 (0.1565)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [490/689]  eta: 0:05:14  lr: 0.000095  loss: 0.1510 (0.1567)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [500/689]  eta: 0:04:58  lr: 0.000095  loss: 0.1530 (0.1566)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [510/689]  eta: 0:04:42  lr: 0.000095  loss: 0.1530 (0.1565)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [520/689]  eta: 0:04:26  lr: 0.000095  loss: 0.1616 (0.1568)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [530/689]  eta: 0:04:10  lr: 0.000095  loss: 0.1596 (0.1568)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [540/689]  eta: 0:03:55  lr: 0.000095  loss: 0.1478 (0.1565)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [550/689]  eta: 0:03:39  lr: 0.000095  loss: 0.1419 (0.1564)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [560/689]  eta: 0:03:23  lr: 0.000095  loss: 0.1515 (0.1564)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [570/689]  eta: 0:03:07  lr: 0.000095  loss: 0.1485 (0.1563)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [580/689]  eta: 0:02:52  lr: 0.000095  loss: 0.1405 (0.1562)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [590/689]  eta: 0:02:36  lr: 0.000095  loss: 0.1480 (0.1562)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [600/689]  eta: 0:02:20  lr: 0.000095  loss: 0.1450 (0.1561)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [610/689]  eta: 0:02:04  lr: 0.000095  loss: 0.1411 (0.1560)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [620/689]  eta: 0:01:48  lr: 0.000095  loss: 0.1541 (0.1561)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [630/689]  eta: 0:01:33  lr: 0.000095  loss: 0.1541 (0.1559)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [640/689]  eta: 0:01:17  lr: 0.000095  loss: 0.1500 (0.1559)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [650/689]  eta: 0:01:01  lr: 0.000095  loss: 0.1532 (0.1558)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [660/689]  eta: 0:00:45  lr: 0.000095  loss: 0.1501 (0.1559)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [670/689]  eta: 0:00:29  lr: 0.000095  loss: 0.1501 (0.1559)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [680/689]  eta: 0:00:14  lr: 0.000095  loss: 0.1498 (0.1559)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148]  [688/689]  eta: 0:00:01  lr: 0.000095  loss: 0.1515 (0.1559)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:148] Total time: 0:18:07 (1.5789 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: 0.1515 (0.1559)\n",
      "Valid: [epoch:148]  [ 0/14]  eta: 0:00:14  loss: 0.1382 (0.1382)  time: 1.0116  data: 0.3874  max mem: 39763\n",
      "Valid: [epoch:148]  [13/14]  eta: 0:00:00  loss: 0.1489 (0.1504)  time: 0.1143  data: 0.0277  max mem: 39763\n",
      "Valid: [epoch:148] Total time: 0:00:01 (0.1234 s / it)\n",
      "Averaged stats: loss: 0.1489 (0.1504)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_148_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.150%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:149]  [  0/689]  eta: 0:11:38  lr: 0.000095  loss: 0.1577 (0.1577)  time: 1.0140  data: 0.5350  max mem: 39763\n",
      "Train: [epoch:149]  [ 10/689]  eta: 0:17:16  lr: 0.000095  loss: 0.1551 (0.1531)  time: 1.5262  data: 0.0487  max mem: 39763\n",
      "Train: [epoch:149]  [ 20/689]  eta: 0:17:17  lr: 0.000095  loss: 0.1487 (0.1536)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [ 30/689]  eta: 0:17:08  lr: 0.000095  loss: 0.1576 (0.1551)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [ 40/689]  eta: 0:16:55  lr: 0.000095  loss: 0.1633 (0.1580)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [ 50/689]  eta: 0:16:41  lr: 0.000095  loss: 0.1707 (0.1597)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [ 60/689]  eta: 0:16:27  lr: 0.000095  loss: 0.1549 (0.1590)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [ 70/689]  eta: 0:16:12  lr: 0.000095  loss: 0.1560 (0.1622)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [ 80/689]  eta: 0:15:57  lr: 0.000095  loss: 0.1614 (0.1620)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [ 90/689]  eta: 0:15:41  lr: 0.000095  loss: 0.1659 (0.1626)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [100/689]  eta: 0:15:26  lr: 0.000095  loss: 0.1672 (0.1628)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [110/689]  eta: 0:15:11  lr: 0.000095  loss: 0.1526 (0.1630)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [120/689]  eta: 0:14:55  lr: 0.000095  loss: 0.1538 (0.1624)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [130/689]  eta: 0:14:40  lr: 0.000095  loss: 0.1555 (0.1628)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [140/689]  eta: 0:14:24  lr: 0.000095  loss: 0.1514 (0.1620)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [150/689]  eta: 0:14:09  lr: 0.000095  loss: 0.1499 (0.1612)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [160/689]  eta: 0:13:53  lr: 0.000095  loss: 0.1564 (0.1618)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [170/689]  eta: 0:13:37  lr: 0.000095  loss: 0.1605 (0.1617)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [180/689]  eta: 0:13:22  lr: 0.000095  loss: 0.1518 (0.1611)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [190/689]  eta: 0:13:06  lr: 0.000095  loss: 0.1552 (0.1612)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [200/689]  eta: 0:12:50  lr: 0.000095  loss: 0.1590 (0.1608)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [210/689]  eta: 0:12:35  lr: 0.000095  loss: 0.1538 (0.1607)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [220/689]  eta: 0:12:19  lr: 0.000095  loss: 0.1461 (0.1604)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [230/689]  eta: 0:12:03  lr: 0.000095  loss: 0.1507 (0.1606)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [240/689]  eta: 0:11:47  lr: 0.000095  loss: 0.1542 (0.1604)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [250/689]  eta: 0:11:32  lr: 0.000095  loss: 0.1621 (0.1608)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [260/689]  eta: 0:11:16  lr: 0.000095  loss: 0.1606 (0.1608)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [270/689]  eta: 0:11:00  lr: 0.000095  loss: 0.1505 (0.1604)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [280/689]  eta: 0:10:44  lr: 0.000095  loss: 0.1506 (0.1604)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [290/689]  eta: 0:10:29  lr: 0.000095  loss: 0.1506 (0.1599)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [300/689]  eta: 0:10:13  lr: 0.000095  loss: 0.1450 (0.1596)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [310/689]  eta: 0:09:57  lr: 0.000095  loss: 0.1450 (0.1595)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [320/689]  eta: 0:09:41  lr: 0.000095  loss: 0.1522 (0.1593)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [330/689]  eta: 0:09:26  lr: 0.000095  loss: 0.1534 (0.1592)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [340/689]  eta: 0:09:10  lr: 0.000095  loss: 0.1523 (0.1589)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [350/689]  eta: 0:08:54  lr: 0.000095  loss: 0.1446 (0.1589)  time: 1.5793  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:149]  [360/689]  eta: 0:08:38  lr: 0.000095  loss: 0.1504 (0.1586)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [370/689]  eta: 0:08:23  lr: 0.000095  loss: 0.1467 (0.1584)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [380/689]  eta: 0:08:07  lr: 0.000095  loss: 0.1464 (0.1582)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [390/689]  eta: 0:07:51  lr: 0.000095  loss: 0.1525 (0.1583)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [400/689]  eta: 0:07:35  lr: 0.000095  loss: 0.1517 (0.1583)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [410/689]  eta: 0:07:20  lr: 0.000095  loss: 0.1505 (0.1581)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [420/689]  eta: 0:07:04  lr: 0.000095  loss: 0.1532 (0.1579)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [430/689]  eta: 0:06:48  lr: 0.000095  loss: 0.1485 (0.1578)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [440/689]  eta: 0:06:32  lr: 0.000095  loss: 0.1485 (0.1578)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [450/689]  eta: 0:06:17  lr: 0.000095  loss: 0.1540 (0.1577)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [460/689]  eta: 0:06:01  lr: 0.000095  loss: 0.1540 (0.1578)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [470/689]  eta: 0:05:45  lr: 0.000095  loss: 0.1658 (0.1579)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [480/689]  eta: 0:05:29  lr: 0.000095  loss: 0.1554 (0.1579)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [490/689]  eta: 0:05:14  lr: 0.000095  loss: 0.1535 (0.1579)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [500/689]  eta: 0:04:58  lr: 0.000095  loss: 0.1560 (0.1579)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [510/689]  eta: 0:04:42  lr: 0.000095  loss: 0.1590 (0.1579)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [520/689]  eta: 0:04:26  lr: 0.000095  loss: 0.1557 (0.1581)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [530/689]  eta: 0:04:10  lr: 0.000095  loss: 0.1489 (0.1580)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [540/689]  eta: 0:03:55  lr: 0.000095  loss: 0.1486 (0.1578)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [550/689]  eta: 0:03:39  lr: 0.000095  loss: 0.1491 (0.1577)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [560/689]  eta: 0:03:23  lr: 0.000095  loss: 0.1517 (0.1578)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [570/689]  eta: 0:03:07  lr: 0.000095  loss: 0.1560 (0.1579)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [580/689]  eta: 0:02:52  lr: 0.000095  loss: 0.1417 (0.1576)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [590/689]  eta: 0:02:36  lr: 0.000095  loss: 0.1412 (0.1575)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [600/689]  eta: 0:02:20  lr: 0.000095  loss: 0.1442 (0.1575)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [610/689]  eta: 0:02:04  lr: 0.000095  loss: 0.1442 (0.1573)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [620/689]  eta: 0:01:48  lr: 0.000095  loss: 0.1556 (0.1574)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [630/689]  eta: 0:01:33  lr: 0.000095  loss: 0.1593 (0.1573)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [640/689]  eta: 0:01:17  lr: 0.000095  loss: 0.1546 (0.1572)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [650/689]  eta: 0:01:01  lr: 0.000095  loss: 0.1494 (0.1571)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [660/689]  eta: 0:00:45  lr: 0.000095  loss: 0.1461 (0.1571)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [670/689]  eta: 0:00:30  lr: 0.000095  loss: 0.1465 (0.1569)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [680/689]  eta: 0:00:14  lr: 0.000095  loss: 0.1481 (0.1569)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149]  [688/689]  eta: 0:00:01  lr: 0.000095  loss: 0.1481 (0.1568)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:149] Total time: 0:18:08 (1.5795 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: 0.1481 (0.1568)\n",
      "Valid: [epoch:149]  [ 0/14]  eta: 0:00:14  loss: 0.1581 (0.1581)  time: 1.0227  data: 0.3706  max mem: 39763\n",
      "Valid: [epoch:149]  [13/14]  eta: 0:00:00  loss: 0.1426 (0.1451)  time: 0.1151  data: 0.0265  max mem: 39763\n",
      "Valid: [epoch:149] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.1426 (0.1451)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_149_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.145%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:150]  [  0/689]  eta: 0:11:35  lr: 0.000095  loss: 0.1366 (0.1366)  time: 1.0095  data: 0.5325  max mem: 39763\n",
      "Train: [epoch:150]  [ 10/689]  eta: 0:17:17  lr: 0.000095  loss: 0.1536 (0.1544)  time: 1.5281  data: 0.0485  max mem: 39763\n",
      "Train: [epoch:150]  [ 20/689]  eta: 0:17:18  lr: 0.000095  loss: 0.1536 (0.1561)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [ 30/689]  eta: 0:17:08  lr: 0.000095  loss: 0.1554 (0.1590)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [ 40/689]  eta: 0:16:56  lr: 0.000095  loss: 0.1554 (0.1568)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [ 50/689]  eta: 0:16:42  lr: 0.000095  loss: 0.1477 (0.1561)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [ 60/689]  eta: 0:16:27  lr: 0.000095  loss: 0.1537 (0.1561)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [ 70/689]  eta: 0:16:12  lr: 0.000095  loss: 0.1539 (0.1563)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [ 80/689]  eta: 0:15:57  lr: 0.000095  loss: 0.1500 (0.1555)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [ 90/689]  eta: 0:15:42  lr: 0.000095  loss: 0.1498 (0.1554)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [100/689]  eta: 0:15:27  lr: 0.000095  loss: 0.1515 (0.1551)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [110/689]  eta: 0:15:11  lr: 0.000095  loss: 0.1515 (0.1554)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [120/689]  eta: 0:14:56  lr: 0.000095  loss: 0.1500 (0.1551)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [130/689]  eta: 0:14:40  lr: 0.000095  loss: 0.1488 (0.1550)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [140/689]  eta: 0:14:24  lr: 0.000095  loss: 0.1530 (0.1550)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [150/689]  eta: 0:14:09  lr: 0.000095  loss: 0.1511 (0.1549)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [160/689]  eta: 0:13:53  lr: 0.000095  loss: 0.1511 (0.1548)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [170/689]  eta: 0:13:38  lr: 0.000095  loss: 0.1583 (0.1554)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [180/689]  eta: 0:13:22  lr: 0.000095  loss: 0.1591 (0.1557)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [190/689]  eta: 0:13:06  lr: 0.000095  loss: 0.1600 (0.1566)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [200/689]  eta: 0:12:50  lr: 0.000095  loss: 0.1557 (0.1563)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [210/689]  eta: 0:12:35  lr: 0.000095  loss: 0.1475 (0.1560)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [220/689]  eta: 0:12:19  lr: 0.000095  loss: 0.1468 (0.1561)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [230/689]  eta: 0:12:03  lr: 0.000095  loss: 0.1560 (0.1564)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [240/689]  eta: 0:11:48  lr: 0.000095  loss: 0.1580 (0.1567)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [250/689]  eta: 0:11:32  lr: 0.000095  loss: 0.1531 (0.1567)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [260/689]  eta: 0:11:16  lr: 0.000095  loss: 0.1512 (0.1568)  time: 1.5796  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:150]  [270/689]  eta: 0:11:00  lr: 0.000095  loss: 0.1512 (0.1566)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [280/689]  eta: 0:10:45  lr: 0.000095  loss: 0.1503 (0.1565)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [290/689]  eta: 0:10:29  lr: 0.000095  loss: 0.1510 (0.1567)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [300/689]  eta: 0:10:13  lr: 0.000095  loss: 0.1632 (0.1571)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [310/689]  eta: 0:09:57  lr: 0.000095  loss: 0.1645 (0.1572)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [320/689]  eta: 0:09:42  lr: 0.000095  loss: 0.1539 (0.1572)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [330/689]  eta: 0:09:26  lr: 0.000095  loss: 0.1543 (0.1574)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [340/689]  eta: 0:09:10  lr: 0.000095  loss: 0.1575 (0.1573)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [350/689]  eta: 0:08:54  lr: 0.000095  loss: 0.1611 (0.1575)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [360/689]  eta: 0:08:39  lr: 0.000095  loss: 0.1611 (0.1576)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [370/689]  eta: 0:08:23  lr: 0.000095  loss: 0.1526 (0.1574)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [380/689]  eta: 0:08:07  lr: 0.000095  loss: 0.1498 (0.1572)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [390/689]  eta: 0:07:51  lr: 0.000095  loss: 0.1503 (0.1572)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [400/689]  eta: 0:07:36  lr: 0.000095  loss: 0.1609 (0.1572)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [410/689]  eta: 0:07:20  lr: 0.000095  loss: 0.1595 (0.1573)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [420/689]  eta: 0:07:04  lr: 0.000095  loss: 0.1591 (0.1573)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [430/689]  eta: 0:06:48  lr: 0.000095  loss: 0.1555 (0.1575)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [440/689]  eta: 0:06:32  lr: 0.000095  loss: 0.1509 (0.1572)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [450/689]  eta: 0:06:17  lr: 0.000095  loss: 0.1430 (0.1571)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [460/689]  eta: 0:06:01  lr: 0.000095  loss: 0.1481 (0.1570)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [470/689]  eta: 0:05:45  lr: 0.000095  loss: 0.1481 (0.1570)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [480/689]  eta: 0:05:29  lr: 0.000095  loss: 0.1441 (0.1569)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [490/689]  eta: 0:05:14  lr: 0.000095  loss: 0.1522 (0.1570)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [500/689]  eta: 0:04:58  lr: 0.000095  loss: 0.1538 (0.1570)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [510/689]  eta: 0:04:42  lr: 0.000095  loss: 0.1494 (0.1568)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [520/689]  eta: 0:04:26  lr: 0.000095  loss: 0.1511 (0.1568)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [530/689]  eta: 0:04:10  lr: 0.000095  loss: 0.1599 (0.1570)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [540/689]  eta: 0:03:55  lr: 0.000095  loss: 0.1542 (0.1569)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [550/689]  eta: 0:03:39  lr: 0.000095  loss: 0.1469 (0.1569)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [560/689]  eta: 0:03:23  lr: 0.000095  loss: 0.1469 (0.1570)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [570/689]  eta: 0:03:07  lr: 0.000095  loss: 0.1512 (0.1570)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [580/689]  eta: 0:02:52  lr: 0.000095  loss: 0.1558 (0.1570)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [590/689]  eta: 0:02:36  lr: 0.000095  loss: 0.1502 (0.1569)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [600/689]  eta: 0:02:20  lr: 0.000095  loss: 0.1502 (0.1569)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [610/689]  eta: 0:02:04  lr: 0.000095  loss: 0.1516 (0.1569)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [620/689]  eta: 0:01:48  lr: 0.000095  loss: 0.1523 (0.1570)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [630/689]  eta: 0:01:33  lr: 0.000095  loss: 0.1526 (0.1569)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [640/689]  eta: 0:01:17  lr: 0.000095  loss: 0.1535 (0.1569)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [650/689]  eta: 0:01:01  lr: 0.000095  loss: 0.1535 (0.1568)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [660/689]  eta: 0:00:45  lr: 0.000095  loss: 0.1557 (0.1570)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [670/689]  eta: 0:00:29  lr: 0.000095  loss: 0.1553 (0.1569)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [680/689]  eta: 0:00:14  lr: 0.000095  loss: 0.1584 (0.1571)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150]  [688/689]  eta: 0:00:01  lr: 0.000095  loss: 0.1553 (0.1569)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:150] Total time: 0:18:07 (1.5786 s / it)\n",
      "Averaged stats: lr: 0.000095  loss: 0.1553 (0.1569)\n",
      "Valid: [epoch:150]  [ 0/14]  eta: 0:00:13  loss: 0.1332 (0.1332)  time: 0.9892  data: 0.4804  max mem: 39763\n",
      "Valid: [epoch:150]  [13/14]  eta: 0:00:00  loss: 0.1458 (0.1463)  time: 0.1126  data: 0.0344  max mem: 39763\n",
      "Valid: [epoch:150] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.1458 (0.1463)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_150_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.146%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:151]  [  0/689]  eta: 0:11:37  lr: 0.000094  loss: 0.1559 (0.1559)  time: 1.0129  data: 0.5353  max mem: 39763\n",
      "Train: [epoch:151]  [ 10/689]  eta: 0:17:16  lr: 0.000094  loss: 0.1566 (0.1624)  time: 1.5262  data: 0.0487  max mem: 39763\n",
      "Train: [epoch:151]  [ 20/689]  eta: 0:17:17  lr: 0.000094  loss: 0.1559 (0.1575)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [ 30/689]  eta: 0:17:08  lr: 0.000094  loss: 0.1505 (0.1593)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [ 40/689]  eta: 0:16:55  lr: 0.000094  loss: 0.1465 (0.1562)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [ 50/689]  eta: 0:16:41  lr: 0.000094  loss: 0.1465 (0.1556)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [ 60/689]  eta: 0:16:26  lr: 0.000094  loss: 0.1484 (0.1557)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [ 70/689]  eta: 0:16:12  lr: 0.000094  loss: 0.1526 (0.1570)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [ 80/689]  eta: 0:15:57  lr: 0.000094  loss: 0.1519 (0.1566)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [ 90/689]  eta: 0:15:41  lr: 0.000094  loss: 0.1519 (0.1566)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [100/689]  eta: 0:15:26  lr: 0.000094  loss: 0.1519 (0.1561)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [110/689]  eta: 0:15:11  lr: 0.000094  loss: 0.1532 (0.1573)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [120/689]  eta: 0:14:55  lr: 0.000094  loss: 0.1532 (0.1569)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [130/689]  eta: 0:14:40  lr: 0.000094  loss: 0.1477 (0.1566)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [140/689]  eta: 0:14:24  lr: 0.000094  loss: 0.1629 (0.1583)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [150/689]  eta: 0:14:08  lr: 0.000094  loss: 0.1608 (0.1581)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [160/689]  eta: 0:13:53  lr: 0.000094  loss: 0.1574 (0.1591)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [170/689]  eta: 0:13:37  lr: 0.000094  loss: 0.1616 (0.1593)  time: 1.5801  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:151]  [180/689]  eta: 0:13:22  lr: 0.000094  loss: 0.1607 (0.1601)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [190/689]  eta: 0:13:06  lr: 0.000094  loss: 0.1607 (0.1600)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [200/689]  eta: 0:12:50  lr: 0.000094  loss: 0.1514 (0.1602)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [210/689]  eta: 0:12:35  lr: 0.000094  loss: 0.1492 (0.1594)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [220/689]  eta: 0:12:19  lr: 0.000094  loss: 0.1522 (0.1598)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [230/689]  eta: 0:12:03  lr: 0.000094  loss: 0.1609 (0.1601)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [240/689]  eta: 0:11:47  lr: 0.000094  loss: 0.1584 (0.1600)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [250/689]  eta: 0:11:32  lr: 0.000094  loss: 0.1584 (0.1604)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [260/689]  eta: 0:11:16  lr: 0.000094  loss: 0.1678 (0.1605)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [270/689]  eta: 0:11:00  lr: 0.000094  loss: 0.1514 (0.1601)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [280/689]  eta: 0:10:45  lr: 0.000094  loss: 0.1474 (0.1596)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [290/689]  eta: 0:10:29  lr: 0.000094  loss: 0.1495 (0.1594)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [300/689]  eta: 0:10:13  lr: 0.000094  loss: 0.1515 (0.1592)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [310/689]  eta: 0:09:57  lr: 0.000094  loss: 0.1431 (0.1588)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [320/689]  eta: 0:09:42  lr: 0.000094  loss: 0.1521 (0.1589)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [330/689]  eta: 0:09:26  lr: 0.000094  loss: 0.1610 (0.1592)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [340/689]  eta: 0:09:10  lr: 0.000094  loss: 0.1665 (0.1595)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [350/689]  eta: 0:08:54  lr: 0.000094  loss: 0.1531 (0.1595)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [360/689]  eta: 0:08:39  lr: 0.000094  loss: 0.1531 (0.1593)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [370/689]  eta: 0:08:23  lr: 0.000094  loss: 0.1536 (0.1593)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [380/689]  eta: 0:08:07  lr: 0.000094  loss: 0.1511 (0.1591)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [390/689]  eta: 0:07:51  lr: 0.000094  loss: 0.1503 (0.1588)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [400/689]  eta: 0:07:35  lr: 0.000094  loss: 0.1582 (0.1590)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [410/689]  eta: 0:07:20  lr: 0.000094  loss: 0.1545 (0.1589)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [420/689]  eta: 0:07:04  lr: 0.000094  loss: 0.1497 (0.1586)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [430/689]  eta: 0:06:48  lr: 0.000094  loss: 0.1466 (0.1585)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [440/689]  eta: 0:06:32  lr: 0.000094  loss: 0.1512 (0.1584)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [450/689]  eta: 0:06:17  lr: 0.000094  loss: 0.1512 (0.1582)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [460/689]  eta: 0:06:01  lr: 0.000094  loss: 0.1578 (0.1586)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [470/689]  eta: 0:05:45  lr: 0.000094  loss: 0.1639 (0.1586)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [480/689]  eta: 0:05:29  lr: 0.000094  loss: 0.1541 (0.1585)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [490/689]  eta: 0:05:14  lr: 0.000094  loss: 0.1499 (0.1584)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [500/689]  eta: 0:04:58  lr: 0.000094  loss: 0.1540 (0.1584)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [510/689]  eta: 0:04:42  lr: 0.000094  loss: 0.1474 (0.1581)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [520/689]  eta: 0:04:26  lr: 0.000094  loss: 0.1478 (0.1583)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [530/689]  eta: 0:04:10  lr: 0.000094  loss: 0.1585 (0.1583)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [540/689]  eta: 0:03:55  lr: 0.000094  loss: 0.1536 (0.1583)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [550/689]  eta: 0:03:39  lr: 0.000094  loss: 0.1553 (0.1582)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [560/689]  eta: 0:03:23  lr: 0.000094  loss: 0.1596 (0.1583)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [570/689]  eta: 0:03:07  lr: 0.000094  loss: 0.1596 (0.1582)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [580/689]  eta: 0:02:52  lr: 0.000094  loss: 0.1540 (0.1582)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [590/689]  eta: 0:02:36  lr: 0.000094  loss: 0.1525 (0.1582)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [600/689]  eta: 0:02:20  lr: 0.000094  loss: 0.1472 (0.1582)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [610/689]  eta: 0:02:04  lr: 0.000094  loss: 0.1578 (0.1583)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [620/689]  eta: 0:01:48  lr: 0.000094  loss: 0.1520 (0.1582)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [630/689]  eta: 0:01:33  lr: 0.000094  loss: 0.1503 (0.1582)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [640/689]  eta: 0:01:17  lr: 0.000094  loss: 0.1503 (0.1582)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [650/689]  eta: 0:01:01  lr: 0.000094  loss: 0.1533 (0.1582)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [660/689]  eta: 0:00:45  lr: 0.000094  loss: 0.1501 (0.1581)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [670/689]  eta: 0:00:29  lr: 0.000094  loss: 0.1470 (0.1580)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [680/689]  eta: 0:00:14  lr: 0.000094  loss: 0.1480 (0.1580)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151]  [688/689]  eta: 0:00:01  lr: 0.000094  loss: 0.1629 (0.1581)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:151] Total time: 0:18:07 (1.5785 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: 0.1629 (0.1581)\n",
      "Valid: [epoch:151]  [ 0/14]  eta: 0:00:14  loss: 0.1403 (0.1403)  time: 1.0215  data: 0.3784  max mem: 39763\n",
      "Valid: [epoch:151]  [13/14]  eta: 0:00:00  loss: 0.1449 (0.1465)  time: 0.1149  data: 0.0271  max mem: 39763\n",
      "Valid: [epoch:151] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.1449 (0.1465)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_151_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.147%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:152]  [  0/689]  eta: 0:11:43  lr: 0.000094  loss: 0.1392 (0.1392)  time: 1.0214  data: 0.5417  max mem: 39763\n",
      "Train: [epoch:152]  [ 10/689]  eta: 0:17:16  lr: 0.000094  loss: 0.1574 (0.1561)  time: 1.5265  data: 0.0493  max mem: 39763\n",
      "Train: [epoch:152]  [ 20/689]  eta: 0:17:17  lr: 0.000094  loss: 0.1559 (0.1544)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [ 30/689]  eta: 0:17:08  lr: 0.000094  loss: 0.1559 (0.1564)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [ 40/689]  eta: 0:16:55  lr: 0.000094  loss: 0.1540 (0.1557)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [ 50/689]  eta: 0:16:41  lr: 0.000094  loss: 0.1540 (0.1567)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [ 60/689]  eta: 0:16:26  lr: 0.000094  loss: 0.1548 (0.1562)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [ 70/689]  eta: 0:16:12  lr: 0.000094  loss: 0.1498 (0.1556)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [ 80/689]  eta: 0:15:57  lr: 0.000094  loss: 0.1437 (0.1550)  time: 1.5788  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:152]  [ 90/689]  eta: 0:15:41  lr: 0.000094  loss: 0.1487 (0.1555)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [100/689]  eta: 0:15:26  lr: 0.000094  loss: 0.1541 (0.1565)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [110/689]  eta: 0:15:11  lr: 0.000094  loss: 0.1529 (0.1565)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [120/689]  eta: 0:14:55  lr: 0.000094  loss: 0.1480 (0.1566)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [130/689]  eta: 0:14:39  lr: 0.000094  loss: 0.1562 (0.1567)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [140/689]  eta: 0:14:24  lr: 0.000094  loss: 0.1562 (0.1568)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [150/689]  eta: 0:14:08  lr: 0.000094  loss: 0.1562 (0.1572)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [160/689]  eta: 0:13:53  lr: 0.000094  loss: 0.1507 (0.1572)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [170/689]  eta: 0:13:37  lr: 0.000094  loss: 0.1507 (0.1574)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [180/689]  eta: 0:13:21  lr: 0.000094  loss: 0.1553 (0.1578)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [190/689]  eta: 0:13:06  lr: 0.000094  loss: 0.1553 (0.1580)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [200/689]  eta: 0:12:50  lr: 0.000094  loss: 0.1506 (0.1575)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [210/689]  eta: 0:12:34  lr: 0.000094  loss: 0.1499 (0.1576)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [220/689]  eta: 0:12:19  lr: 0.000094  loss: 0.1499 (0.1575)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [230/689]  eta: 0:12:03  lr: 0.000094  loss: 0.1595 (0.1576)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [240/689]  eta: 0:11:47  lr: 0.000094  loss: 0.1581 (0.1578)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [250/689]  eta: 0:11:32  lr: 0.000094  loss: 0.1549 (0.1574)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [260/689]  eta: 0:11:16  lr: 0.000094  loss: 0.1470 (0.1571)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [270/689]  eta: 0:11:00  lr: 0.000094  loss: 0.1439 (0.1568)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [280/689]  eta: 0:10:44  lr: 0.000094  loss: 0.1567 (0.1571)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [290/689]  eta: 0:10:29  lr: 0.000094  loss: 0.1619 (0.1570)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [300/689]  eta: 0:10:13  lr: 0.000094  loss: 0.1563 (0.1571)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [310/689]  eta: 0:09:57  lr: 0.000094  loss: 0.1563 (0.1574)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [320/689]  eta: 0:09:41  lr: 0.000094  loss: 0.1559 (0.1574)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [330/689]  eta: 0:09:26  lr: 0.000094  loss: 0.1626 (0.1580)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [340/689]  eta: 0:09:10  lr: 0.000094  loss: 0.1626 (0.1582)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [350/689]  eta: 0:08:54  lr: 0.000094  loss: 0.1540 (0.1582)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [360/689]  eta: 0:08:38  lr: 0.000094  loss: 0.1553 (0.1585)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [370/689]  eta: 0:08:23  lr: 0.000094  loss: 0.1549 (0.1584)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [380/689]  eta: 0:08:07  lr: 0.000094  loss: 0.1520 (0.1584)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [390/689]  eta: 0:07:51  lr: 0.000094  loss: 0.1564 (0.1584)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [400/689]  eta: 0:07:35  lr: 0.000094  loss: 0.1595 (0.1587)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [410/689]  eta: 0:07:20  lr: 0.000094  loss: 0.1729 (0.1590)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [420/689]  eta: 0:07:04  lr: 0.000094  loss: 0.1542 (0.1587)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [430/689]  eta: 0:06:48  lr: 0.000094  loss: 0.1533 (0.1591)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [440/689]  eta: 0:06:32  lr: 0.000094  loss: 0.1555 (0.1590)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [450/689]  eta: 0:06:17  lr: 0.000094  loss: 0.1555 (0.1590)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [460/689]  eta: 0:06:01  lr: 0.000094  loss: 0.1579 (0.1592)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [470/689]  eta: 0:05:45  lr: 0.000094  loss: 0.1544 (0.1590)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [480/689]  eta: 0:05:29  lr: 0.000094  loss: 0.1492 (0.1589)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [490/689]  eta: 0:05:13  lr: 0.000094  loss: 0.1475 (0.1588)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [500/689]  eta: 0:04:58  lr: 0.000094  loss: 0.1514 (0.1588)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [510/689]  eta: 0:04:42  lr: 0.000094  loss: 0.1614 (0.1588)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [520/689]  eta: 0:04:26  lr: 0.000094  loss: 0.1555 (0.1587)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [530/689]  eta: 0:04:10  lr: 0.000094  loss: 0.1516 (0.1587)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [540/689]  eta: 0:03:55  lr: 0.000094  loss: 0.1483 (0.1586)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [550/689]  eta: 0:03:39  lr: 0.000094  loss: 0.1483 (0.1585)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [560/689]  eta: 0:03:23  lr: 0.000094  loss: 0.1484 (0.1585)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [570/689]  eta: 0:03:07  lr: 0.000094  loss: 0.1562 (0.1585)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [580/689]  eta: 0:02:51  lr: 0.000094  loss: 0.1555 (0.1584)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [590/689]  eta: 0:02:36  lr: 0.000094  loss: 0.1489 (0.1584)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [600/689]  eta: 0:02:20  lr: 0.000094  loss: 0.1553 (0.1584)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [610/689]  eta: 0:02:04  lr: 0.000094  loss: 0.1553 (0.1583)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [620/689]  eta: 0:01:48  lr: 0.000094  loss: 0.1469 (0.1582)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [630/689]  eta: 0:01:33  lr: 0.000094  loss: 0.1469 (0.1582)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [640/689]  eta: 0:01:17  lr: 0.000094  loss: 0.1618 (0.1582)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [650/689]  eta: 0:01:01  lr: 0.000094  loss: 0.1600 (0.1583)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [660/689]  eta: 0:00:45  lr: 0.000094  loss: 0.1463 (0.1581)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [670/689]  eta: 0:00:29  lr: 0.000094  loss: 0.1514 (0.1582)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [680/689]  eta: 0:00:14  lr: 0.000094  loss: 0.1617 (0.1583)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152]  [688/689]  eta: 0:00:01  lr: 0.000094  loss: 0.1554 (0.1583)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:152] Total time: 0:18:07 (1.5782 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: 0.1554 (0.1583)\n",
      "Valid: [epoch:152]  [ 0/14]  eta: 0:00:14  loss: 0.1461 (0.1461)  time: 1.0359  data: 0.3815  max mem: 39763\n",
      "Valid: [epoch:152]  [13/14]  eta: 0:00:00  loss: 0.1461 (0.1476)  time: 0.1160  data: 0.0273  max mem: 39763\n",
      "Valid: [epoch:152] Total time: 0:00:01 (0.1250 s / it)\n",
      "Averaged stats: loss: 0.1461 (0.1476)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_152_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.148%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:153]  [  0/689]  eta: 0:11:30  lr: 0.000094  loss: 0.1795 (0.1795)  time: 1.0018  data: 0.5246  max mem: 39763\n",
      "Train: [epoch:153]  [ 10/689]  eta: 0:17:14  lr: 0.000094  loss: 0.1543 (0.1586)  time: 1.5243  data: 0.0478  max mem: 39763\n",
      "Train: [epoch:153]  [ 20/689]  eta: 0:17:16  lr: 0.000094  loss: 0.1510 (0.1550)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [ 30/689]  eta: 0:17:07  lr: 0.000094  loss: 0.1510 (0.1591)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [ 40/689]  eta: 0:16:54  lr: 0.000094  loss: 0.1582 (0.1604)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [ 50/689]  eta: 0:16:40  lr: 0.000094  loss: 0.1555 (0.1588)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [ 60/689]  eta: 0:16:26  lr: 0.000094  loss: 0.1577 (0.1593)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [ 70/689]  eta: 0:16:11  lr: 0.000094  loss: 0.1675 (0.1611)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [ 80/689]  eta: 0:15:56  lr: 0.000094  loss: 0.1658 (0.1611)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [ 90/689]  eta: 0:15:41  lr: 0.000094  loss: 0.1618 (0.1617)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [100/689]  eta: 0:15:26  lr: 0.000094  loss: 0.1656 (0.1622)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [110/689]  eta: 0:15:10  lr: 0.000094  loss: 0.1596 (0.1618)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [120/689]  eta: 0:14:55  lr: 0.000094  loss: 0.1644 (0.1632)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [130/689]  eta: 0:14:39  lr: 0.000094  loss: 0.1670 (0.1634)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [140/689]  eta: 0:14:24  lr: 0.000094  loss: 0.1639 (0.1633)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [150/689]  eta: 0:14:08  lr: 0.000094  loss: 0.1515 (0.1627)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [160/689]  eta: 0:13:53  lr: 0.000094  loss: 0.1499 (0.1629)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [170/689]  eta: 0:13:37  lr: 0.000094  loss: 0.1590 (0.1630)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [180/689]  eta: 0:13:21  lr: 0.000094  loss: 0.1623 (0.1627)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [190/689]  eta: 0:13:06  lr: 0.000094  loss: 0.1571 (0.1625)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [200/689]  eta: 0:12:50  lr: 0.000094  loss: 0.1513 (0.1621)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [210/689]  eta: 0:12:34  lr: 0.000094  loss: 0.1513 (0.1624)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [220/689]  eta: 0:12:19  lr: 0.000094  loss: 0.1501 (0.1625)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [230/689]  eta: 0:12:03  lr: 0.000094  loss: 0.1588 (0.1626)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [240/689]  eta: 0:11:47  lr: 0.000094  loss: 0.1629 (0.1628)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [250/689]  eta: 0:11:32  lr: 0.000094  loss: 0.1553 (0.1627)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [260/689]  eta: 0:11:16  lr: 0.000094  loss: 0.1488 (0.1624)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [270/689]  eta: 0:11:00  lr: 0.000094  loss: 0.1533 (0.1622)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [280/689]  eta: 0:10:44  lr: 0.000094  loss: 0.1540 (0.1621)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [290/689]  eta: 0:10:29  lr: 0.000094  loss: 0.1512 (0.1617)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [300/689]  eta: 0:10:13  lr: 0.000094  loss: 0.1490 (0.1613)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [310/689]  eta: 0:09:57  lr: 0.000094  loss: 0.1493 (0.1610)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [320/689]  eta: 0:09:41  lr: 0.000094  loss: 0.1560 (0.1614)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [330/689]  eta: 0:09:26  lr: 0.000094  loss: 0.1604 (0.1613)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [340/689]  eta: 0:09:10  lr: 0.000094  loss: 0.1542 (0.1615)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [350/689]  eta: 0:08:54  lr: 0.000094  loss: 0.1666 (0.1618)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [360/689]  eta: 0:08:38  lr: 0.000094  loss: 0.1704 (0.1621)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [370/689]  eta: 0:08:23  lr: 0.000094  loss: 0.1736 (0.1627)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [380/689]  eta: 0:08:07  lr: 0.000094  loss: 0.1743 (0.1630)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [390/689]  eta: 0:07:51  lr: 0.000094  loss: 0.1715 (0.1632)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [400/689]  eta: 0:07:35  lr: 0.000094  loss: 0.1715 (0.1634)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [410/689]  eta: 0:07:20  lr: 0.000094  loss: 0.1654 (0.1634)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [420/689]  eta: 0:07:04  lr: 0.000094  loss: 0.1547 (0.1634)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [430/689]  eta: 0:06:48  lr: 0.000094  loss: 0.1565 (0.1632)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [440/689]  eta: 0:06:32  lr: 0.000094  loss: 0.1476 (0.1630)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [450/689]  eta: 0:06:17  lr: 0.000094  loss: 0.1534 (0.1630)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [460/689]  eta: 0:06:01  lr: 0.000094  loss: 0.1623 (0.1629)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [470/689]  eta: 0:05:45  lr: 0.000094  loss: 0.1611 (0.1630)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [480/689]  eta: 0:05:29  lr: 0.000094  loss: 0.1632 (0.1631)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [490/689]  eta: 0:05:13  lr: 0.000094  loss: 0.1639 (0.1634)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [500/689]  eta: 0:04:58  lr: 0.000094  loss: 0.1643 (0.1635)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [510/689]  eta: 0:04:42  lr: 0.000094  loss: 0.1557 (0.1634)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [520/689]  eta: 0:04:26  lr: 0.000094  loss: 0.1526 (0.1633)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [530/689]  eta: 0:04:10  lr: 0.000094  loss: 0.1495 (0.1631)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [540/689]  eta: 0:03:55  lr: 0.000094  loss: 0.1495 (0.1629)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [550/689]  eta: 0:03:39  lr: 0.000094  loss: 0.1524 (0.1628)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [560/689]  eta: 0:03:23  lr: 0.000094  loss: 0.1547 (0.1627)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [570/689]  eta: 0:03:07  lr: 0.000094  loss: 0.1525 (0.1625)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [580/689]  eta: 0:02:51  lr: 0.000094  loss: 0.1500 (0.1624)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [590/689]  eta: 0:02:36  lr: 0.000094  loss: 0.1518 (0.1623)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [600/689]  eta: 0:02:20  lr: 0.000094  loss: 0.1559 (0.1622)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [610/689]  eta: 0:02:04  lr: 0.000094  loss: 0.1591 (0.1622)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [620/689]  eta: 0:01:48  lr: 0.000094  loss: 0.1556 (0.1621)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [630/689]  eta: 0:01:33  lr: 0.000094  loss: 0.1606 (0.1621)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [640/689]  eta: 0:01:17  lr: 0.000094  loss: 0.1720 (0.1625)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [650/689]  eta: 0:01:01  lr: 0.000094  loss: 0.1719 (0.1626)  time: 1.5791  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:153]  [660/689]  eta: 0:00:45  lr: 0.000094  loss: 0.1580 (0.1625)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [670/689]  eta: 0:00:29  lr: 0.000094  loss: 0.1563 (0.1624)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [680/689]  eta: 0:00:14  lr: 0.000094  loss: 0.1617 (0.1625)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153]  [688/689]  eta: 0:00:01  lr: 0.000094  loss: 0.1617 (0.1625)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:153] Total time: 0:18:07 (1.5783 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: 0.1617 (0.1625)\n",
      "Valid: [epoch:153]  [ 0/14]  eta: 0:00:14  loss: 0.1596 (0.1596)  time: 1.0248  data: 0.4442  max mem: 39763\n",
      "Valid: [epoch:153]  [13/14]  eta: 0:00:00  loss: 0.1504 (0.1523)  time: 0.1151  data: 0.0318  max mem: 39763\n",
      "Valid: [epoch:153] Total time: 0:00:01 (0.1245 s / it)\n",
      "Averaged stats: loss: 0.1504 (0.1523)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_153_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.152%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:154]  [  0/689]  eta: 0:11:31  lr: 0.000094  loss: 0.1748 (0.1748)  time: 1.0030  data: 0.5235  max mem: 39763\n",
      "Train: [epoch:154]  [ 10/689]  eta: 0:17:15  lr: 0.000094  loss: 0.1694 (0.1658)  time: 1.5251  data: 0.0477  max mem: 39763\n",
      "Train: [epoch:154]  [ 20/689]  eta: 0:17:17  lr: 0.000094  loss: 0.1573 (0.1627)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [ 30/689]  eta: 0:17:08  lr: 0.000094  loss: 0.1545 (0.1604)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [ 40/689]  eta: 0:16:55  lr: 0.000094  loss: 0.1541 (0.1612)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [ 50/689]  eta: 0:16:41  lr: 0.000094  loss: 0.1444 (0.1574)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [ 60/689]  eta: 0:16:27  lr: 0.000094  loss: 0.1444 (0.1579)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [ 70/689]  eta: 0:16:12  lr: 0.000094  loss: 0.1469 (0.1576)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [ 80/689]  eta: 0:15:57  lr: 0.000094  loss: 0.1504 (0.1586)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [ 90/689]  eta: 0:15:42  lr: 0.000094  loss: 0.1537 (0.1582)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [100/689]  eta: 0:15:26  lr: 0.000094  loss: 0.1537 (0.1585)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [110/689]  eta: 0:15:11  lr: 0.000094  loss: 0.1622 (0.1588)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [120/689]  eta: 0:14:55  lr: 0.000094  loss: 0.1534 (0.1584)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [130/689]  eta: 0:14:40  lr: 0.000094  loss: 0.1514 (0.1582)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [140/689]  eta: 0:14:24  lr: 0.000094  loss: 0.1533 (0.1581)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [150/689]  eta: 0:14:09  lr: 0.000094  loss: 0.1618 (0.1587)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [160/689]  eta: 0:13:53  lr: 0.000094  loss: 0.1636 (0.1592)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [170/689]  eta: 0:13:37  lr: 0.000094  loss: 0.1600 (0.1598)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [180/689]  eta: 0:13:22  lr: 0.000094  loss: 0.1573 (0.1600)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [190/689]  eta: 0:13:06  lr: 0.000094  loss: 0.1754 (0.1608)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [200/689]  eta: 0:12:50  lr: 0.000094  loss: 0.1754 (0.1606)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [210/689]  eta: 0:12:35  lr: 0.000094  loss: 0.1496 (0.1601)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [220/689]  eta: 0:12:19  lr: 0.000094  loss: 0.1504 (0.1602)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [230/689]  eta: 0:12:03  lr: 0.000094  loss: 0.1647 (0.1607)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [240/689]  eta: 0:11:48  lr: 0.000094  loss: 0.1676 (0.1612)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [250/689]  eta: 0:11:32  lr: 0.000094  loss: 0.1595 (0.1609)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [260/689]  eta: 0:11:16  lr: 0.000094  loss: 0.1466 (0.1611)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [270/689]  eta: 0:11:00  lr: 0.000094  loss: 0.1467 (0.1607)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [280/689]  eta: 0:10:45  lr: 0.000094  loss: 0.1520 (0.1608)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [290/689]  eta: 0:10:29  lr: 0.000094  loss: 0.1635 (0.1607)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [300/689]  eta: 0:10:13  lr: 0.000094  loss: 0.1548 (0.1606)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [310/689]  eta: 0:09:57  lr: 0.000094  loss: 0.1548 (0.1602)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [320/689]  eta: 0:09:42  lr: 0.000094  loss: 0.1530 (0.1602)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [330/689]  eta: 0:09:26  lr: 0.000094  loss: 0.1589 (0.1605)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [340/689]  eta: 0:09:10  lr: 0.000094  loss: 0.1556 (0.1604)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [350/689]  eta: 0:08:54  lr: 0.000094  loss: 0.1470 (0.1603)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [360/689]  eta: 0:08:39  lr: 0.000094  loss: 0.1487 (0.1602)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [370/689]  eta: 0:08:23  lr: 0.000094  loss: 0.1487 (0.1602)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [380/689]  eta: 0:08:07  lr: 0.000094  loss: 0.1494 (0.1602)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [390/689]  eta: 0:07:51  lr: 0.000094  loss: 0.1553 (0.1604)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [400/689]  eta: 0:07:36  lr: 0.000094  loss: 0.1553 (0.1604)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [410/689]  eta: 0:07:20  lr: 0.000094  loss: 0.1542 (0.1604)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [420/689]  eta: 0:07:04  lr: 0.000094  loss: 0.1551 (0.1605)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [430/689]  eta: 0:06:48  lr: 0.000094  loss: 0.1683 (0.1609)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [440/689]  eta: 0:06:32  lr: 0.000094  loss: 0.1706 (0.1610)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [450/689]  eta: 0:06:17  lr: 0.000094  loss: 0.1556 (0.1608)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [460/689]  eta: 0:06:01  lr: 0.000094  loss: 0.1498 (0.1608)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [470/689]  eta: 0:05:45  lr: 0.000094  loss: 0.1560 (0.1607)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [480/689]  eta: 0:05:29  lr: 0.000094  loss: 0.1525 (0.1606)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [490/689]  eta: 0:05:14  lr: 0.000094  loss: 0.1490 (0.1604)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [500/689]  eta: 0:04:58  lr: 0.000094  loss: 0.1527 (0.1605)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [510/689]  eta: 0:04:42  lr: 0.000094  loss: 0.1523 (0.1604)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [520/689]  eta: 0:04:26  lr: 0.000094  loss: 0.1530 (0.1605)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [530/689]  eta: 0:04:10  lr: 0.000094  loss: 0.1599 (0.1606)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [540/689]  eta: 0:03:55  lr: 0.000094  loss: 0.1583 (0.1606)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [550/689]  eta: 0:03:39  lr: 0.000094  loss: 0.1590 (0.1606)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [560/689]  eta: 0:03:23  lr: 0.000094  loss: 0.1629 (0.1607)  time: 1.5797  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:154]  [570/689]  eta: 0:03:07  lr: 0.000094  loss: 0.1594 (0.1608)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [580/689]  eta: 0:02:52  lr: 0.000094  loss: 0.1628 (0.1609)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [590/689]  eta: 0:02:36  lr: 0.000094  loss: 0.1514 (0.1608)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [600/689]  eta: 0:02:20  lr: 0.000094  loss: 0.1543 (0.1608)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [610/689]  eta: 0:02:04  lr: 0.000094  loss: 0.1541 (0.1607)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [620/689]  eta: 0:01:48  lr: 0.000094  loss: 0.1480 (0.1607)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [630/689]  eta: 0:01:33  lr: 0.000094  loss: 0.1627 (0.1609)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [640/689]  eta: 0:01:17  lr: 0.000094  loss: 0.1601 (0.1608)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [650/689]  eta: 0:01:01  lr: 0.000094  loss: 0.1524 (0.1608)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [660/689]  eta: 0:00:45  lr: 0.000094  loss: 0.1633 (0.1609)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [670/689]  eta: 0:00:29  lr: 0.000094  loss: 0.1608 (0.1609)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [680/689]  eta: 0:00:14  lr: 0.000094  loss: 0.1569 (0.1608)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154]  [688/689]  eta: 0:00:01  lr: 0.000094  loss: 0.1609 (0.1608)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:154] Total time: 0:18:07 (1.5787 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: 0.1609 (0.1608)\n",
      "Valid: [epoch:154]  [ 0/14]  eta: 0:00:13  loss: 0.1397 (0.1397)  time: 0.9931  data: 0.3913  max mem: 39763\n",
      "Valid: [epoch:154]  [13/14]  eta: 0:00:00  loss: 0.1495 (0.1507)  time: 0.1129  data: 0.0280  max mem: 39763\n",
      "Valid: [epoch:154] Total time: 0:00:01 (0.1220 s / it)\n",
      "Averaged stats: loss: 0.1495 (0.1507)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_154_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.151%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:155]  [  0/689]  eta: 0:12:06  lr: 0.000094  loss: 0.1581 (0.1581)  time: 1.0540  data: 0.5768  max mem: 39763\n",
      "Train: [epoch:155]  [ 10/689]  eta: 0:17:18  lr: 0.000094  loss: 0.1581 (0.1531)  time: 1.5294  data: 0.0525  max mem: 39763\n",
      "Train: [epoch:155]  [ 20/689]  eta: 0:17:19  lr: 0.000094  loss: 0.1494 (0.1499)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [ 30/689]  eta: 0:17:08  lr: 0.000094  loss: 0.1534 (0.1552)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [ 40/689]  eta: 0:16:55  lr: 0.000094  loss: 0.1612 (0.1563)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [ 50/689]  eta: 0:16:41  lr: 0.000094  loss: 0.1561 (0.1578)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [ 60/689]  eta: 0:16:27  lr: 0.000094  loss: 0.1588 (0.1593)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [ 70/689]  eta: 0:16:12  lr: 0.000094  loss: 0.1588 (0.1590)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [ 80/689]  eta: 0:15:57  lr: 0.000094  loss: 0.1538 (0.1600)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [ 90/689]  eta: 0:15:42  lr: 0.000094  loss: 0.1552 (0.1608)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [100/689]  eta: 0:15:26  lr: 0.000094  loss: 0.1617 (0.1619)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [110/689]  eta: 0:15:11  lr: 0.000094  loss: 0.1712 (0.1629)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [120/689]  eta: 0:14:55  lr: 0.000094  loss: 0.1754 (0.1639)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [130/689]  eta: 0:14:40  lr: 0.000094  loss: 0.1646 (0.1640)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [140/689]  eta: 0:14:24  lr: 0.000094  loss: 0.1621 (0.1640)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [150/689]  eta: 0:14:09  lr: 0.000094  loss: 0.1621 (0.1639)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [160/689]  eta: 0:13:53  lr: 0.000094  loss: 0.1607 (0.1631)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [170/689]  eta: 0:13:37  lr: 0.000094  loss: 0.1530 (0.1628)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [180/689]  eta: 0:13:22  lr: 0.000094  loss: 0.1543 (0.1623)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [190/689]  eta: 0:13:06  lr: 0.000094  loss: 0.1572 (0.1623)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [200/689]  eta: 0:12:50  lr: 0.000094  loss: 0.1529 (0.1618)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [210/689]  eta: 0:12:35  lr: 0.000094  loss: 0.1505 (0.1614)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [220/689]  eta: 0:12:19  lr: 0.000094  loss: 0.1541 (0.1615)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [230/689]  eta: 0:12:03  lr: 0.000094  loss: 0.1618 (0.1618)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [240/689]  eta: 0:11:48  lr: 0.000094  loss: 0.1610 (0.1617)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [250/689]  eta: 0:11:32  lr: 0.000094  loss: 0.1593 (0.1619)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [260/689]  eta: 0:11:16  lr: 0.000094  loss: 0.1596 (0.1617)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [270/689]  eta: 0:11:00  lr: 0.000094  loss: 0.1539 (0.1617)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [280/689]  eta: 0:10:45  lr: 0.000094  loss: 0.1544 (0.1616)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [290/689]  eta: 0:10:29  lr: 0.000094  loss: 0.1544 (0.1616)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [300/689]  eta: 0:10:13  lr: 0.000094  loss: 0.1555 (0.1617)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [310/689]  eta: 0:09:57  lr: 0.000094  loss: 0.1595 (0.1619)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [320/689]  eta: 0:09:42  lr: 0.000094  loss: 0.1519 (0.1615)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [330/689]  eta: 0:09:26  lr: 0.000094  loss: 0.1519 (0.1616)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [340/689]  eta: 0:09:10  lr: 0.000094  loss: 0.1601 (0.1618)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [350/689]  eta: 0:08:54  lr: 0.000094  loss: 0.1629 (0.1619)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [360/689]  eta: 0:08:39  lr: 0.000094  loss: 0.1663 (0.1620)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [370/689]  eta: 0:08:23  lr: 0.000094  loss: 0.1594 (0.1619)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [380/689]  eta: 0:08:07  lr: 0.000094  loss: 0.1515 (0.1619)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [390/689]  eta: 0:07:51  lr: 0.000094  loss: 0.1502 (0.1618)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [400/689]  eta: 0:07:36  lr: 0.000094  loss: 0.1597 (0.1621)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [410/689]  eta: 0:07:20  lr: 0.000094  loss: 0.1710 (0.1623)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [420/689]  eta: 0:07:04  lr: 0.000094  loss: 0.1607 (0.1622)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [430/689]  eta: 0:06:48  lr: 0.000094  loss: 0.1595 (0.1621)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [440/689]  eta: 0:06:32  lr: 0.000094  loss: 0.1589 (0.1622)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [450/689]  eta: 0:06:17  lr: 0.000094  loss: 0.1653 (0.1621)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [460/689]  eta: 0:06:01  lr: 0.000094  loss: 0.1559 (0.1621)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [470/689]  eta: 0:05:45  lr: 0.000094  loss: 0.1555 (0.1620)  time: 1.5796  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:155]  [480/689]  eta: 0:05:29  lr: 0.000094  loss: 0.1566 (0.1621)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [490/689]  eta: 0:05:14  lr: 0.000094  loss: 0.1573 (0.1622)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [500/689]  eta: 0:04:58  lr: 0.000094  loss: 0.1678 (0.1622)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [510/689]  eta: 0:04:42  lr: 0.000094  loss: 0.1665 (0.1623)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [520/689]  eta: 0:04:26  lr: 0.000094  loss: 0.1622 (0.1623)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [530/689]  eta: 0:04:10  lr: 0.000094  loss: 0.1608 (0.1623)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [540/689]  eta: 0:03:55  lr: 0.000094  loss: 0.1584 (0.1623)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [550/689]  eta: 0:03:39  lr: 0.000094  loss: 0.1518 (0.1622)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [560/689]  eta: 0:03:23  lr: 0.000094  loss: 0.1532 (0.1621)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [570/689]  eta: 0:03:07  lr: 0.000094  loss: 0.1549 (0.1621)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [580/689]  eta: 0:02:52  lr: 0.000094  loss: 0.1519 (0.1620)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [590/689]  eta: 0:02:36  lr: 0.000094  loss: 0.1537 (0.1619)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [600/689]  eta: 0:02:20  lr: 0.000094  loss: 0.1600 (0.1621)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [610/689]  eta: 0:02:04  lr: 0.000094  loss: 0.1622 (0.1619)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [620/689]  eta: 0:01:48  lr: 0.000094  loss: 0.1645 (0.1620)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [630/689]  eta: 0:01:33  lr: 0.000094  loss: 0.1647 (0.1621)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [640/689]  eta: 0:01:17  lr: 0.000094  loss: 0.1524 (0.1620)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [650/689]  eta: 0:01:01  lr: 0.000094  loss: 0.1524 (0.1620)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [660/689]  eta: 0:00:45  lr: 0.000094  loss: 0.1586 (0.1619)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [670/689]  eta: 0:00:29  lr: 0.000094  loss: 0.1572 (0.1619)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [680/689]  eta: 0:00:14  lr: 0.000094  loss: 0.1582 (0.1619)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155]  [688/689]  eta: 0:00:01  lr: 0.000094  loss: 0.1594 (0.1619)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:155] Total time: 0:18:07 (1.5787 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: 0.1594 (0.1619)\n",
      "Valid: [epoch:155]  [ 0/14]  eta: 0:00:14  loss: 0.1597 (0.1597)  time: 1.0140  data: 0.4100  max mem: 39763\n",
      "Valid: [epoch:155]  [13/14]  eta: 0:00:00  loss: 0.1496 (0.1517)  time: 0.1145  data: 0.0293  max mem: 39763\n",
      "Valid: [epoch:155] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.1496 (0.1517)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_155_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.152%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:156]  [  0/689]  eta: 0:11:43  lr: 0.000094  loss: 0.1503 (0.1503)  time: 1.0211  data: 0.5382  max mem: 39763\n",
      "Train: [epoch:156]  [ 10/689]  eta: 0:17:17  lr: 0.000094  loss: 0.1541 (0.1610)  time: 1.5275  data: 0.0490  max mem: 39763\n",
      "Train: [epoch:156]  [ 20/689]  eta: 0:17:18  lr: 0.000094  loss: 0.1541 (0.1612)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [ 30/689]  eta: 0:17:08  lr: 0.000094  loss: 0.1585 (0.1608)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [ 40/689]  eta: 0:16:56  lr: 0.000094  loss: 0.1547 (0.1602)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [ 50/689]  eta: 0:16:42  lr: 0.000094  loss: 0.1499 (0.1592)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [ 60/689]  eta: 0:16:27  lr: 0.000094  loss: 0.1620 (0.1604)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [ 70/689]  eta: 0:16:12  lr: 0.000094  loss: 0.1550 (0.1604)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [ 80/689]  eta: 0:15:57  lr: 0.000094  loss: 0.1521 (0.1606)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [ 90/689]  eta: 0:15:42  lr: 0.000094  loss: 0.1691 (0.1622)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [100/689]  eta: 0:15:27  lr: 0.000094  loss: 0.1649 (0.1623)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [110/689]  eta: 0:15:11  lr: 0.000094  loss: 0.1637 (0.1632)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [120/689]  eta: 0:14:56  lr: 0.000094  loss: 0.1703 (0.1643)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [130/689]  eta: 0:14:41  lr: 0.000094  loss: 0.1636 (0.1636)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [140/689]  eta: 0:14:25  lr: 0.000094  loss: 0.1572 (0.1642)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [150/689]  eta: 0:14:09  lr: 0.000094  loss: 0.1604 (0.1644)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [160/689]  eta: 0:13:54  lr: 0.000094  loss: 0.1529 (0.1638)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [170/689]  eta: 0:13:38  lr: 0.000094  loss: 0.1465 (0.1636)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [180/689]  eta: 0:13:23  lr: 0.000094  loss: 0.1594 (0.1635)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [190/689]  eta: 0:13:07  lr: 0.000094  loss: 0.1594 (0.1633)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [200/689]  eta: 0:12:51  lr: 0.000094  loss: 0.1622 (0.1635)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [210/689]  eta: 0:12:36  lr: 0.000094  loss: 0.1618 (0.1638)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [220/689]  eta: 0:12:20  lr: 0.000094  loss: 0.1615 (0.1641)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [230/689]  eta: 0:12:04  lr: 0.000094  loss: 0.1568 (0.1636)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [240/689]  eta: 0:11:49  lr: 0.000094  loss: 0.1522 (0.1635)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [250/689]  eta: 0:11:33  lr: 0.000094  loss: 0.1511 (0.1631)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [260/689]  eta: 0:11:17  lr: 0.000094  loss: 0.1506 (0.1630)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [270/689]  eta: 0:11:01  lr: 0.000094  loss: 0.1518 (0.1628)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [280/689]  eta: 0:10:46  lr: 0.000094  loss: 0.1529 (0.1627)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [290/689]  eta: 0:10:30  lr: 0.000094  loss: 0.1526 (0.1624)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [300/689]  eta: 0:10:14  lr: 0.000094  loss: 0.1522 (0.1622)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [310/689]  eta: 0:09:58  lr: 0.000094  loss: 0.1525 (0.1619)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [320/689]  eta: 0:09:43  lr: 0.000094  loss: 0.1498 (0.1616)  time: 1.5837  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [330/689]  eta: 0:09:27  lr: 0.000094  loss: 0.1547 (0.1619)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [340/689]  eta: 0:09:11  lr: 0.000094  loss: 0.1630 (0.1617)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [350/689]  eta: 0:08:55  lr: 0.000094  loss: 0.1567 (0.1618)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [360/689]  eta: 0:08:40  lr: 0.000094  loss: 0.1567 (0.1617)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [370/689]  eta: 0:08:24  lr: 0.000094  loss: 0.1621 (0.1618)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [380/689]  eta: 0:08:08  lr: 0.000094  loss: 0.1573 (0.1618)  time: 1.5825  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:156]  [390/689]  eta: 0:07:52  lr: 0.000094  loss: 0.1568 (0.1617)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [400/689]  eta: 0:07:36  lr: 0.000094  loss: 0.1550 (0.1615)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [410/689]  eta: 0:07:21  lr: 0.000094  loss: 0.1550 (0.1618)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [420/689]  eta: 0:07:05  lr: 0.000094  loss: 0.1534 (0.1617)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [430/689]  eta: 0:06:49  lr: 0.000094  loss: 0.1531 (0.1617)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [440/689]  eta: 0:06:33  lr: 0.000094  loss: 0.1638 (0.1621)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [450/689]  eta: 0:06:17  lr: 0.000094  loss: 0.1623 (0.1620)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [460/689]  eta: 0:06:02  lr: 0.000094  loss: 0.1529 (0.1619)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [470/689]  eta: 0:05:46  lr: 0.000094  loss: 0.1540 (0.1619)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [480/689]  eta: 0:05:30  lr: 0.000094  loss: 0.1548 (0.1619)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [490/689]  eta: 0:05:14  lr: 0.000094  loss: 0.1537 (0.1618)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [500/689]  eta: 0:04:58  lr: 0.000094  loss: 0.1569 (0.1618)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [510/689]  eta: 0:04:43  lr: 0.000094  loss: 0.1581 (0.1620)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [520/689]  eta: 0:04:27  lr: 0.000094  loss: 0.1692 (0.1622)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [530/689]  eta: 0:04:11  lr: 0.000094  loss: 0.1628 (0.1622)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [540/689]  eta: 0:03:55  lr: 0.000094  loss: 0.1638 (0.1624)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [550/689]  eta: 0:03:39  lr: 0.000094  loss: 0.1646 (0.1624)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [560/689]  eta: 0:03:23  lr: 0.000094  loss: 0.1646 (0.1626)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [570/689]  eta: 0:03:08  lr: 0.000094  loss: 0.1645 (0.1626)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [580/689]  eta: 0:02:52  lr: 0.000094  loss: 0.1601 (0.1625)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [590/689]  eta: 0:02:36  lr: 0.000094  loss: 0.1486 (0.1624)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [600/689]  eta: 0:02:20  lr: 0.000094  loss: 0.1494 (0.1624)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [610/689]  eta: 0:02:04  lr: 0.000094  loss: 0.1548 (0.1624)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [620/689]  eta: 0:01:49  lr: 0.000094  loss: 0.1549 (0.1624)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [630/689]  eta: 0:01:33  lr: 0.000094  loss: 0.1602 (0.1624)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [640/689]  eta: 0:01:17  lr: 0.000094  loss: 0.1516 (0.1623)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [650/689]  eta: 0:01:01  lr: 0.000094  loss: 0.1489 (0.1622)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [660/689]  eta: 0:00:45  lr: 0.000094  loss: 0.1556 (0.1622)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [670/689]  eta: 0:00:30  lr: 0.000094  loss: 0.1593 (0.1622)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [680/689]  eta: 0:00:14  lr: 0.000094  loss: 0.1593 (0.1623)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156]  [688/689]  eta: 0:00:01  lr: 0.000094  loss: 0.1636 (0.1624)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:156] Total time: 0:18:09 (1.5818 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: 0.1636 (0.1624)\n",
      "Valid: [epoch:156]  [ 0/14]  eta: 0:00:14  loss: 0.1414 (0.1414)  time: 1.0013  data: 0.4010  max mem: 39763\n",
      "Valid: [epoch:156]  [13/14]  eta: 0:00:00  loss: 0.1501 (0.1529)  time: 0.1136  data: 0.0287  max mem: 39763\n",
      "Valid: [epoch:156] Total time: 0:00:01 (0.1225 s / it)\n",
      "Averaged stats: loss: 0.1501 (0.1529)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_156_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.153%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:157]  [  0/689]  eta: 0:11:57  lr: 0.000094  loss: 0.1671 (0.1671)  time: 1.0414  data: 0.5639  max mem: 39763\n",
      "Train: [epoch:157]  [ 10/689]  eta: 0:17:16  lr: 0.000094  loss: 0.1670 (0.1633)  time: 1.5272  data: 0.0513  max mem: 39763\n",
      "Train: [epoch:157]  [ 20/689]  eta: 0:17:18  lr: 0.000094  loss: 0.1607 (0.1619)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [ 30/689]  eta: 0:17:08  lr: 0.000094  loss: 0.1578 (0.1631)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [ 40/689]  eta: 0:16:55  lr: 0.000094  loss: 0.1587 (0.1633)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [ 50/689]  eta: 0:16:41  lr: 0.000094  loss: 0.1587 (0.1635)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [ 60/689]  eta: 0:16:26  lr: 0.000094  loss: 0.1538 (0.1619)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [ 70/689]  eta: 0:16:12  lr: 0.000094  loss: 0.1576 (0.1630)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [ 80/689]  eta: 0:15:57  lr: 0.000094  loss: 0.1603 (0.1623)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [ 90/689]  eta: 0:15:41  lr: 0.000094  loss: 0.1584 (0.1626)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [100/689]  eta: 0:15:26  lr: 0.000094  loss: 0.1622 (0.1626)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [110/689]  eta: 0:15:11  lr: 0.000094  loss: 0.1594 (0.1627)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [120/689]  eta: 0:14:55  lr: 0.000094  loss: 0.1600 (0.1626)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [130/689]  eta: 0:14:40  lr: 0.000094  loss: 0.1630 (0.1626)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [140/689]  eta: 0:14:24  lr: 0.000094  loss: 0.1627 (0.1628)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [150/689]  eta: 0:14:08  lr: 0.000094  loss: 0.1627 (0.1628)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [160/689]  eta: 0:13:53  lr: 0.000094  loss: 0.1636 (0.1633)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [170/689]  eta: 0:13:37  lr: 0.000094  loss: 0.1642 (0.1636)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [180/689]  eta: 0:13:22  lr: 0.000094  loss: 0.1631 (0.1638)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [190/689]  eta: 0:13:06  lr: 0.000094  loss: 0.1599 (0.1641)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [200/689]  eta: 0:12:50  lr: 0.000094  loss: 0.1610 (0.1641)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [210/689]  eta: 0:12:34  lr: 0.000094  loss: 0.1610 (0.1641)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [220/689]  eta: 0:12:19  lr: 0.000094  loss: 0.1602 (0.1643)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [230/689]  eta: 0:12:03  lr: 0.000094  loss: 0.1491 (0.1638)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [240/689]  eta: 0:11:47  lr: 0.000094  loss: 0.1550 (0.1638)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [250/689]  eta: 0:11:32  lr: 0.000094  loss: 0.1611 (0.1636)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [260/689]  eta: 0:11:16  lr: 0.000094  loss: 0.1585 (0.1637)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [270/689]  eta: 0:11:00  lr: 0.000094  loss: 0.1593 (0.1638)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [280/689]  eta: 0:10:44  lr: 0.000094  loss: 0.1611 (0.1638)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [290/689]  eta: 0:10:29  lr: 0.000094  loss: 0.1600 (0.1635)  time: 1.5797  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:157]  [300/689]  eta: 0:10:13  lr: 0.000094  loss: 0.1632 (0.1637)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [310/689]  eta: 0:09:57  lr: 0.000094  loss: 0.1649 (0.1638)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [320/689]  eta: 0:09:42  lr: 0.000094  loss: 0.1619 (0.1638)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [330/689]  eta: 0:09:26  lr: 0.000094  loss: 0.1592 (0.1638)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [340/689]  eta: 0:09:10  lr: 0.000094  loss: 0.1527 (0.1637)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [350/689]  eta: 0:08:54  lr: 0.000094  loss: 0.1527 (0.1635)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [360/689]  eta: 0:08:38  lr: 0.000094  loss: 0.1501 (0.1632)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [370/689]  eta: 0:08:23  lr: 0.000094  loss: 0.1466 (0.1629)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [380/689]  eta: 0:08:07  lr: 0.000094  loss: 0.1455 (0.1626)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [390/689]  eta: 0:07:51  lr: 0.000094  loss: 0.1562 (0.1627)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [400/689]  eta: 0:07:35  lr: 0.000094  loss: 0.1638 (0.1627)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [410/689]  eta: 0:07:20  lr: 0.000094  loss: 0.1549 (0.1626)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [420/689]  eta: 0:07:04  lr: 0.000094  loss: 0.1467 (0.1623)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [430/689]  eta: 0:06:48  lr: 0.000094  loss: 0.1535 (0.1624)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [440/689]  eta: 0:06:32  lr: 0.000094  loss: 0.1655 (0.1626)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [450/689]  eta: 0:06:17  lr: 0.000094  loss: 0.1654 (0.1628)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [460/689]  eta: 0:06:01  lr: 0.000094  loss: 0.1629 (0.1629)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [470/689]  eta: 0:05:45  lr: 0.000094  loss: 0.1541 (0.1627)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [480/689]  eta: 0:05:29  lr: 0.000094  loss: 0.1540 (0.1627)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [490/689]  eta: 0:05:14  lr: 0.000094  loss: 0.1555 (0.1628)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [500/689]  eta: 0:04:58  lr: 0.000094  loss: 0.1566 (0.1627)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [510/689]  eta: 0:04:42  lr: 0.000094  loss: 0.1602 (0.1630)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [520/689]  eta: 0:04:26  lr: 0.000094  loss: 0.1648 (0.1630)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [530/689]  eta: 0:04:10  lr: 0.000094  loss: 0.1593 (0.1630)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [540/689]  eta: 0:03:55  lr: 0.000094  loss: 0.1661 (0.1632)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [550/689]  eta: 0:03:39  lr: 0.000094  loss: 0.1643 (0.1631)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [560/689]  eta: 0:03:23  lr: 0.000094  loss: 0.1562 (0.1632)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [570/689]  eta: 0:03:07  lr: 0.000094  loss: 0.1562 (0.1633)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [580/689]  eta: 0:02:52  lr: 0.000094  loss: 0.1568 (0.1633)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [590/689]  eta: 0:02:36  lr: 0.000094  loss: 0.1560 (0.1632)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [600/689]  eta: 0:02:20  lr: 0.000094  loss: 0.1540 (0.1632)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [610/689]  eta: 0:02:04  lr: 0.000094  loss: 0.1486 (0.1631)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [620/689]  eta: 0:01:48  lr: 0.000094  loss: 0.1538 (0.1631)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [630/689]  eta: 0:01:33  lr: 0.000094  loss: 0.1663 (0.1632)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [640/689]  eta: 0:01:17  lr: 0.000094  loss: 0.1706 (0.1633)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [650/689]  eta: 0:01:01  lr: 0.000094  loss: 0.1602 (0.1633)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [660/689]  eta: 0:00:45  lr: 0.000094  loss: 0.1575 (0.1633)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [670/689]  eta: 0:00:29  lr: 0.000094  loss: 0.1574 (0.1632)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [680/689]  eta: 0:00:14  lr: 0.000094  loss: 0.1551 (0.1632)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157]  [688/689]  eta: 0:00:01  lr: 0.000094  loss: 0.1553 (0.1631)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:157] Total time: 0:18:07 (1.5785 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: 0.1553 (0.1631)\n",
      "Valid: [epoch:157]  [ 0/14]  eta: 0:00:14  loss: 0.1617 (0.1617)  time: 1.0086  data: 0.4591  max mem: 39763\n",
      "Valid: [epoch:157]  [13/14]  eta: 0:00:00  loss: 0.1534 (0.1549)  time: 0.1140  data: 0.0328  max mem: 39763\n",
      "Valid: [epoch:157] Total time: 0:00:01 (0.1218 s / it)\n",
      "Averaged stats: loss: 0.1534 (0.1549)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_157_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.155%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:158]  [  0/689]  eta: 0:11:37  lr: 0.000094  loss: 0.1610 (0.1610)  time: 1.0124  data: 0.5335  max mem: 39763\n",
      "Train: [epoch:158]  [ 10/689]  eta: 0:17:16  lr: 0.000094  loss: 0.1563 (0.1583)  time: 1.5260  data: 0.0486  max mem: 39763\n",
      "Train: [epoch:158]  [ 20/689]  eta: 0:17:17  lr: 0.000094  loss: 0.1502 (0.1521)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [ 30/689]  eta: 0:17:08  lr: 0.000094  loss: 0.1531 (0.1550)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [ 40/689]  eta: 0:16:55  lr: 0.000094  loss: 0.1556 (0.1565)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [ 50/689]  eta: 0:16:41  lr: 0.000094  loss: 0.1579 (0.1580)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [ 60/689]  eta: 0:16:27  lr: 0.000094  loss: 0.1574 (0.1575)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [ 70/689]  eta: 0:16:12  lr: 0.000094  loss: 0.1529 (0.1578)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [ 80/689]  eta: 0:15:57  lr: 0.000094  loss: 0.1491 (0.1572)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [ 90/689]  eta: 0:15:42  lr: 0.000094  loss: 0.1546 (0.1583)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [100/689]  eta: 0:15:26  lr: 0.000094  loss: 0.1612 (0.1586)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [110/689]  eta: 0:15:11  lr: 0.000094  loss: 0.1607 (0.1588)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [120/689]  eta: 0:14:55  lr: 0.000094  loss: 0.1651 (0.1594)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [130/689]  eta: 0:14:40  lr: 0.000094  loss: 0.1665 (0.1599)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [140/689]  eta: 0:14:24  lr: 0.000094  loss: 0.1643 (0.1598)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [150/689]  eta: 0:14:09  lr: 0.000094  loss: 0.1611 (0.1606)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [160/689]  eta: 0:13:53  lr: 0.000094  loss: 0.1630 (0.1611)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [170/689]  eta: 0:13:37  lr: 0.000094  loss: 0.1585 (0.1611)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [180/689]  eta: 0:13:22  lr: 0.000094  loss: 0.1641 (0.1621)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [190/689]  eta: 0:13:06  lr: 0.000094  loss: 0.1756 (0.1628)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [200/689]  eta: 0:12:50  lr: 0.000094  loss: 0.1698 (0.1632)  time: 1.5793  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:158]  [210/689]  eta: 0:12:35  lr: 0.000094  loss: 0.1698 (0.1633)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [220/689]  eta: 0:12:19  lr: 0.000094  loss: 0.1631 (0.1635)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [230/689]  eta: 0:12:03  lr: 0.000094  loss: 0.1616 (0.1636)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [240/689]  eta: 0:11:47  lr: 0.000094  loss: 0.1542 (0.1632)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [250/689]  eta: 0:11:32  lr: 0.000094  loss: 0.1591 (0.1635)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [260/689]  eta: 0:11:16  lr: 0.000094  loss: 0.1654 (0.1636)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [270/689]  eta: 0:11:00  lr: 0.000094  loss: 0.1592 (0.1634)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [280/689]  eta: 0:10:45  lr: 0.000094  loss: 0.1573 (0.1632)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [290/689]  eta: 0:10:29  lr: 0.000094  loss: 0.1514 (0.1630)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [300/689]  eta: 0:10:13  lr: 0.000094  loss: 0.1536 (0.1628)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [310/689]  eta: 0:09:57  lr: 0.000094  loss: 0.1493 (0.1624)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [320/689]  eta: 0:09:42  lr: 0.000094  loss: 0.1471 (0.1623)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [330/689]  eta: 0:09:26  lr: 0.000094  loss: 0.1538 (0.1623)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [340/689]  eta: 0:09:10  lr: 0.000094  loss: 0.1540 (0.1624)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [350/689]  eta: 0:08:54  lr: 0.000094  loss: 0.1657 (0.1626)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [360/689]  eta: 0:08:38  lr: 0.000094  loss: 0.1669 (0.1627)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [370/689]  eta: 0:08:23  lr: 0.000094  loss: 0.1600 (0.1628)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [380/689]  eta: 0:08:07  lr: 0.000094  loss: 0.1632 (0.1629)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [390/689]  eta: 0:07:51  lr: 0.000094  loss: 0.1579 (0.1628)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [400/689]  eta: 0:07:35  lr: 0.000094  loss: 0.1633 (0.1630)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [410/689]  eta: 0:07:20  lr: 0.000094  loss: 0.1613 (0.1629)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [420/689]  eta: 0:07:04  lr: 0.000094  loss: 0.1548 (0.1631)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [430/689]  eta: 0:06:48  lr: 0.000094  loss: 0.1671 (0.1631)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [440/689]  eta: 0:06:32  lr: 0.000094  loss: 0.1665 (0.1631)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [450/689]  eta: 0:06:17  lr: 0.000094  loss: 0.1590 (0.1628)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [460/689]  eta: 0:06:01  lr: 0.000094  loss: 0.1541 (0.1630)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [470/689]  eta: 0:05:45  lr: 0.000094  loss: 0.1621 (0.1631)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [480/689]  eta: 0:05:29  lr: 0.000094  loss: 0.1552 (0.1630)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [490/689]  eta: 0:05:13  lr: 0.000094  loss: 0.1493 (0.1629)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [500/689]  eta: 0:04:58  lr: 0.000094  loss: 0.1601 (0.1630)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [510/689]  eta: 0:04:42  lr: 0.000094  loss: 0.1723 (0.1633)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [520/689]  eta: 0:04:26  lr: 0.000094  loss: 0.1616 (0.1632)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [530/689]  eta: 0:04:10  lr: 0.000094  loss: 0.1572 (0.1633)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [540/689]  eta: 0:03:55  lr: 0.000094  loss: 0.1573 (0.1633)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [550/689]  eta: 0:03:39  lr: 0.000094  loss: 0.1603 (0.1634)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [560/689]  eta: 0:03:23  lr: 0.000094  loss: 0.1681 (0.1635)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [570/689]  eta: 0:03:07  lr: 0.000094  loss: 0.1648 (0.1636)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [580/689]  eta: 0:02:52  lr: 0.000094  loss: 0.1711 (0.1638)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [590/689]  eta: 0:02:36  lr: 0.000094  loss: 0.1619 (0.1637)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [600/689]  eta: 0:02:20  lr: 0.000094  loss: 0.1606 (0.1638)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [610/689]  eta: 0:02:04  lr: 0.000094  loss: 0.1595 (0.1636)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [620/689]  eta: 0:01:48  lr: 0.000094  loss: 0.1516 (0.1635)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [630/689]  eta: 0:01:33  lr: 0.000094  loss: 0.1542 (0.1634)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [640/689]  eta: 0:01:17  lr: 0.000094  loss: 0.1563 (0.1635)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [650/689]  eta: 0:01:01  lr: 0.000094  loss: 0.1566 (0.1634)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [660/689]  eta: 0:00:45  lr: 0.000094  loss: 0.1609 (0.1635)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [670/689]  eta: 0:00:29  lr: 0.000094  loss: 0.1617 (0.1635)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [680/689]  eta: 0:00:14  lr: 0.000094  loss: 0.1717 (0.1637)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158]  [688/689]  eta: 0:00:01  lr: 0.000094  loss: 0.1717 (0.1637)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:158] Total time: 0:18:07 (1.5785 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: 0.1717 (0.1637)\n",
      "Valid: [epoch:158]  [ 0/14]  eta: 0:00:13  loss: 0.1471 (0.1471)  time: 0.9940  data: 0.3851  max mem: 39763\n",
      "Valid: [epoch:158]  [13/14]  eta: 0:00:00  loss: 0.1512 (0.1539)  time: 0.1130  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:158] Total time: 0:00:01 (0.1222 s / it)\n",
      "Averaged stats: loss: 0.1512 (0.1539)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_158_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.154%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:159]  [  0/689]  eta: 0:11:38  lr: 0.000094  loss: 0.1601 (0.1601)  time: 1.0141  data: 0.5368  max mem: 39763\n",
      "Train: [epoch:159]  [ 10/689]  eta: 0:17:15  lr: 0.000094  loss: 0.1903 (0.1853)  time: 1.5252  data: 0.0489  max mem: 39763\n",
      "Train: [epoch:159]  [ 20/689]  eta: 0:17:17  lr: 0.000094  loss: 0.1778 (0.1791)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [ 30/689]  eta: 0:17:07  lr: 0.000094  loss: 0.1637 (0.1765)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [ 40/689]  eta: 0:16:54  lr: 0.000094  loss: 0.1608 (0.1734)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [ 50/689]  eta: 0:16:41  lr: 0.000094  loss: 0.1628 (0.1726)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [ 60/689]  eta: 0:16:26  lr: 0.000094  loss: 0.1628 (0.1700)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [ 70/689]  eta: 0:16:11  lr: 0.000094  loss: 0.1615 (0.1705)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [ 80/689]  eta: 0:15:56  lr: 0.000094  loss: 0.1608 (0.1687)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [ 90/689]  eta: 0:15:41  lr: 0.000094  loss: 0.1555 (0.1679)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [100/689]  eta: 0:15:26  lr: 0.000094  loss: 0.1645 (0.1680)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [110/689]  eta: 0:15:10  lr: 0.000094  loss: 0.1701 (0.1682)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:159]  [120/689]  eta: 0:14:55  lr: 0.000094  loss: 0.1701 (0.1682)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [130/689]  eta: 0:14:39  lr: 0.000094  loss: 0.1606 (0.1686)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [140/689]  eta: 0:14:24  lr: 0.000094  loss: 0.1606 (0.1681)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [150/689]  eta: 0:14:08  lr: 0.000094  loss: 0.1616 (0.1683)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [160/689]  eta: 0:13:53  lr: 0.000094  loss: 0.1631 (0.1679)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [170/689]  eta: 0:13:37  lr: 0.000094  loss: 0.1656 (0.1680)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [180/689]  eta: 0:13:21  lr: 0.000094  loss: 0.1658 (0.1679)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [190/689]  eta: 0:13:06  lr: 0.000094  loss: 0.1554 (0.1675)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [200/689]  eta: 0:12:50  lr: 0.000094  loss: 0.1554 (0.1670)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [210/689]  eta: 0:12:34  lr: 0.000094  loss: 0.1560 (0.1668)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [220/689]  eta: 0:12:19  lr: 0.000094  loss: 0.1610 (0.1669)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [230/689]  eta: 0:12:03  lr: 0.000094  loss: 0.1659 (0.1673)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [240/689]  eta: 0:11:47  lr: 0.000094  loss: 0.1636 (0.1671)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [250/689]  eta: 0:11:31  lr: 0.000094  loss: 0.1612 (0.1673)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [260/689]  eta: 0:11:16  lr: 0.000094  loss: 0.1609 (0.1672)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [270/689]  eta: 0:11:00  lr: 0.000094  loss: 0.1610 (0.1675)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [280/689]  eta: 0:10:44  lr: 0.000094  loss: 0.1764 (0.1676)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [290/689]  eta: 0:10:29  lr: 0.000094  loss: 0.1594 (0.1677)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [300/689]  eta: 0:10:13  lr: 0.000094  loss: 0.1651 (0.1679)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [310/689]  eta: 0:09:57  lr: 0.000094  loss: 0.1700 (0.1681)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [320/689]  eta: 0:09:41  lr: 0.000094  loss: 0.1794 (0.1684)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [330/689]  eta: 0:09:26  lr: 0.000094  loss: 0.1702 (0.1683)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [340/689]  eta: 0:09:10  lr: 0.000094  loss: 0.1702 (0.1686)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [350/689]  eta: 0:08:54  lr: 0.000094  loss: 0.1622 (0.1684)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [360/689]  eta: 0:08:38  lr: 0.000094  loss: 0.1625 (0.1683)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [370/689]  eta: 0:08:23  lr: 0.000094  loss: 0.1610 (0.1680)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [380/689]  eta: 0:08:07  lr: 0.000094  loss: 0.1678 (0.1682)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [390/689]  eta: 0:07:51  lr: 0.000094  loss: 0.1704 (0.1684)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [400/689]  eta: 0:07:35  lr: 0.000094  loss: 0.1674 (0.1685)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [410/689]  eta: 0:07:20  lr: 0.000094  loss: 0.1674 (0.1690)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [420/689]  eta: 0:07:04  lr: 0.000094  loss: 0.1674 (0.1690)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [430/689]  eta: 0:06:48  lr: 0.000094  loss: 0.1717 (0.1692)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [440/689]  eta: 0:06:32  lr: 0.000094  loss: 0.1756 (0.1696)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [450/689]  eta: 0:06:16  lr: 0.000094  loss: 0.1639 (0.1693)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [460/689]  eta: 0:06:01  lr: 0.000094  loss: 0.1593 (0.1693)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [470/689]  eta: 0:05:45  lr: 0.000094  loss: 0.1670 (0.1693)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [480/689]  eta: 0:05:29  lr: 0.000094  loss: 0.1627 (0.1692)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [490/689]  eta: 0:05:13  lr: 0.000094  loss: 0.1539 (0.1689)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [500/689]  eta: 0:04:58  lr: 0.000094  loss: 0.1546 (0.1688)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [510/689]  eta: 0:04:42  lr: 0.000094  loss: 0.1546 (0.1685)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [520/689]  eta: 0:04:26  lr: 0.000094  loss: 0.1556 (0.1683)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [530/689]  eta: 0:04:10  lr: 0.000094  loss: 0.1568 (0.1681)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [540/689]  eta: 0:03:55  lr: 0.000094  loss: 0.1600 (0.1682)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [550/689]  eta: 0:03:39  lr: 0.000094  loss: 0.1581 (0.1681)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [560/689]  eta: 0:03:23  lr: 0.000094  loss: 0.1588 (0.1680)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [570/689]  eta: 0:03:07  lr: 0.000094  loss: 0.1630 (0.1680)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [580/689]  eta: 0:02:51  lr: 0.000094  loss: 0.1580 (0.1679)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [590/689]  eta: 0:02:36  lr: 0.000094  loss: 0.1564 (0.1678)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [600/689]  eta: 0:02:20  lr: 0.000094  loss: 0.1629 (0.1678)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [610/689]  eta: 0:02:04  lr: 0.000094  loss: 0.1659 (0.1678)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [620/689]  eta: 0:01:48  lr: 0.000094  loss: 0.1624 (0.1677)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [630/689]  eta: 0:01:33  lr: 0.000094  loss: 0.1555 (0.1675)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [640/689]  eta: 0:01:17  lr: 0.000094  loss: 0.1551 (0.1675)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [650/689]  eta: 0:01:01  lr: 0.000094  loss: 0.1490 (0.1674)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [660/689]  eta: 0:00:45  lr: 0.000094  loss: 0.1490 (0.1674)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [670/689]  eta: 0:00:29  lr: 0.000094  loss: 0.1720 (0.1676)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [680/689]  eta: 0:00:14  lr: 0.000094  loss: 0.1660 (0.1674)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159]  [688/689]  eta: 0:00:01  lr: 0.000094  loss: 0.1558 (0.1674)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:159] Total time: 0:18:07 (1.5782 s / it)\n",
      "Averaged stats: lr: 0.000094  loss: 0.1558 (0.1674)\n",
      "Valid: [epoch:159]  [ 0/14]  eta: 0:00:14  loss: 0.1620 (0.1620)  time: 1.0030  data: 0.3744  max mem: 39763\n",
      "Valid: [epoch:159]  [13/14]  eta: 0:00:00  loss: 0.1517 (0.1541)  time: 0.1137  data: 0.0268  max mem: 39763\n",
      "Valid: [epoch:159] Total time: 0:00:01 (0.1228 s / it)\n",
      "Averaged stats: loss: 0.1517 (0.1541)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_159_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.154%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:160]  [  0/689]  eta: 0:11:51  lr: 0.000093  loss: 0.1844 (0.1844)  time: 1.0332  data: 0.5542  max mem: 39763\n",
      "Train: [epoch:160]  [ 10/689]  eta: 0:17:17  lr: 0.000093  loss: 0.1591 (0.1665)  time: 1.5280  data: 0.0505  max mem: 39763\n",
      "Train: [epoch:160]  [ 20/689]  eta: 0:17:18  lr: 0.000093  loss: 0.1585 (0.1619)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:160]  [ 30/689]  eta: 0:17:08  lr: 0.000093  loss: 0.1619 (0.1648)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [ 40/689]  eta: 0:16:55  lr: 0.000093  loss: 0.1628 (0.1660)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [ 50/689]  eta: 0:16:41  lr: 0.000093  loss: 0.1570 (0.1658)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [ 60/689]  eta: 0:16:27  lr: 0.000093  loss: 0.1565 (0.1657)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [ 70/689]  eta: 0:16:12  lr: 0.000093  loss: 0.1642 (0.1664)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [ 80/689]  eta: 0:15:57  lr: 0.000093  loss: 0.1565 (0.1650)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [ 90/689]  eta: 0:15:41  lr: 0.000093  loss: 0.1600 (0.1650)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [100/689]  eta: 0:15:26  lr: 0.000093  loss: 0.1724 (0.1665)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [110/689]  eta: 0:15:11  lr: 0.000093  loss: 0.1741 (0.1676)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [120/689]  eta: 0:14:55  lr: 0.000093  loss: 0.1642 (0.1673)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [130/689]  eta: 0:14:40  lr: 0.000093  loss: 0.1540 (0.1665)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [140/689]  eta: 0:14:24  lr: 0.000093  loss: 0.1577 (0.1664)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [150/689]  eta: 0:14:08  lr: 0.000093  loss: 0.1588 (0.1662)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [160/689]  eta: 0:13:53  lr: 0.000093  loss: 0.1585 (0.1661)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [170/689]  eta: 0:13:37  lr: 0.000093  loss: 0.1588 (0.1661)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [180/689]  eta: 0:13:21  lr: 0.000093  loss: 0.1577 (0.1659)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [190/689]  eta: 0:13:06  lr: 0.000093  loss: 0.1577 (0.1658)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [200/689]  eta: 0:12:50  lr: 0.000093  loss: 0.1570 (0.1656)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [210/689]  eta: 0:12:34  lr: 0.000093  loss: 0.1648 (0.1659)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [220/689]  eta: 0:12:19  lr: 0.000093  loss: 0.1641 (0.1657)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [230/689]  eta: 0:12:03  lr: 0.000093  loss: 0.1548 (0.1654)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [240/689]  eta: 0:11:47  lr: 0.000093  loss: 0.1549 (0.1655)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [250/689]  eta: 0:11:32  lr: 0.000093  loss: 0.1697 (0.1660)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [260/689]  eta: 0:11:16  lr: 0.000093  loss: 0.1716 (0.1661)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [270/689]  eta: 0:11:00  lr: 0.000093  loss: 0.1675 (0.1664)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [280/689]  eta: 0:10:44  lr: 0.000093  loss: 0.1636 (0.1666)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [290/689]  eta: 0:10:29  lr: 0.000093  loss: 0.1605 (0.1663)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [300/689]  eta: 0:10:13  lr: 0.000093  loss: 0.1562 (0.1663)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [310/689]  eta: 0:09:57  lr: 0.000093  loss: 0.1550 (0.1657)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [320/689]  eta: 0:09:41  lr: 0.000093  loss: 0.1579 (0.1658)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [330/689]  eta: 0:09:26  lr: 0.000093  loss: 0.1682 (0.1659)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [340/689]  eta: 0:09:10  lr: 0.000093  loss: 0.1628 (0.1658)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [350/689]  eta: 0:08:54  lr: 0.000093  loss: 0.1591 (0.1658)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [360/689]  eta: 0:08:38  lr: 0.000093  loss: 0.1607 (0.1660)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [370/689]  eta: 0:08:23  lr: 0.000093  loss: 0.1631 (0.1658)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [380/689]  eta: 0:08:07  lr: 0.000093  loss: 0.1542 (0.1656)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [390/689]  eta: 0:07:51  lr: 0.000093  loss: 0.1566 (0.1656)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [400/689]  eta: 0:07:35  lr: 0.000093  loss: 0.1646 (0.1658)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [410/689]  eta: 0:07:20  lr: 0.000093  loss: 0.1684 (0.1659)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [420/689]  eta: 0:07:04  lr: 0.000093  loss: 0.1674 (0.1660)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [430/689]  eta: 0:06:48  lr: 0.000093  loss: 0.1674 (0.1659)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [440/689]  eta: 0:06:32  lr: 0.000093  loss: 0.1554 (0.1658)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [450/689]  eta: 0:06:17  lr: 0.000093  loss: 0.1509 (0.1655)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [460/689]  eta: 0:06:01  lr: 0.000093  loss: 0.1606 (0.1656)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [470/689]  eta: 0:05:45  lr: 0.000093  loss: 0.1698 (0.1657)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [480/689]  eta: 0:05:29  lr: 0.000093  loss: 0.1678 (0.1658)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [490/689]  eta: 0:05:13  lr: 0.000093  loss: 0.1631 (0.1656)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [500/689]  eta: 0:04:58  lr: 0.000093  loss: 0.1631 (0.1657)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [510/689]  eta: 0:04:42  lr: 0.000093  loss: 0.1681 (0.1659)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [520/689]  eta: 0:04:26  lr: 0.000093  loss: 0.1681 (0.1659)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [530/689]  eta: 0:04:10  lr: 0.000093  loss: 0.1582 (0.1657)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [540/689]  eta: 0:03:55  lr: 0.000093  loss: 0.1582 (0.1658)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [550/689]  eta: 0:03:39  lr: 0.000093  loss: 0.1569 (0.1658)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [560/689]  eta: 0:03:23  lr: 0.000093  loss: 0.1778 (0.1660)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [570/689]  eta: 0:03:07  lr: 0.000093  loss: 0.1736 (0.1659)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [580/689]  eta: 0:02:51  lr: 0.000093  loss: 0.1576 (0.1658)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [590/689]  eta: 0:02:36  lr: 0.000093  loss: 0.1584 (0.1659)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [600/689]  eta: 0:02:20  lr: 0.000093  loss: 0.1651 (0.1659)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [610/689]  eta: 0:02:04  lr: 0.000093  loss: 0.1638 (0.1659)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [620/689]  eta: 0:01:48  lr: 0.000093  loss: 0.1597 (0.1660)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [630/689]  eta: 0:01:33  lr: 0.000093  loss: 0.1603 (0.1659)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [640/689]  eta: 0:01:17  lr: 0.000093  loss: 0.1625 (0.1660)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [650/689]  eta: 0:01:01  lr: 0.000093  loss: 0.1686 (0.1660)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [660/689]  eta: 0:00:45  lr: 0.000093  loss: 0.1643 (0.1660)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [670/689]  eta: 0:00:29  lr: 0.000093  loss: 0.1643 (0.1660)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160]  [680/689]  eta: 0:00:14  lr: 0.000093  loss: 0.1661 (0.1660)  time: 1.5790  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:160]  [688/689]  eta: 0:00:01  lr: 0.000093  loss: 0.1671 (0.1661)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:160] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: 0.1671 (0.1661)\n",
      "Valid: [epoch:160]  [ 0/14]  eta: 0:00:14  loss: 0.1504 (0.1504)  time: 1.0027  data: 0.3855  max mem: 39763\n",
      "Valid: [epoch:160]  [13/14]  eta: 0:00:00  loss: 0.1544 (0.1562)  time: 0.1136  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:160] Total time: 0:00:01 (0.1228 s / it)\n",
      "Averaged stats: loss: 0.1544 (0.1562)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_160_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.156%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:161]  [  0/689]  eta: 0:11:52  lr: 0.000093  loss: 0.1556 (0.1556)  time: 1.0345  data: 0.5554  max mem: 39763\n",
      "Train: [epoch:161]  [ 10/689]  eta: 0:17:16  lr: 0.000093  loss: 0.1614 (0.1672)  time: 1.5270  data: 0.0506  max mem: 39763\n",
      "Train: [epoch:161]  [ 20/689]  eta: 0:17:17  lr: 0.000093  loss: 0.1644 (0.1708)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [ 30/689]  eta: 0:17:07  lr: 0.000093  loss: 0.1599 (0.1703)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [ 40/689]  eta: 0:16:55  lr: 0.000093  loss: 0.1585 (0.1690)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [ 50/689]  eta: 0:16:41  lr: 0.000093  loss: 0.1607 (0.1676)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [ 60/689]  eta: 0:16:26  lr: 0.000093  loss: 0.1607 (0.1664)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [ 70/689]  eta: 0:16:11  lr: 0.000093  loss: 0.1610 (0.1666)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [ 80/689]  eta: 0:15:56  lr: 0.000093  loss: 0.1641 (0.1679)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [ 90/689]  eta: 0:15:41  lr: 0.000093  loss: 0.1723 (0.1682)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [100/689]  eta: 0:15:26  lr: 0.000093  loss: 0.1628 (0.1680)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [110/689]  eta: 0:15:10  lr: 0.000093  loss: 0.1588 (0.1687)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [120/689]  eta: 0:14:55  lr: 0.000093  loss: 0.1645 (0.1686)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [130/689]  eta: 0:14:39  lr: 0.000093  loss: 0.1604 (0.1684)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [140/689]  eta: 0:14:24  lr: 0.000093  loss: 0.1580 (0.1677)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [150/689]  eta: 0:14:08  lr: 0.000093  loss: 0.1573 (0.1674)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [160/689]  eta: 0:13:53  lr: 0.000093  loss: 0.1567 (0.1670)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [170/689]  eta: 0:13:37  lr: 0.000093  loss: 0.1642 (0.1675)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [180/689]  eta: 0:13:21  lr: 0.000093  loss: 0.1630 (0.1671)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [190/689]  eta: 0:13:06  lr: 0.000093  loss: 0.1619 (0.1672)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [200/689]  eta: 0:12:50  lr: 0.000093  loss: 0.1710 (0.1675)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [210/689]  eta: 0:12:34  lr: 0.000093  loss: 0.1687 (0.1675)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [220/689]  eta: 0:12:19  lr: 0.000093  loss: 0.1656 (0.1676)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [230/689]  eta: 0:12:03  lr: 0.000093  loss: 0.1656 (0.1676)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [240/689]  eta: 0:11:47  lr: 0.000093  loss: 0.1645 (0.1676)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [250/689]  eta: 0:11:31  lr: 0.000093  loss: 0.1631 (0.1675)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [260/689]  eta: 0:11:16  lr: 0.000093  loss: 0.1631 (0.1673)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [270/689]  eta: 0:11:00  lr: 0.000093  loss: 0.1604 (0.1672)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [280/689]  eta: 0:10:44  lr: 0.000093  loss: 0.1653 (0.1671)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [290/689]  eta: 0:10:29  lr: 0.000093  loss: 0.1662 (0.1671)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [300/689]  eta: 0:10:13  lr: 0.000093  loss: 0.1587 (0.1668)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [310/689]  eta: 0:09:57  lr: 0.000093  loss: 0.1592 (0.1669)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [320/689]  eta: 0:09:41  lr: 0.000093  loss: 0.1692 (0.1671)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [330/689]  eta: 0:09:26  lr: 0.000093  loss: 0.1684 (0.1675)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [340/689]  eta: 0:09:10  lr: 0.000093  loss: 0.1630 (0.1673)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [350/689]  eta: 0:08:54  lr: 0.000093  loss: 0.1662 (0.1677)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [360/689]  eta: 0:08:38  lr: 0.000093  loss: 0.1716 (0.1679)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [370/689]  eta: 0:08:23  lr: 0.000093  loss: 0.1700 (0.1681)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [380/689]  eta: 0:08:07  lr: 0.000093  loss: 0.1674 (0.1680)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [390/689]  eta: 0:07:51  lr: 0.000093  loss: 0.1674 (0.1681)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [400/689]  eta: 0:07:35  lr: 0.000093  loss: 0.1805 (0.1687)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [410/689]  eta: 0:07:20  lr: 0.000093  loss: 0.1805 (0.1687)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [420/689]  eta: 0:07:04  lr: 0.000093  loss: 0.1562 (0.1683)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [430/689]  eta: 0:06:48  lr: 0.000093  loss: 0.1655 (0.1686)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [440/689]  eta: 0:06:32  lr: 0.000093  loss: 0.1687 (0.1684)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [450/689]  eta: 0:06:16  lr: 0.000093  loss: 0.1554 (0.1682)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [460/689]  eta: 0:06:01  lr: 0.000093  loss: 0.1579 (0.1682)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [470/689]  eta: 0:05:45  lr: 0.000093  loss: 0.1623 (0.1682)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [480/689]  eta: 0:05:29  lr: 0.000093  loss: 0.1621 (0.1681)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [490/689]  eta: 0:05:13  lr: 0.000093  loss: 0.1598 (0.1680)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [500/689]  eta: 0:04:58  lr: 0.000093  loss: 0.1604 (0.1680)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [510/689]  eta: 0:04:42  lr: 0.000093  loss: 0.1637 (0.1680)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [520/689]  eta: 0:04:26  lr: 0.000093  loss: 0.1653 (0.1681)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [530/689]  eta: 0:04:10  lr: 0.000093  loss: 0.1665 (0.1680)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [540/689]  eta: 0:03:55  lr: 0.000093  loss: 0.1560 (0.1678)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [550/689]  eta: 0:03:39  lr: 0.000093  loss: 0.1593 (0.1678)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [560/689]  eta: 0:03:23  lr: 0.000093  loss: 0.1622 (0.1677)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [570/689]  eta: 0:03:07  lr: 0.000093  loss: 0.1599 (0.1677)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [580/689]  eta: 0:02:51  lr: 0.000093  loss: 0.1576 (0.1677)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [590/689]  eta: 0:02:36  lr: 0.000093  loss: 0.1571 (0.1676)  time: 1.5785  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:161]  [600/689]  eta: 0:02:20  lr: 0.000093  loss: 0.1571 (0.1676)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [610/689]  eta: 0:02:04  lr: 0.000093  loss: 0.1601 (0.1675)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [620/689]  eta: 0:01:48  lr: 0.000093  loss: 0.1603 (0.1676)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [630/689]  eta: 0:01:33  lr: 0.000093  loss: 0.1678 (0.1677)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [640/689]  eta: 0:01:17  lr: 0.000093  loss: 0.1651 (0.1677)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [650/689]  eta: 0:01:01  lr: 0.000093  loss: 0.1649 (0.1677)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [660/689]  eta: 0:00:45  lr: 0.000093  loss: 0.1664 (0.1676)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [670/689]  eta: 0:00:29  lr: 0.000093  loss: 0.1625 (0.1676)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [680/689]  eta: 0:00:14  lr: 0.000093  loss: 0.1580 (0.1675)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161]  [688/689]  eta: 0:00:01  lr: 0.000093  loss: 0.1569 (0.1674)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:161] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: 0.1569 (0.1674)\n",
      "Valid: [epoch:161]  [ 0/14]  eta: 0:00:13  loss: 0.1496 (0.1496)  time: 0.9727  data: 0.3778  max mem: 39763\n",
      "Valid: [epoch:161]  [13/14]  eta: 0:00:00  loss: 0.1538 (0.1562)  time: 0.1114  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:161] Total time: 0:00:01 (0.1208 s / it)\n",
      "Averaged stats: loss: 0.1538 (0.1562)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_161_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.156%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:162]  [  0/689]  eta: 0:11:43  lr: 0.000093  loss: 0.1613 (0.1613)  time: 1.0212  data: 0.5443  max mem: 39763\n",
      "Train: [epoch:162]  [ 10/689]  eta: 0:17:16  lr: 0.000093  loss: 0.1570 (0.1613)  time: 1.5263  data: 0.0496  max mem: 39763\n",
      "Train: [epoch:162]  [ 20/689]  eta: 0:17:17  lr: 0.000093  loss: 0.1576 (0.1633)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [ 30/689]  eta: 0:17:07  lr: 0.000093  loss: 0.1651 (0.1675)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [ 40/689]  eta: 0:16:55  lr: 0.000093  loss: 0.1708 (0.1695)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [ 50/689]  eta: 0:16:41  lr: 0.000093  loss: 0.1687 (0.1677)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [ 60/689]  eta: 0:16:26  lr: 0.000093  loss: 0.1675 (0.1684)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [ 70/689]  eta: 0:16:12  lr: 0.000093  loss: 0.1695 (0.1688)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [ 80/689]  eta: 0:15:56  lr: 0.000093  loss: 0.1583 (0.1673)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [ 90/689]  eta: 0:15:41  lr: 0.000093  loss: 0.1562 (0.1671)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [100/689]  eta: 0:15:26  lr: 0.000093  loss: 0.1575 (0.1673)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [110/689]  eta: 0:15:10  lr: 0.000093  loss: 0.1589 (0.1665)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [120/689]  eta: 0:14:55  lr: 0.000093  loss: 0.1594 (0.1665)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [130/689]  eta: 0:14:39  lr: 0.000093  loss: 0.1687 (0.1665)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [140/689]  eta: 0:14:24  lr: 0.000093  loss: 0.1664 (0.1668)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [150/689]  eta: 0:14:08  lr: 0.000093  loss: 0.1688 (0.1675)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [160/689]  eta: 0:13:53  lr: 0.000093  loss: 0.1688 (0.1672)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [170/689]  eta: 0:13:37  lr: 0.000093  loss: 0.1656 (0.1675)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [180/689]  eta: 0:13:21  lr: 0.000093  loss: 0.1685 (0.1677)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [190/689]  eta: 0:13:06  lr: 0.000093  loss: 0.1716 (0.1682)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [200/689]  eta: 0:12:50  lr: 0.000093  loss: 0.1716 (0.1680)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [210/689]  eta: 0:12:34  lr: 0.000093  loss: 0.1577 (0.1675)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [220/689]  eta: 0:12:19  lr: 0.000093  loss: 0.1605 (0.1676)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [230/689]  eta: 0:12:03  lr: 0.000093  loss: 0.1626 (0.1674)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [240/689]  eta: 0:11:47  lr: 0.000093  loss: 0.1622 (0.1675)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [250/689]  eta: 0:11:31  lr: 0.000093  loss: 0.1638 (0.1678)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [260/689]  eta: 0:11:16  lr: 0.000093  loss: 0.1695 (0.1679)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [270/689]  eta: 0:11:00  lr: 0.000093  loss: 0.1655 (0.1680)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [280/689]  eta: 0:10:44  lr: 0.000093  loss: 0.1585 (0.1678)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [290/689]  eta: 0:10:29  lr: 0.000093  loss: 0.1585 (0.1679)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [300/689]  eta: 0:10:13  lr: 0.000093  loss: 0.1710 (0.1680)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [310/689]  eta: 0:09:57  lr: 0.000093  loss: 0.1652 (0.1680)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [320/689]  eta: 0:09:41  lr: 0.000093  loss: 0.1661 (0.1680)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [330/689]  eta: 0:09:26  lr: 0.000093  loss: 0.1677 (0.1682)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [340/689]  eta: 0:09:10  lr: 0.000093  loss: 0.1648 (0.1681)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [350/689]  eta: 0:08:54  lr: 0.000093  loss: 0.1635 (0.1678)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [360/689]  eta: 0:08:38  lr: 0.000093  loss: 0.1635 (0.1679)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [370/689]  eta: 0:08:23  lr: 0.000093  loss: 0.1669 (0.1679)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [380/689]  eta: 0:08:07  lr: 0.000093  loss: 0.1613 (0.1678)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [390/689]  eta: 0:07:51  lr: 0.000093  loss: 0.1589 (0.1675)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [400/689]  eta: 0:07:35  lr: 0.000093  loss: 0.1636 (0.1678)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [410/689]  eta: 0:07:19  lr: 0.000093  loss: 0.1720 (0.1680)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [420/689]  eta: 0:07:04  lr: 0.000093  loss: 0.1652 (0.1678)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [430/689]  eta: 0:06:48  lr: 0.000093  loss: 0.1575 (0.1678)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [440/689]  eta: 0:06:32  lr: 0.000093  loss: 0.1575 (0.1677)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [450/689]  eta: 0:06:16  lr: 0.000093  loss: 0.1572 (0.1675)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [460/689]  eta: 0:06:01  lr: 0.000093  loss: 0.1552 (0.1675)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [470/689]  eta: 0:05:45  lr: 0.000093  loss: 0.1610 (0.1673)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [480/689]  eta: 0:05:29  lr: 0.000093  loss: 0.1654 (0.1675)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [490/689]  eta: 0:05:13  lr: 0.000093  loss: 0.1661 (0.1675)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [500/689]  eta: 0:04:58  lr: 0.000093  loss: 0.1603 (0.1675)  time: 1.5791  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:162]  [510/689]  eta: 0:04:42  lr: 0.000093  loss: 0.1678 (0.1677)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [520/689]  eta: 0:04:26  lr: 0.000093  loss: 0.1707 (0.1679)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [530/689]  eta: 0:04:10  lr: 0.000093  loss: 0.1703 (0.1680)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [540/689]  eta: 0:03:55  lr: 0.000093  loss: 0.1699 (0.1681)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [550/689]  eta: 0:03:39  lr: 0.000093  loss: 0.1653 (0.1681)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [560/689]  eta: 0:03:23  lr: 0.000093  loss: 0.1579 (0.1678)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [570/689]  eta: 0:03:07  lr: 0.000093  loss: 0.1556 (0.1677)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [580/689]  eta: 0:02:51  lr: 0.000093  loss: 0.1589 (0.1677)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [590/689]  eta: 0:02:36  lr: 0.000093  loss: 0.1686 (0.1679)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [600/689]  eta: 0:02:20  lr: 0.000093  loss: 0.1722 (0.1680)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [610/689]  eta: 0:02:04  lr: 0.000093  loss: 0.1636 (0.1679)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [620/689]  eta: 0:01:48  lr: 0.000093  loss: 0.1560 (0.1679)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [630/689]  eta: 0:01:33  lr: 0.000093  loss: 0.1668 (0.1682)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [640/689]  eta: 0:01:17  lr: 0.000093  loss: 0.1672 (0.1682)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [650/689]  eta: 0:01:01  lr: 0.000093  loss: 0.1662 (0.1682)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [660/689]  eta: 0:00:45  lr: 0.000093  loss: 0.1698 (0.1684)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [670/689]  eta: 0:00:29  lr: 0.000093  loss: 0.1687 (0.1683)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [680/689]  eta: 0:00:14  lr: 0.000093  loss: 0.1622 (0.1684)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162]  [688/689]  eta: 0:00:01  lr: 0.000093  loss: 0.1641 (0.1683)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:162] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: 0.1641 (0.1683)\n",
      "Valid: [epoch:162]  [ 0/14]  eta: 0:00:14  loss: 0.1538 (0.1538)  time: 1.0181  data: 0.3451  max mem: 39763\n",
      "Valid: [epoch:162]  [13/14]  eta: 0:00:00  loss: 0.1544 (0.1568)  time: 0.1147  data: 0.0247  max mem: 39763\n",
      "Valid: [epoch:162] Total time: 0:00:01 (0.1222 s / it)\n",
      "Averaged stats: loss: 0.1544 (0.1568)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_162_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.157%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:163]  [  0/689]  eta: 0:11:47  lr: 0.000093  loss: 0.1587 (0.1587)  time: 1.0267  data: 0.5490  max mem: 39763\n",
      "Train: [epoch:163]  [ 10/689]  eta: 0:17:16  lr: 0.000093  loss: 0.1634 (0.1627)  time: 1.5270  data: 0.0500  max mem: 39763\n",
      "Train: [epoch:163]  [ 20/689]  eta: 0:17:17  lr: 0.000093  loss: 0.1634 (0.1626)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [ 30/689]  eta: 0:17:08  lr: 0.000093  loss: 0.1627 (0.1625)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [ 40/689]  eta: 0:16:55  lr: 0.000093  loss: 0.1635 (0.1640)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [ 50/689]  eta: 0:16:41  lr: 0.000093  loss: 0.1634 (0.1646)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [ 60/689]  eta: 0:16:26  lr: 0.000093  loss: 0.1634 (0.1642)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [ 70/689]  eta: 0:16:11  lr: 0.000093  loss: 0.1580 (0.1645)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [ 80/689]  eta: 0:15:56  lr: 0.000093  loss: 0.1791 (0.1679)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [ 90/689]  eta: 0:15:41  lr: 0.000093  loss: 0.1740 (0.1676)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [100/689]  eta: 0:15:26  lr: 0.000093  loss: 0.1656 (0.1685)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [110/689]  eta: 0:15:10  lr: 0.000093  loss: 0.1626 (0.1684)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [120/689]  eta: 0:14:55  lr: 0.000093  loss: 0.1604 (0.1680)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [130/689]  eta: 0:14:39  lr: 0.000093  loss: 0.1609 (0.1676)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [140/689]  eta: 0:14:24  lr: 0.000093  loss: 0.1636 (0.1679)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [150/689]  eta: 0:14:08  lr: 0.000093  loss: 0.1671 (0.1680)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [160/689]  eta: 0:13:53  lr: 0.000093  loss: 0.1675 (0.1680)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [170/689]  eta: 0:13:37  lr: 0.000093  loss: 0.1631 (0.1671)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [180/689]  eta: 0:13:21  lr: 0.000093  loss: 0.1640 (0.1675)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [190/689]  eta: 0:13:06  lr: 0.000093  loss: 0.1752 (0.1679)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [200/689]  eta: 0:12:50  lr: 0.000093  loss: 0.1759 (0.1684)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [210/689]  eta: 0:12:34  lr: 0.000093  loss: 0.1747 (0.1683)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [220/689]  eta: 0:12:19  lr: 0.000093  loss: 0.1590 (0.1681)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [230/689]  eta: 0:12:03  lr: 0.000093  loss: 0.1588 (0.1679)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [240/689]  eta: 0:11:47  lr: 0.000093  loss: 0.1588 (0.1676)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [250/689]  eta: 0:11:32  lr: 0.000093  loss: 0.1567 (0.1675)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [260/689]  eta: 0:11:16  lr: 0.000093  loss: 0.1599 (0.1673)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [270/689]  eta: 0:11:00  lr: 0.000093  loss: 0.1602 (0.1673)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [280/689]  eta: 0:10:44  lr: 0.000093  loss: 0.1635 (0.1671)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [290/689]  eta: 0:10:29  lr: 0.000093  loss: 0.1604 (0.1674)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [300/689]  eta: 0:10:13  lr: 0.000093  loss: 0.1659 (0.1674)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [310/689]  eta: 0:09:57  lr: 0.000093  loss: 0.1659 (0.1671)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [320/689]  eta: 0:09:41  lr: 0.000093  loss: 0.1599 (0.1671)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [330/689]  eta: 0:09:26  lr: 0.000093  loss: 0.1599 (0.1669)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [340/689]  eta: 0:09:10  lr: 0.000093  loss: 0.1552 (0.1667)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [350/689]  eta: 0:08:54  lr: 0.000093  loss: 0.1549 (0.1667)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [360/689]  eta: 0:08:38  lr: 0.000093  loss: 0.1707 (0.1670)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [370/689]  eta: 0:08:23  lr: 0.000093  loss: 0.1686 (0.1670)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [380/689]  eta: 0:08:07  lr: 0.000093  loss: 0.1590 (0.1670)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [390/689]  eta: 0:07:51  lr: 0.000093  loss: 0.1585 (0.1670)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [400/689]  eta: 0:07:35  lr: 0.000093  loss: 0.1629 (0.1672)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [410/689]  eta: 0:07:20  lr: 0.000093  loss: 0.1711 (0.1673)  time: 1.5792  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:163]  [420/689]  eta: 0:07:04  lr: 0.000093  loss: 0.1703 (0.1674)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [430/689]  eta: 0:06:48  lr: 0.000093  loss: 0.1654 (0.1672)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [440/689]  eta: 0:06:32  lr: 0.000093  loss: 0.1617 (0.1671)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [450/689]  eta: 0:06:17  lr: 0.000093  loss: 0.1624 (0.1671)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [460/689]  eta: 0:06:01  lr: 0.000093  loss: 0.1657 (0.1673)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [470/689]  eta: 0:05:45  lr: 0.000093  loss: 0.1681 (0.1674)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [480/689]  eta: 0:05:29  lr: 0.000093  loss: 0.1701 (0.1674)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [490/689]  eta: 0:05:13  lr: 0.000093  loss: 0.1693 (0.1674)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [500/689]  eta: 0:04:58  lr: 0.000093  loss: 0.1693 (0.1675)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [510/689]  eta: 0:04:42  lr: 0.000093  loss: 0.1588 (0.1674)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [520/689]  eta: 0:04:26  lr: 0.000093  loss: 0.1543 (0.1673)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [530/689]  eta: 0:04:10  lr: 0.000093  loss: 0.1641 (0.1675)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [540/689]  eta: 0:03:55  lr: 0.000093  loss: 0.1714 (0.1675)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [550/689]  eta: 0:03:39  lr: 0.000093  loss: 0.1678 (0.1675)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [560/689]  eta: 0:03:23  lr: 0.000093  loss: 0.1602 (0.1675)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [570/689]  eta: 0:03:07  lr: 0.000093  loss: 0.1597 (0.1677)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [580/689]  eta: 0:02:51  lr: 0.000093  loss: 0.1596 (0.1676)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [590/689]  eta: 0:02:36  lr: 0.000093  loss: 0.1596 (0.1676)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [600/689]  eta: 0:02:20  lr: 0.000093  loss: 0.1701 (0.1678)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [610/689]  eta: 0:02:04  lr: 0.000093  loss: 0.1667 (0.1679)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [620/689]  eta: 0:01:48  lr: 0.000093  loss: 0.1648 (0.1680)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [630/689]  eta: 0:01:33  lr: 0.000093  loss: 0.1678 (0.1681)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [640/689]  eta: 0:01:17  lr: 0.000093  loss: 0.1749 (0.1682)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [650/689]  eta: 0:01:01  lr: 0.000093  loss: 0.1749 (0.1682)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [660/689]  eta: 0:00:45  lr: 0.000093  loss: 0.1581 (0.1680)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [670/689]  eta: 0:00:29  lr: 0.000093  loss: 0.1611 (0.1681)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [680/689]  eta: 0:00:14  lr: 0.000093  loss: 0.1679 (0.1682)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163]  [688/689]  eta: 0:00:01  lr: 0.000093  loss: 0.1647 (0.1681)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:163] Total time: 0:18:07 (1.5784 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: 0.1647 (0.1681)\n",
      "Valid: [epoch:163]  [ 0/14]  eta: 0:00:13  loss: 0.1650 (0.1650)  time: 0.9855  data: 0.4403  max mem: 39763\n",
      "Valid: [epoch:163]  [13/14]  eta: 0:00:00  loss: 0.1566 (0.1584)  time: 0.1124  data: 0.0315  max mem: 39763\n",
      "Valid: [epoch:163] Total time: 0:00:01 (0.1215 s / it)\n",
      "Averaged stats: loss: 0.1566 (0.1584)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_163_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.158%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:164]  [  0/689]  eta: 0:11:39  lr: 0.000093  loss: 0.1424 (0.1424)  time: 1.0154  data: 0.5376  max mem: 39763\n",
      "Train: [epoch:164]  [ 10/689]  eta: 0:17:16  lr: 0.000093  loss: 0.1562 (0.1624)  time: 1.5268  data: 0.0490  max mem: 39763\n",
      "Train: [epoch:164]  [ 20/689]  eta: 0:17:18  lr: 0.000093  loss: 0.1588 (0.1622)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [ 30/689]  eta: 0:17:08  lr: 0.000093  loss: 0.1652 (0.1646)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [ 40/689]  eta: 0:16:55  lr: 0.000093  loss: 0.1712 (0.1682)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [ 50/689]  eta: 0:16:41  lr: 0.000093  loss: 0.1747 (0.1684)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [ 60/689]  eta: 0:16:27  lr: 0.000093  loss: 0.1673 (0.1673)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [ 70/689]  eta: 0:16:12  lr: 0.000093  loss: 0.1592 (0.1675)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [ 80/689]  eta: 0:15:57  lr: 0.000093  loss: 0.1613 (0.1668)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [ 90/689]  eta: 0:15:41  lr: 0.000093  loss: 0.1626 (0.1671)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [100/689]  eta: 0:15:26  lr: 0.000093  loss: 0.1615 (0.1665)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [110/689]  eta: 0:15:10  lr: 0.000093  loss: 0.1569 (0.1666)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [120/689]  eta: 0:14:55  lr: 0.000093  loss: 0.1626 (0.1669)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [130/689]  eta: 0:14:39  lr: 0.000093  loss: 0.1704 (0.1672)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [140/689]  eta: 0:14:24  lr: 0.000093  loss: 0.1681 (0.1678)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [150/689]  eta: 0:14:08  lr: 0.000093  loss: 0.1657 (0.1678)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [160/689]  eta: 0:13:53  lr: 0.000093  loss: 0.1658 (0.1682)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [170/689]  eta: 0:13:37  lr: 0.000093  loss: 0.1632 (0.1682)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [180/689]  eta: 0:13:21  lr: 0.000093  loss: 0.1667 (0.1689)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [190/689]  eta: 0:13:06  lr: 0.000093  loss: 0.1681 (0.1689)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [200/689]  eta: 0:12:50  lr: 0.000093  loss: 0.1657 (0.1689)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [210/689]  eta: 0:12:34  lr: 0.000093  loss: 0.1569 (0.1690)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [220/689]  eta: 0:12:19  lr: 0.000093  loss: 0.1698 (0.1693)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [230/689]  eta: 0:12:03  lr: 0.000093  loss: 0.1633 (0.1689)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [240/689]  eta: 0:11:47  lr: 0.000093  loss: 0.1619 (0.1690)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [250/689]  eta: 0:11:32  lr: 0.000093  loss: 0.1698 (0.1693)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [260/689]  eta: 0:11:16  lr: 0.000093  loss: 0.1833 (0.1697)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [270/689]  eta: 0:11:00  lr: 0.000093  loss: 0.1701 (0.1695)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [280/689]  eta: 0:10:44  lr: 0.000093  loss: 0.1638 (0.1696)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [290/689]  eta: 0:10:29  lr: 0.000093  loss: 0.1594 (0.1691)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [300/689]  eta: 0:10:13  lr: 0.000093  loss: 0.1625 (0.1694)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [310/689]  eta: 0:09:57  lr: 0.000093  loss: 0.1713 (0.1692)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [320/689]  eta: 0:09:41  lr: 0.000093  loss: 0.1621 (0.1694)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:164]  [330/689]  eta: 0:09:26  lr: 0.000093  loss: 0.1765 (0.1696)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [340/689]  eta: 0:09:10  lr: 0.000093  loss: 0.1713 (0.1697)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [350/689]  eta: 0:08:54  lr: 0.000093  loss: 0.1645 (0.1698)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [360/689]  eta: 0:08:38  lr: 0.000093  loss: 0.1550 (0.1697)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [370/689]  eta: 0:08:23  lr: 0.000093  loss: 0.1631 (0.1697)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [380/689]  eta: 0:08:07  lr: 0.000093  loss: 0.1763 (0.1700)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [390/689]  eta: 0:07:51  lr: 0.000093  loss: 0.1763 (0.1702)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [400/689]  eta: 0:07:35  lr: 0.000093  loss: 0.1693 (0.1701)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [410/689]  eta: 0:07:20  lr: 0.000093  loss: 0.1538 (0.1696)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [420/689]  eta: 0:07:04  lr: 0.000093  loss: 0.1524 (0.1696)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [430/689]  eta: 0:06:48  lr: 0.000093  loss: 0.1631 (0.1695)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [440/689]  eta: 0:06:32  lr: 0.000093  loss: 0.1649 (0.1697)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [450/689]  eta: 0:06:17  lr: 0.000093  loss: 0.1783 (0.1702)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [460/689]  eta: 0:06:01  lr: 0.000093  loss: 0.1749 (0.1703)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [470/689]  eta: 0:05:45  lr: 0.000093  loss: 0.1729 (0.1703)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [480/689]  eta: 0:05:29  lr: 0.000093  loss: 0.1723 (0.1704)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [490/689]  eta: 0:05:13  lr: 0.000093  loss: 0.1646 (0.1703)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [500/689]  eta: 0:04:58  lr: 0.000093  loss: 0.1620 (0.1702)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [510/689]  eta: 0:04:42  lr: 0.000093  loss: 0.1643 (0.1701)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [520/689]  eta: 0:04:26  lr: 0.000093  loss: 0.1633 (0.1700)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [530/689]  eta: 0:04:10  lr: 0.000093  loss: 0.1652 (0.1700)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [540/689]  eta: 0:03:55  lr: 0.000093  loss: 0.1628 (0.1702)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [550/689]  eta: 0:03:39  lr: 0.000093  loss: 0.1607 (0.1700)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [560/689]  eta: 0:03:23  lr: 0.000093  loss: 0.1650 (0.1701)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [570/689]  eta: 0:03:07  lr: 0.000093  loss: 0.1773 (0.1701)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [580/689]  eta: 0:02:51  lr: 0.000093  loss: 0.1629 (0.1700)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [590/689]  eta: 0:02:36  lr: 0.000093  loss: 0.1573 (0.1699)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [600/689]  eta: 0:02:20  lr: 0.000093  loss: 0.1634 (0.1699)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [610/689]  eta: 0:02:04  lr: 0.000093  loss: 0.1641 (0.1697)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [620/689]  eta: 0:01:48  lr: 0.000093  loss: 0.1567 (0.1696)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [630/689]  eta: 0:01:33  lr: 0.000093  loss: 0.1660 (0.1697)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [640/689]  eta: 0:01:17  lr: 0.000093  loss: 0.1661 (0.1697)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [650/689]  eta: 0:01:01  lr: 0.000093  loss: 0.1622 (0.1697)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [660/689]  eta: 0:00:45  lr: 0.000093  loss: 0.1595 (0.1696)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [670/689]  eta: 0:00:29  lr: 0.000093  loss: 0.1670 (0.1696)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [680/689]  eta: 0:00:14  lr: 0.000093  loss: 0.1636 (0.1694)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164]  [688/689]  eta: 0:00:01  lr: 0.000093  loss: 0.1572 (0.1694)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:164] Total time: 0:18:07 (1.5781 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: 0.1572 (0.1694)\n",
      "Valid: [epoch:164]  [ 0/14]  eta: 0:00:13  loss: 0.1534 (0.1534)  time: 0.9980  data: 0.3912  max mem: 39763\n",
      "Valid: [epoch:164]  [13/14]  eta: 0:00:00  loss: 0.1582 (0.1605)  time: 0.1134  data: 0.0280  max mem: 39763\n",
      "Valid: [epoch:164] Total time: 0:00:01 (0.1223 s / it)\n",
      "Averaged stats: loss: 0.1582 (0.1605)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_164_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.161%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:165]  [  0/689]  eta: 0:11:57  lr: 0.000093  loss: 0.2224 (0.2224)  time: 1.0409  data: 0.5627  max mem: 39763\n",
      "Train: [epoch:165]  [ 10/689]  eta: 0:17:16  lr: 0.000093  loss: 0.1704 (0.1655)  time: 1.5264  data: 0.0512  max mem: 39763\n",
      "Train: [epoch:165]  [ 20/689]  eta: 0:17:17  lr: 0.000093  loss: 0.1722 (0.1715)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [ 30/689]  eta: 0:17:07  lr: 0.000093  loss: 0.1752 (0.1738)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [ 40/689]  eta: 0:16:55  lr: 0.000093  loss: 0.1723 (0.1747)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [ 50/689]  eta: 0:16:41  lr: 0.000093  loss: 0.1697 (0.1738)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [ 60/689]  eta: 0:16:26  lr: 0.000093  loss: 0.1746 (0.1763)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [ 70/689]  eta: 0:16:11  lr: 0.000093  loss: 0.1940 (0.1799)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [ 80/689]  eta: 0:15:56  lr: 0.000093  loss: 0.1940 (0.1813)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [ 90/689]  eta: 0:15:41  lr: 0.000093  loss: 0.1770 (0.1813)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [100/689]  eta: 0:15:26  lr: 0.000093  loss: 0.1775 (0.1823)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [110/689]  eta: 0:15:10  lr: 0.000093  loss: 0.1825 (0.1824)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [120/689]  eta: 0:14:55  lr: 0.000093  loss: 0.1737 (0.1816)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [130/689]  eta: 0:14:39  lr: 0.000093  loss: 0.1676 (0.1819)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [140/689]  eta: 0:14:24  lr: 0.000093  loss: 0.1804 (0.1822)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [150/689]  eta: 0:14:08  lr: 0.000093  loss: 0.1705 (0.1815)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [160/689]  eta: 0:13:52  lr: 0.000093  loss: 0.1664 (0.1801)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [170/689]  eta: 0:13:37  lr: 0.000093  loss: 0.1608 (0.1792)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [180/689]  eta: 0:13:21  lr: 0.000093  loss: 0.1693 (0.1787)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [190/689]  eta: 0:13:06  lr: 0.000093  loss: 0.1716 (0.1787)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [200/689]  eta: 0:12:50  lr: 0.000093  loss: 0.1652 (0.1781)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [210/689]  eta: 0:12:34  lr: 0.000093  loss: 0.1590 (0.1774)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [220/689]  eta: 0:12:19  lr: 0.000093  loss: 0.1663 (0.1775)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [230/689]  eta: 0:12:03  lr: 0.000093  loss: 0.1685 (0.1772)  time: 1.5796  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:165]  [240/689]  eta: 0:11:47  lr: 0.000093  loss: 0.1638 (0.1770)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [250/689]  eta: 0:11:31  lr: 0.000093  loss: 0.1638 (0.1763)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [260/689]  eta: 0:11:16  lr: 0.000093  loss: 0.1660 (0.1764)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [270/689]  eta: 0:11:00  lr: 0.000093  loss: 0.1804 (0.1766)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [280/689]  eta: 0:10:44  lr: 0.000093  loss: 0.1727 (0.1765)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [290/689]  eta: 0:10:29  lr: 0.000093  loss: 0.1708 (0.1764)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [300/689]  eta: 0:10:13  lr: 0.000093  loss: 0.1728 (0.1768)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [310/689]  eta: 0:09:57  lr: 0.000093  loss: 0.1860 (0.1769)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [320/689]  eta: 0:09:41  lr: 0.000093  loss: 0.1820 (0.1770)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [330/689]  eta: 0:09:26  lr: 0.000093  loss: 0.1793 (0.1772)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [340/689]  eta: 0:09:10  lr: 0.000093  loss: 0.1793 (0.1773)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [350/689]  eta: 0:08:54  lr: 0.000093  loss: 0.1733 (0.1772)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [360/689]  eta: 0:08:38  lr: 0.000093  loss: 0.1733 (0.1771)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [370/689]  eta: 0:08:23  lr: 0.000093  loss: 0.1659 (0.1771)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [380/689]  eta: 0:08:07  lr: 0.000093  loss: 0.1659 (0.1768)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [390/689]  eta: 0:07:51  lr: 0.000093  loss: 0.1671 (0.1765)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [400/689]  eta: 0:07:35  lr: 0.000093  loss: 0.1647 (0.1762)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [410/689]  eta: 0:07:20  lr: 0.000093  loss: 0.1586 (0.1758)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [420/689]  eta: 0:07:04  lr: 0.000093  loss: 0.1551 (0.1756)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [430/689]  eta: 0:06:48  lr: 0.000093  loss: 0.1656 (0.1756)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [440/689]  eta: 0:06:32  lr: 0.000093  loss: 0.1697 (0.1754)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [450/689]  eta: 0:06:16  lr: 0.000093  loss: 0.1621 (0.1753)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [460/689]  eta: 0:06:01  lr: 0.000093  loss: 0.1743 (0.1755)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [470/689]  eta: 0:05:45  lr: 0.000093  loss: 0.1743 (0.1752)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [480/689]  eta: 0:05:29  lr: 0.000093  loss: 0.1609 (0.1750)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [490/689]  eta: 0:05:13  lr: 0.000093  loss: 0.1609 (0.1748)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [500/689]  eta: 0:04:58  lr: 0.000093  loss: 0.1567 (0.1744)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [510/689]  eta: 0:04:42  lr: 0.000093  loss: 0.1596 (0.1742)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [520/689]  eta: 0:04:26  lr: 0.000093  loss: 0.1635 (0.1743)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [530/689]  eta: 0:04:10  lr: 0.000093  loss: 0.1884 (0.1747)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [540/689]  eta: 0:03:55  lr: 0.000093  loss: 0.1716 (0.1744)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [550/689]  eta: 0:03:39  lr: 0.000093  loss: 0.1597 (0.1741)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [560/689]  eta: 0:03:23  lr: 0.000093  loss: 0.1661 (0.1742)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [570/689]  eta: 0:03:07  lr: 0.000093  loss: 0.1735 (0.1741)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [580/689]  eta: 0:02:51  lr: 0.000093  loss: 0.1735 (0.1740)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [590/689]  eta: 0:02:36  lr: 0.000093  loss: 0.1678 (0.1740)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [600/689]  eta: 0:02:20  lr: 0.000093  loss: 0.1667 (0.1740)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [610/689]  eta: 0:02:04  lr: 0.000093  loss: 0.1667 (0.1740)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [620/689]  eta: 0:01:48  lr: 0.000093  loss: 0.1700 (0.1739)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [630/689]  eta: 0:01:33  lr: 0.000093  loss: 0.1700 (0.1739)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [640/689]  eta: 0:01:17  lr: 0.000093  loss: 0.1707 (0.1737)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [650/689]  eta: 0:01:01  lr: 0.000093  loss: 0.1694 (0.1738)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [660/689]  eta: 0:00:45  lr: 0.000093  loss: 0.1742 (0.1739)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [670/689]  eta: 0:00:29  lr: 0.000093  loss: 0.1742 (0.1738)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [680/689]  eta: 0:00:14  lr: 0.000093  loss: 0.1724 (0.1739)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165]  [688/689]  eta: 0:00:01  lr: 0.000093  loss: 0.1678 (0.1738)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:165] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: 0.1678 (0.1738)\n",
      "Valid: [epoch:165]  [ 0/14]  eta: 0:00:14  loss: 0.1590 (0.1590)  time: 1.0129  data: 0.3900  max mem: 39763\n",
      "Valid: [epoch:165]  [13/14]  eta: 0:00:00  loss: 0.1590 (0.1608)  time: 0.1143  data: 0.0279  max mem: 39763\n",
      "Valid: [epoch:165] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.1590 (0.1608)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_165_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.161%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:166]  [  0/689]  eta: 0:11:33  lr: 0.000093  loss: 0.1629 (0.1629)  time: 1.0067  data: 0.5255  max mem: 39763\n",
      "Train: [epoch:166]  [ 10/689]  eta: 0:17:15  lr: 0.000093  loss: 0.1605 (0.1666)  time: 1.5257  data: 0.0478  max mem: 39763\n",
      "Train: [epoch:166]  [ 20/689]  eta: 0:17:17  lr: 0.000093  loss: 0.1605 (0.1677)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [ 30/689]  eta: 0:17:07  lr: 0.000093  loss: 0.1639 (0.1680)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [ 40/689]  eta: 0:16:55  lr: 0.000093  loss: 0.1648 (0.1686)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [ 50/689]  eta: 0:16:41  lr: 0.000093  loss: 0.1648 (0.1692)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [ 60/689]  eta: 0:16:26  lr: 0.000093  loss: 0.1647 (0.1698)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [ 70/689]  eta: 0:16:11  lr: 0.000093  loss: 0.1700 (0.1704)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [ 80/689]  eta: 0:15:56  lr: 0.000093  loss: 0.1690 (0.1702)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [ 90/689]  eta: 0:15:41  lr: 0.000093  loss: 0.1662 (0.1700)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [100/689]  eta: 0:15:26  lr: 0.000093  loss: 0.1646 (0.1703)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [110/689]  eta: 0:15:10  lr: 0.000093  loss: 0.1715 (0.1712)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [120/689]  eta: 0:14:55  lr: 0.000093  loss: 0.1750 (0.1715)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [130/689]  eta: 0:14:39  lr: 0.000093  loss: 0.1709 (0.1711)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [140/689]  eta: 0:14:24  lr: 0.000093  loss: 0.1582 (0.1704)  time: 1.5781  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:166]  [150/689]  eta: 0:14:08  lr: 0.000093  loss: 0.1615 (0.1706)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [160/689]  eta: 0:13:52  lr: 0.000093  loss: 0.1627 (0.1703)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [170/689]  eta: 0:13:37  lr: 0.000093  loss: 0.1627 (0.1701)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [180/689]  eta: 0:13:21  lr: 0.000093  loss: 0.1645 (0.1708)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [190/689]  eta: 0:13:06  lr: 0.000093  loss: 0.1736 (0.1711)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [200/689]  eta: 0:12:50  lr: 0.000093  loss: 0.1671 (0.1708)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [210/689]  eta: 0:12:34  lr: 0.000093  loss: 0.1660 (0.1712)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [220/689]  eta: 0:12:18  lr: 0.000093  loss: 0.1617 (0.1707)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [230/689]  eta: 0:12:03  lr: 0.000093  loss: 0.1625 (0.1709)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [240/689]  eta: 0:11:47  lr: 0.000093  loss: 0.1686 (0.1709)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [250/689]  eta: 0:11:31  lr: 0.000093  loss: 0.1686 (0.1707)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [260/689]  eta: 0:11:16  lr: 0.000093  loss: 0.1614 (0.1704)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [270/689]  eta: 0:11:00  lr: 0.000093  loss: 0.1647 (0.1705)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [280/689]  eta: 0:10:44  lr: 0.000093  loss: 0.1752 (0.1711)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [290/689]  eta: 0:10:28  lr: 0.000093  loss: 0.1695 (0.1708)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [300/689]  eta: 0:10:13  lr: 0.000093  loss: 0.1685 (0.1707)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [310/689]  eta: 0:09:57  lr: 0.000093  loss: 0.1685 (0.1705)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [320/689]  eta: 0:09:41  lr: 0.000093  loss: 0.1665 (0.1704)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [330/689]  eta: 0:09:25  lr: 0.000093  loss: 0.1716 (0.1707)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [340/689]  eta: 0:09:10  lr: 0.000093  loss: 0.1748 (0.1708)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [350/689]  eta: 0:08:54  lr: 0.000093  loss: 0.1594 (0.1705)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [360/689]  eta: 0:08:38  lr: 0.000093  loss: 0.1605 (0.1708)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [370/689]  eta: 0:08:22  lr: 0.000093  loss: 0.1694 (0.1710)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [380/689]  eta: 0:08:07  lr: 0.000093  loss: 0.1574 (0.1707)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [390/689]  eta: 0:07:51  lr: 0.000093  loss: 0.1565 (0.1708)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [400/689]  eta: 0:07:35  lr: 0.000093  loss: 0.1674 (0.1707)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [410/689]  eta: 0:07:19  lr: 0.000093  loss: 0.1598 (0.1706)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [420/689]  eta: 0:07:04  lr: 0.000093  loss: 0.1571 (0.1706)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [430/689]  eta: 0:06:48  lr: 0.000093  loss: 0.1706 (0.1708)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [440/689]  eta: 0:06:32  lr: 0.000093  loss: 0.1706 (0.1709)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [450/689]  eta: 0:06:16  lr: 0.000093  loss: 0.1621 (0.1707)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [460/689]  eta: 0:06:01  lr: 0.000093  loss: 0.1624 (0.1706)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [470/689]  eta: 0:05:45  lr: 0.000093  loss: 0.1661 (0.1706)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [480/689]  eta: 0:05:29  lr: 0.000093  loss: 0.1706 (0.1706)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [490/689]  eta: 0:05:13  lr: 0.000093  loss: 0.1738 (0.1707)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [500/689]  eta: 0:04:58  lr: 0.000093  loss: 0.1780 (0.1709)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [510/689]  eta: 0:04:42  lr: 0.000093  loss: 0.1680 (0.1709)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [520/689]  eta: 0:04:26  lr: 0.000093  loss: 0.1669 (0.1712)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [530/689]  eta: 0:04:10  lr: 0.000093  loss: 0.1729 (0.1711)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [540/689]  eta: 0:03:54  lr: 0.000093  loss: 0.1729 (0.1712)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [550/689]  eta: 0:03:39  lr: 0.000093  loss: 0.1708 (0.1712)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [560/689]  eta: 0:03:23  lr: 0.000093  loss: 0.1726 (0.1714)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [570/689]  eta: 0:03:07  lr: 0.000093  loss: 0.1666 (0.1712)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [580/689]  eta: 0:02:51  lr: 0.000093  loss: 0.1661 (0.1712)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [590/689]  eta: 0:02:36  lr: 0.000093  loss: 0.1678 (0.1713)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [600/689]  eta: 0:02:20  lr: 0.000093  loss: 0.1666 (0.1714)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [610/689]  eta: 0:02:04  lr: 0.000093  loss: 0.1667 (0.1714)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [620/689]  eta: 0:01:48  lr: 0.000093  loss: 0.1677 (0.1715)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [630/689]  eta: 0:01:33  lr: 0.000093  loss: 0.1731 (0.1715)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [640/689]  eta: 0:01:17  lr: 0.000093  loss: 0.1683 (0.1715)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [650/689]  eta: 0:01:01  lr: 0.000093  loss: 0.1659 (0.1714)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [660/689]  eta: 0:00:45  lr: 0.000093  loss: 0.1711 (0.1716)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [670/689]  eta: 0:00:29  lr: 0.000093  loss: 0.1788 (0.1716)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [680/689]  eta: 0:00:14  lr: 0.000093  loss: 0.1761 (0.1717)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166]  [688/689]  eta: 0:00:01  lr: 0.000093  loss: 0.1754 (0.1716)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:166] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: 0.1754 (0.1716)\n",
      "Valid: [epoch:166]  [ 0/14]  eta: 0:00:13  loss: 0.1721 (0.1721)  time: 0.9522  data: 0.3893  max mem: 39763\n",
      "Valid: [epoch:166]  [13/14]  eta: 0:00:00  loss: 0.1637 (0.1648)  time: 0.1100  data: 0.0279  max mem: 39763\n",
      "Valid: [epoch:166] Total time: 0:00:01 (0.1193 s / it)\n",
      "Averaged stats: loss: 0.1637 (0.1648)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_166_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.165%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:167]  [  0/689]  eta: 0:11:36  lr: 0.000093  loss: 0.1636 (0.1636)  time: 1.0111  data: 0.5343  max mem: 39763\n",
      "Train: [epoch:167]  [ 10/689]  eta: 0:17:15  lr: 0.000093  loss: 0.1636 (0.1690)  time: 1.5248  data: 0.0486  max mem: 39763\n",
      "Train: [epoch:167]  [ 20/689]  eta: 0:17:17  lr: 0.000093  loss: 0.1634 (0.1655)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [ 30/689]  eta: 0:17:07  lr: 0.000093  loss: 0.1716 (0.1707)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [ 40/689]  eta: 0:16:54  lr: 0.000093  loss: 0.1676 (0.1686)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [ 50/689]  eta: 0:16:40  lr: 0.000093  loss: 0.1603 (0.1670)  time: 1.5772  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:167]  [ 60/689]  eta: 0:16:26  lr: 0.000093  loss: 0.1603 (0.1666)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [ 70/689]  eta: 0:16:11  lr: 0.000093  loss: 0.1586 (0.1654)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [ 80/689]  eta: 0:15:56  lr: 0.000093  loss: 0.1538 (0.1654)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [ 90/689]  eta: 0:15:41  lr: 0.000093  loss: 0.1770 (0.1678)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [100/689]  eta: 0:15:26  lr: 0.000093  loss: 0.1770 (0.1684)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [110/689]  eta: 0:15:10  lr: 0.000093  loss: 0.1716 (0.1696)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [120/689]  eta: 0:14:55  lr: 0.000093  loss: 0.1683 (0.1700)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [130/689]  eta: 0:14:39  lr: 0.000093  loss: 0.1684 (0.1700)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [140/689]  eta: 0:14:24  lr: 0.000093  loss: 0.1776 (0.1712)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [150/689]  eta: 0:14:08  lr: 0.000093  loss: 0.1776 (0.1711)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [160/689]  eta: 0:13:53  lr: 0.000093  loss: 0.1714 (0.1715)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [170/689]  eta: 0:13:37  lr: 0.000093  loss: 0.1724 (0.1721)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [180/689]  eta: 0:13:21  lr: 0.000093  loss: 0.1725 (0.1723)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [190/689]  eta: 0:13:06  lr: 0.000093  loss: 0.1690 (0.1722)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [200/689]  eta: 0:12:50  lr: 0.000093  loss: 0.1678 (0.1718)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [210/689]  eta: 0:12:34  lr: 0.000093  loss: 0.1583 (0.1717)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [220/689]  eta: 0:12:19  lr: 0.000093  loss: 0.1638 (0.1717)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [230/689]  eta: 0:12:03  lr: 0.000093  loss: 0.1638 (0.1714)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [240/689]  eta: 0:11:47  lr: 0.000093  loss: 0.1657 (0.1720)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [250/689]  eta: 0:11:31  lr: 0.000093  loss: 0.1649 (0.1719)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [260/689]  eta: 0:11:16  lr: 0.000093  loss: 0.1595 (0.1716)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [270/689]  eta: 0:11:00  lr: 0.000093  loss: 0.1596 (0.1717)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [280/689]  eta: 0:10:44  lr: 0.000093  loss: 0.1646 (0.1717)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [290/689]  eta: 0:10:29  lr: 0.000093  loss: 0.1788 (0.1721)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [300/689]  eta: 0:10:13  lr: 0.000093  loss: 0.1773 (0.1720)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [310/689]  eta: 0:09:57  lr: 0.000093  loss: 0.1622 (0.1719)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [320/689]  eta: 0:09:41  lr: 0.000093  loss: 0.1718 (0.1722)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [330/689]  eta: 0:09:26  lr: 0.000093  loss: 0.1613 (0.1720)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [340/689]  eta: 0:09:10  lr: 0.000093  loss: 0.1584 (0.1716)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [350/689]  eta: 0:08:54  lr: 0.000093  loss: 0.1638 (0.1716)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [360/689]  eta: 0:08:38  lr: 0.000093  loss: 0.1697 (0.1719)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [370/689]  eta: 0:08:23  lr: 0.000093  loss: 0.1705 (0.1720)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [380/689]  eta: 0:08:07  lr: 0.000093  loss: 0.1705 (0.1721)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [390/689]  eta: 0:07:51  lr: 0.000093  loss: 0.1722 (0.1721)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [400/689]  eta: 0:07:35  lr: 0.000093  loss: 0.1690 (0.1720)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [410/689]  eta: 0:07:20  lr: 0.000093  loss: 0.1685 (0.1720)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [420/689]  eta: 0:07:04  lr: 0.000093  loss: 0.1649 (0.1719)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [430/689]  eta: 0:06:48  lr: 0.000093  loss: 0.1657 (0.1721)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [440/689]  eta: 0:06:32  lr: 0.000093  loss: 0.1719 (0.1721)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [450/689]  eta: 0:06:16  lr: 0.000093  loss: 0.1703 (0.1722)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [460/689]  eta: 0:06:01  lr: 0.000093  loss: 0.1751 (0.1723)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [470/689]  eta: 0:05:45  lr: 0.000093  loss: 0.1726 (0.1724)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [480/689]  eta: 0:05:29  lr: 0.000093  loss: 0.1649 (0.1722)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [490/689]  eta: 0:05:13  lr: 0.000093  loss: 0.1662 (0.1723)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [500/689]  eta: 0:04:58  lr: 0.000093  loss: 0.1705 (0.1723)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [510/689]  eta: 0:04:42  lr: 0.000093  loss: 0.1699 (0.1723)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [520/689]  eta: 0:04:26  lr: 0.000093  loss: 0.1690 (0.1721)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [530/689]  eta: 0:04:10  lr: 0.000093  loss: 0.1694 (0.1722)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [540/689]  eta: 0:03:55  lr: 0.000093  loss: 0.1694 (0.1722)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [550/689]  eta: 0:03:39  lr: 0.000093  loss: 0.1608 (0.1719)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [560/689]  eta: 0:03:23  lr: 0.000093  loss: 0.1578 (0.1719)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [570/689]  eta: 0:03:07  lr: 0.000093  loss: 0.1744 (0.1719)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [580/689]  eta: 0:02:51  lr: 0.000093  loss: 0.1744 (0.1720)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [590/689]  eta: 0:02:36  lr: 0.000093  loss: 0.1746 (0.1721)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [600/689]  eta: 0:02:20  lr: 0.000093  loss: 0.1751 (0.1722)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [610/689]  eta: 0:02:04  lr: 0.000093  loss: 0.1652 (0.1721)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [620/689]  eta: 0:01:48  lr: 0.000093  loss: 0.1589 (0.1720)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [630/689]  eta: 0:01:33  lr: 0.000093  loss: 0.1589 (0.1720)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [640/689]  eta: 0:01:17  lr: 0.000093  loss: 0.1735 (0.1721)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [650/689]  eta: 0:01:01  lr: 0.000093  loss: 0.1637 (0.1719)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [660/689]  eta: 0:00:45  lr: 0.000093  loss: 0.1637 (0.1719)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [670/689]  eta: 0:00:29  lr: 0.000093  loss: 0.1699 (0.1721)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [680/689]  eta: 0:00:14  lr: 0.000093  loss: 0.1703 (0.1723)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167]  [688/689]  eta: 0:00:01  lr: 0.000093  loss: 0.1649 (0.1722)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:167] Total time: 0:18:07 (1.5781 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: 0.1649 (0.1722)\n",
      "Valid: [epoch:167]  [ 0/14]  eta: 0:00:14  loss: 0.1802 (0.1802)  time: 1.0136  data: 0.4064  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:167]  [13/14]  eta: 0:00:00  loss: 0.1627 (0.1651)  time: 0.1144  data: 0.0291  max mem: 39763\n",
      "Valid: [epoch:167] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.1627 (0.1651)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_167_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.165%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:168]  [  0/689]  eta: 0:11:45  lr: 0.000093  loss: 0.1929 (0.1929)  time: 1.0235  data: 0.5360  max mem: 39763\n",
      "Train: [epoch:168]  [ 10/689]  eta: 0:17:16  lr: 0.000093  loss: 0.1663 (0.1722)  time: 1.5267  data: 0.0488  max mem: 39763\n",
      "Train: [epoch:168]  [ 20/689]  eta: 0:17:17  lr: 0.000093  loss: 0.1666 (0.1744)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [ 30/689]  eta: 0:17:08  lr: 0.000093  loss: 0.1663 (0.1714)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [ 40/689]  eta: 0:16:55  lr: 0.000093  loss: 0.1644 (0.1727)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [ 50/689]  eta: 0:16:41  lr: 0.000093  loss: 0.1673 (0.1719)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [ 60/689]  eta: 0:16:27  lr: 0.000093  loss: 0.1673 (0.1717)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [ 70/689]  eta: 0:16:12  lr: 0.000093  loss: 0.1649 (0.1706)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [ 80/689]  eta: 0:15:57  lr: 0.000093  loss: 0.1662 (0.1714)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [ 90/689]  eta: 0:15:41  lr: 0.000093  loss: 0.1674 (0.1709)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [100/689]  eta: 0:15:26  lr: 0.000093  loss: 0.1599 (0.1707)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [110/689]  eta: 0:15:11  lr: 0.000093  loss: 0.1683 (0.1710)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [120/689]  eta: 0:14:55  lr: 0.000093  loss: 0.1690 (0.1715)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [130/689]  eta: 0:14:40  lr: 0.000093  loss: 0.1698 (0.1722)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [140/689]  eta: 0:14:24  lr: 0.000093  loss: 0.1698 (0.1727)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [150/689]  eta: 0:14:09  lr: 0.000093  loss: 0.1764 (0.1737)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [160/689]  eta: 0:13:53  lr: 0.000093  loss: 0.1778 (0.1741)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [170/689]  eta: 0:13:37  lr: 0.000093  loss: 0.1767 (0.1744)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [180/689]  eta: 0:13:22  lr: 0.000093  loss: 0.1741 (0.1748)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [190/689]  eta: 0:13:06  lr: 0.000093  loss: 0.1806 (0.1753)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [200/689]  eta: 0:12:50  lr: 0.000093  loss: 0.1731 (0.1749)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [210/689]  eta: 0:12:35  lr: 0.000093  loss: 0.1615 (0.1746)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [220/689]  eta: 0:12:19  lr: 0.000093  loss: 0.1677 (0.1750)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [230/689]  eta: 0:12:03  lr: 0.000093  loss: 0.1800 (0.1753)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [240/689]  eta: 0:11:47  lr: 0.000093  loss: 0.1786 (0.1752)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [250/689]  eta: 0:11:32  lr: 0.000093  loss: 0.1673 (0.1751)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [260/689]  eta: 0:11:16  lr: 0.000093  loss: 0.1651 (0.1752)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [270/689]  eta: 0:11:00  lr: 0.000093  loss: 0.1709 (0.1752)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [280/689]  eta: 0:10:45  lr: 0.000093  loss: 0.1691 (0.1752)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [290/689]  eta: 0:10:29  lr: 0.000093  loss: 0.1662 (0.1752)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [300/689]  eta: 0:10:13  lr: 0.000093  loss: 0.1609 (0.1751)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [310/689]  eta: 0:09:57  lr: 0.000093  loss: 0.1636 (0.1748)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [320/689]  eta: 0:09:42  lr: 0.000093  loss: 0.1645 (0.1747)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [330/689]  eta: 0:09:26  lr: 0.000093  loss: 0.1774 (0.1750)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [340/689]  eta: 0:09:10  lr: 0.000093  loss: 0.1729 (0.1750)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [350/689]  eta: 0:08:54  lr: 0.000093  loss: 0.1689 (0.1749)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [360/689]  eta: 0:08:38  lr: 0.000093  loss: 0.1647 (0.1747)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [370/689]  eta: 0:08:23  lr: 0.000093  loss: 0.1647 (0.1745)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [380/689]  eta: 0:08:07  lr: 0.000093  loss: 0.1693 (0.1743)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [390/689]  eta: 0:07:51  lr: 0.000093  loss: 0.1693 (0.1745)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [400/689]  eta: 0:07:35  lr: 0.000093  loss: 0.1750 (0.1746)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [410/689]  eta: 0:07:20  lr: 0.000093  loss: 0.1900 (0.1753)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [420/689]  eta: 0:07:04  lr: 0.000093  loss: 0.1911 (0.1754)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [430/689]  eta: 0:06:48  lr: 0.000093  loss: 0.1748 (0.1753)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [440/689]  eta: 0:06:32  lr: 0.000093  loss: 0.1649 (0.1752)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [450/689]  eta: 0:06:17  lr: 0.000093  loss: 0.1649 (0.1751)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [460/689]  eta: 0:06:01  lr: 0.000093  loss: 0.1829 (0.1756)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [470/689]  eta: 0:05:45  lr: 0.000093  loss: 0.1829 (0.1756)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [480/689]  eta: 0:05:29  lr: 0.000093  loss: 0.1655 (0.1754)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [490/689]  eta: 0:05:14  lr: 0.000093  loss: 0.1609 (0.1752)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [500/689]  eta: 0:04:58  lr: 0.000093  loss: 0.1675 (0.1751)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [510/689]  eta: 0:04:42  lr: 0.000093  loss: 0.1652 (0.1748)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [520/689]  eta: 0:04:26  lr: 0.000093  loss: 0.1681 (0.1750)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [530/689]  eta: 0:04:10  lr: 0.000093  loss: 0.1710 (0.1749)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [540/689]  eta: 0:03:55  lr: 0.000093  loss: 0.1673 (0.1749)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [550/689]  eta: 0:03:39  lr: 0.000093  loss: 0.1693 (0.1748)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [560/689]  eta: 0:03:23  lr: 0.000093  loss: 0.1653 (0.1747)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [570/689]  eta: 0:03:07  lr: 0.000093  loss: 0.1580 (0.1746)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [580/689]  eta: 0:02:52  lr: 0.000093  loss: 0.1666 (0.1747)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [590/689]  eta: 0:02:36  lr: 0.000093  loss: 0.1774 (0.1747)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [600/689]  eta: 0:02:20  lr: 0.000093  loss: 0.1774 (0.1748)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [610/689]  eta: 0:02:04  lr: 0.000093  loss: 0.1689 (0.1747)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [620/689]  eta: 0:01:48  lr: 0.000093  loss: 0.1656 (0.1747)  time: 1.5795  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:168]  [630/689]  eta: 0:01:33  lr: 0.000093  loss: 0.1647 (0.1746)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [640/689]  eta: 0:01:17  lr: 0.000093  loss: 0.1662 (0.1745)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [650/689]  eta: 0:01:01  lr: 0.000093  loss: 0.1662 (0.1744)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [660/689]  eta: 0:00:45  lr: 0.000093  loss: 0.1652 (0.1744)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [670/689]  eta: 0:00:29  lr: 0.000093  loss: 0.1721 (0.1745)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [680/689]  eta: 0:00:14  lr: 0.000093  loss: 0.1774 (0.1746)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168]  [688/689]  eta: 0:00:01  lr: 0.000093  loss: 0.1708 (0.1745)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:168] Total time: 0:18:07 (1.5786 s / it)\n",
      "Averaged stats: lr: 0.000093  loss: 0.1708 (0.1745)\n",
      "Valid: [epoch:168]  [ 0/14]  eta: 0:00:14  loss: 0.1735 (0.1735)  time: 1.0086  data: 0.3828  max mem: 39763\n",
      "Valid: [epoch:168]  [13/14]  eta: 0:00:00  loss: 0.1624 (0.1644)  time: 0.1140  data: 0.0274  max mem: 39763\n",
      "Valid: [epoch:168] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.1624 (0.1644)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_168_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.164%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:169]  [  0/689]  eta: 0:11:40  lr: 0.000092  loss: 0.1374 (0.1374)  time: 1.0163  data: 0.5385  max mem: 39763\n",
      "Train: [epoch:169]  [ 10/689]  eta: 0:17:16  lr: 0.000092  loss: 0.1636 (0.1664)  time: 1.5264  data: 0.0490  max mem: 39763\n",
      "Train: [epoch:169]  [ 20/689]  eta: 0:17:17  lr: 0.000092  loss: 0.1700 (0.1755)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [ 30/689]  eta: 0:17:08  lr: 0.000092  loss: 0.1722 (0.1738)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [ 40/689]  eta: 0:16:55  lr: 0.000092  loss: 0.1679 (0.1728)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [ 50/689]  eta: 0:16:41  lr: 0.000092  loss: 0.1710 (0.1741)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [ 60/689]  eta: 0:16:27  lr: 0.000092  loss: 0.1741 (0.1739)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [ 70/689]  eta: 0:16:12  lr: 0.000092  loss: 0.1753 (0.1754)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [ 80/689]  eta: 0:15:57  lr: 0.000092  loss: 0.1621 (0.1740)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [ 90/689]  eta: 0:15:42  lr: 0.000092  loss: 0.1606 (0.1744)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [100/689]  eta: 0:15:26  lr: 0.000092  loss: 0.1670 (0.1740)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [110/689]  eta: 0:15:11  lr: 0.000092  loss: 0.1606 (0.1737)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [120/689]  eta: 0:14:55  lr: 0.000092  loss: 0.1777 (0.1745)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [130/689]  eta: 0:14:40  lr: 0.000092  loss: 0.1811 (0.1751)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [140/689]  eta: 0:14:24  lr: 0.000092  loss: 0.1771 (0.1748)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [150/689]  eta: 0:14:09  lr: 0.000092  loss: 0.1688 (0.1749)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [160/689]  eta: 0:13:53  lr: 0.000092  loss: 0.1710 (0.1752)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [170/689]  eta: 0:13:37  lr: 0.000092  loss: 0.1731 (0.1748)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [180/689]  eta: 0:13:22  lr: 0.000092  loss: 0.1692 (0.1746)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [190/689]  eta: 0:13:06  lr: 0.000092  loss: 0.1673 (0.1753)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [200/689]  eta: 0:12:50  lr: 0.000092  loss: 0.1627 (0.1747)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [210/689]  eta: 0:12:35  lr: 0.000092  loss: 0.1627 (0.1748)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [220/689]  eta: 0:12:19  lr: 0.000092  loss: 0.1735 (0.1759)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [230/689]  eta: 0:12:03  lr: 0.000092  loss: 0.1809 (0.1763)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [240/689]  eta: 0:11:48  lr: 0.000092  loss: 0.1748 (0.1768)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [250/689]  eta: 0:11:32  lr: 0.000092  loss: 0.1749 (0.1770)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [260/689]  eta: 0:11:16  lr: 0.000092  loss: 0.1754 (0.1773)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [270/689]  eta: 0:11:01  lr: 0.000092  loss: 0.1804 (0.1777)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [280/689]  eta: 0:10:45  lr: 0.000092  loss: 0.1841 (0.1782)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [290/689]  eta: 0:10:29  lr: 0.000092  loss: 0.1838 (0.1781)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [300/689]  eta: 0:10:13  lr: 0.000092  loss: 0.1709 (0.1780)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [310/689]  eta: 0:09:58  lr: 0.000092  loss: 0.1754 (0.1780)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [320/689]  eta: 0:09:42  lr: 0.000092  loss: 0.1691 (0.1778)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [330/689]  eta: 0:09:26  lr: 0.000092  loss: 0.1684 (0.1781)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [340/689]  eta: 0:09:10  lr: 0.000092  loss: 0.1779 (0.1779)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [350/689]  eta: 0:08:55  lr: 0.000092  loss: 0.1651 (0.1774)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [360/689]  eta: 0:08:39  lr: 0.000092  loss: 0.1631 (0.1772)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [370/689]  eta: 0:08:23  lr: 0.000092  loss: 0.1644 (0.1769)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [380/689]  eta: 0:08:07  lr: 0.000092  loss: 0.1656 (0.1767)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [390/689]  eta: 0:07:52  lr: 0.000092  loss: 0.1656 (0.1767)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [400/689]  eta: 0:07:36  lr: 0.000092  loss: 0.1742 (0.1769)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [410/689]  eta: 0:07:20  lr: 0.000092  loss: 0.1789 (0.1768)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [420/689]  eta: 0:07:04  lr: 0.000092  loss: 0.1745 (0.1766)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [430/689]  eta: 0:06:49  lr: 0.000092  loss: 0.1711 (0.1766)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [440/689]  eta: 0:06:33  lr: 0.000092  loss: 0.1724 (0.1765)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [450/689]  eta: 0:06:17  lr: 0.000092  loss: 0.1723 (0.1765)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [460/689]  eta: 0:06:01  lr: 0.000092  loss: 0.1758 (0.1768)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [470/689]  eta: 0:05:45  lr: 0.000092  loss: 0.1758 (0.1767)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [480/689]  eta: 0:05:30  lr: 0.000092  loss: 0.1745 (0.1768)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [490/689]  eta: 0:05:14  lr: 0.000092  loss: 0.1785 (0.1768)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [500/689]  eta: 0:04:58  lr: 0.000092  loss: 0.1771 (0.1768)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [510/689]  eta: 0:04:42  lr: 0.000092  loss: 0.1666 (0.1766)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [520/689]  eta: 0:04:27  lr: 0.000092  loss: 0.1662 (0.1766)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [530/689]  eta: 0:04:11  lr: 0.000092  loss: 0.1671 (0.1765)  time: 1.5826  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:169]  [540/689]  eta: 0:03:55  lr: 0.000092  loss: 0.1664 (0.1765)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [550/689]  eta: 0:03:39  lr: 0.000092  loss: 0.1649 (0.1763)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [560/689]  eta: 0:03:23  lr: 0.000092  loss: 0.1667 (0.1764)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [570/689]  eta: 0:03:08  lr: 0.000092  loss: 0.1667 (0.1762)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [580/689]  eta: 0:02:52  lr: 0.000092  loss: 0.1602 (0.1760)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [590/689]  eta: 0:02:36  lr: 0.000092  loss: 0.1679 (0.1762)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [600/689]  eta: 0:02:20  lr: 0.000092  loss: 0.1709 (0.1762)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [610/689]  eta: 0:02:04  lr: 0.000092  loss: 0.1665 (0.1760)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [620/689]  eta: 0:01:49  lr: 0.000092  loss: 0.1675 (0.1760)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [630/689]  eta: 0:01:33  lr: 0.000092  loss: 0.1692 (0.1760)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [640/689]  eta: 0:01:17  lr: 0.000092  loss: 0.1689 (0.1762)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [650/689]  eta: 0:01:01  lr: 0.000092  loss: 0.1689 (0.1760)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [660/689]  eta: 0:00:45  lr: 0.000092  loss: 0.1775 (0.1761)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [670/689]  eta: 0:00:30  lr: 0.000092  loss: 0.1749 (0.1761)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [680/689]  eta: 0:00:14  lr: 0.000092  loss: 0.1747 (0.1763)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169]  [688/689]  eta: 0:00:01  lr: 0.000092  loss: 0.1738 (0.1762)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:169] Total time: 0:18:09 (1.5811 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: 0.1738 (0.1762)\n",
      "Valid: [epoch:169]  [ 0/14]  eta: 0:00:14  loss: 0.1629 (0.1629)  time: 1.0191  data: 0.3970  max mem: 39763\n",
      "Valid: [epoch:169]  [13/14]  eta: 0:00:00  loss: 0.1629 (0.1645)  time: 0.1149  data: 0.0284  max mem: 39763\n",
      "Valid: [epoch:169] Total time: 0:00:01 (0.1228 s / it)\n",
      "Averaged stats: loss: 0.1629 (0.1645)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_169_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.165%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:170]  [  0/689]  eta: 0:11:50  lr: 0.000092  loss: 0.1633 (0.1633)  time: 1.0311  data: 0.5526  max mem: 39763\n",
      "Train: [epoch:170]  [ 10/689]  eta: 0:17:20  lr: 0.000092  loss: 0.1713 (0.1814)  time: 1.5318  data: 0.0503  max mem: 39763\n",
      "Train: [epoch:170]  [ 20/689]  eta: 0:17:21  lr: 0.000092  loss: 0.1673 (0.1771)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [ 30/689]  eta: 0:17:11  lr: 0.000092  loss: 0.1712 (0.1759)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [ 40/689]  eta: 0:16:58  lr: 0.000092  loss: 0.1596 (0.1719)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [ 50/689]  eta: 0:16:44  lr: 0.000092  loss: 0.1626 (0.1717)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [ 60/689]  eta: 0:16:29  lr: 0.000092  loss: 0.1693 (0.1715)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [ 70/689]  eta: 0:16:15  lr: 0.000092  loss: 0.1693 (0.1733)  time: 1.5838  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [ 80/689]  eta: 0:15:59  lr: 0.000092  loss: 0.1728 (0.1736)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [ 90/689]  eta: 0:15:44  lr: 0.000092  loss: 0.1676 (0.1731)  time: 1.5837  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [100/689]  eta: 0:15:29  lr: 0.000092  loss: 0.1736 (0.1746)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [110/689]  eta: 0:15:13  lr: 0.000092  loss: 0.1790 (0.1750)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [120/689]  eta: 0:14:58  lr: 0.000092  loss: 0.1745 (0.1746)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [130/689]  eta: 0:14:42  lr: 0.000092  loss: 0.1745 (0.1747)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [140/689]  eta: 0:14:26  lr: 0.000092  loss: 0.1737 (0.1752)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [150/689]  eta: 0:14:11  lr: 0.000092  loss: 0.1777 (0.1758)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [160/689]  eta: 0:13:55  lr: 0.000092  loss: 0.1777 (0.1759)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [170/689]  eta: 0:13:39  lr: 0.000092  loss: 0.1694 (0.1760)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [180/689]  eta: 0:13:24  lr: 0.000092  loss: 0.1657 (0.1757)  time: 1.5838  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [190/689]  eta: 0:13:08  lr: 0.000092  loss: 0.1631 (0.1753)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [200/689]  eta: 0:12:52  lr: 0.000092  loss: 0.1715 (0.1757)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [210/689]  eta: 0:12:37  lr: 0.000092  loss: 0.1721 (0.1756)  time: 1.5838  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [220/689]  eta: 0:12:21  lr: 0.000092  loss: 0.1721 (0.1759)  time: 1.5840  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [230/689]  eta: 0:12:05  lr: 0.000092  loss: 0.1858 (0.1764)  time: 1.5844  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [240/689]  eta: 0:11:49  lr: 0.000092  loss: 0.1821 (0.1766)  time: 1.5845  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [250/689]  eta: 0:11:34  lr: 0.000092  loss: 0.1811 (0.1765)  time: 1.5839  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [260/689]  eta: 0:11:18  lr: 0.000092  loss: 0.1698 (0.1763)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [270/689]  eta: 0:11:02  lr: 0.000092  loss: 0.1774 (0.1767)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [280/689]  eta: 0:10:46  lr: 0.000092  loss: 0.1807 (0.1766)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [290/689]  eta: 0:10:30  lr: 0.000092  loss: 0.1763 (0.1767)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [300/689]  eta: 0:10:15  lr: 0.000092  loss: 0.1765 (0.1767)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [310/689]  eta: 0:09:59  lr: 0.000092  loss: 0.1765 (0.1766)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [320/689]  eta: 0:09:43  lr: 0.000092  loss: 0.1712 (0.1765)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [330/689]  eta: 0:09:27  lr: 0.000092  loss: 0.1719 (0.1768)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [340/689]  eta: 0:09:11  lr: 0.000092  loss: 0.1750 (0.1766)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [350/689]  eta: 0:08:56  lr: 0.000092  loss: 0.1649 (0.1764)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [360/689]  eta: 0:08:40  lr: 0.000092  loss: 0.1649 (0.1764)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [370/689]  eta: 0:08:24  lr: 0.000092  loss: 0.1843 (0.1766)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [380/689]  eta: 0:08:08  lr: 0.000092  loss: 0.1759 (0.1764)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [390/689]  eta: 0:07:52  lr: 0.000092  loss: 0.1645 (0.1762)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [400/689]  eta: 0:07:37  lr: 0.000092  loss: 0.1667 (0.1762)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [410/689]  eta: 0:07:21  lr: 0.000092  loss: 0.1672 (0.1759)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [420/689]  eta: 0:07:05  lr: 0.000092  loss: 0.1601 (0.1756)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [430/689]  eta: 0:06:49  lr: 0.000092  loss: 0.1601 (0.1756)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [440/689]  eta: 0:06:33  lr: 0.000092  loss: 0.1726 (0.1758)  time: 1.5824  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:170]  [450/689]  eta: 0:06:18  lr: 0.000092  loss: 0.1654 (0.1756)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [460/689]  eta: 0:06:02  lr: 0.000092  loss: 0.1683 (0.1763)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [470/689]  eta: 0:05:46  lr: 0.000092  loss: 0.1841 (0.1763)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [480/689]  eta: 0:05:30  lr: 0.000092  loss: 0.1765 (0.1762)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [490/689]  eta: 0:05:14  lr: 0.000092  loss: 0.1685 (0.1761)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [500/689]  eta: 0:04:58  lr: 0.000092  loss: 0.1668 (0.1759)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [510/689]  eta: 0:04:43  lr: 0.000092  loss: 0.1672 (0.1759)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [520/689]  eta: 0:04:27  lr: 0.000092  loss: 0.1718 (0.1758)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [530/689]  eta: 0:04:11  lr: 0.000092  loss: 0.1683 (0.1757)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [540/689]  eta: 0:03:55  lr: 0.000092  loss: 0.1782 (0.1758)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [550/689]  eta: 0:03:39  lr: 0.000092  loss: 0.1719 (0.1756)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [560/689]  eta: 0:03:24  lr: 0.000092  loss: 0.1694 (0.1758)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [570/689]  eta: 0:03:08  lr: 0.000092  loss: 0.1694 (0.1757)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [580/689]  eta: 0:02:52  lr: 0.000092  loss: 0.1635 (0.1756)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [590/689]  eta: 0:02:36  lr: 0.000092  loss: 0.1662 (0.1756)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [600/689]  eta: 0:02:20  lr: 0.000092  loss: 0.1702 (0.1756)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [610/689]  eta: 0:02:04  lr: 0.000092  loss: 0.1705 (0.1755)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [620/689]  eta: 0:01:49  lr: 0.000092  loss: 0.1767 (0.1756)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [630/689]  eta: 0:01:33  lr: 0.000092  loss: 0.1756 (0.1756)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [640/689]  eta: 0:01:17  lr: 0.000092  loss: 0.1721 (0.1756)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [650/689]  eta: 0:01:01  lr: 0.000092  loss: 0.1664 (0.1755)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [660/689]  eta: 0:00:45  lr: 0.000092  loss: 0.1655 (0.1754)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [670/689]  eta: 0:00:30  lr: 0.000092  loss: 0.1670 (0.1753)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [680/689]  eta: 0:00:14  lr: 0.000092  loss: 0.1690 (0.1753)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170]  [688/689]  eta: 0:00:01  lr: 0.000092  loss: 0.1692 (0.1752)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:170] Total time: 0:18:10 (1.5823 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: 0.1692 (0.1752)\n",
      "Valid: [epoch:170]  [ 0/14]  eta: 0:00:14  loss: 0.1585 (0.1585)  time: 1.0180  data: 0.3593  max mem: 39763\n",
      "Valid: [epoch:170]  [13/14]  eta: 0:00:00  loss: 0.1637 (0.1652)  time: 0.1148  data: 0.0257  max mem: 39763\n",
      "Valid: [epoch:170] Total time: 0:00:01 (0.1222 s / it)\n",
      "Averaged stats: loss: 0.1637 (0.1652)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_170_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.165%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:171]  [  0/689]  eta: 0:11:28  lr: 0.000092  loss: 0.1928 (0.1928)  time: 0.9992  data: 0.5218  max mem: 39763\n",
      "Train: [epoch:171]  [ 10/689]  eta: 0:17:15  lr: 0.000092  loss: 0.1733 (0.1845)  time: 1.5252  data: 0.0475  max mem: 39763\n",
      "Train: [epoch:171]  [ 20/689]  eta: 0:17:17  lr: 0.000092  loss: 0.1688 (0.1770)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [ 30/689]  eta: 0:17:07  lr: 0.000092  loss: 0.1664 (0.1766)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [ 40/689]  eta: 0:16:55  lr: 0.000092  loss: 0.1777 (0.1767)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [ 50/689]  eta: 0:16:41  lr: 0.000092  loss: 0.1777 (0.1793)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [ 60/689]  eta: 0:16:26  lr: 0.000092  loss: 0.1708 (0.1777)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [ 70/689]  eta: 0:16:12  lr: 0.000092  loss: 0.1673 (0.1777)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [ 80/689]  eta: 0:15:57  lr: 0.000092  loss: 0.1766 (0.1781)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [ 90/689]  eta: 0:15:41  lr: 0.000092  loss: 0.1766 (0.1778)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [100/689]  eta: 0:15:26  lr: 0.000092  loss: 0.1715 (0.1772)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [110/689]  eta: 0:15:11  lr: 0.000092  loss: 0.1719 (0.1772)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [120/689]  eta: 0:14:55  lr: 0.000092  loss: 0.1710 (0.1764)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [130/689]  eta: 0:14:40  lr: 0.000092  loss: 0.1662 (0.1769)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [140/689]  eta: 0:14:24  lr: 0.000092  loss: 0.1699 (0.1768)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [150/689]  eta: 0:14:09  lr: 0.000092  loss: 0.1742 (0.1769)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [160/689]  eta: 0:13:53  lr: 0.000092  loss: 0.1742 (0.1766)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [170/689]  eta: 0:13:37  lr: 0.000092  loss: 0.1691 (0.1764)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [180/689]  eta: 0:13:22  lr: 0.000092  loss: 0.1760 (0.1769)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [190/689]  eta: 0:13:06  lr: 0.000092  loss: 0.1760 (0.1767)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [200/689]  eta: 0:12:50  lr: 0.000092  loss: 0.1660 (0.1762)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [210/689]  eta: 0:12:35  lr: 0.000092  loss: 0.1597 (0.1754)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [220/689]  eta: 0:12:19  lr: 0.000092  loss: 0.1662 (0.1754)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [230/689]  eta: 0:12:03  lr: 0.000092  loss: 0.1701 (0.1758)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [240/689]  eta: 0:11:47  lr: 0.000092  loss: 0.1785 (0.1762)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [250/689]  eta: 0:11:32  lr: 0.000092  loss: 0.1785 (0.1764)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [260/689]  eta: 0:11:16  lr: 0.000092  loss: 0.1639 (0.1763)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [270/689]  eta: 0:11:00  lr: 0.000092  loss: 0.1657 (0.1758)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [280/689]  eta: 0:10:45  lr: 0.000092  loss: 0.1676 (0.1761)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [290/689]  eta: 0:10:29  lr: 0.000092  loss: 0.1747 (0.1760)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [300/689]  eta: 0:10:13  lr: 0.000092  loss: 0.1709 (0.1762)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [310/689]  eta: 0:09:57  lr: 0.000092  loss: 0.1685 (0.1758)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [320/689]  eta: 0:09:42  lr: 0.000092  loss: 0.1699 (0.1759)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [330/689]  eta: 0:09:26  lr: 0.000092  loss: 0.1696 (0.1759)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [340/689]  eta: 0:09:10  lr: 0.000092  loss: 0.1724 (0.1760)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [350/689]  eta: 0:08:54  lr: 0.000092  loss: 0.1720 (0.1759)  time: 1.5794  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:171]  [360/689]  eta: 0:08:39  lr: 0.000092  loss: 0.1701 (0.1761)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [370/689]  eta: 0:08:23  lr: 0.000092  loss: 0.1736 (0.1761)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [380/689]  eta: 0:08:07  lr: 0.000092  loss: 0.1736 (0.1760)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [390/689]  eta: 0:07:51  lr: 0.000092  loss: 0.1687 (0.1760)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [400/689]  eta: 0:07:35  lr: 0.000092  loss: 0.1735 (0.1763)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [410/689]  eta: 0:07:20  lr: 0.000092  loss: 0.1886 (0.1767)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [420/689]  eta: 0:07:04  lr: 0.000092  loss: 0.1741 (0.1766)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [430/689]  eta: 0:06:48  lr: 0.000092  loss: 0.1741 (0.1767)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [440/689]  eta: 0:06:32  lr: 0.000092  loss: 0.1769 (0.1769)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [450/689]  eta: 0:06:17  lr: 0.000092  loss: 0.1769 (0.1769)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [460/689]  eta: 0:06:01  lr: 0.000092  loss: 0.1771 (0.1769)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [470/689]  eta: 0:05:45  lr: 0.000092  loss: 0.1659 (0.1767)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [480/689]  eta: 0:05:29  lr: 0.000092  loss: 0.1674 (0.1769)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [490/689]  eta: 0:05:14  lr: 0.000092  loss: 0.1818 (0.1769)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [500/689]  eta: 0:04:58  lr: 0.000092  loss: 0.1660 (0.1766)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [510/689]  eta: 0:04:42  lr: 0.000092  loss: 0.1679 (0.1767)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [520/689]  eta: 0:04:26  lr: 0.000092  loss: 0.1748 (0.1768)  time: 1.5860  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [530/689]  eta: 0:04:11  lr: 0.000092  loss: 0.1711 (0.1769)  time: 1.6067  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [540/689]  eta: 0:03:55  lr: 0.000092  loss: 0.1781 (0.1770)  time: 1.5999  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [550/689]  eta: 0:03:39  lr: 0.000092  loss: 0.1751 (0.1769)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [560/689]  eta: 0:03:23  lr: 0.000092  loss: 0.1696 (0.1770)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [570/689]  eta: 0:03:07  lr: 0.000092  loss: 0.1642 (0.1769)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [580/689]  eta: 0:02:52  lr: 0.000092  loss: 0.1623 (0.1768)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [590/689]  eta: 0:02:36  lr: 0.000092  loss: 0.1720 (0.1768)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [600/689]  eta: 0:02:20  lr: 0.000092  loss: 0.1747 (0.1768)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [610/689]  eta: 0:02:04  lr: 0.000092  loss: 0.1683 (0.1766)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [620/689]  eta: 0:01:48  lr: 0.000092  loss: 0.1725 (0.1766)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [630/689]  eta: 0:01:33  lr: 0.000092  loss: 0.1686 (0.1764)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [640/689]  eta: 0:01:17  lr: 0.000092  loss: 0.1681 (0.1765)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [650/689]  eta: 0:01:01  lr: 0.000092  loss: 0.1803 (0.1765)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [660/689]  eta: 0:00:45  lr: 0.000092  loss: 0.1665 (0.1764)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [670/689]  eta: 0:00:30  lr: 0.000092  loss: 0.1653 (0.1764)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [680/689]  eta: 0:00:14  lr: 0.000092  loss: 0.1653 (0.1762)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171]  [688/689]  eta: 0:00:01  lr: 0.000092  loss: 0.1567 (0.1760)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:171] Total time: 0:18:08 (1.5793 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: 0.1567 (0.1760)\n",
      "Valid: [epoch:171]  [ 0/14]  eta: 0:00:14  loss: 0.1635 (0.1635)  time: 1.0439  data: 0.4035  max mem: 39763\n",
      "Valid: [epoch:171]  [13/14]  eta: 0:00:00  loss: 0.1635 (0.1657)  time: 0.1165  data: 0.0289  max mem: 39763\n",
      "Valid: [epoch:171] Total time: 0:00:01 (0.1264 s / it)\n",
      "Averaged stats: loss: 0.1635 (0.1657)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_171_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.166%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:172]  [  0/689]  eta: 0:11:38  lr: 0.000092  loss: 0.1562 (0.1562)  time: 1.0136  data: 0.5249  max mem: 39763\n",
      "Train: [epoch:172]  [ 10/689]  eta: 0:17:16  lr: 0.000092  loss: 0.1648 (0.1733)  time: 1.5269  data: 0.0478  max mem: 39763\n",
      "Train: [epoch:172]  [ 20/689]  eta: 0:17:18  lr: 0.000092  loss: 0.1647 (0.1720)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [ 30/689]  eta: 0:17:11  lr: 0.000092  loss: 0.1655 (0.1740)  time: 1.5856  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [ 40/689]  eta: 0:17:04  lr: 0.000092  loss: 0.1667 (0.1731)  time: 1.6073  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [ 50/689]  eta: 0:16:49  lr: 0.000092  loss: 0.1623 (0.1713)  time: 1.6015  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [ 60/689]  eta: 0:16:33  lr: 0.000092  loss: 0.1665 (0.1714)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [ 70/689]  eta: 0:16:20  lr: 0.000092  loss: 0.1672 (0.1711)  time: 1.5979  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [ 80/689]  eta: 0:16:06  lr: 0.000092  loss: 0.1673 (0.1711)  time: 1.6080  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [ 90/689]  eta: 0:15:49  lr: 0.000092  loss: 0.1608 (0.1703)  time: 1.5890  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [100/689]  eta: 0:15:33  lr: 0.000092  loss: 0.1650 (0.1718)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [110/689]  eta: 0:15:17  lr: 0.000092  loss: 0.1750 (0.1732)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [120/689]  eta: 0:15:01  lr: 0.000092  loss: 0.1677 (0.1724)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [130/689]  eta: 0:14:45  lr: 0.000092  loss: 0.1703 (0.1730)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [140/689]  eta: 0:14:29  lr: 0.000092  loss: 0.1754 (0.1742)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [150/689]  eta: 0:14:13  lr: 0.000092  loss: 0.1800 (0.1749)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [160/689]  eta: 0:13:57  lr: 0.000092  loss: 0.1804 (0.1756)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [170/689]  eta: 0:13:41  lr: 0.000092  loss: 0.1758 (0.1760)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [180/689]  eta: 0:13:25  lr: 0.000092  loss: 0.1729 (0.1760)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [190/689]  eta: 0:13:09  lr: 0.000092  loss: 0.1698 (0.1758)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [200/689]  eta: 0:12:53  lr: 0.000092  loss: 0.1657 (0.1751)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [210/689]  eta: 0:12:37  lr: 0.000092  loss: 0.1601 (0.1748)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [220/689]  eta: 0:12:21  lr: 0.000092  loss: 0.1731 (0.1750)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [230/689]  eta: 0:12:06  lr: 0.000092  loss: 0.1761 (0.1750)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [240/689]  eta: 0:11:50  lr: 0.000092  loss: 0.1660 (0.1749)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [250/689]  eta: 0:11:34  lr: 0.000092  loss: 0.1679 (0.1751)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [260/689]  eta: 0:11:18  lr: 0.000092  loss: 0.1754 (0.1754)  time: 1.5796  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:172]  [270/689]  eta: 0:11:02  lr: 0.000092  loss: 0.1862 (0.1758)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [280/689]  eta: 0:10:46  lr: 0.000092  loss: 0.1784 (0.1758)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [290/689]  eta: 0:10:30  lr: 0.000092  loss: 0.1705 (0.1758)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [300/689]  eta: 0:10:15  lr: 0.000092  loss: 0.1695 (0.1756)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [310/689]  eta: 0:09:59  lr: 0.000092  loss: 0.1692 (0.1755)  time: 1.6075  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [320/689]  eta: 0:09:44  lr: 0.000092  loss: 0.1675 (0.1755)  time: 1.6079  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [330/689]  eta: 0:09:28  lr: 0.000092  loss: 0.1702 (0.1755)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [340/689]  eta: 0:09:12  lr: 0.000092  loss: 0.1702 (0.1756)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [350/689]  eta: 0:08:56  lr: 0.000092  loss: 0.1752 (0.1758)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [360/689]  eta: 0:08:40  lr: 0.000092  loss: 0.1744 (0.1756)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [370/689]  eta: 0:08:24  lr: 0.000092  loss: 0.1759 (0.1760)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [380/689]  eta: 0:08:08  lr: 0.000092  loss: 0.1864 (0.1763)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [390/689]  eta: 0:07:53  lr: 0.000092  loss: 0.1861 (0.1764)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [400/689]  eta: 0:07:37  lr: 0.000092  loss: 0.1815 (0.1766)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [410/689]  eta: 0:07:21  lr: 0.000092  loss: 0.1879 (0.1769)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [420/689]  eta: 0:07:05  lr: 0.000092  loss: 0.1729 (0.1766)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [430/689]  eta: 0:06:49  lr: 0.000092  loss: 0.1668 (0.1767)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [440/689]  eta: 0:06:33  lr: 0.000092  loss: 0.1721 (0.1767)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [450/689]  eta: 0:06:18  lr: 0.000092  loss: 0.1682 (0.1766)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [460/689]  eta: 0:06:02  lr: 0.000092  loss: 0.1682 (0.1766)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [470/689]  eta: 0:05:46  lr: 0.000092  loss: 0.1710 (0.1765)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [480/689]  eta: 0:05:30  lr: 0.000092  loss: 0.1719 (0.1766)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [490/689]  eta: 0:05:14  lr: 0.000092  loss: 0.1719 (0.1766)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [500/689]  eta: 0:04:58  lr: 0.000092  loss: 0.1746 (0.1765)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [510/689]  eta: 0:04:43  lr: 0.000092  loss: 0.1780 (0.1768)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [520/689]  eta: 0:04:27  lr: 0.000092  loss: 0.1817 (0.1770)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [530/689]  eta: 0:04:11  lr: 0.000092  loss: 0.1808 (0.1767)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [540/689]  eta: 0:03:55  lr: 0.000092  loss: 0.1670 (0.1769)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [550/689]  eta: 0:03:39  lr: 0.000092  loss: 0.1677 (0.1768)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [560/689]  eta: 0:03:23  lr: 0.000092  loss: 0.1829 (0.1770)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [570/689]  eta: 0:03:08  lr: 0.000092  loss: 0.1882 (0.1774)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [580/689]  eta: 0:02:52  lr: 0.000092  loss: 0.1854 (0.1775)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [590/689]  eta: 0:02:36  lr: 0.000092  loss: 0.1690 (0.1775)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [600/689]  eta: 0:02:20  lr: 0.000092  loss: 0.1700 (0.1776)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [610/689]  eta: 0:02:04  lr: 0.000092  loss: 0.1773 (0.1776)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [620/689]  eta: 0:01:49  lr: 0.000092  loss: 0.1837 (0.1779)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [630/689]  eta: 0:01:33  lr: 0.000092  loss: 0.1812 (0.1778)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [640/689]  eta: 0:01:17  lr: 0.000092  loss: 0.1764 (0.1778)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [650/689]  eta: 0:01:01  lr: 0.000092  loss: 0.1763 (0.1778)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [660/689]  eta: 0:00:45  lr: 0.000092  loss: 0.1802 (0.1779)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [670/689]  eta: 0:00:30  lr: 0.000092  loss: 0.1713 (0.1777)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [680/689]  eta: 0:00:14  lr: 0.000092  loss: 0.1697 (0.1777)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172]  [688/689]  eta: 0:00:01  lr: 0.000092  loss: 0.1686 (0.1776)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:172] Total time: 0:18:09 (1.5812 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: 0.1686 (0.1776)\n",
      "Valid: [epoch:172]  [ 0/14]  eta: 0:00:14  loss: 0.1733 (0.1733)  time: 1.0217  data: 0.3857  max mem: 39763\n",
      "Valid: [epoch:172]  [13/14]  eta: 0:00:00  loss: 0.1641 (0.1662)  time: 0.1150  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:172] Total time: 0:00:01 (0.1242 s / it)\n",
      "Averaged stats: loss: 0.1641 (0.1662)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_172_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.166%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:173]  [  0/689]  eta: 0:11:33  lr: 0.000092  loss: 0.1622 (0.1622)  time: 1.0059  data: 0.5308  max mem: 39763\n",
      "Train: [epoch:173]  [ 10/689]  eta: 0:17:15  lr: 0.000092  loss: 0.1663 (0.1730)  time: 1.5249  data: 0.0483  max mem: 39763\n",
      "Train: [epoch:173]  [ 20/689]  eta: 0:17:17  lr: 0.000092  loss: 0.1644 (0.1685)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [ 30/689]  eta: 0:17:07  lr: 0.000092  loss: 0.1656 (0.1713)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [ 40/689]  eta: 0:17:03  lr: 0.000092  loss: 0.1760 (0.1723)  time: 1.6068  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [ 50/689]  eta: 0:16:48  lr: 0.000092  loss: 0.1620 (0.1703)  time: 1.6062  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [ 60/689]  eta: 0:16:32  lr: 0.000092  loss: 0.1659 (0.1723)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [ 70/689]  eta: 0:16:17  lr: 0.000092  loss: 0.1690 (0.1718)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [ 80/689]  eta: 0:16:01  lr: 0.000092  loss: 0.1678 (0.1722)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [ 90/689]  eta: 0:15:45  lr: 0.000092  loss: 0.1725 (0.1730)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [100/689]  eta: 0:15:29  lr: 0.000092  loss: 0.1725 (0.1738)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [110/689]  eta: 0:15:14  lr: 0.000092  loss: 0.1789 (0.1752)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [120/689]  eta: 0:14:58  lr: 0.000092  loss: 0.1701 (0.1749)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [130/689]  eta: 0:14:42  lr: 0.000092  loss: 0.1700 (0.1749)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [140/689]  eta: 0:14:26  lr: 0.000092  loss: 0.1790 (0.1756)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [150/689]  eta: 0:14:10  lr: 0.000092  loss: 0.1837 (0.1762)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [160/689]  eta: 0:13:55  lr: 0.000092  loss: 0.1878 (0.1769)  time: 1.5793  data: 0.0002  max mem: 39763\n",
      "Train: [epoch:173]  [170/689]  eta: 0:13:41  lr: 0.000092  loss: 0.1855 (0.1773)  time: 1.6068  data: 0.0002  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:173]  [180/689]  eta: 0:13:25  lr: 0.000092  loss: 0.1769 (0.1776)  time: 1.6087  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [190/689]  eta: 0:13:09  lr: 0.000092  loss: 0.1727 (0.1781)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [200/689]  eta: 0:12:53  lr: 0.000092  loss: 0.1820 (0.1785)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [210/689]  eta: 0:12:37  lr: 0.000092  loss: 0.1708 (0.1780)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [220/689]  eta: 0:12:21  lr: 0.000092  loss: 0.1758 (0.1785)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [230/689]  eta: 0:12:05  lr: 0.000092  loss: 0.1740 (0.1781)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [240/689]  eta: 0:11:50  lr: 0.000092  loss: 0.1740 (0.1786)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [250/689]  eta: 0:11:34  lr: 0.000092  loss: 0.1865 (0.1790)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [260/689]  eta: 0:11:18  lr: 0.000092  loss: 0.1720 (0.1791)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [270/689]  eta: 0:11:02  lr: 0.000092  loss: 0.1761 (0.1795)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [280/689]  eta: 0:10:46  lr: 0.000092  loss: 0.1854 (0.1799)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [290/689]  eta: 0:10:30  lr: 0.000092  loss: 0.1656 (0.1797)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [300/689]  eta: 0:10:14  lr: 0.000092  loss: 0.1760 (0.1797)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [310/689]  eta: 0:09:59  lr: 0.000092  loss: 0.1669 (0.1793)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [320/689]  eta: 0:09:43  lr: 0.000092  loss: 0.1669 (0.1794)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [330/689]  eta: 0:09:27  lr: 0.000092  loss: 0.1849 (0.1797)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [340/689]  eta: 0:09:11  lr: 0.000092  loss: 0.1854 (0.1799)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [350/689]  eta: 0:08:55  lr: 0.000092  loss: 0.1808 (0.1800)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [360/689]  eta: 0:08:40  lr: 0.000092  loss: 0.1717 (0.1799)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [370/689]  eta: 0:08:24  lr: 0.000092  loss: 0.1717 (0.1800)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [380/689]  eta: 0:08:08  lr: 0.000092  loss: 0.1738 (0.1799)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [390/689]  eta: 0:07:52  lr: 0.000092  loss: 0.1722 (0.1797)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [400/689]  eta: 0:07:36  lr: 0.000092  loss: 0.1633 (0.1793)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [410/689]  eta: 0:07:20  lr: 0.000092  loss: 0.1695 (0.1797)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [420/689]  eta: 0:07:05  lr: 0.000092  loss: 0.1856 (0.1798)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [430/689]  eta: 0:06:49  lr: 0.000092  loss: 0.1704 (0.1795)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [440/689]  eta: 0:06:33  lr: 0.000092  loss: 0.1694 (0.1793)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [450/689]  eta: 0:06:17  lr: 0.000092  loss: 0.1717 (0.1793)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [460/689]  eta: 0:06:01  lr: 0.000092  loss: 0.1764 (0.1794)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [470/689]  eta: 0:05:46  lr: 0.000092  loss: 0.1741 (0.1794)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [480/689]  eta: 0:05:30  lr: 0.000092  loss: 0.1736 (0.1794)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [490/689]  eta: 0:05:14  lr: 0.000092  loss: 0.1735 (0.1794)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [500/689]  eta: 0:04:58  lr: 0.000092  loss: 0.1751 (0.1794)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [510/689]  eta: 0:04:42  lr: 0.000092  loss: 0.1751 (0.1793)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [520/689]  eta: 0:04:27  lr: 0.000092  loss: 0.1712 (0.1792)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [530/689]  eta: 0:04:11  lr: 0.000092  loss: 0.1740 (0.1792)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [540/689]  eta: 0:03:55  lr: 0.000092  loss: 0.1782 (0.1791)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [550/689]  eta: 0:03:39  lr: 0.000092  loss: 0.1644 (0.1788)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [560/689]  eta: 0:03:23  lr: 0.000092  loss: 0.1666 (0.1789)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [570/689]  eta: 0:03:08  lr: 0.000092  loss: 0.1714 (0.1788)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [580/689]  eta: 0:02:52  lr: 0.000092  loss: 0.1712 (0.1787)  time: 1.5839  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [590/689]  eta: 0:02:36  lr: 0.000092  loss: 0.1715 (0.1787)  time: 1.5839  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [600/689]  eta: 0:02:20  lr: 0.000092  loss: 0.1773 (0.1787)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [610/689]  eta: 0:02:04  lr: 0.000092  loss: 0.1789 (0.1787)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [620/689]  eta: 0:01:49  lr: 0.000092  loss: 0.1658 (0.1785)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [630/689]  eta: 0:01:33  lr: 0.000092  loss: 0.1647 (0.1785)  time: 1.5838  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [640/689]  eta: 0:01:17  lr: 0.000092  loss: 0.1710 (0.1785)  time: 1.5838  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [650/689]  eta: 0:01:01  lr: 0.000092  loss: 0.1680 (0.1784)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [660/689]  eta: 0:00:45  lr: 0.000092  loss: 0.1663 (0.1784)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [670/689]  eta: 0:00:30  lr: 0.000092  loss: 0.1701 (0.1784)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [680/689]  eta: 0:00:14  lr: 0.000092  loss: 0.1731 (0.1783)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173]  [688/689]  eta: 0:00:01  lr: 0.000092  loss: 0.1734 (0.1785)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:173] Total time: 0:18:09 (1.5811 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: 0.1734 (0.1785)\n",
      "Valid: [epoch:173]  [ 0/14]  eta: 0:00:14  loss: 0.1530 (0.1530)  time: 1.0219  data: 0.4049  max mem: 39763\n",
      "Valid: [epoch:173]  [13/14]  eta: 0:00:00  loss: 0.1648 (0.1664)  time: 0.1151  data: 0.0290  max mem: 39763\n",
      "Valid: [epoch:173] Total time: 0:00:01 (0.1241 s / it)\n",
      "Averaged stats: loss: 0.1648 (0.1664)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_173_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.166%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:174]  [  0/689]  eta: 0:11:48  lr: 0.000092  loss: 0.1618 (0.1618)  time: 1.0289  data: 0.5471  max mem: 39763\n",
      "Train: [epoch:174]  [ 10/689]  eta: 0:17:20  lr: 0.000092  loss: 0.1652 (0.1752)  time: 1.5320  data: 0.0498  max mem: 39763\n",
      "Train: [epoch:174]  [ 20/689]  eta: 0:17:21  lr: 0.000092  loss: 0.1809 (0.1784)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [ 30/689]  eta: 0:17:11  lr: 0.000092  loss: 0.1809 (0.1787)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [ 40/689]  eta: 0:16:58  lr: 0.000092  loss: 0.1699 (0.1774)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [ 50/689]  eta: 0:16:44  lr: 0.000092  loss: 0.1687 (0.1755)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [ 60/689]  eta: 0:16:30  lr: 0.000092  loss: 0.1689 (0.1758)  time: 1.5837  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [ 70/689]  eta: 0:16:15  lr: 0.000092  loss: 0.1776 (0.1761)  time: 1.5842  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [ 80/689]  eta: 0:16:00  lr: 0.000092  loss: 0.1725 (0.1757)  time: 1.5839  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:174]  [ 90/689]  eta: 0:15:44  lr: 0.000092  loss: 0.1770 (0.1764)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [100/689]  eta: 0:15:29  lr: 0.000092  loss: 0.1697 (0.1758)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [110/689]  eta: 0:15:13  lr: 0.000092  loss: 0.1766 (0.1766)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [120/689]  eta: 0:14:58  lr: 0.000092  loss: 0.1791 (0.1767)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [130/689]  eta: 0:14:42  lr: 0.000092  loss: 0.1757 (0.1768)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [140/689]  eta: 0:14:26  lr: 0.000092  loss: 0.1707 (0.1769)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [150/689]  eta: 0:14:13  lr: 0.000092  loss: 0.1670 (0.1761)  time: 1.6130  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [160/689]  eta: 0:13:57  lr: 0.000092  loss: 0.1730 (0.1765)  time: 1.6143  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [170/689]  eta: 0:13:43  lr: 0.000092  loss: 0.1841 (0.1771)  time: 1.6126  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [180/689]  eta: 0:13:27  lr: 0.000092  loss: 0.1708 (0.1770)  time: 1.6115  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [190/689]  eta: 0:13:11  lr: 0.000092  loss: 0.1714 (0.1772)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [200/689]  eta: 0:12:55  lr: 0.000092  loss: 0.1714 (0.1769)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [210/689]  eta: 0:12:39  lr: 0.000092  loss: 0.1665 (0.1765)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [220/689]  eta: 0:12:23  lr: 0.000092  loss: 0.1752 (0.1764)  time: 1.5837  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [230/689]  eta: 0:12:07  lr: 0.000092  loss: 0.1701 (0.1763)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [240/689]  eta: 0:11:52  lr: 0.000092  loss: 0.1792 (0.1769)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [250/689]  eta: 0:11:36  lr: 0.000092  loss: 0.1819 (0.1773)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [260/689]  eta: 0:11:20  lr: 0.000092  loss: 0.1748 (0.1775)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [270/689]  eta: 0:11:04  lr: 0.000092  loss: 0.1741 (0.1774)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [280/689]  eta: 0:10:48  lr: 0.000092  loss: 0.1690 (0.1771)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [290/689]  eta: 0:10:32  lr: 0.000092  loss: 0.1782 (0.1775)  time: 1.5837  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [300/689]  eta: 0:10:16  lr: 0.000092  loss: 0.1757 (0.1774)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [310/689]  eta: 0:10:00  lr: 0.000092  loss: 0.1720 (0.1771)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [320/689]  eta: 0:09:44  lr: 0.000092  loss: 0.1721 (0.1772)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [330/689]  eta: 0:09:29  lr: 0.000092  loss: 0.1745 (0.1774)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [340/689]  eta: 0:09:13  lr: 0.000092  loss: 0.1752 (0.1773)  time: 1.6057  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [350/689]  eta: 0:08:57  lr: 0.000092  loss: 0.1755 (0.1772)  time: 1.6111  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [360/689]  eta: 0:08:41  lr: 0.000092  loss: 0.1681 (0.1770)  time: 1.5883  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [370/689]  eta: 0:08:26  lr: 0.000092  loss: 0.1642 (0.1768)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [380/689]  eta: 0:08:10  lr: 0.000092  loss: 0.1661 (0.1767)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [390/689]  eta: 0:07:54  lr: 0.000092  loss: 0.1781 (0.1769)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [400/689]  eta: 0:07:38  lr: 0.000092  loss: 0.1781 (0.1769)  time: 1.6082  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [410/689]  eta: 0:07:22  lr: 0.000092  loss: 0.1722 (0.1771)  time: 1.6124  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [420/689]  eta: 0:07:06  lr: 0.000092  loss: 0.1715 (0.1771)  time: 1.5871  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [430/689]  eta: 0:06:51  lr: 0.000092  loss: 0.1715 (0.1772)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [440/689]  eta: 0:06:35  lr: 0.000092  loss: 0.1780 (0.1773)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [450/689]  eta: 0:06:19  lr: 0.000092  loss: 0.1850 (0.1776)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [460/689]  eta: 0:06:03  lr: 0.000092  loss: 0.1902 (0.1779)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [470/689]  eta: 0:05:47  lr: 0.000092  loss: 0.1766 (0.1779)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [480/689]  eta: 0:05:31  lr: 0.000092  loss: 0.1722 (0.1781)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [490/689]  eta: 0:05:15  lr: 0.000092  loss: 0.1655 (0.1779)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [500/689]  eta: 0:04:59  lr: 0.000092  loss: 0.1656 (0.1779)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [510/689]  eta: 0:04:43  lr: 0.000092  loss: 0.1752 (0.1778)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [520/689]  eta: 0:04:28  lr: 0.000092  loss: 0.1759 (0.1779)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [530/689]  eta: 0:04:12  lr: 0.000092  loss: 0.1703 (0.1778)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [540/689]  eta: 0:03:56  lr: 0.000092  loss: 0.1708 (0.1782)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [550/689]  eta: 0:03:40  lr: 0.000092  loss: 0.1909 (0.1783)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [560/689]  eta: 0:03:24  lr: 0.000092  loss: 0.1699 (0.1783)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [570/689]  eta: 0:03:08  lr: 0.000092  loss: 0.1820 (0.1783)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [580/689]  eta: 0:02:52  lr: 0.000092  loss: 0.1823 (0.1784)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [590/689]  eta: 0:02:37  lr: 0.000092  loss: 0.1768 (0.1786)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [600/689]  eta: 0:02:21  lr: 0.000092  loss: 0.1688 (0.1785)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [610/689]  eta: 0:02:05  lr: 0.000092  loss: 0.1777 (0.1785)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [620/689]  eta: 0:01:49  lr: 0.000092  loss: 0.1899 (0.1788)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [630/689]  eta: 0:01:33  lr: 0.000092  loss: 0.1924 (0.1790)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [640/689]  eta: 0:01:17  lr: 0.000092  loss: 0.1816 (0.1791)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [650/689]  eta: 0:01:01  lr: 0.000092  loss: 0.1816 (0.1793)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [660/689]  eta: 0:00:45  lr: 0.000092  loss: 0.1877 (0.1794)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [670/689]  eta: 0:00:30  lr: 0.000092  loss: 0.1790 (0.1794)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [680/689]  eta: 0:00:14  lr: 0.000092  loss: 0.1834 (0.1796)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174]  [688/689]  eta: 0:00:01  lr: 0.000092  loss: 0.1887 (0.1797)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:174] Total time: 0:18:12 (1.5858 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: 0.1887 (0.1797)\n",
      "Valid: [epoch:174]  [ 0/14]  eta: 0:00:14  loss: 0.1838 (0.1838)  time: 1.0329  data: 0.3953  max mem: 39763\n",
      "Valid: [epoch:174]  [13/14]  eta: 0:00:00  loss: 0.1751 (0.1768)  time: 0.1158  data: 0.0283  max mem: 39763\n",
      "Valid: [epoch:174] Total time: 0:00:01 (0.1252 s / it)\n",
      "Averaged stats: loss: 0.1751 (0.1768)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_174_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.177%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:175]  [  0/689]  eta: 0:11:40  lr: 0.000092  loss: 0.1691 (0.1691)  time: 1.0170  data: 0.5396  max mem: 39763\n",
      "Train: [epoch:175]  [ 10/689]  eta: 0:17:16  lr: 0.000092  loss: 0.1856 (0.1833)  time: 1.5266  data: 0.0491  max mem: 39763\n",
      "Train: [epoch:175]  [ 20/689]  eta: 0:17:17  lr: 0.000092  loss: 0.1856 (0.1829)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [ 30/689]  eta: 0:17:08  lr: 0.000092  loss: 0.1876 (0.1870)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [ 40/689]  eta: 0:16:55  lr: 0.000092  loss: 0.1820 (0.1856)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [ 50/689]  eta: 0:16:41  lr: 0.000092  loss: 0.1789 (0.1858)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [ 60/689]  eta: 0:16:27  lr: 0.000092  loss: 0.1756 (0.1844)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [ 70/689]  eta: 0:16:12  lr: 0.000092  loss: 0.1667 (0.1824)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [ 80/689]  eta: 0:15:57  lr: 0.000092  loss: 0.1663 (0.1815)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [ 90/689]  eta: 0:15:42  lr: 0.000092  loss: 0.1666 (0.1803)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [100/689]  eta: 0:15:26  lr: 0.000092  loss: 0.1783 (0.1816)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [110/689]  eta: 0:15:11  lr: 0.000092  loss: 0.1814 (0.1824)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [120/689]  eta: 0:14:55  lr: 0.000092  loss: 0.1772 (0.1820)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [130/689]  eta: 0:14:40  lr: 0.000092  loss: 0.1704 (0.1818)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [140/689]  eta: 0:14:24  lr: 0.000092  loss: 0.1856 (0.1825)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [150/689]  eta: 0:14:09  lr: 0.000092  loss: 0.1892 (0.1824)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [160/689]  eta: 0:13:53  lr: 0.000092  loss: 0.1699 (0.1824)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [170/689]  eta: 0:13:37  lr: 0.000092  loss: 0.1680 (0.1817)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [180/689]  eta: 0:13:22  lr: 0.000092  loss: 0.1752 (0.1816)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [190/689]  eta: 0:13:06  lr: 0.000092  loss: 0.1703 (0.1813)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [200/689]  eta: 0:12:50  lr: 0.000092  loss: 0.1703 (0.1809)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [210/689]  eta: 0:12:35  lr: 0.000092  loss: 0.1792 (0.1812)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [220/689]  eta: 0:12:19  lr: 0.000092  loss: 0.1835 (0.1811)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [230/689]  eta: 0:12:03  lr: 0.000092  loss: 0.1726 (0.1811)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [240/689]  eta: 0:11:48  lr: 0.000092  loss: 0.1787 (0.1814)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [250/689]  eta: 0:11:32  lr: 0.000092  loss: 0.1787 (0.1817)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [260/689]  eta: 0:11:16  lr: 0.000092  loss: 0.1859 (0.1819)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [270/689]  eta: 0:11:00  lr: 0.000092  loss: 0.1859 (0.1822)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [280/689]  eta: 0:10:45  lr: 0.000092  loss: 0.1787 (0.1820)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [290/689]  eta: 0:10:29  lr: 0.000092  loss: 0.1749 (0.1820)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [300/689]  eta: 0:10:13  lr: 0.000092  loss: 0.1701 (0.1817)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [310/689]  eta: 0:09:57  lr: 0.000092  loss: 0.1725 (0.1815)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [320/689]  eta: 0:09:42  lr: 0.000092  loss: 0.1720 (0.1812)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [330/689]  eta: 0:09:26  lr: 0.000092  loss: 0.1672 (0.1811)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [340/689]  eta: 0:09:10  lr: 0.000092  loss: 0.1672 (0.1810)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [350/689]  eta: 0:08:54  lr: 0.000092  loss: 0.1684 (0.1812)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [360/689]  eta: 0:08:39  lr: 0.000092  loss: 0.1690 (0.1811)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [370/689]  eta: 0:08:23  lr: 0.000092  loss: 0.1690 (0.1809)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [380/689]  eta: 0:08:07  lr: 0.000092  loss: 0.1693 (0.1810)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [390/689]  eta: 0:07:51  lr: 0.000092  loss: 0.1801 (0.1810)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [400/689]  eta: 0:07:36  lr: 0.000092  loss: 0.1796 (0.1810)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [410/689]  eta: 0:07:20  lr: 0.000092  loss: 0.1715 (0.1807)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [420/689]  eta: 0:07:04  lr: 0.000092  loss: 0.1717 (0.1806)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [430/689]  eta: 0:06:48  lr: 0.000092  loss: 0.1810 (0.1809)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [440/689]  eta: 0:06:32  lr: 0.000092  loss: 0.1936 (0.1811)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [450/689]  eta: 0:06:17  lr: 0.000092  loss: 0.1840 (0.1810)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [460/689]  eta: 0:06:01  lr: 0.000092  loss: 0.1792 (0.1812)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [470/689]  eta: 0:05:45  lr: 0.000092  loss: 0.1792 (0.1810)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [480/689]  eta: 0:05:29  lr: 0.000092  loss: 0.1803 (0.1811)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [490/689]  eta: 0:05:14  lr: 0.000092  loss: 0.1827 (0.1812)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [500/689]  eta: 0:04:58  lr: 0.000092  loss: 0.1722 (0.1812)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [510/689]  eta: 0:04:42  lr: 0.000092  loss: 0.1671 (0.1810)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [520/689]  eta: 0:04:26  lr: 0.000092  loss: 0.1665 (0.1810)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [530/689]  eta: 0:04:10  lr: 0.000092  loss: 0.1761 (0.1811)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [540/689]  eta: 0:03:55  lr: 0.000092  loss: 0.1710 (0.1808)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [550/689]  eta: 0:03:39  lr: 0.000092  loss: 0.1705 (0.1810)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [560/689]  eta: 0:03:23  lr: 0.000092  loss: 0.1839 (0.1811)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [570/689]  eta: 0:03:07  lr: 0.000092  loss: 0.1861 (0.1812)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [580/689]  eta: 0:02:52  lr: 0.000092  loss: 0.1736 (0.1812)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [590/689]  eta: 0:02:36  lr: 0.000092  loss: 0.1736 (0.1811)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [600/689]  eta: 0:02:20  lr: 0.000092  loss: 0.1754 (0.1812)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [610/689]  eta: 0:02:04  lr: 0.000092  loss: 0.1724 (0.1811)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [620/689]  eta: 0:01:48  lr: 0.000092  loss: 0.1724 (0.1811)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [630/689]  eta: 0:01:33  lr: 0.000092  loss: 0.1736 (0.1810)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [640/689]  eta: 0:01:17  lr: 0.000092  loss: 0.1742 (0.1811)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [650/689]  eta: 0:01:01  lr: 0.000092  loss: 0.1705 (0.1809)  time: 1.5787  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:175]  [660/689]  eta: 0:00:45  lr: 0.000092  loss: 0.1675 (0.1807)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [670/689]  eta: 0:00:29  lr: 0.000092  loss: 0.1737 (0.1807)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [680/689]  eta: 0:00:14  lr: 0.000092  loss: 0.1777 (0.1808)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175]  [688/689]  eta: 0:00:01  lr: 0.000092  loss: 0.1779 (0.1807)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:175] Total time: 0:18:07 (1.5788 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: 0.1779 (0.1807)\n",
      "Valid: [epoch:175]  [ 0/14]  eta: 0:00:14  loss: 0.1650 (0.1650)  time: 1.0192  data: 0.3540  max mem: 39763\n",
      "Valid: [epoch:175]  [13/14]  eta: 0:00:00  loss: 0.1696 (0.1717)  time: 0.1148  data: 0.0253  max mem: 39763\n",
      "Valid: [epoch:175] Total time: 0:00:01 (0.1227 s / it)\n",
      "Averaged stats: loss: 0.1696 (0.1717)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_175_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.172%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:176]  [  0/689]  eta: 0:11:26  lr: 0.000092  loss: 0.1771 (0.1771)  time: 0.9969  data: 0.5165  max mem: 39763\n",
      "Train: [epoch:176]  [ 10/689]  eta: 0:17:15  lr: 0.000092  loss: 0.1752 (0.1766)  time: 1.5256  data: 0.0470  max mem: 39763\n",
      "Train: [epoch:176]  [ 20/689]  eta: 0:17:17  lr: 0.000092  loss: 0.1716 (0.1757)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [ 30/689]  eta: 0:17:07  lr: 0.000092  loss: 0.1707 (0.1756)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [ 40/689]  eta: 0:16:55  lr: 0.000092  loss: 0.1739 (0.1747)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [ 50/689]  eta: 0:16:41  lr: 0.000092  loss: 0.1804 (0.1791)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [ 60/689]  eta: 0:16:26  lr: 0.000092  loss: 0.1833 (0.1776)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [ 70/689]  eta: 0:16:12  lr: 0.000092  loss: 0.1733 (0.1782)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [ 80/689]  eta: 0:15:57  lr: 0.000092  loss: 0.1759 (0.1781)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [ 90/689]  eta: 0:15:41  lr: 0.000092  loss: 0.1709 (0.1772)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [100/689]  eta: 0:15:26  lr: 0.000092  loss: 0.1674 (0.1770)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [110/689]  eta: 0:15:10  lr: 0.000092  loss: 0.1775 (0.1788)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [120/689]  eta: 0:14:55  lr: 0.000092  loss: 0.1916 (0.1805)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [130/689]  eta: 0:14:39  lr: 0.000092  loss: 0.1794 (0.1801)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [140/689]  eta: 0:14:24  lr: 0.000092  loss: 0.1738 (0.1804)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [150/689]  eta: 0:14:08  lr: 0.000092  loss: 0.1738 (0.1801)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [160/689]  eta: 0:13:53  lr: 0.000092  loss: 0.1813 (0.1811)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [170/689]  eta: 0:13:37  lr: 0.000092  loss: 0.1819 (0.1810)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [180/689]  eta: 0:13:21  lr: 0.000092  loss: 0.1839 (0.1813)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [190/689]  eta: 0:13:06  lr: 0.000092  loss: 0.1801 (0.1810)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [200/689]  eta: 0:12:50  lr: 0.000092  loss: 0.1687 (0.1803)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [210/689]  eta: 0:12:34  lr: 0.000092  loss: 0.1720 (0.1803)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [220/689]  eta: 0:12:19  lr: 0.000092  loss: 0.1895 (0.1811)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [230/689]  eta: 0:12:03  lr: 0.000092  loss: 0.1870 (0.1811)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [240/689]  eta: 0:11:47  lr: 0.000092  loss: 0.1775 (0.1810)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [250/689]  eta: 0:11:32  lr: 0.000092  loss: 0.1800 (0.1812)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [260/689]  eta: 0:11:16  lr: 0.000092  loss: 0.1789 (0.1813)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [270/689]  eta: 0:11:00  lr: 0.000092  loss: 0.1749 (0.1810)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [280/689]  eta: 0:10:44  lr: 0.000092  loss: 0.1764 (0.1813)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [290/689]  eta: 0:10:29  lr: 0.000092  loss: 0.1819 (0.1814)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [300/689]  eta: 0:10:13  lr: 0.000092  loss: 0.1812 (0.1812)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [310/689]  eta: 0:09:57  lr: 0.000092  loss: 0.1669 (0.1809)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [320/689]  eta: 0:09:41  lr: 0.000092  loss: 0.1826 (0.1810)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [330/689]  eta: 0:09:26  lr: 0.000092  loss: 0.1826 (0.1809)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [340/689]  eta: 0:09:10  lr: 0.000092  loss: 0.1796 (0.1810)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [350/689]  eta: 0:08:54  lr: 0.000092  loss: 0.1815 (0.1810)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [360/689]  eta: 0:08:38  lr: 0.000092  loss: 0.1887 (0.1813)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [370/689]  eta: 0:08:23  lr: 0.000092  loss: 0.1862 (0.1811)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [380/689]  eta: 0:08:07  lr: 0.000092  loss: 0.1629 (0.1807)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [390/689]  eta: 0:07:51  lr: 0.000092  loss: 0.1724 (0.1809)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [400/689]  eta: 0:07:35  lr: 0.000092  loss: 0.1761 (0.1809)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [410/689]  eta: 0:07:20  lr: 0.000092  loss: 0.1744 (0.1810)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [420/689]  eta: 0:07:04  lr: 0.000092  loss: 0.1727 (0.1808)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [430/689]  eta: 0:06:48  lr: 0.000092  loss: 0.1694 (0.1808)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [440/689]  eta: 0:06:32  lr: 0.000092  loss: 0.1935 (0.1813)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [450/689]  eta: 0:06:17  lr: 0.000092  loss: 0.1888 (0.1811)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [460/689]  eta: 0:06:01  lr: 0.000092  loss: 0.1811 (0.1813)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [470/689]  eta: 0:05:45  lr: 0.000092  loss: 0.1811 (0.1813)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [480/689]  eta: 0:05:29  lr: 0.000092  loss: 0.1756 (0.1813)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [490/689]  eta: 0:05:14  lr: 0.000092  loss: 0.1709 (0.1812)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [500/689]  eta: 0:04:58  lr: 0.000092  loss: 0.1769 (0.1812)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [510/689]  eta: 0:04:42  lr: 0.000092  loss: 0.1895 (0.1815)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [520/689]  eta: 0:04:26  lr: 0.000092  loss: 0.1895 (0.1816)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [530/689]  eta: 0:04:10  lr: 0.000092  loss: 0.1837 (0.1817)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [540/689]  eta: 0:03:55  lr: 0.000092  loss: 0.1817 (0.1816)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [550/689]  eta: 0:03:39  lr: 0.000092  loss: 0.1782 (0.1816)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [560/689]  eta: 0:03:23  lr: 0.000092  loss: 0.1818 (0.1818)  time: 1.5793  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:176]  [570/689]  eta: 0:03:07  lr: 0.000092  loss: 0.1812 (0.1818)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [580/689]  eta: 0:02:52  lr: 0.000092  loss: 0.1782 (0.1819)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [590/689]  eta: 0:02:36  lr: 0.000092  loss: 0.1826 (0.1820)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [600/689]  eta: 0:02:20  lr: 0.000092  loss: 0.1823 (0.1820)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [610/689]  eta: 0:02:04  lr: 0.000092  loss: 0.1702 (0.1817)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [620/689]  eta: 0:01:48  lr: 0.000092  loss: 0.1653 (0.1818)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [630/689]  eta: 0:01:33  lr: 0.000092  loss: 0.1750 (0.1816)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [640/689]  eta: 0:01:17  lr: 0.000092  loss: 0.1766 (0.1817)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [650/689]  eta: 0:01:01  lr: 0.000092  loss: 0.1734 (0.1816)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [660/689]  eta: 0:00:45  lr: 0.000092  loss: 0.1705 (0.1815)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [670/689]  eta: 0:00:29  lr: 0.000092  loss: 0.1770 (0.1816)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [680/689]  eta: 0:00:14  lr: 0.000092  loss: 0.1787 (0.1815)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176]  [688/689]  eta: 0:00:01  lr: 0.000092  loss: 0.1770 (0.1815)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:176] Total time: 0:18:07 (1.5785 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: 0.1770 (0.1815)\n",
      "Valid: [epoch:176]  [ 0/14]  eta: 0:00:14  loss: 0.1579 (0.1579)  time: 1.0176  data: 0.3999  max mem: 39763\n",
      "Valid: [epoch:176]  [13/14]  eta: 0:00:00  loss: 0.1674 (0.1698)  time: 0.1146  data: 0.0286  max mem: 39763\n",
      "Valid: [epoch:176] Total time: 0:00:01 (0.1221 s / it)\n",
      "Averaged stats: loss: 0.1674 (0.1698)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_176_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.170%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:177]  [  0/689]  eta: 0:12:02  lr: 0.000092  loss: 0.1955 (0.1955)  time: 1.0485  data: 0.5733  max mem: 39763\n",
      "Train: [epoch:177]  [ 10/689]  eta: 0:17:17  lr: 0.000092  loss: 0.1922 (0.1935)  time: 1.5285  data: 0.0522  max mem: 39763\n",
      "Train: [epoch:177]  [ 20/689]  eta: 0:17:18  lr: 0.000092  loss: 0.1779 (0.1827)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [ 30/689]  eta: 0:17:08  lr: 0.000092  loss: 0.1687 (0.1853)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [ 40/689]  eta: 0:16:55  lr: 0.000092  loss: 0.1773 (0.1852)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [ 50/689]  eta: 0:16:41  lr: 0.000092  loss: 0.1733 (0.1821)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [ 60/689]  eta: 0:16:26  lr: 0.000092  loss: 0.1723 (0.1823)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [ 70/689]  eta: 0:16:12  lr: 0.000092  loss: 0.1714 (0.1804)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [ 80/689]  eta: 0:15:56  lr: 0.000092  loss: 0.1725 (0.1802)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [ 90/689]  eta: 0:15:41  lr: 0.000092  loss: 0.1749 (0.1799)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [100/689]  eta: 0:15:26  lr: 0.000092  loss: 0.1731 (0.1801)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [110/689]  eta: 0:15:10  lr: 0.000092  loss: 0.1735 (0.1800)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [120/689]  eta: 0:14:55  lr: 0.000092  loss: 0.1782 (0.1810)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [130/689]  eta: 0:14:39  lr: 0.000092  loss: 0.1868 (0.1811)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [140/689]  eta: 0:14:24  lr: 0.000092  loss: 0.1860 (0.1816)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [150/689]  eta: 0:14:08  lr: 0.000092  loss: 0.1850 (0.1817)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [160/689]  eta: 0:13:53  lr: 0.000092  loss: 0.1800 (0.1820)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [170/689]  eta: 0:13:37  lr: 0.000092  loss: 0.1800 (0.1824)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [180/689]  eta: 0:13:21  lr: 0.000092  loss: 0.1751 (0.1821)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [190/689]  eta: 0:13:06  lr: 0.000092  loss: 0.1751 (0.1821)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [200/689]  eta: 0:12:50  lr: 0.000092  loss: 0.1744 (0.1816)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [210/689]  eta: 0:12:34  lr: 0.000092  loss: 0.1744 (0.1813)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [220/689]  eta: 0:12:19  lr: 0.000092  loss: 0.1682 (0.1808)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [230/689]  eta: 0:12:03  lr: 0.000092  loss: 0.1678 (0.1807)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [240/689]  eta: 0:11:47  lr: 0.000092  loss: 0.1718 (0.1806)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [250/689]  eta: 0:11:32  lr: 0.000092  loss: 0.1787 (0.1806)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [260/689]  eta: 0:11:16  lr: 0.000092  loss: 0.1828 (0.1810)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [270/689]  eta: 0:11:00  lr: 0.000092  loss: 0.1688 (0.1805)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [280/689]  eta: 0:10:44  lr: 0.000092  loss: 0.1638 (0.1803)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [290/689]  eta: 0:10:29  lr: 0.000092  loss: 0.1659 (0.1799)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [300/689]  eta: 0:10:13  lr: 0.000092  loss: 0.1659 (0.1800)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [310/689]  eta: 0:09:57  lr: 0.000092  loss: 0.1741 (0.1800)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [320/689]  eta: 0:09:41  lr: 0.000092  loss: 0.1760 (0.1800)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [330/689]  eta: 0:09:26  lr: 0.000092  loss: 0.1815 (0.1801)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [340/689]  eta: 0:09:10  lr: 0.000092  loss: 0.1867 (0.1805)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [350/689]  eta: 0:08:54  lr: 0.000092  loss: 0.1880 (0.1805)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [360/689]  eta: 0:08:38  lr: 0.000092  loss: 0.1865 (0.1808)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [370/689]  eta: 0:08:23  lr: 0.000092  loss: 0.1800 (0.1809)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [380/689]  eta: 0:08:07  lr: 0.000092  loss: 0.1742 (0.1810)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [390/689]  eta: 0:07:51  lr: 0.000092  loss: 0.1826 (0.1812)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [400/689]  eta: 0:07:35  lr: 0.000092  loss: 0.1826 (0.1812)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [410/689]  eta: 0:07:20  lr: 0.000092  loss: 0.1773 (0.1812)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [420/689]  eta: 0:07:04  lr: 0.000092  loss: 0.1782 (0.1812)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [430/689]  eta: 0:06:48  lr: 0.000092  loss: 0.1789 (0.1814)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [440/689]  eta: 0:06:32  lr: 0.000092  loss: 0.1745 (0.1813)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [450/689]  eta: 0:06:17  lr: 0.000092  loss: 0.1719 (0.1812)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [460/689]  eta: 0:06:01  lr: 0.000092  loss: 0.1770 (0.1815)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [470/689]  eta: 0:05:45  lr: 0.000092  loss: 0.1788 (0.1813)  time: 1.5808  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:177]  [480/689]  eta: 0:05:29  lr: 0.000092  loss: 0.1785 (0.1815)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [490/689]  eta: 0:05:14  lr: 0.000092  loss: 0.1796 (0.1816)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [500/689]  eta: 0:04:58  lr: 0.000092  loss: 0.1824 (0.1819)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [510/689]  eta: 0:04:42  lr: 0.000092  loss: 0.1895 (0.1819)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [520/689]  eta: 0:04:26  lr: 0.000092  loss: 0.1784 (0.1819)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [530/689]  eta: 0:04:10  lr: 0.000092  loss: 0.1784 (0.1819)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [540/689]  eta: 0:03:55  lr: 0.000092  loss: 0.1783 (0.1819)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [550/689]  eta: 0:03:39  lr: 0.000092  loss: 0.1714 (0.1817)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [560/689]  eta: 0:03:23  lr: 0.000092  loss: 0.1711 (0.1817)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [570/689]  eta: 0:03:07  lr: 0.000092  loss: 0.1781 (0.1818)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [580/689]  eta: 0:02:52  lr: 0.000092  loss: 0.1811 (0.1819)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [590/689]  eta: 0:02:36  lr: 0.000092  loss: 0.1809 (0.1818)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [600/689]  eta: 0:02:20  lr: 0.000092  loss: 0.1758 (0.1819)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [610/689]  eta: 0:02:04  lr: 0.000092  loss: 0.1758 (0.1818)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [620/689]  eta: 0:01:48  lr: 0.000092  loss: 0.1758 (0.1819)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [630/689]  eta: 0:01:33  lr: 0.000092  loss: 0.1829 (0.1819)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [640/689]  eta: 0:01:17  lr: 0.000092  loss: 0.1775 (0.1819)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [650/689]  eta: 0:01:01  lr: 0.000092  loss: 0.1741 (0.1820)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [660/689]  eta: 0:00:45  lr: 0.000092  loss: 0.1741 (0.1820)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [670/689]  eta: 0:00:30  lr: 0.000092  loss: 0.1856 (0.1821)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [680/689]  eta: 0:00:14  lr: 0.000092  loss: 0.1857 (0.1821)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177]  [688/689]  eta: 0:00:01  lr: 0.000092  loss: 0.1765 (0.1820)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:177] Total time: 0:18:08 (1.5797 s / it)\n",
      "Averaged stats: lr: 0.000092  loss: 0.1765 (0.1820)\n",
      "Valid: [epoch:177]  [ 0/14]  eta: 0:00:14  loss: 0.1681 (0.1681)  time: 1.0267  data: 0.3887  max mem: 39763\n",
      "Valid: [epoch:177]  [13/14]  eta: 0:00:00  loss: 0.1692 (0.1714)  time: 0.1154  data: 0.0278  max mem: 39763\n",
      "Valid: [epoch:177] Total time: 0:00:01 (0.1246 s / it)\n",
      "Averaged stats: loss: 0.1692 (0.1714)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_177_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.171%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:178]  [  0/689]  eta: 0:11:34  lr: 0.000091  loss: 0.1614 (0.1614)  time: 1.0085  data: 0.5290  max mem: 39763\n",
      "Train: [epoch:178]  [ 10/689]  eta: 0:17:18  lr: 0.000091  loss: 0.1719 (0.1781)  time: 1.5296  data: 0.0482  max mem: 39763\n",
      "Train: [epoch:178]  [ 20/689]  eta: 0:17:20  lr: 0.000091  loss: 0.1746 (0.1790)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [ 30/689]  eta: 0:17:10  lr: 0.000091  loss: 0.1827 (0.1828)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [ 40/689]  eta: 0:16:57  lr: 0.000091  loss: 0.1820 (0.1822)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [ 50/689]  eta: 0:16:44  lr: 0.000091  loss: 0.1692 (0.1807)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [ 60/689]  eta: 0:16:29  lr: 0.000091  loss: 0.1688 (0.1797)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [ 70/689]  eta: 0:16:14  lr: 0.000091  loss: 0.1681 (0.1792)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [ 80/689]  eta: 0:15:59  lr: 0.000091  loss: 0.1743 (0.1812)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [ 90/689]  eta: 0:15:44  lr: 0.000091  loss: 0.1809 (0.1811)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [100/689]  eta: 0:15:28  lr: 0.000091  loss: 0.1804 (0.1817)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [110/689]  eta: 0:15:13  lr: 0.000091  loss: 0.1790 (0.1822)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [120/689]  eta: 0:14:57  lr: 0.000091  loss: 0.1803 (0.1830)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [130/689]  eta: 0:14:42  lr: 0.000091  loss: 0.1871 (0.1832)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [140/689]  eta: 0:14:26  lr: 0.000091  loss: 0.1695 (0.1828)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [150/689]  eta: 0:14:11  lr: 0.000091  loss: 0.1676 (0.1825)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [160/689]  eta: 0:13:55  lr: 0.000091  loss: 0.1759 (0.1822)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [170/689]  eta: 0:13:39  lr: 0.000091  loss: 0.1831 (0.1826)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [180/689]  eta: 0:13:24  lr: 0.000091  loss: 0.1768 (0.1822)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [190/689]  eta: 0:13:08  lr: 0.000091  loss: 0.1768 (0.1826)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [200/689]  eta: 0:12:52  lr: 0.000091  loss: 0.1861 (0.1830)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [210/689]  eta: 0:12:36  lr: 0.000091  loss: 0.1729 (0.1825)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [220/689]  eta: 0:12:21  lr: 0.000091  loss: 0.1722 (0.1825)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [230/689]  eta: 0:12:05  lr: 0.000091  loss: 0.1734 (0.1822)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [240/689]  eta: 0:11:49  lr: 0.000091  loss: 0.1709 (0.1823)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [250/689]  eta: 0:11:33  lr: 0.000091  loss: 0.1734 (0.1822)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [260/689]  eta: 0:11:18  lr: 0.000091  loss: 0.1762 (0.1822)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [270/689]  eta: 0:11:02  lr: 0.000091  loss: 0.1857 (0.1823)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [280/689]  eta: 0:10:46  lr: 0.000091  loss: 0.1909 (0.1831)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [290/689]  eta: 0:10:30  lr: 0.000091  loss: 0.1852 (0.1829)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [300/689]  eta: 0:10:14  lr: 0.000091  loss: 0.1684 (0.1828)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [310/689]  eta: 0:09:59  lr: 0.000091  loss: 0.1702 (0.1827)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [320/689]  eta: 0:09:43  lr: 0.000091  loss: 0.1718 (0.1825)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [330/689]  eta: 0:09:27  lr: 0.000091  loss: 0.1755 (0.1825)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [340/689]  eta: 0:09:11  lr: 0.000091  loss: 0.1830 (0.1827)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [350/689]  eta: 0:08:55  lr: 0.000091  loss: 0.1830 (0.1827)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [360/689]  eta: 0:08:40  lr: 0.000091  loss: 0.1770 (0.1827)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [370/689]  eta: 0:08:24  lr: 0.000091  loss: 0.1738 (0.1825)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [380/689]  eta: 0:08:08  lr: 0.000091  loss: 0.1714 (0.1824)  time: 1.5815  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:178]  [390/689]  eta: 0:07:52  lr: 0.000091  loss: 0.1730 (0.1825)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [400/689]  eta: 0:07:36  lr: 0.000091  loss: 0.1771 (0.1825)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [410/689]  eta: 0:07:21  lr: 0.000091  loss: 0.1809 (0.1827)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [420/689]  eta: 0:07:05  lr: 0.000091  loss: 0.1884 (0.1827)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [430/689]  eta: 0:06:49  lr: 0.000091  loss: 0.1710 (0.1827)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [440/689]  eta: 0:06:33  lr: 0.000091  loss: 0.1710 (0.1826)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [450/689]  eta: 0:06:17  lr: 0.000091  loss: 0.1766 (0.1827)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [460/689]  eta: 0:06:02  lr: 0.000091  loss: 0.1826 (0.1830)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [470/689]  eta: 0:05:46  lr: 0.000091  loss: 0.1885 (0.1831)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [480/689]  eta: 0:05:30  lr: 0.000091  loss: 0.1753 (0.1829)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [490/689]  eta: 0:05:14  lr: 0.000091  loss: 0.1731 (0.1830)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [500/689]  eta: 0:04:58  lr: 0.000091  loss: 0.1726 (0.1828)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [510/689]  eta: 0:04:43  lr: 0.000091  loss: 0.1739 (0.1829)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [520/689]  eta: 0:04:27  lr: 0.000091  loss: 0.1842 (0.1829)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [530/689]  eta: 0:04:11  lr: 0.000091  loss: 0.1844 (0.1828)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [540/689]  eta: 0:03:55  lr: 0.000091  loss: 0.1807 (0.1829)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [550/689]  eta: 0:03:39  lr: 0.000091  loss: 0.1803 (0.1828)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [560/689]  eta: 0:03:23  lr: 0.000091  loss: 0.1777 (0.1829)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [570/689]  eta: 0:03:08  lr: 0.000091  loss: 0.1792 (0.1829)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [580/689]  eta: 0:02:52  lr: 0.000091  loss: 0.1887 (0.1831)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [590/689]  eta: 0:02:36  lr: 0.000091  loss: 0.1855 (0.1831)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [600/689]  eta: 0:02:20  lr: 0.000091  loss: 0.1761 (0.1831)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [610/689]  eta: 0:02:04  lr: 0.000091  loss: 0.1751 (0.1831)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [620/689]  eta: 0:01:49  lr: 0.000091  loss: 0.1734 (0.1831)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [630/689]  eta: 0:01:33  lr: 0.000091  loss: 0.1747 (0.1830)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [640/689]  eta: 0:01:17  lr: 0.000091  loss: 0.1829 (0.1831)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [650/689]  eta: 0:01:01  lr: 0.000091  loss: 0.1865 (0.1831)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [660/689]  eta: 0:00:45  lr: 0.000091  loss: 0.1813 (0.1830)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [670/689]  eta: 0:00:30  lr: 0.000091  loss: 0.1852 (0.1831)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [680/689]  eta: 0:00:14  lr: 0.000091  loss: 0.1852 (0.1831)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178]  [688/689]  eta: 0:00:01  lr: 0.000091  loss: 0.1797 (0.1832)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:178] Total time: 0:18:09 (1.5816 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: 0.1797 (0.1832)\n",
      "Valid: [epoch:178]  [ 0/14]  eta: 0:00:14  loss: 0.1831 (0.1831)  time: 1.0236  data: 0.4281  max mem: 39763\n",
      "Valid: [epoch:178]  [13/14]  eta: 0:00:00  loss: 0.1741 (0.1766)  time: 0.1151  data: 0.0306  max mem: 39763\n",
      "Valid: [epoch:178] Total time: 0:00:01 (0.1257 s / it)\n",
      "Averaged stats: loss: 0.1741 (0.1766)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_178_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.177%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:179]  [  0/689]  eta: 0:11:54  lr: 0.000091  loss: 0.1808 (0.1808)  time: 1.0368  data: 0.5547  max mem: 39763\n",
      "Train: [epoch:179]  [ 10/689]  eta: 0:17:16  lr: 0.000091  loss: 0.1808 (0.1852)  time: 1.5261  data: 0.0505  max mem: 39763\n",
      "Train: [epoch:179]  [ 20/689]  eta: 0:17:17  lr: 0.000091  loss: 0.1738 (0.1826)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [ 30/689]  eta: 0:17:07  lr: 0.000091  loss: 0.1738 (0.1837)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [ 40/689]  eta: 0:16:54  lr: 0.000091  loss: 0.1757 (0.1819)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [ 50/689]  eta: 0:16:40  lr: 0.000091  loss: 0.1764 (0.1827)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [ 60/689]  eta: 0:16:26  lr: 0.000091  loss: 0.1863 (0.1836)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [ 70/689]  eta: 0:16:11  lr: 0.000091  loss: 0.1826 (0.1831)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [ 80/689]  eta: 0:15:56  lr: 0.000091  loss: 0.1763 (0.1830)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [ 90/689]  eta: 0:15:41  lr: 0.000091  loss: 0.1763 (0.1822)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [100/689]  eta: 0:15:26  lr: 0.000091  loss: 0.1807 (0.1820)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [110/689]  eta: 0:15:10  lr: 0.000091  loss: 0.1853 (0.1831)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [120/689]  eta: 0:14:55  lr: 0.000091  loss: 0.1805 (0.1829)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [130/689]  eta: 0:14:39  lr: 0.000091  loss: 0.1758 (0.1828)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [140/689]  eta: 0:14:24  lr: 0.000091  loss: 0.1885 (0.1835)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [150/689]  eta: 0:14:08  lr: 0.000091  loss: 0.1874 (0.1834)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [160/689]  eta: 0:13:52  lr: 0.000091  loss: 0.1745 (0.1829)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [170/689]  eta: 0:13:37  lr: 0.000091  loss: 0.1787 (0.1830)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [180/689]  eta: 0:13:21  lr: 0.000091  loss: 0.1802 (0.1833)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [190/689]  eta: 0:13:06  lr: 0.000091  loss: 0.1912 (0.1839)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [200/689]  eta: 0:12:50  lr: 0.000091  loss: 0.1912 (0.1842)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [210/689]  eta: 0:12:34  lr: 0.000091  loss: 0.1816 (0.1843)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [220/689]  eta: 0:12:19  lr: 0.000091  loss: 0.1802 (0.1844)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [230/689]  eta: 0:12:03  lr: 0.000091  loss: 0.1842 (0.1849)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [240/689]  eta: 0:11:47  lr: 0.000091  loss: 0.1916 (0.1855)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [250/689]  eta: 0:11:31  lr: 0.000091  loss: 0.1859 (0.1852)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [260/689]  eta: 0:11:16  lr: 0.000091  loss: 0.1765 (0.1849)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [270/689]  eta: 0:11:00  lr: 0.000091  loss: 0.1715 (0.1849)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [280/689]  eta: 0:10:44  lr: 0.000091  loss: 0.1788 (0.1851)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [290/689]  eta: 0:10:29  lr: 0.000091  loss: 0.1750 (0.1846)  time: 1.5791  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:179]  [300/689]  eta: 0:10:13  lr: 0.000091  loss: 0.1750 (0.1846)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [310/689]  eta: 0:09:57  lr: 0.000091  loss: 0.1752 (0.1842)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [320/689]  eta: 0:09:41  lr: 0.000091  loss: 0.1641 (0.1835)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [330/689]  eta: 0:09:26  lr: 0.000091  loss: 0.1686 (0.1841)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [340/689]  eta: 0:09:10  lr: 0.000091  loss: 0.1849 (0.1840)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [350/689]  eta: 0:08:54  lr: 0.000091  loss: 0.1727 (0.1840)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [360/689]  eta: 0:08:38  lr: 0.000091  loss: 0.1727 (0.1837)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [370/689]  eta: 0:08:23  lr: 0.000091  loss: 0.1727 (0.1836)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [380/689]  eta: 0:08:07  lr: 0.000091  loss: 0.1669 (0.1832)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [390/689]  eta: 0:07:51  lr: 0.000091  loss: 0.1748 (0.1833)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [400/689]  eta: 0:07:35  lr: 0.000091  loss: 0.1890 (0.1838)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [410/689]  eta: 0:07:19  lr: 0.000091  loss: 0.1796 (0.1838)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [420/689]  eta: 0:07:04  lr: 0.000091  loss: 0.1720 (0.1835)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [430/689]  eta: 0:06:48  lr: 0.000091  loss: 0.1644 (0.1834)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [440/689]  eta: 0:06:32  lr: 0.000091  loss: 0.1776 (0.1834)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [450/689]  eta: 0:06:16  lr: 0.000091  loss: 0.1750 (0.1834)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [460/689]  eta: 0:06:01  lr: 0.000091  loss: 0.1750 (0.1833)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [470/689]  eta: 0:05:45  lr: 0.000091  loss: 0.1769 (0.1835)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [480/689]  eta: 0:05:29  lr: 0.000091  loss: 0.1763 (0.1834)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [490/689]  eta: 0:05:13  lr: 0.000091  loss: 0.1752 (0.1834)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [500/689]  eta: 0:04:58  lr: 0.000091  loss: 0.1750 (0.1834)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [510/689]  eta: 0:04:42  lr: 0.000091  loss: 0.1870 (0.1837)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [520/689]  eta: 0:04:26  lr: 0.000091  loss: 0.1899 (0.1840)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [530/689]  eta: 0:04:10  lr: 0.000091  loss: 0.1878 (0.1841)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [540/689]  eta: 0:03:55  lr: 0.000091  loss: 0.1762 (0.1839)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [550/689]  eta: 0:03:39  lr: 0.000091  loss: 0.1767 (0.1840)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [560/689]  eta: 0:03:23  lr: 0.000091  loss: 0.1875 (0.1840)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [570/689]  eta: 0:03:07  lr: 0.000091  loss: 0.1746 (0.1839)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [580/689]  eta: 0:02:51  lr: 0.000091  loss: 0.1790 (0.1838)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [590/689]  eta: 0:02:36  lr: 0.000091  loss: 0.1795 (0.1839)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [600/689]  eta: 0:02:20  lr: 0.000091  loss: 0.1752 (0.1839)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [610/689]  eta: 0:02:04  lr: 0.000091  loss: 0.1762 (0.1838)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [620/689]  eta: 0:01:48  lr: 0.000091  loss: 0.1738 (0.1838)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [630/689]  eta: 0:01:33  lr: 0.000091  loss: 0.1738 (0.1837)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [640/689]  eta: 0:01:17  lr: 0.000091  loss: 0.1744 (0.1839)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [650/689]  eta: 0:01:01  lr: 0.000091  loss: 0.1831 (0.1839)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [660/689]  eta: 0:00:45  lr: 0.000091  loss: 0.1726 (0.1837)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [670/689]  eta: 0:00:29  lr: 0.000091  loss: 0.1762 (0.1838)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [680/689]  eta: 0:00:14  lr: 0.000091  loss: 0.1796 (0.1839)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179]  [688/689]  eta: 0:00:01  lr: 0.000091  loss: 0.1796 (0.1839)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:179] Total time: 0:18:07 (1.5780 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: 0.1796 (0.1839)\n",
      "Valid: [epoch:179]  [ 0/14]  eta: 0:00:14  loss: 0.1706 (0.1706)  time: 1.0135  data: 0.4215  max mem: 39763\n",
      "Valid: [epoch:179]  [13/14]  eta: 0:00:00  loss: 0.1706 (0.1727)  time: 0.1144  data: 0.0302  max mem: 39763\n",
      "Valid: [epoch:179] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.1706 (0.1727)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_179_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.173%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:180]  [  0/689]  eta: 0:11:29  lr: 0.000091  loss: 0.2111 (0.2111)  time: 1.0005  data: 0.5190  max mem: 39763\n",
      "Train: [epoch:180]  [ 10/689]  eta: 0:17:15  lr: 0.000091  loss: 0.1876 (0.1878)  time: 1.5254  data: 0.0473  max mem: 39763\n",
      "Train: [epoch:180]  [ 20/689]  eta: 0:17:17  lr: 0.000091  loss: 0.1763 (0.1821)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [ 30/689]  eta: 0:17:08  lr: 0.000091  loss: 0.1796 (0.1856)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [ 40/689]  eta: 0:16:55  lr: 0.000091  loss: 0.1830 (0.1831)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [ 50/689]  eta: 0:16:41  lr: 0.000091  loss: 0.1737 (0.1834)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [ 60/689]  eta: 0:16:27  lr: 0.000091  loss: 0.1672 (0.1814)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [ 70/689]  eta: 0:16:12  lr: 0.000091  loss: 0.1644 (0.1790)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [ 80/689]  eta: 0:15:57  lr: 0.000091  loss: 0.1677 (0.1794)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [ 90/689]  eta: 0:15:42  lr: 0.000091  loss: 0.1873 (0.1808)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [100/689]  eta: 0:15:26  lr: 0.000091  loss: 0.1753 (0.1824)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [110/689]  eta: 0:15:11  lr: 0.000091  loss: 0.1939 (0.1840)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [120/689]  eta: 0:14:55  lr: 0.000091  loss: 0.1939 (0.1844)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [130/689]  eta: 0:14:40  lr: 0.000091  loss: 0.1904 (0.1856)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [140/689]  eta: 0:14:24  lr: 0.000091  loss: 0.1938 (0.1870)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [150/689]  eta: 0:14:09  lr: 0.000091  loss: 0.1874 (0.1867)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [160/689]  eta: 0:13:53  lr: 0.000091  loss: 0.1819 (0.1873)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [170/689]  eta: 0:13:37  lr: 0.000091  loss: 0.1812 (0.1868)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [180/689]  eta: 0:13:22  lr: 0.000091  loss: 0.1677 (0.1872)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [190/689]  eta: 0:13:06  lr: 0.000091  loss: 0.1796 (0.1869)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [200/689]  eta: 0:12:50  lr: 0.000091  loss: 0.1813 (0.1872)  time: 1.5787  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:180]  [210/689]  eta: 0:12:35  lr: 0.000091  loss: 0.1878 (0.1872)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [220/689]  eta: 0:12:19  lr: 0.000091  loss: 0.1827 (0.1874)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [230/689]  eta: 0:12:03  lr: 0.000091  loss: 0.1850 (0.1873)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [240/689]  eta: 0:11:48  lr: 0.000091  loss: 0.1862 (0.1873)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [250/689]  eta: 0:11:32  lr: 0.000091  loss: 0.1766 (0.1871)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [260/689]  eta: 0:11:16  lr: 0.000091  loss: 0.1720 (0.1869)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [270/689]  eta: 0:11:00  lr: 0.000091  loss: 0.1807 (0.1866)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [280/689]  eta: 0:10:45  lr: 0.000091  loss: 0.1829 (0.1868)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [290/689]  eta: 0:10:29  lr: 0.000091  loss: 0.1835 (0.1865)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [300/689]  eta: 0:10:13  lr: 0.000091  loss: 0.1777 (0.1864)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [310/689]  eta: 0:09:57  lr: 0.000091  loss: 0.1777 (0.1863)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [320/689]  eta: 0:09:42  lr: 0.000091  loss: 0.1868 (0.1866)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [330/689]  eta: 0:09:26  lr: 0.000091  loss: 0.1976 (0.1872)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [340/689]  eta: 0:09:10  lr: 0.000091  loss: 0.1969 (0.1873)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [350/689]  eta: 0:08:54  lr: 0.000091  loss: 0.1782 (0.1871)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [360/689]  eta: 0:08:39  lr: 0.000091  loss: 0.1776 (0.1869)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [370/689]  eta: 0:08:23  lr: 0.000091  loss: 0.1776 (0.1868)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [380/689]  eta: 0:08:07  lr: 0.000091  loss: 0.1857 (0.1869)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [390/689]  eta: 0:07:51  lr: 0.000091  loss: 0.1863 (0.1871)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [400/689]  eta: 0:07:35  lr: 0.000091  loss: 0.1856 (0.1871)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [410/689]  eta: 0:07:20  lr: 0.000091  loss: 0.1815 (0.1870)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [420/689]  eta: 0:07:04  lr: 0.000091  loss: 0.1804 (0.1870)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [430/689]  eta: 0:06:48  lr: 0.000091  loss: 0.1844 (0.1872)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [440/689]  eta: 0:06:32  lr: 0.000091  loss: 0.1823 (0.1871)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [450/689]  eta: 0:06:17  lr: 0.000091  loss: 0.1791 (0.1868)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [460/689]  eta: 0:06:01  lr: 0.000091  loss: 0.1819 (0.1871)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [470/689]  eta: 0:05:45  lr: 0.000091  loss: 0.1915 (0.1871)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [480/689]  eta: 0:05:29  lr: 0.000091  loss: 0.1780 (0.1869)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [490/689]  eta: 0:05:14  lr: 0.000091  loss: 0.1897 (0.1871)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [500/689]  eta: 0:04:58  lr: 0.000091  loss: 0.1897 (0.1870)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [510/689]  eta: 0:04:42  lr: 0.000091  loss: 0.1730 (0.1869)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [520/689]  eta: 0:04:26  lr: 0.000091  loss: 0.1822 (0.1870)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [530/689]  eta: 0:04:10  lr: 0.000091  loss: 0.1821 (0.1868)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [540/689]  eta: 0:03:55  lr: 0.000091  loss: 0.1812 (0.1870)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [550/689]  eta: 0:03:39  lr: 0.000091  loss: 0.1844 (0.1869)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [560/689]  eta: 0:03:23  lr: 0.000091  loss: 0.1874 (0.1872)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [570/689]  eta: 0:03:07  lr: 0.000091  loss: 0.1866 (0.1869)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [580/689]  eta: 0:02:52  lr: 0.000091  loss: 0.1737 (0.1869)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [590/689]  eta: 0:02:36  lr: 0.000091  loss: 0.1736 (0.1867)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [600/689]  eta: 0:02:20  lr: 0.000091  loss: 0.1736 (0.1866)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [610/689]  eta: 0:02:04  lr: 0.000091  loss: 0.1755 (0.1866)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [620/689]  eta: 0:01:48  lr: 0.000091  loss: 0.1790 (0.1867)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [630/689]  eta: 0:01:33  lr: 0.000091  loss: 0.1769 (0.1864)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [640/689]  eta: 0:01:17  lr: 0.000091  loss: 0.1771 (0.1864)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [650/689]  eta: 0:01:01  lr: 0.000091  loss: 0.1867 (0.1865)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [660/689]  eta: 0:00:45  lr: 0.000091  loss: 0.1938 (0.1866)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [670/689]  eta: 0:00:29  lr: 0.000091  loss: 0.1831 (0.1867)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [680/689]  eta: 0:00:14  lr: 0.000091  loss: 0.1754 (0.1866)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180]  [688/689]  eta: 0:00:01  lr: 0.000091  loss: 0.1783 (0.1867)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:180] Total time: 0:18:07 (1.5785 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: 0.1783 (0.1867)\n",
      "Valid: [epoch:180]  [ 0/14]  eta: 0:00:14  loss: 0.1672 (0.1672)  time: 1.0178  data: 0.3891  max mem: 39763\n",
      "Valid: [epoch:180]  [13/14]  eta: 0:00:00  loss: 0.1715 (0.1742)  time: 0.1147  data: 0.0279  max mem: 39763\n",
      "Valid: [epoch:180] Total time: 0:00:01 (0.1238 s / it)\n",
      "Averaged stats: loss: 0.1715 (0.1742)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_180_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.174%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:181]  [  0/689]  eta: 0:12:07  lr: 0.000091  loss: 0.1597 (0.1597)  time: 1.0565  data: 0.5803  max mem: 39763\n",
      "Train: [epoch:181]  [ 10/689]  eta: 0:17:17  lr: 0.000091  loss: 0.1755 (0.1755)  time: 1.5287  data: 0.0528  max mem: 39763\n",
      "Train: [epoch:181]  [ 20/689]  eta: 0:17:17  lr: 0.000091  loss: 0.1830 (0.1805)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [ 30/689]  eta: 0:17:07  lr: 0.000091  loss: 0.1870 (0.1864)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [ 40/689]  eta: 0:16:55  lr: 0.000091  loss: 0.1786 (0.1833)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [ 50/689]  eta: 0:16:41  lr: 0.000091  loss: 0.1786 (0.1852)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [ 60/689]  eta: 0:16:26  lr: 0.000091  loss: 0.1844 (0.1843)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [ 70/689]  eta: 0:16:11  lr: 0.000091  loss: 0.1802 (0.1841)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [ 80/689]  eta: 0:15:56  lr: 0.000091  loss: 0.1818 (0.1856)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [ 90/689]  eta: 0:15:41  lr: 0.000091  loss: 0.1844 (0.1857)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [100/689]  eta: 0:15:25  lr: 0.000091  loss: 0.1817 (0.1855)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [110/689]  eta: 0:15:10  lr: 0.000091  loss: 0.1871 (0.1868)  time: 1.5773  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:181]  [120/689]  eta: 0:14:54  lr: 0.000091  loss: 0.1871 (0.1870)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [130/689]  eta: 0:14:39  lr: 0.000091  loss: 0.1851 (0.1879)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [140/689]  eta: 0:14:23  lr: 0.000091  loss: 0.1909 (0.1882)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [150/689]  eta: 0:14:08  lr: 0.000091  loss: 0.1863 (0.1891)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [160/689]  eta: 0:13:52  lr: 0.000091  loss: 0.1865 (0.1897)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [170/689]  eta: 0:13:37  lr: 0.000091  loss: 0.1870 (0.1903)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [180/689]  eta: 0:13:21  lr: 0.000091  loss: 0.1865 (0.1896)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [190/689]  eta: 0:13:05  lr: 0.000091  loss: 0.1812 (0.1893)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [200/689]  eta: 0:12:50  lr: 0.000091  loss: 0.1850 (0.1892)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [210/689]  eta: 0:12:34  lr: 0.000091  loss: 0.1822 (0.1889)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [220/689]  eta: 0:12:18  lr: 0.000091  loss: 0.1822 (0.1889)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [230/689]  eta: 0:12:03  lr: 0.000091  loss: 0.1833 (0.1890)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [240/689]  eta: 0:11:47  lr: 0.000091  loss: 0.1831 (0.1891)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [250/689]  eta: 0:11:31  lr: 0.000091  loss: 0.1838 (0.1889)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [260/689]  eta: 0:11:15  lr: 0.000091  loss: 0.1825 (0.1887)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [270/689]  eta: 0:11:00  lr: 0.000091  loss: 0.1834 (0.1889)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [280/689]  eta: 0:10:44  lr: 0.000091  loss: 0.1834 (0.1888)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [290/689]  eta: 0:10:28  lr: 0.000091  loss: 0.1814 (0.1888)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [300/689]  eta: 0:10:13  lr: 0.000091  loss: 0.1834 (0.1887)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [310/689]  eta: 0:09:57  lr: 0.000091  loss: 0.1858 (0.1885)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [320/689]  eta: 0:09:41  lr: 0.000091  loss: 0.1782 (0.1882)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [330/689]  eta: 0:09:25  lr: 0.000091  loss: 0.1828 (0.1883)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [340/689]  eta: 0:09:10  lr: 0.000091  loss: 0.1845 (0.1885)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [350/689]  eta: 0:08:54  lr: 0.000091  loss: 0.1835 (0.1884)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [360/689]  eta: 0:08:38  lr: 0.000091  loss: 0.1798 (0.1883)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [370/689]  eta: 0:08:22  lr: 0.000091  loss: 0.1770 (0.1880)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [380/689]  eta: 0:08:07  lr: 0.000091  loss: 0.1815 (0.1880)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [390/689]  eta: 0:07:51  lr: 0.000091  loss: 0.1815 (0.1880)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [400/689]  eta: 0:07:35  lr: 0.000091  loss: 0.1885 (0.1882)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [410/689]  eta: 0:07:19  lr: 0.000091  loss: 0.1799 (0.1881)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [420/689]  eta: 0:07:04  lr: 0.000091  loss: 0.1742 (0.1878)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [430/689]  eta: 0:06:48  lr: 0.000091  loss: 0.1721 (0.1876)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [440/689]  eta: 0:06:32  lr: 0.000091  loss: 0.1726 (0.1873)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [450/689]  eta: 0:06:16  lr: 0.000091  loss: 0.1772 (0.1873)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [460/689]  eta: 0:06:01  lr: 0.000091  loss: 0.1842 (0.1872)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [470/689]  eta: 0:05:45  lr: 0.000091  loss: 0.1803 (0.1870)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [480/689]  eta: 0:05:29  lr: 0.000091  loss: 0.1734 (0.1869)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [490/689]  eta: 0:05:13  lr: 0.000091  loss: 0.1792 (0.1868)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [500/689]  eta: 0:04:58  lr: 0.000091  loss: 0.1865 (0.1869)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [510/689]  eta: 0:04:42  lr: 0.000091  loss: 0.1922 (0.1872)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [520/689]  eta: 0:04:26  lr: 0.000091  loss: 0.1901 (0.1872)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [530/689]  eta: 0:04:10  lr: 0.000091  loss: 0.1793 (0.1869)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [540/689]  eta: 0:03:55  lr: 0.000091  loss: 0.1792 (0.1868)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [550/689]  eta: 0:03:39  lr: 0.000091  loss: 0.1776 (0.1865)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [560/689]  eta: 0:03:23  lr: 0.000091  loss: 0.1770 (0.1867)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [570/689]  eta: 0:03:07  lr: 0.000091  loss: 0.1840 (0.1865)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [580/689]  eta: 0:02:51  lr: 0.000091  loss: 0.1844 (0.1866)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [590/689]  eta: 0:02:36  lr: 0.000091  loss: 0.1913 (0.1867)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [600/689]  eta: 0:02:20  lr: 0.000091  loss: 0.1711 (0.1863)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [610/689]  eta: 0:02:04  lr: 0.000091  loss: 0.1726 (0.1862)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [620/689]  eta: 0:01:48  lr: 0.000091  loss: 0.1815 (0.1863)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [630/689]  eta: 0:01:33  lr: 0.000091  loss: 0.1943 (0.1865)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [640/689]  eta: 0:01:17  lr: 0.000091  loss: 0.1870 (0.1866)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [650/689]  eta: 0:01:01  lr: 0.000091  loss: 0.1802 (0.1864)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [660/689]  eta: 0:00:45  lr: 0.000091  loss: 0.1820 (0.1865)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [670/689]  eta: 0:00:29  lr: 0.000091  loss: 0.1838 (0.1865)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [680/689]  eta: 0:00:14  lr: 0.000091  loss: 0.1750 (0.1864)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181]  [688/689]  eta: 0:00:01  lr: 0.000091  loss: 0.1747 (0.1863)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:181] Total time: 0:18:07 (1.5781 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: 0.1747 (0.1863)\n",
      "Valid: [epoch:181]  [ 0/14]  eta: 0:00:14  loss: 0.1858 (0.1858)  time: 1.0379  data: 0.4618  max mem: 39763\n",
      "Valid: [epoch:181]  [13/14]  eta: 0:00:00  loss: 0.1754 (0.1781)  time: 0.1161  data: 0.0330  max mem: 39763\n",
      "Valid: [epoch:181] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.1754 (0.1781)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_181_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.178%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:182]  [  0/689]  eta: 0:11:27  lr: 0.000091  loss: 0.1847 (0.1847)  time: 0.9985  data: 0.5277  max mem: 39763\n",
      "Train: [epoch:182]  [ 10/689]  eta: 0:17:15  lr: 0.000091  loss: 0.1851 (0.1937)  time: 1.5253  data: 0.0480  max mem: 39763\n",
      "Train: [epoch:182]  [ 20/689]  eta: 0:17:17  lr: 0.000091  loss: 0.1773 (0.1877)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:182]  [ 30/689]  eta: 0:17:07  lr: 0.000091  loss: 0.1744 (0.1864)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [ 40/689]  eta: 0:16:55  lr: 0.000091  loss: 0.1760 (0.1838)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [ 50/689]  eta: 0:16:41  lr: 0.000091  loss: 0.1766 (0.1833)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [ 60/689]  eta: 0:16:26  lr: 0.000091  loss: 0.1777 (0.1841)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [ 70/689]  eta: 0:16:12  lr: 0.000091  loss: 0.1810 (0.1844)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [ 80/689]  eta: 0:15:56  lr: 0.000091  loss: 0.1799 (0.1847)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [ 90/689]  eta: 0:15:41  lr: 0.000091  loss: 0.1788 (0.1854)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [100/689]  eta: 0:15:26  lr: 0.000091  loss: 0.1910 (0.1864)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [110/689]  eta: 0:15:10  lr: 0.000091  loss: 0.1994 (0.1878)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [120/689]  eta: 0:14:55  lr: 0.000091  loss: 0.1989 (0.1880)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [130/689]  eta: 0:14:39  lr: 0.000091  loss: 0.1989 (0.1884)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [140/689]  eta: 0:14:24  lr: 0.000091  loss: 0.1924 (0.1885)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [150/689]  eta: 0:14:08  lr: 0.000091  loss: 0.1896 (0.1885)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [160/689]  eta: 0:13:53  lr: 0.000091  loss: 0.1746 (0.1875)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [170/689]  eta: 0:13:37  lr: 0.000091  loss: 0.1727 (0.1872)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [180/689]  eta: 0:13:21  lr: 0.000091  loss: 0.1760 (0.1867)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [190/689]  eta: 0:13:06  lr: 0.000091  loss: 0.1802 (0.1866)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [200/689]  eta: 0:12:50  lr: 0.000091  loss: 0.1856 (0.1866)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [210/689]  eta: 0:12:34  lr: 0.000091  loss: 0.1760 (0.1860)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [220/689]  eta: 0:12:19  lr: 0.000091  loss: 0.1760 (0.1862)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [230/689]  eta: 0:12:03  lr: 0.000091  loss: 0.1856 (0.1862)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [240/689]  eta: 0:11:47  lr: 0.000091  loss: 0.1856 (0.1865)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [250/689]  eta: 0:11:32  lr: 0.000091  loss: 0.1834 (0.1863)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [260/689]  eta: 0:11:16  lr: 0.000091  loss: 0.1753 (0.1858)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [270/689]  eta: 0:11:00  lr: 0.000091  loss: 0.1727 (0.1856)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [280/689]  eta: 0:10:44  lr: 0.000091  loss: 0.1861 (0.1855)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [290/689]  eta: 0:10:29  lr: 0.000091  loss: 0.1771 (0.1851)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [300/689]  eta: 0:10:13  lr: 0.000091  loss: 0.1709 (0.1851)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [310/689]  eta: 0:09:57  lr: 0.000091  loss: 0.1709 (0.1850)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [320/689]  eta: 0:09:42  lr: 0.000091  loss: 0.1797 (0.1854)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [330/689]  eta: 0:09:26  lr: 0.000091  loss: 0.1810 (0.1855)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [340/689]  eta: 0:09:10  lr: 0.000091  loss: 0.1807 (0.1853)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [350/689]  eta: 0:08:54  lr: 0.000091  loss: 0.1781 (0.1855)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [360/689]  eta: 0:08:39  lr: 0.000091  loss: 0.1906 (0.1856)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [370/689]  eta: 0:08:23  lr: 0.000091  loss: 0.1946 (0.1859)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [380/689]  eta: 0:08:07  lr: 0.000091  loss: 0.1907 (0.1862)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [390/689]  eta: 0:07:51  lr: 0.000091  loss: 0.1792 (0.1862)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [400/689]  eta: 0:07:36  lr: 0.000091  loss: 0.1792 (0.1863)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [410/689]  eta: 0:07:20  lr: 0.000091  loss: 0.1813 (0.1862)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [420/689]  eta: 0:07:04  lr: 0.000091  loss: 0.1744 (0.1860)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [430/689]  eta: 0:06:48  lr: 0.000091  loss: 0.1797 (0.1861)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [440/689]  eta: 0:06:33  lr: 0.000091  loss: 0.1864 (0.1862)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [450/689]  eta: 0:06:17  lr: 0.000091  loss: 0.1825 (0.1861)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [460/689]  eta: 0:06:01  lr: 0.000091  loss: 0.1819 (0.1865)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [470/689]  eta: 0:05:45  lr: 0.000091  loss: 0.1912 (0.1865)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [480/689]  eta: 0:05:30  lr: 0.000091  loss: 0.1785 (0.1864)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [490/689]  eta: 0:05:14  lr: 0.000091  loss: 0.1758 (0.1864)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [500/689]  eta: 0:04:58  lr: 0.000091  loss: 0.1994 (0.1869)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [510/689]  eta: 0:04:42  lr: 0.000091  loss: 0.1994 (0.1871)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [520/689]  eta: 0:04:26  lr: 0.000091  loss: 0.1885 (0.1871)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [530/689]  eta: 0:04:11  lr: 0.000091  loss: 0.1860 (0.1871)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [540/689]  eta: 0:03:55  lr: 0.000091  loss: 0.1828 (0.1871)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [550/689]  eta: 0:03:39  lr: 0.000091  loss: 0.1784 (0.1869)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [560/689]  eta: 0:03:23  lr: 0.000091  loss: 0.1798 (0.1870)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [570/689]  eta: 0:03:07  lr: 0.000091  loss: 0.1880 (0.1870)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [580/689]  eta: 0:02:52  lr: 0.000091  loss: 0.1896 (0.1873)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [590/689]  eta: 0:02:36  lr: 0.000091  loss: 0.1896 (0.1875)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [600/689]  eta: 0:02:20  lr: 0.000091  loss: 0.1895 (0.1875)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [610/689]  eta: 0:02:04  lr: 0.000091  loss: 0.1800 (0.1875)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [620/689]  eta: 0:01:49  lr: 0.000091  loss: 0.1775 (0.1874)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [630/689]  eta: 0:01:33  lr: 0.000091  loss: 0.1847 (0.1875)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [640/689]  eta: 0:01:17  lr: 0.000091  loss: 0.1875 (0.1877)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [650/689]  eta: 0:01:01  lr: 0.000091  loss: 0.1883 (0.1877)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [660/689]  eta: 0:00:45  lr: 0.000091  loss: 0.1829 (0.1877)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [670/689]  eta: 0:00:30  lr: 0.000091  loss: 0.1801 (0.1877)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182]  [680/689]  eta: 0:00:14  lr: 0.000091  loss: 0.1821 (0.1877)  time: 1.5826  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:182]  [688/689]  eta: 0:00:01  lr: 0.000091  loss: 0.1825 (0.1877)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:182] Total time: 0:18:08 (1.5802 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: 0.1825 (0.1877)\n",
      "Valid: [epoch:182]  [ 0/14]  eta: 0:00:14  loss: 0.1729 (0.1729)  time: 1.0552  data: 0.3865  max mem: 39763\n",
      "Valid: [epoch:182]  [13/14]  eta: 0:00:00  loss: 0.1735 (0.1757)  time: 0.1175  data: 0.0277  max mem: 39763\n",
      "Valid: [epoch:182] Total time: 0:00:01 (0.1268 s / it)\n",
      "Averaged stats: loss: 0.1735 (0.1757)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_182_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.176%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:183]  [  0/689]  eta: 0:12:11  lr: 0.000091  loss: 0.1850 (0.1850)  time: 1.0616  data: 0.5850  max mem: 39763\n",
      "Train: [epoch:183]  [ 10/689]  eta: 0:17:19  lr: 0.000091  loss: 0.1850 (0.1824)  time: 1.5307  data: 0.0533  max mem: 39763\n",
      "Train: [epoch:183]  [ 20/689]  eta: 0:17:19  lr: 0.000091  loss: 0.1803 (0.1834)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [ 30/689]  eta: 0:17:09  lr: 0.000091  loss: 0.1759 (0.1854)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [ 40/689]  eta: 0:16:56  lr: 0.000091  loss: 0.1828 (0.1862)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [ 50/689]  eta: 0:16:42  lr: 0.000091  loss: 0.1770 (0.1835)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [ 60/689]  eta: 0:16:27  lr: 0.000091  loss: 0.1734 (0.1845)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [ 70/689]  eta: 0:16:12  lr: 0.000091  loss: 0.1897 (0.1854)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [ 80/689]  eta: 0:15:57  lr: 0.000091  loss: 0.1797 (0.1849)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [ 90/689]  eta: 0:15:42  lr: 0.000091  loss: 0.1808 (0.1862)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [100/689]  eta: 0:15:27  lr: 0.000091  loss: 0.2008 (0.1883)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [110/689]  eta: 0:15:11  lr: 0.000091  loss: 0.1982 (0.1902)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [120/689]  eta: 0:14:56  lr: 0.000091  loss: 0.1953 (0.1907)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [130/689]  eta: 0:14:40  lr: 0.000091  loss: 0.1873 (0.1913)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [140/689]  eta: 0:14:25  lr: 0.000091  loss: 0.1980 (0.1927)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [150/689]  eta: 0:14:09  lr: 0.000091  loss: 0.2086 (0.1933)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [160/689]  eta: 0:13:53  lr: 0.000091  loss: 0.2001 (0.1933)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [170/689]  eta: 0:13:38  lr: 0.000091  loss: 0.1889 (0.1930)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [180/689]  eta: 0:13:22  lr: 0.000091  loss: 0.1841 (0.1932)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [190/689]  eta: 0:13:06  lr: 0.000091  loss: 0.1906 (0.1935)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [200/689]  eta: 0:12:51  lr: 0.000091  loss: 0.1824 (0.1931)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [210/689]  eta: 0:12:35  lr: 0.000091  loss: 0.1752 (0.1928)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [220/689]  eta: 0:12:19  lr: 0.000091  loss: 0.1817 (0.1928)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [230/689]  eta: 0:12:03  lr: 0.000091  loss: 0.1847 (0.1930)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [240/689]  eta: 0:11:48  lr: 0.000091  loss: 0.1819 (0.1923)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [250/689]  eta: 0:11:32  lr: 0.000091  loss: 0.1737 (0.1919)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [260/689]  eta: 0:11:16  lr: 0.000091  loss: 0.1877 (0.1919)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [270/689]  eta: 0:11:00  lr: 0.000091  loss: 0.1811 (0.1918)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [280/689]  eta: 0:10:45  lr: 0.000091  loss: 0.1775 (0.1914)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [290/689]  eta: 0:10:29  lr: 0.000091  loss: 0.1817 (0.1916)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [300/689]  eta: 0:10:13  lr: 0.000091  loss: 0.1876 (0.1915)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [310/689]  eta: 0:09:57  lr: 0.000091  loss: 0.1849 (0.1913)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [320/689]  eta: 0:09:42  lr: 0.000091  loss: 0.1831 (0.1912)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [330/689]  eta: 0:09:26  lr: 0.000091  loss: 0.1824 (0.1913)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [340/689]  eta: 0:09:10  lr: 0.000091  loss: 0.1814 (0.1912)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [350/689]  eta: 0:08:54  lr: 0.000091  loss: 0.1825 (0.1912)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [360/689]  eta: 0:08:39  lr: 0.000091  loss: 0.1859 (0.1911)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [370/689]  eta: 0:08:23  lr: 0.000091  loss: 0.1789 (0.1910)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [380/689]  eta: 0:08:07  lr: 0.000091  loss: 0.1868 (0.1910)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [390/689]  eta: 0:07:51  lr: 0.000091  loss: 0.1868 (0.1909)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [400/689]  eta: 0:07:36  lr: 0.000091  loss: 0.1831 (0.1910)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [410/689]  eta: 0:07:20  lr: 0.000091  loss: 0.1795 (0.1911)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [420/689]  eta: 0:07:04  lr: 0.000091  loss: 0.1911 (0.1911)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [430/689]  eta: 0:06:48  lr: 0.000091  loss: 0.1911 (0.1911)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [440/689]  eta: 0:06:32  lr: 0.000091  loss: 0.1849 (0.1911)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [450/689]  eta: 0:06:17  lr: 0.000091  loss: 0.1853 (0.1912)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [460/689]  eta: 0:06:01  lr: 0.000091  loss: 0.1897 (0.1912)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [470/689]  eta: 0:05:45  lr: 0.000091  loss: 0.1959 (0.1915)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [480/689]  eta: 0:05:29  lr: 0.000091  loss: 0.1884 (0.1914)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [490/689]  eta: 0:05:14  lr: 0.000091  loss: 0.1828 (0.1915)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [500/689]  eta: 0:04:58  lr: 0.000091  loss: 0.1899 (0.1915)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [510/689]  eta: 0:04:42  lr: 0.000091  loss: 0.1838 (0.1914)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [520/689]  eta: 0:04:26  lr: 0.000091  loss: 0.1896 (0.1914)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [530/689]  eta: 0:04:10  lr: 0.000091  loss: 0.1851 (0.1914)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [540/689]  eta: 0:03:55  lr: 0.000091  loss: 0.1839 (0.1912)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [550/689]  eta: 0:03:39  lr: 0.000091  loss: 0.1822 (0.1912)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [560/689]  eta: 0:03:23  lr: 0.000091  loss: 0.1888 (0.1913)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [570/689]  eta: 0:03:07  lr: 0.000091  loss: 0.1860 (0.1911)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [580/689]  eta: 0:02:52  lr: 0.000091  loss: 0.1794 (0.1910)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [590/689]  eta: 0:02:36  lr: 0.000091  loss: 0.1951 (0.1913)  time: 1.5789  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:183]  [600/689]  eta: 0:02:20  lr: 0.000091  loss: 0.1908 (0.1910)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [610/689]  eta: 0:02:04  lr: 0.000091  loss: 0.1798 (0.1908)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [620/689]  eta: 0:01:48  lr: 0.000091  loss: 0.1798 (0.1907)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [630/689]  eta: 0:01:33  lr: 0.000091  loss: 0.1883 (0.1907)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [640/689]  eta: 0:01:17  lr: 0.000091  loss: 0.1907 (0.1908)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [650/689]  eta: 0:01:01  lr: 0.000091  loss: 0.1877 (0.1907)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [660/689]  eta: 0:00:45  lr: 0.000091  loss: 0.1797 (0.1906)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [670/689]  eta: 0:00:29  lr: 0.000091  loss: 0.1888 (0.1908)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [680/689]  eta: 0:00:14  lr: 0.000091  loss: 0.1976 (0.1909)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183]  [688/689]  eta: 0:00:01  lr: 0.000091  loss: 0.1991 (0.1911)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:183] Total time: 0:18:07 (1.5789 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: 0.1991 (0.1911)\n",
      "Valid: [epoch:183]  [ 0/14]  eta: 0:00:14  loss: 0.2263 (0.2263)  time: 1.0195  data: 0.4027  max mem: 39763\n",
      "Valid: [epoch:183]  [13/14]  eta: 0:00:00  loss: 0.1750 (0.1809)  time: 0.1148  data: 0.0288  max mem: 39763\n",
      "Valid: [epoch:183] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.1750 (0.1809)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_183_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.181%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:184]  [  0/689]  eta: 0:11:22  lr: 0.000091  loss: 0.1648 (0.1648)  time: 0.9909  data: 0.5156  max mem: 39763\n",
      "Train: [epoch:184]  [ 10/689]  eta: 0:17:14  lr: 0.000091  loss: 0.1901 (0.1872)  time: 1.5242  data: 0.0469  max mem: 39763\n",
      "Train: [epoch:184]  [ 20/689]  eta: 0:17:16  lr: 0.000091  loss: 0.1901 (0.1898)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [ 30/689]  eta: 0:17:07  lr: 0.000091  loss: 0.1848 (0.1901)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [ 40/689]  eta: 0:16:55  lr: 0.000091  loss: 0.1868 (0.1893)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [ 50/689]  eta: 0:16:41  lr: 0.000091  loss: 0.1822 (0.1889)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [ 60/689]  eta: 0:16:26  lr: 0.000091  loss: 0.1833 (0.1893)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [ 70/689]  eta: 0:16:12  lr: 0.000091  loss: 0.1837 (0.1882)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [ 80/689]  eta: 0:15:57  lr: 0.000091  loss: 0.1837 (0.1886)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [ 90/689]  eta: 0:15:42  lr: 0.000091  loss: 0.1825 (0.1878)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [100/689]  eta: 0:15:27  lr: 0.000091  loss: 0.1793 (0.1876)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [110/689]  eta: 0:15:11  lr: 0.000091  loss: 0.1853 (0.1878)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [120/689]  eta: 0:14:56  lr: 0.000091  loss: 0.1952 (0.1892)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [130/689]  eta: 0:14:41  lr: 0.000091  loss: 0.1985 (0.1895)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [140/689]  eta: 0:14:25  lr: 0.000091  loss: 0.1902 (0.1900)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [150/689]  eta: 0:14:09  lr: 0.000091  loss: 0.1955 (0.1901)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [160/689]  eta: 0:13:54  lr: 0.000091  loss: 0.1881 (0.1902)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [170/689]  eta: 0:13:38  lr: 0.000091  loss: 0.1846 (0.1903)  time: 1.5832  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [180/689]  eta: 0:13:23  lr: 0.000091  loss: 0.1807 (0.1897)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [190/689]  eta: 0:13:07  lr: 0.000091  loss: 0.1822 (0.1898)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [200/689]  eta: 0:12:51  lr: 0.000091  loss: 0.1778 (0.1891)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [210/689]  eta: 0:12:36  lr: 0.000091  loss: 0.1818 (0.1891)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [220/689]  eta: 0:12:20  lr: 0.000091  loss: 0.1807 (0.1889)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [230/689]  eta: 0:12:04  lr: 0.000091  loss: 0.1781 (0.1888)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [240/689]  eta: 0:11:49  lr: 0.000091  loss: 0.1879 (0.1893)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [250/689]  eta: 0:11:33  lr: 0.000091  loss: 0.1862 (0.1891)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [260/689]  eta: 0:11:17  lr: 0.000091  loss: 0.1837 (0.1892)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [270/689]  eta: 0:11:01  lr: 0.000091  loss: 0.1843 (0.1892)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [280/689]  eta: 0:10:46  lr: 0.000091  loss: 0.1861 (0.1893)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [290/689]  eta: 0:10:30  lr: 0.000091  loss: 0.1812 (0.1890)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [300/689]  eta: 0:10:14  lr: 0.000091  loss: 0.1884 (0.1897)  time: 1.5835  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [310/689]  eta: 0:09:58  lr: 0.000091  loss: 0.1916 (0.1895)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [320/689]  eta: 0:09:43  lr: 0.000091  loss: 0.1756 (0.1893)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [330/689]  eta: 0:09:27  lr: 0.000091  loss: 0.1786 (0.1895)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [340/689]  eta: 0:09:11  lr: 0.000091  loss: 0.1874 (0.1894)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [350/689]  eta: 0:08:55  lr: 0.000091  loss: 0.1865 (0.1895)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [360/689]  eta: 0:08:39  lr: 0.000091  loss: 0.1740 (0.1891)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [370/689]  eta: 0:08:24  lr: 0.000091  loss: 0.1786 (0.1894)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [380/689]  eta: 0:08:08  lr: 0.000091  loss: 0.1839 (0.1893)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [390/689]  eta: 0:07:52  lr: 0.000091  loss: 0.1838 (0.1897)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [400/689]  eta: 0:07:36  lr: 0.000091  loss: 0.2017 (0.1902)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [410/689]  eta: 0:07:20  lr: 0.000091  loss: 0.1907 (0.1900)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [420/689]  eta: 0:07:05  lr: 0.000091  loss: 0.1790 (0.1897)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [430/689]  eta: 0:06:49  lr: 0.000091  loss: 0.1811 (0.1897)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [440/689]  eta: 0:06:33  lr: 0.000091  loss: 0.1888 (0.1900)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [450/689]  eta: 0:06:17  lr: 0.000091  loss: 0.1962 (0.1901)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [460/689]  eta: 0:06:01  lr: 0.000091  loss: 0.1953 (0.1901)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [470/689]  eta: 0:05:46  lr: 0.000091  loss: 0.1867 (0.1901)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [480/689]  eta: 0:05:30  lr: 0.000091  loss: 0.1866 (0.1901)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [490/689]  eta: 0:05:14  lr: 0.000091  loss: 0.1866 (0.1901)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [500/689]  eta: 0:04:58  lr: 0.000091  loss: 0.1790 (0.1900)  time: 1.5822  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:184]  [510/689]  eta: 0:04:42  lr: 0.000091  loss: 0.1882 (0.1901)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [520/689]  eta: 0:04:27  lr: 0.000091  loss: 0.1815 (0.1900)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [530/689]  eta: 0:04:11  lr: 0.000091  loss: 0.1815 (0.1903)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [540/689]  eta: 0:03:55  lr: 0.000091  loss: 0.1937 (0.1904)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [550/689]  eta: 0:03:39  lr: 0.000091  loss: 0.1835 (0.1902)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [560/689]  eta: 0:03:23  lr: 0.000091  loss: 0.1898 (0.1904)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [570/689]  eta: 0:03:08  lr: 0.000091  loss: 0.1996 (0.1905)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [580/689]  eta: 0:02:52  lr: 0.000091  loss: 0.2015 (0.1907)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [590/689]  eta: 0:02:36  lr: 0.000091  loss: 0.1964 (0.1907)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [600/689]  eta: 0:02:20  lr: 0.000091  loss: 0.1908 (0.1908)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [610/689]  eta: 0:02:04  lr: 0.000091  loss: 0.1795 (0.1905)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [620/689]  eta: 0:01:49  lr: 0.000091  loss: 0.1728 (0.1904)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [630/689]  eta: 0:01:33  lr: 0.000091  loss: 0.1762 (0.1902)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [640/689]  eta: 0:01:17  lr: 0.000091  loss: 0.1812 (0.1903)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [650/689]  eta: 0:01:01  lr: 0.000091  loss: 0.1891 (0.1904)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [660/689]  eta: 0:00:45  lr: 0.000091  loss: 0.1943 (0.1905)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [670/689]  eta: 0:00:30  lr: 0.000091  loss: 0.1906 (0.1906)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [680/689]  eta: 0:00:14  lr: 0.000091  loss: 0.1813 (0.1906)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184]  [688/689]  eta: 0:00:01  lr: 0.000091  loss: 0.1813 (0.1906)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:184] Total time: 0:18:09 (1.5814 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: 0.1813 (0.1906)\n",
      "Valid: [epoch:184]  [ 0/14]  eta: 0:00:14  loss: 0.1755 (0.1755)  time: 1.0190  data: 0.3744  max mem: 39763\n",
      "Valid: [epoch:184]  [13/14]  eta: 0:00:00  loss: 0.1755 (0.1781)  time: 0.1148  data: 0.0268  max mem: 39763\n",
      "Valid: [epoch:184] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.1755 (0.1781)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_184_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.178%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:185]  [  0/689]  eta: 0:11:22  lr: 0.000091  loss: 0.1826 (0.1826)  time: 0.9910  data: 0.5131  max mem: 39763\n",
      "Train: [epoch:185]  [ 10/689]  eta: 0:17:13  lr: 0.000091  loss: 0.1884 (0.1901)  time: 1.5224  data: 0.0467  max mem: 39763\n",
      "Train: [epoch:185]  [ 20/689]  eta: 0:17:15  lr: 0.000091  loss: 0.1884 (0.1890)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [ 30/689]  eta: 0:17:06  lr: 0.000091  loss: 0.1912 (0.1890)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [ 40/689]  eta: 0:16:54  lr: 0.000091  loss: 0.1891 (0.1903)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [ 50/689]  eta: 0:16:40  lr: 0.000091  loss: 0.1873 (0.1904)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [ 60/689]  eta: 0:16:26  lr: 0.000091  loss: 0.1805 (0.1898)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [ 70/689]  eta: 0:16:11  lr: 0.000091  loss: 0.1821 (0.1894)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [ 80/689]  eta: 0:15:56  lr: 0.000091  loss: 0.1828 (0.1892)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [ 90/689]  eta: 0:15:41  lr: 0.000091  loss: 0.1807 (0.1894)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [100/689]  eta: 0:15:25  lr: 0.000091  loss: 0.1908 (0.1906)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [110/689]  eta: 0:15:10  lr: 0.000091  loss: 0.1967 (0.1912)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [120/689]  eta: 0:14:55  lr: 0.000091  loss: 0.1868 (0.1907)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [130/689]  eta: 0:14:39  lr: 0.000091  loss: 0.1764 (0.1904)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [140/689]  eta: 0:14:24  lr: 0.000091  loss: 0.1764 (0.1899)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [150/689]  eta: 0:14:08  lr: 0.000091  loss: 0.1795 (0.1901)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [160/689]  eta: 0:13:52  lr: 0.000091  loss: 0.1865 (0.1900)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [170/689]  eta: 0:13:37  lr: 0.000091  loss: 0.1860 (0.1902)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [180/689]  eta: 0:13:21  lr: 0.000091  loss: 0.1853 (0.1898)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [190/689]  eta: 0:13:06  lr: 0.000091  loss: 0.1854 (0.1900)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [200/689]  eta: 0:12:50  lr: 0.000091  loss: 0.1889 (0.1901)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [210/689]  eta: 0:12:34  lr: 0.000091  loss: 0.1862 (0.1900)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [220/689]  eta: 0:12:18  lr: 0.000091  loss: 0.1876 (0.1901)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [230/689]  eta: 0:12:03  lr: 0.000091  loss: 0.1898 (0.1902)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [240/689]  eta: 0:11:47  lr: 0.000091  loss: 0.1909 (0.1901)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [250/689]  eta: 0:11:31  lr: 0.000091  loss: 0.1883 (0.1902)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [260/689]  eta: 0:11:16  lr: 0.000091  loss: 0.1883 (0.1906)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [270/689]  eta: 0:11:00  lr: 0.000091  loss: 0.1920 (0.1911)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [280/689]  eta: 0:10:44  lr: 0.000091  loss: 0.1897 (0.1910)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [290/689]  eta: 0:10:28  lr: 0.000091  loss: 0.1911 (0.1910)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [300/689]  eta: 0:10:13  lr: 0.000091  loss: 0.1889 (0.1910)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [310/689]  eta: 0:09:57  lr: 0.000091  loss: 0.1829 (0.1907)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [320/689]  eta: 0:09:41  lr: 0.000091  loss: 0.1790 (0.1906)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [330/689]  eta: 0:09:26  lr: 0.000091  loss: 0.1822 (0.1906)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [340/689]  eta: 0:09:10  lr: 0.000091  loss: 0.1812 (0.1902)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [350/689]  eta: 0:08:54  lr: 0.000091  loss: 0.1812 (0.1903)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [360/689]  eta: 0:08:38  lr: 0.000091  loss: 0.1830 (0.1901)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [370/689]  eta: 0:08:23  lr: 0.000091  loss: 0.1830 (0.1903)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [380/689]  eta: 0:08:07  lr: 0.000091  loss: 0.1871 (0.1899)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [390/689]  eta: 0:07:51  lr: 0.000091  loss: 0.1784 (0.1901)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [400/689]  eta: 0:07:35  lr: 0.000091  loss: 0.1844 (0.1902)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [410/689]  eta: 0:07:19  lr: 0.000091  loss: 0.2022 (0.1909)  time: 1.5786  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:185]  [420/689]  eta: 0:07:04  lr: 0.000091  loss: 0.2020 (0.1909)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [430/689]  eta: 0:06:48  lr: 0.000091  loss: 0.1884 (0.1911)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [440/689]  eta: 0:06:32  lr: 0.000091  loss: 0.1808 (0.1909)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [450/689]  eta: 0:06:16  lr: 0.000091  loss: 0.1728 (0.1905)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [460/689]  eta: 0:06:01  lr: 0.000091  loss: 0.1793 (0.1907)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [470/689]  eta: 0:05:45  lr: 0.000091  loss: 0.1922 (0.1907)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [480/689]  eta: 0:05:29  lr: 0.000091  loss: 0.1893 (0.1908)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [490/689]  eta: 0:05:13  lr: 0.000091  loss: 0.1903 (0.1910)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [500/689]  eta: 0:04:58  lr: 0.000091  loss: 0.1903 (0.1911)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [510/689]  eta: 0:04:42  lr: 0.000091  loss: 0.1805 (0.1910)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [520/689]  eta: 0:04:26  lr: 0.000091  loss: 0.1811 (0.1910)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [530/689]  eta: 0:04:10  lr: 0.000091  loss: 0.1852 (0.1909)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [540/689]  eta: 0:03:55  lr: 0.000091  loss: 0.1787 (0.1908)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [550/689]  eta: 0:03:39  lr: 0.000091  loss: 0.1831 (0.1909)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [560/689]  eta: 0:03:23  lr: 0.000091  loss: 0.1978 (0.1910)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [570/689]  eta: 0:03:07  lr: 0.000091  loss: 0.1815 (0.1909)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [580/689]  eta: 0:02:51  lr: 0.000091  loss: 0.1792 (0.1908)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [590/689]  eta: 0:02:36  lr: 0.000091  loss: 0.1792 (0.1906)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [600/689]  eta: 0:02:20  lr: 0.000091  loss: 0.1740 (0.1904)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [610/689]  eta: 0:02:04  lr: 0.000091  loss: 0.1773 (0.1903)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [620/689]  eta: 0:01:48  lr: 0.000091  loss: 0.1831 (0.1903)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [630/689]  eta: 0:01:33  lr: 0.000091  loss: 0.1915 (0.1905)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [640/689]  eta: 0:01:17  lr: 0.000091  loss: 0.1959 (0.1906)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [650/689]  eta: 0:01:01  lr: 0.000091  loss: 0.1769 (0.1905)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [660/689]  eta: 0:00:45  lr: 0.000091  loss: 0.1745 (0.1904)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [670/689]  eta: 0:00:29  lr: 0.000091  loss: 0.1760 (0.1905)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [680/689]  eta: 0:00:14  lr: 0.000091  loss: 0.1900 (0.1906)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185]  [688/689]  eta: 0:00:01  lr: 0.000091  loss: 0.1897 (0.1905)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:185] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: 0.1897 (0.1905)\n",
      "Valid: [epoch:185]  [ 0/14]  eta: 0:00:14  loss: 0.1716 (0.1716)  time: 1.0251  data: 0.3798  max mem: 39763\n",
      "Valid: [epoch:185]  [13/14]  eta: 0:00:00  loss: 0.1755 (0.1779)  time: 0.1152  data: 0.0272  max mem: 39763\n",
      "Valid: [epoch:185] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.1755 (0.1779)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_185_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.178%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:186]  [  0/689]  eta: 0:11:17  lr: 0.000091  loss: 0.2093 (0.2093)  time: 0.9829  data: 0.5056  max mem: 39763\n",
      "Train: [epoch:186]  [ 10/689]  eta: 0:17:14  lr: 0.000091  loss: 0.1828 (0.1882)  time: 1.5242  data: 0.0461  max mem: 39763\n",
      "Train: [epoch:186]  [ 20/689]  eta: 0:17:16  lr: 0.000091  loss: 0.1813 (0.1860)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [ 30/689]  eta: 0:17:07  lr: 0.000091  loss: 0.1877 (0.1887)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [ 40/689]  eta: 0:16:54  lr: 0.000091  loss: 0.1822 (0.1859)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [ 50/689]  eta: 0:16:40  lr: 0.000091  loss: 0.1803 (0.1855)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [ 60/689]  eta: 0:16:26  lr: 0.000091  loss: 0.1742 (0.1838)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [ 70/689]  eta: 0:16:11  lr: 0.000091  loss: 0.1768 (0.1846)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [ 80/689]  eta: 0:15:56  lr: 0.000091  loss: 0.1787 (0.1841)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [ 90/689]  eta: 0:15:41  lr: 0.000091  loss: 0.1778 (0.1841)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [100/689]  eta: 0:15:26  lr: 0.000091  loss: 0.1800 (0.1843)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [110/689]  eta: 0:15:10  lr: 0.000091  loss: 0.1899 (0.1853)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [120/689]  eta: 0:14:55  lr: 0.000091  loss: 0.1899 (0.1855)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [130/689]  eta: 0:14:39  lr: 0.000091  loss: 0.1806 (0.1857)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [140/689]  eta: 0:14:24  lr: 0.000091  loss: 0.1823 (0.1860)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [150/689]  eta: 0:14:08  lr: 0.000091  loss: 0.1832 (0.1859)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [160/689]  eta: 0:13:53  lr: 0.000091  loss: 0.1930 (0.1863)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [170/689]  eta: 0:13:37  lr: 0.000091  loss: 0.1972 (0.1874)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [180/689]  eta: 0:13:21  lr: 0.000091  loss: 0.1886 (0.1873)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [190/689]  eta: 0:13:06  lr: 0.000091  loss: 0.1845 (0.1876)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [200/689]  eta: 0:12:50  lr: 0.000091  loss: 0.1878 (0.1875)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [210/689]  eta: 0:12:34  lr: 0.000091  loss: 0.1840 (0.1872)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [220/689]  eta: 0:12:18  lr: 0.000091  loss: 0.1870 (0.1876)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [230/689]  eta: 0:12:03  lr: 0.000091  loss: 0.1903 (0.1877)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [240/689]  eta: 0:11:47  lr: 0.000091  loss: 0.1786 (0.1876)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [250/689]  eta: 0:11:31  lr: 0.000091  loss: 0.1824 (0.1878)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [260/689]  eta: 0:11:16  lr: 0.000091  loss: 0.1899 (0.1879)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [270/689]  eta: 0:11:00  lr: 0.000091  loss: 0.1925 (0.1880)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [280/689]  eta: 0:10:44  lr: 0.000091  loss: 0.1862 (0.1880)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [290/689]  eta: 0:10:28  lr: 0.000091  loss: 0.1844 (0.1881)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [300/689]  eta: 0:10:13  lr: 0.000091  loss: 0.1864 (0.1881)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [310/689]  eta: 0:09:57  lr: 0.000091  loss: 0.1874 (0.1879)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [320/689]  eta: 0:09:41  lr: 0.000091  loss: 0.1863 (0.1880)  time: 1.5788  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:186]  [330/689]  eta: 0:09:26  lr: 0.000091  loss: 0.1817 (0.1877)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [340/689]  eta: 0:09:10  lr: 0.000091  loss: 0.1766 (0.1879)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [350/689]  eta: 0:08:54  lr: 0.000091  loss: 0.1836 (0.1879)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [360/689]  eta: 0:08:38  lr: 0.000091  loss: 0.1888 (0.1879)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [370/689]  eta: 0:08:23  lr: 0.000091  loss: 0.1885 (0.1879)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [380/689]  eta: 0:08:07  lr: 0.000091  loss: 0.1812 (0.1877)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [390/689]  eta: 0:07:51  lr: 0.000091  loss: 0.1861 (0.1881)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [400/689]  eta: 0:07:35  lr: 0.000091  loss: 0.2076 (0.1886)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [410/689]  eta: 0:07:19  lr: 0.000091  loss: 0.1988 (0.1888)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [420/689]  eta: 0:07:04  lr: 0.000091  loss: 0.1844 (0.1887)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [430/689]  eta: 0:06:48  lr: 0.000091  loss: 0.1820 (0.1887)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [440/689]  eta: 0:06:32  lr: 0.000091  loss: 0.1879 (0.1887)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [450/689]  eta: 0:06:16  lr: 0.000091  loss: 0.1925 (0.1888)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [460/689]  eta: 0:06:01  lr: 0.000091  loss: 0.2002 (0.1892)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [470/689]  eta: 0:05:45  lr: 0.000091  loss: 0.2057 (0.1895)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [480/689]  eta: 0:05:29  lr: 0.000091  loss: 0.1842 (0.1895)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [490/689]  eta: 0:05:13  lr: 0.000091  loss: 0.1821 (0.1897)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [500/689]  eta: 0:04:58  lr: 0.000091  loss: 0.1840 (0.1897)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [510/689]  eta: 0:04:42  lr: 0.000091  loss: 0.1873 (0.1897)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [520/689]  eta: 0:04:26  lr: 0.000091  loss: 0.1861 (0.1896)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [530/689]  eta: 0:04:10  lr: 0.000091  loss: 0.1842 (0.1897)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [540/689]  eta: 0:03:55  lr: 0.000091  loss: 0.1870 (0.1898)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [550/689]  eta: 0:03:39  lr: 0.000091  loss: 0.1826 (0.1896)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [560/689]  eta: 0:03:23  lr: 0.000091  loss: 0.1848 (0.1896)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [570/689]  eta: 0:03:07  lr: 0.000091  loss: 0.1888 (0.1897)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [580/689]  eta: 0:02:51  lr: 0.000091  loss: 0.1860 (0.1899)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [590/689]  eta: 0:02:36  lr: 0.000091  loss: 0.1799 (0.1897)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [600/689]  eta: 0:02:20  lr: 0.000091  loss: 0.1745 (0.1896)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [610/689]  eta: 0:02:04  lr: 0.000091  loss: 0.1801 (0.1895)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [620/689]  eta: 0:01:48  lr: 0.000091  loss: 0.1962 (0.1898)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [630/689]  eta: 0:01:33  lr: 0.000091  loss: 0.1884 (0.1898)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [640/689]  eta: 0:01:17  lr: 0.000091  loss: 0.1847 (0.1898)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [650/689]  eta: 0:01:01  lr: 0.000091  loss: 0.1877 (0.1899)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [660/689]  eta: 0:00:45  lr: 0.000091  loss: 0.1912 (0.1900)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [670/689]  eta: 0:00:29  lr: 0.000091  loss: 0.1927 (0.1900)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [680/689]  eta: 0:00:14  lr: 0.000091  loss: 0.1832 (0.1900)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186]  [688/689]  eta: 0:00:01  lr: 0.000091  loss: 0.1832 (0.1901)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:186] Total time: 0:18:07 (1.5779 s / it)\n",
      "Averaged stats: lr: 0.000091  loss: 0.1832 (0.1901)\n",
      "Valid: [epoch:186]  [ 0/14]  eta: 0:00:14  loss: 0.1719 (0.1719)  time: 1.0139  data: 0.3934  max mem: 39763\n",
      "Valid: [epoch:186]  [13/14]  eta: 0:00:00  loss: 0.1764 (0.1783)  time: 0.1143  data: 0.0282  max mem: 39763\n",
      "Valid: [epoch:186] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.1764 (0.1783)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_186_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.178%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:187]  [  0/689]  eta: 0:11:25  lr: 0.000090  loss: 0.1681 (0.1681)  time: 0.9954  data: 0.5183  max mem: 39763\n",
      "Train: [epoch:187]  [ 10/689]  eta: 0:17:14  lr: 0.000090  loss: 0.1855 (0.1944)  time: 1.5240  data: 0.0472  max mem: 39763\n",
      "Train: [epoch:187]  [ 20/689]  eta: 0:17:17  lr: 0.000090  loss: 0.1820 (0.1860)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [ 30/689]  eta: 0:17:07  lr: 0.000090  loss: 0.1820 (0.1898)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [ 40/689]  eta: 0:16:54  lr: 0.000090  loss: 0.1910 (0.1896)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [ 50/689]  eta: 0:16:41  lr: 0.000090  loss: 0.1789 (0.1886)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [ 60/689]  eta: 0:16:26  lr: 0.000090  loss: 0.1789 (0.1895)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [ 70/689]  eta: 0:16:12  lr: 0.000090  loss: 0.1831 (0.1892)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [ 80/689]  eta: 0:15:57  lr: 0.000090  loss: 0.1831 (0.1888)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [ 90/689]  eta: 0:15:41  lr: 0.000090  loss: 0.1819 (0.1880)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [100/689]  eta: 0:15:26  lr: 0.000090  loss: 0.1781 (0.1880)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [110/689]  eta: 0:15:11  lr: 0.000090  loss: 0.1818 (0.1893)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [120/689]  eta: 0:14:55  lr: 0.000090  loss: 0.1860 (0.1899)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [130/689]  eta: 0:14:40  lr: 0.000090  loss: 0.1921 (0.1901)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [140/689]  eta: 0:14:25  lr: 0.000090  loss: 0.1874 (0.1902)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [150/689]  eta: 0:14:09  lr: 0.000090  loss: 0.1858 (0.1897)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [160/689]  eta: 0:13:54  lr: 0.000090  loss: 0.1875 (0.1901)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [170/689]  eta: 0:13:38  lr: 0.000090  loss: 0.1902 (0.1901)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [180/689]  eta: 0:13:22  lr: 0.000090  loss: 0.1898 (0.1901)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [190/689]  eta: 0:13:07  lr: 0.000090  loss: 0.1848 (0.1899)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [200/689]  eta: 0:12:51  lr: 0.000090  loss: 0.1810 (0.1898)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [210/689]  eta: 0:12:35  lr: 0.000090  loss: 0.1849 (0.1896)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [220/689]  eta: 0:12:20  lr: 0.000090  loss: 0.1842 (0.1899)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [230/689]  eta: 0:12:04  lr: 0.000090  loss: 0.1974 (0.1905)  time: 1.5840  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:187]  [240/689]  eta: 0:11:48  lr: 0.000090  loss: 0.1916 (0.1908)  time: 1.5842  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [250/689]  eta: 0:11:33  lr: 0.000090  loss: 0.1895 (0.1911)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [260/689]  eta: 0:11:17  lr: 0.000090  loss: 0.1825 (0.1909)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [270/689]  eta: 0:11:01  lr: 0.000090  loss: 0.1792 (0.1908)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [280/689]  eta: 0:10:45  lr: 0.000090  loss: 0.1835 (0.1908)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [290/689]  eta: 0:10:30  lr: 0.000090  loss: 0.1817 (0.1903)  time: 1.5837  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [300/689]  eta: 0:10:14  lr: 0.000090  loss: 0.1817 (0.1905)  time: 1.5840  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [310/689]  eta: 0:09:58  lr: 0.000090  loss: 0.2000 (0.1906)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [320/689]  eta: 0:09:43  lr: 0.000090  loss: 0.1949 (0.1911)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [330/689]  eta: 0:09:27  lr: 0.000090  loss: 0.1968 (0.1913)  time: 1.5839  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [340/689]  eta: 0:09:11  lr: 0.000090  loss: 0.1927 (0.1913)  time: 1.5836  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [350/689]  eta: 0:08:55  lr: 0.000090  loss: 0.1810 (0.1912)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [360/689]  eta: 0:08:39  lr: 0.000090  loss: 0.1836 (0.1915)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [370/689]  eta: 0:08:24  lr: 0.000090  loss: 0.1825 (0.1913)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [380/689]  eta: 0:08:08  lr: 0.000090  loss: 0.1816 (0.1913)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [390/689]  eta: 0:07:52  lr: 0.000090  loss: 0.1816 (0.1913)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [400/689]  eta: 0:07:36  lr: 0.000090  loss: 0.1902 (0.1916)  time: 1.5834  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [410/689]  eta: 0:07:21  lr: 0.000090  loss: 0.1841 (0.1917)  time: 1.5842  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [420/689]  eta: 0:07:05  lr: 0.000090  loss: 0.1825 (0.1915)  time: 1.5838  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [430/689]  eta: 0:06:49  lr: 0.000090  loss: 0.1825 (0.1915)  time: 1.5839  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [440/689]  eta: 0:06:33  lr: 0.000090  loss: 0.1864 (0.1915)  time: 1.5842  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [450/689]  eta: 0:06:17  lr: 0.000090  loss: 0.1919 (0.1915)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [460/689]  eta: 0:06:02  lr: 0.000090  loss: 0.2054 (0.1917)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [470/689]  eta: 0:05:46  lr: 0.000090  loss: 0.1846 (0.1915)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [480/689]  eta: 0:05:30  lr: 0.000090  loss: 0.1795 (0.1913)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [490/689]  eta: 0:05:14  lr: 0.000090  loss: 0.1857 (0.1914)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [500/689]  eta: 0:04:58  lr: 0.000090  loss: 0.1941 (0.1919)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [510/689]  eta: 0:04:43  lr: 0.000090  loss: 0.1975 (0.1921)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [520/689]  eta: 0:04:27  lr: 0.000090  loss: 0.1934 (0.1922)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [530/689]  eta: 0:04:11  lr: 0.000090  loss: 0.1893 (0.1921)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [540/689]  eta: 0:03:55  lr: 0.000090  loss: 0.1918 (0.1922)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [550/689]  eta: 0:03:39  lr: 0.000090  loss: 0.1944 (0.1922)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [560/689]  eta: 0:03:23  lr: 0.000090  loss: 0.1885 (0.1922)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [570/689]  eta: 0:03:08  lr: 0.000090  loss: 0.1889 (0.1923)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [580/689]  eta: 0:02:52  lr: 0.000090  loss: 0.1957 (0.1923)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [590/689]  eta: 0:02:36  lr: 0.000090  loss: 0.1888 (0.1922)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [600/689]  eta: 0:02:20  lr: 0.000090  loss: 0.1772 (0.1923)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [610/689]  eta: 0:02:04  lr: 0.000090  loss: 0.1813 (0.1921)  time: 1.5833  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [620/689]  eta: 0:01:49  lr: 0.000090  loss: 0.1816 (0.1920)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [630/689]  eta: 0:01:33  lr: 0.000090  loss: 0.1908 (0.1921)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [640/689]  eta: 0:01:17  lr: 0.000090  loss: 0.1956 (0.1922)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [650/689]  eta: 0:01:01  lr: 0.000090  loss: 0.1898 (0.1922)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [660/689]  eta: 0:00:45  lr: 0.000090  loss: 0.1898 (0.1923)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [670/689]  eta: 0:00:30  lr: 0.000090  loss: 0.1885 (0.1924)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [680/689]  eta: 0:00:14  lr: 0.000090  loss: 0.1996 (0.1925)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187]  [688/689]  eta: 0:00:01  lr: 0.000090  loss: 0.1946 (0.1923)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:187] Total time: 0:18:09 (1.5818 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.1946 (0.1923)\n",
      "Valid: [epoch:187]  [ 0/14]  eta: 0:00:14  loss: 0.1734 (0.1734)  time: 1.0144  data: 0.3989  max mem: 39763\n",
      "Valid: [epoch:187]  [13/14]  eta: 0:00:00  loss: 0.1783 (0.1806)  time: 0.1146  data: 0.0285  max mem: 39763\n",
      "Valid: [epoch:187] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.1783 (0.1806)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_187_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.181%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:188]  [  0/689]  eta: 0:12:13  lr: 0.000090  loss: 0.1789 (0.1789)  time: 1.0651  data: 0.5844  max mem: 39763\n",
      "Train: [epoch:188]  [ 10/689]  eta: 0:17:20  lr: 0.000090  loss: 0.2128 (0.2036)  time: 1.5318  data: 0.0532  max mem: 39763\n",
      "Train: [epoch:188]  [ 20/689]  eta: 0:17:19  lr: 0.000090  loss: 0.1910 (0.1953)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [ 30/689]  eta: 0:17:09  lr: 0.000090  loss: 0.1872 (0.1949)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [ 40/689]  eta: 0:16:56  lr: 0.000090  loss: 0.1923 (0.1939)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [ 50/689]  eta: 0:16:42  lr: 0.000090  loss: 0.1827 (0.1915)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [ 60/689]  eta: 0:16:27  lr: 0.000090  loss: 0.1756 (0.1898)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [ 70/689]  eta: 0:16:12  lr: 0.000090  loss: 0.1820 (0.1935)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [ 80/689]  eta: 0:15:57  lr: 0.000090  loss: 0.2032 (0.1951)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [ 90/689]  eta: 0:15:42  lr: 0.000090  loss: 0.2032 (0.1954)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [100/689]  eta: 0:15:26  lr: 0.000090  loss: 0.1904 (0.1947)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [110/689]  eta: 0:15:11  lr: 0.000090  loss: 0.1942 (0.1956)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [120/689]  eta: 0:14:55  lr: 0.000090  loss: 0.1957 (0.1958)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [130/689]  eta: 0:14:40  lr: 0.000090  loss: 0.1873 (0.1955)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [140/689]  eta: 0:14:24  lr: 0.000090  loss: 0.1865 (0.1948)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:188]  [150/689]  eta: 0:14:08  lr: 0.000090  loss: 0.1869 (0.1951)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [160/689]  eta: 0:13:53  lr: 0.000090  loss: 0.1894 (0.1951)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [170/689]  eta: 0:13:37  lr: 0.000090  loss: 0.1892 (0.1950)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [180/689]  eta: 0:13:22  lr: 0.000090  loss: 0.1924 (0.1952)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [190/689]  eta: 0:13:06  lr: 0.000090  loss: 0.1885 (0.1950)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [200/689]  eta: 0:12:50  lr: 0.000090  loss: 0.1892 (0.1954)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [210/689]  eta: 0:12:34  lr: 0.000090  loss: 0.1925 (0.1950)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [220/689]  eta: 0:12:19  lr: 0.000090  loss: 0.1932 (0.1951)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [230/689]  eta: 0:12:03  lr: 0.000090  loss: 0.1912 (0.1950)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [240/689]  eta: 0:11:47  lr: 0.000090  loss: 0.1935 (0.1955)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [250/689]  eta: 0:11:32  lr: 0.000090  loss: 0.1935 (0.1952)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [260/689]  eta: 0:11:16  lr: 0.000090  loss: 0.1908 (0.1954)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [270/689]  eta: 0:11:00  lr: 0.000090  loss: 0.1873 (0.1951)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [280/689]  eta: 0:10:44  lr: 0.000090  loss: 0.1917 (0.1955)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [290/689]  eta: 0:10:29  lr: 0.000090  loss: 0.1917 (0.1952)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [300/689]  eta: 0:10:13  lr: 0.000090  loss: 0.1833 (0.1952)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [310/689]  eta: 0:09:57  lr: 0.000090  loss: 0.1870 (0.1952)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [320/689]  eta: 0:09:41  lr: 0.000090  loss: 0.1870 (0.1951)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [330/689]  eta: 0:09:26  lr: 0.000090  loss: 0.1919 (0.1952)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [340/689]  eta: 0:09:10  lr: 0.000090  loss: 0.1912 (0.1952)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [350/689]  eta: 0:08:54  lr: 0.000090  loss: 0.1876 (0.1951)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [360/689]  eta: 0:08:38  lr: 0.000090  loss: 0.1876 (0.1951)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [370/689]  eta: 0:08:23  lr: 0.000090  loss: 0.1839 (0.1948)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [380/689]  eta: 0:08:07  lr: 0.000090  loss: 0.1839 (0.1949)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [390/689]  eta: 0:07:51  lr: 0.000090  loss: 0.1909 (0.1949)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [400/689]  eta: 0:07:35  lr: 0.000090  loss: 0.2011 (0.1951)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [410/689]  eta: 0:07:20  lr: 0.000090  loss: 0.1904 (0.1951)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [420/689]  eta: 0:07:04  lr: 0.000090  loss: 0.1833 (0.1948)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [430/689]  eta: 0:06:48  lr: 0.000090  loss: 0.1805 (0.1946)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [440/689]  eta: 0:06:32  lr: 0.000090  loss: 0.1826 (0.1944)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [450/689]  eta: 0:06:17  lr: 0.000090  loss: 0.1858 (0.1944)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [460/689]  eta: 0:06:01  lr: 0.000090  loss: 0.1914 (0.1944)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [470/689]  eta: 0:05:45  lr: 0.000090  loss: 0.1925 (0.1945)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [480/689]  eta: 0:05:29  lr: 0.000090  loss: 0.1949 (0.1946)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [490/689]  eta: 0:05:13  lr: 0.000090  loss: 0.1949 (0.1946)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [500/689]  eta: 0:04:58  lr: 0.000090  loss: 0.1956 (0.1946)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [510/689]  eta: 0:04:42  lr: 0.000090  loss: 0.1934 (0.1948)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [520/689]  eta: 0:04:26  lr: 0.000090  loss: 0.1907 (0.1948)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [530/689]  eta: 0:04:10  lr: 0.000090  loss: 0.1856 (0.1948)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [540/689]  eta: 0:03:55  lr: 0.000090  loss: 0.1997 (0.1950)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [550/689]  eta: 0:03:39  lr: 0.000090  loss: 0.1991 (0.1949)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [560/689]  eta: 0:03:23  lr: 0.000090  loss: 0.1888 (0.1950)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [570/689]  eta: 0:03:07  lr: 0.000090  loss: 0.1869 (0.1948)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [580/689]  eta: 0:02:52  lr: 0.000090  loss: 0.1809 (0.1946)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [590/689]  eta: 0:02:36  lr: 0.000090  loss: 0.1877 (0.1947)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [600/689]  eta: 0:02:20  lr: 0.000090  loss: 0.1923 (0.1949)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [610/689]  eta: 0:02:04  lr: 0.000090  loss: 0.1923 (0.1949)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [620/689]  eta: 0:01:48  lr: 0.000090  loss: 0.1962 (0.1950)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [630/689]  eta: 0:01:33  lr: 0.000090  loss: 0.1927 (0.1951)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [640/689]  eta: 0:01:17  lr: 0.000090  loss: 0.1874 (0.1950)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [650/689]  eta: 0:01:01  lr: 0.000090  loss: 0.1805 (0.1949)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [660/689]  eta: 0:00:45  lr: 0.000090  loss: 0.1925 (0.1950)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [670/689]  eta: 0:00:29  lr: 0.000090  loss: 0.1954 (0.1948)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [680/689]  eta: 0:00:14  lr: 0.000090  loss: 0.1821 (0.1947)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188]  [688/689]  eta: 0:00:01  lr: 0.000090  loss: 0.1832 (0.1947)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:188] Total time: 0:18:07 (1.5784 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.1832 (0.1947)\n",
      "Valid: [epoch:188]  [ 0/14]  eta: 0:00:14  loss: 0.1786 (0.1786)  time: 1.0228  data: 0.3808  max mem: 39763\n",
      "Valid: [epoch:188]  [13/14]  eta: 0:00:00  loss: 0.1825 (0.1847)  time: 0.1150  data: 0.0273  max mem: 39763\n",
      "Valid: [epoch:188] Total time: 0:00:01 (0.1271 s / it)\n",
      "Averaged stats: loss: 0.1825 (0.1847)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_188_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.185%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:189]  [  0/689]  eta: 0:11:46  lr: 0.000090  loss: 0.1885 (0.1885)  time: 1.0253  data: 0.5478  max mem: 39763\n",
      "Train: [epoch:189]  [ 10/689]  eta: 0:17:15  lr: 0.000090  loss: 0.1965 (0.2021)  time: 1.5258  data: 0.0499  max mem: 39763\n",
      "Train: [epoch:189]  [ 20/689]  eta: 0:17:17  lr: 0.000090  loss: 0.1868 (0.1943)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [ 30/689]  eta: 0:17:07  lr: 0.000090  loss: 0.1814 (0.1923)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [ 40/689]  eta: 0:16:55  lr: 0.000090  loss: 0.1890 (0.1933)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [ 50/689]  eta: 0:16:41  lr: 0.000090  loss: 0.1924 (0.1927)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:189]  [ 60/689]  eta: 0:16:26  lr: 0.000090  loss: 0.1823 (0.1921)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [ 70/689]  eta: 0:16:11  lr: 0.000090  loss: 0.1823 (0.1915)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [ 80/689]  eta: 0:15:56  lr: 0.000090  loss: 0.1865 (0.1923)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [ 90/689]  eta: 0:15:41  lr: 0.000090  loss: 0.2009 (0.1934)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [100/689]  eta: 0:15:26  lr: 0.000090  loss: 0.2033 (0.1941)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [110/689]  eta: 0:15:10  lr: 0.000090  loss: 0.2032 (0.1953)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [120/689]  eta: 0:14:55  lr: 0.000090  loss: 0.1894 (0.1948)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [130/689]  eta: 0:14:40  lr: 0.000090  loss: 0.1882 (0.1943)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [140/689]  eta: 0:14:24  lr: 0.000090  loss: 0.1823 (0.1934)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [150/689]  eta: 0:14:08  lr: 0.000090  loss: 0.1861 (0.1941)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [160/689]  eta: 0:13:53  lr: 0.000090  loss: 0.1961 (0.1944)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [170/689]  eta: 0:13:37  lr: 0.000090  loss: 0.1874 (0.1936)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [180/689]  eta: 0:13:21  lr: 0.000090  loss: 0.1874 (0.1944)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [190/689]  eta: 0:13:06  lr: 0.000090  loss: 0.1890 (0.1939)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [200/689]  eta: 0:12:50  lr: 0.000090  loss: 0.1815 (0.1936)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [210/689]  eta: 0:12:34  lr: 0.000090  loss: 0.1857 (0.1935)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [220/689]  eta: 0:12:19  lr: 0.000090  loss: 0.1917 (0.1938)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [230/689]  eta: 0:12:03  lr: 0.000090  loss: 0.1917 (0.1942)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [240/689]  eta: 0:11:47  lr: 0.000090  loss: 0.1892 (0.1943)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [250/689]  eta: 0:11:32  lr: 0.000090  loss: 0.1860 (0.1941)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [260/689]  eta: 0:11:16  lr: 0.000090  loss: 0.1910 (0.1942)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [270/689]  eta: 0:11:00  lr: 0.000090  loss: 0.1805 (0.1935)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [280/689]  eta: 0:10:44  lr: 0.000090  loss: 0.1782 (0.1934)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [290/689]  eta: 0:10:29  lr: 0.000090  loss: 0.1868 (0.1931)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [300/689]  eta: 0:10:13  lr: 0.000090  loss: 0.1872 (0.1932)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [310/689]  eta: 0:09:57  lr: 0.000090  loss: 0.1816 (0.1930)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [320/689]  eta: 0:09:41  lr: 0.000090  loss: 0.1836 (0.1932)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [330/689]  eta: 0:09:26  lr: 0.000090  loss: 0.1956 (0.1934)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [340/689]  eta: 0:09:10  lr: 0.000090  loss: 0.1956 (0.1931)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [350/689]  eta: 0:08:54  lr: 0.000090  loss: 0.1832 (0.1931)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [360/689]  eta: 0:08:38  lr: 0.000090  loss: 0.1863 (0.1932)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [370/689]  eta: 0:08:23  lr: 0.000090  loss: 0.1898 (0.1934)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [380/689]  eta: 0:08:07  lr: 0.000090  loss: 0.1849 (0.1931)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [390/689]  eta: 0:07:51  lr: 0.000090  loss: 0.1850 (0.1932)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [400/689]  eta: 0:07:35  lr: 0.000090  loss: 0.1886 (0.1932)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [410/689]  eta: 0:07:20  lr: 0.000090  loss: 0.1888 (0.1934)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [420/689]  eta: 0:07:04  lr: 0.000090  loss: 0.1841 (0.1931)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [430/689]  eta: 0:06:48  lr: 0.000090  loss: 0.1908 (0.1932)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [440/689]  eta: 0:06:32  lr: 0.000090  loss: 0.1972 (0.1934)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [450/689]  eta: 0:06:17  lr: 0.000090  loss: 0.1928 (0.1934)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [460/689]  eta: 0:06:01  lr: 0.000090  loss: 0.1971 (0.1937)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [470/689]  eta: 0:05:45  lr: 0.000090  loss: 0.1919 (0.1936)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [480/689]  eta: 0:05:29  lr: 0.000090  loss: 0.1877 (0.1935)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [490/689]  eta: 0:05:13  lr: 0.000090  loss: 0.1886 (0.1935)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [500/689]  eta: 0:04:58  lr: 0.000090  loss: 0.1824 (0.1934)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [510/689]  eta: 0:04:42  lr: 0.000090  loss: 0.1843 (0.1936)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [520/689]  eta: 0:04:26  lr: 0.000090  loss: 0.2029 (0.1939)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [530/689]  eta: 0:04:10  lr: 0.000090  loss: 0.1936 (0.1936)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [540/689]  eta: 0:03:55  lr: 0.000090  loss: 0.1754 (0.1934)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [550/689]  eta: 0:03:39  lr: 0.000090  loss: 0.1777 (0.1934)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [560/689]  eta: 0:03:23  lr: 0.000090  loss: 0.1799 (0.1933)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [570/689]  eta: 0:03:07  lr: 0.000090  loss: 0.1889 (0.1936)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [580/689]  eta: 0:02:52  lr: 0.000090  loss: 0.1929 (0.1935)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [590/689]  eta: 0:02:36  lr: 0.000090  loss: 0.1832 (0.1934)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [600/689]  eta: 0:02:20  lr: 0.000090  loss: 0.1877 (0.1934)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [610/689]  eta: 0:02:04  lr: 0.000090  loss: 0.1878 (0.1935)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [620/689]  eta: 0:01:48  lr: 0.000090  loss: 0.1977 (0.1935)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [630/689]  eta: 0:01:33  lr: 0.000090  loss: 0.1850 (0.1935)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [640/689]  eta: 0:01:17  lr: 0.000090  loss: 0.1886 (0.1937)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [650/689]  eta: 0:01:01  lr: 0.000090  loss: 0.1865 (0.1935)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [660/689]  eta: 0:00:45  lr: 0.000090  loss: 0.1813 (0.1936)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [670/689]  eta: 0:00:29  lr: 0.000090  loss: 0.1965 (0.1936)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [680/689]  eta: 0:00:14  lr: 0.000090  loss: 0.1908 (0.1937)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189]  [688/689]  eta: 0:00:01  lr: 0.000090  loss: 0.1935 (0.1937)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:189] Total time: 0:18:07 (1.5784 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.1935 (0.1937)\n",
      "Valid: [epoch:189]  [ 0/14]  eta: 0:00:14  loss: 0.1771 (0.1771)  time: 1.0182  data: 0.4210  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:189]  [13/14]  eta: 0:00:00  loss: 0.1812 (0.1840)  time: 0.1147  data: 0.0301  max mem: 39763\n",
      "Valid: [epoch:189] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.1812 (0.1840)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_189_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.184%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:190]  [  0/689]  eta: 0:11:59  lr: 0.000090  loss: 0.1739 (0.1739)  time: 1.0448  data: 0.5667  max mem: 39763\n",
      "Train: [epoch:190]  [ 10/689]  eta: 0:17:18  lr: 0.000090  loss: 0.1858 (0.1887)  time: 1.5293  data: 0.0516  max mem: 39763\n",
      "Train: [epoch:190]  [ 20/689]  eta: 0:17:18  lr: 0.000090  loss: 0.1916 (0.1943)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [ 30/689]  eta: 0:17:08  lr: 0.000090  loss: 0.1916 (0.1953)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [ 40/689]  eta: 0:16:56  lr: 0.000090  loss: 0.1833 (0.1963)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [ 50/689]  eta: 0:16:41  lr: 0.000090  loss: 0.1927 (0.1967)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [ 60/689]  eta: 0:16:27  lr: 0.000090  loss: 0.1979 (0.1981)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [ 70/689]  eta: 0:16:12  lr: 0.000090  loss: 0.1852 (0.1960)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [ 80/689]  eta: 0:15:57  lr: 0.000090  loss: 0.1885 (0.1965)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [ 90/689]  eta: 0:15:42  lr: 0.000090  loss: 0.1948 (0.1968)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [100/689]  eta: 0:15:26  lr: 0.000090  loss: 0.1944 (0.1972)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [110/689]  eta: 0:15:11  lr: 0.000090  loss: 0.1947 (0.1967)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [120/689]  eta: 0:14:55  lr: 0.000090  loss: 0.1947 (0.1973)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [130/689]  eta: 0:14:40  lr: 0.000090  loss: 0.1897 (0.1969)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [140/689]  eta: 0:14:24  lr: 0.000090  loss: 0.1887 (0.1970)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [150/689]  eta: 0:14:09  lr: 0.000090  loss: 0.1887 (0.1971)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [160/689]  eta: 0:13:53  lr: 0.000090  loss: 0.1861 (0.1971)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [170/689]  eta: 0:13:37  lr: 0.000090  loss: 0.1883 (0.1970)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [180/689]  eta: 0:13:22  lr: 0.000090  loss: 0.1854 (0.1965)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [190/689]  eta: 0:13:06  lr: 0.000090  loss: 0.1854 (0.1970)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [200/689]  eta: 0:12:50  lr: 0.000090  loss: 0.1945 (0.1969)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [210/689]  eta: 0:12:35  lr: 0.000090  loss: 0.1845 (0.1969)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [220/689]  eta: 0:12:19  lr: 0.000090  loss: 0.1853 (0.1967)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [230/689]  eta: 0:12:03  lr: 0.000090  loss: 0.1941 (0.1966)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [240/689]  eta: 0:11:48  lr: 0.000090  loss: 0.1951 (0.1971)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [250/689]  eta: 0:11:32  lr: 0.000090  loss: 0.1987 (0.1971)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [260/689]  eta: 0:11:16  lr: 0.000090  loss: 0.1890 (0.1974)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [270/689]  eta: 0:11:00  lr: 0.000090  loss: 0.1838 (0.1971)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [280/689]  eta: 0:10:45  lr: 0.000090  loss: 0.1840 (0.1968)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [290/689]  eta: 0:10:29  lr: 0.000090  loss: 0.1890 (0.1967)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [300/689]  eta: 0:10:13  lr: 0.000090  loss: 0.1866 (0.1967)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [310/689]  eta: 0:09:57  lr: 0.000090  loss: 0.1876 (0.1965)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [320/689]  eta: 0:09:42  lr: 0.000090  loss: 0.1909 (0.1962)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [330/689]  eta: 0:09:26  lr: 0.000090  loss: 0.1860 (0.1965)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [340/689]  eta: 0:09:10  lr: 0.000090  loss: 0.1849 (0.1963)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [350/689]  eta: 0:08:54  lr: 0.000090  loss: 0.1849 (0.1965)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [360/689]  eta: 0:08:39  lr: 0.000090  loss: 0.1947 (0.1965)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [370/689]  eta: 0:08:23  lr: 0.000090  loss: 0.1896 (0.1962)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [380/689]  eta: 0:08:07  lr: 0.000090  loss: 0.1858 (0.1960)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [390/689]  eta: 0:07:51  lr: 0.000090  loss: 0.1949 (0.1960)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [400/689]  eta: 0:07:36  lr: 0.000090  loss: 0.1884 (0.1957)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [410/689]  eta: 0:07:20  lr: 0.000090  loss: 0.1884 (0.1960)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [420/689]  eta: 0:07:04  lr: 0.000090  loss: 0.1905 (0.1960)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [430/689]  eta: 0:06:48  lr: 0.000090  loss: 0.1864 (0.1957)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [440/689]  eta: 0:06:32  lr: 0.000090  loss: 0.1896 (0.1956)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [450/689]  eta: 0:06:17  lr: 0.000090  loss: 0.1862 (0.1955)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [460/689]  eta: 0:06:01  lr: 0.000090  loss: 0.1918 (0.1958)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [470/689]  eta: 0:05:45  lr: 0.000090  loss: 0.1976 (0.1958)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [480/689]  eta: 0:05:29  lr: 0.000090  loss: 0.1810 (0.1955)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [490/689]  eta: 0:05:14  lr: 0.000090  loss: 0.1837 (0.1956)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [500/689]  eta: 0:04:58  lr: 0.000090  loss: 0.1873 (0.1954)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [510/689]  eta: 0:04:42  lr: 0.000090  loss: 0.1864 (0.1955)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [520/689]  eta: 0:04:26  lr: 0.000090  loss: 0.1886 (0.1954)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [530/689]  eta: 0:04:10  lr: 0.000090  loss: 0.1863 (0.1953)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [540/689]  eta: 0:03:55  lr: 0.000090  loss: 0.1838 (0.1953)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [550/689]  eta: 0:03:39  lr: 0.000090  loss: 0.1971 (0.1955)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [560/689]  eta: 0:03:23  lr: 0.000090  loss: 0.1953 (0.1956)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [570/689]  eta: 0:03:07  lr: 0.000090  loss: 0.1902 (0.1954)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [580/689]  eta: 0:02:52  lr: 0.000090  loss: 0.1922 (0.1955)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [590/689]  eta: 0:02:36  lr: 0.000090  loss: 0.1978 (0.1956)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [600/689]  eta: 0:02:20  lr: 0.000090  loss: 0.1919 (0.1954)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [610/689]  eta: 0:02:04  lr: 0.000090  loss: 0.1878 (0.1954)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [620/689]  eta: 0:01:48  lr: 0.000090  loss: 0.1832 (0.1952)  time: 1.5790  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:190]  [630/689]  eta: 0:01:33  lr: 0.000090  loss: 0.1844 (0.1952)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [640/689]  eta: 0:01:17  lr: 0.000090  loss: 0.1898 (0.1951)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [650/689]  eta: 0:01:01  lr: 0.000090  loss: 0.1873 (0.1951)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [660/689]  eta: 0:00:45  lr: 0.000090  loss: 0.1883 (0.1951)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [670/689]  eta: 0:00:29  lr: 0.000090  loss: 0.1836 (0.1950)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [680/689]  eta: 0:00:14  lr: 0.000090  loss: 0.1804 (0.1949)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190]  [688/689]  eta: 0:00:01  lr: 0.000090  loss: 0.1850 (0.1948)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:190] Total time: 0:18:07 (1.5784 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.1850 (0.1948)\n",
      "Valid: [epoch:190]  [ 0/14]  eta: 0:00:14  loss: 0.1939 (0.1939)  time: 1.0398  data: 0.3619  max mem: 39763\n",
      "Valid: [epoch:190]  [13/14]  eta: 0:00:00  loss: 0.1815 (0.1841)  time: 0.1162  data: 0.0259  max mem: 39763\n",
      "Valid: [epoch:190] Total time: 0:00:01 (0.1254 s / it)\n",
      "Averaged stats: loss: 0.1815 (0.1841)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_190_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.184%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:191]  [  0/689]  eta: 0:11:37  lr: 0.000090  loss: 0.1936 (0.1936)  time: 1.0130  data: 0.5370  max mem: 39763\n",
      "Train: [epoch:191]  [ 10/689]  eta: 0:17:15  lr: 0.000090  loss: 0.1895 (0.1946)  time: 1.5244  data: 0.0489  max mem: 39763\n",
      "Train: [epoch:191]  [ 20/689]  eta: 0:17:16  lr: 0.000090  loss: 0.1895 (0.1960)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [ 30/689]  eta: 0:17:07  lr: 0.000090  loss: 0.1879 (0.1944)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [ 40/689]  eta: 0:16:54  lr: 0.000090  loss: 0.1801 (0.1916)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [ 50/689]  eta: 0:16:40  lr: 0.000090  loss: 0.1839 (0.1934)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [ 60/689]  eta: 0:16:26  lr: 0.000090  loss: 0.1838 (0.1915)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [ 70/689]  eta: 0:16:11  lr: 0.000090  loss: 0.1815 (0.1903)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [ 80/689]  eta: 0:15:56  lr: 0.000090  loss: 0.1805 (0.1896)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [ 90/689]  eta: 0:15:41  lr: 0.000090  loss: 0.1816 (0.1908)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [100/689]  eta: 0:15:26  lr: 0.000090  loss: 0.1982 (0.1919)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [110/689]  eta: 0:15:10  lr: 0.000090  loss: 0.1982 (0.1918)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [120/689]  eta: 0:14:55  lr: 0.000090  loss: 0.1828 (0.1922)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [130/689]  eta: 0:14:39  lr: 0.000090  loss: 0.1839 (0.1917)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [140/689]  eta: 0:14:24  lr: 0.000090  loss: 0.1883 (0.1923)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [150/689]  eta: 0:14:08  lr: 0.000090  loss: 0.2028 (0.1928)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [160/689]  eta: 0:13:52  lr: 0.000090  loss: 0.1920 (0.1931)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [170/689]  eta: 0:13:37  lr: 0.000090  loss: 0.1920 (0.1935)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [180/689]  eta: 0:13:21  lr: 0.000090  loss: 0.1915 (0.1935)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [190/689]  eta: 0:13:05  lr: 0.000090  loss: 0.1913 (0.1936)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [200/689]  eta: 0:12:50  lr: 0.000090  loss: 0.1842 (0.1930)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [210/689]  eta: 0:12:34  lr: 0.000090  loss: 0.1817 (0.1929)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [220/689]  eta: 0:12:18  lr: 0.000090  loss: 0.1867 (0.1929)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [230/689]  eta: 0:12:03  lr: 0.000090  loss: 0.1854 (0.1927)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [240/689]  eta: 0:11:47  lr: 0.000090  loss: 0.1854 (0.1929)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [250/689]  eta: 0:11:31  lr: 0.000090  loss: 0.1905 (0.1926)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [260/689]  eta: 0:11:16  lr: 0.000090  loss: 0.1895 (0.1926)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [270/689]  eta: 0:11:00  lr: 0.000090  loss: 0.1907 (0.1929)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [280/689]  eta: 0:10:44  lr: 0.000090  loss: 0.1936 (0.1929)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [290/689]  eta: 0:10:28  lr: 0.000090  loss: 0.1922 (0.1934)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [300/689]  eta: 0:10:13  lr: 0.000090  loss: 0.2004 (0.1937)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [310/689]  eta: 0:09:57  lr: 0.000090  loss: 0.1917 (0.1933)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [320/689]  eta: 0:09:41  lr: 0.000090  loss: 0.1917 (0.1937)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [330/689]  eta: 0:09:26  lr: 0.000090  loss: 0.1977 (0.1940)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [340/689]  eta: 0:09:10  lr: 0.000090  loss: 0.1977 (0.1939)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [350/689]  eta: 0:08:54  lr: 0.000090  loss: 0.1954 (0.1938)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [360/689]  eta: 0:08:38  lr: 0.000090  loss: 0.1870 (0.1938)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [370/689]  eta: 0:08:23  lr: 0.000090  loss: 0.1864 (0.1934)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [380/689]  eta: 0:08:07  lr: 0.000090  loss: 0.1898 (0.1934)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [390/689]  eta: 0:07:51  lr: 0.000090  loss: 0.1903 (0.1935)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [400/689]  eta: 0:07:35  lr: 0.000090  loss: 0.1996 (0.1939)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [410/689]  eta: 0:07:19  lr: 0.000090  loss: 0.1977 (0.1938)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [420/689]  eta: 0:07:04  lr: 0.000090  loss: 0.1829 (0.1936)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [430/689]  eta: 0:06:48  lr: 0.000090  loss: 0.1900 (0.1940)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [440/689]  eta: 0:06:32  lr: 0.000090  loss: 0.2004 (0.1941)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [450/689]  eta: 0:06:16  lr: 0.000090  loss: 0.1908 (0.1943)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [460/689]  eta: 0:06:01  lr: 0.000090  loss: 0.1971 (0.1945)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [470/689]  eta: 0:05:45  lr: 0.000090  loss: 0.1964 (0.1944)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [480/689]  eta: 0:05:29  lr: 0.000090  loss: 0.1892 (0.1946)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [490/689]  eta: 0:05:13  lr: 0.000090  loss: 0.1874 (0.1945)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [500/689]  eta: 0:04:58  lr: 0.000090  loss: 0.1888 (0.1946)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [510/689]  eta: 0:04:42  lr: 0.000090  loss: 0.1952 (0.1947)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [520/689]  eta: 0:04:26  lr: 0.000090  loss: 0.1993 (0.1949)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [530/689]  eta: 0:04:10  lr: 0.000090  loss: 0.1942 (0.1948)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:191]  [540/689]  eta: 0:03:55  lr: 0.000090  loss: 0.1913 (0.1949)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [550/689]  eta: 0:03:39  lr: 0.000090  loss: 0.1883 (0.1948)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [560/689]  eta: 0:03:23  lr: 0.000090  loss: 0.1847 (0.1946)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [570/689]  eta: 0:03:07  lr: 0.000090  loss: 0.1847 (0.1945)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [580/689]  eta: 0:02:51  lr: 0.000090  loss: 0.1798 (0.1942)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [590/689]  eta: 0:02:36  lr: 0.000090  loss: 0.1873 (0.1942)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [600/689]  eta: 0:02:20  lr: 0.000090  loss: 0.1909 (0.1942)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [610/689]  eta: 0:02:04  lr: 0.000090  loss: 0.1915 (0.1944)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [620/689]  eta: 0:01:48  lr: 0.000090  loss: 0.2006 (0.1946)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [630/689]  eta: 0:01:33  lr: 0.000090  loss: 0.1954 (0.1945)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [640/689]  eta: 0:01:17  lr: 0.000090  loss: 0.1856 (0.1947)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [650/689]  eta: 0:01:01  lr: 0.000090  loss: 0.1902 (0.1947)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [660/689]  eta: 0:00:45  lr: 0.000090  loss: 0.1888 (0.1947)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [670/689]  eta: 0:00:29  lr: 0.000090  loss: 0.1871 (0.1947)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [680/689]  eta: 0:00:14  lr: 0.000090  loss: 0.1967 (0.1948)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191]  [688/689]  eta: 0:00:01  lr: 0.000090  loss: 0.1870 (0.1947)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:191] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.1870 (0.1947)\n",
      "Valid: [epoch:191]  [ 0/14]  eta: 0:00:14  loss: 0.1931 (0.1931)  time: 1.0136  data: 0.3422  max mem: 39763\n",
      "Valid: [epoch:191]  [13/14]  eta: 0:00:00  loss: 0.1816 (0.1843)  time: 0.1143  data: 0.0245  max mem: 39763\n",
      "Valid: [epoch:191] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.1816 (0.1843)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_191_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.184%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:192]  [  0/689]  eta: 0:11:40  lr: 0.000090  loss: 0.1889 (0.1889)  time: 1.0164  data: 0.5363  max mem: 39763\n",
      "Train: [epoch:192]  [ 10/689]  eta: 0:17:15  lr: 0.000090  loss: 0.1997 (0.2044)  time: 1.5254  data: 0.0488  max mem: 39763\n",
      "Train: [epoch:192]  [ 20/689]  eta: 0:17:17  lr: 0.000090  loss: 0.1972 (0.2013)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [ 30/689]  eta: 0:17:07  lr: 0.000090  loss: 0.1983 (0.2032)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [ 40/689]  eta: 0:16:54  lr: 0.000090  loss: 0.1990 (0.2020)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [ 50/689]  eta: 0:16:40  lr: 0.000090  loss: 0.1966 (0.2025)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [ 60/689]  eta: 0:16:26  lr: 0.000090  loss: 0.2025 (0.2019)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [ 70/689]  eta: 0:16:11  lr: 0.000090  loss: 0.1869 (0.2001)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [ 80/689]  eta: 0:15:56  lr: 0.000090  loss: 0.1881 (0.1993)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [ 90/689]  eta: 0:15:41  lr: 0.000090  loss: 0.1969 (0.1992)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [100/689]  eta: 0:15:25  lr: 0.000090  loss: 0.1949 (0.1999)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [110/689]  eta: 0:15:10  lr: 0.000090  loss: 0.1944 (0.1994)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [120/689]  eta: 0:14:54  lr: 0.000090  loss: 0.2005 (0.2005)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [130/689]  eta: 0:14:39  lr: 0.000090  loss: 0.1915 (0.1997)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [140/689]  eta: 0:14:23  lr: 0.000090  loss: 0.1914 (0.1995)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [150/689]  eta: 0:14:08  lr: 0.000090  loss: 0.1923 (0.1989)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [160/689]  eta: 0:13:52  lr: 0.000090  loss: 0.1923 (0.1996)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [170/689]  eta: 0:13:37  lr: 0.000090  loss: 0.1887 (0.1992)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [180/689]  eta: 0:13:21  lr: 0.000090  loss: 0.1962 (0.1993)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [190/689]  eta: 0:13:05  lr: 0.000090  loss: 0.2023 (0.1994)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [200/689]  eta: 0:12:50  lr: 0.000090  loss: 0.1918 (0.1988)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [210/689]  eta: 0:12:34  lr: 0.000090  loss: 0.1826 (0.1985)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [220/689]  eta: 0:12:18  lr: 0.000090  loss: 0.1804 (0.1979)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [230/689]  eta: 0:12:03  lr: 0.000090  loss: 0.1855 (0.1977)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [240/689]  eta: 0:11:47  lr: 0.000090  loss: 0.1909 (0.1976)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [250/689]  eta: 0:11:31  lr: 0.000090  loss: 0.1885 (0.1972)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [260/689]  eta: 0:11:15  lr: 0.000090  loss: 0.1864 (0.1975)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [270/689]  eta: 0:11:00  lr: 0.000090  loss: 0.1929 (0.1973)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [280/689]  eta: 0:10:44  lr: 0.000090  loss: 0.1929 (0.1973)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [290/689]  eta: 0:10:28  lr: 0.000090  loss: 0.1933 (0.1973)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [300/689]  eta: 0:10:13  lr: 0.000090  loss: 0.1953 (0.1978)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [310/689]  eta: 0:09:57  lr: 0.000090  loss: 0.1939 (0.1974)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [320/689]  eta: 0:09:41  lr: 0.000090  loss: 0.1806 (0.1972)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [330/689]  eta: 0:09:25  lr: 0.000090  loss: 0.1984 (0.1975)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [340/689]  eta: 0:09:10  lr: 0.000090  loss: 0.1986 (0.1974)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [350/689]  eta: 0:08:54  lr: 0.000090  loss: 0.1859 (0.1972)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [360/689]  eta: 0:08:38  lr: 0.000090  loss: 0.1874 (0.1971)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [370/689]  eta: 0:08:22  lr: 0.000090  loss: 0.1816 (0.1968)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [380/689]  eta: 0:08:07  lr: 0.000090  loss: 0.1835 (0.1972)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [390/689]  eta: 0:07:51  lr: 0.000090  loss: 0.1985 (0.1971)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [400/689]  eta: 0:07:35  lr: 0.000090  loss: 0.1938 (0.1973)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [410/689]  eta: 0:07:19  lr: 0.000090  loss: 0.1938 (0.1974)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [420/689]  eta: 0:07:04  lr: 0.000090  loss: 0.1929 (0.1973)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [430/689]  eta: 0:06:48  lr: 0.000090  loss: 0.1929 (0.1976)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [440/689]  eta: 0:06:32  lr: 0.000090  loss: 0.1917 (0.1974)  time: 1.5779  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:192]  [450/689]  eta: 0:06:16  lr: 0.000090  loss: 0.1883 (0.1972)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [460/689]  eta: 0:06:01  lr: 0.000090  loss: 0.1887 (0.1971)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [470/689]  eta: 0:05:45  lr: 0.000090  loss: 0.1946 (0.1972)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [480/689]  eta: 0:05:29  lr: 0.000090  loss: 0.2017 (0.1975)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [490/689]  eta: 0:05:13  lr: 0.000090  loss: 0.2017 (0.1977)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [500/689]  eta: 0:04:57  lr: 0.000090  loss: 0.1882 (0.1976)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [510/689]  eta: 0:04:42  lr: 0.000090  loss: 0.1891 (0.1975)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [520/689]  eta: 0:04:26  lr: 0.000090  loss: 0.1949 (0.1978)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [530/689]  eta: 0:04:10  lr: 0.000090  loss: 0.1875 (0.1976)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [540/689]  eta: 0:03:54  lr: 0.000090  loss: 0.1851 (0.1974)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [550/689]  eta: 0:03:39  lr: 0.000090  loss: 0.1927 (0.1975)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [560/689]  eta: 0:03:23  lr: 0.000090  loss: 0.1965 (0.1975)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [570/689]  eta: 0:03:07  lr: 0.000090  loss: 0.1886 (0.1973)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [580/689]  eta: 0:02:51  lr: 0.000090  loss: 0.1825 (0.1972)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [590/689]  eta: 0:02:36  lr: 0.000090  loss: 0.1906 (0.1972)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [600/689]  eta: 0:02:20  lr: 0.000090  loss: 0.1855 (0.1970)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [610/689]  eta: 0:02:04  lr: 0.000090  loss: 0.1802 (0.1969)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [620/689]  eta: 0:01:48  lr: 0.000090  loss: 0.1873 (0.1969)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [630/689]  eta: 0:01:33  lr: 0.000090  loss: 0.1879 (0.1968)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [640/689]  eta: 0:01:17  lr: 0.000090  loss: 0.1831 (0.1967)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [650/689]  eta: 0:01:01  lr: 0.000090  loss: 0.1850 (0.1967)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [660/689]  eta: 0:00:45  lr: 0.000090  loss: 0.1857 (0.1965)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [670/689]  eta: 0:00:29  lr: 0.000090  loss: 0.1844 (0.1965)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [680/689]  eta: 0:00:14  lr: 0.000090  loss: 0.1844 (0.1964)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192]  [688/689]  eta: 0:00:01  lr: 0.000090  loss: 0.1858 (0.1965)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:192] Total time: 0:18:06 (1.5773 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.1858 (0.1965)\n",
      "Valid: [epoch:192]  [ 0/14]  eta: 0:00:14  loss: 0.1785 (0.1785)  time: 1.0173  data: 0.3850  max mem: 39763\n",
      "Valid: [epoch:192]  [13/14]  eta: 0:00:00  loss: 0.1832 (0.1858)  time: 0.1146  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:192] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.1832 (0.1858)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_192_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.186%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:193]  [  0/689]  eta: 0:11:39  lr: 0.000090  loss: 0.1540 (0.1540)  time: 1.0145  data: 0.5382  max mem: 39763\n",
      "Train: [epoch:193]  [ 10/689]  eta: 0:17:15  lr: 0.000090  loss: 0.1791 (0.1799)  time: 1.5244  data: 0.0490  max mem: 39763\n",
      "Train: [epoch:193]  [ 20/689]  eta: 0:17:16  lr: 0.000090  loss: 0.1852 (0.1845)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [ 30/689]  eta: 0:17:07  lr: 0.000090  loss: 0.1957 (0.1941)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [ 40/689]  eta: 0:16:54  lr: 0.000090  loss: 0.2088 (0.1990)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [ 50/689]  eta: 0:16:40  lr: 0.000090  loss: 0.1955 (0.1965)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [ 60/689]  eta: 0:16:26  lr: 0.000090  loss: 0.1838 (0.1955)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [ 70/689]  eta: 0:16:11  lr: 0.000090  loss: 0.1841 (0.1963)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [ 80/689]  eta: 0:15:56  lr: 0.000090  loss: 0.1871 (0.1968)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [ 90/689]  eta: 0:15:41  lr: 0.000090  loss: 0.1873 (0.1966)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [100/689]  eta: 0:15:25  lr: 0.000090  loss: 0.1843 (0.1962)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [110/689]  eta: 0:15:10  lr: 0.000090  loss: 0.1967 (0.1983)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [120/689]  eta: 0:14:54  lr: 0.000090  loss: 0.2006 (0.1982)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [130/689]  eta: 0:14:39  lr: 0.000090  loss: 0.1912 (0.1988)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [140/689]  eta: 0:14:23  lr: 0.000090  loss: 0.1949 (0.1990)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [150/689]  eta: 0:14:08  lr: 0.000090  loss: 0.1907 (0.1988)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [160/689]  eta: 0:13:52  lr: 0.000090  loss: 0.1894 (0.1986)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [170/689]  eta: 0:13:37  lr: 0.000090  loss: 0.1903 (0.1983)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [180/689]  eta: 0:13:21  lr: 0.000090  loss: 0.1805 (0.1973)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [190/689]  eta: 0:13:05  lr: 0.000090  loss: 0.1872 (0.1977)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [200/689]  eta: 0:12:50  lr: 0.000090  loss: 0.1937 (0.1973)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [210/689]  eta: 0:12:34  lr: 0.000090  loss: 0.1826 (0.1971)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [220/689]  eta: 0:12:18  lr: 0.000090  loss: 0.1840 (0.1967)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [230/689]  eta: 0:12:03  lr: 0.000090  loss: 0.1951 (0.1972)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [240/689]  eta: 0:11:47  lr: 0.000090  loss: 0.1973 (0.1971)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [250/689]  eta: 0:11:31  lr: 0.000090  loss: 0.1879 (0.1967)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [260/689]  eta: 0:11:16  lr: 0.000090  loss: 0.1921 (0.1971)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [270/689]  eta: 0:11:00  lr: 0.000090  loss: 0.1949 (0.1970)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [280/689]  eta: 0:10:44  lr: 0.000090  loss: 0.1893 (0.1974)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [290/689]  eta: 0:10:28  lr: 0.000090  loss: 0.1851 (0.1968)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [300/689]  eta: 0:10:13  lr: 0.000090  loss: 0.1800 (0.1966)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [310/689]  eta: 0:09:57  lr: 0.000090  loss: 0.1900 (0.1966)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [320/689]  eta: 0:09:41  lr: 0.000090  loss: 0.2095 (0.1972)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [330/689]  eta: 0:09:25  lr: 0.000090  loss: 0.1989 (0.1970)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [340/689]  eta: 0:09:10  lr: 0.000090  loss: 0.1832 (0.1967)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [350/689]  eta: 0:08:54  lr: 0.000090  loss: 0.1938 (0.1969)  time: 1.5782  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:193]  [360/689]  eta: 0:08:38  lr: 0.000090  loss: 0.1938 (0.1967)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [370/689]  eta: 0:08:22  lr: 0.000090  loss: 0.1849 (0.1966)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [380/689]  eta: 0:08:07  lr: 0.000090  loss: 0.1848 (0.1963)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [390/689]  eta: 0:07:51  lr: 0.000090  loss: 0.1848 (0.1964)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [400/689]  eta: 0:07:35  lr: 0.000090  loss: 0.2044 (0.1968)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [410/689]  eta: 0:07:19  lr: 0.000090  loss: 0.1953 (0.1968)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [420/689]  eta: 0:07:04  lr: 0.000090  loss: 0.1874 (0.1967)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [430/689]  eta: 0:06:48  lr: 0.000090  loss: 0.1874 (0.1968)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [440/689]  eta: 0:06:32  lr: 0.000090  loss: 0.1941 (0.1970)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [450/689]  eta: 0:06:16  lr: 0.000090  loss: 0.2022 (0.1972)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [460/689]  eta: 0:06:01  lr: 0.000090  loss: 0.1873 (0.1971)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [470/689]  eta: 0:05:45  lr: 0.000090  loss: 0.1831 (0.1970)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [480/689]  eta: 0:05:29  lr: 0.000090  loss: 0.1829 (0.1968)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [490/689]  eta: 0:05:13  lr: 0.000090  loss: 0.1896 (0.1969)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [500/689]  eta: 0:04:58  lr: 0.000090  loss: 0.1931 (0.1967)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [510/689]  eta: 0:04:42  lr: 0.000090  loss: 0.1892 (0.1967)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [520/689]  eta: 0:04:26  lr: 0.000090  loss: 0.1869 (0.1966)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [530/689]  eta: 0:04:10  lr: 0.000090  loss: 0.1837 (0.1966)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [540/689]  eta: 0:03:54  lr: 0.000090  loss: 0.1923 (0.1966)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [550/689]  eta: 0:03:39  lr: 0.000090  loss: 0.1941 (0.1966)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [560/689]  eta: 0:03:23  lr: 0.000090  loss: 0.1881 (0.1966)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [570/689]  eta: 0:03:07  lr: 0.000090  loss: 0.1944 (0.1968)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [580/689]  eta: 0:02:51  lr: 0.000090  loss: 0.1990 (0.1968)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [590/689]  eta: 0:02:36  lr: 0.000090  loss: 0.1990 (0.1969)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [600/689]  eta: 0:02:20  lr: 0.000090  loss: 0.1885 (0.1969)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [610/689]  eta: 0:02:04  lr: 0.000090  loss: 0.1880 (0.1969)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [620/689]  eta: 0:01:48  lr: 0.000090  loss: 0.1958 (0.1971)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [630/689]  eta: 0:01:33  lr: 0.000090  loss: 0.1957 (0.1971)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [640/689]  eta: 0:01:17  lr: 0.000090  loss: 0.1875 (0.1970)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [650/689]  eta: 0:01:01  lr: 0.000090  loss: 0.1905 (0.1969)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [660/689]  eta: 0:00:45  lr: 0.000090  loss: 0.2030 (0.1972)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [670/689]  eta: 0:00:29  lr: 0.000090  loss: 0.2101 (0.1975)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [680/689]  eta: 0:00:14  lr: 0.000090  loss: 0.1906 (0.1974)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193]  [688/689]  eta: 0:00:01  lr: 0.000090  loss: 0.1906 (0.1975)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:193] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.1906 (0.1975)\n",
      "Valid: [epoch:193]  [ 0/14]  eta: 0:00:14  loss: 0.1970 (0.1970)  time: 1.0225  data: 0.4307  max mem: 39763\n",
      "Valid: [epoch:193]  [13/14]  eta: 0:00:00  loss: 0.1842 (0.1875)  time: 0.1150  data: 0.0308  max mem: 39763\n",
      "Valid: [epoch:193] Total time: 0:00:01 (0.1245 s / it)\n",
      "Averaged stats: loss: 0.1842 (0.1875)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_193_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.188%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:194]  [  0/689]  eta: 0:11:29  lr: 0.000090  loss: 0.1933 (0.1933)  time: 1.0009  data: 0.5210  max mem: 39763\n",
      "Train: [epoch:194]  [ 10/689]  eta: 0:17:15  lr: 0.000090  loss: 0.1902 (0.1944)  time: 1.5254  data: 0.0474  max mem: 39763\n",
      "Train: [epoch:194]  [ 20/689]  eta: 0:17:17  lr: 0.000090  loss: 0.1822 (0.1938)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [ 30/689]  eta: 0:17:07  lr: 0.000090  loss: 0.1895 (0.1952)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [ 40/689]  eta: 0:16:55  lr: 0.000090  loss: 0.1976 (0.1971)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [ 50/689]  eta: 0:16:41  lr: 0.000090  loss: 0.2039 (0.1998)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [ 60/689]  eta: 0:16:26  lr: 0.000090  loss: 0.1981 (0.1990)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [ 70/689]  eta: 0:16:12  lr: 0.000090  loss: 0.1949 (0.1991)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [ 80/689]  eta: 0:15:56  lr: 0.000090  loss: 0.1901 (0.1972)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [ 90/689]  eta: 0:15:41  lr: 0.000090  loss: 0.1862 (0.1986)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [100/689]  eta: 0:15:26  lr: 0.000090  loss: 0.1930 (0.1975)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [110/689]  eta: 0:15:10  lr: 0.000090  loss: 0.1857 (0.1973)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [120/689]  eta: 0:14:55  lr: 0.000090  loss: 0.1855 (0.1967)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [130/689]  eta: 0:14:39  lr: 0.000090  loss: 0.1913 (0.1970)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [140/689]  eta: 0:14:24  lr: 0.000090  loss: 0.2045 (0.1984)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [150/689]  eta: 0:14:08  lr: 0.000090  loss: 0.2121 (0.1991)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [160/689]  eta: 0:13:53  lr: 0.000090  loss: 0.1975 (0.1989)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [170/689]  eta: 0:13:37  lr: 0.000090  loss: 0.1862 (0.1985)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [180/689]  eta: 0:13:21  lr: 0.000090  loss: 0.1872 (0.1986)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [190/689]  eta: 0:13:06  lr: 0.000090  loss: 0.1888 (0.1985)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [200/689]  eta: 0:12:50  lr: 0.000090  loss: 0.1923 (0.1988)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [210/689]  eta: 0:12:34  lr: 0.000090  loss: 0.1955 (0.1984)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [220/689]  eta: 0:12:19  lr: 0.000090  loss: 0.1927 (0.1980)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [230/689]  eta: 0:12:03  lr: 0.000090  loss: 0.1979 (0.1986)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [240/689]  eta: 0:11:47  lr: 0.000090  loss: 0.2140 (0.1991)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [250/689]  eta: 0:11:31  lr: 0.000090  loss: 0.1898 (0.1986)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [260/689]  eta: 0:11:16  lr: 0.000090  loss: 0.1869 (0.1985)  time: 1.5792  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:194]  [270/689]  eta: 0:11:00  lr: 0.000090  loss: 0.1869 (0.1983)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [280/689]  eta: 0:10:44  lr: 0.000090  loss: 0.1908 (0.1987)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [290/689]  eta: 0:10:29  lr: 0.000090  loss: 0.1919 (0.1982)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [300/689]  eta: 0:10:13  lr: 0.000090  loss: 0.1869 (0.1982)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [310/689]  eta: 0:09:57  lr: 0.000090  loss: 0.1847 (0.1977)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [320/689]  eta: 0:09:41  lr: 0.000090  loss: 0.1893 (0.1981)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [330/689]  eta: 0:09:26  lr: 0.000090  loss: 0.2062 (0.1983)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [340/689]  eta: 0:09:10  lr: 0.000090  loss: 0.2047 (0.1982)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [350/689]  eta: 0:08:54  lr: 0.000090  loss: 0.1956 (0.1984)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [360/689]  eta: 0:08:38  lr: 0.000090  loss: 0.1947 (0.1986)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [370/689]  eta: 0:08:23  lr: 0.000090  loss: 0.1947 (0.1988)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [380/689]  eta: 0:08:07  lr: 0.000090  loss: 0.1913 (0.1987)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [390/689]  eta: 0:07:51  lr: 0.000090  loss: 0.1882 (0.1987)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [400/689]  eta: 0:07:35  lr: 0.000090  loss: 0.1966 (0.1987)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [410/689]  eta: 0:07:19  lr: 0.000090  loss: 0.1992 (0.1987)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [420/689]  eta: 0:07:04  lr: 0.000090  loss: 0.1944 (0.1986)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [430/689]  eta: 0:06:48  lr: 0.000090  loss: 0.1927 (0.1986)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [440/689]  eta: 0:06:32  lr: 0.000090  loss: 0.1927 (0.1985)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [450/689]  eta: 0:06:16  lr: 0.000090  loss: 0.2001 (0.1986)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [460/689]  eta: 0:06:01  lr: 0.000090  loss: 0.2046 (0.1986)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [470/689]  eta: 0:05:45  lr: 0.000090  loss: 0.1930 (0.1986)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [480/689]  eta: 0:05:29  lr: 0.000090  loss: 0.1963 (0.1987)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [490/689]  eta: 0:05:13  lr: 0.000090  loss: 0.1914 (0.1985)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [500/689]  eta: 0:04:58  lr: 0.000090  loss: 0.1932 (0.1986)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [510/689]  eta: 0:04:42  lr: 0.000090  loss: 0.1969 (0.1986)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [520/689]  eta: 0:04:26  lr: 0.000090  loss: 0.1895 (0.1986)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [530/689]  eta: 0:04:10  lr: 0.000090  loss: 0.1871 (0.1985)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [540/689]  eta: 0:03:55  lr: 0.000090  loss: 0.1853 (0.1984)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [550/689]  eta: 0:03:39  lr: 0.000090  loss: 0.1891 (0.1985)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [560/689]  eta: 0:03:23  lr: 0.000090  loss: 0.1891 (0.1985)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [570/689]  eta: 0:03:07  lr: 0.000090  loss: 0.1933 (0.1986)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [580/689]  eta: 0:02:51  lr: 0.000090  loss: 0.1933 (0.1986)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [590/689]  eta: 0:02:36  lr: 0.000090  loss: 0.1923 (0.1984)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [600/689]  eta: 0:02:20  lr: 0.000090  loss: 0.1905 (0.1985)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [610/689]  eta: 0:02:04  lr: 0.000090  loss: 0.1922 (0.1985)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [620/689]  eta: 0:01:48  lr: 0.000090  loss: 0.1895 (0.1984)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [630/689]  eta: 0:01:33  lr: 0.000090  loss: 0.1895 (0.1985)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [640/689]  eta: 0:01:17  lr: 0.000090  loss: 0.2026 (0.1986)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [650/689]  eta: 0:01:01  lr: 0.000090  loss: 0.2026 (0.1986)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [660/689]  eta: 0:00:45  lr: 0.000090  loss: 0.2067 (0.1987)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [670/689]  eta: 0:00:29  lr: 0.000090  loss: 0.1978 (0.1988)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [680/689]  eta: 0:00:14  lr: 0.000090  loss: 0.1966 (0.1988)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194]  [688/689]  eta: 0:00:01  lr: 0.000090  loss: 0.1986 (0.1987)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:194] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.1986 (0.1987)\n",
      "Valid: [epoch:194]  [ 0/14]  eta: 0:00:14  loss: 0.1804 (0.1804)  time: 1.0158  data: 0.3733  max mem: 39763\n",
      "Valid: [epoch:194]  [13/14]  eta: 0:00:00  loss: 0.1855 (0.1880)  time: 0.1145  data: 0.0267  max mem: 39763\n",
      "Valid: [epoch:194] Total time: 0:00:01 (0.1223 s / it)\n",
      "Averaged stats: loss: 0.1855 (0.1880)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_194_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.188%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:195]  [  0/689]  eta: 0:11:56  lr: 0.000090  loss: 0.1784 (0.1784)  time: 1.0405  data: 0.5618  max mem: 39763\n",
      "Train: [epoch:195]  [ 10/689]  eta: 0:17:16  lr: 0.000090  loss: 0.1899 (0.2045)  time: 1.5266  data: 0.0511  max mem: 39763\n",
      "Train: [epoch:195]  [ 20/689]  eta: 0:17:17  lr: 0.000090  loss: 0.1898 (0.2017)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [ 30/689]  eta: 0:17:07  lr: 0.000090  loss: 0.1896 (0.2013)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [ 40/689]  eta: 0:16:54  lr: 0.000090  loss: 0.1963 (0.2003)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [ 50/689]  eta: 0:16:40  lr: 0.000090  loss: 0.1873 (0.1986)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [ 60/689]  eta: 0:16:26  lr: 0.000090  loss: 0.1934 (0.1988)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [ 70/689]  eta: 0:16:11  lr: 0.000090  loss: 0.1942 (0.1985)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [ 80/689]  eta: 0:15:56  lr: 0.000090  loss: 0.1971 (0.1993)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [ 90/689]  eta: 0:15:41  lr: 0.000090  loss: 0.1972 (0.1987)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [100/689]  eta: 0:15:26  lr: 0.000090  loss: 0.1913 (0.1993)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [110/689]  eta: 0:15:10  lr: 0.000090  loss: 0.2071 (0.2001)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [120/689]  eta: 0:14:55  lr: 0.000090  loss: 0.2026 (0.2004)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [130/689]  eta: 0:14:39  lr: 0.000090  loss: 0.1943 (0.1996)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [140/689]  eta: 0:14:24  lr: 0.000090  loss: 0.1916 (0.1993)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [150/689]  eta: 0:14:08  lr: 0.000090  loss: 0.1924 (0.1998)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [160/689]  eta: 0:13:52  lr: 0.000090  loss: 0.1851 (0.1993)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [170/689]  eta: 0:13:37  lr: 0.000090  loss: 0.1851 (0.1989)  time: 1.5782  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:195]  [180/689]  eta: 0:13:21  lr: 0.000090  loss: 0.1927 (0.1991)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [190/689]  eta: 0:13:05  lr: 0.000090  loss: 0.1927 (0.1992)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [200/689]  eta: 0:12:50  lr: 0.000090  loss: 0.1897 (0.1990)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [210/689]  eta: 0:12:34  lr: 0.000090  loss: 0.1911 (0.1989)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [220/689]  eta: 0:12:18  lr: 0.000090  loss: 0.1944 (0.1990)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [230/689]  eta: 0:12:03  lr: 0.000090  loss: 0.1944 (0.1989)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [240/689]  eta: 0:11:47  lr: 0.000090  loss: 0.1995 (0.1994)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [250/689]  eta: 0:11:31  lr: 0.000090  loss: 0.1935 (0.1990)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [260/689]  eta: 0:11:16  lr: 0.000090  loss: 0.1920 (0.1991)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [270/689]  eta: 0:11:00  lr: 0.000090  loss: 0.2006 (0.1994)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [280/689]  eta: 0:10:44  lr: 0.000090  loss: 0.2000 (0.1993)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [290/689]  eta: 0:10:28  lr: 0.000090  loss: 0.2018 (0.1993)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [300/689]  eta: 0:10:13  lr: 0.000090  loss: 0.1970 (0.1991)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [310/689]  eta: 0:09:57  lr: 0.000090  loss: 0.1906 (0.1991)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [320/689]  eta: 0:09:41  lr: 0.000090  loss: 0.2001 (0.1993)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [330/689]  eta: 0:09:25  lr: 0.000090  loss: 0.2058 (0.1997)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [340/689]  eta: 0:09:10  lr: 0.000090  loss: 0.1926 (0.1998)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [350/689]  eta: 0:08:54  lr: 0.000090  loss: 0.1903 (0.1997)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [360/689]  eta: 0:08:38  lr: 0.000090  loss: 0.1915 (0.1997)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [370/689]  eta: 0:08:22  lr: 0.000090  loss: 0.1854 (0.1992)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [380/689]  eta: 0:08:07  lr: 0.000090  loss: 0.1903 (0.1996)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [390/689]  eta: 0:07:51  lr: 0.000090  loss: 0.2007 (0.1997)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [400/689]  eta: 0:07:35  lr: 0.000090  loss: 0.2003 (0.1997)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [410/689]  eta: 0:07:19  lr: 0.000090  loss: 0.2079 (0.1998)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [420/689]  eta: 0:07:04  lr: 0.000090  loss: 0.1977 (0.1997)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [430/689]  eta: 0:06:48  lr: 0.000090  loss: 0.1924 (0.1998)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [440/689]  eta: 0:06:32  lr: 0.000090  loss: 0.1860 (0.1995)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [450/689]  eta: 0:06:16  lr: 0.000090  loss: 0.1932 (0.1995)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [460/689]  eta: 0:06:01  lr: 0.000090  loss: 0.1859 (0.1992)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [470/689]  eta: 0:05:45  lr: 0.000090  loss: 0.1875 (0.1993)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [480/689]  eta: 0:05:29  lr: 0.000090  loss: 0.1960 (0.1992)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [490/689]  eta: 0:05:13  lr: 0.000090  loss: 0.1960 (0.1993)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [500/689]  eta: 0:04:58  lr: 0.000090  loss: 0.2002 (0.1995)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [510/689]  eta: 0:04:42  lr: 0.000090  loss: 0.2009 (0.1995)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [520/689]  eta: 0:04:26  lr: 0.000090  loss: 0.2009 (0.1996)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [530/689]  eta: 0:04:10  lr: 0.000090  loss: 0.1905 (0.1994)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [540/689]  eta: 0:03:54  lr: 0.000090  loss: 0.1854 (0.1993)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [550/689]  eta: 0:03:39  lr: 0.000090  loss: 0.1861 (0.1994)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [560/689]  eta: 0:03:23  lr: 0.000090  loss: 0.1934 (0.1994)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [570/689]  eta: 0:03:07  lr: 0.000090  loss: 0.1888 (0.1992)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [580/689]  eta: 0:02:51  lr: 0.000090  loss: 0.1949 (0.1993)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [590/689]  eta: 0:02:36  lr: 0.000090  loss: 0.1954 (0.1993)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [600/689]  eta: 0:02:20  lr: 0.000090  loss: 0.1919 (0.1992)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [610/689]  eta: 0:02:04  lr: 0.000090  loss: 0.1896 (0.1990)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [620/689]  eta: 0:01:48  lr: 0.000090  loss: 0.1896 (0.1991)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [630/689]  eta: 0:01:33  lr: 0.000090  loss: 0.1966 (0.1990)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [640/689]  eta: 0:01:17  lr: 0.000090  loss: 0.1994 (0.1992)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [650/689]  eta: 0:01:01  lr: 0.000090  loss: 0.1923 (0.1991)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [660/689]  eta: 0:00:45  lr: 0.000090  loss: 0.1963 (0.1991)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [670/689]  eta: 0:00:29  lr: 0.000090  loss: 0.1966 (0.1990)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [680/689]  eta: 0:00:14  lr: 0.000090  loss: 0.1887 (0.1989)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195]  [688/689]  eta: 0:00:01  lr: 0.000090  loss: 0.1962 (0.1989)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:195] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000090  loss: 0.1962 (0.1989)\n",
      "Valid: [epoch:195]  [ 0/14]  eta: 0:00:14  loss: 0.1987 (0.1987)  time: 1.0118  data: 0.3806  max mem: 39763\n",
      "Valid: [epoch:195]  [13/14]  eta: 0:00:00  loss: 0.1884 (0.1900)  time: 0.1143  data: 0.0272  max mem: 39763\n",
      "Valid: [epoch:195] Total time: 0:00:01 (0.1241 s / it)\n",
      "Averaged stats: loss: 0.1884 (0.1900)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_195_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.190%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:196]  [  0/689]  eta: 0:11:30  lr: 0.000089  loss: 0.1927 (0.1927)  time: 1.0024  data: 0.5230  max mem: 39763\n",
      "Train: [epoch:196]  [ 10/689]  eta: 0:17:15  lr: 0.000089  loss: 0.1927 (0.1934)  time: 1.5250  data: 0.0476  max mem: 39763\n",
      "Train: [epoch:196]  [ 20/689]  eta: 0:17:17  lr: 0.000089  loss: 0.1967 (0.1999)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [ 30/689]  eta: 0:17:07  lr: 0.000089  loss: 0.1997 (0.2014)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [ 40/689]  eta: 0:16:55  lr: 0.000089  loss: 0.1935 (0.1989)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [ 50/689]  eta: 0:16:41  lr: 0.000089  loss: 0.1917 (0.2002)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [ 60/689]  eta: 0:16:26  lr: 0.000089  loss: 0.1903 (0.1985)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [ 70/689]  eta: 0:16:11  lr: 0.000089  loss: 0.1834 (0.1969)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [ 80/689]  eta: 0:15:56  lr: 0.000089  loss: 0.1924 (0.1975)  time: 1.5777  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:196]  [ 90/689]  eta: 0:15:41  lr: 0.000089  loss: 0.1957 (0.1986)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [100/689]  eta: 0:15:26  lr: 0.000089  loss: 0.2030 (0.1989)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [110/689]  eta: 0:15:10  lr: 0.000089  loss: 0.2054 (0.2003)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [120/689]  eta: 0:14:55  lr: 0.000089  loss: 0.2020 (0.2005)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [130/689]  eta: 0:14:39  lr: 0.000089  loss: 0.1937 (0.2001)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [140/689]  eta: 0:14:24  lr: 0.000089  loss: 0.2020 (0.2006)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [150/689]  eta: 0:14:08  lr: 0.000089  loss: 0.2055 (0.2008)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [160/689]  eta: 0:13:52  lr: 0.000089  loss: 0.1963 (0.2007)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [170/689]  eta: 0:13:37  lr: 0.000089  loss: 0.1994 (0.2011)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [180/689]  eta: 0:13:21  lr: 0.000089  loss: 0.1919 (0.2006)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [190/689]  eta: 0:13:05  lr: 0.000089  loss: 0.1919 (0.2006)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [200/689]  eta: 0:12:50  lr: 0.000089  loss: 0.1945 (0.2006)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [210/689]  eta: 0:12:34  lr: 0.000089  loss: 0.1947 (0.2004)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [220/689]  eta: 0:12:18  lr: 0.000089  loss: 0.1906 (0.2004)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [230/689]  eta: 0:12:03  lr: 0.000089  loss: 0.1906 (0.2005)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [240/689]  eta: 0:11:47  lr: 0.000089  loss: 0.1949 (0.2006)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [250/689]  eta: 0:11:31  lr: 0.000089  loss: 0.1991 (0.2006)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [260/689]  eta: 0:11:16  lr: 0.000089  loss: 0.1964 (0.2007)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [270/689]  eta: 0:11:00  lr: 0.000089  loss: 0.1946 (0.2003)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [280/689]  eta: 0:10:44  lr: 0.000089  loss: 0.1973 (0.2005)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [290/689]  eta: 0:10:28  lr: 0.000089  loss: 0.2020 (0.2007)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [300/689]  eta: 0:10:13  lr: 0.000089  loss: 0.1938 (0.2006)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [310/689]  eta: 0:09:57  lr: 0.000089  loss: 0.1845 (0.2003)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [320/689]  eta: 0:09:41  lr: 0.000089  loss: 0.1977 (0.2005)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [330/689]  eta: 0:09:26  lr: 0.000089  loss: 0.1934 (0.2006)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [340/689]  eta: 0:09:10  lr: 0.000089  loss: 0.1922 (0.2006)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [350/689]  eta: 0:08:54  lr: 0.000089  loss: 0.1909 (0.2004)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [360/689]  eta: 0:08:38  lr: 0.000089  loss: 0.1906 (0.2000)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [370/689]  eta: 0:08:23  lr: 0.000089  loss: 0.1828 (0.1997)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [380/689]  eta: 0:08:07  lr: 0.000089  loss: 0.1832 (0.1996)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [390/689]  eta: 0:07:51  lr: 0.000089  loss: 0.1994 (0.1998)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [400/689]  eta: 0:07:35  lr: 0.000089  loss: 0.1994 (0.2001)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [410/689]  eta: 0:07:19  lr: 0.000089  loss: 0.1932 (0.2002)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [420/689]  eta: 0:07:04  lr: 0.000089  loss: 0.1903 (0.2000)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [430/689]  eta: 0:06:48  lr: 0.000089  loss: 0.1932 (0.1999)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [440/689]  eta: 0:06:32  lr: 0.000089  loss: 0.1919 (0.1998)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [450/689]  eta: 0:06:16  lr: 0.000089  loss: 0.1871 (0.1996)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [460/689]  eta: 0:06:01  lr: 0.000089  loss: 0.1963 (0.1999)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [470/689]  eta: 0:05:45  lr: 0.000089  loss: 0.1956 (0.1998)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [480/689]  eta: 0:05:29  lr: 0.000089  loss: 0.1829 (0.1996)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [490/689]  eta: 0:05:13  lr: 0.000089  loss: 0.1895 (0.1996)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [500/689]  eta: 0:04:58  lr: 0.000089  loss: 0.2011 (0.2000)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [510/689]  eta: 0:04:42  lr: 0.000089  loss: 0.1945 (0.1998)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [520/689]  eta: 0:04:26  lr: 0.000089  loss: 0.1901 (0.1997)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [530/689]  eta: 0:04:10  lr: 0.000089  loss: 0.1931 (0.1996)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [540/689]  eta: 0:03:55  lr: 0.000089  loss: 0.1976 (0.1998)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [550/689]  eta: 0:03:39  lr: 0.000089  loss: 0.1964 (0.1997)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [560/689]  eta: 0:03:23  lr: 0.000089  loss: 0.2056 (0.2001)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [570/689]  eta: 0:03:07  lr: 0.000089  loss: 0.2068 (0.2001)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [580/689]  eta: 0:02:51  lr: 0.000089  loss: 0.1970 (0.2001)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [590/689]  eta: 0:02:36  lr: 0.000089  loss: 0.1966 (0.2000)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [600/689]  eta: 0:02:20  lr: 0.000089  loss: 0.1981 (0.2002)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [610/689]  eta: 0:02:04  lr: 0.000089  loss: 0.2045 (0.2004)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [620/689]  eta: 0:01:48  lr: 0.000089  loss: 0.1921 (0.2003)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [630/689]  eta: 0:01:33  lr: 0.000089  loss: 0.1877 (0.2003)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [640/689]  eta: 0:01:17  lr: 0.000089  loss: 0.1926 (0.2003)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [650/689]  eta: 0:01:01  lr: 0.000089  loss: 0.1892 (0.2002)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [660/689]  eta: 0:00:45  lr: 0.000089  loss: 0.1989 (0.2003)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [670/689]  eta: 0:00:29  lr: 0.000089  loss: 0.1989 (0.2002)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [680/689]  eta: 0:00:14  lr: 0.000089  loss: 0.1880 (0.2003)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196]  [688/689]  eta: 0:00:01  lr: 0.000089  loss: 0.1910 (0.2003)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:196] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: 0.1910 (0.2003)\n",
      "Valid: [epoch:196]  [ 0/14]  eta: 0:00:14  loss: 0.2052 (0.2052)  time: 1.0153  data: 0.4383  max mem: 39763\n",
      "Valid: [epoch:196]  [13/14]  eta: 0:00:00  loss: 0.1861 (0.1879)  time: 0.1144  data: 0.0314  max mem: 39763\n",
      "Valid: [epoch:196] Total time: 0:00:01 (0.1224 s / it)\n",
      "Averaged stats: loss: 0.1861 (0.1879)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_196_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.188%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:197]  [  0/689]  eta: 0:12:41  lr: 0.000089  loss: 0.2237 (0.2237)  time: 1.1048  data: 0.6289  max mem: 39763\n",
      "Train: [epoch:197]  [ 10/689]  eta: 0:17:20  lr: 0.000089  loss: 0.1856 (0.1943)  time: 1.5318  data: 0.0572  max mem: 39763\n",
      "Train: [epoch:197]  [ 20/689]  eta: 0:17:18  lr: 0.000089  loss: 0.1849 (0.1933)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [ 30/689]  eta: 0:17:08  lr: 0.000089  loss: 0.1955 (0.1987)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [ 40/689]  eta: 0:16:55  lr: 0.000089  loss: 0.2056 (0.1996)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [ 50/689]  eta: 0:16:41  lr: 0.000089  loss: 0.1932 (0.2003)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [ 60/689]  eta: 0:16:26  lr: 0.000089  loss: 0.1950 (0.2017)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [ 70/689]  eta: 0:16:11  lr: 0.000089  loss: 0.1895 (0.2009)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [ 80/689]  eta: 0:15:56  lr: 0.000089  loss: 0.1888 (0.2011)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [ 90/689]  eta: 0:15:41  lr: 0.000089  loss: 0.2040 (0.2011)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [100/689]  eta: 0:15:25  lr: 0.000089  loss: 0.1877 (0.2007)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [110/689]  eta: 0:15:10  lr: 0.000089  loss: 0.1927 (0.2014)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [120/689]  eta: 0:14:55  lr: 0.000089  loss: 0.1989 (0.2021)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [130/689]  eta: 0:14:39  lr: 0.000089  loss: 0.1966 (0.2015)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [140/689]  eta: 0:14:23  lr: 0.000089  loss: 0.1957 (0.2014)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [150/689]  eta: 0:14:08  lr: 0.000089  loss: 0.1987 (0.2018)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [160/689]  eta: 0:13:52  lr: 0.000089  loss: 0.2039 (0.2022)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [170/689]  eta: 0:13:37  lr: 0.000089  loss: 0.2017 (0.2021)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [180/689]  eta: 0:13:21  lr: 0.000089  loss: 0.1988 (0.2023)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [190/689]  eta: 0:13:05  lr: 0.000089  loss: 0.1936 (0.2021)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [200/689]  eta: 0:12:50  lr: 0.000089  loss: 0.1905 (0.2017)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [210/689]  eta: 0:12:34  lr: 0.000089  loss: 0.1812 (0.2013)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [220/689]  eta: 0:12:18  lr: 0.000089  loss: 0.1890 (0.2009)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [230/689]  eta: 0:12:03  lr: 0.000089  loss: 0.1898 (0.2007)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [240/689]  eta: 0:11:47  lr: 0.000089  loss: 0.1991 (0.2009)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [250/689]  eta: 0:11:31  lr: 0.000089  loss: 0.2075 (0.2014)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [260/689]  eta: 0:11:15  lr: 0.000089  loss: 0.2065 (0.2015)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [270/689]  eta: 0:11:00  lr: 0.000089  loss: 0.1989 (0.2015)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [280/689]  eta: 0:10:44  lr: 0.000089  loss: 0.1931 (0.2013)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [290/689]  eta: 0:10:28  lr: 0.000089  loss: 0.1888 (0.2009)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [300/689]  eta: 0:10:13  lr: 0.000089  loss: 0.1813 (0.2004)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [310/689]  eta: 0:09:57  lr: 0.000089  loss: 0.1786 (0.1999)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [320/689]  eta: 0:09:41  lr: 0.000089  loss: 0.1912 (0.2001)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [330/689]  eta: 0:09:25  lr: 0.000089  loss: 0.1983 (0.2002)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [340/689]  eta: 0:09:10  lr: 0.000089  loss: 0.2073 (0.2007)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [350/689]  eta: 0:08:54  lr: 0.000089  loss: 0.2082 (0.2010)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [360/689]  eta: 0:08:38  lr: 0.000089  loss: 0.2056 (0.2011)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [370/689]  eta: 0:08:22  lr: 0.000089  loss: 0.2068 (0.2013)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [380/689]  eta: 0:08:07  lr: 0.000089  loss: 0.1917 (0.2011)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [390/689]  eta: 0:07:51  lr: 0.000089  loss: 0.1871 (0.2008)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [400/689]  eta: 0:07:35  lr: 0.000089  loss: 0.1917 (0.2009)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [410/689]  eta: 0:07:19  lr: 0.000089  loss: 0.1934 (0.2009)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [420/689]  eta: 0:07:04  lr: 0.000089  loss: 0.1920 (0.2007)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [430/689]  eta: 0:06:48  lr: 0.000089  loss: 0.1920 (0.2007)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [440/689]  eta: 0:06:32  lr: 0.000089  loss: 0.2047 (0.2010)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [450/689]  eta: 0:06:16  lr: 0.000089  loss: 0.1989 (0.2009)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [460/689]  eta: 0:06:01  lr: 0.000089  loss: 0.1983 (0.2013)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [470/689]  eta: 0:05:45  lr: 0.000089  loss: 0.2015 (0.2014)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [480/689]  eta: 0:05:29  lr: 0.000089  loss: 0.1960 (0.2013)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [490/689]  eta: 0:05:13  lr: 0.000089  loss: 0.2026 (0.2014)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [500/689]  eta: 0:04:58  lr: 0.000089  loss: 0.1919 (0.2013)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [510/689]  eta: 0:04:42  lr: 0.000089  loss: 0.1889 (0.2013)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [520/689]  eta: 0:04:26  lr: 0.000089  loss: 0.1967 (0.2013)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [530/689]  eta: 0:04:10  lr: 0.000089  loss: 0.2055 (0.2013)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [540/689]  eta: 0:03:54  lr: 0.000089  loss: 0.2013 (0.2014)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [550/689]  eta: 0:03:39  lr: 0.000089  loss: 0.1964 (0.2015)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [560/689]  eta: 0:03:23  lr: 0.000089  loss: 0.2237 (0.2022)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [570/689]  eta: 0:03:07  lr: 0.000089  loss: 0.2075 (0.2019)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [580/689]  eta: 0:02:51  lr: 0.000089  loss: 0.1982 (0.2020)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [590/689]  eta: 0:02:36  lr: 0.000089  loss: 0.2027 (0.2020)  time: 1.5785  data: 0.0002  max mem: 39763\n",
      "Train: [epoch:197]  [600/689]  eta: 0:02:20  lr: 0.000089  loss: 0.1988 (0.2019)  time: 1.5784  data: 0.0002  max mem: 39763\n",
      "Train: [epoch:197]  [610/689]  eta: 0:02:04  lr: 0.000089  loss: 0.1859 (0.2017)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [620/689]  eta: 0:01:48  lr: 0.000089  loss: 0.1859 (0.2015)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [630/689]  eta: 0:01:33  lr: 0.000089  loss: 0.1941 (0.2016)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [640/689]  eta: 0:01:17  lr: 0.000089  loss: 0.1963 (0.2016)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [650/689]  eta: 0:01:01  lr: 0.000089  loss: 0.1914 (0.2017)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:197]  [660/689]  eta: 0:00:45  lr: 0.000089  loss: 0.1935 (0.2017)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [670/689]  eta: 0:00:29  lr: 0.000089  loss: 0.1951 (0.2017)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [680/689]  eta: 0:00:14  lr: 0.000089  loss: 0.2002 (0.2018)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197]  [688/689]  eta: 0:00:01  lr: 0.000089  loss: 0.1933 (0.2016)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:197] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: 0.1933 (0.2016)\n",
      "Valid: [epoch:197]  [ 0/14]  eta: 0:00:14  loss: 0.1983 (0.1983)  time: 1.0254  data: 0.3772  max mem: 39763\n",
      "Valid: [epoch:197]  [13/14]  eta: 0:00:00  loss: 0.1874 (0.1889)  time: 0.1152  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:197] Total time: 0:00:01 (0.1279 s / it)\n",
      "Averaged stats: loss: 0.1874 (0.1889)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_197_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.189%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:198]  [  0/689]  eta: 0:12:08  lr: 0.000089  loss: 0.1958 (0.1958)  time: 1.0580  data: 0.5812  max mem: 39763\n",
      "Train: [epoch:198]  [ 10/689]  eta: 0:17:18  lr: 0.000089  loss: 0.1958 (0.1994)  time: 1.5302  data: 0.0529  max mem: 39763\n",
      "Train: [epoch:198]  [ 20/689]  eta: 0:17:18  lr: 0.000089  loss: 0.1913 (0.1945)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [ 30/689]  eta: 0:17:08  lr: 0.000089  loss: 0.1943 (0.1988)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [ 40/689]  eta: 0:16:55  lr: 0.000089  loss: 0.1993 (0.1995)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [ 50/689]  eta: 0:16:41  lr: 0.000089  loss: 0.1946 (0.1987)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [ 60/689]  eta: 0:16:27  lr: 0.000089  loss: 0.1912 (0.1965)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [ 70/689]  eta: 0:16:12  lr: 0.000089  loss: 0.1894 (0.1967)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [ 80/689]  eta: 0:15:56  lr: 0.000089  loss: 0.1886 (0.1957)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [ 90/689]  eta: 0:15:41  lr: 0.000089  loss: 0.1957 (0.1966)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [100/689]  eta: 0:15:26  lr: 0.000089  loss: 0.1999 (0.1970)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [110/689]  eta: 0:15:10  lr: 0.000089  loss: 0.2024 (0.1979)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [120/689]  eta: 0:14:55  lr: 0.000089  loss: 0.2011 (0.1982)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [130/689]  eta: 0:14:39  lr: 0.000089  loss: 0.2049 (0.1999)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [140/689]  eta: 0:14:24  lr: 0.000089  loss: 0.2018 (0.1992)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [150/689]  eta: 0:14:08  lr: 0.000089  loss: 0.1910 (0.1992)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [160/689]  eta: 0:13:53  lr: 0.000089  loss: 0.1979 (0.1999)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [170/689]  eta: 0:13:37  lr: 0.000089  loss: 0.1979 (0.1997)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [180/689]  eta: 0:13:21  lr: 0.000089  loss: 0.1959 (0.1999)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [190/689]  eta: 0:13:06  lr: 0.000089  loss: 0.2036 (0.2008)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [200/689]  eta: 0:12:50  lr: 0.000089  loss: 0.2004 (0.2009)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [210/689]  eta: 0:12:34  lr: 0.000089  loss: 0.1984 (0.2011)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [220/689]  eta: 0:12:19  lr: 0.000089  loss: 0.1976 (0.2012)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [230/689]  eta: 0:12:03  lr: 0.000089  loss: 0.1924 (0.2008)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [240/689]  eta: 0:11:47  lr: 0.000089  loss: 0.1950 (0.2009)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [250/689]  eta: 0:11:31  lr: 0.000089  loss: 0.2106 (0.2010)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [260/689]  eta: 0:11:16  lr: 0.000089  loss: 0.1955 (0.2009)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [270/689]  eta: 0:11:00  lr: 0.000089  loss: 0.1955 (0.2011)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [280/689]  eta: 0:10:44  lr: 0.000089  loss: 0.1999 (0.2013)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [290/689]  eta: 0:10:29  lr: 0.000089  loss: 0.1842 (0.2006)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [300/689]  eta: 0:10:13  lr: 0.000089  loss: 0.1865 (0.2008)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [310/689]  eta: 0:09:57  lr: 0.000089  loss: 0.1809 (0.2001)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [320/689]  eta: 0:09:41  lr: 0.000089  loss: 0.1815 (0.2002)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [330/689]  eta: 0:09:26  lr: 0.000089  loss: 0.1984 (0.2004)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [340/689]  eta: 0:09:10  lr: 0.000089  loss: 0.2125 (0.2007)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [350/689]  eta: 0:08:54  lr: 0.000089  loss: 0.2125 (0.2009)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [360/689]  eta: 0:08:38  lr: 0.000089  loss: 0.2032 (0.2013)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [370/689]  eta: 0:08:23  lr: 0.000089  loss: 0.2028 (0.2015)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [380/689]  eta: 0:08:07  lr: 0.000089  loss: 0.1977 (0.2014)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [390/689]  eta: 0:07:51  lr: 0.000089  loss: 0.1917 (0.2012)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [400/689]  eta: 0:07:35  lr: 0.000089  loss: 0.1917 (0.2009)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [410/689]  eta: 0:07:19  lr: 0.000089  loss: 0.1933 (0.2010)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [420/689]  eta: 0:07:04  lr: 0.000089  loss: 0.1962 (0.2010)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [430/689]  eta: 0:06:48  lr: 0.000089  loss: 0.1962 (0.2013)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [440/689]  eta: 0:06:32  lr: 0.000089  loss: 0.1940 (0.2012)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [450/689]  eta: 0:06:16  lr: 0.000089  loss: 0.1894 (0.2009)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [460/689]  eta: 0:06:01  lr: 0.000089  loss: 0.1911 (0.2008)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [470/689]  eta: 0:05:45  lr: 0.000089  loss: 0.1963 (0.2009)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [480/689]  eta: 0:05:29  lr: 0.000089  loss: 0.1898 (0.2006)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [490/689]  eta: 0:05:13  lr: 0.000089  loss: 0.1898 (0.2005)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [500/689]  eta: 0:04:58  lr: 0.000089  loss: 0.1944 (0.2005)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [510/689]  eta: 0:04:42  lr: 0.000089  loss: 0.1932 (0.2004)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [520/689]  eta: 0:04:26  lr: 0.000089  loss: 0.1914 (0.2004)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [530/689]  eta: 0:04:10  lr: 0.000089  loss: 0.1931 (0.2003)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [540/689]  eta: 0:03:55  lr: 0.000089  loss: 0.1931 (0.2005)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [550/689]  eta: 0:03:39  lr: 0.000089  loss: 0.1934 (0.2006)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [560/689]  eta: 0:03:23  lr: 0.000089  loss: 0.1982 (0.2006)  time: 1.5788  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:198]  [570/689]  eta: 0:03:07  lr: 0.000089  loss: 0.2043 (0.2009)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [580/689]  eta: 0:02:51  lr: 0.000089  loss: 0.2064 (0.2010)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [590/689]  eta: 0:02:36  lr: 0.000089  loss: 0.1891 (0.2010)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [600/689]  eta: 0:02:20  lr: 0.000089  loss: 0.1918 (0.2010)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [610/689]  eta: 0:02:04  lr: 0.000089  loss: 0.1975 (0.2010)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [620/689]  eta: 0:01:48  lr: 0.000089  loss: 0.2039 (0.2011)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [630/689]  eta: 0:01:33  lr: 0.000089  loss: 0.2039 (0.2011)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [640/689]  eta: 0:01:17  lr: 0.000089  loss: 0.2001 (0.2011)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [650/689]  eta: 0:01:01  lr: 0.000089  loss: 0.1911 (0.2010)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [660/689]  eta: 0:00:45  lr: 0.000089  loss: 0.1911 (0.2011)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [670/689]  eta: 0:00:29  lr: 0.000089  loss: 0.1979 (0.2012)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [680/689]  eta: 0:00:14  lr: 0.000089  loss: 0.1956 (0.2012)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198]  [688/689]  eta: 0:00:01  lr: 0.000089  loss: 0.2040 (0.2013)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:198] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: 0.2040 (0.2013)\n",
      "Valid: [epoch:198]  [ 0/14]  eta: 0:00:14  loss: 0.1875 (0.1875)  time: 1.0335  data: 0.3584  max mem: 39763\n",
      "Valid: [epoch:198]  [13/14]  eta: 0:00:00  loss: 0.1875 (0.1898)  time: 0.1157  data: 0.0256  max mem: 39763\n",
      "Valid: [epoch:198] Total time: 0:00:01 (0.1248 s / it)\n",
      "Averaged stats: loss: 0.1875 (0.1898)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_198_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.190%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:199]  [  0/689]  eta: 0:12:14  lr: 0.000089  loss: 0.1863 (0.1863)  time: 1.0667  data: 0.5873  max mem: 39763\n",
      "Train: [epoch:199]  [ 10/689]  eta: 0:17:17  lr: 0.000089  loss: 0.1932 (0.2050)  time: 1.5282  data: 0.0535  max mem: 39763\n",
      "Train: [epoch:199]  [ 20/689]  eta: 0:17:18  lr: 0.000089  loss: 0.2236 (0.2185)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [ 30/689]  eta: 0:17:08  lr: 0.000089  loss: 0.2040 (0.2104)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [ 40/689]  eta: 0:16:55  lr: 0.000089  loss: 0.1927 (0.2052)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [ 50/689]  eta: 0:16:41  lr: 0.000089  loss: 0.1903 (0.2022)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [ 60/689]  eta: 0:16:26  lr: 0.000089  loss: 0.1954 (0.2017)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [ 70/689]  eta: 0:16:11  lr: 0.000089  loss: 0.1999 (0.2026)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [ 80/689]  eta: 0:15:56  lr: 0.000089  loss: 0.1997 (0.2011)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [ 90/689]  eta: 0:15:41  lr: 0.000089  loss: 0.1934 (0.2007)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [100/689]  eta: 0:15:26  lr: 0.000089  loss: 0.1993 (0.2010)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [110/689]  eta: 0:15:10  lr: 0.000089  loss: 0.2009 (0.2012)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [120/689]  eta: 0:14:55  lr: 0.000089  loss: 0.1984 (0.2010)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [130/689]  eta: 0:14:39  lr: 0.000089  loss: 0.2106 (0.2025)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [140/689]  eta: 0:14:24  lr: 0.000089  loss: 0.1994 (0.2020)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [150/689]  eta: 0:14:08  lr: 0.000089  loss: 0.1925 (0.2017)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [160/689]  eta: 0:13:53  lr: 0.000089  loss: 0.1946 (0.2018)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [170/689]  eta: 0:13:37  lr: 0.000089  loss: 0.2034 (0.2022)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [180/689]  eta: 0:13:21  lr: 0.000089  loss: 0.2052 (0.2023)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [190/689]  eta: 0:13:06  lr: 0.000089  loss: 0.2011 (0.2028)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [200/689]  eta: 0:12:50  lr: 0.000089  loss: 0.1959 (0.2029)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [210/689]  eta: 0:12:34  lr: 0.000089  loss: 0.1999 (0.2027)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [220/689]  eta: 0:12:19  lr: 0.000089  loss: 0.2026 (0.2030)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [230/689]  eta: 0:12:03  lr: 0.000089  loss: 0.2026 (0.2032)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [240/689]  eta: 0:11:47  lr: 0.000089  loss: 0.2101 (0.2038)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [250/689]  eta: 0:11:31  lr: 0.000089  loss: 0.2051 (0.2042)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [260/689]  eta: 0:11:16  lr: 0.000089  loss: 0.1977 (0.2045)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [270/689]  eta: 0:11:00  lr: 0.000089  loss: 0.1987 (0.2042)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [280/689]  eta: 0:10:44  lr: 0.000089  loss: 0.2038 (0.2044)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [290/689]  eta: 0:10:29  lr: 0.000089  loss: 0.2108 (0.2045)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [300/689]  eta: 0:10:13  lr: 0.000089  loss: 0.1998 (0.2042)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [310/689]  eta: 0:09:57  lr: 0.000089  loss: 0.1910 (0.2039)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [320/689]  eta: 0:09:41  lr: 0.000089  loss: 0.1971 (0.2038)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [330/689]  eta: 0:09:26  lr: 0.000089  loss: 0.2023 (0.2041)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [340/689]  eta: 0:09:10  lr: 0.000089  loss: 0.1911 (0.2037)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [350/689]  eta: 0:08:54  lr: 0.000089  loss: 0.1909 (0.2035)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [360/689]  eta: 0:08:38  lr: 0.000089  loss: 0.1909 (0.2035)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [370/689]  eta: 0:08:23  lr: 0.000089  loss: 0.1884 (0.2031)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [380/689]  eta: 0:08:07  lr: 0.000089  loss: 0.1887 (0.2029)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [390/689]  eta: 0:07:51  lr: 0.000089  loss: 0.1887 (0.2028)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [400/689]  eta: 0:07:35  lr: 0.000089  loss: 0.1927 (0.2030)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [410/689]  eta: 0:07:20  lr: 0.000089  loss: 0.2143 (0.2034)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [420/689]  eta: 0:07:04  lr: 0.000089  loss: 0.2117 (0.2035)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [430/689]  eta: 0:06:48  lr: 0.000089  loss: 0.2074 (0.2036)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [440/689]  eta: 0:06:32  lr: 0.000089  loss: 0.2030 (0.2038)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [450/689]  eta: 0:06:16  lr: 0.000089  loss: 0.1952 (0.2038)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [460/689]  eta: 0:06:01  lr: 0.000089  loss: 0.1952 (0.2038)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [470/689]  eta: 0:05:45  lr: 0.000089  loss: 0.2009 (0.2038)  time: 1.5789  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:199]  [480/689]  eta: 0:05:29  lr: 0.000089  loss: 0.1973 (0.2036)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [490/689]  eta: 0:05:13  lr: 0.000089  loss: 0.1874 (0.2034)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [500/689]  eta: 0:04:58  lr: 0.000089  loss: 0.1893 (0.2032)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [510/689]  eta: 0:04:42  lr: 0.000089  loss: 0.1935 (0.2032)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [520/689]  eta: 0:04:26  lr: 0.000089  loss: 0.2008 (0.2033)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [530/689]  eta: 0:04:10  lr: 0.000089  loss: 0.2019 (0.2034)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [540/689]  eta: 0:03:55  lr: 0.000089  loss: 0.1911 (0.2032)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [550/689]  eta: 0:03:39  lr: 0.000089  loss: 0.1911 (0.2033)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [560/689]  eta: 0:03:23  lr: 0.000089  loss: 0.2012 (0.2033)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [570/689]  eta: 0:03:07  lr: 0.000089  loss: 0.1932 (0.2031)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [580/689]  eta: 0:02:51  lr: 0.000089  loss: 0.1923 (0.2029)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [590/689]  eta: 0:02:36  lr: 0.000089  loss: 0.1924 (0.2030)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [600/689]  eta: 0:02:20  lr: 0.000089  loss: 0.1974 (0.2030)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [610/689]  eta: 0:02:04  lr: 0.000089  loss: 0.2010 (0.2030)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [620/689]  eta: 0:01:48  lr: 0.000089  loss: 0.2104 (0.2033)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [630/689]  eta: 0:01:33  lr: 0.000089  loss: 0.2036 (0.2034)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [640/689]  eta: 0:01:17  lr: 0.000089  loss: 0.2024 (0.2036)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [650/689]  eta: 0:01:01  lr: 0.000089  loss: 0.2035 (0.2036)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [660/689]  eta: 0:00:45  lr: 0.000089  loss: 0.2058 (0.2038)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [670/689]  eta: 0:00:29  lr: 0.000089  loss: 0.2058 (0.2037)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [680/689]  eta: 0:00:14  lr: 0.000089  loss: 0.2082 (0.2037)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199]  [688/689]  eta: 0:00:01  lr: 0.000089  loss: 0.1921 (0.2035)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:199] Total time: 0:18:07 (1.5781 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: 0.1921 (0.2035)\n",
      "Valid: [epoch:199]  [ 0/14]  eta: 0:00:14  loss: 0.1999 (0.1999)  time: 1.0113  data: 0.3507  max mem: 39763\n",
      "Valid: [epoch:199]  [13/14]  eta: 0:00:00  loss: 0.1893 (0.1917)  time: 0.1142  data: 0.0251  max mem: 39763\n",
      "Valid: [epoch:199] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.1893 (0.1917)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_199_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.192%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:200]  [  0/689]  eta: 0:11:52  lr: 0.000089  loss: 0.2592 (0.2592)  time: 1.0334  data: 0.5550  max mem: 39763\n",
      "Train: [epoch:200]  [ 10/689]  eta: 0:17:17  lr: 0.000089  loss: 0.2015 (0.2038)  time: 1.5280  data: 0.0505  max mem: 39763\n",
      "Train: [epoch:200]  [ 20/689]  eta: 0:17:18  lr: 0.000089  loss: 0.1987 (0.2004)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [ 30/689]  eta: 0:17:08  lr: 0.000089  loss: 0.1983 (0.2003)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [ 40/689]  eta: 0:16:55  lr: 0.000089  loss: 0.1973 (0.1994)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [ 50/689]  eta: 0:16:41  lr: 0.000089  loss: 0.1942 (0.1998)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [ 60/689]  eta: 0:16:26  lr: 0.000089  loss: 0.1860 (0.1972)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [ 70/689]  eta: 0:16:11  lr: 0.000089  loss: 0.1874 (0.1960)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [ 80/689]  eta: 0:15:56  lr: 0.000089  loss: 0.1907 (0.1977)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [ 90/689]  eta: 0:15:41  lr: 0.000089  loss: 0.1941 (0.1976)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [100/689]  eta: 0:15:26  lr: 0.000089  loss: 0.1968 (0.1977)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [110/689]  eta: 0:15:10  lr: 0.000089  loss: 0.1968 (0.1990)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [120/689]  eta: 0:14:55  lr: 0.000089  loss: 0.1894 (0.1984)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [130/689]  eta: 0:14:39  lr: 0.000089  loss: 0.1937 (0.1987)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [140/689]  eta: 0:14:24  lr: 0.000089  loss: 0.1996 (0.1997)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [150/689]  eta: 0:14:08  lr: 0.000089  loss: 0.1972 (0.1995)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [160/689]  eta: 0:13:52  lr: 0.000089  loss: 0.1972 (0.1995)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [170/689]  eta: 0:13:37  lr: 0.000089  loss: 0.2003 (0.1997)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [180/689]  eta: 0:13:21  lr: 0.000089  loss: 0.1995 (0.1997)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [190/689]  eta: 0:13:05  lr: 0.000089  loss: 0.1995 (0.2005)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [200/689]  eta: 0:12:50  lr: 0.000089  loss: 0.2145 (0.2011)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [210/689]  eta: 0:12:34  lr: 0.000089  loss: 0.2159 (0.2017)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [220/689]  eta: 0:12:18  lr: 0.000089  loss: 0.1942 (0.2019)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [230/689]  eta: 0:12:03  lr: 0.000089  loss: 0.2066 (0.2021)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [240/689]  eta: 0:11:47  lr: 0.000089  loss: 0.2129 (0.2028)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [250/689]  eta: 0:11:31  lr: 0.000089  loss: 0.2184 (0.2033)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [260/689]  eta: 0:11:16  lr: 0.000089  loss: 0.1958 (0.2034)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [270/689]  eta: 0:11:00  lr: 0.000089  loss: 0.1976 (0.2038)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [280/689]  eta: 0:10:44  lr: 0.000089  loss: 0.2062 (0.2039)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [290/689]  eta: 0:10:28  lr: 0.000089  loss: 0.1930 (0.2037)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [300/689]  eta: 0:10:13  lr: 0.000089  loss: 0.1999 (0.2036)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [310/689]  eta: 0:09:57  lr: 0.000089  loss: 0.1999 (0.2035)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [320/689]  eta: 0:09:41  lr: 0.000089  loss: 0.1899 (0.2032)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [330/689]  eta: 0:09:26  lr: 0.000089  loss: 0.1904 (0.2031)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [340/689]  eta: 0:09:10  lr: 0.000089  loss: 0.2000 (0.2036)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [350/689]  eta: 0:08:54  lr: 0.000089  loss: 0.2124 (0.2037)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [360/689]  eta: 0:08:38  lr: 0.000089  loss: 0.2072 (0.2042)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [370/689]  eta: 0:08:23  lr: 0.000089  loss: 0.2011 (0.2042)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [380/689]  eta: 0:08:07  lr: 0.000089  loss: 0.1947 (0.2040)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:200]  [390/689]  eta: 0:07:51  lr: 0.000089  loss: 0.1997 (0.2043)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [400/689]  eta: 0:07:35  lr: 0.000089  loss: 0.2034 (0.2042)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [410/689]  eta: 0:07:19  lr: 0.000089  loss: 0.1983 (0.2042)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [420/689]  eta: 0:07:04  lr: 0.000089  loss: 0.2015 (0.2044)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [430/689]  eta: 0:06:48  lr: 0.000089  loss: 0.2090 (0.2046)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [440/689]  eta: 0:06:32  lr: 0.000089  loss: 0.2098 (0.2047)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [450/689]  eta: 0:06:16  lr: 0.000089  loss: 0.2098 (0.2049)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [460/689]  eta: 0:06:01  lr: 0.000089  loss: 0.2096 (0.2052)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [470/689]  eta: 0:05:45  lr: 0.000089  loss: 0.2006 (0.2049)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [480/689]  eta: 0:05:29  lr: 0.000089  loss: 0.1919 (0.2049)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [490/689]  eta: 0:05:13  lr: 0.000089  loss: 0.1960 (0.2047)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [500/689]  eta: 0:04:58  lr: 0.000089  loss: 0.1979 (0.2050)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [510/689]  eta: 0:04:42  lr: 0.000089  loss: 0.1979 (0.2049)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [520/689]  eta: 0:04:26  lr: 0.000089  loss: 0.1975 (0.2047)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [530/689]  eta: 0:04:10  lr: 0.000089  loss: 0.1934 (0.2045)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [540/689]  eta: 0:03:54  lr: 0.000089  loss: 0.1928 (0.2044)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [550/689]  eta: 0:03:39  lr: 0.000089  loss: 0.2067 (0.2045)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [560/689]  eta: 0:03:23  lr: 0.000089  loss: 0.2067 (0.2047)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [570/689]  eta: 0:03:07  lr: 0.000089  loss: 0.2007 (0.2046)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [580/689]  eta: 0:02:51  lr: 0.000089  loss: 0.1988 (0.2046)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [590/689]  eta: 0:02:36  lr: 0.000089  loss: 0.1947 (0.2044)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [600/689]  eta: 0:02:20  lr: 0.000089  loss: 0.1947 (0.2044)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [610/689]  eta: 0:02:04  lr: 0.000089  loss: 0.1894 (0.2042)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [620/689]  eta: 0:01:48  lr: 0.000089  loss: 0.1949 (0.2043)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [630/689]  eta: 0:01:33  lr: 0.000089  loss: 0.2000 (0.2044)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [640/689]  eta: 0:01:17  lr: 0.000089  loss: 0.1995 (0.2044)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [650/689]  eta: 0:01:01  lr: 0.000089  loss: 0.1993 (0.2044)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [660/689]  eta: 0:00:45  lr: 0.000089  loss: 0.1984 (0.2044)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [670/689]  eta: 0:00:29  lr: 0.000089  loss: 0.2020 (0.2045)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [680/689]  eta: 0:00:14  lr: 0.000089  loss: 0.2007 (0.2044)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200]  [688/689]  eta: 0:00:01  lr: 0.000089  loss: 0.1949 (0.2043)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:200] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: 0.1949 (0.2043)\n",
      "Valid: [epoch:200]  [ 0/14]  eta: 0:00:14  loss: 0.1846 (0.1846)  time: 1.0215  data: 0.3893  max mem: 39763\n",
      "Valid: [epoch:200]  [13/14]  eta: 0:00:00  loss: 0.1894 (0.1928)  time: 0.1150  data: 0.0279  max mem: 39763\n",
      "Valid: [epoch:200] Total time: 0:00:01 (0.1247 s / it)\n",
      "Averaged stats: loss: 0.1894 (0.1928)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_200_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.193%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:201]  [  0/689]  eta: 0:12:09  lr: 0.000089  loss: 0.1858 (0.1858)  time: 1.0595  data: 0.5819  max mem: 39763\n",
      "Train: [epoch:201]  [ 10/689]  eta: 0:17:18  lr: 0.000089  loss: 0.2107 (0.2197)  time: 1.5301  data: 0.0530  max mem: 39763\n",
      "Train: [epoch:201]  [ 20/689]  eta: 0:17:19  lr: 0.000089  loss: 0.2041 (0.2090)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [ 30/689]  eta: 0:17:08  lr: 0.000089  loss: 0.1881 (0.2068)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [ 40/689]  eta: 0:16:55  lr: 0.000089  loss: 0.1890 (0.2061)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [ 50/689]  eta: 0:16:41  lr: 0.000089  loss: 0.2013 (0.2070)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [ 60/689]  eta: 0:16:27  lr: 0.000089  loss: 0.2013 (0.2050)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [ 70/689]  eta: 0:16:12  lr: 0.000089  loss: 0.1903 (0.2040)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [ 80/689]  eta: 0:15:57  lr: 0.000089  loss: 0.1947 (0.2045)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [ 90/689]  eta: 0:15:42  lr: 0.000089  loss: 0.1987 (0.2046)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [100/689]  eta: 0:15:27  lr: 0.000089  loss: 0.2026 (0.2044)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [110/689]  eta: 0:15:12  lr: 0.000089  loss: 0.2062 (0.2048)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [120/689]  eta: 0:14:56  lr: 0.000089  loss: 0.2006 (0.2044)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [130/689]  eta: 0:14:41  lr: 0.000089  loss: 0.1992 (0.2054)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [140/689]  eta: 0:14:25  lr: 0.000089  loss: 0.1988 (0.2047)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [150/689]  eta: 0:14:09  lr: 0.000089  loss: 0.1904 (0.2038)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [160/689]  eta: 0:13:54  lr: 0.000089  loss: 0.1998 (0.2043)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [170/689]  eta: 0:13:38  lr: 0.000089  loss: 0.2043 (0.2045)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [180/689]  eta: 0:13:23  lr: 0.000089  loss: 0.2054 (0.2051)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [190/689]  eta: 0:13:07  lr: 0.000089  loss: 0.2071 (0.2054)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [200/689]  eta: 0:12:51  lr: 0.000089  loss: 0.2055 (0.2052)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [210/689]  eta: 0:12:36  lr: 0.000089  loss: 0.1942 (0.2047)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [220/689]  eta: 0:12:20  lr: 0.000089  loss: 0.2070 (0.2053)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [230/689]  eta: 0:12:04  lr: 0.000089  loss: 0.2093 (0.2054)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [240/689]  eta: 0:11:48  lr: 0.000089  loss: 0.2021 (0.2058)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [250/689]  eta: 0:11:33  lr: 0.000089  loss: 0.2087 (0.2057)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [260/689]  eta: 0:11:17  lr: 0.000089  loss: 0.1998 (0.2054)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [270/689]  eta: 0:11:01  lr: 0.000089  loss: 0.1957 (0.2055)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [280/689]  eta: 0:10:46  lr: 0.000089  loss: 0.1932 (0.2051)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [290/689]  eta: 0:10:30  lr: 0.000089  loss: 0.1955 (0.2057)  time: 1.5826  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:201]  [300/689]  eta: 0:10:14  lr: 0.000089  loss: 0.1989 (0.2054)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [310/689]  eta: 0:09:58  lr: 0.000089  loss: 0.1920 (0.2050)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [320/689]  eta: 0:09:42  lr: 0.000089  loss: 0.2018 (0.2054)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [330/689]  eta: 0:09:27  lr: 0.000089  loss: 0.2170 (0.2059)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [340/689]  eta: 0:09:11  lr: 0.000089  loss: 0.2095 (0.2058)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [350/689]  eta: 0:08:55  lr: 0.000089  loss: 0.1938 (0.2055)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [360/689]  eta: 0:08:39  lr: 0.000089  loss: 0.1993 (0.2056)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [370/689]  eta: 0:08:24  lr: 0.000089  loss: 0.1995 (0.2054)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [380/689]  eta: 0:08:08  lr: 0.000089  loss: 0.1963 (0.2052)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [390/689]  eta: 0:07:52  lr: 0.000089  loss: 0.1963 (0.2051)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [400/689]  eta: 0:07:36  lr: 0.000089  loss: 0.2000 (0.2051)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [410/689]  eta: 0:07:20  lr: 0.000089  loss: 0.2063 (0.2054)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [420/689]  eta: 0:07:05  lr: 0.000089  loss: 0.2042 (0.2052)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [430/689]  eta: 0:06:49  lr: 0.000089  loss: 0.2042 (0.2054)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [440/689]  eta: 0:06:33  lr: 0.000089  loss: 0.2104 (0.2054)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [450/689]  eta: 0:06:17  lr: 0.000089  loss: 0.2062 (0.2055)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [460/689]  eta: 0:06:01  lr: 0.000089  loss: 0.2075 (0.2056)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [470/689]  eta: 0:05:46  lr: 0.000089  loss: 0.2024 (0.2054)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [480/689]  eta: 0:05:30  lr: 0.000089  loss: 0.1903 (0.2052)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [490/689]  eta: 0:05:14  lr: 0.000089  loss: 0.2074 (0.2055)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [500/689]  eta: 0:04:58  lr: 0.000089  loss: 0.2151 (0.2057)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [510/689]  eta: 0:04:42  lr: 0.000089  loss: 0.2147 (0.2057)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [520/689]  eta: 0:04:27  lr: 0.000089  loss: 0.2030 (0.2059)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [530/689]  eta: 0:04:11  lr: 0.000089  loss: 0.2052 (0.2060)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [540/689]  eta: 0:03:55  lr: 0.000089  loss: 0.2052 (0.2061)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [550/689]  eta: 0:03:39  lr: 0.000089  loss: 0.2021 (0.2063)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [560/689]  eta: 0:03:23  lr: 0.000089  loss: 0.1999 (0.2062)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [570/689]  eta: 0:03:08  lr: 0.000089  loss: 0.1915 (0.2060)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [580/689]  eta: 0:02:52  lr: 0.000089  loss: 0.1907 (0.2058)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [590/689]  eta: 0:02:36  lr: 0.000089  loss: 0.1960 (0.2059)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [600/689]  eta: 0:02:20  lr: 0.000089  loss: 0.1960 (0.2057)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [610/689]  eta: 0:02:04  lr: 0.000089  loss: 0.1886 (0.2054)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [620/689]  eta: 0:01:49  lr: 0.000089  loss: 0.1927 (0.2055)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [630/689]  eta: 0:01:33  lr: 0.000089  loss: 0.1998 (0.2055)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [640/689]  eta: 0:01:17  lr: 0.000089  loss: 0.2030 (0.2056)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [650/689]  eta: 0:01:01  lr: 0.000089  loss: 0.2030 (0.2056)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [660/689]  eta: 0:00:45  lr: 0.000089  loss: 0.2056 (0.2057)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [670/689]  eta: 0:00:30  lr: 0.000089  loss: 0.2078 (0.2058)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [680/689]  eta: 0:00:14  lr: 0.000089  loss: 0.2075 (0.2058)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201]  [688/689]  eta: 0:00:01  lr: 0.000089  loss: 0.2060 (0.2057)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:201] Total time: 0:18:09 (1.5813 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: 0.2060 (0.2057)\n",
      "Valid: [epoch:201]  [ 0/14]  eta: 0:00:14  loss: 0.1859 (0.1859)  time: 1.0215  data: 0.4064  max mem: 39763\n",
      "Valid: [epoch:201]  [13/14]  eta: 0:00:00  loss: 0.1910 (0.1936)  time: 0.1150  data: 0.0291  max mem: 39763\n",
      "Valid: [epoch:201] Total time: 0:00:01 (0.1243 s / it)\n",
      "Averaged stats: loss: 0.1910 (0.1936)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_201_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.194%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:202]  [  0/689]  eta: 0:11:59  lr: 0.000089  loss: 0.2029 (0.2029)  time: 1.0450  data: 0.5664  max mem: 39763\n",
      "Train: [epoch:202]  [ 10/689]  eta: 0:17:18  lr: 0.000089  loss: 0.2029 (0.2125)  time: 1.5292  data: 0.0516  max mem: 39763\n",
      "Train: [epoch:202]  [ 20/689]  eta: 0:17:18  lr: 0.000089  loss: 0.2027 (0.2130)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [ 30/689]  eta: 0:17:08  lr: 0.000089  loss: 0.2148 (0.2134)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [ 40/689]  eta: 0:16:55  lr: 0.000089  loss: 0.2096 (0.2134)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [ 50/689]  eta: 0:16:41  lr: 0.000089  loss: 0.1989 (0.2109)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [ 60/689]  eta: 0:16:26  lr: 0.000089  loss: 0.1954 (0.2080)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [ 70/689]  eta: 0:16:11  lr: 0.000089  loss: 0.1894 (0.2061)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [ 80/689]  eta: 0:15:56  lr: 0.000089  loss: 0.1924 (0.2062)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [ 90/689]  eta: 0:15:41  lr: 0.000089  loss: 0.1986 (0.2062)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [100/689]  eta: 0:15:25  lr: 0.000089  loss: 0.1986 (0.2058)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [110/689]  eta: 0:15:10  lr: 0.000089  loss: 0.1996 (0.2052)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [120/689]  eta: 0:14:55  lr: 0.000089  loss: 0.1965 (0.2047)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [130/689]  eta: 0:14:39  lr: 0.000089  loss: 0.2005 (0.2050)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [140/689]  eta: 0:14:24  lr: 0.000089  loss: 0.2044 (0.2051)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [150/689]  eta: 0:14:08  lr: 0.000089  loss: 0.2071 (0.2056)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [160/689]  eta: 0:13:52  lr: 0.000089  loss: 0.2057 (0.2056)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [170/689]  eta: 0:13:37  lr: 0.000089  loss: 0.1962 (0.2050)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [180/689]  eta: 0:13:21  lr: 0.000089  loss: 0.1969 (0.2053)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [190/689]  eta: 0:13:05  lr: 0.000089  loss: 0.2043 (0.2051)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [200/689]  eta: 0:12:50  lr: 0.000089  loss: 0.1999 (0.2051)  time: 1.5786  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:202]  [210/689]  eta: 0:12:34  lr: 0.000089  loss: 0.1966 (0.2046)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [220/689]  eta: 0:12:18  lr: 0.000089  loss: 0.1966 (0.2050)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [230/689]  eta: 0:12:03  lr: 0.000089  loss: 0.2045 (0.2048)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [240/689]  eta: 0:11:47  lr: 0.000089  loss: 0.2002 (0.2052)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [250/689]  eta: 0:11:31  lr: 0.000089  loss: 0.2076 (0.2055)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [260/689]  eta: 0:11:16  lr: 0.000089  loss: 0.2128 (0.2059)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [270/689]  eta: 0:11:00  lr: 0.000089  loss: 0.2157 (0.2064)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [280/689]  eta: 0:10:44  lr: 0.000089  loss: 0.2157 (0.2066)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [290/689]  eta: 0:10:28  lr: 0.000089  loss: 0.2048 (0.2065)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [300/689]  eta: 0:10:13  lr: 0.000089  loss: 0.2041 (0.2063)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [310/689]  eta: 0:09:57  lr: 0.000089  loss: 0.1980 (0.2059)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [320/689]  eta: 0:09:41  lr: 0.000089  loss: 0.1978 (0.2062)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [330/689]  eta: 0:09:25  lr: 0.000089  loss: 0.2142 (0.2065)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [340/689]  eta: 0:09:10  lr: 0.000089  loss: 0.2106 (0.2071)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [350/689]  eta: 0:08:54  lr: 0.000089  loss: 0.2051 (0.2071)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [360/689]  eta: 0:08:38  lr: 0.000089  loss: 0.1981 (0.2069)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [370/689]  eta: 0:08:22  lr: 0.000089  loss: 0.1965 (0.2066)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [380/689]  eta: 0:08:07  lr: 0.000089  loss: 0.1958 (0.2063)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [390/689]  eta: 0:07:51  lr: 0.000089  loss: 0.2001 (0.2067)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [400/689]  eta: 0:07:35  lr: 0.000089  loss: 0.2044 (0.2066)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [410/689]  eta: 0:07:19  lr: 0.000089  loss: 0.2044 (0.2067)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [420/689]  eta: 0:07:04  lr: 0.000089  loss: 0.2026 (0.2068)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [430/689]  eta: 0:06:48  lr: 0.000089  loss: 0.2026 (0.2069)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [440/689]  eta: 0:06:32  lr: 0.000089  loss: 0.2050 (0.2069)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [450/689]  eta: 0:06:16  lr: 0.000089  loss: 0.2003 (0.2069)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [460/689]  eta: 0:06:01  lr: 0.000089  loss: 0.2068 (0.2071)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [470/689]  eta: 0:05:45  lr: 0.000089  loss: 0.2033 (0.2069)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [480/689]  eta: 0:05:29  lr: 0.000089  loss: 0.1965 (0.2068)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [490/689]  eta: 0:05:13  lr: 0.000089  loss: 0.1988 (0.2068)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [500/689]  eta: 0:04:58  lr: 0.000089  loss: 0.2013 (0.2066)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [510/689]  eta: 0:04:42  lr: 0.000089  loss: 0.1949 (0.2064)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [520/689]  eta: 0:04:26  lr: 0.000089  loss: 0.1956 (0.2065)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [530/689]  eta: 0:04:10  lr: 0.000089  loss: 0.2074 (0.2065)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [540/689]  eta: 0:03:54  lr: 0.000089  loss: 0.2032 (0.2064)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [550/689]  eta: 0:03:39  lr: 0.000089  loss: 0.1953 (0.2063)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [560/689]  eta: 0:03:23  lr: 0.000089  loss: 0.2025 (0.2065)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [570/689]  eta: 0:03:07  lr: 0.000089  loss: 0.2106 (0.2068)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [580/689]  eta: 0:02:51  lr: 0.000089  loss: 0.1948 (0.2065)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [590/689]  eta: 0:02:36  lr: 0.000089  loss: 0.1910 (0.2066)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [600/689]  eta: 0:02:20  lr: 0.000089  loss: 0.2087 (0.2067)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [610/689]  eta: 0:02:04  lr: 0.000089  loss: 0.2096 (0.2066)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [620/689]  eta: 0:01:48  lr: 0.000089  loss: 0.2162 (0.2067)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [630/689]  eta: 0:01:33  lr: 0.000089  loss: 0.2050 (0.2067)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [640/689]  eta: 0:01:17  lr: 0.000089  loss: 0.1988 (0.2067)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [650/689]  eta: 0:01:01  lr: 0.000089  loss: 0.1884 (0.2064)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [660/689]  eta: 0:00:45  lr: 0.000089  loss: 0.1928 (0.2063)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [670/689]  eta: 0:00:29  lr: 0.000089  loss: 0.2020 (0.2063)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [680/689]  eta: 0:00:14  lr: 0.000089  loss: 0.2034 (0.2062)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202]  [688/689]  eta: 0:00:01  lr: 0.000089  loss: 0.1957 (0.2061)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:202] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: 0.1957 (0.2061)\n",
      "Valid: [epoch:202]  [ 0/14]  eta: 0:00:14  loss: 0.2063 (0.2063)  time: 1.0392  data: 0.3442  max mem: 39763\n",
      "Valid: [epoch:202]  [13/14]  eta: 0:00:00  loss: 0.1910 (0.1947)  time: 0.1162  data: 0.0246  max mem: 39763\n",
      "Valid: [epoch:202] Total time: 0:00:01 (0.1247 s / it)\n",
      "Averaged stats: loss: 0.1910 (0.1947)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_202_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.195%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:203]  [  0/689]  eta: 0:12:15  lr: 0.000089  loss: 0.2510 (0.2510)  time: 1.0679  data: 0.5904  max mem: 39763\n",
      "Train: [epoch:203]  [ 10/689]  eta: 0:17:18  lr: 0.000089  loss: 0.2011 (0.2077)  time: 1.5292  data: 0.0537  max mem: 39763\n",
      "Train: [epoch:203]  [ 20/689]  eta: 0:17:18  lr: 0.000089  loss: 0.1940 (0.2033)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [ 30/689]  eta: 0:17:08  lr: 0.000089  loss: 0.1954 (0.2031)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [ 40/689]  eta: 0:16:55  lr: 0.000089  loss: 0.1976 (0.2020)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [ 50/689]  eta: 0:16:41  lr: 0.000089  loss: 0.1997 (0.2026)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [ 60/689]  eta: 0:16:26  lr: 0.000089  loss: 0.1929 (0.2014)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [ 70/689]  eta: 0:16:11  lr: 0.000089  loss: 0.2024 (0.2037)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [ 80/689]  eta: 0:15:56  lr: 0.000089  loss: 0.2058 (0.2036)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [ 90/689]  eta: 0:15:41  lr: 0.000089  loss: 0.1967 (0.2036)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [100/689]  eta: 0:15:26  lr: 0.000089  loss: 0.1979 (0.2043)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [110/689]  eta: 0:15:10  lr: 0.000089  loss: 0.2067 (0.2038)  time: 1.5779  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:203]  [120/689]  eta: 0:14:55  lr: 0.000089  loss: 0.2025 (0.2047)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [130/689]  eta: 0:14:39  lr: 0.000089  loss: 0.2025 (0.2051)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [140/689]  eta: 0:14:24  lr: 0.000089  loss: 0.2037 (0.2054)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [150/689]  eta: 0:14:08  lr: 0.000089  loss: 0.2042 (0.2062)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [160/689]  eta: 0:13:53  lr: 0.000089  loss: 0.2042 (0.2064)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [170/689]  eta: 0:13:37  lr: 0.000089  loss: 0.2169 (0.2072)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [180/689]  eta: 0:13:21  lr: 0.000089  loss: 0.2211 (0.2080)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [190/689]  eta: 0:13:06  lr: 0.000089  loss: 0.2193 (0.2089)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [200/689]  eta: 0:12:50  lr: 0.000089  loss: 0.1937 (0.2084)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [210/689]  eta: 0:12:34  lr: 0.000089  loss: 0.1937 (0.2079)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [220/689]  eta: 0:12:19  lr: 0.000089  loss: 0.2010 (0.2079)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [230/689]  eta: 0:12:03  lr: 0.000089  loss: 0.2084 (0.2082)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [240/689]  eta: 0:11:47  lr: 0.000089  loss: 0.2024 (0.2077)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [250/689]  eta: 0:11:31  lr: 0.000089  loss: 0.1885 (0.2071)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [260/689]  eta: 0:11:16  lr: 0.000089  loss: 0.1984 (0.2071)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [270/689]  eta: 0:11:00  lr: 0.000089  loss: 0.2029 (0.2068)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [280/689]  eta: 0:10:44  lr: 0.000089  loss: 0.1965 (0.2068)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [290/689]  eta: 0:10:29  lr: 0.000089  loss: 0.1929 (0.2065)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [300/689]  eta: 0:10:13  lr: 0.000089  loss: 0.1929 (0.2066)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [310/689]  eta: 0:09:57  lr: 0.000089  loss: 0.2123 (0.2064)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [320/689]  eta: 0:09:41  lr: 0.000089  loss: 0.2019 (0.2064)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [330/689]  eta: 0:09:26  lr: 0.000089  loss: 0.2012 (0.2062)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [340/689]  eta: 0:09:10  lr: 0.000089  loss: 0.1990 (0.2060)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [350/689]  eta: 0:08:54  lr: 0.000089  loss: 0.2058 (0.2062)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [360/689]  eta: 0:08:38  lr: 0.000089  loss: 0.2058 (0.2062)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [370/689]  eta: 0:08:23  lr: 0.000089  loss: 0.2014 (0.2062)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [380/689]  eta: 0:08:07  lr: 0.000089  loss: 0.2043 (0.2063)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [390/689]  eta: 0:07:51  lr: 0.000089  loss: 0.2019 (0.2063)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [400/689]  eta: 0:07:35  lr: 0.000089  loss: 0.2097 (0.2067)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [410/689]  eta: 0:07:19  lr: 0.000089  loss: 0.2097 (0.2068)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [420/689]  eta: 0:07:04  lr: 0.000089  loss: 0.2032 (0.2069)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [430/689]  eta: 0:06:48  lr: 0.000089  loss: 0.2093 (0.2072)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [440/689]  eta: 0:06:32  lr: 0.000089  loss: 0.1971 (0.2069)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [450/689]  eta: 0:06:16  lr: 0.000089  loss: 0.1950 (0.2067)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [460/689]  eta: 0:06:01  lr: 0.000089  loss: 0.1960 (0.2067)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [470/689]  eta: 0:05:45  lr: 0.000089  loss: 0.1970 (0.2065)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [480/689]  eta: 0:05:29  lr: 0.000089  loss: 0.1945 (0.2063)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [490/689]  eta: 0:05:13  lr: 0.000089  loss: 0.1977 (0.2063)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [500/689]  eta: 0:04:58  lr: 0.000089  loss: 0.2075 (0.2064)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [510/689]  eta: 0:04:42  lr: 0.000089  loss: 0.2143 (0.2067)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [520/689]  eta: 0:04:26  lr: 0.000089  loss: 0.2039 (0.2066)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [530/689]  eta: 0:04:10  lr: 0.000089  loss: 0.1969 (0.2065)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [540/689]  eta: 0:03:55  lr: 0.000089  loss: 0.2023 (0.2065)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [550/689]  eta: 0:03:39  lr: 0.000089  loss: 0.2058 (0.2066)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [560/689]  eta: 0:03:23  lr: 0.000089  loss: 0.2021 (0.2066)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [570/689]  eta: 0:03:07  lr: 0.000089  loss: 0.1954 (0.2065)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [580/689]  eta: 0:02:51  lr: 0.000089  loss: 0.1966 (0.2065)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [590/689]  eta: 0:02:36  lr: 0.000089  loss: 0.1969 (0.2065)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [600/689]  eta: 0:02:20  lr: 0.000089  loss: 0.1929 (0.2062)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [610/689]  eta: 0:02:04  lr: 0.000089  loss: 0.1929 (0.2061)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [620/689]  eta: 0:01:48  lr: 0.000089  loss: 0.1999 (0.2060)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [630/689]  eta: 0:01:33  lr: 0.000089  loss: 0.1992 (0.2060)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [640/689]  eta: 0:01:17  lr: 0.000089  loss: 0.1967 (0.2059)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [650/689]  eta: 0:01:01  lr: 0.000089  loss: 0.1951 (0.2058)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [660/689]  eta: 0:00:45  lr: 0.000089  loss: 0.2014 (0.2059)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [670/689]  eta: 0:00:29  lr: 0.000089  loss: 0.2014 (0.2060)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [680/689]  eta: 0:00:14  lr: 0.000089  loss: 0.2006 (0.2060)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203]  [688/689]  eta: 0:00:01  lr: 0.000089  loss: 0.1904 (0.2059)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:203] Total time: 0:18:06 (1.5776 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: 0.1904 (0.2059)\n",
      "Valid: [epoch:203]  [ 0/14]  eta: 0:00:14  loss: 0.2051 (0.2051)  time: 1.0079  data: 0.3871  max mem: 39763\n",
      "Valid: [epoch:203]  [13/14]  eta: 0:00:00  loss: 0.1950 (0.1979)  time: 0.1139  data: 0.0277  max mem: 39763\n",
      "Valid: [epoch:203] Total time: 0:00:01 (0.1226 s / it)\n",
      "Averaged stats: loss: 0.1950 (0.1979)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_203_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.198%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:204]  [  0/689]  eta: 0:11:26  lr: 0.000089  loss: 0.2766 (0.2766)  time: 0.9959  data: 0.5179  max mem: 39763\n",
      "Train: [epoch:204]  [ 10/689]  eta: 0:17:15  lr: 0.000089  loss: 0.2135 (0.2088)  time: 1.5245  data: 0.0472  max mem: 39763\n",
      "Train: [epoch:204]  [ 20/689]  eta: 0:17:16  lr: 0.000089  loss: 0.1999 (0.2075)  time: 1.5776  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:204]  [ 30/689]  eta: 0:17:07  lr: 0.000089  loss: 0.2059 (0.2086)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [ 40/689]  eta: 0:16:54  lr: 0.000089  loss: 0.2031 (0.2110)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [ 50/689]  eta: 0:16:40  lr: 0.000089  loss: 0.2081 (0.2115)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [ 60/689]  eta: 0:16:26  lr: 0.000089  loss: 0.2055 (0.2087)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [ 70/689]  eta: 0:16:11  lr: 0.000089  loss: 0.2038 (0.2092)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [ 80/689]  eta: 0:15:56  lr: 0.000089  loss: 0.2026 (0.2076)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [ 90/689]  eta: 0:15:41  lr: 0.000089  loss: 0.1968 (0.2065)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [100/689]  eta: 0:15:25  lr: 0.000089  loss: 0.2035 (0.2072)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [110/689]  eta: 0:15:10  lr: 0.000089  loss: 0.2083 (0.2085)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [120/689]  eta: 0:14:54  lr: 0.000089  loss: 0.2045 (0.2075)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [130/689]  eta: 0:14:39  lr: 0.000089  loss: 0.1950 (0.2078)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [140/689]  eta: 0:14:23  lr: 0.000089  loss: 0.2047 (0.2080)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [150/689]  eta: 0:14:08  lr: 0.000089  loss: 0.2006 (0.2079)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [160/689]  eta: 0:13:52  lr: 0.000089  loss: 0.2006 (0.2080)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [170/689]  eta: 0:13:36  lr: 0.000089  loss: 0.2013 (0.2077)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [180/689]  eta: 0:13:21  lr: 0.000089  loss: 0.2073 (0.2077)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [190/689]  eta: 0:13:05  lr: 0.000089  loss: 0.2085 (0.2076)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [200/689]  eta: 0:12:49  lr: 0.000089  loss: 0.2025 (0.2076)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [210/689]  eta: 0:12:34  lr: 0.000089  loss: 0.1987 (0.2070)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [220/689]  eta: 0:12:18  lr: 0.000089  loss: 0.2014 (0.2071)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [230/689]  eta: 0:12:02  lr: 0.000089  loss: 0.2068 (0.2074)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [240/689]  eta: 0:11:47  lr: 0.000089  loss: 0.2068 (0.2077)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [250/689]  eta: 0:11:31  lr: 0.000089  loss: 0.2005 (0.2078)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [260/689]  eta: 0:11:15  lr: 0.000089  loss: 0.2043 (0.2083)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [270/689]  eta: 0:11:00  lr: 0.000089  loss: 0.2074 (0.2085)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [280/689]  eta: 0:10:44  lr: 0.000089  loss: 0.2060 (0.2089)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [290/689]  eta: 0:10:28  lr: 0.000089  loss: 0.2051 (0.2087)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [300/689]  eta: 0:10:12  lr: 0.000089  loss: 0.2069 (0.2089)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [310/689]  eta: 0:09:57  lr: 0.000089  loss: 0.2158 (0.2088)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [320/689]  eta: 0:09:41  lr: 0.000089  loss: 0.1982 (0.2086)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [330/689]  eta: 0:09:25  lr: 0.000089  loss: 0.1982 (0.2082)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [340/689]  eta: 0:09:09  lr: 0.000089  loss: 0.2047 (0.2083)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [350/689]  eta: 0:08:54  lr: 0.000089  loss: 0.2048 (0.2082)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [360/689]  eta: 0:08:38  lr: 0.000089  loss: 0.1929 (0.2078)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [370/689]  eta: 0:08:22  lr: 0.000089  loss: 0.1916 (0.2076)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [380/689]  eta: 0:08:06  lr: 0.000089  loss: 0.1952 (0.2074)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [390/689]  eta: 0:07:51  lr: 0.000089  loss: 0.2056 (0.2075)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [400/689]  eta: 0:07:35  lr: 0.000089  loss: 0.2047 (0.2076)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [410/689]  eta: 0:07:19  lr: 0.000089  loss: 0.2006 (0.2074)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [420/689]  eta: 0:07:03  lr: 0.000089  loss: 0.1974 (0.2072)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [430/689]  eta: 0:06:48  lr: 0.000089  loss: 0.1989 (0.2072)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [440/689]  eta: 0:06:32  lr: 0.000089  loss: 0.2009 (0.2072)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [450/689]  eta: 0:06:16  lr: 0.000089  loss: 0.2009 (0.2073)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [460/689]  eta: 0:06:00  lr: 0.000089  loss: 0.2104 (0.2073)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [470/689]  eta: 0:05:45  lr: 0.000089  loss: 0.2100 (0.2073)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [480/689]  eta: 0:05:29  lr: 0.000089  loss: 0.2033 (0.2071)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [490/689]  eta: 0:05:13  lr: 0.000089  loss: 0.2052 (0.2072)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [500/689]  eta: 0:04:57  lr: 0.000089  loss: 0.2070 (0.2073)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [510/689]  eta: 0:04:42  lr: 0.000089  loss: 0.2033 (0.2072)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [520/689]  eta: 0:04:26  lr: 0.000089  loss: 0.2070 (0.2073)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [530/689]  eta: 0:04:10  lr: 0.000089  loss: 0.2084 (0.2074)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [540/689]  eta: 0:03:54  lr: 0.000089  loss: 0.2039 (0.2073)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [550/689]  eta: 0:03:39  lr: 0.000089  loss: 0.2050 (0.2075)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [560/689]  eta: 0:03:23  lr: 0.000089  loss: 0.2171 (0.2081)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [570/689]  eta: 0:03:07  lr: 0.000089  loss: 0.2031 (0.2081)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [580/689]  eta: 0:02:51  lr: 0.000089  loss: 0.2074 (0.2083)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [590/689]  eta: 0:02:36  lr: 0.000089  loss: 0.2068 (0.2082)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [600/689]  eta: 0:02:20  lr: 0.000089  loss: 0.1884 (0.2081)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [610/689]  eta: 0:02:04  lr: 0.000089  loss: 0.1884 (0.2079)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [620/689]  eta: 0:01:48  lr: 0.000089  loss: 0.1977 (0.2079)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [630/689]  eta: 0:01:33  lr: 0.000089  loss: 0.2023 (0.2079)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [640/689]  eta: 0:01:17  lr: 0.000089  loss: 0.2043 (0.2079)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [650/689]  eta: 0:01:01  lr: 0.000089  loss: 0.2025 (0.2079)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [660/689]  eta: 0:00:45  lr: 0.000089  loss: 0.1989 (0.2078)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [670/689]  eta: 0:00:29  lr: 0.000089  loss: 0.2004 (0.2078)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204]  [680/689]  eta: 0:00:14  lr: 0.000089  loss: 0.2045 (0.2078)  time: 1.5771  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:204]  [688/689]  eta: 0:00:01  lr: 0.000089  loss: 0.2004 (0.2076)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:204] Total time: 0:18:06 (1.5769 s / it)\n",
      "Averaged stats: lr: 0.000089  loss: 0.2004 (0.2076)\n",
      "Valid: [epoch:204]  [ 0/14]  eta: 0:00:14  loss: 0.1733 (0.1733)  time: 1.0075  data: 0.4173  max mem: 39763\n",
      "Valid: [epoch:204]  [13/14]  eta: 0:00:00  loss: 0.1934 (0.1969)  time: 0.1139  data: 0.0299  max mem: 39763\n",
      "Valid: [epoch:204] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.1934 (0.1969)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_204_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.197%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:205]  [  0/689]  eta: 0:12:23  lr: 0.000088  loss: 0.2161 (0.2161)  time: 1.0787  data: 0.6021  max mem: 39763\n",
      "Train: [epoch:205]  [ 10/689]  eta: 0:17:18  lr: 0.000088  loss: 0.2007 (0.2064)  time: 1.5294  data: 0.0548  max mem: 39763\n",
      "Train: [epoch:205]  [ 20/689]  eta: 0:17:17  lr: 0.000088  loss: 0.1985 (0.2025)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [ 30/689]  eta: 0:17:07  lr: 0.000088  loss: 0.1999 (0.2055)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [ 40/689]  eta: 0:16:54  lr: 0.000088  loss: 0.1962 (0.2045)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [ 50/689]  eta: 0:16:40  lr: 0.000088  loss: 0.1852 (0.2017)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [ 60/689]  eta: 0:16:26  lr: 0.000088  loss: 0.1840 (0.2012)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [ 70/689]  eta: 0:16:11  lr: 0.000088  loss: 0.1994 (0.2018)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [ 80/689]  eta: 0:15:56  lr: 0.000088  loss: 0.2040 (0.2037)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [ 90/689]  eta: 0:15:41  lr: 0.000088  loss: 0.2055 (0.2044)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [100/689]  eta: 0:15:25  lr: 0.000088  loss: 0.2032 (0.2045)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [110/689]  eta: 0:15:10  lr: 0.000088  loss: 0.2012 (0.2051)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [120/689]  eta: 0:14:54  lr: 0.000088  loss: 0.2011 (0.2055)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [130/689]  eta: 0:14:39  lr: 0.000088  loss: 0.1978 (0.2053)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [140/689]  eta: 0:14:23  lr: 0.000088  loss: 0.1980 (0.2053)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [150/689]  eta: 0:14:08  lr: 0.000088  loss: 0.2015 (0.2060)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [160/689]  eta: 0:13:52  lr: 0.000088  loss: 0.2050 (0.2064)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [170/689]  eta: 0:13:36  lr: 0.000088  loss: 0.2019 (0.2059)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [180/689]  eta: 0:13:21  lr: 0.000088  loss: 0.1991 (0.2057)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [190/689]  eta: 0:13:05  lr: 0.000088  loss: 0.2043 (0.2060)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [200/689]  eta: 0:12:50  lr: 0.000088  loss: 0.2017 (0.2058)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [210/689]  eta: 0:12:34  lr: 0.000088  loss: 0.1974 (0.2058)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [220/689]  eta: 0:12:18  lr: 0.000088  loss: 0.2030 (0.2059)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [230/689]  eta: 0:12:02  lr: 0.000088  loss: 0.1961 (0.2054)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [240/689]  eta: 0:11:47  lr: 0.000088  loss: 0.1961 (0.2056)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [250/689]  eta: 0:11:31  lr: 0.000088  loss: 0.1947 (0.2054)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [260/689]  eta: 0:11:15  lr: 0.000088  loss: 0.1947 (0.2065)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [270/689]  eta: 0:11:00  lr: 0.000088  loss: 0.2109 (0.2064)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [280/689]  eta: 0:10:44  lr: 0.000088  loss: 0.2052 (0.2065)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [290/689]  eta: 0:10:28  lr: 0.000088  loss: 0.2064 (0.2065)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [300/689]  eta: 0:10:12  lr: 0.000088  loss: 0.2003 (0.2062)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [310/689]  eta: 0:09:57  lr: 0.000088  loss: 0.1991 (0.2059)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [320/689]  eta: 0:09:41  lr: 0.000088  loss: 0.2030 (0.2062)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [330/689]  eta: 0:09:25  lr: 0.000088  loss: 0.2126 (0.2066)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [340/689]  eta: 0:09:10  lr: 0.000088  loss: 0.2126 (0.2068)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [350/689]  eta: 0:08:54  lr: 0.000088  loss: 0.2094 (0.2071)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [360/689]  eta: 0:08:38  lr: 0.000088  loss: 0.2070 (0.2076)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [370/689]  eta: 0:08:22  lr: 0.000088  loss: 0.2063 (0.2076)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [380/689]  eta: 0:08:07  lr: 0.000088  loss: 0.2063 (0.2078)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [390/689]  eta: 0:07:51  lr: 0.000088  loss: 0.2096 (0.2081)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [400/689]  eta: 0:07:35  lr: 0.000088  loss: 0.2127 (0.2084)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [410/689]  eta: 0:07:19  lr: 0.000088  loss: 0.2127 (0.2086)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [420/689]  eta: 0:07:04  lr: 0.000088  loss: 0.2082 (0.2087)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [430/689]  eta: 0:06:48  lr: 0.000088  loss: 0.2118 (0.2091)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [440/689]  eta: 0:06:32  lr: 0.000088  loss: 0.2142 (0.2092)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [450/689]  eta: 0:06:16  lr: 0.000088  loss: 0.2098 (0.2093)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [460/689]  eta: 0:06:00  lr: 0.000088  loss: 0.2099 (0.2096)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [470/689]  eta: 0:05:45  lr: 0.000088  loss: 0.2085 (0.2096)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [480/689]  eta: 0:05:29  lr: 0.000088  loss: 0.2086 (0.2097)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [490/689]  eta: 0:05:13  lr: 0.000088  loss: 0.2083 (0.2096)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [500/689]  eta: 0:04:57  lr: 0.000088  loss: 0.2019 (0.2097)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [510/689]  eta: 0:04:42  lr: 0.000088  loss: 0.2019 (0.2098)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [520/689]  eta: 0:04:26  lr: 0.000088  loss: 0.2112 (0.2100)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [530/689]  eta: 0:04:10  lr: 0.000088  loss: 0.2112 (0.2102)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [540/689]  eta: 0:03:54  lr: 0.000088  loss: 0.2111 (0.2104)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [550/689]  eta: 0:03:39  lr: 0.000088  loss: 0.1989 (0.2102)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [560/689]  eta: 0:03:23  lr: 0.000088  loss: 0.1961 (0.2101)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [570/689]  eta: 0:03:07  lr: 0.000088  loss: 0.2014 (0.2100)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [580/689]  eta: 0:02:51  lr: 0.000088  loss: 0.2031 (0.2101)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [590/689]  eta: 0:02:36  lr: 0.000088  loss: 0.2104 (0.2102)  time: 1.5768  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:205]  [600/689]  eta: 0:02:20  lr: 0.000088  loss: 0.2132 (0.2103)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [610/689]  eta: 0:02:04  lr: 0.000088  loss: 0.2145 (0.2104)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [620/689]  eta: 0:01:48  lr: 0.000088  loss: 0.2102 (0.2105)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [630/689]  eta: 0:01:33  lr: 0.000088  loss: 0.1983 (0.2103)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [640/689]  eta: 0:01:17  lr: 0.000088  loss: 0.1987 (0.2105)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [650/689]  eta: 0:01:01  lr: 0.000088  loss: 0.1998 (0.2104)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [660/689]  eta: 0:00:45  lr: 0.000088  loss: 0.1925 (0.2103)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [670/689]  eta: 0:00:29  lr: 0.000088  loss: 0.1925 (0.2102)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [680/689]  eta: 0:00:14  lr: 0.000088  loss: 0.1989 (0.2101)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205]  [688/689]  eta: 0:00:01  lr: 0.000088  loss: 0.1989 (0.2101)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:205] Total time: 0:18:06 (1.5768 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: 0.1989 (0.2101)\n",
      "Valid: [epoch:205]  [ 0/14]  eta: 0:00:14  loss: 0.1938 (0.1938)  time: 1.0200  data: 0.3451  max mem: 39763\n",
      "Valid: [epoch:205]  [13/14]  eta: 0:00:00  loss: 0.1943 (0.1977)  time: 0.1147  data: 0.0247  max mem: 39763\n",
      "Valid: [epoch:205] Total time: 0:00:01 (0.1240 s / it)\n",
      "Averaged stats: loss: 0.1943 (0.1977)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_205_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.198%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:206]  [  0/689]  eta: 0:11:47  lr: 0.000088  loss: 0.2368 (0.2368)  time: 1.0273  data: 0.5495  max mem: 39763\n",
      "Train: [epoch:206]  [ 10/689]  eta: 0:17:16  lr: 0.000088  loss: 0.2007 (0.1989)  time: 1.5261  data: 0.0500  max mem: 39763\n",
      "Train: [epoch:206]  [ 20/689]  eta: 0:17:17  lr: 0.000088  loss: 0.2011 (0.2040)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [ 30/689]  eta: 0:17:07  lr: 0.000088  loss: 0.2080 (0.2067)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [ 40/689]  eta: 0:16:54  lr: 0.000088  loss: 0.2008 (0.2049)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [ 50/689]  eta: 0:16:40  lr: 0.000088  loss: 0.2059 (0.2066)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [ 60/689]  eta: 0:16:26  lr: 0.000088  loss: 0.2076 (0.2068)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [ 70/689]  eta: 0:16:11  lr: 0.000088  loss: 0.1911 (0.2052)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [ 80/689]  eta: 0:15:56  lr: 0.000088  loss: 0.1921 (0.2054)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [ 90/689]  eta: 0:15:40  lr: 0.000088  loss: 0.1971 (0.2050)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [100/689]  eta: 0:15:25  lr: 0.000088  loss: 0.2030 (0.2053)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [110/689]  eta: 0:15:10  lr: 0.000088  loss: 0.2177 (0.2072)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [120/689]  eta: 0:14:54  lr: 0.000088  loss: 0.2174 (0.2072)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [130/689]  eta: 0:14:39  lr: 0.000088  loss: 0.2008 (0.2066)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [140/689]  eta: 0:14:23  lr: 0.000088  loss: 0.2058 (0.2075)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [150/689]  eta: 0:14:08  lr: 0.000088  loss: 0.2064 (0.2080)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [160/689]  eta: 0:13:52  lr: 0.000088  loss: 0.2026 (0.2081)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [170/689]  eta: 0:13:36  lr: 0.000088  loss: 0.2078 (0.2082)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [180/689]  eta: 0:13:21  lr: 0.000088  loss: 0.1958 (0.2078)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [190/689]  eta: 0:13:05  lr: 0.000088  loss: 0.1992 (0.2076)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [200/689]  eta: 0:12:49  lr: 0.000088  loss: 0.2036 (0.2077)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [210/689]  eta: 0:12:34  lr: 0.000088  loss: 0.2029 (0.2070)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [220/689]  eta: 0:12:18  lr: 0.000088  loss: 0.2018 (0.2069)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [230/689]  eta: 0:12:02  lr: 0.000088  loss: 0.2047 (0.2068)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [240/689]  eta: 0:11:47  lr: 0.000088  loss: 0.2075 (0.2078)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [250/689]  eta: 0:11:31  lr: 0.000088  loss: 0.2110 (0.2078)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [260/689]  eta: 0:11:15  lr: 0.000088  loss: 0.1946 (0.2074)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [270/689]  eta: 0:11:00  lr: 0.000088  loss: 0.1910 (0.2070)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [280/689]  eta: 0:10:44  lr: 0.000088  loss: 0.2007 (0.2074)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [290/689]  eta: 0:10:28  lr: 0.000088  loss: 0.2135 (0.2074)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [300/689]  eta: 0:10:12  lr: 0.000088  loss: 0.1951 (0.2068)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [310/689]  eta: 0:09:57  lr: 0.000088  loss: 0.1927 (0.2067)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [320/689]  eta: 0:09:41  lr: 0.000088  loss: 0.1977 (0.2067)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [330/689]  eta: 0:09:25  lr: 0.000088  loss: 0.2079 (0.2070)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [340/689]  eta: 0:09:09  lr: 0.000088  loss: 0.2122 (0.2072)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [350/689]  eta: 0:08:54  lr: 0.000088  loss: 0.2082 (0.2073)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [360/689]  eta: 0:08:38  lr: 0.000088  loss: 0.2076 (0.2075)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [370/689]  eta: 0:08:22  lr: 0.000088  loss: 0.2084 (0.2076)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [380/689]  eta: 0:08:06  lr: 0.000088  loss: 0.2084 (0.2077)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [390/689]  eta: 0:07:51  lr: 0.000088  loss: 0.2083 (0.2077)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [400/689]  eta: 0:07:35  lr: 0.000088  loss: 0.2110 (0.2083)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [410/689]  eta: 0:07:19  lr: 0.000088  loss: 0.2137 (0.2085)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [420/689]  eta: 0:07:03  lr: 0.000088  loss: 0.2049 (0.2085)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [430/689]  eta: 0:06:48  lr: 0.000088  loss: 0.2084 (0.2088)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [440/689]  eta: 0:06:32  lr: 0.000088  loss: 0.2192 (0.2091)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [450/689]  eta: 0:06:16  lr: 0.000088  loss: 0.2217 (0.2092)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [460/689]  eta: 0:06:00  lr: 0.000088  loss: 0.2043 (0.2092)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [470/689]  eta: 0:05:45  lr: 0.000088  loss: 0.2059 (0.2093)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [480/689]  eta: 0:05:29  lr: 0.000088  loss: 0.2172 (0.2095)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [490/689]  eta: 0:05:13  lr: 0.000088  loss: 0.2082 (0.2094)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [500/689]  eta: 0:04:57  lr: 0.000088  loss: 0.1970 (0.2092)  time: 1.5775  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:206]  [510/689]  eta: 0:04:42  lr: 0.000088  loss: 0.2028 (0.2093)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [520/689]  eta: 0:04:26  lr: 0.000088  loss: 0.2156 (0.2093)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [530/689]  eta: 0:04:10  lr: 0.000088  loss: 0.2029 (0.2092)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [540/689]  eta: 0:03:54  lr: 0.000088  loss: 0.1921 (0.2091)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [550/689]  eta: 0:03:39  lr: 0.000088  loss: 0.2002 (0.2091)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [560/689]  eta: 0:03:23  lr: 0.000088  loss: 0.2042 (0.2090)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [570/689]  eta: 0:03:07  lr: 0.000088  loss: 0.2078 (0.2092)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [580/689]  eta: 0:02:51  lr: 0.000088  loss: 0.2008 (0.2090)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [590/689]  eta: 0:02:36  lr: 0.000088  loss: 0.1983 (0.2091)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [600/689]  eta: 0:02:20  lr: 0.000088  loss: 0.2034 (0.2090)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [610/689]  eta: 0:02:04  lr: 0.000088  loss: 0.2034 (0.2090)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [620/689]  eta: 0:01:48  lr: 0.000088  loss: 0.1990 (0.2089)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [630/689]  eta: 0:01:33  lr: 0.000088  loss: 0.1990 (0.2089)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [640/689]  eta: 0:01:17  lr: 0.000088  loss: 0.2031 (0.2090)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [650/689]  eta: 0:01:01  lr: 0.000088  loss: 0.2031 (0.2089)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [660/689]  eta: 0:00:45  lr: 0.000088  loss: 0.2006 (0.2090)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [670/689]  eta: 0:00:29  lr: 0.000088  loss: 0.2106 (0.2091)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [680/689]  eta: 0:00:14  lr: 0.000088  loss: 0.2060 (0.2090)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206]  [688/689]  eta: 0:00:01  lr: 0.000088  loss: 0.1983 (0.2089)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:206] Total time: 0:18:06 (1.5769 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: 0.1983 (0.2089)\n",
      "Valid: [epoch:206]  [ 0/14]  eta: 0:00:14  loss: 0.2101 (0.2101)  time: 1.0107  data: 0.3777  max mem: 39763\n",
      "Valid: [epoch:206]  [13/14]  eta: 0:00:00  loss: 0.1949 (0.1979)  time: 0.1141  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:206] Total time: 0:00:01 (0.1220 s / it)\n",
      "Averaged stats: loss: 0.1949 (0.1979)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_206_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.198%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:207]  [  0/689]  eta: 0:12:19  lr: 0.000088  loss: 0.1805 (0.1805)  time: 1.0733  data: 0.5970  max mem: 39763\n",
      "Train: [epoch:207]  [ 10/689]  eta: 0:17:18  lr: 0.000088  loss: 0.1978 (0.1995)  time: 1.5294  data: 0.0544  max mem: 39763\n",
      "Train: [epoch:207]  [ 20/689]  eta: 0:17:17  lr: 0.000088  loss: 0.1978 (0.2026)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [ 30/689]  eta: 0:17:07  lr: 0.000088  loss: 0.1984 (0.2007)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [ 40/689]  eta: 0:16:54  lr: 0.000088  loss: 0.1941 (0.2015)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [ 50/689]  eta: 0:16:40  lr: 0.000088  loss: 0.1979 (0.2014)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [ 60/689]  eta: 0:16:26  lr: 0.000088  loss: 0.2022 (0.2039)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [ 70/689]  eta: 0:16:11  lr: 0.000088  loss: 0.1975 (0.2027)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [ 80/689]  eta: 0:15:56  lr: 0.000088  loss: 0.1983 (0.2040)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [ 90/689]  eta: 0:15:40  lr: 0.000088  loss: 0.2024 (0.2034)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [100/689]  eta: 0:15:25  lr: 0.000088  loss: 0.2053 (0.2054)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [110/689]  eta: 0:15:10  lr: 0.000088  loss: 0.2215 (0.2072)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [120/689]  eta: 0:14:54  lr: 0.000088  loss: 0.2113 (0.2077)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [130/689]  eta: 0:14:39  lr: 0.000088  loss: 0.2095 (0.2086)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [140/689]  eta: 0:14:23  lr: 0.000088  loss: 0.2095 (0.2090)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [150/689]  eta: 0:14:07  lr: 0.000088  loss: 0.2174 (0.2108)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [160/689]  eta: 0:13:52  lr: 0.000088  loss: 0.2174 (0.2109)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [170/689]  eta: 0:13:36  lr: 0.000088  loss: 0.2060 (0.2107)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [180/689]  eta: 0:13:21  lr: 0.000088  loss: 0.2029 (0.2107)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [190/689]  eta: 0:13:05  lr: 0.000088  loss: 0.2126 (0.2118)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [200/689]  eta: 0:12:49  lr: 0.000088  loss: 0.2123 (0.2117)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [210/689]  eta: 0:12:34  lr: 0.000088  loss: 0.2035 (0.2114)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [220/689]  eta: 0:12:18  lr: 0.000088  loss: 0.2187 (0.2120)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [230/689]  eta: 0:12:02  lr: 0.000088  loss: 0.2240 (0.2120)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [240/689]  eta: 0:11:47  lr: 0.000088  loss: 0.2055 (0.2119)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [250/689]  eta: 0:11:31  lr: 0.000088  loss: 0.2055 (0.2118)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [260/689]  eta: 0:11:15  lr: 0.000088  loss: 0.2134 (0.2121)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [270/689]  eta: 0:10:59  lr: 0.000088  loss: 0.2101 (0.2120)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [280/689]  eta: 0:10:44  lr: 0.000088  loss: 0.2017 (0.2118)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [290/689]  eta: 0:10:28  lr: 0.000088  loss: 0.2017 (0.2118)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [300/689]  eta: 0:10:12  lr: 0.000088  loss: 0.1969 (0.2117)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [310/689]  eta: 0:09:57  lr: 0.000088  loss: 0.1994 (0.2116)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [320/689]  eta: 0:09:41  lr: 0.000088  loss: 0.2048 (0.2116)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [330/689]  eta: 0:09:25  lr: 0.000088  loss: 0.2048 (0.2119)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [340/689]  eta: 0:09:09  lr: 0.000088  loss: 0.2072 (0.2119)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [350/689]  eta: 0:08:54  lr: 0.000088  loss: 0.2022 (0.2113)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [360/689]  eta: 0:08:38  lr: 0.000088  loss: 0.2078 (0.2118)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [370/689]  eta: 0:08:22  lr: 0.000088  loss: 0.2335 (0.2184)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [380/689]  eta: 0:08:06  lr: 0.000088  loss: 0.2495 (0.2196)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [390/689]  eta: 0:07:51  lr: 0.000088  loss: 0.2466 (0.2203)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [400/689]  eta: 0:07:35  lr: 0.000088  loss: 0.2384 (0.2206)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [410/689]  eta: 0:07:19  lr: 0.000088  loss: 0.2234 (0.2204)  time: 1.5769  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:207]  [420/689]  eta: 0:07:03  lr: 0.000088  loss: 0.2013 (0.2201)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [430/689]  eta: 0:06:48  lr: 0.000088  loss: 0.2119 (0.2205)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [440/689]  eta: 0:06:32  lr: 0.000088  loss: 0.2305 (0.2208)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [450/689]  eta: 0:06:16  lr: 0.000088  loss: 0.2070 (0.2205)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [460/689]  eta: 0:06:00  lr: 0.000088  loss: 0.2070 (0.2204)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [470/689]  eta: 0:05:45  lr: 0.000088  loss: 0.2105 (0.2202)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [480/689]  eta: 0:05:29  lr: 0.000088  loss: 0.2099 (0.2201)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [490/689]  eta: 0:05:13  lr: 0.000088  loss: 0.2127 (0.2200)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [500/689]  eta: 0:04:57  lr: 0.000088  loss: 0.2049 (0.2197)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [510/689]  eta: 0:04:42  lr: 0.000088  loss: 0.2071 (0.2196)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [520/689]  eta: 0:04:26  lr: 0.000088  loss: 0.2052 (0.2196)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [530/689]  eta: 0:04:10  lr: 0.000088  loss: 0.2028 (0.2193)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [540/689]  eta: 0:03:54  lr: 0.000088  loss: 0.2156 (0.2194)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [550/689]  eta: 0:03:39  lr: 0.000088  loss: 0.1948 (0.2190)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [560/689]  eta: 0:03:23  lr: 0.000088  loss: 0.1947 (0.2189)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [570/689]  eta: 0:03:07  lr: 0.000088  loss: 0.2066 (0.2188)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [580/689]  eta: 0:02:51  lr: 0.000088  loss: 0.2051 (0.2186)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [590/689]  eta: 0:02:36  lr: 0.000088  loss: 0.2068 (0.2186)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [600/689]  eta: 0:02:20  lr: 0.000088  loss: 0.2161 (0.2185)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [610/689]  eta: 0:02:04  lr: 0.000088  loss: 0.2125 (0.2183)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [620/689]  eta: 0:01:48  lr: 0.000088  loss: 0.2103 (0.2182)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [630/689]  eta: 0:01:32  lr: 0.000088  loss: 0.2103 (0.2180)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [640/689]  eta: 0:01:17  lr: 0.000088  loss: 0.1974 (0.2178)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [650/689]  eta: 0:01:01  lr: 0.000088  loss: 0.1935 (0.2174)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [660/689]  eta: 0:00:45  lr: 0.000088  loss: 0.1974 (0.2172)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [670/689]  eta: 0:00:29  lr: 0.000088  loss: 0.1982 (0.2170)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [680/689]  eta: 0:00:14  lr: 0.000088  loss: 0.2031 (0.2169)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207]  [688/689]  eta: 0:00:01  lr: 0.000088  loss: 0.2093 (0.2169)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:207] Total time: 0:18:06 (1.5764 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: 0.2093 (0.2169)\n",
      "Valid: [epoch:207]  [ 0/14]  eta: 0:00:14  loss: 0.1909 (0.1909)  time: 1.0357  data: 0.3864  max mem: 39763\n",
      "Valid: [epoch:207]  [13/14]  eta: 0:00:00  loss: 0.1961 (0.1991)  time: 0.1160  data: 0.0277  max mem: 39763\n",
      "Valid: [epoch:207] Total time: 0:00:01 (0.1254 s / it)\n",
      "Averaged stats: loss: 0.1961 (0.1991)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_207_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.199%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:208]  [  0/689]  eta: 0:11:46  lr: 0.000088  loss: 0.2607 (0.2607)  time: 1.0253  data: 0.5474  max mem: 39763\n",
      "Train: [epoch:208]  [ 10/689]  eta: 0:17:15  lr: 0.000088  loss: 0.1939 (0.1987)  time: 1.5254  data: 0.0498  max mem: 39763\n",
      "Train: [epoch:208]  [ 20/689]  eta: 0:17:16  lr: 0.000088  loss: 0.1939 (0.2028)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [ 30/689]  eta: 0:17:06  lr: 0.000088  loss: 0.2088 (0.2091)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [ 40/689]  eta: 0:16:53  lr: 0.000088  loss: 0.2037 (0.2074)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [ 50/689]  eta: 0:16:39  lr: 0.000088  loss: 0.2025 (0.2088)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [ 60/689]  eta: 0:16:25  lr: 0.000088  loss: 0.2101 (0.2077)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [ 70/689]  eta: 0:16:10  lr: 0.000088  loss: 0.1960 (0.2067)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [ 80/689]  eta: 0:15:55  lr: 0.000088  loss: 0.1935 (0.2073)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [ 90/689]  eta: 0:15:40  lr: 0.000088  loss: 0.1977 (0.2067)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [100/689]  eta: 0:15:25  lr: 0.000088  loss: 0.2057 (0.2070)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [110/689]  eta: 0:15:09  lr: 0.000088  loss: 0.2090 (0.2075)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [120/689]  eta: 0:14:54  lr: 0.000088  loss: 0.2133 (0.2077)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [130/689]  eta: 0:14:38  lr: 0.000088  loss: 0.2110 (0.2072)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [140/689]  eta: 0:14:23  lr: 0.000088  loss: 0.2049 (0.2070)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [150/689]  eta: 0:14:07  lr: 0.000088  loss: 0.2069 (0.2078)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [160/689]  eta: 0:13:52  lr: 0.000088  loss: 0.2044 (0.2069)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [170/689]  eta: 0:13:36  lr: 0.000088  loss: 0.1899 (0.2065)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [180/689]  eta: 0:13:20  lr: 0.000088  loss: 0.1951 (0.2061)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [190/689]  eta: 0:13:05  lr: 0.000088  loss: 0.2071 (0.2068)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [200/689]  eta: 0:12:49  lr: 0.000088  loss: 0.2071 (0.2070)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [210/689]  eta: 0:12:33  lr: 0.000088  loss: 0.2015 (0.2069)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [220/689]  eta: 0:12:18  lr: 0.000088  loss: 0.2008 (0.2069)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [230/689]  eta: 0:12:02  lr: 0.000088  loss: 0.2072 (0.2069)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [240/689]  eta: 0:11:46  lr: 0.000088  loss: 0.2123 (0.2075)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [250/689]  eta: 0:11:31  lr: 0.000088  loss: 0.2147 (0.2079)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [260/689]  eta: 0:11:15  lr: 0.000088  loss: 0.2123 (0.2084)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [270/689]  eta: 0:10:59  lr: 0.000088  loss: 0.2030 (0.2079)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [280/689]  eta: 0:10:44  lr: 0.000088  loss: 0.2030 (0.2085)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [290/689]  eta: 0:10:28  lr: 0.000088  loss: 0.2099 (0.2084)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [300/689]  eta: 0:10:12  lr: 0.000088  loss: 0.1981 (0.2086)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [310/689]  eta: 0:09:56  lr: 0.000088  loss: 0.1916 (0.2080)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [320/689]  eta: 0:09:41  lr: 0.000088  loss: 0.1980 (0.2084)  time: 1.5771  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:208]  [330/689]  eta: 0:09:25  lr: 0.000088  loss: 0.2074 (0.2084)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [340/689]  eta: 0:09:09  lr: 0.000088  loss: 0.2080 (0.2088)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [350/689]  eta: 0:08:53  lr: 0.000088  loss: 0.2141 (0.2092)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [360/689]  eta: 0:08:38  lr: 0.000088  loss: 0.2084 (0.2093)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [370/689]  eta: 0:08:22  lr: 0.000088  loss: 0.2010 (0.2091)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [380/689]  eta: 0:08:06  lr: 0.000088  loss: 0.2016 (0.2092)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [390/689]  eta: 0:07:50  lr: 0.000088  loss: 0.2152 (0.2095)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [400/689]  eta: 0:07:35  lr: 0.000088  loss: 0.2098 (0.2096)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [410/689]  eta: 0:07:19  lr: 0.000088  loss: 0.2080 (0.2095)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [420/689]  eta: 0:07:03  lr: 0.000088  loss: 0.2051 (0.2095)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [430/689]  eta: 0:06:48  lr: 0.000088  loss: 0.2095 (0.2095)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [440/689]  eta: 0:06:32  lr: 0.000088  loss: 0.2106 (0.2097)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [450/689]  eta: 0:06:16  lr: 0.000088  loss: 0.1933 (0.2093)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [460/689]  eta: 0:06:00  lr: 0.000088  loss: 0.1977 (0.2093)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [470/689]  eta: 0:05:45  lr: 0.000088  loss: 0.2141 (0.2094)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [480/689]  eta: 0:05:29  lr: 0.000088  loss: 0.2141 (0.2095)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [490/689]  eta: 0:05:13  lr: 0.000088  loss: 0.2006 (0.2096)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [500/689]  eta: 0:04:57  lr: 0.000088  loss: 0.2064 (0.2100)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [510/689]  eta: 0:04:42  lr: 0.000088  loss: 0.2227 (0.2101)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [520/689]  eta: 0:04:26  lr: 0.000088  loss: 0.2089 (0.2102)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [530/689]  eta: 0:04:10  lr: 0.000088  loss: 0.1997 (0.2103)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [540/689]  eta: 0:03:54  lr: 0.000088  loss: 0.2012 (0.2101)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [550/689]  eta: 0:03:39  lr: 0.000088  loss: 0.1946 (0.2100)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [560/689]  eta: 0:03:23  lr: 0.000088  loss: 0.2021 (0.2101)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [570/689]  eta: 0:03:07  lr: 0.000088  loss: 0.2065 (0.2102)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [580/689]  eta: 0:02:51  lr: 0.000088  loss: 0.2021 (0.2100)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [590/689]  eta: 0:02:35  lr: 0.000088  loss: 0.2028 (0.2100)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [600/689]  eta: 0:02:20  lr: 0.000088  loss: 0.2062 (0.2102)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [610/689]  eta: 0:02:04  lr: 0.000088  loss: 0.2099 (0.2104)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [620/689]  eta: 0:01:48  lr: 0.000088  loss: 0.2099 (0.2104)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [630/689]  eta: 0:01:32  lr: 0.000088  loss: 0.2075 (0.2104)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [640/689]  eta: 0:01:17  lr: 0.000088  loss: 0.2112 (0.2107)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [650/689]  eta: 0:01:01  lr: 0.000088  loss: 0.2176 (0.2107)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [660/689]  eta: 0:00:45  lr: 0.000088  loss: 0.2107 (0.2108)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [670/689]  eta: 0:00:29  lr: 0.000088  loss: 0.2086 (0.2108)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [680/689]  eta: 0:00:14  lr: 0.000088  loss: 0.2060 (0.2108)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208]  [688/689]  eta: 0:00:01  lr: 0.000088  loss: 0.2016 (0.2107)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:208] Total time: 0:18:05 (1.5761 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: 0.2016 (0.2107)\n",
      "Valid: [epoch:208]  [ 0/14]  eta: 0:00:14  loss: 0.2173 (0.2173)  time: 1.0131  data: 0.3415  max mem: 39763\n",
      "Valid: [epoch:208]  [13/14]  eta: 0:00:00  loss: 0.1964 (0.1996)  time: 0.1143  data: 0.0244  max mem: 39763\n",
      "Valid: [epoch:208] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.1964 (0.1996)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_208_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.200%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:209]  [  0/689]  eta: 0:12:08  lr: 0.000088  loss: 0.1952 (0.1952)  time: 1.0568  data: 0.5815  max mem: 39763\n",
      "Train: [epoch:209]  [ 10/689]  eta: 0:17:16  lr: 0.000088  loss: 0.1993 (0.1996)  time: 1.5267  data: 0.0530  max mem: 39763\n",
      "Train: [epoch:209]  [ 20/689]  eta: 0:17:16  lr: 0.000088  loss: 0.1978 (0.2017)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [ 30/689]  eta: 0:17:06  lr: 0.000088  loss: 0.2057 (0.2084)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [ 40/689]  eta: 0:16:53  lr: 0.000088  loss: 0.2057 (0.2075)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [ 50/689]  eta: 0:16:39  lr: 0.000088  loss: 0.2054 (0.2066)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [ 60/689]  eta: 0:16:25  lr: 0.000088  loss: 0.2032 (0.2053)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [ 70/689]  eta: 0:16:10  lr: 0.000088  loss: 0.2031 (0.2066)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [ 80/689]  eta: 0:15:55  lr: 0.000088  loss: 0.2031 (0.2079)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [ 90/689]  eta: 0:15:40  lr: 0.000088  loss: 0.2004 (0.2072)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [100/689]  eta: 0:15:24  lr: 0.000088  loss: 0.2048 (0.2090)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [110/689]  eta: 0:15:09  lr: 0.000088  loss: 0.2157 (0.2100)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [120/689]  eta: 0:14:54  lr: 0.000088  loss: 0.2076 (0.2102)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [130/689]  eta: 0:14:38  lr: 0.000088  loss: 0.2071 (0.2110)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [140/689]  eta: 0:14:23  lr: 0.000088  loss: 0.2071 (0.2108)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [150/689]  eta: 0:14:07  lr: 0.000088  loss: 0.2108 (0.2108)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [160/689]  eta: 0:13:51  lr: 0.000088  loss: 0.2108 (0.2113)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [170/689]  eta: 0:13:36  lr: 0.000088  loss: 0.2085 (0.2110)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [180/689]  eta: 0:13:20  lr: 0.000088  loss: 0.2063 (0.2110)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [190/689]  eta: 0:13:04  lr: 0.000088  loss: 0.2069 (0.2119)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [200/689]  eta: 0:12:49  lr: 0.000088  loss: 0.2050 (0.2112)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [210/689]  eta: 0:12:33  lr: 0.000088  loss: 0.1981 (0.2108)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [220/689]  eta: 0:12:18  lr: 0.000088  loss: 0.2057 (0.2113)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [230/689]  eta: 0:12:02  lr: 0.000088  loss: 0.2140 (0.2120)  time: 1.5768  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:209]  [240/689]  eta: 0:11:46  lr: 0.000088  loss: 0.2071 (0.2123)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [250/689]  eta: 0:11:30  lr: 0.000088  loss: 0.2142 (0.2128)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [260/689]  eta: 0:11:15  lr: 0.000088  loss: 0.2142 (0.2129)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [270/689]  eta: 0:10:59  lr: 0.000088  loss: 0.2100 (0.2126)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [280/689]  eta: 0:10:43  lr: 0.000088  loss: 0.2056 (0.2124)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [290/689]  eta: 0:10:28  lr: 0.000088  loss: 0.1998 (0.2122)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [300/689]  eta: 0:10:12  lr: 0.000088  loss: 0.1993 (0.2121)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [310/689]  eta: 0:09:56  lr: 0.000088  loss: 0.2032 (0.2118)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [320/689]  eta: 0:09:41  lr: 0.000088  loss: 0.2094 (0.2122)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [330/689]  eta: 0:09:25  lr: 0.000088  loss: 0.2065 (0.2121)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [340/689]  eta: 0:09:09  lr: 0.000088  loss: 0.2065 (0.2119)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [350/689]  eta: 0:08:53  lr: 0.000088  loss: 0.2082 (0.2115)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [360/689]  eta: 0:08:38  lr: 0.000088  loss: 0.1952 (0.2113)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [370/689]  eta: 0:08:22  lr: 0.000088  loss: 0.1961 (0.2114)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [380/689]  eta: 0:08:06  lr: 0.000088  loss: 0.1961 (0.2115)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [390/689]  eta: 0:07:51  lr: 0.000088  loss: 0.2095 (0.2116)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [400/689]  eta: 0:07:35  lr: 0.000088  loss: 0.2095 (0.2115)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [410/689]  eta: 0:07:19  lr: 0.000088  loss: 0.2073 (0.2116)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [420/689]  eta: 0:07:03  lr: 0.000088  loss: 0.1914 (0.2114)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [430/689]  eta: 0:06:48  lr: 0.000088  loss: 0.2061 (0.2114)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [440/689]  eta: 0:06:32  lr: 0.000088  loss: 0.2168 (0.2117)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [450/689]  eta: 0:06:16  lr: 0.000088  loss: 0.2185 (0.2116)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [460/689]  eta: 0:06:00  lr: 0.000088  loss: 0.2111 (0.2121)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [470/689]  eta: 0:05:45  lr: 0.000088  loss: 0.2076 (0.2119)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [480/689]  eta: 0:05:29  lr: 0.000088  loss: 0.2087 (0.2120)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [490/689]  eta: 0:05:13  lr: 0.000088  loss: 0.2102 (0.2118)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [500/689]  eta: 0:04:57  lr: 0.000088  loss: 0.2093 (0.2120)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [510/689]  eta: 0:04:42  lr: 0.000088  loss: 0.2098 (0.2121)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [520/689]  eta: 0:04:26  lr: 0.000088  loss: 0.2060 (0.2122)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [530/689]  eta: 0:04:10  lr: 0.000088  loss: 0.2081 (0.2121)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [540/689]  eta: 0:03:54  lr: 0.000088  loss: 0.2107 (0.2123)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [550/689]  eta: 0:03:39  lr: 0.000088  loss: 0.2114 (0.2124)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [560/689]  eta: 0:03:23  lr: 0.000088  loss: 0.2066 (0.2122)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [570/689]  eta: 0:03:07  lr: 0.000088  loss: 0.2009 (0.2122)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [580/689]  eta: 0:02:51  lr: 0.000088  loss: 0.2012 (0.2121)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [590/689]  eta: 0:02:36  lr: 0.000088  loss: 0.2044 (0.2121)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [600/689]  eta: 0:02:20  lr: 0.000088  loss: 0.2129 (0.2123)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [610/689]  eta: 0:02:04  lr: 0.000088  loss: 0.2030 (0.2120)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [620/689]  eta: 0:01:48  lr: 0.000088  loss: 0.1972 (0.2118)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [630/689]  eta: 0:01:33  lr: 0.000088  loss: 0.1995 (0.2117)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [640/689]  eta: 0:01:17  lr: 0.000088  loss: 0.2075 (0.2117)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [650/689]  eta: 0:01:01  lr: 0.000088  loss: 0.2008 (0.2115)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [660/689]  eta: 0:00:45  lr: 0.000088  loss: 0.2057 (0.2117)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [670/689]  eta: 0:00:29  lr: 0.000088  loss: 0.2081 (0.2117)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [680/689]  eta: 0:00:14  lr: 0.000088  loss: 0.2042 (0.2116)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209]  [688/689]  eta: 0:00:01  lr: 0.000088  loss: 0.2012 (0.2116)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:209] Total time: 0:18:06 (1.5767 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: 0.2012 (0.2116)\n",
      "Valid: [epoch:209]  [ 0/14]  eta: 0:00:14  loss: 0.1970 (0.1970)  time: 1.0135  data: 0.3514  max mem: 39763\n",
      "Valid: [epoch:209]  [13/14]  eta: 0:00:00  loss: 0.1977 (0.2009)  time: 0.1144  data: 0.0252  max mem: 39763\n",
      "Valid: [epoch:209] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.1977 (0.2009)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_209_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.201%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:210]  [  0/689]  eta: 0:11:30  lr: 0.000088  loss: 0.1952 (0.1952)  time: 1.0022  data: 0.5234  max mem: 39763\n",
      "Train: [epoch:210]  [ 10/689]  eta: 0:17:14  lr: 0.000088  loss: 0.1952 (0.2091)  time: 1.5242  data: 0.0477  max mem: 39763\n",
      "Train: [epoch:210]  [ 20/689]  eta: 0:17:16  lr: 0.000088  loss: 0.1939 (0.2091)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [ 30/689]  eta: 0:17:06  lr: 0.000088  loss: 0.2066 (0.2128)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [ 40/689]  eta: 0:16:54  lr: 0.000088  loss: 0.2088 (0.2121)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [ 50/689]  eta: 0:16:40  lr: 0.000088  loss: 0.1991 (0.2097)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [ 60/689]  eta: 0:16:25  lr: 0.000088  loss: 0.1991 (0.2103)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [ 70/689]  eta: 0:16:10  lr: 0.000088  loss: 0.2107 (0.2100)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [ 80/689]  eta: 0:15:55  lr: 0.000088  loss: 0.2107 (0.2101)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [ 90/689]  eta: 0:15:40  lr: 0.000088  loss: 0.2203 (0.2118)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [100/689]  eta: 0:15:25  lr: 0.000088  loss: 0.2226 (0.2128)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [110/689]  eta: 0:15:09  lr: 0.000088  loss: 0.2215 (0.2136)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [120/689]  eta: 0:14:54  lr: 0.000088  loss: 0.2180 (0.2131)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [130/689]  eta: 0:14:38  lr: 0.000088  loss: 0.2180 (0.2139)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [140/689]  eta: 0:14:23  lr: 0.000088  loss: 0.2205 (0.2143)  time: 1.5763  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:210]  [150/689]  eta: 0:14:07  lr: 0.000088  loss: 0.2190 (0.2145)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [160/689]  eta: 0:13:52  lr: 0.000088  loss: 0.2084 (0.2148)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [170/689]  eta: 0:13:36  lr: 0.000088  loss: 0.2070 (0.2144)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [180/689]  eta: 0:13:20  lr: 0.000088  loss: 0.2133 (0.2147)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [190/689]  eta: 0:13:05  lr: 0.000088  loss: 0.2218 (0.2150)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [200/689]  eta: 0:12:49  lr: 0.000088  loss: 0.2214 (0.2151)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [210/689]  eta: 0:12:33  lr: 0.000088  loss: 0.2018 (0.2143)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [220/689]  eta: 0:12:18  lr: 0.000088  loss: 0.1962 (0.2143)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [230/689]  eta: 0:12:02  lr: 0.000088  loss: 0.2098 (0.2147)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [240/689]  eta: 0:11:46  lr: 0.000088  loss: 0.2087 (0.2145)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [250/689]  eta: 0:11:31  lr: 0.000088  loss: 0.2087 (0.2145)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [260/689]  eta: 0:11:15  lr: 0.000088  loss: 0.2100 (0.2146)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [270/689]  eta: 0:10:59  lr: 0.000088  loss: 0.2072 (0.2144)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [280/689]  eta: 0:10:44  lr: 0.000088  loss: 0.2097 (0.2147)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [290/689]  eta: 0:10:28  lr: 0.000088  loss: 0.2091 (0.2146)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [300/689]  eta: 0:10:12  lr: 0.000088  loss: 0.2091 (0.2145)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [310/689]  eta: 0:09:56  lr: 0.000088  loss: 0.2071 (0.2141)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [320/689]  eta: 0:09:41  lr: 0.000088  loss: 0.2028 (0.2142)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [330/689]  eta: 0:09:25  lr: 0.000088  loss: 0.2112 (0.2142)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [340/689]  eta: 0:09:09  lr: 0.000088  loss: 0.1970 (0.2140)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [350/689]  eta: 0:08:53  lr: 0.000088  loss: 0.2012 (0.2140)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [360/689]  eta: 0:08:38  lr: 0.000088  loss: 0.2012 (0.2137)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [370/689]  eta: 0:08:22  lr: 0.000088  loss: 0.2032 (0.2137)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [380/689]  eta: 0:08:06  lr: 0.000088  loss: 0.2035 (0.2137)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [390/689]  eta: 0:07:51  lr: 0.000088  loss: 0.2025 (0.2136)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [400/689]  eta: 0:07:35  lr: 0.000088  loss: 0.2038 (0.2135)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [410/689]  eta: 0:07:19  lr: 0.000088  loss: 0.2134 (0.2136)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [420/689]  eta: 0:07:03  lr: 0.000088  loss: 0.2065 (0.2133)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [430/689]  eta: 0:06:48  lr: 0.000088  loss: 0.2065 (0.2133)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [440/689]  eta: 0:06:32  lr: 0.000088  loss: 0.2031 (0.2132)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [450/689]  eta: 0:06:16  lr: 0.000088  loss: 0.1984 (0.2128)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [460/689]  eta: 0:06:00  lr: 0.000088  loss: 0.2082 (0.2131)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [470/689]  eta: 0:05:45  lr: 0.000088  loss: 0.2175 (0.2131)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [480/689]  eta: 0:05:29  lr: 0.000088  loss: 0.2097 (0.2131)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [490/689]  eta: 0:05:13  lr: 0.000088  loss: 0.2074 (0.2130)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [500/689]  eta: 0:04:57  lr: 0.000088  loss: 0.2074 (0.2131)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [510/689]  eta: 0:04:42  lr: 0.000088  loss: 0.2058 (0.2129)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [520/689]  eta: 0:04:26  lr: 0.000088  loss: 0.2004 (0.2128)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [530/689]  eta: 0:04:10  lr: 0.000088  loss: 0.2066 (0.2128)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [540/689]  eta: 0:03:54  lr: 0.000088  loss: 0.2084 (0.2129)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [550/689]  eta: 0:03:39  lr: 0.000088  loss: 0.2069 (0.2126)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [560/689]  eta: 0:03:23  lr: 0.000088  loss: 0.2104 (0.2128)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [570/689]  eta: 0:03:07  lr: 0.000088  loss: 0.2015 (0.2126)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [580/689]  eta: 0:02:51  lr: 0.000088  loss: 0.1970 (0.2125)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [590/689]  eta: 0:02:36  lr: 0.000088  loss: 0.2065 (0.2128)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [600/689]  eta: 0:02:20  lr: 0.000088  loss: 0.2130 (0.2129)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [610/689]  eta: 0:02:04  lr: 0.000088  loss: 0.2092 (0.2128)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [620/689]  eta: 0:01:48  lr: 0.000088  loss: 0.2038 (0.2128)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [630/689]  eta: 0:01:33  lr: 0.000088  loss: 0.2024 (0.2127)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [640/689]  eta: 0:01:17  lr: 0.000088  loss: 0.2086 (0.2129)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [650/689]  eta: 0:01:01  lr: 0.000088  loss: 0.2162 (0.2129)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [660/689]  eta: 0:00:45  lr: 0.000088  loss: 0.2090 (0.2129)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [670/689]  eta: 0:00:29  lr: 0.000088  loss: 0.2117 (0.2131)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [680/689]  eta: 0:00:14  lr: 0.000088  loss: 0.2091 (0.2132)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210]  [688/689]  eta: 0:00:01  lr: 0.000088  loss: 0.2006 (0.2129)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:210] Total time: 0:18:06 (1.5768 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: 0.2006 (0.2129)\n",
      "Valid: [epoch:210]  [ 0/14]  eta: 0:00:14  loss: 0.1985 (0.1985)  time: 1.0156  data: 0.3819  max mem: 39763\n",
      "Valid: [epoch:210]  [13/14]  eta: 0:00:00  loss: 0.1985 (0.2022)  time: 0.1145  data: 0.0273  max mem: 39763\n",
      "Valid: [epoch:210] Total time: 0:00:01 (0.1218 s / it)\n",
      "Averaged stats: loss: 0.1985 (0.2022)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_210_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.202%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:211]  [  0/689]  eta: 0:11:35  lr: 0.000088  loss: 0.2084 (0.2084)  time: 1.0089  data: 0.5310  max mem: 39763\n",
      "Train: [epoch:211]  [ 10/689]  eta: 0:17:15  lr: 0.000088  loss: 0.2012 (0.2072)  time: 1.5246  data: 0.0484  max mem: 39763\n",
      "Train: [epoch:211]  [ 20/689]  eta: 0:17:16  lr: 0.000088  loss: 0.2064 (0.2146)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [ 30/689]  eta: 0:17:07  lr: 0.000088  loss: 0.2051 (0.2102)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [ 40/689]  eta: 0:16:54  lr: 0.000088  loss: 0.1991 (0.2094)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [ 50/689]  eta: 0:16:40  lr: 0.000088  loss: 0.2153 (0.2130)  time: 1.5770  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:211]  [ 60/689]  eta: 0:16:26  lr: 0.000088  loss: 0.2178 (0.2124)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [ 70/689]  eta: 0:16:11  lr: 0.000088  loss: 0.2034 (0.2118)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [ 80/689]  eta: 0:15:56  lr: 0.000088  loss: 0.2039 (0.2127)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [ 90/689]  eta: 0:15:41  lr: 0.000088  loss: 0.2293 (0.2131)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [100/689]  eta: 0:15:25  lr: 0.000088  loss: 0.2270 (0.2142)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [110/689]  eta: 0:15:10  lr: 0.000088  loss: 0.2207 (0.2145)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [120/689]  eta: 0:14:54  lr: 0.000088  loss: 0.2160 (0.2141)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [130/689]  eta: 0:14:39  lr: 0.000088  loss: 0.2097 (0.2140)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [140/689]  eta: 0:14:23  lr: 0.000088  loss: 0.2097 (0.2145)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [150/689]  eta: 0:14:08  lr: 0.000088  loss: 0.2122 (0.2142)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [160/689]  eta: 0:13:52  lr: 0.000088  loss: 0.2074 (0.2138)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [170/689]  eta: 0:13:37  lr: 0.000088  loss: 0.2152 (0.2142)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [180/689]  eta: 0:13:21  lr: 0.000088  loss: 0.2187 (0.2142)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [190/689]  eta: 0:13:05  lr: 0.000088  loss: 0.2187 (0.2146)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [200/689]  eta: 0:12:50  lr: 0.000088  loss: 0.2116 (0.2140)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [210/689]  eta: 0:12:34  lr: 0.000088  loss: 0.2031 (0.2136)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [220/689]  eta: 0:12:18  lr: 0.000088  loss: 0.2035 (0.2137)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [230/689]  eta: 0:12:02  lr: 0.000088  loss: 0.2091 (0.2135)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [240/689]  eta: 0:11:47  lr: 0.000088  loss: 0.2109 (0.2135)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [250/689]  eta: 0:11:31  lr: 0.000088  loss: 0.2137 (0.2142)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [260/689]  eta: 0:11:15  lr: 0.000088  loss: 0.2221 (0.2144)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [270/689]  eta: 0:11:00  lr: 0.000088  loss: 0.2230 (0.2150)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [280/689]  eta: 0:10:44  lr: 0.000088  loss: 0.2230 (0.2153)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [290/689]  eta: 0:10:28  lr: 0.000088  loss: 0.2063 (0.2149)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [300/689]  eta: 0:10:12  lr: 0.000088  loss: 0.2034 (0.2148)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [310/689]  eta: 0:09:57  lr: 0.000088  loss: 0.2040 (0.2145)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [320/689]  eta: 0:09:41  lr: 0.000088  loss: 0.2023 (0.2143)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [330/689]  eta: 0:09:25  lr: 0.000088  loss: 0.2035 (0.2142)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [340/689]  eta: 0:09:09  lr: 0.000088  loss: 0.2076 (0.2141)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [350/689]  eta: 0:08:54  lr: 0.000088  loss: 0.2118 (0.2140)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [360/689]  eta: 0:08:38  lr: 0.000088  loss: 0.2122 (0.2141)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [370/689]  eta: 0:08:22  lr: 0.000088  loss: 0.2081 (0.2138)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [380/689]  eta: 0:08:06  lr: 0.000088  loss: 0.2025 (0.2138)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [390/689]  eta: 0:07:51  lr: 0.000088  loss: 0.2175 (0.2139)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [400/689]  eta: 0:07:35  lr: 0.000088  loss: 0.2152 (0.2139)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [410/689]  eta: 0:07:19  lr: 0.000088  loss: 0.2120 (0.2140)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [420/689]  eta: 0:07:03  lr: 0.000088  loss: 0.2120 (0.2141)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [430/689]  eta: 0:06:48  lr: 0.000088  loss: 0.2096 (0.2141)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [440/689]  eta: 0:06:32  lr: 0.000088  loss: 0.2092 (0.2141)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [450/689]  eta: 0:06:16  lr: 0.000088  loss: 0.2063 (0.2142)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [460/689]  eta: 0:06:00  lr: 0.000088  loss: 0.2063 (0.2141)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [470/689]  eta: 0:05:45  lr: 0.000088  loss: 0.2081 (0.2141)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [480/689]  eta: 0:05:29  lr: 0.000088  loss: 0.2082 (0.2141)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [490/689]  eta: 0:05:13  lr: 0.000088  loss: 0.2084 (0.2140)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [500/689]  eta: 0:04:57  lr: 0.000088  loss: 0.2123 (0.2140)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [510/689]  eta: 0:04:42  lr: 0.000088  loss: 0.2108 (0.2140)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [520/689]  eta: 0:04:26  lr: 0.000088  loss: 0.2171 (0.2142)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [530/689]  eta: 0:04:10  lr: 0.000088  loss: 0.2141 (0.2141)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [540/689]  eta: 0:03:54  lr: 0.000088  loss: 0.2124 (0.2142)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [550/689]  eta: 0:03:39  lr: 0.000088  loss: 0.2106 (0.2140)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [560/689]  eta: 0:03:23  lr: 0.000088  loss: 0.2011 (0.2138)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [570/689]  eta: 0:03:07  lr: 0.000088  loss: 0.1996 (0.2136)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [580/689]  eta: 0:02:51  lr: 0.000088  loss: 0.1997 (0.2135)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [590/689]  eta: 0:02:36  lr: 0.000088  loss: 0.2167 (0.2138)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [600/689]  eta: 0:02:20  lr: 0.000088  loss: 0.2247 (0.2139)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [610/689]  eta: 0:02:04  lr: 0.000088  loss: 0.1938 (0.2138)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [620/689]  eta: 0:01:48  lr: 0.000088  loss: 0.1920 (0.2136)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [630/689]  eta: 0:01:32  lr: 0.000088  loss: 0.2013 (0.2136)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [640/689]  eta: 0:01:17  lr: 0.000088  loss: 0.2171 (0.2137)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [650/689]  eta: 0:01:01  lr: 0.000088  loss: 0.2070 (0.2135)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [660/689]  eta: 0:00:45  lr: 0.000088  loss: 0.2017 (0.2135)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [670/689]  eta: 0:00:29  lr: 0.000088  loss: 0.1976 (0.2134)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [680/689]  eta: 0:00:14  lr: 0.000088  loss: 0.1977 (0.2136)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211]  [688/689]  eta: 0:00:01  lr: 0.000088  loss: 0.1996 (0.2134)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:211] Total time: 0:18:06 (1.5766 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: 0.1996 (0.2134)\n",
      "Valid: [epoch:211]  [ 0/14]  eta: 0:00:14  loss: 0.2119 (0.2119)  time: 1.0096  data: 0.3848  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:211]  [13/14]  eta: 0:00:00  loss: 0.1993 (0.2024)  time: 0.1140  data: 0.0275  max mem: 39763\n",
      "Valid: [epoch:211] Total time: 0:00:01 (0.1218 s / it)\n",
      "Averaged stats: loss: 0.1993 (0.2024)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_211_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.202%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:212]  [  0/689]  eta: 0:11:28  lr: 0.000088  loss: 0.1874 (0.1874)  time: 0.9986  data: 0.5218  max mem: 39763\n",
      "Train: [epoch:212]  [ 10/689]  eta: 0:17:13  lr: 0.000088  loss: 0.2032 (0.2082)  time: 1.5227  data: 0.0475  max mem: 39763\n",
      "Train: [epoch:212]  [ 20/689]  eta: 0:17:16  lr: 0.000088  loss: 0.2032 (0.2079)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [ 30/689]  eta: 0:17:06  lr: 0.000088  loss: 0.2043 (0.2112)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [ 40/689]  eta: 0:16:53  lr: 0.000088  loss: 0.2043 (0.2092)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [ 50/689]  eta: 0:16:39  lr: 0.000088  loss: 0.2053 (0.2097)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [ 60/689]  eta: 0:16:25  lr: 0.000088  loss: 0.2121 (0.2122)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [ 70/689]  eta: 0:16:10  lr: 0.000088  loss: 0.2121 (0.2120)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [ 80/689]  eta: 0:15:55  lr: 0.000088  loss: 0.2018 (0.2112)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [ 90/689]  eta: 0:15:40  lr: 0.000088  loss: 0.2015 (0.2108)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [100/689]  eta: 0:15:25  lr: 0.000088  loss: 0.2120 (0.2120)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [110/689]  eta: 0:15:09  lr: 0.000088  loss: 0.2177 (0.2126)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [120/689]  eta: 0:14:54  lr: 0.000088  loss: 0.2207 (0.2132)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [130/689]  eta: 0:14:39  lr: 0.000088  loss: 0.2221 (0.2144)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [140/689]  eta: 0:14:23  lr: 0.000088  loss: 0.2318 (0.2164)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [150/689]  eta: 0:14:07  lr: 0.000088  loss: 0.2258 (0.2167)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [160/689]  eta: 0:13:52  lr: 0.000088  loss: 0.2244 (0.2173)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [170/689]  eta: 0:13:36  lr: 0.000088  loss: 0.2259 (0.2180)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [180/689]  eta: 0:13:21  lr: 0.000088  loss: 0.2223 (0.2181)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [190/689]  eta: 0:13:05  lr: 0.000088  loss: 0.2117 (0.2182)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [200/689]  eta: 0:12:49  lr: 0.000088  loss: 0.2015 (0.2177)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [210/689]  eta: 0:12:34  lr: 0.000088  loss: 0.2079 (0.2181)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [220/689]  eta: 0:12:18  lr: 0.000088  loss: 0.2134 (0.2178)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [230/689]  eta: 0:12:02  lr: 0.000088  loss: 0.2107 (0.2182)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [240/689]  eta: 0:11:47  lr: 0.000088  loss: 0.2179 (0.2185)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [250/689]  eta: 0:11:31  lr: 0.000088  loss: 0.2144 (0.2184)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [260/689]  eta: 0:11:15  lr: 0.000088  loss: 0.2132 (0.2186)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [270/689]  eta: 0:11:00  lr: 0.000088  loss: 0.2165 (0.2186)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [280/689]  eta: 0:10:44  lr: 0.000088  loss: 0.2116 (0.2187)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [290/689]  eta: 0:10:28  lr: 0.000088  loss: 0.2138 (0.2190)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [300/689]  eta: 0:10:13  lr: 0.000088  loss: 0.2115 (0.2186)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [310/689]  eta: 0:09:57  lr: 0.000088  loss: 0.2015 (0.2184)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [320/689]  eta: 0:09:41  lr: 0.000088  loss: 0.2097 (0.2183)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [330/689]  eta: 0:09:25  lr: 0.000088  loss: 0.2173 (0.2185)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [340/689]  eta: 0:09:10  lr: 0.000088  loss: 0.2168 (0.2184)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [350/689]  eta: 0:08:54  lr: 0.000088  loss: 0.2097 (0.2182)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [360/689]  eta: 0:08:38  lr: 0.000088  loss: 0.2037 (0.2183)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [370/689]  eta: 0:08:22  lr: 0.000088  loss: 0.2030 (0.2180)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [380/689]  eta: 0:08:07  lr: 0.000088  loss: 0.2043 (0.2179)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [390/689]  eta: 0:07:51  lr: 0.000088  loss: 0.2043 (0.2176)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [400/689]  eta: 0:07:35  lr: 0.000088  loss: 0.2029 (0.2174)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [410/689]  eta: 0:07:19  lr: 0.000088  loss: 0.2106 (0.2177)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [420/689]  eta: 0:07:04  lr: 0.000088  loss: 0.2108 (0.2175)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [430/689]  eta: 0:06:48  lr: 0.000088  loss: 0.2133 (0.2176)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [440/689]  eta: 0:06:32  lr: 0.000088  loss: 0.2141 (0.2177)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [450/689]  eta: 0:06:16  lr: 0.000088  loss: 0.2114 (0.2174)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [460/689]  eta: 0:06:01  lr: 0.000088  loss: 0.2114 (0.2175)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [470/689]  eta: 0:05:45  lr: 0.000088  loss: 0.2137 (0.2174)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [480/689]  eta: 0:05:29  lr: 0.000088  loss: 0.2168 (0.2176)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [490/689]  eta: 0:05:13  lr: 0.000088  loss: 0.2330 (0.2180)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [500/689]  eta: 0:04:58  lr: 0.000088  loss: 0.2276 (0.2180)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [510/689]  eta: 0:04:42  lr: 0.000088  loss: 0.2126 (0.2180)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [520/689]  eta: 0:04:26  lr: 0.000088  loss: 0.2238 (0.2184)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [530/689]  eta: 0:04:10  lr: 0.000088  loss: 0.2254 (0.2183)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [540/689]  eta: 0:03:54  lr: 0.000088  loss: 0.2124 (0.2183)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [550/689]  eta: 0:03:39  lr: 0.000088  loss: 0.2063 (0.2182)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [560/689]  eta: 0:03:23  lr: 0.000088  loss: 0.2051 (0.2182)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [570/689]  eta: 0:03:07  lr: 0.000088  loss: 0.2073 (0.2182)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [580/689]  eta: 0:02:51  lr: 0.000088  loss: 0.2073 (0.2181)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [590/689]  eta: 0:02:36  lr: 0.000088  loss: 0.2012 (0.2179)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [600/689]  eta: 0:02:20  lr: 0.000088  loss: 0.2086 (0.2178)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [610/689]  eta: 0:02:04  lr: 0.000088  loss: 0.2109 (0.2178)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [620/689]  eta: 0:01:48  lr: 0.000088  loss: 0.2059 (0.2177)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:212]  [630/689]  eta: 0:01:33  lr: 0.000088  loss: 0.2121 (0.2177)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [640/689]  eta: 0:01:17  lr: 0.000088  loss: 0.2123 (0.2179)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [650/689]  eta: 0:01:01  lr: 0.000088  loss: 0.2033 (0.2176)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [660/689]  eta: 0:00:45  lr: 0.000088  loss: 0.2031 (0.2176)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [670/689]  eta: 0:00:29  lr: 0.000088  loss: 0.2065 (0.2176)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [680/689]  eta: 0:00:14  lr: 0.000088  loss: 0.2085 (0.2176)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212]  [688/689]  eta: 0:00:01  lr: 0.000088  loss: 0.2054 (0.2174)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:212] Total time: 0:18:06 (1.5775 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: 0.2054 (0.2174)\n",
      "Valid: [epoch:212]  [ 0/14]  eta: 0:00:14  loss: 0.1908 (0.1908)  time: 1.0308  data: 0.3822  max mem: 39763\n",
      "Valid: [epoch:212]  [13/14]  eta: 0:00:00  loss: 0.2020 (0.2051)  time: 0.1155  data: 0.0274  max mem: 39763\n",
      "Valid: [epoch:212] Total time: 0:00:01 (0.1248 s / it)\n",
      "Averaged stats: loss: 0.2020 (0.2051)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_212_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.205%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:213]  [  0/689]  eta: 0:12:03  lr: 0.000088  loss: 0.1989 (0.1989)  time: 1.0496  data: 0.5718  max mem: 39763\n",
      "Train: [epoch:213]  [ 10/689]  eta: 0:17:16  lr: 0.000088  loss: 0.2004 (0.2100)  time: 1.5264  data: 0.0521  max mem: 39763\n",
      "Train: [epoch:213]  [ 20/689]  eta: 0:17:16  lr: 0.000088  loss: 0.2069 (0.2099)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [ 30/689]  eta: 0:17:07  lr: 0.000088  loss: 0.2095 (0.2120)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [ 40/689]  eta: 0:16:54  lr: 0.000088  loss: 0.2029 (0.2106)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [ 50/689]  eta: 0:16:40  lr: 0.000088  loss: 0.2090 (0.2126)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [ 60/689]  eta: 0:16:25  lr: 0.000088  loss: 0.2166 (0.2126)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [ 70/689]  eta: 0:16:11  lr: 0.000088  loss: 0.2107 (0.2124)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [ 80/689]  eta: 0:15:55  lr: 0.000088  loss: 0.2085 (0.2112)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [ 90/689]  eta: 0:15:40  lr: 0.000088  loss: 0.1981 (0.2096)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [100/689]  eta: 0:15:25  lr: 0.000088  loss: 0.1988 (0.2100)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [110/689]  eta: 0:15:10  lr: 0.000088  loss: 0.2082 (0.2095)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [120/689]  eta: 0:14:54  lr: 0.000088  loss: 0.2018 (0.2090)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [130/689]  eta: 0:14:39  lr: 0.000088  loss: 0.2019 (0.2096)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [140/689]  eta: 0:14:23  lr: 0.000088  loss: 0.2020 (0.2092)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [150/689]  eta: 0:14:07  lr: 0.000088  loss: 0.2082 (0.2102)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [160/689]  eta: 0:13:52  lr: 0.000088  loss: 0.2260 (0.2112)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [170/689]  eta: 0:13:36  lr: 0.000088  loss: 0.2248 (0.2115)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [180/689]  eta: 0:13:21  lr: 0.000088  loss: 0.2160 (0.2122)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [190/689]  eta: 0:13:05  lr: 0.000088  loss: 0.2162 (0.2130)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [200/689]  eta: 0:12:49  lr: 0.000088  loss: 0.2147 (0.2128)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [210/689]  eta: 0:12:34  lr: 0.000088  loss: 0.2015 (0.2126)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [220/689]  eta: 0:12:18  lr: 0.000088  loss: 0.2040 (0.2127)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [230/689]  eta: 0:12:02  lr: 0.000088  loss: 0.2224 (0.2131)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [240/689]  eta: 0:11:47  lr: 0.000088  loss: 0.2118 (0.2130)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [250/689]  eta: 0:11:31  lr: 0.000088  loss: 0.2078 (0.2129)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [260/689]  eta: 0:11:15  lr: 0.000088  loss: 0.2088 (0.2133)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [270/689]  eta: 0:10:59  lr: 0.000088  loss: 0.2088 (0.2132)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [280/689]  eta: 0:10:44  lr: 0.000088  loss: 0.2011 (0.2126)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [290/689]  eta: 0:10:28  lr: 0.000088  loss: 0.2034 (0.2129)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [300/689]  eta: 0:10:12  lr: 0.000088  loss: 0.2108 (0.2127)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [310/689]  eta: 0:09:57  lr: 0.000088  loss: 0.2013 (0.2129)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [320/689]  eta: 0:09:41  lr: 0.000088  loss: 0.2111 (0.2129)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [330/689]  eta: 0:09:25  lr: 0.000088  loss: 0.2091 (0.2129)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [340/689]  eta: 0:09:09  lr: 0.000088  loss: 0.2048 (0.2130)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [350/689]  eta: 0:08:54  lr: 0.000088  loss: 0.2056 (0.2132)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [360/689]  eta: 0:08:38  lr: 0.000088  loss: 0.2186 (0.2133)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [370/689]  eta: 0:08:22  lr: 0.000088  loss: 0.2104 (0.2132)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [380/689]  eta: 0:08:06  lr: 0.000088  loss: 0.2104 (0.2132)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [390/689]  eta: 0:07:51  lr: 0.000088  loss: 0.2084 (0.2134)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [400/689]  eta: 0:07:35  lr: 0.000088  loss: 0.2179 (0.2138)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [410/689]  eta: 0:07:19  lr: 0.000088  loss: 0.2263 (0.2140)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [420/689]  eta: 0:07:03  lr: 0.000088  loss: 0.2111 (0.2141)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [430/689]  eta: 0:06:48  lr: 0.000088  loss: 0.2103 (0.2142)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [440/689]  eta: 0:06:32  lr: 0.000088  loss: 0.2026 (0.2141)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [450/689]  eta: 0:06:16  lr: 0.000088  loss: 0.2038 (0.2142)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [460/689]  eta: 0:06:00  lr: 0.000088  loss: 0.2119 (0.2141)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [470/689]  eta: 0:05:45  lr: 0.000088  loss: 0.2072 (0.2140)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [480/689]  eta: 0:05:29  lr: 0.000088  loss: 0.2024 (0.2141)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [490/689]  eta: 0:05:13  lr: 0.000088  loss: 0.1987 (0.2140)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [500/689]  eta: 0:04:57  lr: 0.000088  loss: 0.1987 (0.2138)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [510/689]  eta: 0:04:42  lr: 0.000088  loss: 0.2035 (0.2138)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [520/689]  eta: 0:04:26  lr: 0.000088  loss: 0.2151 (0.2138)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [530/689]  eta: 0:04:10  lr: 0.000088  loss: 0.2179 (0.2141)  time: 1.5774  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:213]  [540/689]  eta: 0:03:54  lr: 0.000088  loss: 0.2133 (0.2141)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [550/689]  eta: 0:03:39  lr: 0.000088  loss: 0.2230 (0.2144)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [560/689]  eta: 0:03:23  lr: 0.000088  loss: 0.2230 (0.2146)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [570/689]  eta: 0:03:07  lr: 0.000088  loss: 0.2098 (0.2146)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [580/689]  eta: 0:02:51  lr: 0.000088  loss: 0.2061 (0.2147)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [590/689]  eta: 0:02:36  lr: 0.000088  loss: 0.2160 (0.2149)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [600/689]  eta: 0:02:20  lr: 0.000088  loss: 0.2160 (0.2150)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [610/689]  eta: 0:02:04  lr: 0.000088  loss: 0.2136 (0.2151)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [620/689]  eta: 0:01:48  lr: 0.000088  loss: 0.2092 (0.2152)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [630/689]  eta: 0:01:33  lr: 0.000088  loss: 0.2033 (0.2151)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [640/689]  eta: 0:01:17  lr: 0.000088  loss: 0.2167 (0.2154)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [650/689]  eta: 0:01:01  lr: 0.000088  loss: 0.2186 (0.2153)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [660/689]  eta: 0:00:45  lr: 0.000088  loss: 0.2132 (0.2154)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [670/689]  eta: 0:00:29  lr: 0.000088  loss: 0.2097 (0.2153)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [680/689]  eta: 0:00:14  lr: 0.000088  loss: 0.2136 (0.2155)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213]  [688/689]  eta: 0:00:01  lr: 0.000088  loss: 0.2161 (0.2154)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:213] Total time: 0:18:06 (1.5767 s / it)\n",
      "Averaged stats: lr: 0.000088  loss: 0.2161 (0.2154)\n",
      "Valid: [epoch:213]  [ 0/14]  eta: 0:00:14  loss: 0.2029 (0.2029)  time: 1.0214  data: 0.3943  max mem: 39763\n",
      "Valid: [epoch:213]  [13/14]  eta: 0:00:00  loss: 0.2029 (0.2062)  time: 0.1148  data: 0.0282  max mem: 39763\n",
      "Valid: [epoch:213] Total time: 0:00:01 (0.1261 s / it)\n",
      "Averaged stats: loss: 0.2029 (0.2062)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_213_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.206%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:214]  [  0/689]  eta: 0:11:24  lr: 0.000087  loss: 0.1951 (0.1951)  time: 0.9937  data: 0.5155  max mem: 39763\n",
      "Train: [epoch:214]  [ 10/689]  eta: 0:17:14  lr: 0.000087  loss: 0.2298 (0.2282)  time: 1.5236  data: 0.0469  max mem: 39763\n",
      "Train: [epoch:214]  [ 20/689]  eta: 0:17:16  lr: 0.000087  loss: 0.2253 (0.2240)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [ 30/689]  eta: 0:17:06  lr: 0.000087  loss: 0.2130 (0.2224)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [ 40/689]  eta: 0:16:54  lr: 0.000087  loss: 0.2278 (0.2241)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [ 50/689]  eta: 0:16:40  lr: 0.000087  loss: 0.2142 (0.2228)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [ 60/689]  eta: 0:16:25  lr: 0.000087  loss: 0.2072 (0.2217)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [ 70/689]  eta: 0:16:10  lr: 0.000087  loss: 0.2059 (0.2198)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [ 80/689]  eta: 0:15:55  lr: 0.000087  loss: 0.2059 (0.2196)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [ 90/689]  eta: 0:15:40  lr: 0.000087  loss: 0.2101 (0.2179)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [100/689]  eta: 0:15:25  lr: 0.000087  loss: 0.2101 (0.2179)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [110/689]  eta: 0:15:09  lr: 0.000087  loss: 0.2123 (0.2176)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [120/689]  eta: 0:14:54  lr: 0.000087  loss: 0.2105 (0.2173)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [130/689]  eta: 0:14:38  lr: 0.000087  loss: 0.2114 (0.2167)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [140/689]  eta: 0:14:23  lr: 0.000087  loss: 0.2025 (0.2154)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [150/689]  eta: 0:14:07  lr: 0.000087  loss: 0.2025 (0.2155)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [160/689]  eta: 0:13:52  lr: 0.000087  loss: 0.2132 (0.2160)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [170/689]  eta: 0:13:36  lr: 0.000087  loss: 0.2129 (0.2157)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [180/689]  eta: 0:13:21  lr: 0.000087  loss: 0.2017 (0.2155)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [190/689]  eta: 0:13:05  lr: 0.000087  loss: 0.2017 (0.2158)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [200/689]  eta: 0:12:49  lr: 0.000087  loss: 0.2065 (0.2159)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [210/689]  eta: 0:12:34  lr: 0.000087  loss: 0.2145 (0.2160)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [220/689]  eta: 0:12:18  lr: 0.000087  loss: 0.2156 (0.2159)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [230/689]  eta: 0:12:02  lr: 0.000087  loss: 0.2192 (0.2159)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [240/689]  eta: 0:11:47  lr: 0.000087  loss: 0.2160 (0.2159)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [250/689]  eta: 0:11:31  lr: 0.000087  loss: 0.2083 (0.2155)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [260/689]  eta: 0:11:15  lr: 0.000087  loss: 0.2042 (0.2151)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [270/689]  eta: 0:10:59  lr: 0.000087  loss: 0.2024 (0.2147)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [280/689]  eta: 0:10:44  lr: 0.000087  loss: 0.2085 (0.2149)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [290/689]  eta: 0:10:28  lr: 0.000087  loss: 0.2088 (0.2148)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [300/689]  eta: 0:10:12  lr: 0.000087  loss: 0.2088 (0.2150)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [310/689]  eta: 0:09:57  lr: 0.000087  loss: 0.2062 (0.2146)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [320/689]  eta: 0:09:41  lr: 0.000087  loss: 0.2005 (0.2145)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [330/689]  eta: 0:09:25  lr: 0.000087  loss: 0.2170 (0.2150)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [340/689]  eta: 0:09:09  lr: 0.000087  loss: 0.2235 (0.2151)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [350/689]  eta: 0:08:54  lr: 0.000087  loss: 0.2235 (0.2154)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [360/689]  eta: 0:08:38  lr: 0.000087  loss: 0.2238 (0.2155)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [370/689]  eta: 0:08:22  lr: 0.000087  loss: 0.2033 (0.2152)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [380/689]  eta: 0:08:06  lr: 0.000087  loss: 0.2022 (0.2152)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [390/689]  eta: 0:07:51  lr: 0.000087  loss: 0.2139 (0.2156)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [400/689]  eta: 0:07:35  lr: 0.000087  loss: 0.2192 (0.2158)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [410/689]  eta: 0:07:19  lr: 0.000087  loss: 0.2153 (0.2156)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [420/689]  eta: 0:07:03  lr: 0.000087  loss: 0.2076 (0.2157)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [430/689]  eta: 0:06:48  lr: 0.000087  loss: 0.2237 (0.2163)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [440/689]  eta: 0:06:32  lr: 0.000087  loss: 0.2116 (0.2163)  time: 1.5779  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:214]  [450/689]  eta: 0:06:16  lr: 0.000087  loss: 0.2093 (0.2165)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [460/689]  eta: 0:06:00  lr: 0.000087  loss: 0.2191 (0.2166)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [470/689]  eta: 0:05:45  lr: 0.000087  loss: 0.2267 (0.2169)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [480/689]  eta: 0:05:29  lr: 0.000087  loss: 0.2233 (0.2169)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [490/689]  eta: 0:05:13  lr: 0.000087  loss: 0.2171 (0.2169)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [500/689]  eta: 0:04:57  lr: 0.000087  loss: 0.2078 (0.2169)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [510/689]  eta: 0:04:42  lr: 0.000087  loss: 0.2124 (0.2170)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [520/689]  eta: 0:04:26  lr: 0.000087  loss: 0.2142 (0.2169)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [530/689]  eta: 0:04:10  lr: 0.000087  loss: 0.2035 (0.2166)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [540/689]  eta: 0:03:54  lr: 0.000087  loss: 0.1999 (0.2166)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [550/689]  eta: 0:03:39  lr: 0.000087  loss: 0.2041 (0.2165)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [560/689]  eta: 0:03:23  lr: 0.000087  loss: 0.2017 (0.2163)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [570/689]  eta: 0:03:07  lr: 0.000087  loss: 0.2024 (0.2162)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [580/689]  eta: 0:02:51  lr: 0.000087  loss: 0.2137 (0.2164)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [590/689]  eta: 0:02:36  lr: 0.000087  loss: 0.2137 (0.2163)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [600/689]  eta: 0:02:20  lr: 0.000087  loss: 0.2079 (0.2163)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [610/689]  eta: 0:02:04  lr: 0.000087  loss: 0.2183 (0.2164)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [620/689]  eta: 0:01:48  lr: 0.000087  loss: 0.2133 (0.2164)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [630/689]  eta: 0:01:33  lr: 0.000087  loss: 0.2140 (0.2167)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [640/689]  eta: 0:01:17  lr: 0.000087  loss: 0.2181 (0.2166)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [650/689]  eta: 0:01:01  lr: 0.000087  loss: 0.2163 (0.2167)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [660/689]  eta: 0:00:45  lr: 0.000087  loss: 0.2092 (0.2167)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [670/689]  eta: 0:00:29  lr: 0.000087  loss: 0.2088 (0.2168)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [680/689]  eta: 0:00:14  lr: 0.000087  loss: 0.2238 (0.2169)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214]  [688/689]  eta: 0:00:01  lr: 0.000087  loss: 0.2215 (0.2169)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:214] Total time: 0:18:06 (1.5768 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: 0.2215 (0.2169)\n",
      "Valid: [epoch:214]  [ 0/14]  eta: 0:00:14  loss: 0.2139 (0.2139)  time: 1.0145  data: 0.3672  max mem: 39763\n",
      "Valid: [epoch:214]  [13/14]  eta: 0:00:00  loss: 0.2022 (0.2049)  time: 0.1144  data: 0.0263  max mem: 39763\n",
      "Valid: [epoch:214] Total time: 0:00:01 (0.1224 s / it)\n",
      "Averaged stats: loss: 0.2022 (0.2049)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_214_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.205%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:215]  [  0/689]  eta: 0:11:44  lr: 0.000087  loss: 0.2080 (0.2080)  time: 1.0223  data: 0.5523  max mem: 39763\n",
      "Train: [epoch:215]  [ 10/689]  eta: 0:17:15  lr: 0.000087  loss: 0.2080 (0.2083)  time: 1.5254  data: 0.0503  max mem: 39763\n",
      "Train: [epoch:215]  [ 20/689]  eta: 0:17:16  lr: 0.000087  loss: 0.2047 (0.2081)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [ 30/689]  eta: 0:17:07  lr: 0.000087  loss: 0.2053 (0.2136)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [ 40/689]  eta: 0:16:54  lr: 0.000087  loss: 0.2139 (0.2125)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [ 50/689]  eta: 0:16:40  lr: 0.000087  loss: 0.2118 (0.2132)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [ 60/689]  eta: 0:16:26  lr: 0.000087  loss: 0.2087 (0.2122)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [ 70/689]  eta: 0:16:11  lr: 0.000087  loss: 0.2087 (0.2144)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [ 80/689]  eta: 0:15:56  lr: 0.000087  loss: 0.2078 (0.2143)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [ 90/689]  eta: 0:15:41  lr: 0.000087  loss: 0.2084 (0.2168)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [100/689]  eta: 0:15:25  lr: 0.000087  loss: 0.2374 (0.2191)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [110/689]  eta: 0:15:10  lr: 0.000087  loss: 0.2327 (0.2204)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [120/689]  eta: 0:14:54  lr: 0.000087  loss: 0.2197 (0.2202)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [130/689]  eta: 0:14:39  lr: 0.000087  loss: 0.2179 (0.2201)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [140/689]  eta: 0:14:23  lr: 0.000087  loss: 0.2124 (0.2199)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [150/689]  eta: 0:14:08  lr: 0.000087  loss: 0.2223 (0.2199)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [160/689]  eta: 0:13:52  lr: 0.000087  loss: 0.2243 (0.2205)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [170/689]  eta: 0:13:37  lr: 0.000087  loss: 0.2244 (0.2208)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [180/689]  eta: 0:13:21  lr: 0.000087  loss: 0.2156 (0.2205)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [190/689]  eta: 0:13:05  lr: 0.000087  loss: 0.2147 (0.2205)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [200/689]  eta: 0:12:50  lr: 0.000087  loss: 0.2128 (0.2197)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [210/689]  eta: 0:12:34  lr: 0.000087  loss: 0.2113 (0.2195)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [220/689]  eta: 0:12:18  lr: 0.000087  loss: 0.2131 (0.2199)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [230/689]  eta: 0:12:03  lr: 0.000087  loss: 0.2131 (0.2199)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [240/689]  eta: 0:11:47  lr: 0.000087  loss: 0.2217 (0.2204)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [250/689]  eta: 0:11:31  lr: 0.000087  loss: 0.2178 (0.2203)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [260/689]  eta: 0:11:15  lr: 0.000087  loss: 0.2095 (0.2201)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [270/689]  eta: 0:11:00  lr: 0.000087  loss: 0.2084 (0.2200)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [280/689]  eta: 0:10:44  lr: 0.000087  loss: 0.2194 (0.2201)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [290/689]  eta: 0:10:28  lr: 0.000087  loss: 0.2167 (0.2200)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [300/689]  eta: 0:10:12  lr: 0.000087  loss: 0.2028 (0.2200)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [310/689]  eta: 0:09:57  lr: 0.000087  loss: 0.2082 (0.2196)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [320/689]  eta: 0:09:41  lr: 0.000087  loss: 0.1994 (0.2194)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [330/689]  eta: 0:09:25  lr: 0.000087  loss: 0.2145 (0.2191)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [340/689]  eta: 0:09:09  lr: 0.000087  loss: 0.2073 (0.2189)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [350/689]  eta: 0:08:54  lr: 0.000087  loss: 0.2103 (0.2192)  time: 1.5773  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:215]  [360/689]  eta: 0:08:38  lr: 0.000087  loss: 0.2120 (0.2194)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [370/689]  eta: 0:08:22  lr: 0.000087  loss: 0.2123 (0.2194)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [380/689]  eta: 0:08:06  lr: 0.000087  loss: 0.2123 (0.2191)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [390/689]  eta: 0:07:51  lr: 0.000087  loss: 0.2112 (0.2194)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [400/689]  eta: 0:07:35  lr: 0.000087  loss: 0.2109 (0.2192)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [410/689]  eta: 0:07:19  lr: 0.000087  loss: 0.2046 (0.2191)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [420/689]  eta: 0:07:03  lr: 0.000087  loss: 0.2128 (0.2192)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [430/689]  eta: 0:06:48  lr: 0.000087  loss: 0.2255 (0.2192)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [440/689]  eta: 0:06:32  lr: 0.000087  loss: 0.2222 (0.2194)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [450/689]  eta: 0:06:16  lr: 0.000087  loss: 0.2186 (0.2193)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [460/689]  eta: 0:06:00  lr: 0.000087  loss: 0.2112 (0.2194)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [470/689]  eta: 0:05:45  lr: 0.000087  loss: 0.2188 (0.2194)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [480/689]  eta: 0:05:29  lr: 0.000087  loss: 0.2061 (0.2191)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [490/689]  eta: 0:05:13  lr: 0.000087  loss: 0.2102 (0.2192)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [500/689]  eta: 0:04:57  lr: 0.000087  loss: 0.2179 (0.2193)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [510/689]  eta: 0:04:42  lr: 0.000087  loss: 0.2179 (0.2195)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [520/689]  eta: 0:04:26  lr: 0.000087  loss: 0.2345 (0.2196)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [530/689]  eta: 0:04:10  lr: 0.000087  loss: 0.2302 (0.2198)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [540/689]  eta: 0:03:54  lr: 0.000087  loss: 0.2218 (0.2197)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [550/689]  eta: 0:03:39  lr: 0.000087  loss: 0.2065 (0.2195)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [560/689]  eta: 0:03:23  lr: 0.000087  loss: 0.2098 (0.2197)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [570/689]  eta: 0:03:07  lr: 0.000087  loss: 0.2167 (0.2197)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [580/689]  eta: 0:02:51  lr: 0.000087  loss: 0.2111 (0.2194)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [590/689]  eta: 0:02:36  lr: 0.000087  loss: 0.2111 (0.2195)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [600/689]  eta: 0:02:20  lr: 0.000087  loss: 0.2161 (0.2194)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [610/689]  eta: 0:02:04  lr: 0.000087  loss: 0.2060 (0.2193)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [620/689]  eta: 0:01:48  lr: 0.000087  loss: 0.2080 (0.2193)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [630/689]  eta: 0:01:32  lr: 0.000087  loss: 0.2128 (0.2193)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [640/689]  eta: 0:01:17  lr: 0.000087  loss: 0.2203 (0.2195)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [650/689]  eta: 0:01:01  lr: 0.000087  loss: 0.2143 (0.2195)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [660/689]  eta: 0:00:45  lr: 0.000087  loss: 0.2084 (0.2194)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [670/689]  eta: 0:00:29  lr: 0.000087  loss: 0.2092 (0.2192)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [680/689]  eta: 0:00:14  lr: 0.000087  loss: 0.2083 (0.2192)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215]  [688/689]  eta: 0:00:01  lr: 0.000087  loss: 0.2168 (0.2193)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:215] Total time: 0:18:06 (1.5765 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: 0.2168 (0.2193)\n",
      "Valid: [epoch:215]  [ 0/14]  eta: 0:00:14  loss: 0.2164 (0.2164)  time: 1.0324  data: 0.4008  max mem: 39763\n",
      "Valid: [epoch:215]  [13/14]  eta: 0:00:00  loss: 0.2028 (0.2060)  time: 0.1157  data: 0.0287  max mem: 39763\n",
      "Valid: [epoch:215] Total time: 0:00:01 (0.1249 s / it)\n",
      "Averaged stats: loss: 0.2028 (0.2060)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_215_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.206%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:216]  [  0/689]  eta: 0:11:22  lr: 0.000087  loss: 0.2050 (0.2050)  time: 0.9910  data: 0.5199  max mem: 39763\n",
      "Train: [epoch:216]  [ 10/689]  eta: 0:17:13  lr: 0.000087  loss: 0.2082 (0.2089)  time: 1.5220  data: 0.0473  max mem: 39763\n",
      "Train: [epoch:216]  [ 20/689]  eta: 0:17:15  lr: 0.000087  loss: 0.2127 (0.2180)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [ 30/689]  eta: 0:17:05  lr: 0.000087  loss: 0.2212 (0.2205)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [ 40/689]  eta: 0:16:53  lr: 0.000087  loss: 0.2205 (0.2214)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [ 50/689]  eta: 0:16:39  lr: 0.000087  loss: 0.2202 (0.2219)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [ 60/689]  eta: 0:16:24  lr: 0.000087  loss: 0.2254 (0.2225)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [ 70/689]  eta: 0:16:09  lr: 0.000087  loss: 0.2253 (0.2233)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [ 80/689]  eta: 0:15:54  lr: 0.000087  loss: 0.2182 (0.2231)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [ 90/689]  eta: 0:15:39  lr: 0.000087  loss: 0.2183 (0.2230)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [100/689]  eta: 0:15:24  lr: 0.000087  loss: 0.2123 (0.2221)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [110/689]  eta: 0:15:09  lr: 0.000087  loss: 0.2167 (0.2222)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [120/689]  eta: 0:14:53  lr: 0.000087  loss: 0.2213 (0.2220)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [130/689]  eta: 0:14:38  lr: 0.000087  loss: 0.2054 (0.2208)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [140/689]  eta: 0:14:22  lr: 0.000087  loss: 0.2061 (0.2209)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [150/689]  eta: 0:14:07  lr: 0.000087  loss: 0.2070 (0.2210)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [160/689]  eta: 0:13:51  lr: 0.000087  loss: 0.2189 (0.2218)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [170/689]  eta: 0:13:35  lr: 0.000087  loss: 0.2359 (0.2225)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [180/689]  eta: 0:13:20  lr: 0.000087  loss: 0.2104 (0.2216)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [190/689]  eta: 0:13:04  lr: 0.000087  loss: 0.2106 (0.2214)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [200/689]  eta: 0:12:49  lr: 0.000087  loss: 0.2108 (0.2209)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [210/689]  eta: 0:12:33  lr: 0.000087  loss: 0.2071 (0.2202)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [220/689]  eta: 0:12:17  lr: 0.000087  loss: 0.2076 (0.2203)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [230/689]  eta: 0:12:02  lr: 0.000087  loss: 0.2182 (0.2204)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [240/689]  eta: 0:11:46  lr: 0.000087  loss: 0.2271 (0.2209)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [250/689]  eta: 0:11:30  lr: 0.000087  loss: 0.2157 (0.2204)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [260/689]  eta: 0:11:15  lr: 0.000087  loss: 0.2032 (0.2199)  time: 1.5774  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:216]  [270/689]  eta: 0:10:59  lr: 0.000087  loss: 0.2124 (0.2196)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [280/689]  eta: 0:10:43  lr: 0.000087  loss: 0.2132 (0.2194)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [290/689]  eta: 0:10:28  lr: 0.000087  loss: 0.2071 (0.2192)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [300/689]  eta: 0:10:12  lr: 0.000087  loss: 0.2063 (0.2192)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [310/689]  eta: 0:09:56  lr: 0.000087  loss: 0.2055 (0.2187)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [320/689]  eta: 0:09:41  lr: 0.000087  loss: 0.2093 (0.2187)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [330/689]  eta: 0:09:25  lr: 0.000087  loss: 0.2105 (0.2186)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [340/689]  eta: 0:09:09  lr: 0.000087  loss: 0.2062 (0.2183)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [350/689]  eta: 0:08:53  lr: 0.000087  loss: 0.2047 (0.2181)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [360/689]  eta: 0:08:38  lr: 0.000087  loss: 0.2119 (0.2182)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [370/689]  eta: 0:08:22  lr: 0.000087  loss: 0.2143 (0.2184)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [380/689]  eta: 0:08:06  lr: 0.000087  loss: 0.2141 (0.2183)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [390/689]  eta: 0:07:50  lr: 0.000087  loss: 0.2123 (0.2185)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [400/689]  eta: 0:07:35  lr: 0.000087  loss: 0.2125 (0.2184)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [410/689]  eta: 0:07:19  lr: 0.000087  loss: 0.2121 (0.2183)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [420/689]  eta: 0:07:03  lr: 0.000087  loss: 0.2111 (0.2184)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [430/689]  eta: 0:06:48  lr: 0.000087  loss: 0.2284 (0.2188)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [440/689]  eta: 0:06:32  lr: 0.000087  loss: 0.2224 (0.2189)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [450/689]  eta: 0:06:16  lr: 0.000087  loss: 0.2166 (0.2188)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [460/689]  eta: 0:06:00  lr: 0.000087  loss: 0.2172 (0.2192)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [470/689]  eta: 0:05:45  lr: 0.000087  loss: 0.2320 (0.2196)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [480/689]  eta: 0:05:29  lr: 0.000087  loss: 0.2169 (0.2193)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [490/689]  eta: 0:05:13  lr: 0.000087  loss: 0.2021 (0.2190)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [500/689]  eta: 0:04:57  lr: 0.000087  loss: 0.2083 (0.2192)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [510/689]  eta: 0:04:42  lr: 0.000087  loss: 0.2158 (0.2193)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [520/689]  eta: 0:04:26  lr: 0.000087  loss: 0.2101 (0.2193)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [530/689]  eta: 0:04:10  lr: 0.000087  loss: 0.2243 (0.2192)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [540/689]  eta: 0:03:54  lr: 0.000087  loss: 0.2227 (0.2192)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [550/689]  eta: 0:03:39  lr: 0.000087  loss: 0.2170 (0.2191)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [560/689]  eta: 0:03:23  lr: 0.000087  loss: 0.2170 (0.2193)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [570/689]  eta: 0:03:07  lr: 0.000087  loss: 0.2151 (0.2194)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [580/689]  eta: 0:02:51  lr: 0.000087  loss: 0.2081 (0.2193)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [590/689]  eta: 0:02:36  lr: 0.000087  loss: 0.2122 (0.2192)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [600/689]  eta: 0:02:20  lr: 0.000087  loss: 0.2119 (0.2192)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [610/689]  eta: 0:02:04  lr: 0.000087  loss: 0.2107 (0.2193)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [620/689]  eta: 0:01:48  lr: 0.000087  loss: 0.2061 (0.2191)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [630/689]  eta: 0:01:32  lr: 0.000087  loss: 0.2028 (0.2191)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [640/689]  eta: 0:01:17  lr: 0.000087  loss: 0.2145 (0.2192)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [650/689]  eta: 0:01:01  lr: 0.000087  loss: 0.2134 (0.2191)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [660/689]  eta: 0:00:45  lr: 0.000087  loss: 0.2105 (0.2190)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [670/689]  eta: 0:00:29  lr: 0.000087  loss: 0.2117 (0.2189)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [680/689]  eta: 0:00:14  lr: 0.000087  loss: 0.2115 (0.2189)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216]  [688/689]  eta: 0:00:01  lr: 0.000087  loss: 0.2163 (0.2189)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:216] Total time: 0:18:06 (1.5763 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: 0.2163 (0.2189)\n",
      "Valid: [epoch:216]  [ 0/14]  eta: 0:00:14  loss: 0.1894 (0.1894)  time: 1.0095  data: 0.3472  max mem: 39763\n",
      "Valid: [epoch:216]  [13/14]  eta: 0:00:00  loss: 0.2037 (0.2068)  time: 0.1140  data: 0.0249  max mem: 39763\n",
      "Valid: [epoch:216] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.2037 (0.2068)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_216_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.207%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:217]  [  0/689]  eta: 0:12:00  lr: 0.000087  loss: 0.2484 (0.2484)  time: 1.0464  data: 0.5690  max mem: 39763\n",
      "Train: [epoch:217]  [ 10/689]  eta: 0:17:15  lr: 0.000087  loss: 0.2313 (0.2308)  time: 1.5253  data: 0.0518  max mem: 39763\n",
      "Train: [epoch:217]  [ 20/689]  eta: 0:17:16  lr: 0.000087  loss: 0.2198 (0.2218)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [ 30/689]  eta: 0:17:07  lr: 0.000087  loss: 0.2061 (0.2226)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [ 40/689]  eta: 0:16:54  lr: 0.000087  loss: 0.2021 (0.2209)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [ 50/689]  eta: 0:16:40  lr: 0.000087  loss: 0.2118 (0.2201)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [ 60/689]  eta: 0:16:25  lr: 0.000087  loss: 0.2170 (0.2192)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [ 70/689]  eta: 0:16:10  lr: 0.000087  loss: 0.2184 (0.2189)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [ 80/689]  eta: 0:15:55  lr: 0.000087  loss: 0.2213 (0.2195)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [ 90/689]  eta: 0:15:40  lr: 0.000087  loss: 0.2159 (0.2195)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [100/689]  eta: 0:15:25  lr: 0.000087  loss: 0.2130 (0.2181)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [110/689]  eta: 0:15:09  lr: 0.000087  loss: 0.2059 (0.2175)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [120/689]  eta: 0:14:54  lr: 0.000087  loss: 0.2085 (0.2173)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [130/689]  eta: 0:14:38  lr: 0.000087  loss: 0.2170 (0.2182)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [140/689]  eta: 0:14:23  lr: 0.000087  loss: 0.2282 (0.2189)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [150/689]  eta: 0:14:07  lr: 0.000087  loss: 0.2281 (0.2197)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [160/689]  eta: 0:13:52  lr: 0.000087  loss: 0.2291 (0.2201)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [170/689]  eta: 0:13:36  lr: 0.000087  loss: 0.2176 (0.2196)  time: 1.5778  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:217]  [180/689]  eta: 0:13:21  lr: 0.000087  loss: 0.2134 (0.2190)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [190/689]  eta: 0:13:05  lr: 0.000087  loss: 0.2150 (0.2188)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [200/689]  eta: 0:12:49  lr: 0.000087  loss: 0.2196 (0.2190)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [210/689]  eta: 0:12:34  lr: 0.000087  loss: 0.2106 (0.2186)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [220/689]  eta: 0:12:18  lr: 0.000087  loss: 0.2125 (0.2189)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [230/689]  eta: 0:12:02  lr: 0.000087  loss: 0.2174 (0.2188)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [240/689]  eta: 0:11:47  lr: 0.000087  loss: 0.2174 (0.2194)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [250/689]  eta: 0:11:31  lr: 0.000087  loss: 0.2211 (0.2194)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [260/689]  eta: 0:11:15  lr: 0.000087  loss: 0.2211 (0.2194)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [270/689]  eta: 0:10:59  lr: 0.000087  loss: 0.2079 (0.2193)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [280/689]  eta: 0:10:44  lr: 0.000087  loss: 0.2138 (0.2194)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [290/689]  eta: 0:10:28  lr: 0.000087  loss: 0.2138 (0.2194)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [300/689]  eta: 0:10:12  lr: 0.000087  loss: 0.2090 (0.2194)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [310/689]  eta: 0:09:57  lr: 0.000087  loss: 0.2108 (0.2190)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [320/689]  eta: 0:09:41  lr: 0.000087  loss: 0.2144 (0.2191)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [330/689]  eta: 0:09:25  lr: 0.000087  loss: 0.2159 (0.2195)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [340/689]  eta: 0:09:09  lr: 0.000087  loss: 0.2159 (0.2194)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [350/689]  eta: 0:08:54  lr: 0.000087  loss: 0.2163 (0.2196)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [360/689]  eta: 0:08:38  lr: 0.000087  loss: 0.2163 (0.2197)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [370/689]  eta: 0:08:22  lr: 0.000087  loss: 0.2134 (0.2196)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [380/689]  eta: 0:08:07  lr: 0.000087  loss: 0.2023 (0.2193)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [390/689]  eta: 0:07:51  lr: 0.000087  loss: 0.2114 (0.2194)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [400/689]  eta: 0:07:35  lr: 0.000087  loss: 0.2152 (0.2195)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [410/689]  eta: 0:07:19  lr: 0.000087  loss: 0.2152 (0.2194)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [420/689]  eta: 0:07:04  lr: 0.000087  loss: 0.2122 (0.2192)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [430/689]  eta: 0:06:48  lr: 0.000087  loss: 0.2137 (0.2194)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [440/689]  eta: 0:06:32  lr: 0.000087  loss: 0.2164 (0.2195)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [450/689]  eta: 0:06:16  lr: 0.000087  loss: 0.2164 (0.2196)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [460/689]  eta: 0:06:01  lr: 0.000087  loss: 0.2292 (0.2202)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [470/689]  eta: 0:05:45  lr: 0.000087  loss: 0.2188 (0.2200)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [480/689]  eta: 0:05:29  lr: 0.000087  loss: 0.2071 (0.2198)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [490/689]  eta: 0:05:13  lr: 0.000087  loss: 0.2095 (0.2197)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [500/689]  eta: 0:04:58  lr: 0.000087  loss: 0.2072 (0.2196)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [510/689]  eta: 0:04:42  lr: 0.000087  loss: 0.2089 (0.2197)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [520/689]  eta: 0:04:26  lr: 0.000087  loss: 0.2149 (0.2197)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [530/689]  eta: 0:04:10  lr: 0.000087  loss: 0.2149 (0.2196)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [540/689]  eta: 0:03:54  lr: 0.000087  loss: 0.2127 (0.2197)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [550/689]  eta: 0:03:39  lr: 0.000087  loss: 0.2133 (0.2198)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [560/689]  eta: 0:03:23  lr: 0.000087  loss: 0.2189 (0.2198)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [570/689]  eta: 0:03:07  lr: 0.000087  loss: 0.2292 (0.2202)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [580/689]  eta: 0:02:51  lr: 0.000087  loss: 0.2306 (0.2202)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [590/689]  eta: 0:02:36  lr: 0.000087  loss: 0.2091 (0.2202)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [600/689]  eta: 0:02:20  lr: 0.000087  loss: 0.2135 (0.2202)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [610/689]  eta: 0:02:04  lr: 0.000087  loss: 0.2097 (0.2200)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [620/689]  eta: 0:01:48  lr: 0.000087  loss: 0.2064 (0.2200)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [630/689]  eta: 0:01:33  lr: 0.000087  loss: 0.2084 (0.2200)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [640/689]  eta: 0:01:17  lr: 0.000087  loss: 0.2085 (0.2199)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [650/689]  eta: 0:01:01  lr: 0.000087  loss: 0.2095 (0.2197)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [660/689]  eta: 0:00:45  lr: 0.000087  loss: 0.1981 (0.2196)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [670/689]  eta: 0:00:29  lr: 0.000087  loss: 0.2078 (0.2195)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [680/689]  eta: 0:00:14  lr: 0.000087  loss: 0.2055 (0.2193)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217]  [688/689]  eta: 0:00:01  lr: 0.000087  loss: 0.2026 (0.2193)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:217] Total time: 0:18:07 (1.5777 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: 0.2026 (0.2193)\n",
      "Valid: [epoch:217]  [ 0/14]  eta: 0:00:14  loss: 0.1997 (0.1997)  time: 1.0052  data: 0.3788  max mem: 39763\n",
      "Valid: [epoch:217]  [13/14]  eta: 0:00:00  loss: 0.2047 (0.2074)  time: 0.1138  data: 0.0271  max mem: 39763\n",
      "Valid: [epoch:217] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.2047 (0.2074)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_217_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.207%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:218]  [  0/689]  eta: 0:11:29  lr: 0.000087  loss: 0.2333 (0.2333)  time: 1.0014  data: 0.5222  max mem: 39763\n",
      "Train: [epoch:218]  [ 10/689]  eta: 0:17:15  lr: 0.000087  loss: 0.2170 (0.2199)  time: 1.5254  data: 0.0476  max mem: 39763\n",
      "Train: [epoch:218]  [ 20/689]  eta: 0:17:17  lr: 0.000087  loss: 0.2086 (0.2139)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [ 30/689]  eta: 0:17:07  lr: 0.000087  loss: 0.2048 (0.2111)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [ 40/689]  eta: 0:16:55  lr: 0.000087  loss: 0.2059 (0.2144)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [ 50/689]  eta: 0:16:41  lr: 0.000087  loss: 0.2127 (0.2154)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [ 60/689]  eta: 0:16:26  lr: 0.000087  loss: 0.2077 (0.2154)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [ 70/689]  eta: 0:16:12  lr: 0.000087  loss: 0.2029 (0.2160)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [ 80/689]  eta: 0:15:56  lr: 0.000087  loss: 0.2003 (0.2153)  time: 1.5789  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:218]  [ 90/689]  eta: 0:15:41  lr: 0.000087  loss: 0.2018 (0.2143)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [100/689]  eta: 0:15:26  lr: 0.000087  loss: 0.2018 (0.2152)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [110/689]  eta: 0:15:10  lr: 0.000087  loss: 0.2200 (0.2164)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [120/689]  eta: 0:14:55  lr: 0.000087  loss: 0.2248 (0.2178)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [130/689]  eta: 0:14:39  lr: 0.000087  loss: 0.2144 (0.2176)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [140/689]  eta: 0:14:24  lr: 0.000087  loss: 0.2055 (0.2173)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [150/689]  eta: 0:14:08  lr: 0.000087  loss: 0.2058 (0.2169)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [160/689]  eta: 0:13:53  lr: 0.000087  loss: 0.2178 (0.2180)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [170/689]  eta: 0:13:37  lr: 0.000087  loss: 0.2251 (0.2183)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [180/689]  eta: 0:13:21  lr: 0.000087  loss: 0.2205 (0.2183)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [190/689]  eta: 0:13:06  lr: 0.000087  loss: 0.2204 (0.2197)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [200/689]  eta: 0:12:50  lr: 0.000087  loss: 0.2200 (0.2198)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [210/689]  eta: 0:12:34  lr: 0.000087  loss: 0.2133 (0.2202)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [220/689]  eta: 0:12:19  lr: 0.000087  loss: 0.2165 (0.2202)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [230/689]  eta: 0:12:03  lr: 0.000087  loss: 0.2191 (0.2208)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [240/689]  eta: 0:11:47  lr: 0.000087  loss: 0.2208 (0.2208)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [250/689]  eta: 0:11:31  lr: 0.000087  loss: 0.2162 (0.2210)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [260/689]  eta: 0:11:16  lr: 0.000087  loss: 0.2209 (0.2214)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [270/689]  eta: 0:11:00  lr: 0.000087  loss: 0.2208 (0.2212)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [280/689]  eta: 0:10:44  lr: 0.000087  loss: 0.2089 (0.2211)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [290/689]  eta: 0:10:28  lr: 0.000087  loss: 0.2190 (0.2213)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [300/689]  eta: 0:10:13  lr: 0.000087  loss: 0.2218 (0.2215)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [310/689]  eta: 0:09:57  lr: 0.000087  loss: 0.2127 (0.2211)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [320/689]  eta: 0:09:41  lr: 0.000087  loss: 0.2177 (0.2217)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [330/689]  eta: 0:09:25  lr: 0.000087  loss: 0.2264 (0.2220)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [340/689]  eta: 0:09:10  lr: 0.000087  loss: 0.2176 (0.2217)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [350/689]  eta: 0:08:54  lr: 0.000087  loss: 0.2110 (0.2215)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [360/689]  eta: 0:08:38  lr: 0.000087  loss: 0.2107 (0.2213)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [370/689]  eta: 0:08:22  lr: 0.000087  loss: 0.2139 (0.2214)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [380/689]  eta: 0:08:07  lr: 0.000087  loss: 0.2187 (0.2216)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [390/689]  eta: 0:07:51  lr: 0.000087  loss: 0.2115 (0.2215)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [400/689]  eta: 0:07:35  lr: 0.000087  loss: 0.2198 (0.2220)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [410/689]  eta: 0:07:19  lr: 0.000087  loss: 0.2244 (0.2221)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [420/689]  eta: 0:07:04  lr: 0.000087  loss: 0.2143 (0.2220)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [430/689]  eta: 0:06:48  lr: 0.000087  loss: 0.2142 (0.2219)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [440/689]  eta: 0:06:32  lr: 0.000087  loss: 0.2175 (0.2220)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [450/689]  eta: 0:06:16  lr: 0.000087  loss: 0.2176 (0.2220)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [460/689]  eta: 0:06:01  lr: 0.000087  loss: 0.2197 (0.2220)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [470/689]  eta: 0:05:45  lr: 0.000087  loss: 0.2126 (0.2216)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [480/689]  eta: 0:05:29  lr: 0.000087  loss: 0.2045 (0.2216)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [490/689]  eta: 0:05:13  lr: 0.000087  loss: 0.2298 (0.2221)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [500/689]  eta: 0:04:58  lr: 0.000087  loss: 0.2293 (0.2220)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [510/689]  eta: 0:04:42  lr: 0.000087  loss: 0.2095 (0.2218)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [520/689]  eta: 0:04:26  lr: 0.000087  loss: 0.2113 (0.2220)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [530/689]  eta: 0:04:10  lr: 0.000087  loss: 0.2421 (0.2222)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [540/689]  eta: 0:03:54  lr: 0.000087  loss: 0.2166 (0.2219)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [550/689]  eta: 0:03:39  lr: 0.000087  loss: 0.2166 (0.2220)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [560/689]  eta: 0:03:23  lr: 0.000087  loss: 0.2350 (0.2223)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [570/689]  eta: 0:03:07  lr: 0.000087  loss: 0.2246 (0.2222)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [580/689]  eta: 0:02:51  lr: 0.000087  loss: 0.2097 (0.2220)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [590/689]  eta: 0:02:36  lr: 0.000087  loss: 0.2111 (0.2219)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [600/689]  eta: 0:02:20  lr: 0.000087  loss: 0.2150 (0.2220)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [610/689]  eta: 0:02:04  lr: 0.000087  loss: 0.2091 (0.2219)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [620/689]  eta: 0:01:48  lr: 0.000087  loss: 0.2051 (0.2220)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [630/689]  eta: 0:01:33  lr: 0.000087  loss: 0.2063 (0.2217)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [640/689]  eta: 0:01:17  lr: 0.000087  loss: 0.2129 (0.2219)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [650/689]  eta: 0:01:01  lr: 0.000087  loss: 0.2248 (0.2219)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [660/689]  eta: 0:00:45  lr: 0.000087  loss: 0.2166 (0.2219)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [670/689]  eta: 0:00:29  lr: 0.000087  loss: 0.2199 (0.2219)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [680/689]  eta: 0:00:14  lr: 0.000087  loss: 0.2220 (0.2221)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218]  [688/689]  eta: 0:00:01  lr: 0.000087  loss: 0.2300 (0.2221)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:218] Total time: 0:18:06 (1.5775 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: 0.2300 (0.2221)\n",
      "Valid: [epoch:218]  [ 0/14]  eta: 0:00:14  loss: 0.2058 (0.2058)  time: 1.0081  data: 0.3994  max mem: 39763\n",
      "Valid: [epoch:218]  [13/14]  eta: 0:00:00  loss: 0.2058 (0.2090)  time: 0.1140  data: 0.0286  max mem: 39763\n",
      "Valid: [epoch:218] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.2058 (0.2090)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_218_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.209%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:219]  [  0/689]  eta: 0:11:50  lr: 0.000087  loss: 0.2528 (0.2528)  time: 1.0319  data: 0.5546  max mem: 39763\n",
      "Train: [epoch:219]  [ 10/689]  eta: 0:17:15  lr: 0.000087  loss: 0.2188 (0.2231)  time: 1.5249  data: 0.0505  max mem: 39763\n",
      "Train: [epoch:219]  [ 20/689]  eta: 0:17:16  lr: 0.000087  loss: 0.2224 (0.2290)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [ 30/689]  eta: 0:17:06  lr: 0.000087  loss: 0.2224 (0.2269)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [ 40/689]  eta: 0:16:54  lr: 0.000087  loss: 0.2123 (0.2238)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [ 50/689]  eta: 0:16:40  lr: 0.000087  loss: 0.2171 (0.2245)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [ 60/689]  eta: 0:16:25  lr: 0.000087  loss: 0.2176 (0.2238)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [ 70/689]  eta: 0:16:11  lr: 0.000087  loss: 0.2174 (0.2215)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [ 80/689]  eta: 0:15:56  lr: 0.000087  loss: 0.2174 (0.2225)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [ 90/689]  eta: 0:15:40  lr: 0.000087  loss: 0.2177 (0.2233)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [100/689]  eta: 0:15:25  lr: 0.000087  loss: 0.2251 (0.2247)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [110/689]  eta: 0:15:10  lr: 0.000087  loss: 0.2231 (0.2244)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [120/689]  eta: 0:14:54  lr: 0.000087  loss: 0.2144 (0.2240)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [130/689]  eta: 0:14:39  lr: 0.000087  loss: 0.2157 (0.2239)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [140/689]  eta: 0:14:23  lr: 0.000087  loss: 0.2182 (0.2240)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [150/689]  eta: 0:14:08  lr: 0.000087  loss: 0.2205 (0.2236)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [160/689]  eta: 0:13:52  lr: 0.000087  loss: 0.2199 (0.2236)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [170/689]  eta: 0:13:36  lr: 0.000087  loss: 0.2193 (0.2244)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [180/689]  eta: 0:13:21  lr: 0.000087  loss: 0.2202 (0.2243)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [190/689]  eta: 0:13:05  lr: 0.000087  loss: 0.2200 (0.2249)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [200/689]  eta: 0:12:49  lr: 0.000087  loss: 0.2160 (0.2246)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [210/689]  eta: 0:12:34  lr: 0.000087  loss: 0.2101 (0.2241)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [220/689]  eta: 0:12:18  lr: 0.000087  loss: 0.2169 (0.2244)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [230/689]  eta: 0:12:02  lr: 0.000087  loss: 0.2170 (0.2241)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [240/689]  eta: 0:11:47  lr: 0.000087  loss: 0.2170 (0.2247)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [250/689]  eta: 0:11:31  lr: 0.000087  loss: 0.2217 (0.2245)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [260/689]  eta: 0:11:15  lr: 0.000087  loss: 0.2169 (0.2244)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [270/689]  eta: 0:11:00  lr: 0.000087  loss: 0.2169 (0.2241)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [280/689]  eta: 0:10:44  lr: 0.000087  loss: 0.2246 (0.2242)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [290/689]  eta: 0:10:28  lr: 0.000087  loss: 0.2244 (0.2241)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [300/689]  eta: 0:10:12  lr: 0.000087  loss: 0.2192 (0.2237)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [310/689]  eta: 0:09:57  lr: 0.000087  loss: 0.2034 (0.2233)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [320/689]  eta: 0:09:41  lr: 0.000087  loss: 0.2084 (0.2232)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [330/689]  eta: 0:09:25  lr: 0.000087  loss: 0.2185 (0.2235)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [340/689]  eta: 0:09:09  lr: 0.000087  loss: 0.2127 (0.2230)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [350/689]  eta: 0:08:54  lr: 0.000087  loss: 0.2132 (0.2233)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [360/689]  eta: 0:08:38  lr: 0.000087  loss: 0.2141 (0.2227)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [370/689]  eta: 0:08:22  lr: 0.000087  loss: 0.2098 (0.2224)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [380/689]  eta: 0:08:07  lr: 0.000087  loss: 0.2222 (0.2226)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [390/689]  eta: 0:07:51  lr: 0.000087  loss: 0.2265 (0.2224)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [400/689]  eta: 0:07:35  lr: 0.000087  loss: 0.2076 (0.2220)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [410/689]  eta: 0:07:19  lr: 0.000087  loss: 0.2118 (0.2223)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [420/689]  eta: 0:07:04  lr: 0.000087  loss: 0.2235 (0.2222)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [430/689]  eta: 0:06:48  lr: 0.000087  loss: 0.2209 (0.2222)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [440/689]  eta: 0:06:32  lr: 0.000087  loss: 0.2255 (0.2226)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [450/689]  eta: 0:06:16  lr: 0.000087  loss: 0.2138 (0.2223)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [460/689]  eta: 0:06:01  lr: 0.000087  loss: 0.2138 (0.2226)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [470/689]  eta: 0:05:45  lr: 0.000087  loss: 0.2148 (0.2225)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [480/689]  eta: 0:05:29  lr: 0.000087  loss: 0.2183 (0.2224)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [490/689]  eta: 0:05:13  lr: 0.000087  loss: 0.2183 (0.2223)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [500/689]  eta: 0:04:58  lr: 0.000087  loss: 0.2125 (0.2222)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [510/689]  eta: 0:04:42  lr: 0.000087  loss: 0.2096 (0.2221)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [520/689]  eta: 0:04:26  lr: 0.000087  loss: 0.2096 (0.2222)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [530/689]  eta: 0:04:10  lr: 0.000087  loss: 0.2279 (0.2223)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [540/689]  eta: 0:03:54  lr: 0.000087  loss: 0.2076 (0.2221)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [550/689]  eta: 0:03:39  lr: 0.000087  loss: 0.2175 (0.2222)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [560/689]  eta: 0:03:23  lr: 0.000087  loss: 0.2228 (0.2222)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [570/689]  eta: 0:03:07  lr: 0.000087  loss: 0.2132 (0.2222)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [580/689]  eta: 0:02:51  lr: 0.000087  loss: 0.2096 (0.2222)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [590/689]  eta: 0:02:36  lr: 0.000087  loss: 0.2096 (0.2221)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [600/689]  eta: 0:02:20  lr: 0.000087  loss: 0.2146 (0.2221)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [610/689]  eta: 0:02:04  lr: 0.000087  loss: 0.2186 (0.2220)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [620/689]  eta: 0:01:48  lr: 0.000087  loss: 0.2219 (0.2221)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [630/689]  eta: 0:01:33  lr: 0.000087  loss: 0.2262 (0.2220)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [640/689]  eta: 0:01:17  lr: 0.000087  loss: 0.2271 (0.2220)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [650/689]  eta: 0:01:01  lr: 0.000087  loss: 0.2256 (0.2222)  time: 1.5792  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:219]  [660/689]  eta: 0:00:45  lr: 0.000087  loss: 0.2256 (0.2224)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [670/689]  eta: 0:00:29  lr: 0.000087  loss: 0.2261 (0.2225)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [680/689]  eta: 0:00:14  lr: 0.000087  loss: 0.2267 (0.2226)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219]  [688/689]  eta: 0:00:01  lr: 0.000087  loss: 0.2267 (0.2226)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:219] Total time: 0:18:07 (1.5778 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: 0.2267 (0.2226)\n",
      "Valid: [epoch:219]  [ 0/14]  eta: 0:00:13  loss: 0.1924 (0.1924)  time: 0.9990  data: 0.4040  max mem: 39763\n",
      "Valid: [epoch:219]  [13/14]  eta: 0:00:00  loss: 0.2068 (0.2102)  time: 0.1133  data: 0.0289  max mem: 39763\n",
      "Valid: [epoch:219] Total time: 0:00:01 (0.1224 s / it)\n",
      "Averaged stats: loss: 0.2068 (0.2102)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_219_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.210%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:220]  [  0/689]  eta: 0:12:04  lr: 0.000087  loss: 0.2238 (0.2238)  time: 1.0517  data: 0.5751  max mem: 39763\n",
      "Train: [epoch:220]  [ 10/689]  eta: 0:17:19  lr: 0.000087  loss: 0.2043 (0.2126)  time: 1.5304  data: 0.0524  max mem: 39763\n",
      "Train: [epoch:220]  [ 20/689]  eta: 0:17:19  lr: 0.000087  loss: 0.2043 (0.2131)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [ 30/689]  eta: 0:17:09  lr: 0.000087  loss: 0.2153 (0.2175)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [ 40/689]  eta: 0:16:55  lr: 0.000087  loss: 0.2196 (0.2198)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [ 50/689]  eta: 0:16:41  lr: 0.000087  loss: 0.2226 (0.2212)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [ 60/689]  eta: 0:16:27  lr: 0.000087  loss: 0.2226 (0.2211)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [ 70/689]  eta: 0:16:12  lr: 0.000087  loss: 0.2196 (0.2213)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [ 80/689]  eta: 0:15:57  lr: 0.000087  loss: 0.2114 (0.2192)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [ 90/689]  eta: 0:15:41  lr: 0.000087  loss: 0.2066 (0.2192)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [100/689]  eta: 0:15:26  lr: 0.000087  loss: 0.2248 (0.2205)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [110/689]  eta: 0:15:11  lr: 0.000087  loss: 0.2392 (0.2224)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [120/689]  eta: 0:14:55  lr: 0.000087  loss: 0.2277 (0.2224)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [130/689]  eta: 0:14:40  lr: 0.000087  loss: 0.2179 (0.2216)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [140/689]  eta: 0:14:24  lr: 0.000087  loss: 0.2127 (0.2223)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [150/689]  eta: 0:14:08  lr: 0.000087  loss: 0.2169 (0.2218)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [160/689]  eta: 0:13:53  lr: 0.000087  loss: 0.2107 (0.2212)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [170/689]  eta: 0:13:37  lr: 0.000087  loss: 0.2157 (0.2217)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [180/689]  eta: 0:13:21  lr: 0.000087  loss: 0.2229 (0.2219)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [190/689]  eta: 0:13:06  lr: 0.000087  loss: 0.2206 (0.2223)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [200/689]  eta: 0:12:50  lr: 0.000087  loss: 0.2204 (0.2222)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [210/689]  eta: 0:12:34  lr: 0.000087  loss: 0.2097 (0.2218)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [220/689]  eta: 0:12:19  lr: 0.000087  loss: 0.2058 (0.2214)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [230/689]  eta: 0:12:03  lr: 0.000087  loss: 0.2169 (0.2221)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [240/689]  eta: 0:11:47  lr: 0.000087  loss: 0.2262 (0.2220)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [250/689]  eta: 0:11:31  lr: 0.000087  loss: 0.2122 (0.2215)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [260/689]  eta: 0:11:16  lr: 0.000087  loss: 0.2172 (0.2216)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [270/689]  eta: 0:11:00  lr: 0.000087  loss: 0.2194 (0.2217)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [280/689]  eta: 0:10:44  lr: 0.000087  loss: 0.2160 (0.2219)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [290/689]  eta: 0:10:28  lr: 0.000087  loss: 0.2126 (0.2218)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [300/689]  eta: 0:10:13  lr: 0.000087  loss: 0.2118 (0.2219)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [310/689]  eta: 0:09:57  lr: 0.000087  loss: 0.2142 (0.2217)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [320/689]  eta: 0:09:41  lr: 0.000087  loss: 0.2142 (0.2218)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [330/689]  eta: 0:09:25  lr: 0.000087  loss: 0.2236 (0.2222)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [340/689]  eta: 0:09:10  lr: 0.000087  loss: 0.2211 (0.2226)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [350/689]  eta: 0:08:54  lr: 0.000087  loss: 0.2144 (0.2223)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [360/689]  eta: 0:08:38  lr: 0.000087  loss: 0.2135 (0.2223)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [370/689]  eta: 0:08:22  lr: 0.000087  loss: 0.2083 (0.2220)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [380/689]  eta: 0:08:07  lr: 0.000087  loss: 0.2074 (0.2221)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [390/689]  eta: 0:07:51  lr: 0.000087  loss: 0.2087 (0.2220)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [400/689]  eta: 0:07:35  lr: 0.000087  loss: 0.2065 (0.2218)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [410/689]  eta: 0:07:19  lr: 0.000087  loss: 0.2136 (0.2221)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [420/689]  eta: 0:07:04  lr: 0.000087  loss: 0.2136 (0.2218)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [430/689]  eta: 0:06:48  lr: 0.000087  loss: 0.2068 (0.2215)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [440/689]  eta: 0:06:32  lr: 0.000087  loss: 0.2229 (0.2218)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [450/689]  eta: 0:06:16  lr: 0.000087  loss: 0.2339 (0.2220)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [460/689]  eta: 0:06:01  lr: 0.000087  loss: 0.2269 (0.2223)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [470/689]  eta: 0:05:45  lr: 0.000087  loss: 0.2223 (0.2223)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [480/689]  eta: 0:05:29  lr: 0.000087  loss: 0.2163 (0.2224)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [490/689]  eta: 0:05:13  lr: 0.000087  loss: 0.2179 (0.2223)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [500/689]  eta: 0:04:58  lr: 0.000087  loss: 0.2135 (0.2223)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [510/689]  eta: 0:04:42  lr: 0.000087  loss: 0.2121 (0.2225)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [520/689]  eta: 0:04:26  lr: 0.000087  loss: 0.2202 (0.2227)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [530/689]  eta: 0:04:10  lr: 0.000087  loss: 0.2227 (0.2227)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [540/689]  eta: 0:03:54  lr: 0.000087  loss: 0.2227 (0.2228)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [550/689]  eta: 0:03:39  lr: 0.000087  loss: 0.2204 (0.2226)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [560/689]  eta: 0:03:23  lr: 0.000087  loss: 0.2140 (0.2226)  time: 1.5777  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:220]  [570/689]  eta: 0:03:07  lr: 0.000087  loss: 0.2185 (0.2228)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [580/689]  eta: 0:02:51  lr: 0.000087  loss: 0.2113 (0.2224)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [590/689]  eta: 0:02:36  lr: 0.000087  loss: 0.2103 (0.2224)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [600/689]  eta: 0:02:20  lr: 0.000087  loss: 0.2159 (0.2224)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [610/689]  eta: 0:02:04  lr: 0.000087  loss: 0.2323 (0.2226)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [620/689]  eta: 0:01:48  lr: 0.000087  loss: 0.2323 (0.2227)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [630/689]  eta: 0:01:33  lr: 0.000087  loss: 0.2131 (0.2227)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [640/689]  eta: 0:01:17  lr: 0.000087  loss: 0.2131 (0.2228)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [650/689]  eta: 0:01:01  lr: 0.000087  loss: 0.2058 (0.2226)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [660/689]  eta: 0:00:45  lr: 0.000087  loss: 0.2155 (0.2227)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [670/689]  eta: 0:00:29  lr: 0.000087  loss: 0.2176 (0.2226)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [680/689]  eta: 0:00:14  lr: 0.000087  loss: 0.2209 (0.2227)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220]  [688/689]  eta: 0:00:01  lr: 0.000087  loss: 0.2321 (0.2228)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:220] Total time: 0:18:06 (1.5773 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: 0.2321 (0.2228)\n",
      "Valid: [epoch:220]  [ 0/14]  eta: 0:00:14  loss: 0.2185 (0.2185)  time: 1.0114  data: 0.4025  max mem: 39763\n",
      "Valid: [epoch:220]  [13/14]  eta: 0:00:00  loss: 0.2077 (0.2107)  time: 0.1142  data: 0.0288  max mem: 39763\n",
      "Valid: [epoch:220] Total time: 0:00:01 (0.1236 s / it)\n",
      "Averaged stats: loss: 0.2077 (0.2107)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_220_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.211%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:221]  [  0/689]  eta: 0:11:59  lr: 0.000087  loss: 0.2023 (0.2023)  time: 1.0443  data: 0.5678  max mem: 39763\n",
      "Train: [epoch:221]  [ 10/689]  eta: 0:17:16  lr: 0.000087  loss: 0.2125 (0.2139)  time: 1.5267  data: 0.0517  max mem: 39763\n",
      "Train: [epoch:221]  [ 20/689]  eta: 0:17:16  lr: 0.000087  loss: 0.2117 (0.2112)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [ 30/689]  eta: 0:17:07  lr: 0.000087  loss: 0.2145 (0.2146)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [ 40/689]  eta: 0:16:54  lr: 0.000087  loss: 0.2209 (0.2156)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [ 50/689]  eta: 0:16:40  lr: 0.000087  loss: 0.2217 (0.2193)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [ 60/689]  eta: 0:16:25  lr: 0.000087  loss: 0.2197 (0.2183)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [ 70/689]  eta: 0:16:11  lr: 0.000087  loss: 0.2140 (0.2184)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [ 80/689]  eta: 0:15:56  lr: 0.000087  loss: 0.2084 (0.2195)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [ 90/689]  eta: 0:15:40  lr: 0.000087  loss: 0.2111 (0.2185)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [100/689]  eta: 0:15:25  lr: 0.000087  loss: 0.2111 (0.2184)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [110/689]  eta: 0:15:10  lr: 0.000087  loss: 0.2127 (0.2198)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [120/689]  eta: 0:14:54  lr: 0.000087  loss: 0.2320 (0.2203)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [130/689]  eta: 0:14:39  lr: 0.000087  loss: 0.2245 (0.2206)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [140/689]  eta: 0:14:23  lr: 0.000087  loss: 0.2159 (0.2204)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [150/689]  eta: 0:14:07  lr: 0.000087  loss: 0.2112 (0.2209)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [160/689]  eta: 0:13:52  lr: 0.000087  loss: 0.2059 (0.2203)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [170/689]  eta: 0:13:36  lr: 0.000087  loss: 0.2010 (0.2197)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [180/689]  eta: 0:13:21  lr: 0.000087  loss: 0.2115 (0.2203)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [190/689]  eta: 0:13:05  lr: 0.000087  loss: 0.2115 (0.2204)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [200/689]  eta: 0:12:49  lr: 0.000087  loss: 0.2119 (0.2208)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [210/689]  eta: 0:12:34  lr: 0.000087  loss: 0.2090 (0.2212)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [220/689]  eta: 0:12:18  lr: 0.000087  loss: 0.2107 (0.2212)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [230/689]  eta: 0:12:02  lr: 0.000087  loss: 0.2109 (0.2213)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [240/689]  eta: 0:11:47  lr: 0.000087  loss: 0.2182 (0.2216)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [250/689]  eta: 0:11:31  lr: 0.000087  loss: 0.2227 (0.2216)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [260/689]  eta: 0:11:15  lr: 0.000087  loss: 0.2297 (0.2221)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [270/689]  eta: 0:11:00  lr: 0.000087  loss: 0.2224 (0.2218)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [280/689]  eta: 0:10:44  lr: 0.000087  loss: 0.2183 (0.2219)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [290/689]  eta: 0:10:28  lr: 0.000087  loss: 0.2098 (0.2217)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [300/689]  eta: 0:10:12  lr: 0.000087  loss: 0.2177 (0.2218)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [310/689]  eta: 0:09:57  lr: 0.000087  loss: 0.2142 (0.2214)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [320/689]  eta: 0:09:41  lr: 0.000087  loss: 0.2093 (0.2214)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [330/689]  eta: 0:09:25  lr: 0.000087  loss: 0.2225 (0.2217)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [340/689]  eta: 0:09:09  lr: 0.000087  loss: 0.2254 (0.2218)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [350/689]  eta: 0:08:54  lr: 0.000087  loss: 0.2146 (0.2215)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [360/689]  eta: 0:08:38  lr: 0.000087  loss: 0.2194 (0.2220)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [370/689]  eta: 0:08:22  lr: 0.000087  loss: 0.2307 (0.2223)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [380/689]  eta: 0:08:06  lr: 0.000087  loss: 0.2298 (0.2223)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [390/689]  eta: 0:07:51  lr: 0.000087  loss: 0.2261 (0.2225)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [400/689]  eta: 0:07:35  lr: 0.000087  loss: 0.2281 (0.2228)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [410/689]  eta: 0:07:19  lr: 0.000087  loss: 0.2212 (0.2227)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [420/689]  eta: 0:07:03  lr: 0.000087  loss: 0.2124 (0.2227)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [430/689]  eta: 0:06:48  lr: 0.000087  loss: 0.2171 (0.2226)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [440/689]  eta: 0:06:32  lr: 0.000087  loss: 0.2177 (0.2228)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [450/689]  eta: 0:06:16  lr: 0.000087  loss: 0.2240 (0.2228)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [460/689]  eta: 0:06:00  lr: 0.000087  loss: 0.2319 (0.2234)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [470/689]  eta: 0:05:45  lr: 0.000087  loss: 0.2261 (0.2232)  time: 1.5783  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:221]  [480/689]  eta: 0:05:29  lr: 0.000087  loss: 0.2131 (0.2234)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [490/689]  eta: 0:05:13  lr: 0.000087  loss: 0.2207 (0.2235)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [500/689]  eta: 0:04:57  lr: 0.000087  loss: 0.2173 (0.2234)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [510/689]  eta: 0:04:42  lr: 0.000087  loss: 0.2124 (0.2233)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [520/689]  eta: 0:04:26  lr: 0.000087  loss: 0.2124 (0.2234)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [530/689]  eta: 0:04:10  lr: 0.000087  loss: 0.2275 (0.2236)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [540/689]  eta: 0:03:54  lr: 0.000087  loss: 0.2252 (0.2235)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [550/689]  eta: 0:03:39  lr: 0.000087  loss: 0.2095 (0.2233)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [560/689]  eta: 0:03:23  lr: 0.000087  loss: 0.2115 (0.2231)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [570/689]  eta: 0:03:07  lr: 0.000087  loss: 0.2149 (0.2231)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [580/689]  eta: 0:02:51  lr: 0.000087  loss: 0.2174 (0.2231)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [590/689]  eta: 0:02:36  lr: 0.000087  loss: 0.2191 (0.2233)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [600/689]  eta: 0:02:20  lr: 0.000087  loss: 0.2299 (0.2235)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [610/689]  eta: 0:02:04  lr: 0.000087  loss: 0.2281 (0.2235)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [620/689]  eta: 0:01:48  lr: 0.000087  loss: 0.2120 (0.2235)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [630/689]  eta: 0:01:33  lr: 0.000087  loss: 0.2157 (0.2235)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [640/689]  eta: 0:01:17  lr: 0.000087  loss: 0.2224 (0.2236)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [650/689]  eta: 0:01:01  lr: 0.000087  loss: 0.2224 (0.2237)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [660/689]  eta: 0:00:45  lr: 0.000087  loss: 0.2414 (0.2239)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [670/689]  eta: 0:00:29  lr: 0.000087  loss: 0.2145 (0.2238)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [680/689]  eta: 0:00:14  lr: 0.000087  loss: 0.2098 (0.2236)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221]  [688/689]  eta: 0:00:01  lr: 0.000087  loss: 0.2150 (0.2238)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:221] Total time: 0:18:06 (1.5775 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: 0.2150 (0.2238)\n",
      "Valid: [epoch:221]  [ 0/14]  eta: 0:00:13  loss: 0.2211 (0.2211)  time: 0.9942  data: 0.3486  max mem: 39763\n",
      "Valid: [epoch:221]  [13/14]  eta: 0:00:00  loss: 0.2083 (0.2112)  time: 0.1130  data: 0.0250  max mem: 39763\n",
      "Valid: [epoch:221] Total time: 0:00:01 (0.1223 s / it)\n",
      "Averaged stats: loss: 0.2083 (0.2112)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_221_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.211%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:222]  [  0/689]  eta: 0:11:33  lr: 0.000087  loss: 0.2518 (0.2518)  time: 1.0067  data: 0.5283  max mem: 39763\n",
      "Train: [epoch:222]  [ 10/689]  eta: 0:17:15  lr: 0.000087  loss: 0.2006 (0.2121)  time: 1.5253  data: 0.0481  max mem: 39763\n",
      "Train: [epoch:222]  [ 20/689]  eta: 0:17:17  lr: 0.000087  loss: 0.2085 (0.2155)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [ 30/689]  eta: 0:17:07  lr: 0.000087  loss: 0.2139 (0.2232)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [ 40/689]  eta: 0:16:54  lr: 0.000087  loss: 0.2238 (0.2215)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [ 50/689]  eta: 0:16:40  lr: 0.000087  loss: 0.2137 (0.2232)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [ 60/689]  eta: 0:16:26  lr: 0.000087  loss: 0.2135 (0.2214)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [ 70/689]  eta: 0:16:11  lr: 0.000087  loss: 0.2060 (0.2195)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [ 80/689]  eta: 0:15:56  lr: 0.000087  loss: 0.2085 (0.2197)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [ 90/689]  eta: 0:15:41  lr: 0.000087  loss: 0.2158 (0.2202)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [100/689]  eta: 0:15:25  lr: 0.000087  loss: 0.2192 (0.2229)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [110/689]  eta: 0:15:10  lr: 0.000087  loss: 0.2303 (0.2239)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [120/689]  eta: 0:14:55  lr: 0.000087  loss: 0.2260 (0.2245)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [130/689]  eta: 0:14:39  lr: 0.000087  loss: 0.2216 (0.2236)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [140/689]  eta: 0:14:23  lr: 0.000087  loss: 0.2107 (0.2242)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [150/689]  eta: 0:14:08  lr: 0.000087  loss: 0.2343 (0.2247)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [160/689]  eta: 0:13:52  lr: 0.000087  loss: 0.2343 (0.2257)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [170/689]  eta: 0:13:37  lr: 0.000087  loss: 0.2371 (0.2261)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [180/689]  eta: 0:13:21  lr: 0.000087  loss: 0.2173 (0.2260)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [190/689]  eta: 0:13:05  lr: 0.000087  loss: 0.2210 (0.2268)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [200/689]  eta: 0:12:50  lr: 0.000087  loss: 0.2326 (0.2270)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [210/689]  eta: 0:12:34  lr: 0.000087  loss: 0.2183 (0.2267)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [220/689]  eta: 0:12:18  lr: 0.000087  loss: 0.2148 (0.2266)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [230/689]  eta: 0:12:03  lr: 0.000087  loss: 0.2124 (0.2262)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [240/689]  eta: 0:11:47  lr: 0.000087  loss: 0.2255 (0.2265)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [250/689]  eta: 0:11:31  lr: 0.000087  loss: 0.2345 (0.2268)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [260/689]  eta: 0:11:15  lr: 0.000087  loss: 0.2248 (0.2262)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [270/689]  eta: 0:11:00  lr: 0.000087  loss: 0.2041 (0.2262)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [280/689]  eta: 0:10:44  lr: 0.000087  loss: 0.2156 (0.2261)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [290/689]  eta: 0:10:28  lr: 0.000087  loss: 0.2139 (0.2258)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [300/689]  eta: 0:10:13  lr: 0.000087  loss: 0.2119 (0.2258)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [310/689]  eta: 0:09:57  lr: 0.000087  loss: 0.2078 (0.2252)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [320/689]  eta: 0:09:41  lr: 0.000087  loss: 0.2071 (0.2249)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [330/689]  eta: 0:09:25  lr: 0.000087  loss: 0.2129 (0.2251)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [340/689]  eta: 0:09:10  lr: 0.000087  loss: 0.2192 (0.2253)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [350/689]  eta: 0:08:54  lr: 0.000087  loss: 0.2368 (0.2259)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [360/689]  eta: 0:08:38  lr: 0.000087  loss: 0.2368 (0.2261)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [370/689]  eta: 0:08:22  lr: 0.000087  loss: 0.2150 (0.2257)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [380/689]  eta: 0:08:07  lr: 0.000087  loss: 0.2170 (0.2258)  time: 1.5776  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:222]  [390/689]  eta: 0:07:51  lr: 0.000087  loss: 0.2226 (0.2258)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [400/689]  eta: 0:07:35  lr: 0.000087  loss: 0.2262 (0.2260)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [410/689]  eta: 0:07:19  lr: 0.000087  loss: 0.2262 (0.2259)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [420/689]  eta: 0:07:04  lr: 0.000087  loss: 0.2189 (0.2258)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [430/689]  eta: 0:06:48  lr: 0.000087  loss: 0.2171 (0.2260)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [440/689]  eta: 0:06:32  lr: 0.000087  loss: 0.2320 (0.2265)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [450/689]  eta: 0:06:16  lr: 0.000087  loss: 0.2302 (0.2263)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [460/689]  eta: 0:06:01  lr: 0.000087  loss: 0.2243 (0.2265)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [470/689]  eta: 0:05:45  lr: 0.000087  loss: 0.2243 (0.2264)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [480/689]  eta: 0:05:29  lr: 0.000087  loss: 0.2293 (0.2264)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [490/689]  eta: 0:05:13  lr: 0.000087  loss: 0.2328 (0.2265)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [500/689]  eta: 0:04:57  lr: 0.000087  loss: 0.2216 (0.2265)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [510/689]  eta: 0:04:42  lr: 0.000087  loss: 0.2136 (0.2264)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [520/689]  eta: 0:04:26  lr: 0.000087  loss: 0.2234 (0.2264)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [530/689]  eta: 0:04:10  lr: 0.000087  loss: 0.2277 (0.2265)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [540/689]  eta: 0:03:54  lr: 0.000087  loss: 0.2197 (0.2263)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [550/689]  eta: 0:03:39  lr: 0.000087  loss: 0.2122 (0.2261)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [560/689]  eta: 0:03:23  lr: 0.000087  loss: 0.2161 (0.2262)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [570/689]  eta: 0:03:07  lr: 0.000087  loss: 0.2161 (0.2262)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [580/689]  eta: 0:02:51  lr: 0.000087  loss: 0.2144 (0.2262)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [590/689]  eta: 0:02:36  lr: 0.000087  loss: 0.2350 (0.2264)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [600/689]  eta: 0:02:20  lr: 0.000087  loss: 0.2367 (0.2264)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [610/689]  eta: 0:02:04  lr: 0.000087  loss: 0.2167 (0.2263)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [620/689]  eta: 0:01:48  lr: 0.000087  loss: 0.2167 (0.2261)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [630/689]  eta: 0:01:33  lr: 0.000087  loss: 0.2205 (0.2261)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [640/689]  eta: 0:01:17  lr: 0.000087  loss: 0.2227 (0.2260)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [650/689]  eta: 0:01:01  lr: 0.000087  loss: 0.2097 (0.2259)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [660/689]  eta: 0:00:45  lr: 0.000087  loss: 0.2097 (0.2258)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [670/689]  eta: 0:00:29  lr: 0.000087  loss: 0.2208 (0.2257)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [680/689]  eta: 0:00:14  lr: 0.000087  loss: 0.2212 (0.2257)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222]  [688/689]  eta: 0:00:01  lr: 0.000087  loss: 0.2189 (0.2257)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:222] Total time: 0:18:06 (1.5772 s / it)\n",
      "Averaged stats: lr: 0.000087  loss: 0.2189 (0.2257)\n",
      "Valid: [epoch:222]  [ 0/14]  eta: 0:00:14  loss: 0.2087 (0.2087)  time: 1.0211  data: 0.3960  max mem: 39763\n",
      "Valid: [epoch:222]  [13/14]  eta: 0:00:00  loss: 0.2093 (0.2117)  time: 0.1148  data: 0.0283  max mem: 39763\n",
      "Valid: [epoch:222] Total time: 0:00:01 (0.1241 s / it)\n",
      "Averaged stats: loss: 0.2093 (0.2117)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_222_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.212%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:223]  [  0/689]  eta: 0:11:30  lr: 0.000086  loss: 0.2243 (0.2243)  time: 1.0028  data: 0.5269  max mem: 39763\n",
      "Train: [epoch:223]  [ 10/689]  eta: 0:17:13  lr: 0.000086  loss: 0.2047 (0.2066)  time: 1.5228  data: 0.0480  max mem: 39763\n",
      "Train: [epoch:223]  [ 20/689]  eta: 0:17:16  lr: 0.000086  loss: 0.2047 (0.2075)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [ 30/689]  eta: 0:17:06  lr: 0.000086  loss: 0.2097 (0.2112)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [ 40/689]  eta: 0:16:54  lr: 0.000086  loss: 0.2194 (0.2134)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [ 50/689]  eta: 0:16:40  lr: 0.000086  loss: 0.2267 (0.2144)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [ 60/689]  eta: 0:16:25  lr: 0.000086  loss: 0.2267 (0.2157)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [ 70/689]  eta: 0:16:11  lr: 0.000086  loss: 0.2152 (0.2156)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [ 80/689]  eta: 0:15:56  lr: 0.000086  loss: 0.2126 (0.2164)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [ 90/689]  eta: 0:15:40  lr: 0.000086  loss: 0.2190 (0.2163)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [100/689]  eta: 0:15:25  lr: 0.000086  loss: 0.2190 (0.2165)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [110/689]  eta: 0:15:10  lr: 0.000086  loss: 0.2135 (0.2165)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [120/689]  eta: 0:14:54  lr: 0.000086  loss: 0.2174 (0.2172)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [130/689]  eta: 0:14:39  lr: 0.000086  loss: 0.2226 (0.2187)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [140/689]  eta: 0:14:23  lr: 0.000086  loss: 0.2175 (0.2192)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [150/689]  eta: 0:14:08  lr: 0.000086  loss: 0.2172 (0.2201)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [160/689]  eta: 0:13:52  lr: 0.000086  loss: 0.2197 (0.2207)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [170/689]  eta: 0:13:36  lr: 0.000086  loss: 0.2152 (0.2209)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [180/689]  eta: 0:13:21  lr: 0.000086  loss: 0.2231 (0.2210)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [190/689]  eta: 0:13:05  lr: 0.000086  loss: 0.2384 (0.2222)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [200/689]  eta: 0:12:49  lr: 0.000086  loss: 0.2320 (0.2224)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [210/689]  eta: 0:12:34  lr: 0.000086  loss: 0.2249 (0.2219)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [220/689]  eta: 0:12:18  lr: 0.000086  loss: 0.2152 (0.2221)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [230/689]  eta: 0:12:02  lr: 0.000086  loss: 0.2226 (0.2225)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [240/689]  eta: 0:11:47  lr: 0.000086  loss: 0.2174 (0.2221)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [250/689]  eta: 0:11:31  lr: 0.000086  loss: 0.2126 (0.2221)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [260/689]  eta: 0:11:15  lr: 0.000086  loss: 0.2169 (0.2224)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [270/689]  eta: 0:11:00  lr: 0.000086  loss: 0.2107 (0.2220)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [280/689]  eta: 0:10:44  lr: 0.000086  loss: 0.2128 (0.2220)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [290/689]  eta: 0:10:28  lr: 0.000086  loss: 0.2145 (0.2222)  time: 1.5775  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:223]  [300/689]  eta: 0:10:12  lr: 0.000086  loss: 0.2219 (0.2223)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [310/689]  eta: 0:09:57  lr: 0.000086  loss: 0.2219 (0.2225)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [320/689]  eta: 0:09:41  lr: 0.000086  loss: 0.2223 (0.2227)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [330/689]  eta: 0:09:25  lr: 0.000086  loss: 0.2241 (0.2228)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [340/689]  eta: 0:09:09  lr: 0.000086  loss: 0.2187 (0.2228)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [350/689]  eta: 0:08:54  lr: 0.000086  loss: 0.2187 (0.2228)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [360/689]  eta: 0:08:38  lr: 0.000086  loss: 0.2202 (0.2232)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [370/689]  eta: 0:08:22  lr: 0.000086  loss: 0.2176 (0.2231)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [380/689]  eta: 0:08:06  lr: 0.000086  loss: 0.2115 (0.2231)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [390/689]  eta: 0:07:51  lr: 0.000086  loss: 0.2415 (0.2237)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [400/689]  eta: 0:07:35  lr: 0.000086  loss: 0.2336 (0.2236)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [410/689]  eta: 0:07:19  lr: 0.000086  loss: 0.2161 (0.2235)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [420/689]  eta: 0:07:03  lr: 0.000086  loss: 0.2128 (0.2235)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [430/689]  eta: 0:06:48  lr: 0.000086  loss: 0.2097 (0.2235)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [440/689]  eta: 0:06:32  lr: 0.000086  loss: 0.2122 (0.2234)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [450/689]  eta: 0:06:16  lr: 0.000086  loss: 0.2131 (0.2237)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [460/689]  eta: 0:06:00  lr: 0.000086  loss: 0.2259 (0.2240)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [470/689]  eta: 0:05:45  lr: 0.000086  loss: 0.2338 (0.2246)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [480/689]  eta: 0:05:29  lr: 0.000086  loss: 0.2310 (0.2245)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [490/689]  eta: 0:05:13  lr: 0.000086  loss: 0.2288 (0.2246)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [500/689]  eta: 0:04:57  lr: 0.000086  loss: 0.2156 (0.2244)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [510/689]  eta: 0:04:42  lr: 0.000086  loss: 0.2166 (0.2245)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [520/689]  eta: 0:04:26  lr: 0.000086  loss: 0.2373 (0.2247)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [530/689]  eta: 0:04:10  lr: 0.000086  loss: 0.2320 (0.2247)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [540/689]  eta: 0:03:54  lr: 0.000086  loss: 0.2165 (0.2249)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [550/689]  eta: 0:03:39  lr: 0.000086  loss: 0.2177 (0.2249)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [560/689]  eta: 0:03:23  lr: 0.000086  loss: 0.2242 (0.2251)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [570/689]  eta: 0:03:07  lr: 0.000086  loss: 0.2229 (0.2253)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [580/689]  eta: 0:02:51  lr: 0.000086  loss: 0.2131 (0.2250)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [590/689]  eta: 0:02:36  lr: 0.000086  loss: 0.2097 (0.2250)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [600/689]  eta: 0:02:20  lr: 0.000086  loss: 0.2338 (0.2253)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [610/689]  eta: 0:02:04  lr: 0.000086  loss: 0.2232 (0.2251)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [620/689]  eta: 0:01:48  lr: 0.000086  loss: 0.2134 (0.2253)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [630/689]  eta: 0:01:33  lr: 0.000086  loss: 0.2271 (0.2253)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [640/689]  eta: 0:01:17  lr: 0.000086  loss: 0.2231 (0.2255)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [650/689]  eta: 0:01:01  lr: 0.000086  loss: 0.2281 (0.2258)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [660/689]  eta: 0:00:45  lr: 0.000086  loss: 0.2364 (0.2260)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [670/689]  eta: 0:00:29  lr: 0.000086  loss: 0.2327 (0.2260)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [680/689]  eta: 0:00:14  lr: 0.000086  loss: 0.2327 (0.2263)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223]  [688/689]  eta: 0:00:01  lr: 0.000086  loss: 0.2307 (0.2262)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:223] Total time: 0:18:06 (1.5768 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: 0.2307 (0.2262)\n",
      "Valid: [epoch:223]  [ 0/14]  eta: 0:00:14  loss: 0.2340 (0.2340)  time: 1.0395  data: 0.3717  max mem: 39763\n",
      "Valid: [epoch:223]  [13/14]  eta: 0:00:00  loss: 0.2115 (0.2134)  time: 0.1162  data: 0.0266  max mem: 39763\n",
      "Valid: [epoch:223] Total time: 0:00:01 (0.1240 s / it)\n",
      "Averaged stats: loss: 0.2115 (0.2134)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_223_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.213%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:224]  [  0/689]  eta: 0:11:38  lr: 0.000086  loss: 0.2346 (0.2346)  time: 1.0144  data: 0.5393  max mem: 39763\n",
      "Train: [epoch:224]  [ 10/689]  eta: 0:17:15  lr: 0.000086  loss: 0.2167 (0.2207)  time: 1.5253  data: 0.0491  max mem: 39763\n",
      "Train: [epoch:224]  [ 20/689]  eta: 0:17:16  lr: 0.000086  loss: 0.2167 (0.2239)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [ 30/689]  eta: 0:17:07  lr: 0.000086  loss: 0.2339 (0.2299)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [ 40/689]  eta: 0:16:54  lr: 0.000086  loss: 0.2339 (0.2296)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [ 50/689]  eta: 0:16:40  lr: 0.000086  loss: 0.2197 (0.2265)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [ 60/689]  eta: 0:16:25  lr: 0.000086  loss: 0.2275 (0.2276)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [ 70/689]  eta: 0:16:10  lr: 0.000086  loss: 0.2321 (0.2287)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [ 80/689]  eta: 0:15:55  lr: 0.000086  loss: 0.2284 (0.2285)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [ 90/689]  eta: 0:15:40  lr: 0.000086  loss: 0.2296 (0.2294)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [100/689]  eta: 0:15:25  lr: 0.000086  loss: 0.2309 (0.2303)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [110/689]  eta: 0:15:09  lr: 0.000086  loss: 0.2383 (0.2313)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [120/689]  eta: 0:14:54  lr: 0.000086  loss: 0.2316 (0.2313)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [130/689]  eta: 0:14:39  lr: 0.000086  loss: 0.2296 (0.2308)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [140/689]  eta: 0:14:23  lr: 0.000086  loss: 0.2219 (0.2302)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [150/689]  eta: 0:14:07  lr: 0.000086  loss: 0.2173 (0.2300)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [160/689]  eta: 0:13:52  lr: 0.000086  loss: 0.2259 (0.2300)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [170/689]  eta: 0:13:36  lr: 0.000086  loss: 0.2193 (0.2294)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [180/689]  eta: 0:13:21  lr: 0.000086  loss: 0.2193 (0.2292)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [190/689]  eta: 0:13:05  lr: 0.000086  loss: 0.2245 (0.2293)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [200/689]  eta: 0:12:49  lr: 0.000086  loss: 0.2233 (0.2289)  time: 1.5773  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:224]  [210/689]  eta: 0:12:34  lr: 0.000086  loss: 0.2198 (0.2283)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [220/689]  eta: 0:12:18  lr: 0.000086  loss: 0.2225 (0.2283)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [230/689]  eta: 0:12:02  lr: 0.000086  loss: 0.2225 (0.2281)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [240/689]  eta: 0:11:46  lr: 0.000086  loss: 0.2319 (0.2284)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [250/689]  eta: 0:11:31  lr: 0.000086  loss: 0.2237 (0.2277)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [260/689]  eta: 0:11:15  lr: 0.000086  loss: 0.2238 (0.2282)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [270/689]  eta: 0:10:59  lr: 0.000086  loss: 0.2288 (0.2280)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [280/689]  eta: 0:10:44  lr: 0.000086  loss: 0.2273 (0.2282)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [290/689]  eta: 0:10:28  lr: 0.000086  loss: 0.2181 (0.2278)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [300/689]  eta: 0:10:12  lr: 0.000086  loss: 0.2136 (0.2277)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [310/689]  eta: 0:09:56  lr: 0.000086  loss: 0.2124 (0.2274)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [320/689]  eta: 0:09:41  lr: 0.000086  loss: 0.2120 (0.2272)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [330/689]  eta: 0:09:25  lr: 0.000086  loss: 0.2198 (0.2274)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [340/689]  eta: 0:09:09  lr: 0.000086  loss: 0.2212 (0.2273)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [350/689]  eta: 0:08:54  lr: 0.000086  loss: 0.2270 (0.2279)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [360/689]  eta: 0:08:38  lr: 0.000086  loss: 0.2360 (0.2281)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [370/689]  eta: 0:08:22  lr: 0.000086  loss: 0.2349 (0.2285)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [380/689]  eta: 0:08:06  lr: 0.000086  loss: 0.2138 (0.2282)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [390/689]  eta: 0:07:51  lr: 0.000086  loss: 0.2093 (0.2279)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [400/689]  eta: 0:07:35  lr: 0.000086  loss: 0.2097 (0.2279)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [410/689]  eta: 0:07:19  lr: 0.000086  loss: 0.2159 (0.2277)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [420/689]  eta: 0:07:03  lr: 0.000086  loss: 0.2107 (0.2278)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [430/689]  eta: 0:06:48  lr: 0.000086  loss: 0.2421 (0.2281)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [440/689]  eta: 0:06:32  lr: 0.000086  loss: 0.2435 (0.2284)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [450/689]  eta: 0:06:16  lr: 0.000086  loss: 0.2236 (0.2282)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [460/689]  eta: 0:06:00  lr: 0.000086  loss: 0.2229 (0.2282)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [470/689]  eta: 0:05:45  lr: 0.000086  loss: 0.2087 (0.2277)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [480/689]  eta: 0:05:29  lr: 0.000086  loss: 0.2101 (0.2276)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [490/689]  eta: 0:05:13  lr: 0.000086  loss: 0.2246 (0.2277)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [500/689]  eta: 0:04:57  lr: 0.000086  loss: 0.2215 (0.2278)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [510/689]  eta: 0:04:42  lr: 0.000086  loss: 0.2193 (0.2277)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [520/689]  eta: 0:04:26  lr: 0.000086  loss: 0.2212 (0.2279)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [530/689]  eta: 0:04:10  lr: 0.000086  loss: 0.2267 (0.2279)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [540/689]  eta: 0:03:54  lr: 0.000086  loss: 0.2247 (0.2279)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [550/689]  eta: 0:03:39  lr: 0.000086  loss: 0.2232 (0.2279)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [560/689]  eta: 0:03:23  lr: 0.000086  loss: 0.2232 (0.2281)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [570/689]  eta: 0:03:07  lr: 0.000086  loss: 0.2202 (0.2280)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [580/689]  eta: 0:02:51  lr: 0.000086  loss: 0.2159 (0.2279)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [590/689]  eta: 0:02:36  lr: 0.000086  loss: 0.2248 (0.2279)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [600/689]  eta: 0:02:20  lr: 0.000086  loss: 0.2226 (0.2277)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [610/689]  eta: 0:02:04  lr: 0.000086  loss: 0.2129 (0.2276)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [620/689]  eta: 0:01:48  lr: 0.000086  loss: 0.2214 (0.2275)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [630/689]  eta: 0:01:32  lr: 0.000086  loss: 0.2131 (0.2274)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [640/689]  eta: 0:01:17  lr: 0.000086  loss: 0.2244 (0.2276)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [650/689]  eta: 0:01:01  lr: 0.000086  loss: 0.2314 (0.2276)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [660/689]  eta: 0:00:45  lr: 0.000086  loss: 0.2253 (0.2277)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [670/689]  eta: 0:00:29  lr: 0.000086  loss: 0.2227 (0.2276)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [680/689]  eta: 0:00:14  lr: 0.000086  loss: 0.2302 (0.2278)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224]  [688/689]  eta: 0:00:01  lr: 0.000086  loss: 0.2191 (0.2277)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:224] Total time: 0:18:06 (1.5765 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: 0.2191 (0.2277)\n",
      "Valid: [epoch:224]  [ 0/14]  eta: 0:00:13  loss: 0.2368 (0.2368)  time: 0.9830  data: 0.3448  max mem: 39763\n",
      "Valid: [epoch:224]  [13/14]  eta: 0:00:00  loss: 0.2158 (0.2177)  time: 0.1121  data: 0.0247  max mem: 39763\n",
      "Valid: [epoch:224] Total time: 0:00:01 (0.1193 s / it)\n",
      "Averaged stats: loss: 0.2158 (0.2177)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_224_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.218%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:225]  [  0/689]  eta: 0:11:46  lr: 0.000086  loss: 0.2636 (0.2636)  time: 1.0260  data: 0.5568  max mem: 39763\n",
      "Train: [epoch:225]  [ 10/689]  eta: 0:17:14  lr: 0.000086  loss: 0.2164 (0.2278)  time: 1.5240  data: 0.0507  max mem: 39763\n",
      "Train: [epoch:225]  [ 20/689]  eta: 0:17:16  lr: 0.000086  loss: 0.2203 (0.2273)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [ 30/689]  eta: 0:17:06  lr: 0.000086  loss: 0.2148 (0.2225)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [ 40/689]  eta: 0:16:53  lr: 0.000086  loss: 0.2124 (0.2214)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [ 50/689]  eta: 0:16:39  lr: 0.000086  loss: 0.2172 (0.2221)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [ 60/689]  eta: 0:16:25  lr: 0.000086  loss: 0.2195 (0.2224)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [ 70/689]  eta: 0:16:10  lr: 0.000086  loss: 0.2151 (0.2220)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [ 80/689]  eta: 0:15:55  lr: 0.000086  loss: 0.2128 (0.2230)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [ 90/689]  eta: 0:15:40  lr: 0.000086  loss: 0.2108 (0.2226)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [100/689]  eta: 0:15:25  lr: 0.000086  loss: 0.2232 (0.2232)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [110/689]  eta: 0:15:09  lr: 0.000086  loss: 0.2311 (0.2243)  time: 1.5770  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:225]  [120/689]  eta: 0:14:54  lr: 0.000086  loss: 0.2239 (0.2242)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [130/689]  eta: 0:14:38  lr: 0.000086  loss: 0.2130 (0.2240)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [140/689]  eta: 0:14:23  lr: 0.000086  loss: 0.2188 (0.2250)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [150/689]  eta: 0:14:07  lr: 0.000086  loss: 0.2372 (0.2261)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [160/689]  eta: 0:13:52  lr: 0.000086  loss: 0.2241 (0.2262)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [170/689]  eta: 0:13:36  lr: 0.000086  loss: 0.2180 (0.2271)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [180/689]  eta: 0:13:20  lr: 0.000086  loss: 0.2190 (0.2269)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [190/689]  eta: 0:13:05  lr: 0.000086  loss: 0.2295 (0.2278)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [200/689]  eta: 0:12:49  lr: 0.000086  loss: 0.2279 (0.2273)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [210/689]  eta: 0:12:34  lr: 0.000086  loss: 0.2203 (0.2275)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [220/689]  eta: 0:12:18  lr: 0.000086  loss: 0.2170 (0.2276)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [230/689]  eta: 0:12:02  lr: 0.000086  loss: 0.2161 (0.2275)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [240/689]  eta: 0:11:46  lr: 0.000086  loss: 0.2170 (0.2274)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [250/689]  eta: 0:11:31  lr: 0.000086  loss: 0.2206 (0.2277)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [260/689]  eta: 0:11:15  lr: 0.000086  loss: 0.2125 (0.2275)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [270/689]  eta: 0:10:59  lr: 0.000086  loss: 0.2122 (0.2271)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [280/689]  eta: 0:10:44  lr: 0.000086  loss: 0.2207 (0.2272)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [290/689]  eta: 0:10:28  lr: 0.000086  loss: 0.2145 (0.2269)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [300/689]  eta: 0:10:12  lr: 0.000086  loss: 0.2124 (0.2265)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [310/689]  eta: 0:09:56  lr: 0.000086  loss: 0.2095 (0.2259)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [320/689]  eta: 0:09:41  lr: 0.000086  loss: 0.2151 (0.2263)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [330/689]  eta: 0:09:25  lr: 0.000086  loss: 0.2231 (0.2262)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [340/689]  eta: 0:09:09  lr: 0.000086  loss: 0.2177 (0.2262)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [350/689]  eta: 0:08:54  lr: 0.000086  loss: 0.2324 (0.2267)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [360/689]  eta: 0:08:38  lr: 0.000086  loss: 0.2329 (0.2270)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [370/689]  eta: 0:08:22  lr: 0.000086  loss: 0.2178 (0.2268)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [380/689]  eta: 0:08:06  lr: 0.000086  loss: 0.2147 (0.2266)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [390/689]  eta: 0:07:51  lr: 0.000086  loss: 0.2246 (0.2266)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [400/689]  eta: 0:07:35  lr: 0.000086  loss: 0.2367 (0.2275)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [410/689]  eta: 0:07:19  lr: 0.000086  loss: 0.2456 (0.2276)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [420/689]  eta: 0:07:03  lr: 0.000086  loss: 0.2237 (0.2277)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [430/689]  eta: 0:06:48  lr: 0.000086  loss: 0.2249 (0.2279)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [440/689]  eta: 0:06:32  lr: 0.000086  loss: 0.2249 (0.2279)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [450/689]  eta: 0:06:16  lr: 0.000086  loss: 0.2239 (0.2279)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [460/689]  eta: 0:06:00  lr: 0.000086  loss: 0.2426 (0.2286)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [470/689]  eta: 0:05:45  lr: 0.000086  loss: 0.2426 (0.2286)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [480/689]  eta: 0:05:29  lr: 0.000086  loss: 0.2153 (0.2284)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [490/689]  eta: 0:05:13  lr: 0.000086  loss: 0.2277 (0.2287)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [500/689]  eta: 0:04:57  lr: 0.000086  loss: 0.2343 (0.2289)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [510/689]  eta: 0:04:42  lr: 0.000086  loss: 0.2194 (0.2286)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [520/689]  eta: 0:04:26  lr: 0.000086  loss: 0.2181 (0.2287)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [530/689]  eta: 0:04:10  lr: 0.000086  loss: 0.2256 (0.2288)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [540/689]  eta: 0:03:54  lr: 0.000086  loss: 0.2250 (0.2288)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [550/689]  eta: 0:03:39  lr: 0.000086  loss: 0.2240 (0.2289)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [560/689]  eta: 0:03:23  lr: 0.000086  loss: 0.2240 (0.2289)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [570/689]  eta: 0:03:07  lr: 0.000086  loss: 0.2247 (0.2289)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [580/689]  eta: 0:02:51  lr: 0.000086  loss: 0.2247 (0.2288)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [590/689]  eta: 0:02:36  lr: 0.000086  loss: 0.2265 (0.2291)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [600/689]  eta: 0:02:20  lr: 0.000086  loss: 0.2248 (0.2292)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [610/689]  eta: 0:02:04  lr: 0.000086  loss: 0.2227 (0.2292)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [620/689]  eta: 0:01:48  lr: 0.000086  loss: 0.2266 (0.2292)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [630/689]  eta: 0:01:33  lr: 0.000086  loss: 0.2212 (0.2292)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [640/689]  eta: 0:01:17  lr: 0.000086  loss: 0.2139 (0.2291)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [650/689]  eta: 0:01:01  lr: 0.000086  loss: 0.2068 (0.2289)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [660/689]  eta: 0:00:45  lr: 0.000086  loss: 0.2113 (0.2287)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [670/689]  eta: 0:00:29  lr: 0.000086  loss: 0.2146 (0.2287)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [680/689]  eta: 0:00:14  lr: 0.000086  loss: 0.2146 (0.2286)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225]  [688/689]  eta: 0:00:01  lr: 0.000086  loss: 0.2228 (0.2287)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:225] Total time: 0:18:06 (1.5768 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: 0.2228 (0.2287)\n",
      "Valid: [epoch:225]  [ 0/14]  eta: 0:00:14  loss: 0.2063 (0.2063)  time: 1.0236  data: 0.3452  max mem: 39763\n",
      "Valid: [epoch:225]  [13/14]  eta: 0:00:00  loss: 0.2122 (0.2143)  time: 0.1150  data: 0.0247  max mem: 39763\n",
      "Valid: [epoch:225] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.2122 (0.2143)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_225_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.214%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:226]  [  0/689]  eta: 0:11:23  lr: 0.000086  loss: 0.2222 (0.2222)  time: 0.9918  data: 0.5190  max mem: 39763\n",
      "Train: [epoch:226]  [ 10/689]  eta: 0:17:14  lr: 0.000086  loss: 0.2216 (0.2239)  time: 1.5235  data: 0.0473  max mem: 39763\n",
      "Train: [epoch:226]  [ 20/689]  eta: 0:17:16  lr: 0.000086  loss: 0.2179 (0.2203)  time: 1.5774  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:226]  [ 30/689]  eta: 0:17:07  lr: 0.000086  loss: 0.2207 (0.2247)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [ 40/689]  eta: 0:16:54  lr: 0.000086  loss: 0.2173 (0.2242)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [ 50/689]  eta: 0:16:40  lr: 0.000086  loss: 0.2173 (0.2249)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [ 60/689]  eta: 0:16:26  lr: 0.000086  loss: 0.2112 (0.2214)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [ 70/689]  eta: 0:16:11  lr: 0.000086  loss: 0.2043 (0.2215)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [ 80/689]  eta: 0:15:56  lr: 0.000086  loss: 0.2119 (0.2218)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [ 90/689]  eta: 0:15:41  lr: 0.000086  loss: 0.2140 (0.2209)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [100/689]  eta: 0:15:26  lr: 0.000086  loss: 0.2218 (0.2220)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [110/689]  eta: 0:15:10  lr: 0.000086  loss: 0.2296 (0.2234)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [120/689]  eta: 0:14:55  lr: 0.000086  loss: 0.2272 (0.2244)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [130/689]  eta: 0:14:39  lr: 0.000086  loss: 0.2192 (0.2242)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [140/689]  eta: 0:14:24  lr: 0.000086  loss: 0.2234 (0.2252)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [150/689]  eta: 0:14:08  lr: 0.000086  loss: 0.2309 (0.2258)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [160/689]  eta: 0:13:53  lr: 0.000086  loss: 0.2256 (0.2267)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [170/689]  eta: 0:13:37  lr: 0.000086  loss: 0.2246 (0.2268)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [180/689]  eta: 0:13:21  lr: 0.000086  loss: 0.2274 (0.2273)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [190/689]  eta: 0:13:06  lr: 0.000086  loss: 0.2274 (0.2274)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [200/689]  eta: 0:12:50  lr: 0.000086  loss: 0.2229 (0.2274)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [210/689]  eta: 0:12:34  lr: 0.000086  loss: 0.2217 (0.2274)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [220/689]  eta: 0:12:19  lr: 0.000086  loss: 0.2171 (0.2269)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [230/689]  eta: 0:12:03  lr: 0.000086  loss: 0.2186 (0.2268)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [240/689]  eta: 0:11:47  lr: 0.000086  loss: 0.2267 (0.2271)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [250/689]  eta: 0:11:32  lr: 0.000086  loss: 0.2288 (0.2273)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [260/689]  eta: 0:11:16  lr: 0.000086  loss: 0.2378 (0.2276)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [270/689]  eta: 0:11:00  lr: 0.000086  loss: 0.2378 (0.2277)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [280/689]  eta: 0:10:44  lr: 0.000086  loss: 0.2245 (0.2280)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [290/689]  eta: 0:10:29  lr: 0.000086  loss: 0.2245 (0.2281)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [300/689]  eta: 0:10:13  lr: 0.000086  loss: 0.2272 (0.2281)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [310/689]  eta: 0:09:57  lr: 0.000086  loss: 0.2139 (0.2277)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [320/689]  eta: 0:09:41  lr: 0.000086  loss: 0.2280 (0.2282)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [330/689]  eta: 0:09:26  lr: 0.000086  loss: 0.2395 (0.2281)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [340/689]  eta: 0:09:10  lr: 0.000086  loss: 0.2245 (0.2284)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [350/689]  eta: 0:08:54  lr: 0.000086  loss: 0.2214 (0.2278)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [360/689]  eta: 0:08:38  lr: 0.000086  loss: 0.2108 (0.2278)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [370/689]  eta: 0:08:23  lr: 0.000086  loss: 0.2135 (0.2276)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [380/689]  eta: 0:08:07  lr: 0.000086  loss: 0.2135 (0.2273)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [390/689]  eta: 0:07:51  lr: 0.000086  loss: 0.2238 (0.2276)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [400/689]  eta: 0:07:35  lr: 0.000086  loss: 0.2287 (0.2277)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [410/689]  eta: 0:07:20  lr: 0.000086  loss: 0.2168 (0.2272)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [420/689]  eta: 0:07:04  lr: 0.000086  loss: 0.2088 (0.2270)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [430/689]  eta: 0:06:48  lr: 0.000086  loss: 0.2250 (0.2274)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [440/689]  eta: 0:06:32  lr: 0.000086  loss: 0.2328 (0.2276)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [450/689]  eta: 0:06:17  lr: 0.000086  loss: 0.2242 (0.2275)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [460/689]  eta: 0:06:01  lr: 0.000086  loss: 0.2267 (0.2278)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [470/689]  eta: 0:05:45  lr: 0.000086  loss: 0.2267 (0.2278)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [480/689]  eta: 0:05:29  lr: 0.000086  loss: 0.2143 (0.2278)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [490/689]  eta: 0:05:13  lr: 0.000086  loss: 0.2362 (0.2282)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [500/689]  eta: 0:04:58  lr: 0.000086  loss: 0.2381 (0.2286)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [510/689]  eta: 0:04:42  lr: 0.000086  loss: 0.2175 (0.2284)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [520/689]  eta: 0:04:26  lr: 0.000086  loss: 0.2194 (0.2284)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [530/689]  eta: 0:04:10  lr: 0.000086  loss: 0.2233 (0.2284)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [540/689]  eta: 0:03:55  lr: 0.000086  loss: 0.2226 (0.2282)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [550/689]  eta: 0:03:39  lr: 0.000086  loss: 0.2217 (0.2284)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [560/689]  eta: 0:03:23  lr: 0.000086  loss: 0.2216 (0.2284)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [570/689]  eta: 0:03:07  lr: 0.000086  loss: 0.2127 (0.2281)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [580/689]  eta: 0:02:51  lr: 0.000086  loss: 0.2167 (0.2281)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [590/689]  eta: 0:02:36  lr: 0.000086  loss: 0.2189 (0.2281)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [600/689]  eta: 0:02:20  lr: 0.000086  loss: 0.2266 (0.2282)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [610/689]  eta: 0:02:04  lr: 0.000086  loss: 0.2266 (0.2281)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [620/689]  eta: 0:01:48  lr: 0.000086  loss: 0.2195 (0.2281)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [630/689]  eta: 0:01:33  lr: 0.000086  loss: 0.2326 (0.2282)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [640/689]  eta: 0:01:17  lr: 0.000086  loss: 0.2326 (0.2284)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [650/689]  eta: 0:01:01  lr: 0.000086  loss: 0.2198 (0.2283)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [660/689]  eta: 0:00:45  lr: 0.000086  loss: 0.2206 (0.2286)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [670/689]  eta: 0:00:29  lr: 0.000086  loss: 0.2241 (0.2285)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226]  [680/689]  eta: 0:00:14  lr: 0.000086  loss: 0.2241 (0.2286)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:226]  [688/689]  eta: 0:00:01  lr: 0.000086  loss: 0.2241 (0.2284)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:226] Total time: 0:18:07 (1.5781 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: 0.2241 (0.2284)\n",
      "Valid: [epoch:226]  [ 0/14]  eta: 0:00:13  loss: 0.2293 (0.2293)  time: 0.9928  data: 0.3514  max mem: 39763\n",
      "Valid: [epoch:226]  [13/14]  eta: 0:00:00  loss: 0.2143 (0.2169)  time: 0.1128  data: 0.0251  max mem: 39763\n",
      "Valid: [epoch:226] Total time: 0:00:01 (0.1203 s / it)\n",
      "Averaged stats: loss: 0.2143 (0.2169)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_226_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.217%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:227]  [  0/689]  eta: 0:13:35  lr: 0.000086  loss: 0.2897 (0.2897)  time: 1.1836  data: 0.7074  max mem: 39763\n",
      "Train: [epoch:227]  [ 10/689]  eta: 0:17:25  lr: 0.000086  loss: 0.2257 (0.2296)  time: 1.5398  data: 0.0644  max mem: 39763\n",
      "Train: [epoch:227]  [ 20/689]  eta: 0:17:21  lr: 0.000086  loss: 0.2156 (0.2206)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [ 30/689]  eta: 0:17:10  lr: 0.000086  loss: 0.2135 (0.2260)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [ 40/689]  eta: 0:16:56  lr: 0.000086  loss: 0.2176 (0.2238)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [ 50/689]  eta: 0:16:42  lr: 0.000086  loss: 0.2241 (0.2276)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [ 60/689]  eta: 0:16:27  lr: 0.000086  loss: 0.2262 (0.2287)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [ 70/689]  eta: 0:16:12  lr: 0.000086  loss: 0.2262 (0.2299)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [ 80/689]  eta: 0:15:57  lr: 0.000086  loss: 0.2296 (0.2293)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [ 90/689]  eta: 0:15:41  lr: 0.000086  loss: 0.2158 (0.2298)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [100/689]  eta: 0:15:26  lr: 0.000086  loss: 0.2154 (0.2287)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [110/689]  eta: 0:15:11  lr: 0.000086  loss: 0.2182 (0.2283)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [120/689]  eta: 0:14:55  lr: 0.000086  loss: 0.2259 (0.2286)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [130/689]  eta: 0:14:39  lr: 0.000086  loss: 0.2360 (0.2289)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [140/689]  eta: 0:14:24  lr: 0.000086  loss: 0.2373 (0.2298)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [150/689]  eta: 0:14:08  lr: 0.000086  loss: 0.2290 (0.2295)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [160/689]  eta: 0:13:53  lr: 0.000086  loss: 0.2233 (0.2298)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [170/689]  eta: 0:13:37  lr: 0.000086  loss: 0.2341 (0.2297)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [180/689]  eta: 0:13:21  lr: 0.000086  loss: 0.2205 (0.2298)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [190/689]  eta: 0:13:06  lr: 0.000086  loss: 0.2257 (0.2296)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [200/689]  eta: 0:12:50  lr: 0.000086  loss: 0.2317 (0.2297)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [210/689]  eta: 0:12:34  lr: 0.000086  loss: 0.2311 (0.2298)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [220/689]  eta: 0:12:19  lr: 0.000086  loss: 0.2256 (0.2295)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [230/689]  eta: 0:12:03  lr: 0.000086  loss: 0.2245 (0.2293)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [240/689]  eta: 0:11:47  lr: 0.000086  loss: 0.2183 (0.2295)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [250/689]  eta: 0:11:31  lr: 0.000086  loss: 0.2315 (0.2303)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [260/689]  eta: 0:11:16  lr: 0.000086  loss: 0.2318 (0.2303)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [270/689]  eta: 0:11:00  lr: 0.000086  loss: 0.2277 (0.2303)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [280/689]  eta: 0:10:44  lr: 0.000086  loss: 0.2277 (0.2303)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [290/689]  eta: 0:10:29  lr: 0.000086  loss: 0.2235 (0.2303)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [300/689]  eta: 0:10:13  lr: 0.000086  loss: 0.2156 (0.2303)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [310/689]  eta: 0:09:57  lr: 0.000086  loss: 0.2196 (0.2302)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [320/689]  eta: 0:09:41  lr: 0.000086  loss: 0.2254 (0.2300)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [330/689]  eta: 0:09:26  lr: 0.000086  loss: 0.2217 (0.2298)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [340/689]  eta: 0:09:10  lr: 0.000086  loss: 0.2171 (0.2294)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [350/689]  eta: 0:08:54  lr: 0.000086  loss: 0.2171 (0.2295)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [360/689]  eta: 0:08:38  lr: 0.000086  loss: 0.2264 (0.2296)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [370/689]  eta: 0:08:23  lr: 0.000086  loss: 0.2246 (0.2295)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [380/689]  eta: 0:08:07  lr: 0.000086  loss: 0.2233 (0.2296)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [390/689]  eta: 0:07:51  lr: 0.000086  loss: 0.2267 (0.2297)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [400/689]  eta: 0:07:35  lr: 0.000086  loss: 0.2281 (0.2298)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [410/689]  eta: 0:07:19  lr: 0.000086  loss: 0.2290 (0.2300)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [420/689]  eta: 0:07:04  lr: 0.000086  loss: 0.2309 (0.2300)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [430/689]  eta: 0:06:48  lr: 0.000086  loss: 0.2229 (0.2299)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [440/689]  eta: 0:06:32  lr: 0.000086  loss: 0.2197 (0.2298)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [450/689]  eta: 0:06:16  lr: 0.000086  loss: 0.2165 (0.2295)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [460/689]  eta: 0:06:01  lr: 0.000086  loss: 0.2189 (0.2299)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [470/689]  eta: 0:05:45  lr: 0.000086  loss: 0.2364 (0.2300)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [480/689]  eta: 0:05:29  lr: 0.000086  loss: 0.2169 (0.2298)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [490/689]  eta: 0:05:13  lr: 0.000086  loss: 0.2276 (0.2299)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [500/689]  eta: 0:04:58  lr: 0.000086  loss: 0.2230 (0.2299)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [510/689]  eta: 0:04:42  lr: 0.000086  loss: 0.2131 (0.2298)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [520/689]  eta: 0:04:26  lr: 0.000086  loss: 0.2289 (0.2302)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [530/689]  eta: 0:04:10  lr: 0.000086  loss: 0.2277 (0.2302)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [540/689]  eta: 0:03:55  lr: 0.000086  loss: 0.2256 (0.2305)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [550/689]  eta: 0:03:39  lr: 0.000086  loss: 0.2302 (0.2305)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [560/689]  eta: 0:03:23  lr: 0.000086  loss: 0.2269 (0.2303)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [570/689]  eta: 0:03:07  lr: 0.000086  loss: 0.2125 (0.2300)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [580/689]  eta: 0:02:51  lr: 0.000086  loss: 0.2144 (0.2300)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [590/689]  eta: 0:02:36  lr: 0.000086  loss: 0.2271 (0.2301)  time: 1.5824  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:227]  [600/689]  eta: 0:02:20  lr: 0.000086  loss: 0.2248 (0.2301)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [610/689]  eta: 0:02:04  lr: 0.000086  loss: 0.2232 (0.2299)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [620/689]  eta: 0:01:48  lr: 0.000086  loss: 0.2185 (0.2298)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [630/689]  eta: 0:01:33  lr: 0.000086  loss: 0.2208 (0.2298)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [640/689]  eta: 0:01:17  lr: 0.000086  loss: 0.2252 (0.2299)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [650/689]  eta: 0:01:01  lr: 0.000086  loss: 0.2386 (0.2300)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [660/689]  eta: 0:00:45  lr: 0.000086  loss: 0.2386 (0.2302)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [670/689]  eta: 0:00:29  lr: 0.000086  loss: 0.2396 (0.2302)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [680/689]  eta: 0:00:14  lr: 0.000086  loss: 0.2500 (0.2306)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227]  [688/689]  eta: 0:00:01  lr: 0.000086  loss: 0.2475 (0.2305)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:227] Total time: 0:18:07 (1.5786 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: 0.2475 (0.2305)\n",
      "Valid: [epoch:227]  [ 0/14]  eta: 0:00:14  loss: 0.2265 (0.2265)  time: 1.0201  data: 0.3402  max mem: 39763\n",
      "Valid: [epoch:227]  [13/14]  eta: 0:00:00  loss: 0.2142 (0.2169)  time: 0.1149  data: 0.0243  max mem: 39763\n",
      "Valid: [epoch:227] Total time: 0:00:01 (0.1241 s / it)\n",
      "Averaged stats: loss: 0.2142 (0.2169)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_227_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.217%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:228]  [  0/689]  eta: 0:12:01  lr: 0.000086  loss: 0.2405 (0.2405)  time: 1.0465  data: 0.5712  max mem: 39763\n",
      "Train: [epoch:228]  [ 10/689]  eta: 0:17:19  lr: 0.000086  loss: 0.2370 (0.2453)  time: 1.5310  data: 0.0520  max mem: 39763\n",
      "Train: [epoch:228]  [ 20/689]  eta: 0:17:19  lr: 0.000086  loss: 0.2316 (0.2370)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [ 30/689]  eta: 0:17:09  lr: 0.000086  loss: 0.2264 (0.2338)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [ 40/689]  eta: 0:16:56  lr: 0.000086  loss: 0.2217 (0.2340)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [ 50/689]  eta: 0:16:42  lr: 0.000086  loss: 0.2153 (0.2312)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [ 60/689]  eta: 0:16:28  lr: 0.000086  loss: 0.2252 (0.2311)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [ 70/689]  eta: 0:16:13  lr: 0.000086  loss: 0.2364 (0.2322)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [ 80/689]  eta: 0:15:58  lr: 0.000086  loss: 0.2371 (0.2333)  time: 1.5831  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [ 90/689]  eta: 0:15:43  lr: 0.000086  loss: 0.2237 (0.2314)  time: 1.5830  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [100/689]  eta: 0:15:28  lr: 0.000086  loss: 0.2142 (0.2302)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [110/689]  eta: 0:15:12  lr: 0.000086  loss: 0.2149 (0.2307)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [120/689]  eta: 0:14:57  lr: 0.000086  loss: 0.2252 (0.2312)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [130/689]  eta: 0:14:41  lr: 0.000086  loss: 0.2252 (0.2314)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [140/689]  eta: 0:14:25  lr: 0.000086  loss: 0.2223 (0.2303)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [150/689]  eta: 0:14:10  lr: 0.000086  loss: 0.2154 (0.2301)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [160/689]  eta: 0:13:54  lr: 0.000086  loss: 0.2188 (0.2294)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [170/689]  eta: 0:13:38  lr: 0.000086  loss: 0.2113 (0.2283)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [180/689]  eta: 0:13:23  lr: 0.000086  loss: 0.2082 (0.2275)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [190/689]  eta: 0:13:07  lr: 0.000086  loss: 0.2151 (0.2273)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [200/689]  eta: 0:12:51  lr: 0.000086  loss: 0.2344 (0.2281)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [210/689]  eta: 0:12:36  lr: 0.000086  loss: 0.2363 (0.2284)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [220/689]  eta: 0:12:20  lr: 0.000086  loss: 0.2377 (0.2293)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [230/689]  eta: 0:12:04  lr: 0.000086  loss: 0.2315 (0.2297)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [240/689]  eta: 0:11:49  lr: 0.000086  loss: 0.2252 (0.2296)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [250/689]  eta: 0:11:33  lr: 0.000086  loss: 0.2179 (0.2291)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [260/689]  eta: 0:11:17  lr: 0.000086  loss: 0.2191 (0.2290)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [270/689]  eta: 0:11:01  lr: 0.000086  loss: 0.2296 (0.2291)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [280/689]  eta: 0:10:46  lr: 0.000086  loss: 0.2243 (0.2290)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [290/689]  eta: 0:10:30  lr: 0.000086  loss: 0.2221 (0.2290)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [300/689]  eta: 0:10:14  lr: 0.000086  loss: 0.2221 (0.2290)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [310/689]  eta: 0:09:58  lr: 0.000086  loss: 0.2318 (0.2292)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [320/689]  eta: 0:09:42  lr: 0.000086  loss: 0.2318 (0.2294)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [330/689]  eta: 0:09:27  lr: 0.000086  loss: 0.2340 (0.2298)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [340/689]  eta: 0:09:11  lr: 0.000086  loss: 0.2307 (0.2298)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [350/689]  eta: 0:08:55  lr: 0.000086  loss: 0.2256 (0.2301)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [360/689]  eta: 0:08:39  lr: 0.000086  loss: 0.2243 (0.2302)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [370/689]  eta: 0:08:24  lr: 0.000086  loss: 0.2196 (0.2302)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [380/689]  eta: 0:08:08  lr: 0.000086  loss: 0.2223 (0.2304)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [390/689]  eta: 0:07:52  lr: 0.000086  loss: 0.2468 (0.2307)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [400/689]  eta: 0:07:36  lr: 0.000086  loss: 0.2382 (0.2311)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [410/689]  eta: 0:07:20  lr: 0.000086  loss: 0.2374 (0.2312)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [420/689]  eta: 0:07:05  lr: 0.000086  loss: 0.2268 (0.2313)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [430/689]  eta: 0:06:49  lr: 0.000086  loss: 0.2280 (0.2312)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [440/689]  eta: 0:06:33  lr: 0.000086  loss: 0.2293 (0.2312)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [450/689]  eta: 0:06:17  lr: 0.000086  loss: 0.2176 (0.2309)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [460/689]  eta: 0:06:01  lr: 0.000086  loss: 0.2270 (0.2311)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [470/689]  eta: 0:05:46  lr: 0.000086  loss: 0.2312 (0.2311)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [480/689]  eta: 0:05:30  lr: 0.000086  loss: 0.2269 (0.2313)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [490/689]  eta: 0:05:14  lr: 0.000086  loss: 0.2208 (0.2310)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [500/689]  eta: 0:04:58  lr: 0.000086  loss: 0.2170 (0.2312)  time: 1.5805  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:228]  [510/689]  eta: 0:04:42  lr: 0.000086  loss: 0.2226 (0.2312)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [520/689]  eta: 0:04:27  lr: 0.000086  loss: 0.2257 (0.2312)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [530/689]  eta: 0:04:11  lr: 0.000086  loss: 0.2268 (0.2310)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [540/689]  eta: 0:03:55  lr: 0.000086  loss: 0.2160 (0.2309)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [550/689]  eta: 0:03:39  lr: 0.000086  loss: 0.2160 (0.2308)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [560/689]  eta: 0:03:23  lr: 0.000086  loss: 0.2258 (0.2308)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [570/689]  eta: 0:03:08  lr: 0.000086  loss: 0.2207 (0.2307)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [580/689]  eta: 0:02:52  lr: 0.000086  loss: 0.2231 (0.2307)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [590/689]  eta: 0:02:36  lr: 0.000086  loss: 0.2231 (0.2307)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [600/689]  eta: 0:02:20  lr: 0.000086  loss: 0.2196 (0.2305)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [610/689]  eta: 0:02:04  lr: 0.000086  loss: 0.2139 (0.2303)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [620/689]  eta: 0:01:49  lr: 0.000086  loss: 0.2166 (0.2302)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [630/689]  eta: 0:01:33  lr: 0.000086  loss: 0.2212 (0.2303)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [640/689]  eta: 0:01:17  lr: 0.000086  loss: 0.2318 (0.2303)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [650/689]  eta: 0:01:01  lr: 0.000086  loss: 0.2206 (0.2302)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [660/689]  eta: 0:00:45  lr: 0.000086  loss: 0.2149 (0.2302)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [670/689]  eta: 0:00:30  lr: 0.000086  loss: 0.2149 (0.2300)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [680/689]  eta: 0:00:14  lr: 0.000086  loss: 0.2218 (0.2301)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228]  [688/689]  eta: 0:00:01  lr: 0.000086  loss: 0.2237 (0.2301)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:228] Total time: 0:18:09 (1.5811 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: 0.2237 (0.2301)\n",
      "Valid: [epoch:228]  [ 0/14]  eta: 0:00:14  loss: 0.2150 (0.2150)  time: 1.0221  data: 0.3457  max mem: 39763\n",
      "Valid: [epoch:228]  [13/14]  eta: 0:00:00  loss: 0.2150 (0.2174)  time: 0.1151  data: 0.0248  max mem: 39763\n",
      "Valid: [epoch:228] Total time: 0:00:01 (0.1238 s / it)\n",
      "Averaged stats: loss: 0.2150 (0.2174)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_228_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.217%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:229]  [  0/689]  eta: 0:11:18  lr: 0.000086  loss: 0.2202 (0.2202)  time: 0.9841  data: 0.5089  max mem: 39763\n",
      "Train: [epoch:229]  [ 10/689]  eta: 0:17:14  lr: 0.000086  loss: 0.2172 (0.2264)  time: 1.5242  data: 0.0463  max mem: 39763\n",
      "Train: [epoch:229]  [ 20/689]  eta: 0:17:17  lr: 0.000086  loss: 0.2152 (0.2225)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [ 30/689]  eta: 0:17:07  lr: 0.000086  loss: 0.2214 (0.2225)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [ 40/689]  eta: 0:16:55  lr: 0.000086  loss: 0.2259 (0.2242)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [ 50/689]  eta: 0:16:41  lr: 0.000086  loss: 0.2259 (0.2254)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [ 60/689]  eta: 0:16:26  lr: 0.000086  loss: 0.2156 (0.2246)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [ 70/689]  eta: 0:16:12  lr: 0.000086  loss: 0.2217 (0.2269)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [ 80/689]  eta: 0:15:57  lr: 0.000086  loss: 0.2223 (0.2274)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [ 90/689]  eta: 0:15:42  lr: 0.000086  loss: 0.2276 (0.2287)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [100/689]  eta: 0:15:27  lr: 0.000086  loss: 0.2353 (0.2305)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [110/689]  eta: 0:15:11  lr: 0.000086  loss: 0.2284 (0.2307)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [120/689]  eta: 0:14:56  lr: 0.000086  loss: 0.2258 (0.2309)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [130/689]  eta: 0:14:40  lr: 0.000086  loss: 0.2169 (0.2299)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [140/689]  eta: 0:14:25  lr: 0.000086  loss: 0.2249 (0.2310)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [150/689]  eta: 0:14:09  lr: 0.000086  loss: 0.2379 (0.2317)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [160/689]  eta: 0:13:54  lr: 0.000086  loss: 0.2346 (0.2321)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [170/689]  eta: 0:13:38  lr: 0.000086  loss: 0.2340 (0.2320)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [180/689]  eta: 0:13:22  lr: 0.000086  loss: 0.2391 (0.2327)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [190/689]  eta: 0:13:07  lr: 0.000086  loss: 0.2405 (0.2326)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [200/689]  eta: 0:12:51  lr: 0.000086  loss: 0.2210 (0.2322)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [210/689]  eta: 0:12:35  lr: 0.000086  loss: 0.2151 (0.2320)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [220/689]  eta: 0:12:20  lr: 0.000086  loss: 0.2336 (0.2325)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [230/689]  eta: 0:12:04  lr: 0.000086  loss: 0.2289 (0.2325)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [240/689]  eta: 0:11:48  lr: 0.000086  loss: 0.2270 (0.2326)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [250/689]  eta: 0:11:33  lr: 0.000086  loss: 0.2312 (0.2326)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [260/689]  eta: 0:11:17  lr: 0.000086  loss: 0.2227 (0.2331)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [270/689]  eta: 0:11:01  lr: 0.000086  loss: 0.2246 (0.2331)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [280/689]  eta: 0:10:45  lr: 0.000086  loss: 0.2372 (0.2334)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [290/689]  eta: 0:10:30  lr: 0.000086  loss: 0.2270 (0.2330)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [300/689]  eta: 0:10:14  lr: 0.000086  loss: 0.2235 (0.2335)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [310/689]  eta: 0:09:58  lr: 0.000086  loss: 0.2203 (0.2330)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [320/689]  eta: 0:09:42  lr: 0.000086  loss: 0.2255 (0.2339)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [330/689]  eta: 0:09:27  lr: 0.000086  loss: 0.2336 (0.2336)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [340/689]  eta: 0:09:11  lr: 0.000086  loss: 0.2266 (0.2334)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [350/689]  eta: 0:08:55  lr: 0.000086  loss: 0.2230 (0.2333)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [360/689]  eta: 0:08:39  lr: 0.000086  loss: 0.2314 (0.2335)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [370/689]  eta: 0:08:23  lr: 0.000086  loss: 0.2361 (0.2336)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [380/689]  eta: 0:08:08  lr: 0.000086  loss: 0.2398 (0.2339)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [390/689]  eta: 0:07:52  lr: 0.000086  loss: 0.2408 (0.2339)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [400/689]  eta: 0:07:36  lr: 0.000086  loss: 0.2363 (0.2339)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [410/689]  eta: 0:07:20  lr: 0.000086  loss: 0.2349 (0.2341)  time: 1.5817  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:229]  [420/689]  eta: 0:07:05  lr: 0.000086  loss: 0.2261 (0.2341)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [430/689]  eta: 0:06:49  lr: 0.000086  loss: 0.2383 (0.2344)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [440/689]  eta: 0:06:33  lr: 0.000086  loss: 0.2363 (0.2342)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [450/689]  eta: 0:06:17  lr: 0.000086  loss: 0.2280 (0.2341)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [460/689]  eta: 0:06:01  lr: 0.000086  loss: 0.2314 (0.2340)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [470/689]  eta: 0:05:46  lr: 0.000086  loss: 0.2300 (0.2339)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [480/689]  eta: 0:05:30  lr: 0.000086  loss: 0.2266 (0.2338)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [490/689]  eta: 0:05:14  lr: 0.000086  loss: 0.2219 (0.2335)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [500/689]  eta: 0:04:58  lr: 0.000086  loss: 0.2193 (0.2332)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [510/689]  eta: 0:04:42  lr: 0.000086  loss: 0.2215 (0.2332)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [520/689]  eta: 0:04:27  lr: 0.000086  loss: 0.2227 (0.2331)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [530/689]  eta: 0:04:11  lr: 0.000086  loss: 0.2233 (0.2331)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [540/689]  eta: 0:03:55  lr: 0.000086  loss: 0.2271 (0.2330)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [550/689]  eta: 0:03:39  lr: 0.000086  loss: 0.2190 (0.2329)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [560/689]  eta: 0:03:23  lr: 0.000086  loss: 0.2202 (0.2330)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [570/689]  eta: 0:03:08  lr: 0.000086  loss: 0.2376 (0.2330)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [580/689]  eta: 0:02:52  lr: 0.000086  loss: 0.2427 (0.2331)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [590/689]  eta: 0:02:36  lr: 0.000086  loss: 0.2324 (0.2331)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [600/689]  eta: 0:02:20  lr: 0.000086  loss: 0.2231 (0.2330)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [610/689]  eta: 0:02:04  lr: 0.000086  loss: 0.2231 (0.2331)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [620/689]  eta: 0:01:49  lr: 0.000086  loss: 0.2372 (0.2331)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [630/689]  eta: 0:01:33  lr: 0.000086  loss: 0.2372 (0.2331)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [640/689]  eta: 0:01:17  lr: 0.000086  loss: 0.2258 (0.2329)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [650/689]  eta: 0:01:01  lr: 0.000086  loss: 0.2241 (0.2329)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [660/689]  eta: 0:00:45  lr: 0.000086  loss: 0.2209 (0.2328)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [670/689]  eta: 0:00:30  lr: 0.000086  loss: 0.2177 (0.2327)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [680/689]  eta: 0:00:14  lr: 0.000086  loss: 0.2177 (0.2325)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229]  [688/689]  eta: 0:00:01  lr: 0.000086  loss: 0.2095 (0.2323)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:229] Total time: 0:18:08 (1.5805 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: 0.2095 (0.2323)\n",
      "Valid: [epoch:229]  [ 0/14]  eta: 0:00:14  loss: 0.2285 (0.2285)  time: 1.0085  data: 0.3741  max mem: 39763\n",
      "Valid: [epoch:229]  [13/14]  eta: 0:00:00  loss: 0.2162 (0.2183)  time: 0.1141  data: 0.0268  max mem: 39763\n",
      "Valid: [epoch:229] Total time: 0:00:01 (0.1229 s / it)\n",
      "Averaged stats: loss: 0.2162 (0.2183)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_229_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.218%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:230]  [  0/689]  eta: 0:13:38  lr: 0.000086  loss: 0.2106 (0.2106)  time: 1.1880  data: 0.7093  max mem: 39763\n",
      "Train: [epoch:230]  [ 10/689]  eta: 0:17:26  lr: 0.000086  loss: 0.2358 (0.2383)  time: 1.5409  data: 0.0646  max mem: 39763\n",
      "Train: [epoch:230]  [ 20/689]  eta: 0:17:22  lr: 0.000086  loss: 0.2199 (0.2266)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [ 30/689]  eta: 0:17:11  lr: 0.000086  loss: 0.2145 (0.2275)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [ 40/689]  eta: 0:16:57  lr: 0.000086  loss: 0.2295 (0.2289)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [ 50/689]  eta: 0:16:43  lr: 0.000086  loss: 0.2267 (0.2291)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [ 60/689]  eta: 0:16:28  lr: 0.000086  loss: 0.2336 (0.2296)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [ 70/689]  eta: 0:16:13  lr: 0.000086  loss: 0.2336 (0.2293)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [ 80/689]  eta: 0:15:58  lr: 0.000086  loss: 0.2219 (0.2286)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [ 90/689]  eta: 0:15:43  lr: 0.000086  loss: 0.2238 (0.2291)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [100/689]  eta: 0:15:27  lr: 0.000086  loss: 0.2273 (0.2297)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [110/689]  eta: 0:15:12  lr: 0.000086  loss: 0.2345 (0.2308)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [120/689]  eta: 0:14:56  lr: 0.000086  loss: 0.2351 (0.2310)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [130/689]  eta: 0:14:41  lr: 0.000086  loss: 0.2268 (0.2322)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [140/689]  eta: 0:14:25  lr: 0.000086  loss: 0.2186 (0.2312)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [150/689]  eta: 0:14:10  lr: 0.000086  loss: 0.2186 (0.2318)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [160/689]  eta: 0:13:54  lr: 0.000086  loss: 0.2453 (0.2327)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [170/689]  eta: 0:13:38  lr: 0.000086  loss: 0.2336 (0.2317)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [180/689]  eta: 0:13:23  lr: 0.000086  loss: 0.2144 (0.2312)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [190/689]  eta: 0:13:07  lr: 0.000086  loss: 0.2271 (0.2315)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [200/689]  eta: 0:12:51  lr: 0.000086  loss: 0.2349 (0.2321)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [210/689]  eta: 0:12:36  lr: 0.000086  loss: 0.2325 (0.2320)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [220/689]  eta: 0:12:20  lr: 0.000086  loss: 0.2230 (0.2321)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [230/689]  eta: 0:12:04  lr: 0.000086  loss: 0.2224 (0.2317)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [240/689]  eta: 0:11:49  lr: 0.000086  loss: 0.2187 (0.2317)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [250/689]  eta: 0:11:33  lr: 0.000086  loss: 0.2314 (0.2319)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [260/689]  eta: 0:11:17  lr: 0.000086  loss: 0.2286 (0.2322)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [270/689]  eta: 0:11:01  lr: 0.000086  loss: 0.2229 (0.2319)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [280/689]  eta: 0:10:46  lr: 0.000086  loss: 0.2160 (0.2321)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [290/689]  eta: 0:10:30  lr: 0.000086  loss: 0.2236 (0.2318)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [300/689]  eta: 0:10:14  lr: 0.000086  loss: 0.2194 (0.2315)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [310/689]  eta: 0:09:58  lr: 0.000086  loss: 0.2144 (0.2310)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [320/689]  eta: 0:09:43  lr: 0.000086  loss: 0.2156 (0.2314)  time: 1.5821  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:230]  [330/689]  eta: 0:09:27  lr: 0.000086  loss: 0.2252 (0.2313)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [340/689]  eta: 0:09:11  lr: 0.000086  loss: 0.2270 (0.2313)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [350/689]  eta: 0:08:55  lr: 0.000086  loss: 0.2296 (0.2316)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [360/689]  eta: 0:08:39  lr: 0.000086  loss: 0.2288 (0.2313)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [370/689]  eta: 0:08:24  lr: 0.000086  loss: 0.2231 (0.2315)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [380/689]  eta: 0:08:08  lr: 0.000086  loss: 0.2244 (0.2312)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [390/689]  eta: 0:07:52  lr: 0.000086  loss: 0.2244 (0.2314)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [400/689]  eta: 0:07:36  lr: 0.000086  loss: 0.2329 (0.2316)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [410/689]  eta: 0:07:20  lr: 0.000086  loss: 0.2338 (0.2317)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [420/689]  eta: 0:07:05  lr: 0.000086  loss: 0.2252 (0.2315)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [430/689]  eta: 0:06:49  lr: 0.000086  loss: 0.2199 (0.2316)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [440/689]  eta: 0:06:33  lr: 0.000086  loss: 0.2260 (0.2317)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [450/689]  eta: 0:06:17  lr: 0.000086  loss: 0.2325 (0.2318)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [460/689]  eta: 0:06:01  lr: 0.000086  loss: 0.2317 (0.2323)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [470/689]  eta: 0:05:46  lr: 0.000086  loss: 0.2402 (0.2324)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [480/689]  eta: 0:05:30  lr: 0.000086  loss: 0.2233 (0.2321)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [490/689]  eta: 0:05:14  lr: 0.000086  loss: 0.2108 (0.2321)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [500/689]  eta: 0:04:58  lr: 0.000086  loss: 0.2158 (0.2319)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [510/689]  eta: 0:04:42  lr: 0.000086  loss: 0.2221 (0.2322)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [520/689]  eta: 0:04:27  lr: 0.000086  loss: 0.2281 (0.2320)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [530/689]  eta: 0:04:11  lr: 0.000086  loss: 0.2213 (0.2320)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [540/689]  eta: 0:03:55  lr: 0.000086  loss: 0.2281 (0.2320)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [550/689]  eta: 0:03:39  lr: 0.000086  loss: 0.2194 (0.2318)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [560/689]  eta: 0:03:23  lr: 0.000086  loss: 0.2179 (0.2317)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [570/689]  eta: 0:03:08  lr: 0.000086  loss: 0.2192 (0.2316)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [580/689]  eta: 0:02:52  lr: 0.000086  loss: 0.2237 (0.2318)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [590/689]  eta: 0:02:36  lr: 0.000086  loss: 0.2332 (0.2318)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [600/689]  eta: 0:02:20  lr: 0.000086  loss: 0.2283 (0.2316)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [610/689]  eta: 0:02:04  lr: 0.000086  loss: 0.2163 (0.2315)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [620/689]  eta: 0:01:49  lr: 0.000086  loss: 0.2221 (0.2316)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [630/689]  eta: 0:01:33  lr: 0.000086  loss: 0.2402 (0.2320)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [640/689]  eta: 0:01:17  lr: 0.000086  loss: 0.2607 (0.2325)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [650/689]  eta: 0:01:01  lr: 0.000086  loss: 0.2520 (0.2325)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [660/689]  eta: 0:00:45  lr: 0.000086  loss: 0.2217 (0.2324)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [670/689]  eta: 0:00:30  lr: 0.000086  loss: 0.2311 (0.2326)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [680/689]  eta: 0:00:14  lr: 0.000086  loss: 0.2459 (0.2329)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230]  [688/689]  eta: 0:00:01  lr: 0.000086  loss: 0.2416 (0.2328)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:230] Total time: 0:18:09 (1.5811 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: 0.2416 (0.2328)\n",
      "Valid: [epoch:230]  [ 0/14]  eta: 0:00:14  loss: 0.2054 (0.2054)  time: 1.0156  data: 0.4400  max mem: 39763\n",
      "Valid: [epoch:230]  [13/14]  eta: 0:00:00  loss: 0.2177 (0.2209)  time: 0.1146  data: 0.0315  max mem: 39763\n",
      "Valid: [epoch:230] Total time: 0:00:01 (0.1223 s / it)\n",
      "Averaged stats: loss: 0.2177 (0.2209)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_230_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.221%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:231]  [  0/689]  eta: 0:11:25  lr: 0.000086  loss: 0.2894 (0.2894)  time: 0.9945  data: 0.5199  max mem: 39763\n",
      "Train: [epoch:231]  [ 10/689]  eta: 0:17:14  lr: 0.000086  loss: 0.2577 (0.2537)  time: 1.5232  data: 0.0473  max mem: 39763\n",
      "Train: [epoch:231]  [ 20/689]  eta: 0:17:16  lr: 0.000086  loss: 0.2383 (0.2515)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [ 30/689]  eta: 0:17:06  lr: 0.000086  loss: 0.2330 (0.2455)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [ 40/689]  eta: 0:16:54  lr: 0.000086  loss: 0.2443 (0.2461)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [ 50/689]  eta: 0:16:40  lr: 0.000086  loss: 0.2465 (0.2450)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [ 60/689]  eta: 0:16:25  lr: 0.000086  loss: 0.2446 (0.2440)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [ 70/689]  eta: 0:16:11  lr: 0.000086  loss: 0.2324 (0.2417)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [ 80/689]  eta: 0:15:56  lr: 0.000086  loss: 0.2264 (0.2404)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [ 90/689]  eta: 0:15:40  lr: 0.000086  loss: 0.2314 (0.2404)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [100/689]  eta: 0:15:25  lr: 0.000086  loss: 0.2253 (0.2386)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [110/689]  eta: 0:15:10  lr: 0.000086  loss: 0.2239 (0.2379)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [120/689]  eta: 0:14:54  lr: 0.000086  loss: 0.2335 (0.2381)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [130/689]  eta: 0:14:39  lr: 0.000086  loss: 0.2360 (0.2376)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [140/689]  eta: 0:14:23  lr: 0.000086  loss: 0.2413 (0.2384)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [150/689]  eta: 0:14:08  lr: 0.000086  loss: 0.2251 (0.2376)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [160/689]  eta: 0:13:52  lr: 0.000086  loss: 0.2208 (0.2376)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [170/689]  eta: 0:13:37  lr: 0.000086  loss: 0.2212 (0.2369)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [180/689]  eta: 0:13:21  lr: 0.000086  loss: 0.2212 (0.2368)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [190/689]  eta: 0:13:05  lr: 0.000086  loss: 0.2304 (0.2368)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [200/689]  eta: 0:12:50  lr: 0.000086  loss: 0.2282 (0.2363)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [210/689]  eta: 0:12:34  lr: 0.000086  loss: 0.2225 (0.2359)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [220/689]  eta: 0:12:19  lr: 0.000086  loss: 0.2179 (0.2358)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [230/689]  eta: 0:12:03  lr: 0.000086  loss: 0.2208 (0.2358)  time: 1.5818  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:231]  [240/689]  eta: 0:11:47  lr: 0.000086  loss: 0.2136 (0.2352)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [250/689]  eta: 0:11:32  lr: 0.000086  loss: 0.2136 (0.2349)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [260/689]  eta: 0:11:16  lr: 0.000086  loss: 0.2340 (0.2352)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [270/689]  eta: 0:11:00  lr: 0.000086  loss: 0.2423 (0.2352)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [280/689]  eta: 0:10:45  lr: 0.000086  loss: 0.2270 (0.2349)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [290/689]  eta: 0:10:29  lr: 0.000086  loss: 0.2270 (0.2355)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [300/689]  eta: 0:10:13  lr: 0.000086  loss: 0.2356 (0.2353)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [310/689]  eta: 0:09:57  lr: 0.000086  loss: 0.2258 (0.2352)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [320/689]  eta: 0:09:42  lr: 0.000086  loss: 0.2246 (0.2352)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [330/689]  eta: 0:09:26  lr: 0.000086  loss: 0.2477 (0.2359)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [340/689]  eta: 0:09:10  lr: 0.000086  loss: 0.2477 (0.2360)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [350/689]  eta: 0:08:54  lr: 0.000086  loss: 0.2332 (0.2359)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [360/689]  eta: 0:08:39  lr: 0.000086  loss: 0.2182 (0.2357)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [370/689]  eta: 0:08:23  lr: 0.000086  loss: 0.2220 (0.2355)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [380/689]  eta: 0:08:07  lr: 0.000086  loss: 0.2310 (0.2355)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [390/689]  eta: 0:07:51  lr: 0.000086  loss: 0.2310 (0.2354)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [400/689]  eta: 0:07:36  lr: 0.000086  loss: 0.2342 (0.2356)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [410/689]  eta: 0:07:20  lr: 0.000086  loss: 0.2342 (0.2357)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [420/689]  eta: 0:07:04  lr: 0.000086  loss: 0.2194 (0.2356)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [430/689]  eta: 0:06:48  lr: 0.000086  loss: 0.2150 (0.2352)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [440/689]  eta: 0:06:33  lr: 0.000086  loss: 0.2150 (0.2354)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [450/689]  eta: 0:06:17  lr: 0.000086  loss: 0.2254 (0.2351)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [460/689]  eta: 0:06:01  lr: 0.000086  loss: 0.2237 (0.2350)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [470/689]  eta: 0:05:45  lr: 0.000086  loss: 0.2203 (0.2348)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [480/689]  eta: 0:05:30  lr: 0.000086  loss: 0.2262 (0.2347)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [490/689]  eta: 0:05:14  lr: 0.000086  loss: 0.2335 (0.2347)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [500/689]  eta: 0:04:58  lr: 0.000086  loss: 0.2308 (0.2347)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [510/689]  eta: 0:04:42  lr: 0.000086  loss: 0.2300 (0.2346)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [520/689]  eta: 0:04:26  lr: 0.000086  loss: 0.2242 (0.2348)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [530/689]  eta: 0:04:11  lr: 0.000086  loss: 0.2264 (0.2348)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [540/689]  eta: 0:03:55  lr: 0.000086  loss: 0.2310 (0.2348)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [550/689]  eta: 0:03:39  lr: 0.000086  loss: 0.2322 (0.2350)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [560/689]  eta: 0:03:23  lr: 0.000086  loss: 0.2272 (0.2348)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [570/689]  eta: 0:03:07  lr: 0.000086  loss: 0.2238 (0.2348)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [580/689]  eta: 0:02:52  lr: 0.000086  loss: 0.2240 (0.2348)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [590/689]  eta: 0:02:36  lr: 0.000086  loss: 0.2370 (0.2350)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [600/689]  eta: 0:02:20  lr: 0.000086  loss: 0.2127 (0.2346)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [610/689]  eta: 0:02:04  lr: 0.000086  loss: 0.2142 (0.2348)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [620/689]  eta: 0:01:49  lr: 0.000086  loss: 0.2314 (0.2348)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [630/689]  eta: 0:01:33  lr: 0.000086  loss: 0.2260 (0.2345)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [640/689]  eta: 0:01:17  lr: 0.000086  loss: 0.2158 (0.2344)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [650/689]  eta: 0:01:01  lr: 0.000086  loss: 0.2093 (0.2340)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [660/689]  eta: 0:00:45  lr: 0.000086  loss: 0.2087 (0.2340)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [670/689]  eta: 0:00:30  lr: 0.000086  loss: 0.2263 (0.2341)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [680/689]  eta: 0:00:14  lr: 0.000086  loss: 0.2372 (0.2342)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231]  [688/689]  eta: 0:00:01  lr: 0.000086  loss: 0.2263 (0.2341)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:231] Total time: 0:18:08 (1.5801 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: 0.2263 (0.2341)\n",
      "Valid: [epoch:231]  [ 0/14]  eta: 0:00:14  loss: 0.2075 (0.2075)  time: 1.0256  data: 0.3836  max mem: 39763\n",
      "Valid: [epoch:231]  [13/14]  eta: 0:00:00  loss: 0.2201 (0.2224)  time: 0.1153  data: 0.0275  max mem: 39763\n",
      "Valid: [epoch:231] Total time: 0:00:01 (0.1245 s / it)\n",
      "Averaged stats: loss: 0.2201 (0.2224)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_231_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.222%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:232]  [  0/689]  eta: 0:11:31  lr: 0.000085  loss: 0.2156 (0.2156)  time: 1.0030  data: 0.5255  max mem: 39763\n",
      "Train: [epoch:232]  [ 10/689]  eta: 0:17:15  lr: 0.000085  loss: 0.2180 (0.2195)  time: 1.5246  data: 0.0478  max mem: 39763\n",
      "Train: [epoch:232]  [ 20/689]  eta: 0:17:16  lr: 0.000085  loss: 0.2249 (0.2257)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [ 30/689]  eta: 0:17:07  lr: 0.000085  loss: 0.2263 (0.2303)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [ 40/689]  eta: 0:16:54  lr: 0.000085  loss: 0.2298 (0.2355)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [ 50/689]  eta: 0:16:40  lr: 0.000085  loss: 0.2310 (0.2351)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [ 60/689]  eta: 0:16:26  lr: 0.000085  loss: 0.2278 (0.2326)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [ 70/689]  eta: 0:16:11  lr: 0.000085  loss: 0.2170 (0.2311)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [ 80/689]  eta: 0:15:56  lr: 0.000085  loss: 0.2160 (0.2307)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [ 90/689]  eta: 0:15:41  lr: 0.000085  loss: 0.2212 (0.2309)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [100/689]  eta: 0:15:25  lr: 0.000085  loss: 0.2206 (0.2296)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [110/689]  eta: 0:15:10  lr: 0.000085  loss: 0.2211 (0.2307)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [120/689]  eta: 0:14:55  lr: 0.000085  loss: 0.2317 (0.2317)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [130/689]  eta: 0:14:39  lr: 0.000085  loss: 0.2349 (0.2316)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [140/689]  eta: 0:14:23  lr: 0.000085  loss: 0.2356 (0.2330)  time: 1.5775  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:232]  [150/689]  eta: 0:14:08  lr: 0.000085  loss: 0.2354 (0.2329)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [160/689]  eta: 0:13:52  lr: 0.000085  loss: 0.2354 (0.2332)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [170/689]  eta: 0:13:37  lr: 0.000085  loss: 0.2226 (0.2325)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [180/689]  eta: 0:13:21  lr: 0.000085  loss: 0.2213 (0.2322)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [190/689]  eta: 0:13:05  lr: 0.000085  loss: 0.2213 (0.2326)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [200/689]  eta: 0:12:50  lr: 0.000085  loss: 0.2321 (0.2325)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [210/689]  eta: 0:12:34  lr: 0.000085  loss: 0.2264 (0.2325)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [220/689]  eta: 0:12:18  lr: 0.000085  loss: 0.2262 (0.2326)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [230/689]  eta: 0:12:03  lr: 0.000085  loss: 0.2420 (0.2328)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [240/689]  eta: 0:11:47  lr: 0.000085  loss: 0.2353 (0.2331)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [250/689]  eta: 0:11:31  lr: 0.000085  loss: 0.2324 (0.2334)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [260/689]  eta: 0:11:15  lr: 0.000085  loss: 0.2299 (0.2333)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [270/689]  eta: 0:11:00  lr: 0.000085  loss: 0.2226 (0.2329)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [280/689]  eta: 0:10:44  lr: 0.000085  loss: 0.2276 (0.2330)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [290/689]  eta: 0:10:28  lr: 0.000085  loss: 0.2280 (0.2328)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [300/689]  eta: 0:10:13  lr: 0.000085  loss: 0.2283 (0.2332)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [310/689]  eta: 0:09:57  lr: 0.000085  loss: 0.2245 (0.2327)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [320/689]  eta: 0:09:41  lr: 0.000085  loss: 0.2198 (0.2326)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [330/689]  eta: 0:09:26  lr: 0.000085  loss: 0.2282 (0.2331)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [340/689]  eta: 0:09:10  lr: 0.000085  loss: 0.2294 (0.2330)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [350/689]  eta: 0:08:54  lr: 0.000085  loss: 0.2207 (0.2328)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [360/689]  eta: 0:08:38  lr: 0.000085  loss: 0.2244 (0.2328)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [370/689]  eta: 0:08:23  lr: 0.000085  loss: 0.2244 (0.2326)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [380/689]  eta: 0:08:07  lr: 0.000085  loss: 0.2234 (0.2325)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [390/689]  eta: 0:07:51  lr: 0.000085  loss: 0.2234 (0.2326)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [400/689]  eta: 0:07:35  lr: 0.000085  loss: 0.2297 (0.2328)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [410/689]  eta: 0:07:20  lr: 0.000085  loss: 0.2375 (0.2331)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [420/689]  eta: 0:07:04  lr: 0.000085  loss: 0.2437 (0.2336)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [430/689]  eta: 0:06:48  lr: 0.000085  loss: 0.2365 (0.2337)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [440/689]  eta: 0:06:32  lr: 0.000085  loss: 0.2363 (0.2336)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [450/689]  eta: 0:06:17  lr: 0.000085  loss: 0.2296 (0.2339)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [460/689]  eta: 0:06:01  lr: 0.000085  loss: 0.2460 (0.2343)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [470/689]  eta: 0:05:45  lr: 0.000085  loss: 0.2251 (0.2342)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [480/689]  eta: 0:05:29  lr: 0.000085  loss: 0.2251 (0.2341)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [490/689]  eta: 0:05:14  lr: 0.000085  loss: 0.2277 (0.2340)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [500/689]  eta: 0:04:58  lr: 0.000085  loss: 0.2302 (0.2340)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [510/689]  eta: 0:04:42  lr: 0.000085  loss: 0.2361 (0.2340)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [520/689]  eta: 0:04:26  lr: 0.000085  loss: 0.2382 (0.2340)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [530/689]  eta: 0:04:11  lr: 0.000085  loss: 0.2249 (0.2338)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [540/689]  eta: 0:03:55  lr: 0.000085  loss: 0.2203 (0.2336)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [550/689]  eta: 0:03:39  lr: 0.000085  loss: 0.2212 (0.2335)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [560/689]  eta: 0:03:23  lr: 0.000085  loss: 0.2244 (0.2336)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [570/689]  eta: 0:03:07  lr: 0.000085  loss: 0.2449 (0.2342)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [580/689]  eta: 0:02:52  lr: 0.000085  loss: 0.2316 (0.2340)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [590/689]  eta: 0:02:36  lr: 0.000085  loss: 0.2225 (0.2340)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [600/689]  eta: 0:02:20  lr: 0.000085  loss: 0.2337 (0.2342)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [610/689]  eta: 0:02:04  lr: 0.000085  loss: 0.2249 (0.2340)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [620/689]  eta: 0:01:48  lr: 0.000085  loss: 0.2298 (0.2341)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [630/689]  eta: 0:01:33  lr: 0.000085  loss: 0.2429 (0.2343)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [640/689]  eta: 0:01:17  lr: 0.000085  loss: 0.2418 (0.2342)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [650/689]  eta: 0:01:01  lr: 0.000085  loss: 0.2375 (0.2342)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [660/689]  eta: 0:00:45  lr: 0.000085  loss: 0.2298 (0.2341)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [670/689]  eta: 0:00:30  lr: 0.000085  loss: 0.2285 (0.2342)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [680/689]  eta: 0:00:14  lr: 0.000085  loss: 0.2366 (0.2342)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232]  [688/689]  eta: 0:00:01  lr: 0.000085  loss: 0.2337 (0.2341)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:232] Total time: 0:18:08 (1.5796 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: 0.2337 (0.2341)\n",
      "Valid: [epoch:232]  [ 0/14]  eta: 0:00:13  loss: 0.2342 (0.2342)  time: 0.9953  data: 0.3928  max mem: 39763\n",
      "Valid: [epoch:232]  [13/14]  eta: 0:00:00  loss: 0.2186 (0.2211)  time: 0.1131  data: 0.0281  max mem: 39763\n",
      "Valid: [epoch:232] Total time: 0:00:01 (0.1224 s / it)\n",
      "Averaged stats: loss: 0.2186 (0.2211)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_232_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.221%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:233]  [  0/689]  eta: 0:11:45  lr: 0.000085  loss: 0.1845 (0.1845)  time: 1.0235  data: 0.5482  max mem: 39763\n",
      "Train: [epoch:233]  [ 10/689]  eta: 0:17:16  lr: 0.000085  loss: 0.2338 (0.2295)  time: 1.5266  data: 0.0499  max mem: 39763\n",
      "Train: [epoch:233]  [ 20/689]  eta: 0:17:17  lr: 0.000085  loss: 0.2286 (0.2229)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [ 30/689]  eta: 0:17:07  lr: 0.000085  loss: 0.2241 (0.2243)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [ 40/689]  eta: 0:16:55  lr: 0.000085  loss: 0.2250 (0.2274)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [ 50/689]  eta: 0:16:41  lr: 0.000085  loss: 0.2213 (0.2263)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:233]  [ 60/689]  eta: 0:16:26  lr: 0.000085  loss: 0.2219 (0.2302)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [ 70/689]  eta: 0:16:11  lr: 0.000085  loss: 0.2185 (0.2287)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [ 80/689]  eta: 0:15:56  lr: 0.000085  loss: 0.2102 (0.2272)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [ 90/689]  eta: 0:15:41  lr: 0.000085  loss: 0.2285 (0.2289)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [100/689]  eta: 0:15:26  lr: 0.000085  loss: 0.2430 (0.2308)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [110/689]  eta: 0:15:11  lr: 0.000085  loss: 0.2424 (0.2316)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [120/689]  eta: 0:14:55  lr: 0.000085  loss: 0.2276 (0.2318)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [130/689]  eta: 0:14:40  lr: 0.000085  loss: 0.2269 (0.2325)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [140/689]  eta: 0:14:24  lr: 0.000085  loss: 0.2296 (0.2325)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [150/689]  eta: 0:14:09  lr: 0.000085  loss: 0.2296 (0.2324)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [160/689]  eta: 0:13:53  lr: 0.000085  loss: 0.2249 (0.2324)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [170/689]  eta: 0:13:38  lr: 0.000085  loss: 0.2283 (0.2327)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [180/689]  eta: 0:13:22  lr: 0.000085  loss: 0.2282 (0.2321)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [190/689]  eta: 0:13:06  lr: 0.000085  loss: 0.2248 (0.2320)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [200/689]  eta: 0:12:51  lr: 0.000085  loss: 0.2248 (0.2321)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [210/689]  eta: 0:12:35  lr: 0.000085  loss: 0.2214 (0.2319)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [220/689]  eta: 0:12:19  lr: 0.000085  loss: 0.2206 (0.2318)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [230/689]  eta: 0:12:04  lr: 0.000085  loss: 0.2175 (0.2312)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [240/689]  eta: 0:11:48  lr: 0.000085  loss: 0.2175 (0.2312)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [250/689]  eta: 0:11:32  lr: 0.000085  loss: 0.2260 (0.2311)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [260/689]  eta: 0:11:16  lr: 0.000085  loss: 0.2212 (0.2311)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [270/689]  eta: 0:11:01  lr: 0.000085  loss: 0.2230 (0.2312)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [280/689]  eta: 0:10:45  lr: 0.000085  loss: 0.2230 (0.2312)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [290/689]  eta: 0:10:29  lr: 0.000085  loss: 0.2342 (0.2312)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [300/689]  eta: 0:10:13  lr: 0.000085  loss: 0.2296 (0.2310)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [310/689]  eta: 0:09:58  lr: 0.000085  loss: 0.2282 (0.2311)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [320/689]  eta: 0:09:42  lr: 0.000085  loss: 0.2301 (0.2314)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [330/689]  eta: 0:09:26  lr: 0.000085  loss: 0.2370 (0.2316)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [340/689]  eta: 0:09:10  lr: 0.000085  loss: 0.2410 (0.2323)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [350/689]  eta: 0:08:55  lr: 0.000085  loss: 0.2440 (0.2328)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [360/689]  eta: 0:08:39  lr: 0.000085  loss: 0.2319 (0.2327)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [370/689]  eta: 0:08:23  lr: 0.000085  loss: 0.2275 (0.2327)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [380/689]  eta: 0:08:07  lr: 0.000085  loss: 0.2349 (0.2328)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [390/689]  eta: 0:07:52  lr: 0.000085  loss: 0.2405 (0.2333)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [400/689]  eta: 0:07:36  lr: 0.000085  loss: 0.2521 (0.2340)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [410/689]  eta: 0:07:20  lr: 0.000085  loss: 0.2420 (0.2341)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [420/689]  eta: 0:07:04  lr: 0.000085  loss: 0.2340 (0.2342)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [430/689]  eta: 0:06:48  lr: 0.000085  loss: 0.2338 (0.2343)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [440/689]  eta: 0:06:33  lr: 0.000085  loss: 0.2338 (0.2344)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [450/689]  eta: 0:06:17  lr: 0.000085  loss: 0.2234 (0.2343)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [460/689]  eta: 0:06:01  lr: 0.000085  loss: 0.2332 (0.2345)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [470/689]  eta: 0:05:45  lr: 0.000085  loss: 0.2369 (0.2347)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [480/689]  eta: 0:05:30  lr: 0.000085  loss: 0.2274 (0.2347)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [490/689]  eta: 0:05:14  lr: 0.000085  loss: 0.2361 (0.2349)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [500/689]  eta: 0:04:58  lr: 0.000085  loss: 0.2261 (0.2348)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [510/689]  eta: 0:04:42  lr: 0.000085  loss: 0.2253 (0.2346)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [520/689]  eta: 0:04:26  lr: 0.000085  loss: 0.2296 (0.2347)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [530/689]  eta: 0:04:11  lr: 0.000085  loss: 0.2282 (0.2345)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [540/689]  eta: 0:03:55  lr: 0.000085  loss: 0.2240 (0.2345)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [550/689]  eta: 0:03:39  lr: 0.000085  loss: 0.2429 (0.2348)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [560/689]  eta: 0:03:23  lr: 0.000085  loss: 0.2453 (0.2351)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [570/689]  eta: 0:03:07  lr: 0.000085  loss: 0.2395 (0.2351)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [580/689]  eta: 0:02:52  lr: 0.000085  loss: 0.2221 (0.2351)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [590/689]  eta: 0:02:36  lr: 0.000085  loss: 0.2316 (0.2352)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [600/689]  eta: 0:02:20  lr: 0.000085  loss: 0.2258 (0.2352)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [610/689]  eta: 0:02:04  lr: 0.000085  loss: 0.2166 (0.2350)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [620/689]  eta: 0:01:48  lr: 0.000085  loss: 0.2126 (0.2346)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [630/689]  eta: 0:01:33  lr: 0.000085  loss: 0.2152 (0.2346)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [640/689]  eta: 0:01:17  lr: 0.000085  loss: 0.2335 (0.2346)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [650/689]  eta: 0:01:01  lr: 0.000085  loss: 0.2335 (0.2346)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [660/689]  eta: 0:00:45  lr: 0.000085  loss: 0.2341 (0.2347)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [670/689]  eta: 0:00:30  lr: 0.000085  loss: 0.2177 (0.2346)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [680/689]  eta: 0:00:14  lr: 0.000085  loss: 0.2204 (0.2346)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233]  [688/689]  eta: 0:00:01  lr: 0.000085  loss: 0.2305 (0.2346)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:233] Total time: 0:18:08 (1.5797 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: 0.2305 (0.2346)\n",
      "Valid: [epoch:233]  [ 0/14]  eta: 0:00:14  loss: 0.2316 (0.2316)  time: 1.0180  data: 0.3912  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:233]  [13/14]  eta: 0:00:00  loss: 0.2197 (0.2222)  time: 0.1147  data: 0.0280  max mem: 39763\n",
      "Valid: [epoch:233] Total time: 0:00:01 (0.1225 s / it)\n",
      "Averaged stats: loss: 0.2197 (0.2222)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_233_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.222%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:234]  [  0/689]  eta: 0:11:37  lr: 0.000085  loss: 0.2328 (0.2328)  time: 1.0128  data: 0.5326  max mem: 39763\n",
      "Train: [epoch:234]  [ 10/689]  eta: 0:17:15  lr: 0.000085  loss: 0.2215 (0.2242)  time: 1.5256  data: 0.0485  max mem: 39763\n",
      "Train: [epoch:234]  [ 20/689]  eta: 0:17:17  lr: 0.000085  loss: 0.2215 (0.2260)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [ 30/689]  eta: 0:17:07  lr: 0.000085  loss: 0.2357 (0.2330)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [ 40/689]  eta: 0:16:54  lr: 0.000085  loss: 0.2425 (0.2354)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [ 50/689]  eta: 0:16:40  lr: 0.000085  loss: 0.2276 (0.2333)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [ 60/689]  eta: 0:16:26  lr: 0.000085  loss: 0.2201 (0.2317)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [ 70/689]  eta: 0:16:11  lr: 0.000085  loss: 0.2219 (0.2349)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [ 80/689]  eta: 0:15:56  lr: 0.000085  loss: 0.2168 (0.2314)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [ 90/689]  eta: 0:15:40  lr: 0.000085  loss: 0.2134 (0.2335)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [100/689]  eta: 0:15:25  lr: 0.000085  loss: 0.2305 (0.2340)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [110/689]  eta: 0:15:10  lr: 0.000085  loss: 0.2278 (0.2328)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [120/689]  eta: 0:14:54  lr: 0.000085  loss: 0.2278 (0.2328)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [130/689]  eta: 0:14:39  lr: 0.000085  loss: 0.2389 (0.2332)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [140/689]  eta: 0:14:23  lr: 0.000085  loss: 0.2389 (0.2340)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [150/689]  eta: 0:14:08  lr: 0.000085  loss: 0.2326 (0.2338)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [160/689]  eta: 0:13:52  lr: 0.000085  loss: 0.2178 (0.2333)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [170/689]  eta: 0:13:36  lr: 0.000085  loss: 0.2310 (0.2346)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [180/689]  eta: 0:13:21  lr: 0.000085  loss: 0.2557 (0.2390)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [190/689]  eta: 0:13:05  lr: 0.000085  loss: 0.3128 (0.2438)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [200/689]  eta: 0:12:50  lr: 0.000085  loss: 0.2789 (0.2445)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [210/689]  eta: 0:12:34  lr: 0.000085  loss: 0.2480 (0.2438)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [220/689]  eta: 0:12:18  lr: 0.000085  loss: 0.2378 (0.2445)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [230/689]  eta: 0:12:02  lr: 0.000085  loss: 0.2502 (0.2446)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [240/689]  eta: 0:11:47  lr: 0.000085  loss: 0.2370 (0.2443)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [250/689]  eta: 0:11:31  lr: 0.000085  loss: 0.2213 (0.2440)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [260/689]  eta: 0:11:15  lr: 0.000085  loss: 0.2311 (0.2442)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [270/689]  eta: 0:11:00  lr: 0.000085  loss: 0.2373 (0.2438)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [280/689]  eta: 0:10:44  lr: 0.000085  loss: 0.2250 (0.2436)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [290/689]  eta: 0:10:28  lr: 0.000085  loss: 0.2450 (0.2437)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [300/689]  eta: 0:10:13  lr: 0.000085  loss: 0.2333 (0.2435)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [310/689]  eta: 0:09:57  lr: 0.000085  loss: 0.2273 (0.2431)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [320/689]  eta: 0:09:41  lr: 0.000085  loss: 0.2255 (0.2425)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [330/689]  eta: 0:09:25  lr: 0.000085  loss: 0.2289 (0.2427)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [340/689]  eta: 0:09:10  lr: 0.000085  loss: 0.2311 (0.2424)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [350/689]  eta: 0:08:54  lr: 0.000085  loss: 0.2398 (0.2425)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [360/689]  eta: 0:08:38  lr: 0.000085  loss: 0.2329 (0.2424)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [370/689]  eta: 0:08:22  lr: 0.000085  loss: 0.2316 (0.2421)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [380/689]  eta: 0:08:07  lr: 0.000085  loss: 0.2359 (0.2423)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [390/689]  eta: 0:07:51  lr: 0.000085  loss: 0.2395 (0.2425)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [400/689]  eta: 0:07:35  lr: 0.000085  loss: 0.2395 (0.2425)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [410/689]  eta: 0:07:19  lr: 0.000085  loss: 0.2278 (0.2422)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [420/689]  eta: 0:07:04  lr: 0.000085  loss: 0.2266 (0.2420)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [430/689]  eta: 0:06:48  lr: 0.000085  loss: 0.2354 (0.2420)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [440/689]  eta: 0:06:32  lr: 0.000085  loss: 0.2366 (0.2419)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [450/689]  eta: 0:06:16  lr: 0.000085  loss: 0.2318 (0.2419)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [460/689]  eta: 0:06:01  lr: 0.000085  loss: 0.2374 (0.2418)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [470/689]  eta: 0:05:45  lr: 0.000085  loss: 0.2386 (0.2418)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [480/689]  eta: 0:05:29  lr: 0.000085  loss: 0.2287 (0.2419)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [490/689]  eta: 0:05:13  lr: 0.000085  loss: 0.2267 (0.2414)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [500/689]  eta: 0:04:57  lr: 0.000085  loss: 0.2214 (0.2411)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [510/689]  eta: 0:04:42  lr: 0.000085  loss: 0.2316 (0.2410)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [520/689]  eta: 0:04:26  lr: 0.000085  loss: 0.2316 (0.2411)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [530/689]  eta: 0:04:10  lr: 0.000085  loss: 0.2273 (0.2409)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [540/689]  eta: 0:03:54  lr: 0.000085  loss: 0.2318 (0.2410)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [550/689]  eta: 0:03:39  lr: 0.000085  loss: 0.2318 (0.2408)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [560/689]  eta: 0:03:23  lr: 0.000085  loss: 0.2374 (0.2409)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [570/689]  eta: 0:03:07  lr: 0.000085  loss: 0.2287 (0.2406)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [580/689]  eta: 0:02:51  lr: 0.000085  loss: 0.2254 (0.2405)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [590/689]  eta: 0:02:36  lr: 0.000085  loss: 0.2285 (0.2403)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [600/689]  eta: 0:02:20  lr: 0.000085  loss: 0.2291 (0.2402)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [610/689]  eta: 0:02:04  lr: 0.000085  loss: 0.2326 (0.2402)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [620/689]  eta: 0:01:48  lr: 0.000085  loss: 0.2180 (0.2398)  time: 1.5780  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:234]  [630/689]  eta: 0:01:33  lr: 0.000085  loss: 0.2222 (0.2398)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [640/689]  eta: 0:01:17  lr: 0.000085  loss: 0.2420 (0.2399)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [650/689]  eta: 0:01:01  lr: 0.000085  loss: 0.2532 (0.2401)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [660/689]  eta: 0:00:45  lr: 0.000085  loss: 0.2331 (0.2400)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [670/689]  eta: 0:00:29  lr: 0.000085  loss: 0.2310 (0.2402)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [680/689]  eta: 0:00:14  lr: 0.000085  loss: 0.2310 (0.2401)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234]  [688/689]  eta: 0:00:01  lr: 0.000085  loss: 0.2250 (0.2399)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:234] Total time: 0:18:06 (1.5772 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: 0.2250 (0.2399)\n",
      "Valid: [epoch:234]  [ 0/14]  eta: 0:00:14  loss: 0.2359 (0.2359)  time: 1.0149  data: 0.3672  max mem: 39763\n",
      "Valid: [epoch:234]  [13/14]  eta: 0:00:00  loss: 0.2206 (0.2236)  time: 0.1144  data: 0.0263  max mem: 39763\n",
      "Valid: [epoch:234] Total time: 0:00:01 (0.1224 s / it)\n",
      "Averaged stats: loss: 0.2206 (0.2236)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_234_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.224%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:235]  [  0/689]  eta: 0:11:42  lr: 0.000085  loss: 0.2057 (0.2057)  time: 1.0199  data: 0.5429  max mem: 39763\n",
      "Train: [epoch:235]  [ 10/689]  eta: 0:17:15  lr: 0.000085  loss: 0.2291 (0.2348)  time: 1.5249  data: 0.0494  max mem: 39763\n",
      "Train: [epoch:235]  [ 20/689]  eta: 0:17:17  lr: 0.000085  loss: 0.2291 (0.2339)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [ 30/689]  eta: 0:17:07  lr: 0.000085  loss: 0.2307 (0.2366)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [ 40/689]  eta: 0:16:54  lr: 0.000085  loss: 0.2361 (0.2380)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [ 50/689]  eta: 0:16:40  lr: 0.000085  loss: 0.2309 (0.2368)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [ 60/689]  eta: 0:16:26  lr: 0.000085  loss: 0.2301 (0.2362)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [ 70/689]  eta: 0:16:11  lr: 0.000085  loss: 0.2322 (0.2355)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [ 80/689]  eta: 0:15:56  lr: 0.000085  loss: 0.2217 (0.2331)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [ 90/689]  eta: 0:15:41  lr: 0.000085  loss: 0.2185 (0.2333)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [100/689]  eta: 0:15:26  lr: 0.000085  loss: 0.2358 (0.2347)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [110/689]  eta: 0:15:10  lr: 0.000085  loss: 0.2378 (0.2359)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [120/689]  eta: 0:14:55  lr: 0.000085  loss: 0.2463 (0.2368)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [130/689]  eta: 0:14:39  lr: 0.000085  loss: 0.2194 (0.2361)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [140/689]  eta: 0:14:24  lr: 0.000085  loss: 0.2164 (0.2357)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [150/689]  eta: 0:14:08  lr: 0.000085  loss: 0.2241 (0.2357)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [160/689]  eta: 0:13:53  lr: 0.000085  loss: 0.2263 (0.2359)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [170/689]  eta: 0:13:37  lr: 0.000085  loss: 0.2195 (0.2358)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [180/689]  eta: 0:13:21  lr: 0.000085  loss: 0.2265 (0.2360)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [190/689]  eta: 0:13:06  lr: 0.000085  loss: 0.2287 (0.2361)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [200/689]  eta: 0:12:50  lr: 0.000085  loss: 0.2235 (0.2359)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [210/689]  eta: 0:12:34  lr: 0.000085  loss: 0.2207 (0.2356)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [220/689]  eta: 0:12:19  lr: 0.000085  loss: 0.2468 (0.2370)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [230/689]  eta: 0:12:03  lr: 0.000085  loss: 0.2468 (0.2367)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [240/689]  eta: 0:11:47  lr: 0.000085  loss: 0.2438 (0.2373)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [250/689]  eta: 0:11:31  lr: 0.000085  loss: 0.2353 (0.2366)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [260/689]  eta: 0:11:16  lr: 0.000085  loss: 0.2234 (0.2371)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [270/689]  eta: 0:11:00  lr: 0.000085  loss: 0.2386 (0.2374)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [280/689]  eta: 0:10:44  lr: 0.000085  loss: 0.2428 (0.2377)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [290/689]  eta: 0:10:29  lr: 0.000085  loss: 0.2279 (0.2374)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [300/689]  eta: 0:10:13  lr: 0.000085  loss: 0.2258 (0.2374)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [310/689]  eta: 0:09:57  lr: 0.000085  loss: 0.2335 (0.2372)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [320/689]  eta: 0:09:41  lr: 0.000085  loss: 0.2322 (0.2375)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [330/689]  eta: 0:09:26  lr: 0.000085  loss: 0.2321 (0.2373)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [340/689]  eta: 0:09:10  lr: 0.000085  loss: 0.2287 (0.2370)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [350/689]  eta: 0:08:54  lr: 0.000085  loss: 0.2339 (0.2370)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [360/689]  eta: 0:08:38  lr: 0.000085  loss: 0.2302 (0.2369)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [370/689]  eta: 0:08:23  lr: 0.000085  loss: 0.2251 (0.2367)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [380/689]  eta: 0:08:07  lr: 0.000085  loss: 0.2240 (0.2365)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [390/689]  eta: 0:07:51  lr: 0.000085  loss: 0.2254 (0.2366)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [400/689]  eta: 0:07:35  lr: 0.000085  loss: 0.2479 (0.2372)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [410/689]  eta: 0:07:19  lr: 0.000085  loss: 0.2339 (0.2369)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [420/689]  eta: 0:07:04  lr: 0.000085  loss: 0.2259 (0.2370)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [430/689]  eta: 0:06:48  lr: 0.000085  loss: 0.2480 (0.2373)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [440/689]  eta: 0:06:32  lr: 0.000085  loss: 0.2323 (0.2373)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [450/689]  eta: 0:06:16  lr: 0.000085  loss: 0.2298 (0.2372)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [460/689]  eta: 0:06:01  lr: 0.000085  loss: 0.2334 (0.2376)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [470/689]  eta: 0:05:45  lr: 0.000085  loss: 0.2576 (0.2381)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [480/689]  eta: 0:05:29  lr: 0.000085  loss: 0.2331 (0.2379)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [490/689]  eta: 0:05:13  lr: 0.000085  loss: 0.2335 (0.2380)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [500/689]  eta: 0:04:58  lr: 0.000085  loss: 0.2374 (0.2381)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [510/689]  eta: 0:04:42  lr: 0.000085  loss: 0.2291 (0.2380)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [520/689]  eta: 0:04:26  lr: 0.000085  loss: 0.2242 (0.2377)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [530/689]  eta: 0:04:10  lr: 0.000085  loss: 0.2242 (0.2377)  time: 1.5775  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:235]  [540/689]  eta: 0:03:55  lr: 0.000085  loss: 0.2321 (0.2374)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [550/689]  eta: 0:03:39  lr: 0.000085  loss: 0.2316 (0.2373)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [560/689]  eta: 0:03:23  lr: 0.000085  loss: 0.2307 (0.2372)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [570/689]  eta: 0:03:07  lr: 0.000085  loss: 0.2325 (0.2374)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [580/689]  eta: 0:02:51  lr: 0.000085  loss: 0.2363 (0.2374)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [590/689]  eta: 0:02:36  lr: 0.000085  loss: 0.2363 (0.2375)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [600/689]  eta: 0:02:20  lr: 0.000085  loss: 0.2391 (0.2376)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [610/689]  eta: 0:02:04  lr: 0.000085  loss: 0.2326 (0.2376)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [620/689]  eta: 0:01:48  lr: 0.000085  loss: 0.2284 (0.2374)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [630/689]  eta: 0:01:33  lr: 0.000085  loss: 0.2282 (0.2373)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [640/689]  eta: 0:01:17  lr: 0.000085  loss: 0.2310 (0.2371)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [650/689]  eta: 0:01:01  lr: 0.000085  loss: 0.2321 (0.2372)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [660/689]  eta: 0:00:45  lr: 0.000085  loss: 0.2375 (0.2370)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [670/689]  eta: 0:00:29  lr: 0.000085  loss: 0.2307 (0.2371)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [680/689]  eta: 0:00:14  lr: 0.000085  loss: 0.2369 (0.2373)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235]  [688/689]  eta: 0:00:01  lr: 0.000085  loss: 0.2369 (0.2373)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:235] Total time: 0:18:06 (1.5775 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: 0.2369 (0.2373)\n",
      "Valid: [epoch:235]  [ 0/14]  eta: 0:00:14  loss: 0.2317 (0.2317)  time: 1.0098  data: 0.3829  max mem: 39763\n",
      "Valid: [epoch:235]  [13/14]  eta: 0:00:00  loss: 0.2216 (0.2240)  time: 0.1141  data: 0.0274  max mem: 39763\n",
      "Valid: [epoch:235] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.2216 (0.2240)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_235_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.224%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:236]  [  0/689]  eta: 0:11:28  lr: 0.000085  loss: 0.2060 (0.2060)  time: 0.9992  data: 0.5245  max mem: 39763\n",
      "Train: [epoch:236]  [ 10/689]  eta: 0:17:15  lr: 0.000085  loss: 0.2249 (0.2314)  time: 1.5245  data: 0.0478  max mem: 39763\n",
      "Train: [epoch:236]  [ 20/689]  eta: 0:17:16  lr: 0.000085  loss: 0.2229 (0.2294)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [ 30/689]  eta: 0:17:07  lr: 0.000085  loss: 0.2229 (0.2297)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [ 40/689]  eta: 0:16:54  lr: 0.000085  loss: 0.2324 (0.2337)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [ 50/689]  eta: 0:16:40  lr: 0.000085  loss: 0.2310 (0.2331)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [ 60/689]  eta: 0:16:26  lr: 0.000085  loss: 0.2280 (0.2334)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [ 70/689]  eta: 0:16:11  lr: 0.000085  loss: 0.2280 (0.2328)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [ 80/689]  eta: 0:15:56  lr: 0.000085  loss: 0.2238 (0.2315)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [ 90/689]  eta: 0:15:41  lr: 0.000085  loss: 0.2255 (0.2319)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [100/689]  eta: 0:15:26  lr: 0.000085  loss: 0.2309 (0.2339)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [110/689]  eta: 0:15:11  lr: 0.000085  loss: 0.2402 (0.2347)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [120/689]  eta: 0:14:55  lr: 0.000085  loss: 0.2365 (0.2347)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [130/689]  eta: 0:14:40  lr: 0.000085  loss: 0.2372 (0.2357)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [140/689]  eta: 0:14:24  lr: 0.000085  loss: 0.2480 (0.2362)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [150/689]  eta: 0:14:09  lr: 0.000085  loss: 0.2372 (0.2360)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [160/689]  eta: 0:13:53  lr: 0.000085  loss: 0.2317 (0.2361)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [170/689]  eta: 0:13:38  lr: 0.000085  loss: 0.2287 (0.2363)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [180/689]  eta: 0:13:22  lr: 0.000085  loss: 0.2287 (0.2362)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [190/689]  eta: 0:13:06  lr: 0.000085  loss: 0.2339 (0.2361)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [200/689]  eta: 0:12:51  lr: 0.000085  loss: 0.2300 (0.2359)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [210/689]  eta: 0:12:35  lr: 0.000085  loss: 0.2245 (0.2358)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [220/689]  eta: 0:12:19  lr: 0.000085  loss: 0.2209 (0.2351)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [230/689]  eta: 0:12:04  lr: 0.000085  loss: 0.2369 (0.2359)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [240/689]  eta: 0:11:48  lr: 0.000085  loss: 0.2410 (0.2361)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [250/689]  eta: 0:11:32  lr: 0.000085  loss: 0.2311 (0.2356)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [260/689]  eta: 0:11:17  lr: 0.000085  loss: 0.2390 (0.2364)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [270/689]  eta: 0:11:01  lr: 0.000085  loss: 0.2472 (0.2368)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [280/689]  eta: 0:10:45  lr: 0.000085  loss: 0.2405 (0.2370)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [290/689]  eta: 0:10:29  lr: 0.000085  loss: 0.2352 (0.2368)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [300/689]  eta: 0:10:14  lr: 0.000085  loss: 0.2352 (0.2375)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [310/689]  eta: 0:09:58  lr: 0.000085  loss: 0.2260 (0.2371)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [320/689]  eta: 0:09:42  lr: 0.000085  loss: 0.2234 (0.2369)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [330/689]  eta: 0:09:26  lr: 0.000085  loss: 0.2170 (0.2368)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [340/689]  eta: 0:09:11  lr: 0.000085  loss: 0.2285 (0.2371)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [350/689]  eta: 0:08:55  lr: 0.000085  loss: 0.2424 (0.2374)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [360/689]  eta: 0:08:39  lr: 0.000085  loss: 0.2389 (0.2375)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [370/689]  eta: 0:08:23  lr: 0.000085  loss: 0.2320 (0.2372)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [380/689]  eta: 0:08:07  lr: 0.000085  loss: 0.2291 (0.2374)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [390/689]  eta: 0:07:52  lr: 0.000085  loss: 0.2293 (0.2373)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [400/689]  eta: 0:07:36  lr: 0.000085  loss: 0.2271 (0.2372)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [410/689]  eta: 0:07:20  lr: 0.000085  loss: 0.2329 (0.2374)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [420/689]  eta: 0:07:04  lr: 0.000085  loss: 0.2381 (0.2374)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [430/689]  eta: 0:06:49  lr: 0.000085  loss: 0.2280 (0.2373)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [440/689]  eta: 0:06:33  lr: 0.000085  loss: 0.2312 (0.2375)  time: 1.5807  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:236]  [450/689]  eta: 0:06:17  lr: 0.000085  loss: 0.2383 (0.2375)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [460/689]  eta: 0:06:01  lr: 0.000085  loss: 0.2350 (0.2376)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [470/689]  eta: 0:05:45  lr: 0.000085  loss: 0.2300 (0.2373)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [480/689]  eta: 0:05:30  lr: 0.000085  loss: 0.2300 (0.2377)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [490/689]  eta: 0:05:14  lr: 0.000085  loss: 0.2315 (0.2376)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [500/689]  eta: 0:04:58  lr: 0.000085  loss: 0.2278 (0.2375)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [510/689]  eta: 0:04:42  lr: 0.000085  loss: 0.2271 (0.2373)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [520/689]  eta: 0:04:26  lr: 0.000085  loss: 0.2317 (0.2375)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [530/689]  eta: 0:04:11  lr: 0.000085  loss: 0.2413 (0.2375)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [540/689]  eta: 0:03:55  lr: 0.000085  loss: 0.2368 (0.2375)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [550/689]  eta: 0:03:39  lr: 0.000085  loss: 0.2353 (0.2375)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [560/689]  eta: 0:03:23  lr: 0.000085  loss: 0.2318 (0.2374)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [570/689]  eta: 0:03:07  lr: 0.000085  loss: 0.2246 (0.2372)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [580/689]  eta: 0:02:52  lr: 0.000085  loss: 0.2287 (0.2373)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [590/689]  eta: 0:02:36  lr: 0.000085  loss: 0.2425 (0.2376)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [600/689]  eta: 0:02:20  lr: 0.000085  loss: 0.2412 (0.2377)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [610/689]  eta: 0:02:04  lr: 0.000085  loss: 0.2397 (0.2377)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [620/689]  eta: 0:01:49  lr: 0.000085  loss: 0.2344 (0.2377)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [630/689]  eta: 0:01:33  lr: 0.000085  loss: 0.2361 (0.2378)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [640/689]  eta: 0:01:17  lr: 0.000085  loss: 0.2510 (0.2383)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [650/689]  eta: 0:01:01  lr: 0.000085  loss: 0.2494 (0.2382)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [660/689]  eta: 0:00:45  lr: 0.000085  loss: 0.2272 (0.2382)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [670/689]  eta: 0:00:30  lr: 0.000085  loss: 0.2277 (0.2382)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [680/689]  eta: 0:00:14  lr: 0.000085  loss: 0.2294 (0.2384)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236]  [688/689]  eta: 0:00:01  lr: 0.000085  loss: 0.2390 (0.2385)  time: 1.5798  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:236] Total time: 0:18:08 (1.5801 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: 0.2390 (0.2385)\n",
      "Valid: [epoch:236]  [ 0/14]  eta: 0:00:14  loss: 0.2410 (0.2410)  time: 1.0104  data: 0.3646  max mem: 39763\n",
      "Valid: [epoch:236]  [13/14]  eta: 0:00:00  loss: 0.2228 (0.2256)  time: 0.1142  data: 0.0261  max mem: 39763\n",
      "Valid: [epoch:236] Total time: 0:00:01 (0.1233 s / it)\n",
      "Averaged stats: loss: 0.2228 (0.2256)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_236_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.226%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:237]  [  0/689]  eta: 0:11:43  lr: 0.000085  loss: 0.2000 (0.2000)  time: 1.0206  data: 0.5457  max mem: 39763\n",
      "Train: [epoch:237]  [ 10/689]  eta: 0:17:14  lr: 0.000085  loss: 0.2402 (0.2345)  time: 1.5242  data: 0.0497  max mem: 39763\n",
      "Train: [epoch:237]  [ 20/689]  eta: 0:17:16  lr: 0.000085  loss: 0.2402 (0.2341)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [ 30/689]  eta: 0:17:06  lr: 0.000085  loss: 0.2434 (0.2429)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [ 40/689]  eta: 0:16:54  lr: 0.000085  loss: 0.2341 (0.2405)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [ 50/689]  eta: 0:16:40  lr: 0.000085  loss: 0.2253 (0.2402)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [ 60/689]  eta: 0:16:25  lr: 0.000085  loss: 0.2253 (0.2410)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [ 70/689]  eta: 0:16:10  lr: 0.000085  loss: 0.2249 (0.2387)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [ 80/689]  eta: 0:15:55  lr: 0.000085  loss: 0.2210 (0.2374)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [ 90/689]  eta: 0:15:40  lr: 0.000085  loss: 0.2290 (0.2399)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [100/689]  eta: 0:15:25  lr: 0.000085  loss: 0.2513 (0.2406)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [110/689]  eta: 0:15:09  lr: 0.000085  loss: 0.2501 (0.2412)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [120/689]  eta: 0:14:54  lr: 0.000085  loss: 0.2383 (0.2406)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [130/689]  eta: 0:14:39  lr: 0.000085  loss: 0.2383 (0.2408)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [140/689]  eta: 0:14:23  lr: 0.000085  loss: 0.2375 (0.2401)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [150/689]  eta: 0:14:07  lr: 0.000085  loss: 0.2286 (0.2404)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [160/689]  eta: 0:13:52  lr: 0.000085  loss: 0.2292 (0.2402)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [170/689]  eta: 0:13:36  lr: 0.000085  loss: 0.2261 (0.2398)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [180/689]  eta: 0:13:21  lr: 0.000085  loss: 0.2409 (0.2395)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [190/689]  eta: 0:13:05  lr: 0.000085  loss: 0.2409 (0.2394)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [200/689]  eta: 0:12:49  lr: 0.000085  loss: 0.2254 (0.2384)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [210/689]  eta: 0:12:34  lr: 0.000085  loss: 0.2161 (0.2379)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [220/689]  eta: 0:12:18  lr: 0.000085  loss: 0.2260 (0.2383)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [230/689]  eta: 0:12:02  lr: 0.000085  loss: 0.2422 (0.2389)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [240/689]  eta: 0:11:47  lr: 0.000085  loss: 0.2216 (0.2383)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [250/689]  eta: 0:11:31  lr: 0.000085  loss: 0.2216 (0.2382)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [260/689]  eta: 0:11:15  lr: 0.000085  loss: 0.2283 (0.2383)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [270/689]  eta: 0:10:59  lr: 0.000085  loss: 0.2269 (0.2378)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [280/689]  eta: 0:10:44  lr: 0.000085  loss: 0.2289 (0.2376)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [290/689]  eta: 0:10:28  lr: 0.000085  loss: 0.2326 (0.2379)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [300/689]  eta: 0:10:12  lr: 0.000085  loss: 0.2294 (0.2379)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [310/689]  eta: 0:09:57  lr: 0.000085  loss: 0.2253 (0.2376)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [320/689]  eta: 0:09:41  lr: 0.000085  loss: 0.2253 (0.2378)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [330/689]  eta: 0:09:25  lr: 0.000085  loss: 0.2310 (0.2379)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [340/689]  eta: 0:09:09  lr: 0.000085  loss: 0.2285 (0.2376)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [350/689]  eta: 0:08:54  lr: 0.000085  loss: 0.2329 (0.2380)  time: 1.5777  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:237]  [360/689]  eta: 0:08:38  lr: 0.000085  loss: 0.2360 (0.2380)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [370/689]  eta: 0:08:22  lr: 0.000085  loss: 0.2322 (0.2380)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [380/689]  eta: 0:08:06  lr: 0.000085  loss: 0.2424 (0.2381)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [390/689]  eta: 0:07:51  lr: 0.000085  loss: 0.2318 (0.2378)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [400/689]  eta: 0:07:35  lr: 0.000085  loss: 0.2318 (0.2379)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [410/689]  eta: 0:07:19  lr: 0.000085  loss: 0.2237 (0.2376)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [420/689]  eta: 0:07:03  lr: 0.000085  loss: 0.2201 (0.2373)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [430/689]  eta: 0:06:48  lr: 0.000085  loss: 0.2312 (0.2376)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [440/689]  eta: 0:06:32  lr: 0.000085  loss: 0.2444 (0.2376)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [450/689]  eta: 0:06:16  lr: 0.000085  loss: 0.2343 (0.2374)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [460/689]  eta: 0:06:00  lr: 0.000085  loss: 0.2320 (0.2376)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [470/689]  eta: 0:05:45  lr: 0.000085  loss: 0.2368 (0.2380)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [480/689]  eta: 0:05:29  lr: 0.000085  loss: 0.2335 (0.2377)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [490/689]  eta: 0:05:13  lr: 0.000085  loss: 0.2272 (0.2377)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [500/689]  eta: 0:04:57  lr: 0.000085  loss: 0.2320 (0.2379)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [510/689]  eta: 0:04:42  lr: 0.000085  loss: 0.2352 (0.2377)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [520/689]  eta: 0:04:26  lr: 0.000085  loss: 0.2275 (0.2376)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [530/689]  eta: 0:04:10  lr: 0.000085  loss: 0.2240 (0.2374)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [540/689]  eta: 0:03:54  lr: 0.000085  loss: 0.2292 (0.2376)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [550/689]  eta: 0:03:39  lr: 0.000085  loss: 0.2410 (0.2378)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [560/689]  eta: 0:03:23  lr: 0.000085  loss: 0.2635 (0.2385)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [570/689]  eta: 0:03:07  lr: 0.000085  loss: 0.2648 (0.2387)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [580/689]  eta: 0:02:51  lr: 0.000085  loss: 0.2345 (0.2386)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [590/689]  eta: 0:02:36  lr: 0.000085  loss: 0.2267 (0.2385)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [600/689]  eta: 0:02:20  lr: 0.000085  loss: 0.2251 (0.2385)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [610/689]  eta: 0:02:04  lr: 0.000085  loss: 0.2284 (0.2384)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [620/689]  eta: 0:01:48  lr: 0.000085  loss: 0.2284 (0.2384)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [630/689]  eta: 0:01:33  lr: 0.000085  loss: 0.2376 (0.2384)  time: 1.5828  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [640/689]  eta: 0:01:17  lr: 0.000085  loss: 0.2382 (0.2385)  time: 1.5829  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [650/689]  eta: 0:01:01  lr: 0.000085  loss: 0.2395 (0.2387)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [660/689]  eta: 0:00:45  lr: 0.000085  loss: 0.2260 (0.2386)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [670/689]  eta: 0:00:29  lr: 0.000085  loss: 0.2306 (0.2389)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [680/689]  eta: 0:00:14  lr: 0.000085  loss: 0.2430 (0.2389)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237]  [688/689]  eta: 0:00:01  lr: 0.000085  loss: 0.2430 (0.2391)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:237] Total time: 0:18:07 (1.5782 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: 0.2430 (0.2391)\n",
      "Valid: [epoch:237]  [ 0/14]  eta: 0:00:14  loss: 0.2253 (0.2253)  time: 1.0392  data: 0.4355  max mem: 39763\n",
      "Valid: [epoch:237]  [13/14]  eta: 0:00:00  loss: 0.2261 (0.2290)  time: 0.1163  data: 0.0312  max mem: 39763\n",
      "Valid: [epoch:237] Total time: 0:00:01 (0.1254 s / it)\n",
      "Averaged stats: loss: 0.2261 (0.2290)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_237_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.229%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:238]  [  0/689]  eta: 0:11:50  lr: 0.000085  loss: 0.2009 (0.2009)  time: 1.0307  data: 0.5525  max mem: 39763\n",
      "Train: [epoch:238]  [ 10/689]  eta: 0:17:19  lr: 0.000085  loss: 0.2445 (0.2452)  time: 1.5316  data: 0.0503  max mem: 39763\n",
      "Train: [epoch:238]  [ 20/689]  eta: 0:17:20  lr: 0.000085  loss: 0.2445 (0.2436)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [ 30/689]  eta: 0:17:10  lr: 0.000085  loss: 0.2388 (0.2436)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [ 40/689]  eta: 0:16:57  lr: 0.000085  loss: 0.2388 (0.2421)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [ 50/689]  eta: 0:16:43  lr: 0.000085  loss: 0.2233 (0.2385)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [ 60/689]  eta: 0:16:29  lr: 0.000085  loss: 0.2220 (0.2384)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [ 70/689]  eta: 0:16:14  lr: 0.000085  loss: 0.2220 (0.2376)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [ 80/689]  eta: 0:15:59  lr: 0.000085  loss: 0.2264 (0.2367)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [ 90/689]  eta: 0:15:43  lr: 0.000085  loss: 0.2264 (0.2357)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [100/689]  eta: 0:15:28  lr: 0.000085  loss: 0.2290 (0.2364)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [110/689]  eta: 0:15:13  lr: 0.000085  loss: 0.2367 (0.2394)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [120/689]  eta: 0:14:57  lr: 0.000085  loss: 0.2340 (0.2393)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [130/689]  eta: 0:14:41  lr: 0.000085  loss: 0.2340 (0.2410)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [140/689]  eta: 0:14:26  lr: 0.000085  loss: 0.2473 (0.2413)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [150/689]  eta: 0:14:10  lr: 0.000085  loss: 0.2384 (0.2417)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [160/689]  eta: 0:13:55  lr: 0.000085  loss: 0.2379 (0.2413)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [170/689]  eta: 0:13:39  lr: 0.000085  loss: 0.2331 (0.2418)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [180/689]  eta: 0:13:23  lr: 0.000085  loss: 0.2294 (0.2414)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [190/689]  eta: 0:13:08  lr: 0.000085  loss: 0.2294 (0.2409)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [200/689]  eta: 0:12:52  lr: 0.000085  loss: 0.2121 (0.2395)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [210/689]  eta: 0:12:36  lr: 0.000085  loss: 0.2245 (0.2397)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [220/689]  eta: 0:12:20  lr: 0.000085  loss: 0.2422 (0.2407)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [230/689]  eta: 0:12:05  lr: 0.000085  loss: 0.2418 (0.2405)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [240/689]  eta: 0:11:49  lr: 0.000085  loss: 0.2393 (0.2410)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [250/689]  eta: 0:11:33  lr: 0.000085  loss: 0.2382 (0.2407)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [260/689]  eta: 0:11:17  lr: 0.000085  loss: 0.2375 (0.2412)  time: 1.5825  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:238]  [270/689]  eta: 0:11:02  lr: 0.000085  loss: 0.2449 (0.2412)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [280/689]  eta: 0:10:46  lr: 0.000085  loss: 0.2251 (0.2406)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [290/689]  eta: 0:10:30  lr: 0.000085  loss: 0.2251 (0.2410)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [300/689]  eta: 0:10:14  lr: 0.000085  loss: 0.2262 (0.2406)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [310/689]  eta: 0:09:58  lr: 0.000085  loss: 0.2227 (0.2403)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [320/689]  eta: 0:09:43  lr: 0.000085  loss: 0.2305 (0.2403)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [330/689]  eta: 0:09:27  lr: 0.000085  loss: 0.2314 (0.2404)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [340/689]  eta: 0:09:11  lr: 0.000085  loss: 0.2299 (0.2398)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [350/689]  eta: 0:08:55  lr: 0.000085  loss: 0.2302 (0.2404)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [360/689]  eta: 0:08:39  lr: 0.000085  loss: 0.2458 (0.2404)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [370/689]  eta: 0:08:24  lr: 0.000085  loss: 0.2270 (0.2401)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [380/689]  eta: 0:08:08  lr: 0.000085  loss: 0.2290 (0.2401)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [390/689]  eta: 0:07:52  lr: 0.000085  loss: 0.2332 (0.2404)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [400/689]  eta: 0:07:36  lr: 0.000085  loss: 0.2292 (0.2401)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [410/689]  eta: 0:07:21  lr: 0.000085  loss: 0.2274 (0.2401)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [420/689]  eta: 0:07:05  lr: 0.000085  loss: 0.2318 (0.2401)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [430/689]  eta: 0:06:49  lr: 0.000085  loss: 0.2341 (0.2400)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [440/689]  eta: 0:06:33  lr: 0.000085  loss: 0.2376 (0.2402)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [450/689]  eta: 0:06:17  lr: 0.000085  loss: 0.2355 (0.2400)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [460/689]  eta: 0:06:01  lr: 0.000085  loss: 0.2313 (0.2401)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [470/689]  eta: 0:05:46  lr: 0.000085  loss: 0.2427 (0.2402)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [480/689]  eta: 0:05:30  lr: 0.000085  loss: 0.2427 (0.2402)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [490/689]  eta: 0:05:14  lr: 0.000085  loss: 0.2481 (0.2405)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [500/689]  eta: 0:04:58  lr: 0.000085  loss: 0.2472 (0.2405)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [510/689]  eta: 0:04:42  lr: 0.000085  loss: 0.2362 (0.2407)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [520/689]  eta: 0:04:27  lr: 0.000085  loss: 0.2491 (0.2408)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [530/689]  eta: 0:04:11  lr: 0.000085  loss: 0.2274 (0.2406)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [540/689]  eta: 0:03:55  lr: 0.000085  loss: 0.2190 (0.2403)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [550/689]  eta: 0:03:39  lr: 0.000085  loss: 0.2278 (0.2403)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [560/689]  eta: 0:03:23  lr: 0.000085  loss: 0.2353 (0.2401)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [570/689]  eta: 0:03:08  lr: 0.000085  loss: 0.2324 (0.2399)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [580/689]  eta: 0:02:52  lr: 0.000085  loss: 0.2312 (0.2401)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [590/689]  eta: 0:02:36  lr: 0.000085  loss: 0.2331 (0.2402)  time: 1.5808  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [600/689]  eta: 0:02:20  lr: 0.000085  loss: 0.2260 (0.2401)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [610/689]  eta: 0:02:04  lr: 0.000085  loss: 0.2212 (0.2400)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [620/689]  eta: 0:01:49  lr: 0.000085  loss: 0.2219 (0.2398)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [630/689]  eta: 0:01:33  lr: 0.000085  loss: 0.2262 (0.2397)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [640/689]  eta: 0:01:17  lr: 0.000085  loss: 0.2412 (0.2399)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [650/689]  eta: 0:01:01  lr: 0.000085  loss: 0.2412 (0.2400)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [660/689]  eta: 0:00:45  lr: 0.000085  loss: 0.2364 (0.2400)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [670/689]  eta: 0:00:30  lr: 0.000085  loss: 0.2349 (0.2399)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [680/689]  eta: 0:00:14  lr: 0.000085  loss: 0.2324 (0.2399)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238]  [688/689]  eta: 0:00:01  lr: 0.000085  loss: 0.2285 (0.2398)  time: 1.5799  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:238] Total time: 0:18:09 (1.5811 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: 0.2285 (0.2398)\n",
      "Valid: [epoch:238]  [ 0/14]  eta: 0:00:14  loss: 0.2399 (0.2399)  time: 1.0183  data: 0.3733  max mem: 39763\n",
      "Valid: [epoch:238]  [13/14]  eta: 0:00:00  loss: 0.2287 (0.2312)  time: 0.1148  data: 0.0267  max mem: 39763\n",
      "Valid: [epoch:238] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.2287 (0.2312)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_238_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.231%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:239]  [  0/689]  eta: 0:12:40  lr: 0.000085  loss: 0.2028 (0.2028)  time: 1.1038  data: 0.6288  max mem: 39763\n",
      "Train: [epoch:239]  [ 10/689]  eta: 0:17:19  lr: 0.000085  loss: 0.2208 (0.2323)  time: 1.5316  data: 0.0572  max mem: 39763\n",
      "Train: [epoch:239]  [ 20/689]  eta: 0:17:18  lr: 0.000085  loss: 0.2314 (0.2353)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [ 30/689]  eta: 0:17:08  lr: 0.000085  loss: 0.2343 (0.2358)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [ 40/689]  eta: 0:16:55  lr: 0.000085  loss: 0.2294 (0.2359)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [ 50/689]  eta: 0:16:41  lr: 0.000085  loss: 0.2332 (0.2355)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [ 60/689]  eta: 0:16:26  lr: 0.000085  loss: 0.2370 (0.2367)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [ 70/689]  eta: 0:16:11  lr: 0.000085  loss: 0.2387 (0.2380)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [ 80/689]  eta: 0:15:56  lr: 0.000085  loss: 0.2538 (0.2394)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [ 90/689]  eta: 0:15:41  lr: 0.000085  loss: 0.2425 (0.2393)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [100/689]  eta: 0:15:25  lr: 0.000085  loss: 0.2423 (0.2402)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [110/689]  eta: 0:15:10  lr: 0.000085  loss: 0.2398 (0.2400)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [120/689]  eta: 0:14:55  lr: 0.000085  loss: 0.2287 (0.2399)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [130/689]  eta: 0:14:39  lr: 0.000085  loss: 0.2276 (0.2390)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [140/689]  eta: 0:14:23  lr: 0.000085  loss: 0.2276 (0.2387)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [150/689]  eta: 0:14:08  lr: 0.000085  loss: 0.2290 (0.2390)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [160/689]  eta: 0:13:52  lr: 0.000085  loss: 0.2303 (0.2387)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [170/689]  eta: 0:13:37  lr: 0.000085  loss: 0.2308 (0.2392)  time: 1.5781  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:239]  [180/689]  eta: 0:13:21  lr: 0.000085  loss: 0.2330 (0.2390)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [190/689]  eta: 0:13:05  lr: 0.000085  loss: 0.2355 (0.2397)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [200/689]  eta: 0:12:50  lr: 0.000085  loss: 0.2355 (0.2395)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [210/689]  eta: 0:12:34  lr: 0.000085  loss: 0.2323 (0.2399)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [220/689]  eta: 0:12:18  lr: 0.000085  loss: 0.2334 (0.2403)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [230/689]  eta: 0:12:03  lr: 0.000085  loss: 0.2333 (0.2406)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [240/689]  eta: 0:11:47  lr: 0.000085  loss: 0.2355 (0.2408)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [250/689]  eta: 0:11:31  lr: 0.000085  loss: 0.2335 (0.2406)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [260/689]  eta: 0:11:15  lr: 0.000085  loss: 0.2348 (0.2406)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [270/689]  eta: 0:11:00  lr: 0.000085  loss: 0.2348 (0.2403)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [280/689]  eta: 0:10:44  lr: 0.000085  loss: 0.2270 (0.2402)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [290/689]  eta: 0:10:28  lr: 0.000085  loss: 0.2270 (0.2400)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [300/689]  eta: 0:10:13  lr: 0.000085  loss: 0.2357 (0.2404)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [310/689]  eta: 0:09:57  lr: 0.000085  loss: 0.2334 (0.2403)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [320/689]  eta: 0:09:41  lr: 0.000085  loss: 0.2403 (0.2408)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [330/689]  eta: 0:09:25  lr: 0.000085  loss: 0.2505 (0.2409)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [340/689]  eta: 0:09:10  lr: 0.000085  loss: 0.2332 (0.2404)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [350/689]  eta: 0:08:54  lr: 0.000085  loss: 0.2308 (0.2402)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [360/689]  eta: 0:08:38  lr: 0.000085  loss: 0.2322 (0.2401)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [370/689]  eta: 0:08:22  lr: 0.000085  loss: 0.2259 (0.2396)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [380/689]  eta: 0:08:07  lr: 0.000085  loss: 0.2275 (0.2395)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [390/689]  eta: 0:07:51  lr: 0.000085  loss: 0.2310 (0.2395)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [400/689]  eta: 0:07:35  lr: 0.000085  loss: 0.2310 (0.2397)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [410/689]  eta: 0:07:19  lr: 0.000085  loss: 0.2458 (0.2399)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [420/689]  eta: 0:07:04  lr: 0.000085  loss: 0.2398 (0.2398)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [430/689]  eta: 0:06:48  lr: 0.000085  loss: 0.2272 (0.2397)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [440/689]  eta: 0:06:32  lr: 0.000085  loss: 0.2378 (0.2397)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [450/689]  eta: 0:06:16  lr: 0.000085  loss: 0.2333 (0.2397)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [460/689]  eta: 0:06:01  lr: 0.000085  loss: 0.2462 (0.2403)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [470/689]  eta: 0:05:45  lr: 0.000085  loss: 0.2340 (0.2402)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [480/689]  eta: 0:05:29  lr: 0.000085  loss: 0.2326 (0.2402)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [490/689]  eta: 0:05:13  lr: 0.000085  loss: 0.2402 (0.2406)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [500/689]  eta: 0:04:57  lr: 0.000085  loss: 0.2390 (0.2405)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [510/689]  eta: 0:04:42  lr: 0.000085  loss: 0.2400 (0.2406)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [520/689]  eta: 0:04:26  lr: 0.000085  loss: 0.2439 (0.2408)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [530/689]  eta: 0:04:10  lr: 0.000085  loss: 0.2439 (0.2409)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [540/689]  eta: 0:03:54  lr: 0.000085  loss: 0.2357 (0.2410)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [550/689]  eta: 0:03:39  lr: 0.000085  loss: 0.2357 (0.2409)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [560/689]  eta: 0:03:23  lr: 0.000085  loss: 0.2295 (0.2409)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [570/689]  eta: 0:03:07  lr: 0.000085  loss: 0.2338 (0.2410)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [580/689]  eta: 0:02:51  lr: 0.000085  loss: 0.2338 (0.2409)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [590/689]  eta: 0:02:36  lr: 0.000085  loss: 0.2384 (0.2409)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [600/689]  eta: 0:02:20  lr: 0.000085  loss: 0.2387 (0.2409)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [610/689]  eta: 0:02:04  lr: 0.000085  loss: 0.2342 (0.2409)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [620/689]  eta: 0:01:48  lr: 0.000085  loss: 0.2333 (0.2408)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [630/689]  eta: 0:01:33  lr: 0.000085  loss: 0.2432 (0.2410)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [640/689]  eta: 0:01:17  lr: 0.000085  loss: 0.2536 (0.2411)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [650/689]  eta: 0:01:01  lr: 0.000085  loss: 0.2505 (0.2412)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [660/689]  eta: 0:00:45  lr: 0.000085  loss: 0.2381 (0.2413)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [670/689]  eta: 0:00:29  lr: 0.000085  loss: 0.2381 (0.2413)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [680/689]  eta: 0:00:14  lr: 0.000085  loss: 0.2345 (0.2412)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239]  [688/689]  eta: 0:00:01  lr: 0.000085  loss: 0.2359 (0.2413)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:239] Total time: 0:18:06 (1.5772 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: 0.2359 (0.2413)\n",
      "Valid: [epoch:239]  [ 0/14]  eta: 0:00:14  loss: 0.2382 (0.2382)  time: 1.0195  data: 0.3403  max mem: 39763\n",
      "Valid: [epoch:239]  [13/14]  eta: 0:00:00  loss: 0.2257 (0.2284)  time: 0.1147  data: 0.0244  max mem: 39763\n",
      "Valid: [epoch:239] Total time: 0:00:01 (0.1225 s / it)\n",
      "Averaged stats: loss: 0.2257 (0.2284)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_239_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.228%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:240]  [  0/689]  eta: 0:11:24  lr: 0.000085  loss: 0.2471 (0.2471)  time: 0.9934  data: 0.5150  max mem: 39763\n",
      "Train: [epoch:240]  [ 10/689]  eta: 0:17:14  lr: 0.000085  loss: 0.2405 (0.2463)  time: 1.5234  data: 0.0469  max mem: 39763\n",
      "Train: [epoch:240]  [ 20/689]  eta: 0:17:16  lr: 0.000085  loss: 0.2365 (0.2400)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [ 30/689]  eta: 0:17:07  lr: 0.000085  loss: 0.2355 (0.2412)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [ 40/689]  eta: 0:16:54  lr: 0.000085  loss: 0.2256 (0.2405)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [ 50/689]  eta: 0:16:40  lr: 0.000085  loss: 0.2338 (0.2446)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [ 60/689]  eta: 0:16:26  lr: 0.000085  loss: 0.2466 (0.2442)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [ 70/689]  eta: 0:16:11  lr: 0.000085  loss: 0.2305 (0.2417)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [ 80/689]  eta: 0:15:56  lr: 0.000085  loss: 0.2305 (0.2438)  time: 1.5776  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:240]  [ 90/689]  eta: 0:15:41  lr: 0.000085  loss: 0.2408 (0.2436)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [100/689]  eta: 0:15:25  lr: 0.000085  loss: 0.2371 (0.2428)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [110/689]  eta: 0:15:10  lr: 0.000085  loss: 0.2306 (0.2439)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [120/689]  eta: 0:14:54  lr: 0.000085  loss: 0.2297 (0.2428)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [130/689]  eta: 0:14:39  lr: 0.000085  loss: 0.2323 (0.2428)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [140/689]  eta: 0:14:23  lr: 0.000085  loss: 0.2371 (0.2424)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [150/689]  eta: 0:14:08  lr: 0.000085  loss: 0.2391 (0.2422)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [160/689]  eta: 0:13:52  lr: 0.000085  loss: 0.2496 (0.2431)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [170/689]  eta: 0:13:36  lr: 0.000085  loss: 0.2507 (0.2430)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [180/689]  eta: 0:13:21  lr: 0.000085  loss: 0.2365 (0.2426)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [190/689]  eta: 0:13:05  lr: 0.000085  loss: 0.2365 (0.2425)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [200/689]  eta: 0:12:50  lr: 0.000085  loss: 0.2388 (0.2421)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [210/689]  eta: 0:12:34  lr: 0.000085  loss: 0.2307 (0.2421)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [220/689]  eta: 0:12:18  lr: 0.000085  loss: 0.2393 (0.2425)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [230/689]  eta: 0:12:03  lr: 0.000085  loss: 0.2410 (0.2423)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [240/689]  eta: 0:11:47  lr: 0.000085  loss: 0.2410 (0.2424)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [250/689]  eta: 0:11:31  lr: 0.000085  loss: 0.2328 (0.2417)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [260/689]  eta: 0:11:16  lr: 0.000085  loss: 0.2233 (0.2412)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [270/689]  eta: 0:11:00  lr: 0.000085  loss: 0.2291 (0.2412)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [280/689]  eta: 0:10:44  lr: 0.000085  loss: 0.2367 (0.2415)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [290/689]  eta: 0:10:29  lr: 0.000085  loss: 0.2439 (0.2417)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [300/689]  eta: 0:10:13  lr: 0.000085  loss: 0.2502 (0.2421)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [310/689]  eta: 0:09:57  lr: 0.000085  loss: 0.2313 (0.2415)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [320/689]  eta: 0:09:41  lr: 0.000085  loss: 0.2195 (0.2412)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [330/689]  eta: 0:09:26  lr: 0.000085  loss: 0.2304 (0.2411)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [340/689]  eta: 0:09:10  lr: 0.000085  loss: 0.2368 (0.2410)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [350/689]  eta: 0:08:54  lr: 0.000085  loss: 0.2442 (0.2413)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [360/689]  eta: 0:08:38  lr: 0.000085  loss: 0.2359 (0.2411)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [370/689]  eta: 0:08:23  lr: 0.000085  loss: 0.2359 (0.2411)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [380/689]  eta: 0:08:07  lr: 0.000085  loss: 0.2364 (0.2409)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [390/689]  eta: 0:07:51  lr: 0.000085  loss: 0.2400 (0.2411)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [400/689]  eta: 0:07:36  lr: 0.000085  loss: 0.2361 (0.2411)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [410/689]  eta: 0:07:20  lr: 0.000085  loss: 0.2331 (0.2411)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [420/689]  eta: 0:07:04  lr: 0.000085  loss: 0.2331 (0.2413)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [430/689]  eta: 0:06:48  lr: 0.000085  loss: 0.2343 (0.2413)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [440/689]  eta: 0:06:32  lr: 0.000085  loss: 0.2407 (0.2416)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [450/689]  eta: 0:06:17  lr: 0.000085  loss: 0.2413 (0.2413)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [460/689]  eta: 0:06:01  lr: 0.000085  loss: 0.2322 (0.2414)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [470/689]  eta: 0:05:45  lr: 0.000085  loss: 0.2462 (0.2415)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [480/689]  eta: 0:05:29  lr: 0.000085  loss: 0.2402 (0.2413)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [490/689]  eta: 0:05:14  lr: 0.000085  loss: 0.2282 (0.2413)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [500/689]  eta: 0:04:58  lr: 0.000085  loss: 0.2295 (0.2414)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [510/689]  eta: 0:04:42  lr: 0.000085  loss: 0.2295 (0.2412)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [520/689]  eta: 0:04:26  lr: 0.000085  loss: 0.2380 (0.2416)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [530/689]  eta: 0:04:11  lr: 0.000085  loss: 0.2376 (0.2415)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [540/689]  eta: 0:03:55  lr: 0.000085  loss: 0.2372 (0.2417)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [550/689]  eta: 0:03:39  lr: 0.000085  loss: 0.2369 (0.2416)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [560/689]  eta: 0:03:23  lr: 0.000085  loss: 0.2394 (0.2420)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [570/689]  eta: 0:03:07  lr: 0.000085  loss: 0.2474 (0.2418)  time: 1.5816  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [580/689]  eta: 0:02:52  lr: 0.000085  loss: 0.2349 (0.2417)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [590/689]  eta: 0:02:36  lr: 0.000085  loss: 0.2264 (0.2414)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [600/689]  eta: 0:02:20  lr: 0.000085  loss: 0.2380 (0.2416)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [610/689]  eta: 0:02:04  lr: 0.000085  loss: 0.2400 (0.2415)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [620/689]  eta: 0:01:48  lr: 0.000085  loss: 0.2414 (0.2415)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [630/689]  eta: 0:01:33  lr: 0.000085  loss: 0.2324 (0.2415)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [640/689]  eta: 0:01:17  lr: 0.000085  loss: 0.2413 (0.2417)  time: 1.5807  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [650/689]  eta: 0:01:01  lr: 0.000085  loss: 0.2438 (0.2419)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [660/689]  eta: 0:00:45  lr: 0.000085  loss: 0.2357 (0.2416)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [670/689]  eta: 0:00:30  lr: 0.000085  loss: 0.2167 (0.2417)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [680/689]  eta: 0:00:14  lr: 0.000085  loss: 0.2362 (0.2416)  time: 1.5809  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240]  [688/689]  eta: 0:00:01  lr: 0.000085  loss: 0.2320 (0.2414)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:240] Total time: 0:18:08 (1.5796 s / it)\n",
      "Averaged stats: lr: 0.000085  loss: 0.2320 (0.2414)\n",
      "Valid: [epoch:240]  [ 0/14]  eta: 0:00:14  loss: 0.2129 (0.2129)  time: 1.0216  data: 0.3771  max mem: 39763\n",
      "Valid: [epoch:240]  [13/14]  eta: 0:00:00  loss: 0.2262 (0.2293)  time: 0.1150  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:240] Total time: 0:00:01 (0.1241 s / it)\n",
      "Averaged stats: loss: 0.2262 (0.2293)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_240_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.229%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:241]  [  0/689]  eta: 0:11:48  lr: 0.000084  loss: 0.1949 (0.1949)  time: 1.0285  data: 0.5499  max mem: 39763\n",
      "Train: [epoch:241]  [ 10/689]  eta: 0:17:15  lr: 0.000084  loss: 0.2358 (0.2375)  time: 1.5255  data: 0.0501  max mem: 39763\n",
      "Train: [epoch:241]  [ 20/689]  eta: 0:17:17  lr: 0.000084  loss: 0.2278 (0.2315)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [ 30/689]  eta: 0:17:07  lr: 0.000084  loss: 0.2278 (0.2361)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [ 40/689]  eta: 0:16:54  lr: 0.000084  loss: 0.2406 (0.2386)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [ 50/689]  eta: 0:16:40  lr: 0.000084  loss: 0.2243 (0.2373)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [ 60/689]  eta: 0:16:26  lr: 0.000084  loss: 0.2243 (0.2369)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [ 70/689]  eta: 0:16:11  lr: 0.000084  loss: 0.2306 (0.2361)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [ 80/689]  eta: 0:15:56  lr: 0.000084  loss: 0.2278 (0.2350)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [ 90/689]  eta: 0:15:41  lr: 0.000084  loss: 0.2373 (0.2364)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [100/689]  eta: 0:15:25  lr: 0.000084  loss: 0.2423 (0.2367)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [110/689]  eta: 0:15:10  lr: 0.000084  loss: 0.2367 (0.2367)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [120/689]  eta: 0:14:55  lr: 0.000084  loss: 0.2308 (0.2364)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [130/689]  eta: 0:14:39  lr: 0.000084  loss: 0.2312 (0.2370)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [140/689]  eta: 0:14:24  lr: 0.000084  loss: 0.2347 (0.2383)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [150/689]  eta: 0:14:08  lr: 0.000084  loss: 0.2432 (0.2390)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [160/689]  eta: 0:13:52  lr: 0.000084  loss: 0.2490 (0.2402)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [170/689]  eta: 0:13:37  lr: 0.000084  loss: 0.2490 (0.2408)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [180/689]  eta: 0:13:21  lr: 0.000084  loss: 0.2273 (0.2400)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [190/689]  eta: 0:13:05  lr: 0.000084  loss: 0.2332 (0.2399)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [200/689]  eta: 0:12:50  lr: 0.000084  loss: 0.2346 (0.2398)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [210/689]  eta: 0:12:34  lr: 0.000084  loss: 0.2308 (0.2398)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [220/689]  eta: 0:12:18  lr: 0.000084  loss: 0.2268 (0.2397)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [230/689]  eta: 0:12:03  lr: 0.000084  loss: 0.2304 (0.2400)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [240/689]  eta: 0:11:47  lr: 0.000084  loss: 0.2338 (0.2402)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [250/689]  eta: 0:11:31  lr: 0.000084  loss: 0.2408 (0.2405)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [260/689]  eta: 0:11:16  lr: 0.000084  loss: 0.2317 (0.2400)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [270/689]  eta: 0:11:00  lr: 0.000084  loss: 0.2264 (0.2403)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [280/689]  eta: 0:10:44  lr: 0.000084  loss: 0.2528 (0.2411)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [290/689]  eta: 0:10:28  lr: 0.000084  loss: 0.2494 (0.2412)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [300/689]  eta: 0:10:13  lr: 0.000084  loss: 0.2434 (0.2413)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [310/689]  eta: 0:09:57  lr: 0.000084  loss: 0.2465 (0.2417)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [320/689]  eta: 0:09:41  lr: 0.000084  loss: 0.2329 (0.2414)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [330/689]  eta: 0:09:26  lr: 0.000084  loss: 0.2346 (0.2417)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [340/689]  eta: 0:09:10  lr: 0.000084  loss: 0.2346 (0.2417)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [350/689]  eta: 0:08:54  lr: 0.000084  loss: 0.2343 (0.2416)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [360/689]  eta: 0:08:38  lr: 0.000084  loss: 0.2319 (0.2413)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [370/689]  eta: 0:08:23  lr: 0.000084  loss: 0.2300 (0.2415)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [380/689]  eta: 0:08:07  lr: 0.000084  loss: 0.2382 (0.2417)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [390/689]  eta: 0:07:51  lr: 0.000084  loss: 0.2402 (0.2416)  time: 1.5826  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [400/689]  eta: 0:07:35  lr: 0.000084  loss: 0.2402 (0.2416)  time: 1.5827  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [410/689]  eta: 0:07:20  lr: 0.000084  loss: 0.2475 (0.2418)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [420/689]  eta: 0:07:04  lr: 0.000084  loss: 0.2366 (0.2415)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [430/689]  eta: 0:06:48  lr: 0.000084  loss: 0.2355 (0.2415)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [440/689]  eta: 0:06:32  lr: 0.000084  loss: 0.2332 (0.2414)  time: 1.5817  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [450/689]  eta: 0:06:17  lr: 0.000084  loss: 0.2415 (0.2417)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [460/689]  eta: 0:06:01  lr: 0.000084  loss: 0.2505 (0.2419)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [470/689]  eta: 0:05:45  lr: 0.000084  loss: 0.2524 (0.2422)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [480/689]  eta: 0:05:29  lr: 0.000084  loss: 0.2447 (0.2425)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [490/689]  eta: 0:05:14  lr: 0.000084  loss: 0.2413 (0.2423)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [500/689]  eta: 0:04:58  lr: 0.000084  loss: 0.2320 (0.2423)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [510/689]  eta: 0:04:42  lr: 0.000084  loss: 0.2396 (0.2423)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [520/689]  eta: 0:04:26  lr: 0.000084  loss: 0.2428 (0.2426)  time: 1.5814  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [530/689]  eta: 0:04:11  lr: 0.000084  loss: 0.2358 (0.2425)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [540/689]  eta: 0:03:55  lr: 0.000084  loss: 0.2316 (0.2424)  time: 1.5819  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [550/689]  eta: 0:03:39  lr: 0.000084  loss: 0.2389 (0.2423)  time: 1.5822  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [560/689]  eta: 0:03:23  lr: 0.000084  loss: 0.2246 (0.2419)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [570/689]  eta: 0:03:07  lr: 0.000084  loss: 0.2280 (0.2420)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [580/689]  eta: 0:02:52  lr: 0.000084  loss: 0.2485 (0.2421)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [590/689]  eta: 0:02:36  lr: 0.000084  loss: 0.2418 (0.2422)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [600/689]  eta: 0:02:20  lr: 0.000084  loss: 0.2418 (0.2424)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [610/689]  eta: 0:02:04  lr: 0.000084  loss: 0.2308 (0.2423)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [620/689]  eta: 0:01:48  lr: 0.000084  loss: 0.2472 (0.2424)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [630/689]  eta: 0:01:33  lr: 0.000084  loss: 0.2553 (0.2427)  time: 1.5824  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [640/689]  eta: 0:01:17  lr: 0.000084  loss: 0.2506 (0.2428)  time: 1.5825  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [650/689]  eta: 0:01:01  lr: 0.000084  loss: 0.2351 (0.2427)  time: 1.5820  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:241]  [660/689]  eta: 0:00:45  lr: 0.000084  loss: 0.2343 (0.2428)  time: 1.5818  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [670/689]  eta: 0:00:30  lr: 0.000084  loss: 0.2464 (0.2430)  time: 1.5821  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [680/689]  eta: 0:00:14  lr: 0.000084  loss: 0.2510 (0.2433)  time: 1.5823  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241]  [688/689]  eta: 0:00:01  lr: 0.000084  loss: 0.2501 (0.2433)  time: 1.5820  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:241] Total time: 0:18:08 (1.5797 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: 0.2501 (0.2433)\n",
      "Valid: [epoch:241]  [ 0/14]  eta: 0:00:14  loss: 0.2434 (0.2434)  time: 1.0211  data: 0.3524  max mem: 39763\n",
      "Valid: [epoch:241]  [13/14]  eta: 0:00:00  loss: 0.2278 (0.2309)  time: 0.1150  data: 0.0252  max mem: 39763\n",
      "Valid: [epoch:241] Total time: 0:00:01 (0.1241 s / it)\n",
      "Averaged stats: loss: 0.2278 (0.2309)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_241_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.231%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:242]  [  0/689]  eta: 0:12:26  lr: 0.000084  loss: 0.2296 (0.2296)  time: 1.0828  data: 0.6045  max mem: 39763\n",
      "Train: [epoch:242]  [ 10/689]  eta: 0:17:20  lr: 0.000084  loss: 0.2421 (0.2439)  time: 1.5328  data: 0.0550  max mem: 39763\n",
      "Train: [epoch:242]  [ 20/689]  eta: 0:17:19  lr: 0.000084  loss: 0.2421 (0.2515)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [ 30/689]  eta: 0:17:09  lr: 0.000084  loss: 0.2406 (0.2490)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [ 40/689]  eta: 0:16:56  lr: 0.000084  loss: 0.2494 (0.2498)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [ 50/689]  eta: 0:16:42  lr: 0.000084  loss: 0.2494 (0.2479)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [ 60/689]  eta: 0:16:27  lr: 0.000084  loss: 0.2351 (0.2470)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [ 70/689]  eta: 0:16:12  lr: 0.000084  loss: 0.2336 (0.2454)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [ 80/689]  eta: 0:15:57  lr: 0.000084  loss: 0.2269 (0.2440)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [ 90/689]  eta: 0:15:42  lr: 0.000084  loss: 0.2277 (0.2442)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [100/689]  eta: 0:15:26  lr: 0.000084  loss: 0.2421 (0.2441)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [110/689]  eta: 0:15:11  lr: 0.000084  loss: 0.2443 (0.2437)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [120/689]  eta: 0:14:55  lr: 0.000084  loss: 0.2461 (0.2446)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [130/689]  eta: 0:14:40  lr: 0.000084  loss: 0.2461 (0.2450)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [140/689]  eta: 0:14:24  lr: 0.000084  loss: 0.2404 (0.2446)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [150/689]  eta: 0:14:09  lr: 0.000084  loss: 0.2395 (0.2443)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [160/689]  eta: 0:13:53  lr: 0.000084  loss: 0.2463 (0.2444)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [170/689]  eta: 0:13:37  lr: 0.000084  loss: 0.2545 (0.2457)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [180/689]  eta: 0:13:22  lr: 0.000084  loss: 0.2493 (0.2456)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [190/689]  eta: 0:13:06  lr: 0.000084  loss: 0.2426 (0.2456)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [200/689]  eta: 0:12:50  lr: 0.000084  loss: 0.2417 (0.2461)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [210/689]  eta: 0:12:35  lr: 0.000084  loss: 0.2284 (0.2452)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [220/689]  eta: 0:12:19  lr: 0.000084  loss: 0.2253 (0.2447)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [230/689]  eta: 0:12:03  lr: 0.000084  loss: 0.2478 (0.2451)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [240/689]  eta: 0:11:47  lr: 0.000084  loss: 0.2398 (0.2446)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [250/689]  eta: 0:11:32  lr: 0.000084  loss: 0.2314 (0.2444)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [260/689]  eta: 0:11:16  lr: 0.000084  loss: 0.2356 (0.2445)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [270/689]  eta: 0:11:00  lr: 0.000084  loss: 0.2330 (0.2443)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [280/689]  eta: 0:10:44  lr: 0.000084  loss: 0.2330 (0.2447)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [290/689]  eta: 0:10:29  lr: 0.000084  loss: 0.2318 (0.2445)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [300/689]  eta: 0:10:13  lr: 0.000084  loss: 0.2317 (0.2443)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [310/689]  eta: 0:09:57  lr: 0.000084  loss: 0.2378 (0.2442)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [320/689]  eta: 0:09:41  lr: 0.000084  loss: 0.2271 (0.2437)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [330/689]  eta: 0:09:26  lr: 0.000084  loss: 0.2483 (0.2448)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [340/689]  eta: 0:09:10  lr: 0.000084  loss: 0.2541 (0.2450)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [350/689]  eta: 0:08:54  lr: 0.000084  loss: 0.2446 (0.2451)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [360/689]  eta: 0:08:38  lr: 0.000084  loss: 0.2486 (0.2454)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [370/689]  eta: 0:08:23  lr: 0.000084  loss: 0.2393 (0.2454)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [380/689]  eta: 0:08:07  lr: 0.000084  loss: 0.2348 (0.2451)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [390/689]  eta: 0:07:51  lr: 0.000084  loss: 0.2408 (0.2456)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [400/689]  eta: 0:07:35  lr: 0.000084  loss: 0.2458 (0.2456)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [410/689]  eta: 0:07:20  lr: 0.000084  loss: 0.2338 (0.2455)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [420/689]  eta: 0:07:04  lr: 0.000084  loss: 0.2429 (0.2453)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [430/689]  eta: 0:06:48  lr: 0.000084  loss: 0.2349 (0.2450)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [440/689]  eta: 0:06:32  lr: 0.000084  loss: 0.2331 (0.2454)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [450/689]  eta: 0:06:16  lr: 0.000084  loss: 0.2399 (0.2453)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [460/689]  eta: 0:06:01  lr: 0.000084  loss: 0.2393 (0.2456)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [470/689]  eta: 0:05:45  lr: 0.000084  loss: 0.2397 (0.2457)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [480/689]  eta: 0:05:29  lr: 0.000084  loss: 0.2342 (0.2455)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [490/689]  eta: 0:05:13  lr: 0.000084  loss: 0.2354 (0.2456)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [500/689]  eta: 0:04:58  lr: 0.000084  loss: 0.2354 (0.2455)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [510/689]  eta: 0:04:42  lr: 0.000084  loss: 0.2390 (0.2455)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [520/689]  eta: 0:04:26  lr: 0.000084  loss: 0.2484 (0.2456)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [530/689]  eta: 0:04:10  lr: 0.000084  loss: 0.2379 (0.2454)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [540/689]  eta: 0:03:54  lr: 0.000084  loss: 0.2395 (0.2453)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [550/689]  eta: 0:03:39  lr: 0.000084  loss: 0.2483 (0.2453)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [560/689]  eta: 0:03:23  lr: 0.000084  loss: 0.2307 (0.2451)  time: 1.5778  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:242]  [570/689]  eta: 0:03:07  lr: 0.000084  loss: 0.2328 (0.2451)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [580/689]  eta: 0:02:51  lr: 0.000084  loss: 0.2528 (0.2452)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [590/689]  eta: 0:02:36  lr: 0.000084  loss: 0.2590 (0.2455)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [600/689]  eta: 0:02:20  lr: 0.000084  loss: 0.2579 (0.2457)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [610/689]  eta: 0:02:04  lr: 0.000084  loss: 0.2296 (0.2453)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [620/689]  eta: 0:01:48  lr: 0.000084  loss: 0.2257 (0.2451)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [630/689]  eta: 0:01:33  lr: 0.000084  loss: 0.2292 (0.2449)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [640/689]  eta: 0:01:17  lr: 0.000084  loss: 0.2356 (0.2450)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [650/689]  eta: 0:01:01  lr: 0.000084  loss: 0.2406 (0.2449)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [660/689]  eta: 0:00:45  lr: 0.000084  loss: 0.2301 (0.2448)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [670/689]  eta: 0:00:29  lr: 0.000084  loss: 0.2238 (0.2446)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [680/689]  eta: 0:00:14  lr: 0.000084  loss: 0.2264 (0.2444)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242]  [688/689]  eta: 0:00:01  lr: 0.000084  loss: 0.2304 (0.2444)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:242] Total time: 0:18:06 (1.5774 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: 0.2304 (0.2444)\n",
      "Valid: [epoch:242]  [ 0/14]  eta: 0:00:14  loss: 0.2442 (0.2442)  time: 1.0018  data: 0.3813  max mem: 39763\n",
      "Valid: [epoch:242]  [13/14]  eta: 0:00:00  loss: 0.2279 (0.2310)  time: 0.1135  data: 0.0273  max mem: 39763\n",
      "Valid: [epoch:242] Total time: 0:00:01 (0.1256 s / it)\n",
      "Averaged stats: loss: 0.2279 (0.2310)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_242_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.231%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:243]  [  0/689]  eta: 0:11:41  lr: 0.000084  loss: 0.2196 (0.2196)  time: 1.0187  data: 0.5419  max mem: 39763\n",
      "Train: [epoch:243]  [ 10/689]  eta: 0:17:14  lr: 0.000084  loss: 0.2524 (0.2582)  time: 1.5229  data: 0.0493  max mem: 39763\n",
      "Train: [epoch:243]  [ 20/689]  eta: 0:17:15  lr: 0.000084  loss: 0.2444 (0.2506)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [ 30/689]  eta: 0:17:06  lr: 0.000084  loss: 0.2358 (0.2518)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [ 40/689]  eta: 0:16:53  lr: 0.000084  loss: 0.2490 (0.2496)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [ 50/689]  eta: 0:16:39  lr: 0.000084  loss: 0.2479 (0.2490)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [ 60/689]  eta: 0:16:25  lr: 0.000084  loss: 0.2507 (0.2498)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [ 70/689]  eta: 0:16:10  lr: 0.000084  loss: 0.2472 (0.2492)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [ 80/689]  eta: 0:15:55  lr: 0.000084  loss: 0.2412 (0.2485)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [ 90/689]  eta: 0:15:40  lr: 0.000084  loss: 0.2415 (0.2485)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [100/689]  eta: 0:15:25  lr: 0.000084  loss: 0.2415 (0.2474)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [110/689]  eta: 0:15:09  lr: 0.000084  loss: 0.2292 (0.2465)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [120/689]  eta: 0:14:54  lr: 0.000084  loss: 0.2411 (0.2473)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [130/689]  eta: 0:14:38  lr: 0.000084  loss: 0.2430 (0.2466)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [140/689]  eta: 0:14:23  lr: 0.000084  loss: 0.2379 (0.2462)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [150/689]  eta: 0:14:07  lr: 0.000084  loss: 0.2427 (0.2468)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [160/689]  eta: 0:13:52  lr: 0.000084  loss: 0.2454 (0.2466)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [170/689]  eta: 0:13:36  lr: 0.000084  loss: 0.2450 (0.2468)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [180/689]  eta: 0:13:20  lr: 0.000084  loss: 0.2450 (0.2462)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [190/689]  eta: 0:13:05  lr: 0.000084  loss: 0.2485 (0.2474)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [200/689]  eta: 0:12:49  lr: 0.000084  loss: 0.2439 (0.2471)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [210/689]  eta: 0:12:33  lr: 0.000084  loss: 0.2439 (0.2479)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [220/689]  eta: 0:12:18  lr: 0.000084  loss: 0.2513 (0.2477)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [230/689]  eta: 0:12:02  lr: 0.000084  loss: 0.2324 (0.2480)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [240/689]  eta: 0:11:46  lr: 0.000084  loss: 0.2324 (0.2480)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [250/689]  eta: 0:11:31  lr: 0.000084  loss: 0.2512 (0.2481)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [260/689]  eta: 0:11:15  lr: 0.000084  loss: 0.2327 (0.2475)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [270/689]  eta: 0:10:59  lr: 0.000084  loss: 0.2305 (0.2470)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [280/689]  eta: 0:10:44  lr: 0.000084  loss: 0.2305 (0.2466)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [290/689]  eta: 0:10:28  lr: 0.000084  loss: 0.2252 (0.2461)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [300/689]  eta: 0:10:12  lr: 0.000084  loss: 0.2300 (0.2460)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [310/689]  eta: 0:09:56  lr: 0.000084  loss: 0.2443 (0.2461)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [320/689]  eta: 0:09:41  lr: 0.000084  loss: 0.2501 (0.2464)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [330/689]  eta: 0:09:25  lr: 0.000084  loss: 0.2501 (0.2465)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [340/689]  eta: 0:09:09  lr: 0.000084  loss: 0.2481 (0.2463)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [350/689]  eta: 0:08:53  lr: 0.000084  loss: 0.2371 (0.2462)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [360/689]  eta: 0:08:38  lr: 0.000084  loss: 0.2372 (0.2463)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [370/689]  eta: 0:08:22  lr: 0.000084  loss: 0.2372 (0.2460)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [380/689]  eta: 0:08:06  lr: 0.000084  loss: 0.2378 (0.2458)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [390/689]  eta: 0:07:51  lr: 0.000084  loss: 0.2378 (0.2457)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [400/689]  eta: 0:07:35  lr: 0.000084  loss: 0.2395 (0.2456)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [410/689]  eta: 0:07:19  lr: 0.000084  loss: 0.2416 (0.2456)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [420/689]  eta: 0:07:03  lr: 0.000084  loss: 0.2465 (0.2456)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [430/689]  eta: 0:06:48  lr: 0.000084  loss: 0.2449 (0.2456)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [440/689]  eta: 0:06:32  lr: 0.000084  loss: 0.2449 (0.2457)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [450/689]  eta: 0:06:16  lr: 0.000084  loss: 0.2482 (0.2456)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [460/689]  eta: 0:06:00  lr: 0.000084  loss: 0.2482 (0.2457)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [470/689]  eta: 0:05:45  lr: 0.000084  loss: 0.2347 (0.2454)  time: 1.5770  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:243]  [480/689]  eta: 0:05:29  lr: 0.000084  loss: 0.2297 (0.2452)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [490/689]  eta: 0:05:13  lr: 0.000084  loss: 0.2340 (0.2453)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [500/689]  eta: 0:04:57  lr: 0.000084  loss: 0.2376 (0.2455)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [510/689]  eta: 0:04:42  lr: 0.000084  loss: 0.2361 (0.2454)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [520/689]  eta: 0:04:26  lr: 0.000084  loss: 0.2327 (0.2452)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [530/689]  eta: 0:04:10  lr: 0.000084  loss: 0.2393 (0.2451)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [540/689]  eta: 0:03:54  lr: 0.000084  loss: 0.2335 (0.2451)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [550/689]  eta: 0:03:39  lr: 0.000084  loss: 0.2335 (0.2453)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [560/689]  eta: 0:03:23  lr: 0.000084  loss: 0.2526 (0.2455)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [570/689]  eta: 0:03:07  lr: 0.000084  loss: 0.2526 (0.2456)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [580/689]  eta: 0:02:51  lr: 0.000084  loss: 0.2443 (0.2455)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [590/689]  eta: 0:02:36  lr: 0.000084  loss: 0.2320 (0.2455)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [600/689]  eta: 0:02:20  lr: 0.000084  loss: 0.2341 (0.2456)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [610/689]  eta: 0:02:04  lr: 0.000084  loss: 0.2387 (0.2456)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [620/689]  eta: 0:01:48  lr: 0.000084  loss: 0.2325 (0.2454)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [630/689]  eta: 0:01:32  lr: 0.000084  loss: 0.2317 (0.2456)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [640/689]  eta: 0:01:17  lr: 0.000084  loss: 0.2401 (0.2457)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [650/689]  eta: 0:01:01  lr: 0.000084  loss: 0.2468 (0.2457)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [660/689]  eta: 0:00:45  lr: 0.000084  loss: 0.2530 (0.2460)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [670/689]  eta: 0:00:29  lr: 0.000084  loss: 0.2519 (0.2459)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [680/689]  eta: 0:00:14  lr: 0.000084  loss: 0.2467 (0.2461)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243]  [688/689]  eta: 0:00:01  lr: 0.000084  loss: 0.2271 (0.2458)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:243] Total time: 0:18:06 (1.5763 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: 0.2271 (0.2458)\n",
      "Valid: [epoch:243]  [ 0/14]  eta: 0:00:14  loss: 0.2129 (0.2129)  time: 1.0240  data: 0.3509  max mem: 39763\n",
      "Valid: [epoch:243]  [13/14]  eta: 0:00:00  loss: 0.2290 (0.2316)  time: 0.1151  data: 0.0251  max mem: 39763\n",
      "Valid: [epoch:243] Total time: 0:00:01 (0.1244 s / it)\n",
      "Averaged stats: loss: 0.2290 (0.2316)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_243_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.232%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:244]  [  0/689]  eta: 0:11:44  lr: 0.000084  loss: 0.2049 (0.2049)  time: 1.0223  data: 0.5443  max mem: 39763\n",
      "Train: [epoch:244]  [ 10/689]  eta: 0:17:16  lr: 0.000084  loss: 0.2369 (0.2378)  time: 1.5268  data: 0.0496  max mem: 39763\n",
      "Train: [epoch:244]  [ 20/689]  eta: 0:17:17  lr: 0.000084  loss: 0.2316 (0.2319)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [ 30/689]  eta: 0:17:07  lr: 0.000084  loss: 0.2295 (0.2351)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [ 40/689]  eta: 0:16:55  lr: 0.000084  loss: 0.2347 (0.2379)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [ 50/689]  eta: 0:16:41  lr: 0.000084  loss: 0.2464 (0.2418)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [ 60/689]  eta: 0:16:26  lr: 0.000084  loss: 0.2338 (0.2414)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [ 70/689]  eta: 0:16:11  lr: 0.000084  loss: 0.2413 (0.2420)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [ 80/689]  eta: 0:15:56  lr: 0.000084  loss: 0.2472 (0.2433)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [ 90/689]  eta: 0:15:41  lr: 0.000084  loss: 0.2362 (0.2433)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [100/689]  eta: 0:15:25  lr: 0.000084  loss: 0.2543 (0.2458)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [110/689]  eta: 0:15:10  lr: 0.000084  loss: 0.2667 (0.2474)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [120/689]  eta: 0:14:55  lr: 0.000084  loss: 0.2478 (0.2470)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [130/689]  eta: 0:14:39  lr: 0.000084  loss: 0.2373 (0.2474)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [140/689]  eta: 0:14:23  lr: 0.000084  loss: 0.2494 (0.2466)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [150/689]  eta: 0:14:08  lr: 0.000084  loss: 0.2345 (0.2459)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [160/689]  eta: 0:13:52  lr: 0.000084  loss: 0.2393 (0.2465)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [170/689]  eta: 0:13:37  lr: 0.000084  loss: 0.2560 (0.2467)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [180/689]  eta: 0:13:21  lr: 0.000084  loss: 0.2460 (0.2471)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [190/689]  eta: 0:13:05  lr: 0.000084  loss: 0.2477 (0.2470)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [200/689]  eta: 0:12:50  lr: 0.000084  loss: 0.2457 (0.2477)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [210/689]  eta: 0:12:34  lr: 0.000084  loss: 0.2363 (0.2479)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [220/689]  eta: 0:12:18  lr: 0.000084  loss: 0.2416 (0.2477)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [230/689]  eta: 0:12:03  lr: 0.000084  loss: 0.2353 (0.2472)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [240/689]  eta: 0:11:47  lr: 0.000084  loss: 0.2396 (0.2478)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [250/689]  eta: 0:11:31  lr: 0.000084  loss: 0.2396 (0.2472)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [260/689]  eta: 0:11:16  lr: 0.000084  loss: 0.2289 (0.2470)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [270/689]  eta: 0:11:00  lr: 0.000084  loss: 0.2289 (0.2463)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [280/689]  eta: 0:10:44  lr: 0.000084  loss: 0.2309 (0.2465)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [290/689]  eta: 0:10:28  lr: 0.000084  loss: 0.2569 (0.2473)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [300/689]  eta: 0:10:13  lr: 0.000084  loss: 0.2591 (0.2476)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [310/689]  eta: 0:09:57  lr: 0.000084  loss: 0.2420 (0.2475)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [320/689]  eta: 0:09:41  lr: 0.000084  loss: 0.2308 (0.2473)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [330/689]  eta: 0:09:25  lr: 0.000084  loss: 0.2566 (0.2478)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [340/689]  eta: 0:09:10  lr: 0.000084  loss: 0.2507 (0.2474)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [350/689]  eta: 0:08:54  lr: 0.000084  loss: 0.2259 (0.2470)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [360/689]  eta: 0:08:38  lr: 0.000084  loss: 0.2391 (0.2471)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [370/689]  eta: 0:08:22  lr: 0.000084  loss: 0.2447 (0.2469)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [380/689]  eta: 0:08:07  lr: 0.000084  loss: 0.2400 (0.2472)  time: 1.5781  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:244]  [390/689]  eta: 0:07:51  lr: 0.000084  loss: 0.2400 (0.2470)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [400/689]  eta: 0:07:35  lr: 0.000084  loss: 0.2317 (0.2468)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [410/689]  eta: 0:07:19  lr: 0.000084  loss: 0.2303 (0.2465)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [420/689]  eta: 0:07:04  lr: 0.000084  loss: 0.2220 (0.2464)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [430/689]  eta: 0:06:48  lr: 0.000084  loss: 0.2510 (0.2467)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [440/689]  eta: 0:06:32  lr: 0.000084  loss: 0.2529 (0.2466)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [450/689]  eta: 0:06:16  lr: 0.000084  loss: 0.2296 (0.2462)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [460/689]  eta: 0:06:01  lr: 0.000084  loss: 0.2376 (0.2464)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [470/689]  eta: 0:05:45  lr: 0.000084  loss: 0.2439 (0.2464)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [480/689]  eta: 0:05:29  lr: 0.000084  loss: 0.2439 (0.2465)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [490/689]  eta: 0:05:13  lr: 0.000084  loss: 0.2381 (0.2461)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [500/689]  eta: 0:04:58  lr: 0.000084  loss: 0.2300 (0.2461)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [510/689]  eta: 0:04:42  lr: 0.000084  loss: 0.2311 (0.2460)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [520/689]  eta: 0:04:26  lr: 0.000084  loss: 0.2419 (0.2460)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [530/689]  eta: 0:04:10  lr: 0.000084  loss: 0.2311 (0.2458)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [540/689]  eta: 0:03:54  lr: 0.000084  loss: 0.2268 (0.2458)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [550/689]  eta: 0:03:39  lr: 0.000084  loss: 0.2417 (0.2458)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [560/689]  eta: 0:03:23  lr: 0.000084  loss: 0.2420 (0.2460)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [570/689]  eta: 0:03:07  lr: 0.000084  loss: 0.2392 (0.2461)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [580/689]  eta: 0:02:51  lr: 0.000084  loss: 0.2309 (0.2456)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [590/689]  eta: 0:02:36  lr: 0.000084  loss: 0.2273 (0.2455)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [600/689]  eta: 0:02:20  lr: 0.000084  loss: 0.2294 (0.2455)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [610/689]  eta: 0:02:04  lr: 0.000084  loss: 0.2385 (0.2452)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [620/689]  eta: 0:01:48  lr: 0.000084  loss: 0.2386 (0.2453)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [630/689]  eta: 0:01:33  lr: 0.000084  loss: 0.2391 (0.2456)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [640/689]  eta: 0:01:17  lr: 0.000084  loss: 0.2391 (0.2456)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [650/689]  eta: 0:01:01  lr: 0.000084  loss: 0.2401 (0.2455)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [660/689]  eta: 0:00:45  lr: 0.000084  loss: 0.2421 (0.2456)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [670/689]  eta: 0:00:29  lr: 0.000084  loss: 0.2334 (0.2456)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [680/689]  eta: 0:00:14  lr: 0.000084  loss: 0.2368 (0.2456)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244]  [688/689]  eta: 0:00:01  lr: 0.000084  loss: 0.2561 (0.2458)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:244] Total time: 0:18:06 (1.5772 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: 0.2561 (0.2458)\n",
      "Valid: [epoch:244]  [ 0/14]  eta: 0:00:14  loss: 0.2061 (0.2061)  time: 1.0073  data: 0.3773  max mem: 39763\n",
      "Valid: [epoch:244]  [13/14]  eta: 0:00:00  loss: 0.2303 (0.2328)  time: 0.1139  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:244] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.2303 (0.2328)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_244_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.233%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:245]  [  0/689]  eta: 0:11:34  lr: 0.000084  loss: 0.2796 (0.2796)  time: 1.0086  data: 0.5325  max mem: 39763\n",
      "Train: [epoch:245]  [ 10/689]  eta: 0:17:14  lr: 0.000084  loss: 0.2492 (0.2507)  time: 1.5231  data: 0.0485  max mem: 39763\n",
      "Train: [epoch:245]  [ 20/689]  eta: 0:17:15  lr: 0.000084  loss: 0.2397 (0.2458)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [ 30/689]  eta: 0:17:06  lr: 0.000084  loss: 0.2346 (0.2438)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [ 40/689]  eta: 0:16:53  lr: 0.000084  loss: 0.2430 (0.2433)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [ 50/689]  eta: 0:16:39  lr: 0.000084  loss: 0.2276 (0.2426)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [ 60/689]  eta: 0:16:25  lr: 0.000084  loss: 0.2241 (0.2402)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [ 70/689]  eta: 0:16:10  lr: 0.000084  loss: 0.2334 (0.2417)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [ 80/689]  eta: 0:15:55  lr: 0.000084  loss: 0.2459 (0.2441)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [ 90/689]  eta: 0:15:40  lr: 0.000084  loss: 0.2506 (0.2441)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [100/689]  eta: 0:15:25  lr: 0.000084  loss: 0.2436 (0.2441)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [110/689]  eta: 0:15:10  lr: 0.000084  loss: 0.2482 (0.2454)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [120/689]  eta: 0:14:54  lr: 0.000084  loss: 0.2503 (0.2454)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [130/689]  eta: 0:14:39  lr: 0.000084  loss: 0.2508 (0.2465)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [140/689]  eta: 0:14:23  lr: 0.000084  loss: 0.2532 (0.2473)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [150/689]  eta: 0:14:08  lr: 0.000084  loss: 0.2570 (0.2489)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [160/689]  eta: 0:13:52  lr: 0.000084  loss: 0.2495 (0.2499)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [170/689]  eta: 0:13:37  lr: 0.000084  loss: 0.2452 (0.2494)  time: 1.5790  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [180/689]  eta: 0:13:21  lr: 0.000084  loss: 0.2350 (0.2487)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [190/689]  eta: 0:13:05  lr: 0.000084  loss: 0.2300 (0.2485)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [200/689]  eta: 0:12:50  lr: 0.000084  loss: 0.2394 (0.2484)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [210/689]  eta: 0:12:34  lr: 0.000084  loss: 0.2443 (0.2488)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [220/689]  eta: 0:12:18  lr: 0.000084  loss: 0.2443 (0.2488)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [230/689]  eta: 0:12:03  lr: 0.000084  loss: 0.2407 (0.2490)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [240/689]  eta: 0:11:47  lr: 0.000084  loss: 0.2427 (0.2494)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [250/689]  eta: 0:11:31  lr: 0.000084  loss: 0.2342 (0.2491)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [260/689]  eta: 0:11:16  lr: 0.000084  loss: 0.2286 (0.2486)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [270/689]  eta: 0:11:00  lr: 0.000084  loss: 0.2312 (0.2483)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [280/689]  eta: 0:10:44  lr: 0.000084  loss: 0.2336 (0.2483)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [290/689]  eta: 0:10:28  lr: 0.000084  loss: 0.2368 (0.2479)  time: 1.5792  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:245]  [300/689]  eta: 0:10:13  lr: 0.000084  loss: 0.2348 (0.2478)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [310/689]  eta: 0:09:57  lr: 0.000084  loss: 0.2310 (0.2474)  time: 1.5789  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [320/689]  eta: 0:09:41  lr: 0.000084  loss: 0.2345 (0.2473)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [330/689]  eta: 0:09:25  lr: 0.000084  loss: 0.2408 (0.2473)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [340/689]  eta: 0:09:10  lr: 0.000084  loss: 0.2421 (0.2470)  time: 1.5815  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [350/689]  eta: 0:08:54  lr: 0.000084  loss: 0.2482 (0.2473)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [360/689]  eta: 0:08:38  lr: 0.000084  loss: 0.2511 (0.2475)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [370/689]  eta: 0:08:23  lr: 0.000084  loss: 0.2340 (0.2472)  time: 1.5806  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [380/689]  eta: 0:08:07  lr: 0.000084  loss: 0.2350 (0.2470)  time: 1.5811  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [390/689]  eta: 0:07:51  lr: 0.000084  loss: 0.2462 (0.2474)  time: 1.5812  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [400/689]  eta: 0:07:35  lr: 0.000084  loss: 0.2466 (0.2472)  time: 1.5805  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [410/689]  eta: 0:07:20  lr: 0.000084  loss: 0.2379 (0.2472)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [420/689]  eta: 0:07:04  lr: 0.000084  loss: 0.2332 (0.2471)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [430/689]  eta: 0:06:48  lr: 0.000084  loss: 0.2555 (0.2477)  time: 1.5810  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [440/689]  eta: 0:06:32  lr: 0.000084  loss: 0.2511 (0.2474)  time: 1.5813  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [450/689]  eta: 0:06:17  lr: 0.000084  loss: 0.2250 (0.2473)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [460/689]  eta: 0:06:01  lr: 0.000084  loss: 0.2392 (0.2473)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [470/689]  eta: 0:05:45  lr: 0.000084  loss: 0.2444 (0.2472)  time: 1.5804  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [480/689]  eta: 0:05:29  lr: 0.000084  loss: 0.2307 (0.2469)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [490/689]  eta: 0:05:14  lr: 0.000084  loss: 0.2383 (0.2469)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [500/689]  eta: 0:04:58  lr: 0.000084  loss: 0.2479 (0.2470)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [510/689]  eta: 0:04:42  lr: 0.000084  loss: 0.2256 (0.2467)  time: 1.5796  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [520/689]  eta: 0:04:26  lr: 0.000084  loss: 0.2281 (0.2466)  time: 1.5802  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [530/689]  eta: 0:04:10  lr: 0.000084  loss: 0.2308 (0.2465)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [540/689]  eta: 0:03:55  lr: 0.000084  loss: 0.2401 (0.2465)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [550/689]  eta: 0:03:39  lr: 0.000084  loss: 0.2454 (0.2467)  time: 1.5800  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [560/689]  eta: 0:03:23  lr: 0.000084  loss: 0.2528 (0.2469)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [570/689]  eta: 0:03:07  lr: 0.000084  loss: 0.2387 (0.2468)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [580/689]  eta: 0:02:52  lr: 0.000084  loss: 0.2329 (0.2467)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [590/689]  eta: 0:02:36  lr: 0.000084  loss: 0.2455 (0.2467)  time: 1.5795  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [600/689]  eta: 0:02:20  lr: 0.000084  loss: 0.2379 (0.2469)  time: 1.5801  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [610/689]  eta: 0:02:04  lr: 0.000084  loss: 0.2354 (0.2469)  time: 1.5803  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [620/689]  eta: 0:01:48  lr: 0.000084  loss: 0.2475 (0.2470)  time: 1.5797  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [630/689]  eta: 0:01:33  lr: 0.000084  loss: 0.2475 (0.2471)  time: 1.5794  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [640/689]  eta: 0:01:17  lr: 0.000084  loss: 0.2440 (0.2473)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [650/689]  eta: 0:01:01  lr: 0.000084  loss: 0.2434 (0.2471)  time: 1.5793  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [660/689]  eta: 0:00:45  lr: 0.000084  loss: 0.2434 (0.2472)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [670/689]  eta: 0:00:29  lr: 0.000084  loss: 0.2412 (0.2472)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [680/689]  eta: 0:00:14  lr: 0.000084  loss: 0.2396 (0.2474)  time: 1.5791  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245]  [688/689]  eta: 0:00:01  lr: 0.000084  loss: 0.2365 (0.2472)  time: 1.5792  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:245] Total time: 0:18:07 (1.5786 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: 0.2365 (0.2472)\n",
      "Valid: [epoch:245]  [ 0/14]  eta: 0:00:14  loss: 0.2542 (0.2542)  time: 1.0168  data: 0.4033  max mem: 39763\n",
      "Valid: [epoch:245]  [13/14]  eta: 0:00:00  loss: 0.2307 (0.2328)  time: 0.1146  data: 0.0289  max mem: 39763\n",
      "Valid: [epoch:245] Total time: 0:00:01 (0.1240 s / it)\n",
      "Averaged stats: loss: 0.2307 (0.2328)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_245_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.233%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:246]  [  0/689]  eta: 0:12:14  lr: 0.000084  loss: 0.2652 (0.2652)  time: 1.0667  data: 0.5887  max mem: 39763\n",
      "Train: [epoch:246]  [ 10/689]  eta: 0:17:17  lr: 0.000084  loss: 0.2410 (0.2493)  time: 1.5283  data: 0.0536  max mem: 39763\n",
      "Train: [epoch:246]  [ 20/689]  eta: 0:17:17  lr: 0.000084  loss: 0.2400 (0.2453)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [ 30/689]  eta: 0:17:07  lr: 0.000084  loss: 0.2351 (0.2442)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [ 40/689]  eta: 0:16:54  lr: 0.000084  loss: 0.2399 (0.2469)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [ 50/689]  eta: 0:16:40  lr: 0.000084  loss: 0.2466 (0.2483)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [ 60/689]  eta: 0:16:25  lr: 0.000084  loss: 0.2466 (0.2520)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [ 70/689]  eta: 0:16:11  lr: 0.000084  loss: 0.2598 (0.2526)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [ 80/689]  eta: 0:15:56  lr: 0.000084  loss: 0.2587 (0.2532)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [ 90/689]  eta: 0:15:40  lr: 0.000084  loss: 0.2556 (0.2533)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [100/689]  eta: 0:15:25  lr: 0.000084  loss: 0.2518 (0.2541)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [110/689]  eta: 0:15:10  lr: 0.000084  loss: 0.2503 (0.2531)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [120/689]  eta: 0:14:54  lr: 0.000084  loss: 0.2496 (0.2530)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [130/689]  eta: 0:14:39  lr: 0.000084  loss: 0.2393 (0.2521)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [140/689]  eta: 0:14:23  lr: 0.000084  loss: 0.2255 (0.2511)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [150/689]  eta: 0:14:07  lr: 0.000084  loss: 0.2383 (0.2514)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [160/689]  eta: 0:13:52  lr: 0.000084  loss: 0.2444 (0.2513)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [170/689]  eta: 0:13:36  lr: 0.000084  loss: 0.2463 (0.2512)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [180/689]  eta: 0:13:21  lr: 0.000084  loss: 0.2463 (0.2507)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [190/689]  eta: 0:13:05  lr: 0.000084  loss: 0.2447 (0.2508)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [200/689]  eta: 0:12:49  lr: 0.000084  loss: 0.2419 (0.2507)  time: 1.5776  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:246]  [210/689]  eta: 0:12:34  lr: 0.000084  loss: 0.2394 (0.2501)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [220/689]  eta: 0:12:18  lr: 0.000084  loss: 0.2394 (0.2499)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [230/689]  eta: 0:12:02  lr: 0.000084  loss: 0.2481 (0.2500)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [240/689]  eta: 0:11:47  lr: 0.000084  loss: 0.2438 (0.2500)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [250/689]  eta: 0:11:31  lr: 0.000084  loss: 0.2443 (0.2500)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [260/689]  eta: 0:11:15  lr: 0.000084  loss: 0.2443 (0.2497)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [270/689]  eta: 0:11:00  lr: 0.000084  loss: 0.2404 (0.2497)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [280/689]  eta: 0:10:44  lr: 0.000084  loss: 0.2303 (0.2495)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [290/689]  eta: 0:10:28  lr: 0.000084  loss: 0.2325 (0.2491)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [300/689]  eta: 0:10:12  lr: 0.000084  loss: 0.2303 (0.2489)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [310/689]  eta: 0:09:57  lr: 0.000084  loss: 0.2600 (0.2492)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [320/689]  eta: 0:09:41  lr: 0.000084  loss: 0.2434 (0.2490)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [330/689]  eta: 0:09:25  lr: 0.000084  loss: 0.2345 (0.2485)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [340/689]  eta: 0:09:09  lr: 0.000084  loss: 0.2291 (0.2481)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [350/689]  eta: 0:08:54  lr: 0.000084  loss: 0.2371 (0.2482)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [360/689]  eta: 0:08:38  lr: 0.000084  loss: 0.2436 (0.2484)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [370/689]  eta: 0:08:22  lr: 0.000084  loss: 0.2590 (0.2487)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [380/689]  eta: 0:08:06  lr: 0.000084  loss: 0.2590 (0.2486)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [390/689]  eta: 0:07:51  lr: 0.000084  loss: 0.2486 (0.2487)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [400/689]  eta: 0:07:35  lr: 0.000084  loss: 0.2474 (0.2490)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [410/689]  eta: 0:07:19  lr: 0.000084  loss: 0.2463 (0.2490)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [420/689]  eta: 0:07:03  lr: 0.000084  loss: 0.2438 (0.2487)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [430/689]  eta: 0:06:48  lr: 0.000084  loss: 0.2444 (0.2491)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [440/689]  eta: 0:06:32  lr: 0.000084  loss: 0.2517 (0.2493)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [450/689]  eta: 0:06:16  lr: 0.000084  loss: 0.2418 (0.2493)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [460/689]  eta: 0:06:00  lr: 0.000084  loss: 0.2478 (0.2495)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [470/689]  eta: 0:05:45  lr: 0.000084  loss: 0.2477 (0.2494)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [480/689]  eta: 0:05:29  lr: 0.000084  loss: 0.2387 (0.2495)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [490/689]  eta: 0:05:13  lr: 0.000084  loss: 0.2426 (0.2496)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [500/689]  eta: 0:04:57  lr: 0.000084  loss: 0.2496 (0.2497)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [510/689]  eta: 0:04:42  lr: 0.000084  loss: 0.2430 (0.2498)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [520/689]  eta: 0:04:26  lr: 0.000084  loss: 0.2369 (0.2498)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [530/689]  eta: 0:04:10  lr: 0.000084  loss: 0.2357 (0.2495)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [540/689]  eta: 0:03:54  lr: 0.000084  loss: 0.2416 (0.2495)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [550/689]  eta: 0:03:39  lr: 0.000084  loss: 0.2450 (0.2495)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [560/689]  eta: 0:03:23  lr: 0.000084  loss: 0.2390 (0.2494)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [570/689]  eta: 0:03:07  lr: 0.000084  loss: 0.2326 (0.2496)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [580/689]  eta: 0:02:51  lr: 0.000084  loss: 0.2449 (0.2496)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [590/689]  eta: 0:02:36  lr: 0.000084  loss: 0.2476 (0.2499)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [600/689]  eta: 0:02:20  lr: 0.000084  loss: 0.2476 (0.2497)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [610/689]  eta: 0:02:04  lr: 0.000084  loss: 0.2416 (0.2496)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [620/689]  eta: 0:01:48  lr: 0.000084  loss: 0.2435 (0.2496)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [630/689]  eta: 0:01:32  lr: 0.000084  loss: 0.2379 (0.2495)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [640/689]  eta: 0:01:17  lr: 0.000084  loss: 0.2423 (0.2496)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [650/689]  eta: 0:01:01  lr: 0.000084  loss: 0.2469 (0.2496)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [660/689]  eta: 0:00:45  lr: 0.000084  loss: 0.2461 (0.2495)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [670/689]  eta: 0:00:29  lr: 0.000084  loss: 0.2456 (0.2496)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [680/689]  eta: 0:00:14  lr: 0.000084  loss: 0.2412 (0.2495)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246]  [688/689]  eta: 0:00:01  lr: 0.000084  loss: 0.2514 (0.2496)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:246] Total time: 0:18:06 (1.5763 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: 0.2514 (0.2496)\n",
      "Valid: [epoch:246]  [ 0/14]  eta: 0:00:14  loss: 0.2451 (0.2451)  time: 1.0390  data: 0.3802  max mem: 39763\n",
      "Valid: [epoch:246]  [13/14]  eta: 0:00:00  loss: 0.2332 (0.2350)  time: 0.1161  data: 0.0272  max mem: 39763\n",
      "Valid: [epoch:246] Total time: 0:00:01 (0.1249 s / it)\n",
      "Averaged stats: loss: 0.2332 (0.2350)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_246_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.235%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:247]  [  0/689]  eta: 0:12:17  lr: 0.000084  loss: 0.2238 (0.2238)  time: 1.0708  data: 0.5946  max mem: 39763\n",
      "Train: [epoch:247]  [ 10/689]  eta: 0:17:16  lr: 0.000084  loss: 0.2514 (0.2669)  time: 1.5265  data: 0.0541  max mem: 39763\n",
      "Train: [epoch:247]  [ 20/689]  eta: 0:17:16  lr: 0.000084  loss: 0.2469 (0.2590)  time: 1.5727  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [ 30/689]  eta: 0:17:06  lr: 0.000084  loss: 0.2397 (0.2552)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [ 40/689]  eta: 0:16:53  lr: 0.000084  loss: 0.2386 (0.2515)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [ 50/689]  eta: 0:16:39  lr: 0.000084  loss: 0.2384 (0.2499)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [ 60/689]  eta: 0:16:25  lr: 0.000084  loss: 0.2370 (0.2476)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [ 70/689]  eta: 0:16:10  lr: 0.000084  loss: 0.2375 (0.2478)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [ 80/689]  eta: 0:15:55  lr: 0.000084  loss: 0.2375 (0.2465)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [ 90/689]  eta: 0:15:40  lr: 0.000084  loss: 0.2377 (0.2455)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [100/689]  eta: 0:15:24  lr: 0.000084  loss: 0.2478 (0.2489)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [110/689]  eta: 0:15:09  lr: 0.000084  loss: 0.2509 (0.2498)  time: 1.5769  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:247]  [120/689]  eta: 0:14:54  lr: 0.000084  loss: 0.2385 (0.2492)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [130/689]  eta: 0:14:38  lr: 0.000084  loss: 0.2405 (0.2493)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [140/689]  eta: 0:14:23  lr: 0.000084  loss: 0.2537 (0.2496)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [150/689]  eta: 0:14:07  lr: 0.000084  loss: 0.2489 (0.2501)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [160/689]  eta: 0:13:51  lr: 0.000084  loss: 0.2501 (0.2499)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [170/689]  eta: 0:13:36  lr: 0.000084  loss: 0.2444 (0.2496)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [180/689]  eta: 0:13:20  lr: 0.000084  loss: 0.2360 (0.2492)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [190/689]  eta: 0:13:05  lr: 0.000084  loss: 0.2412 (0.2492)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [200/689]  eta: 0:12:49  lr: 0.000084  loss: 0.2645 (0.2496)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [210/689]  eta: 0:12:33  lr: 0.000084  loss: 0.2446 (0.2495)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [220/689]  eta: 0:12:18  lr: 0.000084  loss: 0.2429 (0.2497)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [230/689]  eta: 0:12:02  lr: 0.000084  loss: 0.2428 (0.2498)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [240/689]  eta: 0:11:46  lr: 0.000084  loss: 0.2408 (0.2494)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [250/689]  eta: 0:11:31  lr: 0.000084  loss: 0.2408 (0.2492)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [260/689]  eta: 0:11:15  lr: 0.000084  loss: 0.2368 (0.2491)  time: 1.5785  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [270/689]  eta: 0:10:59  lr: 0.000084  loss: 0.2362 (0.2489)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [280/689]  eta: 0:10:44  lr: 0.000084  loss: 0.2348 (0.2486)  time: 1.5786  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [290/689]  eta: 0:10:28  lr: 0.000084  loss: 0.2304 (0.2482)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [300/689]  eta: 0:10:12  lr: 0.000084  loss: 0.2313 (0.2482)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [310/689]  eta: 0:09:57  lr: 0.000084  loss: 0.2359 (0.2478)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [320/689]  eta: 0:09:41  lr: 0.000084  loss: 0.2386 (0.2483)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [330/689]  eta: 0:09:25  lr: 0.000084  loss: 0.2499 (0.2489)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [340/689]  eta: 0:09:09  lr: 0.000084  loss: 0.2499 (0.2490)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [350/689]  eta: 0:08:54  lr: 0.000084  loss: 0.2409 (0.2490)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [360/689]  eta: 0:08:38  lr: 0.000084  loss: 0.2367 (0.2488)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [370/689]  eta: 0:08:22  lr: 0.000084  loss: 0.2347 (0.2486)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [380/689]  eta: 0:08:06  lr: 0.000084  loss: 0.2418 (0.2485)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [390/689]  eta: 0:07:51  lr: 0.000084  loss: 0.2374 (0.2484)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [400/689]  eta: 0:07:35  lr: 0.000084  loss: 0.2448 (0.2491)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [410/689]  eta: 0:07:19  lr: 0.000084  loss: 0.2493 (0.2492)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [420/689]  eta: 0:07:03  lr: 0.000084  loss: 0.2416 (0.2493)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [430/689]  eta: 0:06:48  lr: 0.000084  loss: 0.2468 (0.2495)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [440/689]  eta: 0:06:32  lr: 0.000084  loss: 0.2501 (0.2496)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [450/689]  eta: 0:06:16  lr: 0.000084  loss: 0.2463 (0.2496)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [460/689]  eta: 0:06:00  lr: 0.000084  loss: 0.2569 (0.2499)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [470/689]  eta: 0:05:45  lr: 0.000084  loss: 0.2559 (0.2498)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [480/689]  eta: 0:05:29  lr: 0.000084  loss: 0.2412 (0.2499)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [490/689]  eta: 0:05:13  lr: 0.000084  loss: 0.2405 (0.2499)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [500/689]  eta: 0:04:57  lr: 0.000084  loss: 0.2345 (0.2496)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [510/689]  eta: 0:04:42  lr: 0.000084  loss: 0.2337 (0.2494)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [520/689]  eta: 0:04:26  lr: 0.000084  loss: 0.2446 (0.2496)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [530/689]  eta: 0:04:10  lr: 0.000084  loss: 0.2486 (0.2495)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [540/689]  eta: 0:03:54  lr: 0.000084  loss: 0.2486 (0.2495)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [550/689]  eta: 0:03:39  lr: 0.000084  loss: 0.2333 (0.2491)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [560/689]  eta: 0:03:23  lr: 0.000084  loss: 0.2278 (0.2490)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [570/689]  eta: 0:03:07  lr: 0.000084  loss: 0.2352 (0.2487)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [580/689]  eta: 0:02:51  lr: 0.000084  loss: 0.2459 (0.2487)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [590/689]  eta: 0:02:36  lr: 0.000084  loss: 0.2531 (0.2487)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [600/689]  eta: 0:02:20  lr: 0.000084  loss: 0.2447 (0.2487)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [610/689]  eta: 0:02:04  lr: 0.000084  loss: 0.2439 (0.2488)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [620/689]  eta: 0:01:48  lr: 0.000084  loss: 0.2439 (0.2490)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [630/689]  eta: 0:01:33  lr: 0.000084  loss: 0.2374 (0.2489)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [640/689]  eta: 0:01:17  lr: 0.000084  loss: 0.2439 (0.2489)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [650/689]  eta: 0:01:01  lr: 0.000084  loss: 0.2585 (0.2491)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [660/689]  eta: 0:00:45  lr: 0.000084  loss: 0.2605 (0.2494)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [670/689]  eta: 0:00:29  lr: 0.000084  loss: 0.2394 (0.2496)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [680/689]  eta: 0:00:14  lr: 0.000084  loss: 0.2378 (0.2494)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247]  [688/689]  eta: 0:00:01  lr: 0.000084  loss: 0.2343 (0.2493)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:247] Total time: 0:18:06 (1.5765 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: 0.2343 (0.2493)\n",
      "Valid: [epoch:247]  [ 0/14]  eta: 0:00:14  loss: 0.2325 (0.2325)  time: 1.0129  data: 0.3661  max mem: 39763\n",
      "Valid: [epoch:247]  [13/14]  eta: 0:00:00  loss: 0.2325 (0.2357)  time: 0.1142  data: 0.0262  max mem: 39763\n",
      "Valid: [epoch:247] Total time: 0:00:01 (0.1229 s / it)\n",
      "Averaged stats: loss: 0.2325 (0.2357)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_247_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.236%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:248]  [  0/689]  eta: 0:11:41  lr: 0.000084  loss: 0.2451 (0.2451)  time: 1.0181  data: 0.5398  max mem: 39763\n",
      "Train: [epoch:248]  [ 10/689]  eta: 0:17:15  lr: 0.000084  loss: 0.2430 (0.2521)  time: 1.5251  data: 0.0491  max mem: 39763\n",
      "Train: [epoch:248]  [ 20/689]  eta: 0:17:16  lr: 0.000084  loss: 0.2510 (0.2547)  time: 1.5759  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:248]  [ 30/689]  eta: 0:17:06  lr: 0.000084  loss: 0.2568 (0.2575)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [ 40/689]  eta: 0:16:53  lr: 0.000084  loss: 0.2382 (0.2506)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [ 50/689]  eta: 0:16:39  lr: 0.000084  loss: 0.2288 (0.2479)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [ 60/689]  eta: 0:16:25  lr: 0.000084  loss: 0.2352 (0.2481)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [ 70/689]  eta: 0:16:10  lr: 0.000084  loss: 0.2352 (0.2468)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [ 80/689]  eta: 0:15:55  lr: 0.000084  loss: 0.2311 (0.2448)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [ 90/689]  eta: 0:15:40  lr: 0.000084  loss: 0.2339 (0.2469)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [100/689]  eta: 0:15:25  lr: 0.000084  loss: 0.2579 (0.2484)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [110/689]  eta: 0:15:09  lr: 0.000084  loss: 0.2472 (0.2486)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [120/689]  eta: 0:14:54  lr: 0.000084  loss: 0.2447 (0.2485)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [130/689]  eta: 0:14:38  lr: 0.000084  loss: 0.2374 (0.2484)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [140/689]  eta: 0:14:23  lr: 0.000084  loss: 0.2330 (0.2483)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [150/689]  eta: 0:14:07  lr: 0.000084  loss: 0.2433 (0.2497)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [160/689]  eta: 0:13:51  lr: 0.000084  loss: 0.2456 (0.2497)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [170/689]  eta: 0:13:36  lr: 0.000084  loss: 0.2432 (0.2498)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [180/689]  eta: 0:13:20  lr: 0.000084  loss: 0.2426 (0.2500)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [190/689]  eta: 0:13:05  lr: 0.000084  loss: 0.2439 (0.2507)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [200/689]  eta: 0:12:49  lr: 0.000084  loss: 0.2486 (0.2511)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [210/689]  eta: 0:12:33  lr: 0.000084  loss: 0.2232 (0.2500)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [220/689]  eta: 0:12:18  lr: 0.000084  loss: 0.2297 (0.2495)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [230/689]  eta: 0:12:02  lr: 0.000084  loss: 0.2384 (0.2492)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [240/689]  eta: 0:11:46  lr: 0.000084  loss: 0.2321 (0.2489)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [250/689]  eta: 0:11:31  lr: 0.000084  loss: 0.2389 (0.2492)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [260/689]  eta: 0:11:15  lr: 0.000084  loss: 0.2498 (0.2495)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [270/689]  eta: 0:10:59  lr: 0.000084  loss: 0.2383 (0.2493)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [280/689]  eta: 0:10:44  lr: 0.000084  loss: 0.2383 (0.2494)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [290/689]  eta: 0:10:28  lr: 0.000084  loss: 0.2465 (0.2497)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [300/689]  eta: 0:10:12  lr: 0.000084  loss: 0.2457 (0.2497)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [310/689]  eta: 0:09:56  lr: 0.000084  loss: 0.2331 (0.2492)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [320/689]  eta: 0:09:41  lr: 0.000084  loss: 0.2309 (0.2489)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [330/689]  eta: 0:09:25  lr: 0.000084  loss: 0.2557 (0.2494)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [340/689]  eta: 0:09:09  lr: 0.000084  loss: 0.2449 (0.2490)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [350/689]  eta: 0:08:53  lr: 0.000084  loss: 0.2360 (0.2491)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [360/689]  eta: 0:08:38  lr: 0.000084  loss: 0.2410 (0.2492)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [370/689]  eta: 0:08:22  lr: 0.000084  loss: 0.2400 (0.2490)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [380/689]  eta: 0:08:06  lr: 0.000084  loss: 0.2342 (0.2484)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [390/689]  eta: 0:07:51  lr: 0.000084  loss: 0.2322 (0.2485)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [400/689]  eta: 0:07:35  lr: 0.000084  loss: 0.2500 (0.2493)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [410/689]  eta: 0:07:19  lr: 0.000084  loss: 0.2456 (0.2490)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [420/689]  eta: 0:07:03  lr: 0.000084  loss: 0.2278 (0.2487)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [430/689]  eta: 0:06:48  lr: 0.000084  loss: 0.2374 (0.2488)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [440/689]  eta: 0:06:32  lr: 0.000084  loss: 0.2496 (0.2490)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [450/689]  eta: 0:06:16  lr: 0.000084  loss: 0.2487 (0.2489)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [460/689]  eta: 0:06:00  lr: 0.000084  loss: 0.2436 (0.2493)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [470/689]  eta: 0:05:45  lr: 0.000084  loss: 0.2567 (0.2494)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [480/689]  eta: 0:05:29  lr: 0.000084  loss: 0.2486 (0.2493)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [490/689]  eta: 0:05:13  lr: 0.000084  loss: 0.2410 (0.2491)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [500/689]  eta: 0:04:57  lr: 0.000084  loss: 0.2424 (0.2491)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [510/689]  eta: 0:04:42  lr: 0.000084  loss: 0.2502 (0.2491)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [520/689]  eta: 0:04:26  lr: 0.000084  loss: 0.2471 (0.2491)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [530/689]  eta: 0:04:10  lr: 0.000084  loss: 0.2471 (0.2491)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [540/689]  eta: 0:03:54  lr: 0.000084  loss: 0.2437 (0.2489)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [550/689]  eta: 0:03:39  lr: 0.000084  loss: 0.2437 (0.2489)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [560/689]  eta: 0:03:23  lr: 0.000084  loss: 0.2489 (0.2491)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [570/689]  eta: 0:03:07  lr: 0.000084  loss: 0.2491 (0.2493)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [580/689]  eta: 0:02:51  lr: 0.000084  loss: 0.2427 (0.2494)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [590/689]  eta: 0:02:36  lr: 0.000084  loss: 0.2335 (0.2493)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [600/689]  eta: 0:02:20  lr: 0.000084  loss: 0.2417 (0.2494)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [610/689]  eta: 0:02:04  lr: 0.000084  loss: 0.2382 (0.2492)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [620/689]  eta: 0:01:48  lr: 0.000084  loss: 0.2385 (0.2495)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [630/689]  eta: 0:01:32  lr: 0.000084  loss: 0.2450 (0.2493)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [640/689]  eta: 0:01:17  lr: 0.000084  loss: 0.2450 (0.2495)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [650/689]  eta: 0:01:01  lr: 0.000084  loss: 0.2524 (0.2496)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [660/689]  eta: 0:00:45  lr: 0.000084  loss: 0.2563 (0.2497)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [670/689]  eta: 0:00:29  lr: 0.000084  loss: 0.2518 (0.2497)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248]  [680/689]  eta: 0:00:14  lr: 0.000084  loss: 0.2515 (0.2497)  time: 1.5762  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:248]  [688/689]  eta: 0:00:01  lr: 0.000084  loss: 0.2452 (0.2496)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:248] Total time: 0:18:05 (1.5762 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: 0.2452 (0.2496)\n",
      "Valid: [epoch:248]  [ 0/14]  eta: 0:00:14  loss: 0.2322 (0.2322)  time: 1.0050  data: 0.3474  max mem: 39763\n",
      "Valid: [epoch:248]  [13/14]  eta: 0:00:00  loss: 0.2340 (0.2354)  time: 0.1137  data: 0.0249  max mem: 39763\n",
      "Valid: [epoch:248] Total time: 0:00:01 (0.1225 s / it)\n",
      "Averaged stats: loss: 0.2340 (0.2354)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_248_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.235%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:249]  [  0/689]  eta: 0:12:30  lr: 0.000084  loss: 0.2631 (0.2631)  time: 1.0886  data: 0.6115  max mem: 39763\n",
      "Train: [epoch:249]  [ 10/689]  eta: 0:17:17  lr: 0.000084  loss: 0.2523 (0.2599)  time: 1.5282  data: 0.0557  max mem: 39763\n",
      "Train: [epoch:249]  [ 20/689]  eta: 0:17:17  lr: 0.000084  loss: 0.2519 (0.2564)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [ 30/689]  eta: 0:17:07  lr: 0.000084  loss: 0.2473 (0.2616)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [ 40/689]  eta: 0:16:54  lr: 0.000084  loss: 0.2406 (0.2557)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [ 50/689]  eta: 0:16:40  lr: 0.000084  loss: 0.2340 (0.2540)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [ 60/689]  eta: 0:16:25  lr: 0.000084  loss: 0.2509 (0.2556)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [ 70/689]  eta: 0:16:10  lr: 0.000084  loss: 0.2549 (0.2561)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [ 80/689]  eta: 0:15:55  lr: 0.000084  loss: 0.2423 (0.2545)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [ 90/689]  eta: 0:15:40  lr: 0.000084  loss: 0.2423 (0.2529)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [100/689]  eta: 0:15:24  lr: 0.000084  loss: 0.2453 (0.2538)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [110/689]  eta: 0:15:09  lr: 0.000084  loss: 0.2511 (0.2538)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [120/689]  eta: 0:14:54  lr: 0.000084  loss: 0.2522 (0.2543)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [130/689]  eta: 0:14:38  lr: 0.000084  loss: 0.2522 (0.2543)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [140/689]  eta: 0:14:22  lr: 0.000084  loss: 0.2440 (0.2537)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [150/689]  eta: 0:14:07  lr: 0.000084  loss: 0.2490 (0.2539)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [160/689]  eta: 0:13:51  lr: 0.000084  loss: 0.2354 (0.2528)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [170/689]  eta: 0:13:36  lr: 0.000084  loss: 0.2438 (0.2541)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [180/689]  eta: 0:13:20  lr: 0.000084  loss: 0.2494 (0.2536)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [190/689]  eta: 0:13:04  lr: 0.000084  loss: 0.2389 (0.2534)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [200/689]  eta: 0:12:49  lr: 0.000084  loss: 0.2340 (0.2529)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [210/689]  eta: 0:12:33  lr: 0.000084  loss: 0.2316 (0.2521)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [220/689]  eta: 0:12:17  lr: 0.000084  loss: 0.2363 (0.2514)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [230/689]  eta: 0:12:02  lr: 0.000084  loss: 0.2441 (0.2514)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [240/689]  eta: 0:11:46  lr: 0.000084  loss: 0.2499 (0.2515)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [250/689]  eta: 0:11:30  lr: 0.000084  loss: 0.2496 (0.2520)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [260/689]  eta: 0:11:15  lr: 0.000084  loss: 0.2591 (0.2521)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [270/689]  eta: 0:10:59  lr: 0.000084  loss: 0.2550 (0.2521)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [280/689]  eta: 0:10:43  lr: 0.000084  loss: 0.2501 (0.2521)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [290/689]  eta: 0:10:27  lr: 0.000084  loss: 0.2470 (0.2521)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [300/689]  eta: 0:10:12  lr: 0.000084  loss: 0.2393 (0.2518)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [310/689]  eta: 0:09:56  lr: 0.000084  loss: 0.2362 (0.2513)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [320/689]  eta: 0:09:40  lr: 0.000084  loss: 0.2341 (0.2515)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [330/689]  eta: 0:09:25  lr: 0.000084  loss: 0.2301 (0.2509)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [340/689]  eta: 0:09:09  lr: 0.000084  loss: 0.2330 (0.2507)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [350/689]  eta: 0:08:53  lr: 0.000084  loss: 0.2439 (0.2505)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [360/689]  eta: 0:08:37  lr: 0.000084  loss: 0.2472 (0.2506)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [370/689]  eta: 0:08:22  lr: 0.000084  loss: 0.2474 (0.2505)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [380/689]  eta: 0:08:06  lr: 0.000084  loss: 0.2452 (0.2505)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [390/689]  eta: 0:07:50  lr: 0.000084  loss: 0.2426 (0.2508)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [400/689]  eta: 0:07:35  lr: 0.000084  loss: 0.2426 (0.2509)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [410/689]  eta: 0:07:19  lr: 0.000084  loss: 0.2547 (0.2513)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [420/689]  eta: 0:07:03  lr: 0.000084  loss: 0.2431 (0.2510)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [430/689]  eta: 0:06:47  lr: 0.000084  loss: 0.2431 (0.2512)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [440/689]  eta: 0:06:32  lr: 0.000084  loss: 0.2537 (0.2514)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [450/689]  eta: 0:06:16  lr: 0.000084  loss: 0.2364 (0.2509)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [460/689]  eta: 0:06:00  lr: 0.000084  loss: 0.2442 (0.2512)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [470/689]  eta: 0:05:44  lr: 0.000084  loss: 0.2538 (0.2510)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [480/689]  eta: 0:05:29  lr: 0.000084  loss: 0.2363 (0.2510)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [490/689]  eta: 0:05:13  lr: 0.000084  loss: 0.2386 (0.2510)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [500/689]  eta: 0:04:57  lr: 0.000084  loss: 0.2459 (0.2512)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [510/689]  eta: 0:04:41  lr: 0.000084  loss: 0.2588 (0.2511)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [520/689]  eta: 0:04:26  lr: 0.000084  loss: 0.2500 (0.2512)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [530/689]  eta: 0:04:10  lr: 0.000084  loss: 0.2453 (0.2511)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [540/689]  eta: 0:03:54  lr: 0.000084  loss: 0.2418 (0.2510)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [550/689]  eta: 0:03:38  lr: 0.000084  loss: 0.2418 (0.2509)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [560/689]  eta: 0:03:23  lr: 0.000084  loss: 0.2434 (0.2508)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [570/689]  eta: 0:03:07  lr: 0.000084  loss: 0.2436 (0.2509)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [580/689]  eta: 0:02:51  lr: 0.000084  loss: 0.2498 (0.2510)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [590/689]  eta: 0:02:35  lr: 0.000084  loss: 0.2498 (0.2510)  time: 1.5764  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:249]  [600/689]  eta: 0:02:20  lr: 0.000084  loss: 0.2464 (0.2510)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [610/689]  eta: 0:02:04  lr: 0.000084  loss: 0.2396 (0.2508)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [620/689]  eta: 0:01:48  lr: 0.000084  loss: 0.2416 (0.2507)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [630/689]  eta: 0:01:32  lr: 0.000084  loss: 0.2424 (0.2508)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [640/689]  eta: 0:01:17  lr: 0.000084  loss: 0.2366 (0.2507)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [650/689]  eta: 0:01:01  lr: 0.000084  loss: 0.2342 (0.2504)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [660/689]  eta: 0:00:45  lr: 0.000084  loss: 0.2319 (0.2504)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [670/689]  eta: 0:00:29  lr: 0.000084  loss: 0.2510 (0.2504)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [680/689]  eta: 0:00:14  lr: 0.000084  loss: 0.2532 (0.2504)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249]  [688/689]  eta: 0:00:01  lr: 0.000084  loss: 0.2301 (0.2502)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:249] Total time: 0:18:05 (1.5754 s / it)\n",
      "Averaged stats: lr: 0.000084  loss: 0.2301 (0.2502)\n",
      "Valid: [epoch:249]  [ 0/14]  eta: 0:00:13  loss: 0.2276 (0.2276)  time: 0.9847  data: 0.3864  max mem: 39763\n",
      "Valid: [epoch:249]  [13/14]  eta: 0:00:00  loss: 0.2342 (0.2363)  time: 0.1123  data: 0.0277  max mem: 39763\n",
      "Valid: [epoch:249] Total time: 0:00:01 (0.1202 s / it)\n",
      "Averaged stats: loss: 0.2342 (0.2363)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_249_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.236%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:250]  [  0/689]  eta: 0:12:14  lr: 0.000083  loss: 0.2315 (0.2315)  time: 1.0656  data: 0.5889  max mem: 39763\n",
      "Train: [epoch:250]  [ 10/689]  eta: 0:17:18  lr: 0.000083  loss: 0.2428 (0.2489)  time: 1.5292  data: 0.0536  max mem: 39763\n",
      "Train: [epoch:250]  [ 20/689]  eta: 0:17:18  lr: 0.000083  loss: 0.2428 (0.2496)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [ 30/689]  eta: 0:17:08  lr: 0.000083  loss: 0.2458 (0.2508)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [ 40/689]  eta: 0:16:55  lr: 0.000083  loss: 0.2481 (0.2510)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [ 50/689]  eta: 0:16:41  lr: 0.000083  loss: 0.2426 (0.2499)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [ 60/689]  eta: 0:16:26  lr: 0.000083  loss: 0.2401 (0.2512)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [ 70/689]  eta: 0:16:11  lr: 0.000083  loss: 0.2337 (0.2497)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [ 80/689]  eta: 0:15:56  lr: 0.000083  loss: 0.2392 (0.2502)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [ 90/689]  eta: 0:15:41  lr: 0.000083  loss: 0.2402 (0.2487)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [100/689]  eta: 0:15:25  lr: 0.000083  loss: 0.2440 (0.2502)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [110/689]  eta: 0:15:10  lr: 0.000083  loss: 0.2494 (0.2507)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [120/689]  eta: 0:14:54  lr: 0.000083  loss: 0.2543 (0.2519)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [130/689]  eta: 0:14:39  lr: 0.000083  loss: 0.2543 (0.2516)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [140/689]  eta: 0:14:23  lr: 0.000083  loss: 0.2571 (0.2531)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [150/689]  eta: 0:14:07  lr: 0.000083  loss: 0.2552 (0.2531)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [160/689]  eta: 0:13:52  lr: 0.000083  loss: 0.2466 (0.2527)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [170/689]  eta: 0:13:36  lr: 0.000083  loss: 0.2360 (0.2515)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [180/689]  eta: 0:13:21  lr: 0.000083  loss: 0.2368 (0.2524)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [190/689]  eta: 0:13:05  lr: 0.000083  loss: 0.2510 (0.2523)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [200/689]  eta: 0:12:49  lr: 0.000083  loss: 0.2543 (0.2526)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [210/689]  eta: 0:12:34  lr: 0.000083  loss: 0.2602 (0.2528)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [220/689]  eta: 0:12:18  lr: 0.000083  loss: 0.2579 (0.2538)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [230/689]  eta: 0:12:02  lr: 0.000083  loss: 0.2497 (0.2537)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [240/689]  eta: 0:11:46  lr: 0.000083  loss: 0.2381 (0.2537)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [250/689]  eta: 0:11:31  lr: 0.000083  loss: 0.2501 (0.2538)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [260/689]  eta: 0:11:15  lr: 0.000083  loss: 0.2432 (0.2532)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [270/689]  eta: 0:10:59  lr: 0.000083  loss: 0.2388 (0.2529)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [280/689]  eta: 0:10:44  lr: 0.000083  loss: 0.2385 (0.2529)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [290/689]  eta: 0:10:28  lr: 0.000083  loss: 0.2385 (0.2524)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [300/689]  eta: 0:10:12  lr: 0.000083  loss: 0.2377 (0.2524)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [310/689]  eta: 0:09:56  lr: 0.000083  loss: 0.2388 (0.2524)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [320/689]  eta: 0:09:41  lr: 0.000083  loss: 0.2499 (0.2525)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [330/689]  eta: 0:09:25  lr: 0.000083  loss: 0.2514 (0.2533)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [340/689]  eta: 0:09:09  lr: 0.000083  loss: 0.2506 (0.2531)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [350/689]  eta: 0:08:53  lr: 0.000083  loss: 0.2466 (0.2533)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [360/689]  eta: 0:08:38  lr: 0.000083  loss: 0.2431 (0.2530)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [370/689]  eta: 0:08:22  lr: 0.000083  loss: 0.2460 (0.2531)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [380/689]  eta: 0:08:06  lr: 0.000083  loss: 0.2501 (0.2536)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [390/689]  eta: 0:07:50  lr: 0.000083  loss: 0.2470 (0.2536)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [400/689]  eta: 0:07:35  lr: 0.000083  loss: 0.2404 (0.2536)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [410/689]  eta: 0:07:19  lr: 0.000083  loss: 0.2532 (0.2537)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [420/689]  eta: 0:07:03  lr: 0.000083  loss: 0.2547 (0.2535)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [430/689]  eta: 0:06:47  lr: 0.000083  loss: 0.2509 (0.2538)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [440/689]  eta: 0:06:32  lr: 0.000083  loss: 0.2460 (0.2536)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [450/689]  eta: 0:06:16  lr: 0.000083  loss: 0.2476 (0.2541)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [460/689]  eta: 0:06:00  lr: 0.000083  loss: 0.2588 (0.2541)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [470/689]  eta: 0:05:44  lr: 0.000083  loss: 0.2388 (0.2540)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [480/689]  eta: 0:05:29  lr: 0.000083  loss: 0.2380 (0.2538)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [490/689]  eta: 0:05:13  lr: 0.000083  loss: 0.2392 (0.2534)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [500/689]  eta: 0:04:57  lr: 0.000083  loss: 0.2383 (0.2534)  time: 1.5750  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:250]  [510/689]  eta: 0:04:41  lr: 0.000083  loss: 0.2389 (0.2534)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [520/689]  eta: 0:04:26  lr: 0.000083  loss: 0.2450 (0.2533)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [530/689]  eta: 0:04:10  lr: 0.000083  loss: 0.2385 (0.2529)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [540/689]  eta: 0:03:54  lr: 0.000083  loss: 0.2377 (0.2530)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [550/689]  eta: 0:03:38  lr: 0.000083  loss: 0.2400 (0.2527)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [560/689]  eta: 0:03:23  lr: 0.000083  loss: 0.2400 (0.2528)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [570/689]  eta: 0:03:07  lr: 0.000083  loss: 0.2530 (0.2526)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [580/689]  eta: 0:02:51  lr: 0.000083  loss: 0.2530 (0.2528)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [590/689]  eta: 0:02:35  lr: 0.000083  loss: 0.2571 (0.2528)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [600/689]  eta: 0:02:20  lr: 0.000083  loss: 0.2423 (0.2528)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [610/689]  eta: 0:02:04  lr: 0.000083  loss: 0.2350 (0.2526)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [620/689]  eta: 0:01:48  lr: 0.000083  loss: 0.2348 (0.2526)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [630/689]  eta: 0:01:32  lr: 0.000083  loss: 0.2371 (0.2525)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [640/689]  eta: 0:01:17  lr: 0.000083  loss: 0.2561 (0.2528)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [650/689]  eta: 0:01:01  lr: 0.000083  loss: 0.2632 (0.2529)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [660/689]  eta: 0:00:45  lr: 0.000083  loss: 0.2622 (0.2530)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [670/689]  eta: 0:00:29  lr: 0.000083  loss: 0.2388 (0.2526)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [680/689]  eta: 0:00:14  lr: 0.000083  loss: 0.2388 (0.2528)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250]  [688/689]  eta: 0:00:01  lr: 0.000083  loss: 0.2349 (0.2526)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:250] Total time: 0:18:05 (1.5759 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: 0.2349 (0.2526)\n",
      "Valid: [epoch:250]  [ 0/14]  eta: 0:00:14  loss: 0.2289 (0.2289)  time: 1.0075  data: 0.3953  max mem: 39763\n",
      "Valid: [epoch:250]  [13/14]  eta: 0:00:00  loss: 0.2348 (0.2372)  time: 0.1139  data: 0.0283  max mem: 39763\n",
      "Valid: [epoch:250] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.2348 (0.2372)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_250_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.237%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:251]  [  0/689]  eta: 0:12:21  lr: 0.000083  loss: 0.2346 (0.2346)  time: 1.0760  data: 0.5999  max mem: 39763\n",
      "Train: [epoch:251]  [ 10/689]  eta: 0:17:17  lr: 0.000083  loss: 0.2457 (0.2503)  time: 1.5282  data: 0.0546  max mem: 39763\n",
      "Train: [epoch:251]  [ 20/689]  eta: 0:17:17  lr: 0.000083  loss: 0.2371 (0.2490)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [ 30/689]  eta: 0:17:07  lr: 0.000083  loss: 0.2445 (0.2505)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [ 40/689]  eta: 0:16:54  lr: 0.000083  loss: 0.2512 (0.2491)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [ 50/689]  eta: 0:16:40  lr: 0.000083  loss: 0.2526 (0.2502)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [ 60/689]  eta: 0:16:25  lr: 0.000083  loss: 0.2502 (0.2503)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [ 70/689]  eta: 0:16:10  lr: 0.000083  loss: 0.2368 (0.2479)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [ 80/689]  eta: 0:15:55  lr: 0.000083  loss: 0.2368 (0.2473)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [ 90/689]  eta: 0:15:40  lr: 0.000083  loss: 0.2359 (0.2463)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [100/689]  eta: 0:15:25  lr: 0.000083  loss: 0.2372 (0.2465)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [110/689]  eta: 0:15:09  lr: 0.000083  loss: 0.2432 (0.2462)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [120/689]  eta: 0:14:54  lr: 0.000083  loss: 0.2432 (0.2466)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [130/689]  eta: 0:14:38  lr: 0.000083  loss: 0.2510 (0.2477)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [140/689]  eta: 0:14:23  lr: 0.000083  loss: 0.2510 (0.2475)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [150/689]  eta: 0:14:07  lr: 0.000083  loss: 0.2440 (0.2477)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [160/689]  eta: 0:13:52  lr: 0.000083  loss: 0.2436 (0.2476)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [170/689]  eta: 0:13:36  lr: 0.000083  loss: 0.2401 (0.2477)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [180/689]  eta: 0:13:21  lr: 0.000083  loss: 0.2421 (0.2484)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [190/689]  eta: 0:13:05  lr: 0.000083  loss: 0.2596 (0.2495)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [200/689]  eta: 0:12:49  lr: 0.000083  loss: 0.2555 (0.2495)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [210/689]  eta: 0:12:34  lr: 0.000083  loss: 0.2412 (0.2496)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [220/689]  eta: 0:12:18  lr: 0.000083  loss: 0.2471 (0.2493)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [230/689]  eta: 0:12:02  lr: 0.000083  loss: 0.2382 (0.2490)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [240/689]  eta: 0:11:46  lr: 0.000083  loss: 0.2537 (0.2513)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [250/689]  eta: 0:11:31  lr: 0.000083  loss: 0.2696 (0.2515)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [260/689]  eta: 0:11:15  lr: 0.000083  loss: 0.2607 (0.2521)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [270/689]  eta: 0:10:59  lr: 0.000083  loss: 0.2537 (0.2518)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [280/689]  eta: 0:10:44  lr: 0.000083  loss: 0.2486 (0.2518)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [290/689]  eta: 0:10:28  lr: 0.000083  loss: 0.2486 (0.2517)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [300/689]  eta: 0:10:12  lr: 0.000083  loss: 0.2361 (0.2515)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [310/689]  eta: 0:09:56  lr: 0.000083  loss: 0.2448 (0.2514)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [320/689]  eta: 0:09:41  lr: 0.000083  loss: 0.2537 (0.2512)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [330/689]  eta: 0:09:25  lr: 0.000083  loss: 0.2537 (0.2513)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [340/689]  eta: 0:09:09  lr: 0.000083  loss: 0.2494 (0.2513)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [350/689]  eta: 0:08:54  lr: 0.000083  loss: 0.2420 (0.2513)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [360/689]  eta: 0:08:38  lr: 0.000083  loss: 0.2495 (0.2516)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [370/689]  eta: 0:08:22  lr: 0.000083  loss: 0.2562 (0.2516)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [380/689]  eta: 0:08:06  lr: 0.000083  loss: 0.2562 (0.2517)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [390/689]  eta: 0:07:51  lr: 0.000083  loss: 0.2548 (0.2518)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [400/689]  eta: 0:07:35  lr: 0.000083  loss: 0.2544 (0.2520)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [410/689]  eta: 0:07:19  lr: 0.000083  loss: 0.2544 (0.2523)  time: 1.5754  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:251]  [420/689]  eta: 0:07:03  lr: 0.000083  loss: 0.2433 (0.2520)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [430/689]  eta: 0:06:47  lr: 0.000083  loss: 0.2409 (0.2520)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [440/689]  eta: 0:06:32  lr: 0.000083  loss: 0.2415 (0.2521)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [450/689]  eta: 0:06:16  lr: 0.000083  loss: 0.2495 (0.2520)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [460/689]  eta: 0:06:00  lr: 0.000083  loss: 0.2495 (0.2519)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [470/689]  eta: 0:05:44  lr: 0.000083  loss: 0.2568 (0.2520)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [480/689]  eta: 0:05:29  lr: 0.000083  loss: 0.2501 (0.2523)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [490/689]  eta: 0:05:13  lr: 0.000083  loss: 0.2505 (0.2523)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [500/689]  eta: 0:04:57  lr: 0.000083  loss: 0.2516 (0.2526)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [510/689]  eta: 0:04:41  lr: 0.000083  loss: 0.2420 (0.2523)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [520/689]  eta: 0:04:26  lr: 0.000083  loss: 0.2386 (0.2521)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [530/689]  eta: 0:04:10  lr: 0.000083  loss: 0.2503 (0.2525)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [540/689]  eta: 0:03:54  lr: 0.000083  loss: 0.2597 (0.2526)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [550/689]  eta: 0:03:38  lr: 0.000083  loss: 0.2322 (0.2522)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [560/689]  eta: 0:03:23  lr: 0.000083  loss: 0.2365 (0.2525)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [570/689]  eta: 0:03:07  lr: 0.000083  loss: 0.2616 (0.2527)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [580/689]  eta: 0:02:51  lr: 0.000083  loss: 0.2461 (0.2525)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [590/689]  eta: 0:02:35  lr: 0.000083  loss: 0.2330 (0.2524)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [600/689]  eta: 0:02:20  lr: 0.000083  loss: 0.2453 (0.2524)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [610/689]  eta: 0:02:04  lr: 0.000083  loss: 0.2453 (0.2521)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [620/689]  eta: 0:01:48  lr: 0.000083  loss: 0.2510 (0.2523)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [630/689]  eta: 0:01:32  lr: 0.000083  loss: 0.2510 (0.2522)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [640/689]  eta: 0:01:17  lr: 0.000083  loss: 0.2501 (0.2523)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [650/689]  eta: 0:01:01  lr: 0.000083  loss: 0.2462 (0.2521)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [660/689]  eta: 0:00:45  lr: 0.000083  loss: 0.2353 (0.2521)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [670/689]  eta: 0:00:29  lr: 0.000083  loss: 0.2363 (0.2519)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [680/689]  eta: 0:00:14  lr: 0.000083  loss: 0.2414 (0.2521)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251]  [688/689]  eta: 0:00:01  lr: 0.000083  loss: 0.2517 (0.2521)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:251] Total time: 0:18:05 (1.5758 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: 0.2517 (0.2521)\n",
      "Valid: [epoch:251]  [ 0/14]  eta: 0:00:14  loss: 0.2225 (0.2225)  time: 1.0156  data: 0.3861  max mem: 39763\n",
      "Valid: [epoch:251]  [13/14]  eta: 0:00:00  loss: 0.2363 (0.2384)  time: 0.1144  data: 0.0276  max mem: 39763\n",
      "Valid: [epoch:251] Total time: 0:00:01 (0.1233 s / it)\n",
      "Averaged stats: loss: 0.2363 (0.2384)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_251_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.238%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:252]  [  0/689]  eta: 0:11:28  lr: 0.000083  loss: 0.2302 (0.2302)  time: 0.9995  data: 0.5227  max mem: 39763\n",
      "Train: [epoch:252]  [ 10/689]  eta: 0:17:13  lr: 0.000083  loss: 0.2746 (0.2738)  time: 1.5227  data: 0.0476  max mem: 39763\n",
      "Train: [epoch:252]  [ 20/689]  eta: 0:17:15  lr: 0.000083  loss: 0.2516 (0.2559)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [ 30/689]  eta: 0:17:06  lr: 0.000083  loss: 0.2431 (0.2596)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [ 40/689]  eta: 0:16:53  lr: 0.000083  loss: 0.2431 (0.2598)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [ 50/689]  eta: 0:16:40  lr: 0.000083  loss: 0.2391 (0.2563)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [ 60/689]  eta: 0:16:25  lr: 0.000083  loss: 0.2453 (0.2550)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [ 70/689]  eta: 0:16:10  lr: 0.000083  loss: 0.2479 (0.2539)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [ 80/689]  eta: 0:15:55  lr: 0.000083  loss: 0.2479 (0.2528)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [ 90/689]  eta: 0:15:40  lr: 0.000083  loss: 0.2416 (0.2521)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [100/689]  eta: 0:15:25  lr: 0.000083  loss: 0.2464 (0.2520)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [110/689]  eta: 0:15:09  lr: 0.000083  loss: 0.2504 (0.2524)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [120/689]  eta: 0:14:54  lr: 0.000083  loss: 0.2447 (0.2521)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [130/689]  eta: 0:14:38  lr: 0.000083  loss: 0.2487 (0.2538)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [140/689]  eta: 0:14:23  lr: 0.000083  loss: 0.2635 (0.2542)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [150/689]  eta: 0:14:07  lr: 0.000083  loss: 0.2501 (0.2538)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [160/689]  eta: 0:13:52  lr: 0.000083  loss: 0.2471 (0.2535)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [170/689]  eta: 0:13:36  lr: 0.000083  loss: 0.2485 (0.2535)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [180/689]  eta: 0:13:20  lr: 0.000083  loss: 0.2485 (0.2533)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [190/689]  eta: 0:13:05  lr: 0.000083  loss: 0.2497 (0.2534)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [200/689]  eta: 0:12:49  lr: 0.000083  loss: 0.2506 (0.2543)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [210/689]  eta: 0:12:33  lr: 0.000083  loss: 0.2443 (0.2538)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [220/689]  eta: 0:12:18  lr: 0.000083  loss: 0.2443 (0.2538)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [230/689]  eta: 0:12:02  lr: 0.000083  loss: 0.2493 (0.2535)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [240/689]  eta: 0:11:46  lr: 0.000083  loss: 0.2483 (0.2535)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [250/689]  eta: 0:11:31  lr: 0.000083  loss: 0.2451 (0.2533)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [260/689]  eta: 0:11:15  lr: 0.000083  loss: 0.2422 (0.2535)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [270/689]  eta: 0:10:59  lr: 0.000083  loss: 0.2557 (0.2535)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [280/689]  eta: 0:10:43  lr: 0.000083  loss: 0.2438 (0.2535)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [290/689]  eta: 0:10:28  lr: 0.000083  loss: 0.2488 (0.2536)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [300/689]  eta: 0:10:12  lr: 0.000083  loss: 0.2488 (0.2533)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [310/689]  eta: 0:09:56  lr: 0.000083  loss: 0.2450 (0.2531)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [320/689]  eta: 0:09:41  lr: 0.000083  loss: 0.2562 (0.2537)  time: 1.5765  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:252]  [330/689]  eta: 0:09:25  lr: 0.000083  loss: 0.2518 (0.2535)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [340/689]  eta: 0:09:09  lr: 0.000083  loss: 0.2518 (0.2537)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [350/689]  eta: 0:08:53  lr: 0.000083  loss: 0.2543 (0.2537)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [360/689]  eta: 0:08:38  lr: 0.000083  loss: 0.2579 (0.2540)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [370/689]  eta: 0:08:22  lr: 0.000083  loss: 0.2579 (0.2539)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [380/689]  eta: 0:08:06  lr: 0.000083  loss: 0.2373 (0.2535)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [390/689]  eta: 0:07:50  lr: 0.000083  loss: 0.2459 (0.2534)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [400/689]  eta: 0:07:35  lr: 0.000083  loss: 0.2482 (0.2534)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [410/689]  eta: 0:07:19  lr: 0.000083  loss: 0.2468 (0.2533)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [420/689]  eta: 0:07:03  lr: 0.000083  loss: 0.2468 (0.2534)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [430/689]  eta: 0:06:47  lr: 0.000083  loss: 0.2665 (0.2537)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [440/689]  eta: 0:06:32  lr: 0.000083  loss: 0.2575 (0.2538)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [450/689]  eta: 0:06:16  lr: 0.000083  loss: 0.2496 (0.2537)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [460/689]  eta: 0:06:00  lr: 0.000083  loss: 0.2580 (0.2542)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [470/689]  eta: 0:05:44  lr: 0.000083  loss: 0.2580 (0.2544)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [480/689]  eta: 0:05:29  lr: 0.000083  loss: 0.2386 (0.2542)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [490/689]  eta: 0:05:13  lr: 0.000083  loss: 0.2442 (0.2541)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [500/689]  eta: 0:04:57  lr: 0.000083  loss: 0.2491 (0.2544)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [510/689]  eta: 0:04:41  lr: 0.000083  loss: 0.2491 (0.2545)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [520/689]  eta: 0:04:26  lr: 0.000083  loss: 0.2478 (0.2542)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [530/689]  eta: 0:04:10  lr: 0.000083  loss: 0.2428 (0.2542)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [540/689]  eta: 0:03:54  lr: 0.000083  loss: 0.2462 (0.2543)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [550/689]  eta: 0:03:38  lr: 0.000083  loss: 0.2494 (0.2542)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [560/689]  eta: 0:03:23  lr: 0.000083  loss: 0.2574 (0.2542)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [570/689]  eta: 0:03:07  lr: 0.000083  loss: 0.2404 (0.2542)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [580/689]  eta: 0:02:51  lr: 0.000083  loss: 0.2400 (0.2542)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [590/689]  eta: 0:02:35  lr: 0.000083  loss: 0.2413 (0.2542)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [600/689]  eta: 0:02:20  lr: 0.000083  loss: 0.2473 (0.2542)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [610/689]  eta: 0:02:04  lr: 0.000083  loss: 0.2489 (0.2542)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [620/689]  eta: 0:01:48  lr: 0.000083  loss: 0.2443 (0.2541)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [630/689]  eta: 0:01:32  lr: 0.000083  loss: 0.2471 (0.2541)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [640/689]  eta: 0:01:17  lr: 0.000083  loss: 0.2471 (0.2539)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [650/689]  eta: 0:01:01  lr: 0.000083  loss: 0.2407 (0.2539)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [660/689]  eta: 0:00:45  lr: 0.000083  loss: 0.2449 (0.2538)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [670/689]  eta: 0:00:29  lr: 0.000083  loss: 0.2467 (0.2537)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [680/689]  eta: 0:00:14  lr: 0.000083  loss: 0.2444 (0.2537)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252]  [688/689]  eta: 0:00:01  lr: 0.000083  loss: 0.2435 (0.2535)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:252] Total time: 0:18:05 (1.5758 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: 0.2435 (0.2535)\n",
      "Valid: [epoch:252]  [ 0/14]  eta: 0:00:14  loss: 0.2204 (0.2204)  time: 1.0085  data: 0.3889  max mem: 39763\n",
      "Valid: [epoch:252]  [13/14]  eta: 0:00:00  loss: 0.2370 (0.2396)  time: 0.1140  data: 0.0278  max mem: 39763\n",
      "Valid: [epoch:252] Total time: 0:00:01 (0.1271 s / it)\n",
      "Averaged stats: loss: 0.2370 (0.2396)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_252_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.240%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:253]  [  0/689]  eta: 0:11:34  lr: 0.000083  loss: 0.2805 (0.2805)  time: 1.0079  data: 0.5306  max mem: 39763\n",
      "Train: [epoch:253]  [ 10/689]  eta: 0:17:13  lr: 0.000083  loss: 0.2481 (0.2614)  time: 1.5224  data: 0.0483  max mem: 39763\n",
      "Train: [epoch:253]  [ 20/689]  eta: 0:17:15  lr: 0.000083  loss: 0.2464 (0.2544)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [ 30/689]  eta: 0:17:05  lr: 0.000083  loss: 0.2464 (0.2555)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [ 40/689]  eta: 0:16:53  lr: 0.000083  loss: 0.2445 (0.2547)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [ 50/689]  eta: 0:16:39  lr: 0.000083  loss: 0.2447 (0.2515)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [ 60/689]  eta: 0:16:25  lr: 0.000083  loss: 0.2407 (0.2520)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [ 70/689]  eta: 0:16:10  lr: 0.000083  loss: 0.2360 (0.2502)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [ 80/689]  eta: 0:15:55  lr: 0.000083  loss: 0.2372 (0.2503)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [ 90/689]  eta: 0:15:40  lr: 0.000083  loss: 0.2395 (0.2490)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [100/689]  eta: 0:15:24  lr: 0.000083  loss: 0.2465 (0.2511)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [110/689]  eta: 0:15:09  lr: 0.000083  loss: 0.2658 (0.2528)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [120/689]  eta: 0:14:54  lr: 0.000083  loss: 0.2548 (0.2523)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [130/689]  eta: 0:14:38  lr: 0.000083  loss: 0.2437 (0.2534)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [140/689]  eta: 0:14:22  lr: 0.000083  loss: 0.2421 (0.2524)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [150/689]  eta: 0:14:07  lr: 0.000083  loss: 0.2375 (0.2520)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [160/689]  eta: 0:13:51  lr: 0.000083  loss: 0.2484 (0.2522)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [170/689]  eta: 0:13:36  lr: 0.000083  loss: 0.2484 (0.2523)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [180/689]  eta: 0:13:20  lr: 0.000083  loss: 0.2565 (0.2529)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [190/689]  eta: 0:13:04  lr: 0.000083  loss: 0.2450 (0.2527)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [200/689]  eta: 0:12:49  lr: 0.000083  loss: 0.2467 (0.2533)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [210/689]  eta: 0:12:33  lr: 0.000083  loss: 0.2508 (0.2527)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [220/689]  eta: 0:12:17  lr: 0.000083  loss: 0.2484 (0.2526)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [230/689]  eta: 0:12:02  lr: 0.000083  loss: 0.2543 (0.2532)  time: 1.5764  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:253]  [240/689]  eta: 0:11:46  lr: 0.000083  loss: 0.2585 (0.2532)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [250/689]  eta: 0:11:30  lr: 0.000083  loss: 0.2450 (0.2529)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [260/689]  eta: 0:11:15  lr: 0.000083  loss: 0.2387 (0.2531)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [270/689]  eta: 0:10:59  lr: 0.000083  loss: 0.2493 (0.2531)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [280/689]  eta: 0:10:43  lr: 0.000083  loss: 0.2471 (0.2532)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [290/689]  eta: 0:10:28  lr: 0.000083  loss: 0.2471 (0.2531)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [300/689]  eta: 0:10:12  lr: 0.000083  loss: 0.2503 (0.2536)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [310/689]  eta: 0:09:56  lr: 0.000083  loss: 0.2481 (0.2533)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [320/689]  eta: 0:09:40  lr: 0.000083  loss: 0.2594 (0.2542)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [330/689]  eta: 0:09:25  lr: 0.000083  loss: 0.2708 (0.2545)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [340/689]  eta: 0:09:09  lr: 0.000083  loss: 0.2708 (0.2549)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [350/689]  eta: 0:08:53  lr: 0.000083  loss: 0.2464 (0.2544)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [360/689]  eta: 0:08:37  lr: 0.000083  loss: 0.2406 (0.2545)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [370/689]  eta: 0:08:22  lr: 0.000083  loss: 0.2607 (0.2548)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [380/689]  eta: 0:08:06  lr: 0.000083  loss: 0.2607 (0.2550)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [390/689]  eta: 0:07:50  lr: 0.000083  loss: 0.2457 (0.2551)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [400/689]  eta: 0:07:34  lr: 0.000083  loss: 0.2370 (0.2548)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [410/689]  eta: 0:07:19  lr: 0.000083  loss: 0.2299 (0.2546)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [420/689]  eta: 0:07:03  lr: 0.000083  loss: 0.2307 (0.2542)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [430/689]  eta: 0:06:47  lr: 0.000083  loss: 0.2471 (0.2541)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [440/689]  eta: 0:06:32  lr: 0.000083  loss: 0.2518 (0.2541)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [450/689]  eta: 0:06:16  lr: 0.000083  loss: 0.2523 (0.2539)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [460/689]  eta: 0:06:00  lr: 0.000083  loss: 0.2582 (0.2540)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [470/689]  eta: 0:05:44  lr: 0.000083  loss: 0.2582 (0.2541)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [480/689]  eta: 0:05:29  lr: 0.000083  loss: 0.2464 (0.2541)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [490/689]  eta: 0:05:13  lr: 0.000083  loss: 0.2468 (0.2541)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [500/689]  eta: 0:04:57  lr: 0.000083  loss: 0.2488 (0.2542)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [510/689]  eta: 0:04:41  lr: 0.000083  loss: 0.2600 (0.2543)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [520/689]  eta: 0:04:26  lr: 0.000083  loss: 0.2460 (0.2541)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [530/689]  eta: 0:04:10  lr: 0.000083  loss: 0.2412 (0.2543)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [540/689]  eta: 0:03:54  lr: 0.000083  loss: 0.2493 (0.2542)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [550/689]  eta: 0:03:38  lr: 0.000083  loss: 0.2508 (0.2541)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [560/689]  eta: 0:03:23  lr: 0.000083  loss: 0.2560 (0.2546)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [570/689]  eta: 0:03:07  lr: 0.000083  loss: 0.2575 (0.2544)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [580/689]  eta: 0:02:51  lr: 0.000083  loss: 0.2463 (0.2545)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [590/689]  eta: 0:02:35  lr: 0.000083  loss: 0.2587 (0.2545)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [600/689]  eta: 0:02:20  lr: 0.000083  loss: 0.2734 (0.2549)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [610/689]  eta: 0:02:04  lr: 0.000083  loss: 0.2617 (0.2548)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [620/689]  eta: 0:01:48  lr: 0.000083  loss: 0.2451 (0.2548)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [630/689]  eta: 0:01:32  lr: 0.000083  loss: 0.2540 (0.2549)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [640/689]  eta: 0:01:17  lr: 0.000083  loss: 0.2643 (0.2552)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [650/689]  eta: 0:01:01  lr: 0.000083  loss: 0.2563 (0.2551)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [660/689]  eta: 0:00:45  lr: 0.000083  loss: 0.2488 (0.2551)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [670/689]  eta: 0:00:29  lr: 0.000083  loss: 0.2457 (0.2548)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [680/689]  eta: 0:00:14  lr: 0.000083  loss: 0.2375 (0.2546)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253]  [688/689]  eta: 0:00:01  lr: 0.000083  loss: 0.2403 (0.2547)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:253] Total time: 0:18:05 (1.5756 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: 0.2403 (0.2547)\n",
      "Valid: [epoch:253]  [ 0/14]  eta: 0:00:13  loss: 0.2315 (0.2315)  time: 0.9973  data: 0.3885  max mem: 39763\n",
      "Valid: [epoch:253]  [13/14]  eta: 0:00:00  loss: 0.2375 (0.2399)  time: 0.1131  data: 0.0278  max mem: 39763\n",
      "Valid: [epoch:253] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.2375 (0.2399)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_253_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.240%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:254]  [  0/689]  eta: 0:12:22  lr: 0.000083  loss: 0.2830 (0.2830)  time: 1.0779  data: 0.6016  max mem: 39763\n",
      "Train: [epoch:254]  [ 10/689]  eta: 0:17:18  lr: 0.000083  loss: 0.2506 (0.2606)  time: 1.5294  data: 0.0548  max mem: 39763\n",
      "Train: [epoch:254]  [ 20/689]  eta: 0:17:18  lr: 0.000083  loss: 0.2452 (0.2507)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [ 30/689]  eta: 0:17:08  lr: 0.000083  loss: 0.2452 (0.2547)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [ 40/689]  eta: 0:16:55  lr: 0.000083  loss: 0.2608 (0.2575)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [ 50/689]  eta: 0:16:40  lr: 0.000083  loss: 0.2545 (0.2583)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [ 60/689]  eta: 0:16:26  lr: 0.000083  loss: 0.2477 (0.2572)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [ 70/689]  eta: 0:16:11  lr: 0.000083  loss: 0.2426 (0.2540)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [ 80/689]  eta: 0:15:56  lr: 0.000083  loss: 0.2395 (0.2534)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [ 90/689]  eta: 0:15:41  lr: 0.000083  loss: 0.2387 (0.2517)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [100/689]  eta: 0:15:25  lr: 0.000083  loss: 0.2387 (0.2529)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [110/689]  eta: 0:15:10  lr: 0.000083  loss: 0.2594 (0.2529)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [120/689]  eta: 0:14:54  lr: 0.000083  loss: 0.2478 (0.2519)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [130/689]  eta: 0:14:39  lr: 0.000083  loss: 0.2434 (0.2528)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [140/689]  eta: 0:14:23  lr: 0.000083  loss: 0.2431 (0.2524)  time: 1.5763  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:254]  [150/689]  eta: 0:14:07  lr: 0.000083  loss: 0.2420 (0.2525)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [160/689]  eta: 0:13:52  lr: 0.000083  loss: 0.2479 (0.2527)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [170/689]  eta: 0:13:36  lr: 0.000083  loss: 0.2651 (0.2538)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [180/689]  eta: 0:13:21  lr: 0.000083  loss: 0.2492 (0.2530)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [190/689]  eta: 0:13:05  lr: 0.000083  loss: 0.2434 (0.2535)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [200/689]  eta: 0:12:49  lr: 0.000083  loss: 0.2551 (0.2534)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [210/689]  eta: 0:12:34  lr: 0.000083  loss: 0.2605 (0.2538)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [220/689]  eta: 0:12:18  lr: 0.000083  loss: 0.2483 (0.2534)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [230/689]  eta: 0:12:02  lr: 0.000083  loss: 0.2387 (0.2526)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [240/689]  eta: 0:11:46  lr: 0.000083  loss: 0.2468 (0.2532)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [250/689]  eta: 0:11:31  lr: 0.000083  loss: 0.2644 (0.2535)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [260/689]  eta: 0:11:15  lr: 0.000083  loss: 0.2370 (0.2528)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [270/689]  eta: 0:10:59  lr: 0.000083  loss: 0.2354 (0.2528)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [280/689]  eta: 0:10:44  lr: 0.000083  loss: 0.2502 (0.2530)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [290/689]  eta: 0:10:28  lr: 0.000083  loss: 0.2582 (0.2534)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [300/689]  eta: 0:10:12  lr: 0.000083  loss: 0.2588 (0.2536)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [310/689]  eta: 0:09:56  lr: 0.000083  loss: 0.2389 (0.2529)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [320/689]  eta: 0:09:41  lr: 0.000083  loss: 0.2347 (0.2526)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [330/689]  eta: 0:09:25  lr: 0.000083  loss: 0.2415 (0.2529)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [340/689]  eta: 0:09:09  lr: 0.000083  loss: 0.2505 (0.2528)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [350/689]  eta: 0:08:53  lr: 0.000083  loss: 0.2498 (0.2530)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [360/689]  eta: 0:08:38  lr: 0.000083  loss: 0.2367 (0.2524)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [370/689]  eta: 0:08:22  lr: 0.000083  loss: 0.2340 (0.2522)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [380/689]  eta: 0:08:06  lr: 0.000083  loss: 0.2372 (0.2524)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [390/689]  eta: 0:07:50  lr: 0.000083  loss: 0.2493 (0.2528)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [400/689]  eta: 0:07:35  lr: 0.000083  loss: 0.2472 (0.2526)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [410/689]  eta: 0:07:19  lr: 0.000083  loss: 0.2439 (0.2528)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [420/689]  eta: 0:07:03  lr: 0.000083  loss: 0.2559 (0.2529)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [430/689]  eta: 0:06:47  lr: 0.000083  loss: 0.2592 (0.2531)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [440/689]  eta: 0:06:32  lr: 0.000083  loss: 0.2565 (0.2532)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [450/689]  eta: 0:06:16  lr: 0.000083  loss: 0.2413 (0.2531)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [460/689]  eta: 0:06:00  lr: 0.000083  loss: 0.2514 (0.2532)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [470/689]  eta: 0:05:44  lr: 0.000083  loss: 0.2608 (0.2535)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [480/689]  eta: 0:05:29  lr: 0.000083  loss: 0.2514 (0.2535)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [490/689]  eta: 0:05:13  lr: 0.000083  loss: 0.2492 (0.2539)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [500/689]  eta: 0:04:57  lr: 0.000083  loss: 0.2639 (0.2541)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [510/689]  eta: 0:04:41  lr: 0.000083  loss: 0.2639 (0.2544)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [520/689]  eta: 0:04:26  lr: 0.000083  loss: 0.2540 (0.2545)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [530/689]  eta: 0:04:10  lr: 0.000083  loss: 0.2540 (0.2544)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [540/689]  eta: 0:03:54  lr: 0.000083  loss: 0.2382 (0.2543)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [550/689]  eta: 0:03:38  lr: 0.000083  loss: 0.2382 (0.2540)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [560/689]  eta: 0:03:23  lr: 0.000083  loss: 0.2457 (0.2542)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [570/689]  eta: 0:03:07  lr: 0.000083  loss: 0.2564 (0.2543)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [580/689]  eta: 0:02:51  lr: 0.000083  loss: 0.2630 (0.2546)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [590/689]  eta: 0:02:35  lr: 0.000083  loss: 0.2541 (0.2546)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [600/689]  eta: 0:02:20  lr: 0.000083  loss: 0.2593 (0.2547)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [610/689]  eta: 0:02:04  lr: 0.000083  loss: 0.2516 (0.2545)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [620/689]  eta: 0:01:48  lr: 0.000083  loss: 0.2383 (0.2544)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [630/689]  eta: 0:01:32  lr: 0.000083  loss: 0.2520 (0.2549)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [640/689]  eta: 0:01:17  lr: 0.000083  loss: 0.2621 (0.2550)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [650/689]  eta: 0:01:01  lr: 0.000083  loss: 0.2559 (0.2550)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [660/689]  eta: 0:00:45  lr: 0.000083  loss: 0.2527 (0.2551)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [670/689]  eta: 0:00:29  lr: 0.000083  loss: 0.2500 (0.2549)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [680/689]  eta: 0:00:14  lr: 0.000083  loss: 0.2425 (0.2550)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254]  [688/689]  eta: 0:00:01  lr: 0.000083  loss: 0.2451 (0.2549)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:254] Total time: 0:18:05 (1.5760 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: 0.2451 (0.2549)\n",
      "Valid: [epoch:254]  [ 0/14]  eta: 0:00:13  loss: 0.2519 (0.2519)  time: 0.9882  data: 0.3404  max mem: 39763\n",
      "Valid: [epoch:254]  [13/14]  eta: 0:00:00  loss: 0.2393 (0.2413)  time: 0.1125  data: 0.0244  max mem: 39763\n",
      "Valid: [epoch:254] Total time: 0:00:01 (0.1201 s / it)\n",
      "Averaged stats: loss: 0.2393 (0.2413)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_254_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.241%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:255]  [  0/689]  eta: 0:11:40  lr: 0.000083  loss: 0.2325 (0.2325)  time: 1.0172  data: 0.5442  max mem: 39763\n",
      "Train: [epoch:255]  [ 10/689]  eta: 0:17:14  lr: 0.000083  loss: 0.2408 (0.2477)  time: 1.5229  data: 0.0496  max mem: 39763\n",
      "Train: [epoch:255]  [ 20/689]  eta: 0:17:15  lr: 0.000083  loss: 0.2385 (0.2460)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [ 30/689]  eta: 0:17:05  lr: 0.000083  loss: 0.2380 (0.2556)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [ 40/689]  eta: 0:16:52  lr: 0.000083  loss: 0.2416 (0.2563)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [ 50/689]  eta: 0:16:39  lr: 0.000083  loss: 0.2378 (0.2520)  time: 1.5745  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:255]  [ 60/689]  eta: 0:16:24  lr: 0.000083  loss: 0.2378 (0.2544)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [ 70/689]  eta: 0:16:09  lr: 0.000083  loss: 0.2469 (0.2541)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [ 80/689]  eta: 0:15:54  lr: 0.000083  loss: 0.2438 (0.2532)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [ 90/689]  eta: 0:15:39  lr: 0.000083  loss: 0.2485 (0.2544)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [100/689]  eta: 0:15:24  lr: 0.000083  loss: 0.2647 (0.2563)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [110/689]  eta: 0:15:08  lr: 0.000083  loss: 0.2530 (0.2561)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [120/689]  eta: 0:14:53  lr: 0.000083  loss: 0.2472 (0.2561)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [130/689]  eta: 0:14:37  lr: 0.000083  loss: 0.2472 (0.2558)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [140/689]  eta: 0:14:22  lr: 0.000083  loss: 0.2575 (0.2566)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [150/689]  eta: 0:14:06  lr: 0.000083  loss: 0.2444 (0.2557)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [160/689]  eta: 0:13:51  lr: 0.000083  loss: 0.2476 (0.2554)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [170/689]  eta: 0:13:35  lr: 0.000083  loss: 0.2504 (0.2552)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [180/689]  eta: 0:13:19  lr: 0.000083  loss: 0.2389 (0.2542)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [190/689]  eta: 0:13:04  lr: 0.000083  loss: 0.2400 (0.2540)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [200/689]  eta: 0:12:48  lr: 0.000083  loss: 0.2445 (0.2537)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [210/689]  eta: 0:12:32  lr: 0.000083  loss: 0.2423 (0.2536)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [220/689]  eta: 0:12:17  lr: 0.000083  loss: 0.2475 (0.2536)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [230/689]  eta: 0:12:01  lr: 0.000083  loss: 0.2490 (0.2537)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [240/689]  eta: 0:11:45  lr: 0.000083  loss: 0.2576 (0.2542)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [250/689]  eta: 0:11:30  lr: 0.000083  loss: 0.2576 (0.2541)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [260/689]  eta: 0:11:14  lr: 0.000083  loss: 0.2393 (0.2538)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [270/689]  eta: 0:10:58  lr: 0.000083  loss: 0.2350 (0.2536)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [280/689]  eta: 0:10:43  lr: 0.000083  loss: 0.2442 (0.2537)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [290/689]  eta: 0:10:27  lr: 0.000083  loss: 0.2431 (0.2532)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [300/689]  eta: 0:10:11  lr: 0.000083  loss: 0.2471 (0.2534)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [310/689]  eta: 0:09:56  lr: 0.000083  loss: 0.2471 (0.2529)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [320/689]  eta: 0:09:40  lr: 0.000083  loss: 0.2482 (0.2526)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [330/689]  eta: 0:09:24  lr: 0.000083  loss: 0.2491 (0.2529)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [340/689]  eta: 0:09:09  lr: 0.000083  loss: 0.2483 (0.2530)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [350/689]  eta: 0:08:53  lr: 0.000083  loss: 0.2525 (0.2532)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [360/689]  eta: 0:08:37  lr: 0.000083  loss: 0.2531 (0.2533)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [370/689]  eta: 0:08:21  lr: 0.000083  loss: 0.2517 (0.2533)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [380/689]  eta: 0:08:06  lr: 0.000083  loss: 0.2508 (0.2532)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [390/689]  eta: 0:07:50  lr: 0.000083  loss: 0.2756 (0.2544)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [400/689]  eta: 0:07:34  lr: 0.000083  loss: 0.2756 (0.2544)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [410/689]  eta: 0:07:19  lr: 0.000083  loss: 0.2489 (0.2546)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [420/689]  eta: 0:07:03  lr: 0.000083  loss: 0.2591 (0.2548)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [430/689]  eta: 0:06:47  lr: 0.000083  loss: 0.2591 (0.2551)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [440/689]  eta: 0:06:31  lr: 0.000083  loss: 0.2491 (0.2552)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [450/689]  eta: 0:06:16  lr: 0.000083  loss: 0.2553 (0.2555)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [460/689]  eta: 0:06:00  lr: 0.000083  loss: 0.2566 (0.2558)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [470/689]  eta: 0:05:44  lr: 0.000083  loss: 0.2493 (0.2558)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [480/689]  eta: 0:05:29  lr: 0.000083  loss: 0.2452 (0.2558)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [490/689]  eta: 0:05:13  lr: 0.000083  loss: 0.2490 (0.2559)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [500/689]  eta: 0:04:57  lr: 0.000083  loss: 0.2501 (0.2558)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [510/689]  eta: 0:04:41  lr: 0.000083  loss: 0.2643 (0.2558)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [520/689]  eta: 0:04:26  lr: 0.000083  loss: 0.2478 (0.2557)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [530/689]  eta: 0:04:10  lr: 0.000083  loss: 0.2478 (0.2557)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [540/689]  eta: 0:03:54  lr: 0.000083  loss: 0.2632 (0.2559)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [550/689]  eta: 0:03:38  lr: 0.000083  loss: 0.2645 (0.2561)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [560/689]  eta: 0:03:23  lr: 0.000083  loss: 0.2515 (0.2561)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [570/689]  eta: 0:03:07  lr: 0.000083  loss: 0.2459 (0.2560)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [580/689]  eta: 0:02:51  lr: 0.000083  loss: 0.2465 (0.2560)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [590/689]  eta: 0:02:35  lr: 0.000083  loss: 0.2573 (0.2563)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [600/689]  eta: 0:02:20  lr: 0.000083  loss: 0.2540 (0.2562)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [610/689]  eta: 0:02:04  lr: 0.000083  loss: 0.2446 (0.2561)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [620/689]  eta: 0:01:48  lr: 0.000083  loss: 0.2453 (0.2561)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [630/689]  eta: 0:01:32  lr: 0.000083  loss: 0.2450 (0.2560)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [640/689]  eta: 0:01:17  lr: 0.000083  loss: 0.2520 (0.2562)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [650/689]  eta: 0:01:01  lr: 0.000083  loss: 0.2585 (0.2561)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [660/689]  eta: 0:00:45  lr: 0.000083  loss: 0.2470 (0.2561)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [670/689]  eta: 0:00:29  lr: 0.000083  loss: 0.2470 (0.2562)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [680/689]  eta: 0:00:14  lr: 0.000083  loss: 0.2450 (0.2562)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255]  [688/689]  eta: 0:00:01  lr: 0.000083  loss: 0.2493 (0.2562)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:255] Total time: 0:18:05 (1.5751 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: 0.2493 (0.2562)\n",
      "Valid: [epoch:255]  [ 0/14]  eta: 0:00:14  loss: 0.2420 (0.2420)  time: 1.0233  data: 0.3679  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:255]  [13/14]  eta: 0:00:00  loss: 0.2420 (0.2452)  time: 0.1150  data: 0.0263  max mem: 39763\n",
      "Valid: [epoch:255] Total time: 0:00:01 (0.1242 s / it)\n",
      "Averaged stats: loss: 0.2420 (0.2452)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_255_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.245%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:256]  [  0/689]  eta: 0:11:39  lr: 0.000083  loss: 0.2348 (0.2348)  time: 1.0150  data: 0.5380  max mem: 39763\n",
      "Train: [epoch:256]  [ 10/689]  eta: 0:17:14  lr: 0.000083  loss: 0.2423 (0.2482)  time: 1.5236  data: 0.0490  max mem: 39763\n",
      "Train: [epoch:256]  [ 20/689]  eta: 0:17:15  lr: 0.000083  loss: 0.2485 (0.2589)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [ 30/689]  eta: 0:17:06  lr: 0.000083  loss: 0.2648 (0.2646)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [ 40/689]  eta: 0:16:53  lr: 0.000083  loss: 0.2523 (0.2619)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [ 50/689]  eta: 0:16:39  lr: 0.000083  loss: 0.2503 (0.2594)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [ 60/689]  eta: 0:16:24  lr: 0.000083  loss: 0.2461 (0.2576)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [ 70/689]  eta: 0:16:10  lr: 0.000083  loss: 0.2502 (0.2574)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [ 80/689]  eta: 0:15:55  lr: 0.000083  loss: 0.2506 (0.2570)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [ 90/689]  eta: 0:15:39  lr: 0.000083  loss: 0.2395 (0.2554)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [100/689]  eta: 0:15:24  lr: 0.000083  loss: 0.2401 (0.2571)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [110/689]  eta: 0:15:09  lr: 0.000083  loss: 0.2508 (0.2569)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [120/689]  eta: 0:14:53  lr: 0.000083  loss: 0.2490 (0.2569)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [130/689]  eta: 0:14:38  lr: 0.000083  loss: 0.2625 (0.2576)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [140/689]  eta: 0:14:22  lr: 0.000083  loss: 0.2643 (0.2578)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [150/689]  eta: 0:14:07  lr: 0.000083  loss: 0.2519 (0.2577)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [160/689]  eta: 0:13:51  lr: 0.000083  loss: 0.2455 (0.2564)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [170/689]  eta: 0:13:35  lr: 0.000083  loss: 0.2416 (0.2559)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [180/689]  eta: 0:13:20  lr: 0.000083  loss: 0.2619 (0.2569)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [190/689]  eta: 0:13:04  lr: 0.000083  loss: 0.2593 (0.2568)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [200/689]  eta: 0:12:48  lr: 0.000083  loss: 0.2557 (0.2571)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [210/689]  eta: 0:12:33  lr: 0.000083  loss: 0.2572 (0.2571)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [220/689]  eta: 0:12:17  lr: 0.000083  loss: 0.2623 (0.2571)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [230/689]  eta: 0:12:01  lr: 0.000083  loss: 0.2675 (0.2573)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [240/689]  eta: 0:11:46  lr: 0.000083  loss: 0.2635 (0.2576)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [250/689]  eta: 0:11:30  lr: 0.000083  loss: 0.2629 (0.2577)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [260/689]  eta: 0:11:14  lr: 0.000083  loss: 0.2487 (0.2578)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [270/689]  eta: 0:10:59  lr: 0.000083  loss: 0.2466 (0.2578)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [280/689]  eta: 0:10:43  lr: 0.000083  loss: 0.2449 (0.2575)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [290/689]  eta: 0:10:27  lr: 0.000083  loss: 0.2449 (0.2573)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [300/689]  eta: 0:10:12  lr: 0.000083  loss: 0.2556 (0.2572)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [310/689]  eta: 0:09:56  lr: 0.000083  loss: 0.2541 (0.2574)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [320/689]  eta: 0:09:40  lr: 0.000083  loss: 0.2470 (0.2572)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [330/689]  eta: 0:09:25  lr: 0.000083  loss: 0.2522 (0.2578)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [340/689]  eta: 0:09:09  lr: 0.000083  loss: 0.2570 (0.2580)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [350/689]  eta: 0:08:53  lr: 0.000083  loss: 0.2428 (0.2583)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [360/689]  eta: 0:08:37  lr: 0.000083  loss: 0.2434 (0.2583)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [370/689]  eta: 0:08:22  lr: 0.000083  loss: 0.2494 (0.2578)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [380/689]  eta: 0:08:06  lr: 0.000083  loss: 0.2526 (0.2578)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [390/689]  eta: 0:07:50  lr: 0.000083  loss: 0.2555 (0.2583)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [400/689]  eta: 0:07:35  lr: 0.000083  loss: 0.2619 (0.2584)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [410/689]  eta: 0:07:19  lr: 0.000083  loss: 0.2645 (0.2587)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [420/689]  eta: 0:07:03  lr: 0.000083  loss: 0.2546 (0.2587)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [430/689]  eta: 0:06:47  lr: 0.000083  loss: 0.2523 (0.2586)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [440/689]  eta: 0:06:32  lr: 0.000083  loss: 0.2497 (0.2584)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [450/689]  eta: 0:06:16  lr: 0.000083  loss: 0.2444 (0.2584)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [460/689]  eta: 0:06:00  lr: 0.000083  loss: 0.2657 (0.2589)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [470/689]  eta: 0:05:44  lr: 0.000083  loss: 0.2679 (0.2589)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [480/689]  eta: 0:05:29  lr: 0.000083  loss: 0.2547 (0.2586)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [490/689]  eta: 0:05:13  lr: 0.000083  loss: 0.2386 (0.2587)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [500/689]  eta: 0:04:57  lr: 0.000083  loss: 0.2439 (0.2587)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [510/689]  eta: 0:04:41  lr: 0.000083  loss: 0.2439 (0.2588)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [520/689]  eta: 0:04:26  lr: 0.000083  loss: 0.2706 (0.2591)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [530/689]  eta: 0:04:10  lr: 0.000083  loss: 0.2549 (0.2587)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [540/689]  eta: 0:03:54  lr: 0.000083  loss: 0.2412 (0.2586)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [550/689]  eta: 0:03:38  lr: 0.000083  loss: 0.2427 (0.2583)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [560/689]  eta: 0:03:23  lr: 0.000083  loss: 0.2439 (0.2583)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [570/689]  eta: 0:03:07  lr: 0.000083  loss: 0.2539 (0.2586)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [580/689]  eta: 0:02:51  lr: 0.000083  loss: 0.2503 (0.2583)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [590/689]  eta: 0:02:35  lr: 0.000083  loss: 0.2372 (0.2584)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [600/689]  eta: 0:02:20  lr: 0.000083  loss: 0.2520 (0.2584)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [610/689]  eta: 0:02:04  lr: 0.000083  loss: 0.2438 (0.2584)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [620/689]  eta: 0:01:48  lr: 0.000083  loss: 0.2726 (0.2588)  time: 1.5771  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:256]  [630/689]  eta: 0:01:32  lr: 0.000083  loss: 0.2649 (0.2588)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [640/689]  eta: 0:01:17  lr: 0.000083  loss: 0.2568 (0.2586)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [650/689]  eta: 0:01:01  lr: 0.000083  loss: 0.2417 (0.2584)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [660/689]  eta: 0:00:45  lr: 0.000083  loss: 0.2384 (0.2583)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [670/689]  eta: 0:00:29  lr: 0.000083  loss: 0.2384 (0.2580)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [680/689]  eta: 0:00:14  lr: 0.000083  loss: 0.2440 (0.2579)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256]  [688/689]  eta: 0:00:01  lr: 0.000083  loss: 0.2469 (0.2578)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:256] Total time: 0:18:05 (1.5756 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: 0.2469 (0.2578)\n",
      "Valid: [epoch:256]  [ 0/14]  eta: 0:00:14  loss: 0.2539 (0.2539)  time: 1.0161  data: 0.4187  max mem: 39763\n",
      "Valid: [epoch:256]  [13/14]  eta: 0:00:00  loss: 0.2426 (0.2433)  time: 0.1145  data: 0.0300  max mem: 39763\n",
      "Valid: [epoch:256] Total time: 0:00:01 (0.1239 s / it)\n",
      "Averaged stats: loss: 0.2426 (0.2433)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_256_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.243%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:257]  [  0/689]  eta: 0:12:29  lr: 0.000083  loss: 0.2309 (0.2309)  time: 1.0878  data: 0.6107  max mem: 39763\n",
      "Train: [epoch:257]  [ 10/689]  eta: 0:17:18  lr: 0.000083  loss: 0.2432 (0.2454)  time: 1.5289  data: 0.0556  max mem: 39763\n",
      "Train: [epoch:257]  [ 20/689]  eta: 0:17:17  lr: 0.000083  loss: 0.2464 (0.2497)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [ 30/689]  eta: 0:17:06  lr: 0.000083  loss: 0.2585 (0.2575)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [ 40/689]  eta: 0:16:53  lr: 0.000083  loss: 0.2519 (0.2551)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [ 50/689]  eta: 0:16:39  lr: 0.000083  loss: 0.2519 (0.2556)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [ 60/689]  eta: 0:16:25  lr: 0.000083  loss: 0.2523 (0.2547)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [ 70/689]  eta: 0:16:10  lr: 0.000083  loss: 0.2377 (0.2511)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [ 80/689]  eta: 0:15:55  lr: 0.000083  loss: 0.2370 (0.2530)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [ 90/689]  eta: 0:15:39  lr: 0.000083  loss: 0.2467 (0.2532)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [100/689]  eta: 0:15:24  lr: 0.000083  loss: 0.2538 (0.2554)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [110/689]  eta: 0:15:09  lr: 0.000083  loss: 0.2538 (0.2560)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [120/689]  eta: 0:14:53  lr: 0.000083  loss: 0.2434 (0.2553)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [130/689]  eta: 0:14:38  lr: 0.000083  loss: 0.2544 (0.2575)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [140/689]  eta: 0:14:22  lr: 0.000083  loss: 0.2631 (0.2579)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [150/689]  eta: 0:14:07  lr: 0.000083  loss: 0.2521 (0.2576)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [160/689]  eta: 0:13:51  lr: 0.000083  loss: 0.2483 (0.2570)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [170/689]  eta: 0:13:35  lr: 0.000083  loss: 0.2508 (0.2568)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [180/689]  eta: 0:13:20  lr: 0.000083  loss: 0.2478 (0.2564)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [190/689]  eta: 0:13:04  lr: 0.000083  loss: 0.2475 (0.2573)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [200/689]  eta: 0:12:49  lr: 0.000083  loss: 0.2523 (0.2565)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [210/689]  eta: 0:12:33  lr: 0.000083  loss: 0.2363 (0.2561)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [220/689]  eta: 0:12:17  lr: 0.000083  loss: 0.2533 (0.2566)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [230/689]  eta: 0:12:02  lr: 0.000083  loss: 0.2471 (0.2560)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [240/689]  eta: 0:11:46  lr: 0.000083  loss: 0.2471 (0.2565)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [250/689]  eta: 0:11:30  lr: 0.000083  loss: 0.2688 (0.2571)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [260/689]  eta: 0:11:15  lr: 0.000083  loss: 0.2614 (0.2573)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [270/689]  eta: 0:10:59  lr: 0.000083  loss: 0.2479 (0.2569)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [280/689]  eta: 0:10:43  lr: 0.000083  loss: 0.2415 (0.2573)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [290/689]  eta: 0:10:28  lr: 0.000083  loss: 0.2432 (0.2571)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [300/689]  eta: 0:10:12  lr: 0.000083  loss: 0.2432 (0.2570)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [310/689]  eta: 0:09:56  lr: 0.000083  loss: 0.2409 (0.2566)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [320/689]  eta: 0:09:40  lr: 0.000083  loss: 0.2434 (0.2566)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [330/689]  eta: 0:09:25  lr: 0.000083  loss: 0.2529 (0.2566)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [340/689]  eta: 0:09:09  lr: 0.000083  loss: 0.2542 (0.2565)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [350/689]  eta: 0:08:53  lr: 0.000083  loss: 0.2511 (0.2565)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [360/689]  eta: 0:08:38  lr: 0.000083  loss: 0.2530 (0.2569)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [370/689]  eta: 0:08:22  lr: 0.000083  loss: 0.2530 (0.2567)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [380/689]  eta: 0:08:06  lr: 0.000083  loss: 0.2423 (0.2565)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [390/689]  eta: 0:07:50  lr: 0.000083  loss: 0.2400 (0.2562)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [400/689]  eta: 0:07:35  lr: 0.000083  loss: 0.2471 (0.2565)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [410/689]  eta: 0:07:19  lr: 0.000083  loss: 0.2576 (0.2569)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [420/689]  eta: 0:07:03  lr: 0.000083  loss: 0.2598 (0.2570)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [430/689]  eta: 0:06:47  lr: 0.000083  loss: 0.2633 (0.2574)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [440/689]  eta: 0:06:32  lr: 0.000083  loss: 0.2639 (0.2576)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [450/689]  eta: 0:06:16  lr: 0.000083  loss: 0.2588 (0.2577)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [460/689]  eta: 0:06:00  lr: 0.000083  loss: 0.2628 (0.2580)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [470/689]  eta: 0:05:44  lr: 0.000083  loss: 0.2580 (0.2580)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [480/689]  eta: 0:05:29  lr: 0.000083  loss: 0.2505 (0.2580)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [490/689]  eta: 0:05:13  lr: 0.000083  loss: 0.2579 (0.2582)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [500/689]  eta: 0:04:57  lr: 0.000083  loss: 0.2585 (0.2580)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [510/689]  eta: 0:04:41  lr: 0.000083  loss: 0.2458 (0.2580)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [520/689]  eta: 0:04:26  lr: 0.000083  loss: 0.2482 (0.2580)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [530/689]  eta: 0:04:10  lr: 0.000083  loss: 0.2522 (0.2580)  time: 1.5765  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:257]  [540/689]  eta: 0:03:54  lr: 0.000083  loss: 0.2656 (0.2581)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [550/689]  eta: 0:03:38  lr: 0.000083  loss: 0.2507 (0.2579)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [560/689]  eta: 0:03:23  lr: 0.000083  loss: 0.2507 (0.2581)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [570/689]  eta: 0:03:07  lr: 0.000083  loss: 0.2489 (0.2579)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [580/689]  eta: 0:02:51  lr: 0.000083  loss: 0.2408 (0.2578)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [590/689]  eta: 0:02:35  lr: 0.000083  loss: 0.2524 (0.2580)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [600/689]  eta: 0:02:20  lr: 0.000083  loss: 0.2581 (0.2579)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [610/689]  eta: 0:02:04  lr: 0.000083  loss: 0.2581 (0.2580)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [620/689]  eta: 0:01:48  lr: 0.000083  loss: 0.2525 (0.2579)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [630/689]  eta: 0:01:32  lr: 0.000083  loss: 0.2522 (0.2581)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [640/689]  eta: 0:01:17  lr: 0.000083  loss: 0.2522 (0.2581)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [650/689]  eta: 0:01:01  lr: 0.000083  loss: 0.2509 (0.2580)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [660/689]  eta: 0:00:45  lr: 0.000083  loss: 0.2533 (0.2581)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [670/689]  eta: 0:00:29  lr: 0.000083  loss: 0.2591 (0.2582)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [680/689]  eta: 0:00:14  lr: 0.000083  loss: 0.2591 (0.2582)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257]  [688/689]  eta: 0:00:01  lr: 0.000083  loss: 0.2530 (0.2580)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:257] Total time: 0:18:05 (1.5756 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: 0.2530 (0.2580)\n",
      "Valid: [epoch:257]  [ 0/14]  eta: 0:00:14  loss: 0.2628 (0.2628)  time: 1.0165  data: 0.3440  max mem: 39763\n",
      "Valid: [epoch:257]  [13/14]  eta: 0:00:00  loss: 0.2531 (0.2547)  time: 0.1145  data: 0.0246  max mem: 39763\n",
      "Valid: [epoch:257] Total time: 0:00:01 (0.1219 s / it)\n",
      "Averaged stats: loss: 0.2531 (0.2547)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_257_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.255%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:258]  [  0/689]  eta: 0:11:42  lr: 0.000083  loss: 0.3019 (0.3019)  time: 1.0199  data: 0.5418  max mem: 39763\n",
      "Train: [epoch:258]  [ 10/689]  eta: 0:17:14  lr: 0.000083  loss: 0.2890 (0.2674)  time: 1.5235  data: 0.0493  max mem: 39763\n",
      "Train: [epoch:258]  [ 20/689]  eta: 0:17:15  lr: 0.000083  loss: 0.2500 (0.2568)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [ 30/689]  eta: 0:17:05  lr: 0.000083  loss: 0.2518 (0.2600)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [ 40/689]  eta: 0:16:52  lr: 0.000083  loss: 0.2557 (0.2627)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [ 50/689]  eta: 0:16:39  lr: 0.000083  loss: 0.2605 (0.2637)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [ 60/689]  eta: 0:16:24  lr: 0.000083  loss: 0.2444 (0.2637)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [ 70/689]  eta: 0:16:09  lr: 0.000083  loss: 0.2451 (0.2622)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [ 80/689]  eta: 0:15:54  lr: 0.000083  loss: 0.2519 (0.2616)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [ 90/689]  eta: 0:15:39  lr: 0.000083  loss: 0.2484 (0.2608)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [100/689]  eta: 0:15:24  lr: 0.000083  loss: 0.2502 (0.2606)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [110/689]  eta: 0:15:08  lr: 0.000083  loss: 0.2540 (0.2608)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [120/689]  eta: 0:14:53  lr: 0.000083  loss: 0.2503 (0.2601)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [130/689]  eta: 0:14:37  lr: 0.000083  loss: 0.2332 (0.2591)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [140/689]  eta: 0:14:22  lr: 0.000083  loss: 0.2364 (0.2582)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [150/689]  eta: 0:14:06  lr: 0.000083  loss: 0.2538 (0.2599)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [160/689]  eta: 0:13:51  lr: 0.000083  loss: 0.2734 (0.2615)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [170/689]  eta: 0:13:35  lr: 0.000083  loss: 0.2585 (0.2610)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [180/689]  eta: 0:13:20  lr: 0.000083  loss: 0.2455 (0.2603)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [190/689]  eta: 0:13:04  lr: 0.000083  loss: 0.2597 (0.2609)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [200/689]  eta: 0:12:48  lr: 0.000083  loss: 0.2671 (0.2608)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [210/689]  eta: 0:12:33  lr: 0.000083  loss: 0.2564 (0.2605)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [220/689]  eta: 0:12:17  lr: 0.000083  loss: 0.2535 (0.2610)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [230/689]  eta: 0:12:01  lr: 0.000083  loss: 0.2535 (0.2610)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [240/689]  eta: 0:11:46  lr: 0.000083  loss: 0.2532 (0.2612)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [250/689]  eta: 0:11:30  lr: 0.000083  loss: 0.2532 (0.2615)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [260/689]  eta: 0:11:15  lr: 0.000083  loss: 0.2582 (0.2611)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [270/689]  eta: 0:10:59  lr: 0.000083  loss: 0.2464 (0.2607)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [280/689]  eta: 0:10:43  lr: 0.000083  loss: 0.2400 (0.2601)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [290/689]  eta: 0:10:27  lr: 0.000083  loss: 0.2426 (0.2599)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [300/689]  eta: 0:10:12  lr: 0.000083  loss: 0.2456 (0.2598)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [310/689]  eta: 0:09:56  lr: 0.000083  loss: 0.2422 (0.2592)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [320/689]  eta: 0:09:40  lr: 0.000083  loss: 0.2445 (0.2594)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [330/689]  eta: 0:09:25  lr: 0.000083  loss: 0.2549 (0.2596)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [340/689]  eta: 0:09:09  lr: 0.000083  loss: 0.2521 (0.2595)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [350/689]  eta: 0:08:53  lr: 0.000083  loss: 0.2534 (0.2594)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [360/689]  eta: 0:08:37  lr: 0.000083  loss: 0.2608 (0.2599)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [370/689]  eta: 0:08:22  lr: 0.000083  loss: 0.2804 (0.2606)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [380/689]  eta: 0:08:06  lr: 0.000083  loss: 0.2497 (0.2605)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [390/689]  eta: 0:07:50  lr: 0.000083  loss: 0.2474 (0.2604)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [400/689]  eta: 0:07:35  lr: 0.000083  loss: 0.2529 (0.2603)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [410/689]  eta: 0:07:19  lr: 0.000083  loss: 0.2549 (0.2605)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [420/689]  eta: 0:07:03  lr: 0.000083  loss: 0.2554 (0.2604)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [430/689]  eta: 0:06:47  lr: 0.000083  loss: 0.2508 (0.2603)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [440/689]  eta: 0:06:32  lr: 0.000083  loss: 0.2564 (0.2604)  time: 1.5771  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:258]  [450/689]  eta: 0:06:16  lr: 0.000083  loss: 0.2589 (0.2607)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [460/689]  eta: 0:06:00  lr: 0.000083  loss: 0.2643 (0.2611)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [470/689]  eta: 0:05:44  lr: 0.000083  loss: 0.2619 (0.2610)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [480/689]  eta: 0:05:29  lr: 0.000083  loss: 0.2619 (0.2610)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [490/689]  eta: 0:05:13  lr: 0.000083  loss: 0.2500 (0.2608)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [500/689]  eta: 0:04:57  lr: 0.000083  loss: 0.2382 (0.2605)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [510/689]  eta: 0:04:41  lr: 0.000083  loss: 0.2442 (0.2605)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [520/689]  eta: 0:04:26  lr: 0.000083  loss: 0.2523 (0.2605)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [530/689]  eta: 0:04:10  lr: 0.000083  loss: 0.2566 (0.2607)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [540/689]  eta: 0:03:54  lr: 0.000083  loss: 0.2540 (0.2606)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [550/689]  eta: 0:03:38  lr: 0.000083  loss: 0.2488 (0.2606)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [560/689]  eta: 0:03:23  lr: 0.000083  loss: 0.2439 (0.2604)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [570/689]  eta: 0:03:07  lr: 0.000083  loss: 0.2579 (0.2606)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [580/689]  eta: 0:02:51  lr: 0.000083  loss: 0.2519 (0.2602)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [590/689]  eta: 0:02:35  lr: 0.000083  loss: 0.2464 (0.2602)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [600/689]  eta: 0:02:20  lr: 0.000083  loss: 0.2579 (0.2603)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [610/689]  eta: 0:02:04  lr: 0.000083  loss: 0.2409 (0.2599)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [620/689]  eta: 0:01:48  lr: 0.000083  loss: 0.2343 (0.2596)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [630/689]  eta: 0:01:32  lr: 0.000083  loss: 0.2337 (0.2594)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [640/689]  eta: 0:01:17  lr: 0.000083  loss: 0.2424 (0.2594)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [650/689]  eta: 0:01:01  lr: 0.000083  loss: 0.2484 (0.2591)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [660/689]  eta: 0:00:45  lr: 0.000083  loss: 0.2491 (0.2590)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [670/689]  eta: 0:00:29  lr: 0.000083  loss: 0.2522 (0.2592)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [680/689]  eta: 0:00:14  lr: 0.000083  loss: 0.2601 (0.2593)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258]  [688/689]  eta: 0:00:01  lr: 0.000083  loss: 0.2514 (0.2593)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:258] Total time: 0:18:05 (1.5757 s / it)\n",
      "Averaged stats: lr: 0.000083  loss: 0.2514 (0.2593)\n",
      "Valid: [epoch:258]  [ 0/14]  eta: 0:00:14  loss: 0.2672 (0.2672)  time: 1.0119  data: 0.3786  max mem: 39763\n",
      "Valid: [epoch:258]  [13/14]  eta: 0:00:00  loss: 0.2426 (0.2451)  time: 0.1142  data: 0.0271  max mem: 39763\n",
      "Valid: [epoch:258] Total time: 0:00:01 (0.1229 s / it)\n",
      "Averaged stats: loss: 0.2426 (0.2451)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_258_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.245%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:259]  [  0/689]  eta: 0:11:30  lr: 0.000082  loss: 0.2304 (0.2304)  time: 1.0019  data: 0.5262  max mem: 39763\n",
      "Train: [epoch:259]  [ 10/689]  eta: 0:17:12  lr: 0.000082  loss: 0.2457 (0.2486)  time: 1.5205  data: 0.0479  max mem: 39763\n",
      "Train: [epoch:259]  [ 20/689]  eta: 0:17:13  lr: 0.000082  loss: 0.2368 (0.2441)  time: 1.5724  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [ 30/689]  eta: 0:17:04  lr: 0.000082  loss: 0.2426 (0.2532)  time: 1.5730  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [ 40/689]  eta: 0:16:51  lr: 0.000082  loss: 0.2422 (0.2492)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [ 50/689]  eta: 0:16:38  lr: 0.000082  loss: 0.2417 (0.2505)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [ 60/689]  eta: 0:16:23  lr: 0.000082  loss: 0.2400 (0.2477)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [ 70/689]  eta: 0:16:09  lr: 0.000082  loss: 0.2389 (0.2495)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [ 80/689]  eta: 0:15:54  lr: 0.000082  loss: 0.2427 (0.2491)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [ 90/689]  eta: 0:15:39  lr: 0.000082  loss: 0.2412 (0.2484)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [100/689]  eta: 0:15:23  lr: 0.000082  loss: 0.2422 (0.2503)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [110/689]  eta: 0:15:08  lr: 0.000082  loss: 0.2584 (0.2535)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [120/689]  eta: 0:14:53  lr: 0.000082  loss: 0.2679 (0.2557)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [130/689]  eta: 0:14:37  lr: 0.000082  loss: 0.2679 (0.2561)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [140/689]  eta: 0:14:22  lr: 0.000082  loss: 0.2572 (0.2560)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [150/689]  eta: 0:14:06  lr: 0.000082  loss: 0.2569 (0.2567)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [160/689]  eta: 0:13:51  lr: 0.000082  loss: 0.2569 (0.2570)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [170/689]  eta: 0:13:35  lr: 0.000082  loss: 0.2617 (0.2578)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [180/689]  eta: 0:13:20  lr: 0.000082  loss: 0.2636 (0.2584)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [190/689]  eta: 0:13:04  lr: 0.000082  loss: 0.2547 (0.2590)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [200/689]  eta: 0:12:49  lr: 0.000082  loss: 0.2508 (0.2582)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [210/689]  eta: 0:12:33  lr: 0.000082  loss: 0.2376 (0.2583)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [220/689]  eta: 0:12:17  lr: 0.000082  loss: 0.2555 (0.2586)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [230/689]  eta: 0:12:02  lr: 0.000082  loss: 0.2608 (0.2587)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [240/689]  eta: 0:11:46  lr: 0.000082  loss: 0.2675 (0.2596)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [250/689]  eta: 0:11:30  lr: 0.000082  loss: 0.2647 (0.2593)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [260/689]  eta: 0:11:15  lr: 0.000082  loss: 0.2645 (0.2595)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [270/689]  eta: 0:10:59  lr: 0.000082  loss: 0.2645 (0.2596)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [280/689]  eta: 0:10:43  lr: 0.000082  loss: 0.2576 (0.2596)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [290/689]  eta: 0:10:28  lr: 0.000082  loss: 0.2523 (0.2594)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [300/689]  eta: 0:10:12  lr: 0.000082  loss: 0.2496 (0.2594)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [310/689]  eta: 0:09:56  lr: 0.000082  loss: 0.2509 (0.2592)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [320/689]  eta: 0:09:41  lr: 0.000082  loss: 0.2574 (0.2594)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [330/689]  eta: 0:09:25  lr: 0.000082  loss: 0.2574 (0.2594)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [340/689]  eta: 0:09:09  lr: 0.000082  loss: 0.2521 (0.2596)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [350/689]  eta: 0:08:53  lr: 0.000082  loss: 0.2622 (0.2600)  time: 1.5778  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:259]  [360/689]  eta: 0:08:38  lr: 0.000082  loss: 0.2611 (0.2600)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [370/689]  eta: 0:08:22  lr: 0.000082  loss: 0.2505 (0.2599)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [380/689]  eta: 0:08:06  lr: 0.000082  loss: 0.2492 (0.2602)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [390/689]  eta: 0:07:50  lr: 0.000082  loss: 0.2492 (0.2601)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [400/689]  eta: 0:07:35  lr: 0.000082  loss: 0.2556 (0.2604)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [410/689]  eta: 0:07:19  lr: 0.000082  loss: 0.2562 (0.2604)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [420/689]  eta: 0:07:03  lr: 0.000082  loss: 0.2533 (0.2602)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [430/689]  eta: 0:06:47  lr: 0.000082  loss: 0.2512 (0.2602)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [440/689]  eta: 0:06:32  lr: 0.000082  loss: 0.2512 (0.2603)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [450/689]  eta: 0:06:16  lr: 0.000082  loss: 0.2513 (0.2602)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [460/689]  eta: 0:06:00  lr: 0.000082  loss: 0.2572 (0.2604)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [470/689]  eta: 0:05:45  lr: 0.000082  loss: 0.2597 (0.2604)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [480/689]  eta: 0:05:29  lr: 0.000082  loss: 0.2597 (0.2604)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [490/689]  eta: 0:05:13  lr: 0.000082  loss: 0.2596 (0.2606)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [500/689]  eta: 0:04:57  lr: 0.000082  loss: 0.2596 (0.2606)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [510/689]  eta: 0:04:42  lr: 0.000082  loss: 0.2401 (0.2604)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [520/689]  eta: 0:04:26  lr: 0.000082  loss: 0.2401 (0.2602)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [530/689]  eta: 0:04:10  lr: 0.000082  loss: 0.2506 (0.2602)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [540/689]  eta: 0:03:54  lr: 0.000082  loss: 0.2588 (0.2602)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [550/689]  eta: 0:03:39  lr: 0.000082  loss: 0.2554 (0.2603)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [560/689]  eta: 0:03:23  lr: 0.000082  loss: 0.2486 (0.2602)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [570/689]  eta: 0:03:07  lr: 0.000082  loss: 0.2486 (0.2602)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [580/689]  eta: 0:02:51  lr: 0.000082  loss: 0.2480 (0.2601)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [590/689]  eta: 0:02:35  lr: 0.000082  loss: 0.2461 (0.2600)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [600/689]  eta: 0:02:20  lr: 0.000082  loss: 0.2504 (0.2598)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [610/689]  eta: 0:02:04  lr: 0.000082  loss: 0.2557 (0.2600)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [620/689]  eta: 0:01:48  lr: 0.000082  loss: 0.2665 (0.2602)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [630/689]  eta: 0:01:32  lr: 0.000082  loss: 0.2524 (0.2602)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [640/689]  eta: 0:01:17  lr: 0.000082  loss: 0.2524 (0.2604)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [650/689]  eta: 0:01:01  lr: 0.000082  loss: 0.2427 (0.2604)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [660/689]  eta: 0:00:45  lr: 0.000082  loss: 0.2563 (0.2605)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [670/689]  eta: 0:00:29  lr: 0.000082  loss: 0.2457 (0.2604)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [680/689]  eta: 0:00:14  lr: 0.000082  loss: 0.2457 (0.2603)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259]  [688/689]  eta: 0:00:01  lr: 0.000082  loss: 0.2451 (0.2600)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:259] Total time: 0:18:05 (1.5757 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: 0.2451 (0.2600)\n",
      "Valid: [epoch:259]  [ 0/14]  eta: 0:00:14  loss: 0.2259 (0.2259)  time: 1.0088  data: 0.3870  max mem: 39763\n",
      "Valid: [epoch:259]  [13/14]  eta: 0:00:00  loss: 0.2433 (0.2466)  time: 0.1139  data: 0.0277  max mem: 39763\n",
      "Valid: [epoch:259] Total time: 0:00:01 (0.1228 s / it)\n",
      "Averaged stats: loss: 0.2433 (0.2466)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_259_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.247%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:260]  [  0/689]  eta: 0:11:19  lr: 0.000082  loss: 0.2491 (0.2491)  time: 0.9868  data: 0.5108  max mem: 39763\n",
      "Train: [epoch:260]  [ 10/689]  eta: 0:17:12  lr: 0.000082  loss: 0.2361 (0.2388)  time: 1.5209  data: 0.0465  max mem: 39763\n",
      "Train: [epoch:260]  [ 20/689]  eta: 0:17:14  lr: 0.000082  loss: 0.2403 (0.2426)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [ 30/689]  eta: 0:17:05  lr: 0.000082  loss: 0.2447 (0.2506)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [ 40/689]  eta: 0:16:52  lr: 0.000082  loss: 0.2532 (0.2529)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [ 50/689]  eta: 0:16:38  lr: 0.000082  loss: 0.2576 (0.2555)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [ 60/689]  eta: 0:16:24  lr: 0.000082  loss: 0.2500 (0.2550)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [ 70/689]  eta: 0:16:09  lr: 0.000082  loss: 0.2598 (0.2573)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [ 80/689]  eta: 0:15:55  lr: 0.000082  loss: 0.2698 (0.2576)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [ 90/689]  eta: 0:15:39  lr: 0.000082  loss: 0.2551 (0.2574)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [100/689]  eta: 0:15:24  lr: 0.000082  loss: 0.2562 (0.2576)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [110/689]  eta: 0:15:09  lr: 0.000082  loss: 0.2509 (0.2593)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [120/689]  eta: 0:14:53  lr: 0.000082  loss: 0.2509 (0.2603)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [130/689]  eta: 0:14:38  lr: 0.000082  loss: 0.2567 (0.2599)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [140/689]  eta: 0:14:22  lr: 0.000082  loss: 0.2585 (0.2604)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [150/689]  eta: 0:14:07  lr: 0.000082  loss: 0.2539 (0.2593)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [160/689]  eta: 0:13:51  lr: 0.000082  loss: 0.2540 (0.2591)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [170/689]  eta: 0:13:36  lr: 0.000082  loss: 0.2550 (0.2590)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [180/689]  eta: 0:13:20  lr: 0.000082  loss: 0.2570 (0.2592)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [190/689]  eta: 0:13:04  lr: 0.000082  loss: 0.2603 (0.2604)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [200/689]  eta: 0:12:49  lr: 0.000082  loss: 0.2508 (0.2593)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [210/689]  eta: 0:12:33  lr: 0.000082  loss: 0.2425 (0.2588)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [220/689]  eta: 0:12:17  lr: 0.000082  loss: 0.2475 (0.2592)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [230/689]  eta: 0:12:02  lr: 0.000082  loss: 0.2633 (0.2596)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [240/689]  eta: 0:11:46  lr: 0.000082  loss: 0.2677 (0.2606)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [250/689]  eta: 0:11:30  lr: 0.000082  loss: 0.2651 (0.2607)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [260/689]  eta: 0:11:15  lr: 0.000082  loss: 0.2644 (0.2609)  time: 1.5764  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:260]  [270/689]  eta: 0:10:59  lr: 0.000082  loss: 0.2690 (0.2612)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [280/689]  eta: 0:10:43  lr: 0.000082  loss: 0.2510 (0.2608)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [290/689]  eta: 0:10:28  lr: 0.000082  loss: 0.2502 (0.2603)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [300/689]  eta: 0:10:12  lr: 0.000082  loss: 0.2513 (0.2603)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [310/689]  eta: 0:09:56  lr: 0.000082  loss: 0.2480 (0.2600)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [320/689]  eta: 0:09:40  lr: 0.000082  loss: 0.2480 (0.2602)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [330/689]  eta: 0:09:25  lr: 0.000082  loss: 0.2469 (0.2599)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [340/689]  eta: 0:09:09  lr: 0.000082  loss: 0.2545 (0.2600)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [350/689]  eta: 0:08:53  lr: 0.000082  loss: 0.2565 (0.2599)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [360/689]  eta: 0:08:38  lr: 0.000082  loss: 0.2553 (0.2599)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [370/689]  eta: 0:08:22  lr: 0.000082  loss: 0.2489 (0.2598)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [380/689]  eta: 0:08:06  lr: 0.000082  loss: 0.2494 (0.2597)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [390/689]  eta: 0:07:50  lr: 0.000082  loss: 0.2603 (0.2600)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [400/689]  eta: 0:07:35  lr: 0.000082  loss: 0.2637 (0.2601)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [410/689]  eta: 0:07:19  lr: 0.000082  loss: 0.2545 (0.2598)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [420/689]  eta: 0:07:03  lr: 0.000082  loss: 0.2457 (0.2598)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [430/689]  eta: 0:06:47  lr: 0.000082  loss: 0.2420 (0.2595)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [440/689]  eta: 0:06:32  lr: 0.000082  loss: 0.2497 (0.2599)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [450/689]  eta: 0:06:16  lr: 0.000082  loss: 0.2592 (0.2598)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [460/689]  eta: 0:06:00  lr: 0.000082  loss: 0.2599 (0.2597)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [470/689]  eta: 0:05:44  lr: 0.000082  loss: 0.2605 (0.2597)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [480/689]  eta: 0:05:29  lr: 0.000082  loss: 0.2520 (0.2598)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [490/689]  eta: 0:05:13  lr: 0.000082  loss: 0.2613 (0.2600)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [500/689]  eta: 0:04:57  lr: 0.000082  loss: 0.2619 (0.2603)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [510/689]  eta: 0:04:41  lr: 0.000082  loss: 0.2619 (0.2604)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [520/689]  eta: 0:04:26  lr: 0.000082  loss: 0.2611 (0.2606)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [530/689]  eta: 0:04:10  lr: 0.000082  loss: 0.2615 (0.2608)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [540/689]  eta: 0:03:54  lr: 0.000082  loss: 0.2573 (0.2605)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [550/689]  eta: 0:03:38  lr: 0.000082  loss: 0.2573 (0.2608)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [560/689]  eta: 0:03:23  lr: 0.000082  loss: 0.2751 (0.2609)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [570/689]  eta: 0:03:07  lr: 0.000082  loss: 0.2672 (0.2609)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [580/689]  eta: 0:02:51  lr: 0.000082  loss: 0.2525 (0.2608)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [590/689]  eta: 0:02:35  lr: 0.000082  loss: 0.2496 (0.2609)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [600/689]  eta: 0:02:20  lr: 0.000082  loss: 0.2546 (0.2610)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [610/689]  eta: 0:02:04  lr: 0.000082  loss: 0.2523 (0.2607)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [620/689]  eta: 0:01:48  lr: 0.000082  loss: 0.2511 (0.2608)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [630/689]  eta: 0:01:32  lr: 0.000082  loss: 0.2577 (0.2607)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [640/689]  eta: 0:01:17  lr: 0.000082  loss: 0.2571 (0.2609)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [650/689]  eta: 0:01:01  lr: 0.000082  loss: 0.2613 (0.2608)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [660/689]  eta: 0:00:45  lr: 0.000082  loss: 0.2621 (0.2610)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [670/689]  eta: 0:00:29  lr: 0.000082  loss: 0.2617 (0.2610)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [680/689]  eta: 0:00:14  lr: 0.000082  loss: 0.2478 (0.2607)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260]  [688/689]  eta: 0:00:01  lr: 0.000082  loss: 0.2493 (0.2606)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:260] Total time: 0:18:05 (1.5756 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: 0.2493 (0.2606)\n",
      "Valid: [epoch:260]  [ 0/14]  eta: 0:00:14  loss: 0.2577 (0.2577)  time: 1.0421  data: 0.3424  max mem: 39763\n",
      "Valid: [epoch:260]  [13/14]  eta: 0:00:00  loss: 0.2444 (0.2462)  time: 0.1163  data: 0.0245  max mem: 39763\n",
      "Valid: [epoch:260] Total time: 0:00:01 (0.1256 s / it)\n",
      "Averaged stats: loss: 0.2444 (0.2462)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_260_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.246%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:261]  [  0/689]  eta: 0:11:43  lr: 0.000082  loss: 0.1914 (0.1914)  time: 1.0207  data: 0.5430  max mem: 39763\n",
      "Train: [epoch:261]  [ 10/689]  eta: 0:17:13  lr: 0.000082  loss: 0.2596 (0.2690)  time: 1.5223  data: 0.0494  max mem: 39763\n",
      "Train: [epoch:261]  [ 20/689]  eta: 0:17:15  lr: 0.000082  loss: 0.2473 (0.2581)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [ 30/689]  eta: 0:17:05  lr: 0.000082  loss: 0.2510 (0.2658)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [ 40/689]  eta: 0:16:52  lr: 0.000082  loss: 0.2631 (0.2637)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [ 50/689]  eta: 0:16:39  lr: 0.000082  loss: 0.2467 (0.2632)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [ 60/689]  eta: 0:16:24  lr: 0.000082  loss: 0.2536 (0.2624)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [ 70/689]  eta: 0:16:10  lr: 0.000082  loss: 0.2636 (0.2630)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [ 80/689]  eta: 0:15:55  lr: 0.000082  loss: 0.2650 (0.2631)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [ 90/689]  eta: 0:15:39  lr: 0.000082  loss: 0.2622 (0.2622)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [100/689]  eta: 0:15:24  lr: 0.000082  loss: 0.2620 (0.2638)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [110/689]  eta: 0:15:09  lr: 0.000082  loss: 0.2655 (0.2635)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [120/689]  eta: 0:14:54  lr: 0.000082  loss: 0.2461 (0.2626)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [130/689]  eta: 0:14:38  lr: 0.000082  loss: 0.2446 (0.2623)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [140/689]  eta: 0:14:23  lr: 0.000082  loss: 0.2555 (0.2635)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [150/689]  eta: 0:14:07  lr: 0.000082  loss: 0.2612 (0.2631)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [160/689]  eta: 0:13:51  lr: 0.000082  loss: 0.2571 (0.2628)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [170/689]  eta: 0:13:36  lr: 0.000082  loss: 0.2574 (0.2632)  time: 1.5774  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:261]  [180/689]  eta: 0:13:20  lr: 0.000082  loss: 0.2511 (0.2622)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [190/689]  eta: 0:13:05  lr: 0.000082  loss: 0.2484 (0.2619)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [200/689]  eta: 0:12:49  lr: 0.000082  loss: 0.2487 (0.2616)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [210/689]  eta: 0:12:33  lr: 0.000082  loss: 0.2384 (0.2613)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [220/689]  eta: 0:12:18  lr: 0.000082  loss: 0.2520 (0.2611)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [230/689]  eta: 0:12:02  lr: 0.000082  loss: 0.2499 (0.2602)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [240/689]  eta: 0:11:46  lr: 0.000082  loss: 0.2502 (0.2607)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [250/689]  eta: 0:11:31  lr: 0.000082  loss: 0.2683 (0.2609)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [260/689]  eta: 0:11:15  lr: 0.000082  loss: 0.2519 (0.2603)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [270/689]  eta: 0:10:59  lr: 0.000082  loss: 0.2436 (0.2600)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [280/689]  eta: 0:10:43  lr: 0.000082  loss: 0.2530 (0.2609)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [290/689]  eta: 0:10:28  lr: 0.000082  loss: 0.2615 (0.2607)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [300/689]  eta: 0:10:12  lr: 0.000082  loss: 0.2537 (0.2612)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [310/689]  eta: 0:09:56  lr: 0.000082  loss: 0.2544 (0.2611)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [320/689]  eta: 0:09:41  lr: 0.000082  loss: 0.2534 (0.2612)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [330/689]  eta: 0:09:25  lr: 0.000082  loss: 0.2534 (0.2613)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [340/689]  eta: 0:09:09  lr: 0.000082  loss: 0.2596 (0.2613)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [350/689]  eta: 0:08:53  lr: 0.000082  loss: 0.2559 (0.2615)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [360/689]  eta: 0:08:38  lr: 0.000082  loss: 0.2470 (0.2614)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [370/689]  eta: 0:08:22  lr: 0.000082  loss: 0.2506 (0.2611)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [380/689]  eta: 0:08:06  lr: 0.000082  loss: 0.2509 (0.2611)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [390/689]  eta: 0:07:50  lr: 0.000082  loss: 0.2526 (0.2611)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [400/689]  eta: 0:07:35  lr: 0.000082  loss: 0.2647 (0.2614)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [410/689]  eta: 0:07:19  lr: 0.000082  loss: 0.2625 (0.2611)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [420/689]  eta: 0:07:03  lr: 0.000082  loss: 0.2418 (0.2607)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [430/689]  eta: 0:06:47  lr: 0.000082  loss: 0.2482 (0.2608)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [440/689]  eta: 0:06:32  lr: 0.000082  loss: 0.2491 (0.2605)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [450/689]  eta: 0:06:16  lr: 0.000082  loss: 0.2449 (0.2604)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [460/689]  eta: 0:06:00  lr: 0.000082  loss: 0.2455 (0.2602)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [470/689]  eta: 0:05:44  lr: 0.000082  loss: 0.2492 (0.2607)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [480/689]  eta: 0:05:29  lr: 0.000082  loss: 0.2641 (0.2608)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [490/689]  eta: 0:05:13  lr: 0.000082  loss: 0.2510 (0.2605)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [500/689]  eta: 0:04:57  lr: 0.000082  loss: 0.2497 (0.2606)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [510/689]  eta: 0:04:41  lr: 0.000082  loss: 0.2558 (0.2605)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [520/689]  eta: 0:04:26  lr: 0.000082  loss: 0.2530 (0.2604)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [530/689]  eta: 0:04:10  lr: 0.000082  loss: 0.2481 (0.2601)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [540/689]  eta: 0:03:54  lr: 0.000082  loss: 0.2481 (0.2599)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [550/689]  eta: 0:03:38  lr: 0.000082  loss: 0.2502 (0.2600)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [560/689]  eta: 0:03:23  lr: 0.000082  loss: 0.2615 (0.2600)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [570/689]  eta: 0:03:07  lr: 0.000082  loss: 0.2692 (0.2605)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [580/689]  eta: 0:02:51  lr: 0.000082  loss: 0.2620 (0.2604)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [590/689]  eta: 0:02:35  lr: 0.000082  loss: 0.2563 (0.2604)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [600/689]  eta: 0:02:20  lr: 0.000082  loss: 0.2587 (0.2606)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [610/689]  eta: 0:02:04  lr: 0.000082  loss: 0.2811 (0.2609)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [620/689]  eta: 0:01:48  lr: 0.000082  loss: 0.2811 (0.2611)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [630/689]  eta: 0:01:32  lr: 0.000082  loss: 0.2694 (0.2614)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [640/689]  eta: 0:01:17  lr: 0.000082  loss: 0.2673 (0.2615)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [650/689]  eta: 0:01:01  lr: 0.000082  loss: 0.2673 (0.2617)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [660/689]  eta: 0:00:45  lr: 0.000082  loss: 0.2512 (0.2618)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [670/689]  eta: 0:00:29  lr: 0.000082  loss: 0.2501 (0.2617)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [680/689]  eta: 0:00:14  lr: 0.000082  loss: 0.2534 (0.2617)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261]  [688/689]  eta: 0:00:01  lr: 0.000082  loss: 0.2565 (0.2615)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:261] Total time: 0:18:05 (1.5753 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: 0.2565 (0.2615)\n",
      "Valid: [epoch:261]  [ 0/14]  eta: 0:00:14  loss: 0.2462 (0.2462)  time: 1.0173  data: 0.3714  max mem: 39763\n",
      "Valid: [epoch:261]  [13/14]  eta: 0:00:00  loss: 0.2462 (0.2483)  time: 0.1147  data: 0.0266  max mem: 39763\n",
      "Valid: [epoch:261] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.2462 (0.2483)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_261_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.248%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:262]  [  0/689]  eta: 0:11:58  lr: 0.000082  loss: 0.2629 (0.2629)  time: 1.0428  data: 0.5635  max mem: 39763\n",
      "Train: [epoch:262]  [ 10/689]  eta: 0:17:16  lr: 0.000082  loss: 0.2699 (0.2757)  time: 1.5260  data: 0.0513  max mem: 39763\n",
      "Train: [epoch:262]  [ 20/689]  eta: 0:17:17  lr: 0.000082  loss: 0.2575 (0.2663)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [ 30/689]  eta: 0:17:07  lr: 0.000082  loss: 0.2484 (0.2641)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [ 40/689]  eta: 0:16:54  lr: 0.000082  loss: 0.2538 (0.2626)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [ 50/689]  eta: 0:16:40  lr: 0.000082  loss: 0.2538 (0.2615)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [ 60/689]  eta: 0:16:25  lr: 0.000082  loss: 0.2478 (0.2605)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [ 70/689]  eta: 0:16:10  lr: 0.000082  loss: 0.2450 (0.2602)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [ 80/689]  eta: 0:15:55  lr: 0.000082  loss: 0.2578 (0.2615)  time: 1.5768  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:262]  [ 90/689]  eta: 0:15:40  lr: 0.000082  loss: 0.2644 (0.2624)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [100/689]  eta: 0:15:25  lr: 0.000082  loss: 0.2447 (0.2602)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [110/689]  eta: 0:15:09  lr: 0.000082  loss: 0.2447 (0.2610)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [120/689]  eta: 0:14:54  lr: 0.000082  loss: 0.2638 (0.2610)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [130/689]  eta: 0:14:38  lr: 0.000082  loss: 0.2451 (0.2597)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [140/689]  eta: 0:14:23  lr: 0.000082  loss: 0.2676 (0.2607)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [150/689]  eta: 0:14:07  lr: 0.000082  loss: 0.2704 (0.2609)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [160/689]  eta: 0:13:52  lr: 0.000082  loss: 0.2624 (0.2608)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [170/689]  eta: 0:13:36  lr: 0.000082  loss: 0.2471 (0.2600)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [180/689]  eta: 0:13:20  lr: 0.000082  loss: 0.2484 (0.2609)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [190/689]  eta: 0:13:05  lr: 0.000082  loss: 0.2524 (0.2607)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [200/689]  eta: 0:12:49  lr: 0.000082  loss: 0.2467 (0.2599)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [210/689]  eta: 0:12:33  lr: 0.000082  loss: 0.2422 (0.2596)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [220/689]  eta: 0:12:18  lr: 0.000082  loss: 0.2576 (0.2603)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [230/689]  eta: 0:12:02  lr: 0.000082  loss: 0.2550 (0.2596)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [240/689]  eta: 0:11:46  lr: 0.000082  loss: 0.2495 (0.2599)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [250/689]  eta: 0:11:31  lr: 0.000082  loss: 0.2549 (0.2607)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [260/689]  eta: 0:11:15  lr: 0.000082  loss: 0.2549 (0.2608)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [270/689]  eta: 0:10:59  lr: 0.000082  loss: 0.2507 (0.2606)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [280/689]  eta: 0:10:44  lr: 0.000082  loss: 0.2522 (0.2610)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [290/689]  eta: 0:10:28  lr: 0.000082  loss: 0.2522 (0.2611)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [300/689]  eta: 0:10:12  lr: 0.000082  loss: 0.2558 (0.2614)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [310/689]  eta: 0:09:56  lr: 0.000082  loss: 0.2593 (0.2618)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [320/689]  eta: 0:09:41  lr: 0.000082  loss: 0.2567 (0.2620)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [330/689]  eta: 0:09:25  lr: 0.000082  loss: 0.2623 (0.2626)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [340/689]  eta: 0:09:09  lr: 0.000082  loss: 0.2623 (0.2627)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [350/689]  eta: 0:08:53  lr: 0.000082  loss: 0.2639 (0.2631)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [360/689]  eta: 0:08:38  lr: 0.000082  loss: 0.2601 (0.2632)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [370/689]  eta: 0:08:22  lr: 0.000082  loss: 0.2585 (0.2633)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [380/689]  eta: 0:08:06  lr: 0.000082  loss: 0.2545 (0.2633)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [390/689]  eta: 0:07:50  lr: 0.000082  loss: 0.2564 (0.2635)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [400/689]  eta: 0:07:35  lr: 0.000082  loss: 0.2642 (0.2634)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [410/689]  eta: 0:07:19  lr: 0.000082  loss: 0.2608 (0.2635)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [420/689]  eta: 0:07:03  lr: 0.000082  loss: 0.2529 (0.2634)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [430/689]  eta: 0:06:47  lr: 0.000082  loss: 0.2588 (0.2639)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [440/689]  eta: 0:06:32  lr: 0.000082  loss: 0.2588 (0.2638)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [450/689]  eta: 0:06:16  lr: 0.000082  loss: 0.2584 (0.2639)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [460/689]  eta: 0:06:00  lr: 0.000082  loss: 0.2694 (0.2641)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [470/689]  eta: 0:05:44  lr: 0.000082  loss: 0.2641 (0.2638)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [480/689]  eta: 0:05:29  lr: 0.000082  loss: 0.2484 (0.2636)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [490/689]  eta: 0:05:13  lr: 0.000082  loss: 0.2541 (0.2635)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [500/689]  eta: 0:04:57  lr: 0.000082  loss: 0.2570 (0.2636)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [510/689]  eta: 0:04:41  lr: 0.000082  loss: 0.2573 (0.2636)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [520/689]  eta: 0:04:26  lr: 0.000082  loss: 0.2613 (0.2637)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [530/689]  eta: 0:04:10  lr: 0.000082  loss: 0.2613 (0.2636)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [540/689]  eta: 0:03:54  lr: 0.000082  loss: 0.2536 (0.2633)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [550/689]  eta: 0:03:38  lr: 0.000082  loss: 0.2538 (0.2633)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [560/689]  eta: 0:03:23  lr: 0.000082  loss: 0.2522 (0.2633)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [570/689]  eta: 0:03:07  lr: 0.000082  loss: 0.2468 (0.2631)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [580/689]  eta: 0:02:51  lr: 0.000082  loss: 0.2478 (0.2632)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [590/689]  eta: 0:02:35  lr: 0.000082  loss: 0.2593 (0.2631)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [600/689]  eta: 0:02:20  lr: 0.000082  loss: 0.2593 (0.2630)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [610/689]  eta: 0:02:04  lr: 0.000082  loss: 0.2548 (0.2628)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [620/689]  eta: 0:01:48  lr: 0.000082  loss: 0.2563 (0.2628)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [630/689]  eta: 0:01:32  lr: 0.000082  loss: 0.2628 (0.2628)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [640/689]  eta: 0:01:17  lr: 0.000082  loss: 0.2599 (0.2628)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [650/689]  eta: 0:01:01  lr: 0.000082  loss: 0.2591 (0.2628)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [660/689]  eta: 0:00:45  lr: 0.000082  loss: 0.2591 (0.2628)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [670/689]  eta: 0:00:29  lr: 0.000082  loss: 0.2741 (0.2629)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [680/689]  eta: 0:00:14  lr: 0.000082  loss: 0.2653 (0.2628)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262]  [688/689]  eta: 0:00:01  lr: 0.000082  loss: 0.2607 (0.2627)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:262] Total time: 0:18:05 (1.5755 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: 0.2607 (0.2627)\n",
      "Valid: [epoch:262]  [ 0/14]  eta: 0:00:14  loss: 0.2708 (0.2708)  time: 1.0079  data: 0.3814  max mem: 39763\n",
      "Valid: [epoch:262]  [13/14]  eta: 0:00:00  loss: 0.2457 (0.2489)  time: 0.1139  data: 0.0273  max mem: 39763\n",
      "Valid: [epoch:262] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.2457 (0.2489)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_262_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.249%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:263]  [  0/689]  eta: 0:12:18  lr: 0.000082  loss: 0.2919 (0.2919)  time: 1.0713  data: 0.5947  max mem: 39763\n",
      "Train: [epoch:263]  [ 10/689]  eta: 0:17:17  lr: 0.000082  loss: 0.2536 (0.2620)  time: 1.5282  data: 0.0541  max mem: 39763\n",
      "Train: [epoch:263]  [ 20/689]  eta: 0:17:16  lr: 0.000082  loss: 0.2573 (0.2628)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [ 30/689]  eta: 0:17:06  lr: 0.000082  loss: 0.2591 (0.2621)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [ 40/689]  eta: 0:16:53  lr: 0.000082  loss: 0.2511 (0.2589)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [ 50/689]  eta: 0:16:39  lr: 0.000082  loss: 0.2446 (0.2574)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [ 60/689]  eta: 0:16:25  lr: 0.000082  loss: 0.2581 (0.2605)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [ 70/689]  eta: 0:16:10  lr: 0.000082  loss: 0.2681 (0.2624)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [ 80/689]  eta: 0:15:55  lr: 0.000082  loss: 0.2544 (0.2604)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [ 90/689]  eta: 0:15:40  lr: 0.000082  loss: 0.2465 (0.2600)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [100/689]  eta: 0:15:25  lr: 0.000082  loss: 0.2616 (0.2609)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [110/689]  eta: 0:15:09  lr: 0.000082  loss: 0.2729 (0.2614)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [120/689]  eta: 0:14:54  lr: 0.000082  loss: 0.2583 (0.2606)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [130/689]  eta: 0:14:38  lr: 0.000082  loss: 0.2527 (0.2608)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [140/689]  eta: 0:14:23  lr: 0.000082  loss: 0.2590 (0.2611)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [150/689]  eta: 0:14:07  lr: 0.000082  loss: 0.2531 (0.2612)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [160/689]  eta: 0:13:51  lr: 0.000082  loss: 0.2552 (0.2623)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [170/689]  eta: 0:13:36  lr: 0.000082  loss: 0.2552 (0.2626)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [180/689]  eta: 0:13:20  lr: 0.000082  loss: 0.2637 (0.2630)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [190/689]  eta: 0:13:05  lr: 0.000082  loss: 0.2658 (0.2637)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [200/689]  eta: 0:12:49  lr: 0.000082  loss: 0.2618 (0.2641)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [210/689]  eta: 0:12:33  lr: 0.000082  loss: 0.2593 (0.2645)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [220/689]  eta: 0:12:18  lr: 0.000082  loss: 0.2720 (0.2651)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [230/689]  eta: 0:12:02  lr: 0.000082  loss: 0.2610 (0.2649)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [240/689]  eta: 0:11:46  lr: 0.000082  loss: 0.2610 (0.2648)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [250/689]  eta: 0:11:30  lr: 0.000082  loss: 0.2557 (0.2641)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [260/689]  eta: 0:11:15  lr: 0.000082  loss: 0.2635 (0.2653)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [270/689]  eta: 0:10:59  lr: 0.000082  loss: 0.2632 (0.2645)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [280/689]  eta: 0:10:43  lr: 0.000082  loss: 0.2618 (0.2649)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [290/689]  eta: 0:10:28  lr: 0.000082  loss: 0.2787 (0.2654)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [300/689]  eta: 0:10:12  lr: 0.000082  loss: 0.2689 (0.2652)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [310/689]  eta: 0:09:56  lr: 0.000082  loss: 0.2479 (0.2648)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [320/689]  eta: 0:09:40  lr: 0.000082  loss: 0.2604 (0.2654)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [330/689]  eta: 0:09:25  lr: 0.000082  loss: 0.2595 (0.2649)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [340/689]  eta: 0:09:09  lr: 0.000082  loss: 0.2572 (0.2653)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [350/689]  eta: 0:08:53  lr: 0.000082  loss: 0.2589 (0.2651)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [360/689]  eta: 0:08:38  lr: 0.000082  loss: 0.2533 (0.2652)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [370/689]  eta: 0:08:22  lr: 0.000082  loss: 0.2541 (0.2650)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [380/689]  eta: 0:08:06  lr: 0.000082  loss: 0.2537 (0.2648)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [390/689]  eta: 0:07:50  lr: 0.000082  loss: 0.2639 (0.2648)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [400/689]  eta: 0:07:35  lr: 0.000082  loss: 0.2682 (0.2649)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [410/689]  eta: 0:07:19  lr: 0.000082  loss: 0.2534 (0.2646)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [420/689]  eta: 0:07:03  lr: 0.000082  loss: 0.2534 (0.2645)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [430/689]  eta: 0:06:47  lr: 0.000082  loss: 0.2546 (0.2644)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [440/689]  eta: 0:06:32  lr: 0.000082  loss: 0.2475 (0.2644)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [450/689]  eta: 0:06:16  lr: 0.000082  loss: 0.2559 (0.2647)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [460/689]  eta: 0:06:00  lr: 0.000082  loss: 0.2765 (0.2650)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [470/689]  eta: 0:05:44  lr: 0.000082  loss: 0.2674 (0.2648)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [480/689]  eta: 0:05:29  lr: 0.000082  loss: 0.2346 (0.2642)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [490/689]  eta: 0:05:13  lr: 0.000082  loss: 0.2455 (0.2642)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [500/689]  eta: 0:04:57  lr: 0.000082  loss: 0.2639 (0.2644)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [510/689]  eta: 0:04:41  lr: 0.000082  loss: 0.2631 (0.2643)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [520/689]  eta: 0:04:26  lr: 0.000082  loss: 0.2579 (0.2642)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [530/689]  eta: 0:04:10  lr: 0.000082  loss: 0.2487 (0.2639)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [540/689]  eta: 0:03:54  lr: 0.000082  loss: 0.2582 (0.2641)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [550/689]  eta: 0:03:38  lr: 0.000082  loss: 0.2703 (0.2640)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [560/689]  eta: 0:03:23  lr: 0.000082  loss: 0.2542 (0.2642)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [570/689]  eta: 0:03:07  lr: 0.000082  loss: 0.2485 (0.2640)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [580/689]  eta: 0:02:51  lr: 0.000082  loss: 0.2402 (0.2637)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [590/689]  eta: 0:02:35  lr: 0.000082  loss: 0.2501 (0.2637)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [600/689]  eta: 0:02:20  lr: 0.000082  loss: 0.2529 (0.2637)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [610/689]  eta: 0:02:04  lr: 0.000082  loss: 0.2607 (0.2637)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [620/689]  eta: 0:01:48  lr: 0.000082  loss: 0.2446 (0.2634)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [630/689]  eta: 0:01:32  lr: 0.000082  loss: 0.2516 (0.2636)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [640/689]  eta: 0:01:17  lr: 0.000082  loss: 0.2587 (0.2636)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [650/689]  eta: 0:01:01  lr: 0.000082  loss: 0.2508 (0.2635)  time: 1.5767  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:263]  [660/689]  eta: 0:00:45  lr: 0.000082  loss: 0.2532 (0.2637)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [670/689]  eta: 0:00:29  lr: 0.000082  loss: 0.2691 (0.2638)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [680/689]  eta: 0:00:14  lr: 0.000082  loss: 0.2575 (0.2636)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263]  [688/689]  eta: 0:00:01  lr: 0.000082  loss: 0.2413 (0.2634)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:263] Total time: 0:18:05 (1.5755 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: 0.2413 (0.2634)\n",
      "Valid: [epoch:263]  [ 0/14]  eta: 0:00:14  loss: 0.2658 (0.2658)  time: 1.0011  data: 0.3840  max mem: 39763\n",
      "Valid: [epoch:263]  [13/14]  eta: 0:00:00  loss: 0.2467 (0.2506)  time: 0.1135  data: 0.0275  max mem: 39763\n",
      "Valid: [epoch:263] Total time: 0:00:01 (0.1225 s / it)\n",
      "Averaged stats: loss: 0.2467 (0.2506)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_263_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.251%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:264]  [  0/689]  eta: 0:12:24  lr: 0.000082  loss: 0.2810 (0.2810)  time: 1.0812  data: 0.5928  max mem: 39763\n",
      "Train: [epoch:264]  [ 10/689]  eta: 0:17:18  lr: 0.000082  loss: 0.2718 (0.2741)  time: 1.5296  data: 0.0540  max mem: 39763\n",
      "Train: [epoch:264]  [ 20/689]  eta: 0:17:18  lr: 0.000082  loss: 0.2628 (0.2680)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [ 30/689]  eta: 0:17:07  lr: 0.000082  loss: 0.2629 (0.2680)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [ 40/689]  eta: 0:16:54  lr: 0.000082  loss: 0.2656 (0.2657)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [ 50/689]  eta: 0:16:40  lr: 0.000082  loss: 0.2463 (0.2625)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [ 60/689]  eta: 0:16:25  lr: 0.000082  loss: 0.2468 (0.2652)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [ 70/689]  eta: 0:16:10  lr: 0.000082  loss: 0.2612 (0.2653)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [ 80/689]  eta: 0:15:55  lr: 0.000082  loss: 0.2494 (0.2627)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [ 90/689]  eta: 0:15:40  lr: 0.000082  loss: 0.2477 (0.2627)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [100/689]  eta: 0:15:25  lr: 0.000082  loss: 0.2626 (0.2642)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [110/689]  eta: 0:15:09  lr: 0.000082  loss: 0.2656 (0.2643)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [120/689]  eta: 0:14:54  lr: 0.000082  loss: 0.2558 (0.2641)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [130/689]  eta: 0:14:38  lr: 0.000082  loss: 0.2510 (0.2638)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [140/689]  eta: 0:14:22  lr: 0.000082  loss: 0.2609 (0.2639)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [150/689]  eta: 0:14:07  lr: 0.000082  loss: 0.2628 (0.2650)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [160/689]  eta: 0:13:51  lr: 0.000082  loss: 0.2602 (0.2660)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [170/689]  eta: 0:13:36  lr: 0.000082  loss: 0.2636 (0.2662)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [180/689]  eta: 0:13:20  lr: 0.000082  loss: 0.2724 (0.2670)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [190/689]  eta: 0:13:04  lr: 0.000082  loss: 0.2635 (0.2663)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [200/689]  eta: 0:12:49  lr: 0.000082  loss: 0.2594 (0.2670)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [210/689]  eta: 0:12:33  lr: 0.000082  loss: 0.2604 (0.2663)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [220/689]  eta: 0:12:17  lr: 0.000082  loss: 0.2573 (0.2662)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [230/689]  eta: 0:12:02  lr: 0.000082  loss: 0.2556 (0.2658)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [240/689]  eta: 0:11:46  lr: 0.000082  loss: 0.2556 (0.2660)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [250/689]  eta: 0:11:30  lr: 0.000082  loss: 0.2519 (0.2658)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [260/689]  eta: 0:11:15  lr: 0.000082  loss: 0.2552 (0.2661)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [270/689]  eta: 0:10:59  lr: 0.000082  loss: 0.2583 (0.2663)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [280/689]  eta: 0:10:43  lr: 0.000082  loss: 0.2764 (0.2667)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [290/689]  eta: 0:10:27  lr: 0.000082  loss: 0.2552 (0.2663)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [300/689]  eta: 0:10:12  lr: 0.000082  loss: 0.2552 (0.2658)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [310/689]  eta: 0:09:56  lr: 0.000082  loss: 0.2449 (0.2652)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [320/689]  eta: 0:09:40  lr: 0.000082  loss: 0.2438 (0.2648)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [330/689]  eta: 0:09:25  lr: 0.000082  loss: 0.2519 (0.2648)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [340/689]  eta: 0:09:09  lr: 0.000082  loss: 0.2572 (0.2648)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [350/689]  eta: 0:08:53  lr: 0.000082  loss: 0.2556 (0.2646)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [360/689]  eta: 0:08:37  lr: 0.000082  loss: 0.2597 (0.2648)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [370/689]  eta: 0:08:22  lr: 0.000082  loss: 0.2544 (0.2647)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [380/689]  eta: 0:08:06  lr: 0.000082  loss: 0.2508 (0.2649)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [390/689]  eta: 0:07:50  lr: 0.000082  loss: 0.2567 (0.2647)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [400/689]  eta: 0:07:34  lr: 0.000082  loss: 0.2671 (0.2649)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [410/689]  eta: 0:07:19  lr: 0.000082  loss: 0.2671 (0.2648)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [420/689]  eta: 0:07:03  lr: 0.000082  loss: 0.2520 (0.2646)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [430/689]  eta: 0:06:47  lr: 0.000082  loss: 0.2634 (0.2649)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [440/689]  eta: 0:06:31  lr: 0.000082  loss: 0.2605 (0.2649)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [450/689]  eta: 0:06:16  lr: 0.000082  loss: 0.2583 (0.2650)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [460/689]  eta: 0:06:00  lr: 0.000082  loss: 0.2673 (0.2655)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [470/689]  eta: 0:05:44  lr: 0.000082  loss: 0.2620 (0.2653)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [480/689]  eta: 0:05:29  lr: 0.000082  loss: 0.2441 (0.2652)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [490/689]  eta: 0:05:13  lr: 0.000082  loss: 0.2568 (0.2651)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [500/689]  eta: 0:04:57  lr: 0.000082  loss: 0.2500 (0.2651)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [510/689]  eta: 0:04:41  lr: 0.000082  loss: 0.2688 (0.2654)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [520/689]  eta: 0:04:26  lr: 0.000082  loss: 0.2688 (0.2653)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [530/689]  eta: 0:04:10  lr: 0.000082  loss: 0.2572 (0.2654)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [540/689]  eta: 0:03:54  lr: 0.000082  loss: 0.2576 (0.2655)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [550/689]  eta: 0:03:38  lr: 0.000082  loss: 0.2576 (0.2653)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [560/689]  eta: 0:03:23  lr: 0.000082  loss: 0.2551 (0.2651)  time: 1.5763  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:264]  [570/689]  eta: 0:03:07  lr: 0.000082  loss: 0.2569 (0.2653)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [580/689]  eta: 0:02:51  lr: 0.000082  loss: 0.2617 (0.2654)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [590/689]  eta: 0:02:35  lr: 0.000082  loss: 0.2594 (0.2654)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [600/689]  eta: 0:02:20  lr: 0.000082  loss: 0.2523 (0.2650)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [610/689]  eta: 0:02:04  lr: 0.000082  loss: 0.2440 (0.2649)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [620/689]  eta: 0:01:48  lr: 0.000082  loss: 0.2545 (0.2651)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [630/689]  eta: 0:01:32  lr: 0.000082  loss: 0.2552 (0.2650)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [640/689]  eta: 0:01:17  lr: 0.000082  loss: 0.2598 (0.2652)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [650/689]  eta: 0:01:01  lr: 0.000082  loss: 0.2598 (0.2650)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [660/689]  eta: 0:00:45  lr: 0.000082  loss: 0.2510 (0.2648)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [670/689]  eta: 0:00:29  lr: 0.000082  loss: 0.2518 (0.2645)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [680/689]  eta: 0:00:14  lr: 0.000082  loss: 0.2516 (0.2643)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264]  [688/689]  eta: 0:00:01  lr: 0.000082  loss: 0.2565 (0.2644)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:264] Total time: 0:18:05 (1.5750 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: 0.2565 (0.2644)\n",
      "Valid: [epoch:264]  [ 0/14]  eta: 0:00:11  loss: 0.2302 (0.2302)  time: 0.8017  data: 0.3891  max mem: 39763\n",
      "Valid: [epoch:264]  [13/14]  eta: 0:00:00  loss: 0.2478 (0.2525)  time: 0.0992  data: 0.0278  max mem: 39763\n",
      "Valid: [epoch:264] Total time: 0:00:01 (0.1084 s / it)\n",
      "Averaged stats: loss: 0.2478 (0.2525)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_264_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.253%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:265]  [  0/689]  eta: 0:12:08  lr: 0.000082  loss: 0.2491 (0.2491)  time: 1.0577  data: 0.5836  max mem: 39763\n",
      "Train: [epoch:265]  [ 10/689]  eta: 0:17:15  lr: 0.000082  loss: 0.2547 (0.2601)  time: 1.5251  data: 0.0531  max mem: 39763\n",
      "Train: [epoch:265]  [ 20/689]  eta: 0:17:15  lr: 0.000082  loss: 0.2472 (0.2594)  time: 1.5725  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [ 30/689]  eta: 0:17:05  lr: 0.000082  loss: 0.2465 (0.2594)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [ 40/689]  eta: 0:16:52  lr: 0.000082  loss: 0.2483 (0.2559)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [ 50/689]  eta: 0:16:38  lr: 0.000082  loss: 0.2540 (0.2591)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [ 60/689]  eta: 0:16:24  lr: 0.000082  loss: 0.2648 (0.2590)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [ 70/689]  eta: 0:16:09  lr: 0.000082  loss: 0.2665 (0.2621)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [ 80/689]  eta: 0:15:54  lr: 0.000082  loss: 0.2665 (0.2617)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [ 90/689]  eta: 0:15:39  lr: 0.000082  loss: 0.2665 (0.2641)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [100/689]  eta: 0:15:24  lr: 0.000082  loss: 0.2710 (0.2645)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [110/689]  eta: 0:15:08  lr: 0.000082  loss: 0.2607 (0.2642)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [120/689]  eta: 0:14:53  lr: 0.000082  loss: 0.2476 (0.2629)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [130/689]  eta: 0:14:37  lr: 0.000082  loss: 0.2506 (0.2638)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [140/689]  eta: 0:14:22  lr: 0.000082  loss: 0.2607 (0.2635)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [150/689]  eta: 0:14:06  lr: 0.000082  loss: 0.2629 (0.2641)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [160/689]  eta: 0:13:51  lr: 0.000082  loss: 0.2628 (0.2636)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [170/689]  eta: 0:13:35  lr: 0.000082  loss: 0.2572 (0.2635)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [180/689]  eta: 0:13:19  lr: 0.000082  loss: 0.2607 (0.2638)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [190/689]  eta: 0:13:04  lr: 0.000082  loss: 0.2510 (0.2635)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [200/689]  eta: 0:12:48  lr: 0.000082  loss: 0.2620 (0.2638)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [210/689]  eta: 0:12:32  lr: 0.000082  loss: 0.2620 (0.2635)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [220/689]  eta: 0:12:17  lr: 0.000082  loss: 0.2525 (0.2634)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [230/689]  eta: 0:12:01  lr: 0.000082  loss: 0.2512 (0.2630)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [240/689]  eta: 0:11:45  lr: 0.000082  loss: 0.2512 (0.2629)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [250/689]  eta: 0:11:30  lr: 0.000082  loss: 0.2525 (0.2625)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [260/689]  eta: 0:11:14  lr: 0.000082  loss: 0.2519 (0.2624)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [270/689]  eta: 0:10:58  lr: 0.000082  loss: 0.2578 (0.2630)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [280/689]  eta: 0:10:43  lr: 0.000082  loss: 0.2578 (0.2629)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [290/689]  eta: 0:10:27  lr: 0.000082  loss: 0.2564 (0.2631)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [300/689]  eta: 0:10:11  lr: 0.000082  loss: 0.2652 (0.2633)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [310/689]  eta: 0:09:56  lr: 0.000082  loss: 0.2490 (0.2630)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [320/689]  eta: 0:09:40  lr: 0.000082  loss: 0.2510 (0.2629)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [330/689]  eta: 0:09:24  lr: 0.000082  loss: 0.2577 (0.2631)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [340/689]  eta: 0:09:08  lr: 0.000082  loss: 0.2596 (0.2632)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [350/689]  eta: 0:08:53  lr: 0.000082  loss: 0.2586 (0.2631)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [360/689]  eta: 0:08:37  lr: 0.000082  loss: 0.2551 (0.2634)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [370/689]  eta: 0:08:21  lr: 0.000082  loss: 0.2551 (0.2632)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [380/689]  eta: 0:08:06  lr: 0.000082  loss: 0.2563 (0.2632)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [390/689]  eta: 0:07:50  lr: 0.000082  loss: 0.2601 (0.2636)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [400/689]  eta: 0:07:34  lr: 0.000082  loss: 0.2656 (0.2634)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [410/689]  eta: 0:07:18  lr: 0.000082  loss: 0.2544 (0.2634)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [420/689]  eta: 0:07:03  lr: 0.000082  loss: 0.2499 (0.2631)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [430/689]  eta: 0:06:47  lr: 0.000082  loss: 0.2546 (0.2634)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [440/689]  eta: 0:06:31  lr: 0.000082  loss: 0.2627 (0.2637)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [450/689]  eta: 0:06:16  lr: 0.000082  loss: 0.2532 (0.2637)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [460/689]  eta: 0:06:00  lr: 0.000082  loss: 0.2545 (0.2642)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [470/689]  eta: 0:05:44  lr: 0.000082  loss: 0.2620 (0.2643)  time: 1.5759  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:265]  [480/689]  eta: 0:05:28  lr: 0.000082  loss: 0.2592 (0.2642)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [490/689]  eta: 0:05:13  lr: 0.000082  loss: 0.2479 (0.2643)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [500/689]  eta: 0:04:57  lr: 0.000082  loss: 0.2697 (0.2645)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [510/689]  eta: 0:04:41  lr: 0.000082  loss: 0.2670 (0.2646)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [520/689]  eta: 0:04:25  lr: 0.000082  loss: 0.2626 (0.2645)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [530/689]  eta: 0:04:10  lr: 0.000082  loss: 0.2594 (0.2645)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [540/689]  eta: 0:03:54  lr: 0.000082  loss: 0.2534 (0.2643)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [550/689]  eta: 0:03:38  lr: 0.000082  loss: 0.2528 (0.2642)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [560/689]  eta: 0:03:23  lr: 0.000082  loss: 0.2596 (0.2644)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [570/689]  eta: 0:03:07  lr: 0.000082  loss: 0.2629 (0.2643)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [580/689]  eta: 0:02:51  lr: 0.000082  loss: 0.2583 (0.2645)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [590/689]  eta: 0:02:35  lr: 0.000082  loss: 0.2576 (0.2647)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [600/689]  eta: 0:02:20  lr: 0.000082  loss: 0.2612 (0.2649)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [610/689]  eta: 0:02:04  lr: 0.000082  loss: 0.2665 (0.2649)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [620/689]  eta: 0:01:48  lr: 0.000082  loss: 0.2642 (0.2651)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [630/689]  eta: 0:01:32  lr: 0.000082  loss: 0.2560 (0.2650)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [640/689]  eta: 0:01:17  lr: 0.000082  loss: 0.2681 (0.2651)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [650/689]  eta: 0:01:01  lr: 0.000082  loss: 0.2629 (0.2651)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [660/689]  eta: 0:00:45  lr: 0.000082  loss: 0.2538 (0.2652)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [670/689]  eta: 0:00:29  lr: 0.000082  loss: 0.2405 (0.2649)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [680/689]  eta: 0:00:14  lr: 0.000082  loss: 0.2512 (0.2651)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265]  [688/689]  eta: 0:00:01  lr: 0.000082  loss: 0.2580 (0.2651)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:265] Total time: 0:18:04 (1.5747 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: 0.2580 (0.2651)\n",
      "Valid: [epoch:265]  [ 0/14]  eta: 0:00:14  loss: 0.2419 (0.2419)  time: 1.0064  data: 0.4311  max mem: 39763\n",
      "Valid: [epoch:265]  [13/14]  eta: 0:00:00  loss: 0.2484 (0.2524)  time: 0.1138  data: 0.0308  max mem: 39763\n",
      "Valid: [epoch:265] Total time: 0:00:01 (0.1228 s / it)\n",
      "Averaged stats: loss: 0.2484 (0.2524)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_265_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.252%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:266]  [  0/689]  eta: 0:11:48  lr: 0.000082  loss: 0.2674 (0.2674)  time: 1.0285  data: 0.5514  max mem: 39763\n",
      "Train: [epoch:266]  [ 10/689]  eta: 0:17:15  lr: 0.000082  loss: 0.2674 (0.2737)  time: 1.5245  data: 0.0502  max mem: 39763\n",
      "Train: [epoch:266]  [ 20/689]  eta: 0:17:16  lr: 0.000082  loss: 0.2603 (0.2716)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [ 30/689]  eta: 0:17:05  lr: 0.000082  loss: 0.2693 (0.2723)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [ 40/689]  eta: 0:16:53  lr: 0.000082  loss: 0.2603 (0.2682)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [ 50/689]  eta: 0:16:39  lr: 0.000082  loss: 0.2588 (0.2679)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [ 60/689]  eta: 0:16:24  lr: 0.000082  loss: 0.2482 (0.2643)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [ 70/689]  eta: 0:16:09  lr: 0.000082  loss: 0.2563 (0.2673)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [ 80/689]  eta: 0:15:54  lr: 0.000082  loss: 0.2657 (0.2651)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [ 90/689]  eta: 0:15:39  lr: 0.000082  loss: 0.2470 (0.2647)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [100/689]  eta: 0:15:24  lr: 0.000082  loss: 0.2511 (0.2651)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [110/689]  eta: 0:15:08  lr: 0.000082  loss: 0.2511 (0.2666)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [120/689]  eta: 0:14:53  lr: 0.000082  loss: 0.2682 (0.2782)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [130/689]  eta: 0:14:37  lr: 0.000082  loss: 0.2837 (0.2801)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [140/689]  eta: 0:14:22  lr: 0.000082  loss: 0.3029 (0.2837)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [150/689]  eta: 0:14:06  lr: 0.000082  loss: 0.2983 (0.2851)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [160/689]  eta: 0:13:50  lr: 0.000082  loss: 0.2928 (0.2856)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [170/689]  eta: 0:13:35  lr: 0.000082  loss: 0.2675 (0.2854)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [180/689]  eta: 0:13:19  lr: 0.000082  loss: 0.2701 (0.2854)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [190/689]  eta: 0:13:04  lr: 0.000082  loss: 0.2776 (0.2844)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [200/689]  eta: 0:12:48  lr: 0.000082  loss: 0.2610 (0.2829)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [210/689]  eta: 0:12:32  lr: 0.000082  loss: 0.2610 (0.2818)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [220/689]  eta: 0:12:17  lr: 0.000082  loss: 0.2638 (0.2824)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [230/689]  eta: 0:12:01  lr: 0.000082  loss: 0.2743 (0.2824)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [240/689]  eta: 0:11:45  lr: 0.000082  loss: 0.2796 (0.2826)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [250/689]  eta: 0:11:30  lr: 0.000082  loss: 0.2774 (0.2823)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [260/689]  eta: 0:11:14  lr: 0.000082  loss: 0.2655 (0.2818)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [270/689]  eta: 0:10:58  lr: 0.000082  loss: 0.2667 (0.2817)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [280/689]  eta: 0:10:43  lr: 0.000082  loss: 0.2667 (0.2813)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [290/689]  eta: 0:10:27  lr: 0.000082  loss: 0.2634 (0.2810)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [300/689]  eta: 0:10:11  lr: 0.000082  loss: 0.2578 (0.2805)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [310/689]  eta: 0:09:56  lr: 0.000082  loss: 0.2574 (0.2798)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [320/689]  eta: 0:09:40  lr: 0.000082  loss: 0.2614 (0.2796)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [330/689]  eta: 0:09:24  lr: 0.000082  loss: 0.2712 (0.2794)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [340/689]  eta: 0:09:08  lr: 0.000082  loss: 0.2781 (0.2796)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [350/689]  eta: 0:08:53  lr: 0.000082  loss: 0.2901 (0.2799)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [360/689]  eta: 0:08:37  lr: 0.000082  loss: 0.2892 (0.2800)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [370/689]  eta: 0:08:21  lr: 0.000082  loss: 0.2571 (0.2793)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [380/689]  eta: 0:08:06  lr: 0.000082  loss: 0.2571 (0.2789)  time: 1.5755  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:266]  [390/689]  eta: 0:07:50  lr: 0.000082  loss: 0.2650 (0.2790)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [400/689]  eta: 0:07:34  lr: 0.000082  loss: 0.2650 (0.2788)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [410/689]  eta: 0:07:19  lr: 0.000082  loss: 0.2609 (0.2783)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [420/689]  eta: 0:07:03  lr: 0.000082  loss: 0.2537 (0.2777)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [430/689]  eta: 0:06:47  lr: 0.000082  loss: 0.2597 (0.2778)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [440/689]  eta: 0:06:31  lr: 0.000082  loss: 0.2712 (0.2777)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [450/689]  eta: 0:06:16  lr: 0.000082  loss: 0.2604 (0.2774)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [460/689]  eta: 0:06:00  lr: 0.000082  loss: 0.2646 (0.2777)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [470/689]  eta: 0:05:44  lr: 0.000082  loss: 0.2725 (0.2776)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [480/689]  eta: 0:05:28  lr: 0.000082  loss: 0.2655 (0.2776)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [490/689]  eta: 0:05:13  lr: 0.000082  loss: 0.2643 (0.2776)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [500/689]  eta: 0:04:57  lr: 0.000082  loss: 0.2643 (0.2774)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [510/689]  eta: 0:04:41  lr: 0.000082  loss: 0.2725 (0.2777)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [520/689]  eta: 0:04:26  lr: 0.000082  loss: 0.2560 (0.2773)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [530/689]  eta: 0:04:10  lr: 0.000082  loss: 0.2532 (0.2772)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [540/689]  eta: 0:03:54  lr: 0.000082  loss: 0.2637 (0.2769)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [550/689]  eta: 0:03:38  lr: 0.000082  loss: 0.2591 (0.2764)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [560/689]  eta: 0:03:23  lr: 0.000082  loss: 0.2549 (0.2760)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [570/689]  eta: 0:03:07  lr: 0.000082  loss: 0.2688 (0.2762)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [580/689]  eta: 0:02:51  lr: 0.000082  loss: 0.2634 (0.2760)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [590/689]  eta: 0:02:35  lr: 0.000082  loss: 0.2602 (0.2760)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [600/689]  eta: 0:02:20  lr: 0.000082  loss: 0.2669 (0.2758)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [610/689]  eta: 0:02:04  lr: 0.000082  loss: 0.2619 (0.2758)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [620/689]  eta: 0:01:48  lr: 0.000082  loss: 0.2662 (0.2761)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [630/689]  eta: 0:01:32  lr: 0.000082  loss: 0.2597 (0.2756)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [640/689]  eta: 0:01:17  lr: 0.000082  loss: 0.2469 (0.2752)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [650/689]  eta: 0:01:01  lr: 0.000082  loss: 0.2610 (0.2756)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [660/689]  eta: 0:00:45  lr: 0.000082  loss: 0.2871 (0.2759)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [670/689]  eta: 0:00:29  lr: 0.000082  loss: 0.2753 (0.2758)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [680/689]  eta: 0:00:14  lr: 0.000082  loss: 0.2753 (0.2758)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266]  [688/689]  eta: 0:00:01  lr: 0.000082  loss: 0.2542 (0.2755)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:266] Total time: 0:18:04 (1.5747 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: 0.2542 (0.2755)\n",
      "Valid: [epoch:266]  [ 0/14]  eta: 0:00:14  loss: 0.2501 (0.2501)  time: 1.0109  data: 0.3787  max mem: 39763\n",
      "Valid: [epoch:266]  [13/14]  eta: 0:00:00  loss: 0.2501 (0.2541)  time: 0.1141  data: 0.0271  max mem: 39763\n",
      "Valid: [epoch:266] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.2501 (0.2541)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_266_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.254%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:267]  [  0/689]  eta: 0:11:47  lr: 0.000082  loss: 0.2808 (0.2808)  time: 1.0262  data: 0.5512  max mem: 39763\n",
      "Train: [epoch:267]  [ 10/689]  eta: 0:17:13  lr: 0.000082  loss: 0.2769 (0.2640)  time: 1.5223  data: 0.0502  max mem: 39763\n",
      "Train: [epoch:267]  [ 20/689]  eta: 0:17:14  lr: 0.000082  loss: 0.2601 (0.2623)  time: 1.5725  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [ 30/689]  eta: 0:17:05  lr: 0.000082  loss: 0.2711 (0.2690)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [ 40/689]  eta: 0:16:52  lr: 0.000082  loss: 0.2689 (0.2675)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [ 50/689]  eta: 0:16:38  lr: 0.000082  loss: 0.2514 (0.2659)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [ 60/689]  eta: 0:16:23  lr: 0.000082  loss: 0.2514 (0.2639)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [ 70/689]  eta: 0:16:09  lr: 0.000082  loss: 0.2532 (0.2660)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [ 80/689]  eta: 0:15:54  lr: 0.000082  loss: 0.2574 (0.2649)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [ 90/689]  eta: 0:15:39  lr: 0.000082  loss: 0.2592 (0.2661)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [100/689]  eta: 0:15:23  lr: 0.000082  loss: 0.2701 (0.2675)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [110/689]  eta: 0:15:08  lr: 0.000082  loss: 0.2618 (0.2666)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [120/689]  eta: 0:14:53  lr: 0.000082  loss: 0.2564 (0.2657)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [130/689]  eta: 0:14:37  lr: 0.000082  loss: 0.2644 (0.2670)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [140/689]  eta: 0:14:22  lr: 0.000082  loss: 0.2671 (0.2681)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [150/689]  eta: 0:14:06  lr: 0.000082  loss: 0.2563 (0.2678)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [160/689]  eta: 0:13:51  lr: 0.000082  loss: 0.2511 (0.2676)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [170/689]  eta: 0:13:35  lr: 0.000082  loss: 0.2529 (0.2684)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [180/689]  eta: 0:13:19  lr: 0.000082  loss: 0.2761 (0.2692)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [190/689]  eta: 0:13:04  lr: 0.000082  loss: 0.2639 (0.2688)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [200/689]  eta: 0:12:48  lr: 0.000082  loss: 0.2582 (0.2685)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [210/689]  eta: 0:12:33  lr: 0.000082  loss: 0.2593 (0.2681)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [220/689]  eta: 0:12:17  lr: 0.000082  loss: 0.2643 (0.2689)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [230/689]  eta: 0:12:01  lr: 0.000082  loss: 0.2648 (0.2686)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [240/689]  eta: 0:11:46  lr: 0.000082  loss: 0.2594 (0.2684)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [250/689]  eta: 0:11:30  lr: 0.000082  loss: 0.2587 (0.2683)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [260/689]  eta: 0:11:14  lr: 0.000082  loss: 0.2592 (0.2679)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [270/689]  eta: 0:10:59  lr: 0.000082  loss: 0.2583 (0.2676)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [280/689]  eta: 0:10:43  lr: 0.000082  loss: 0.2737 (0.2683)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [290/689]  eta: 0:10:27  lr: 0.000082  loss: 0.2767 (0.2684)  time: 1.5759  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:267]  [300/689]  eta: 0:10:12  lr: 0.000082  loss: 0.2538 (0.2681)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [310/689]  eta: 0:09:56  lr: 0.000082  loss: 0.2443 (0.2672)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [320/689]  eta: 0:09:40  lr: 0.000082  loss: 0.2563 (0.2678)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [330/689]  eta: 0:09:24  lr: 0.000082  loss: 0.2752 (0.2680)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [340/689]  eta: 0:09:09  lr: 0.000082  loss: 0.2663 (0.2678)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [350/689]  eta: 0:08:53  lr: 0.000082  loss: 0.2570 (0.2684)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [360/689]  eta: 0:08:37  lr: 0.000082  loss: 0.2578 (0.2686)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [370/689]  eta: 0:08:22  lr: 0.000082  loss: 0.2578 (0.2686)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [380/689]  eta: 0:08:06  lr: 0.000082  loss: 0.2508 (0.2682)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [390/689]  eta: 0:07:50  lr: 0.000082  loss: 0.2598 (0.2687)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [400/689]  eta: 0:07:34  lr: 0.000082  loss: 0.2838 (0.2688)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [410/689]  eta: 0:07:19  lr: 0.000082  loss: 0.2704 (0.2690)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [420/689]  eta: 0:07:03  lr: 0.000082  loss: 0.2535 (0.2687)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [430/689]  eta: 0:06:47  lr: 0.000082  loss: 0.2559 (0.2686)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [440/689]  eta: 0:06:31  lr: 0.000082  loss: 0.2633 (0.2687)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [450/689]  eta: 0:06:16  lr: 0.000082  loss: 0.2566 (0.2683)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [460/689]  eta: 0:06:00  lr: 0.000082  loss: 0.2577 (0.2682)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [470/689]  eta: 0:05:44  lr: 0.000082  loss: 0.2664 (0.2684)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [480/689]  eta: 0:05:29  lr: 0.000082  loss: 0.2564 (0.2681)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [490/689]  eta: 0:05:13  lr: 0.000082  loss: 0.2523 (0.2680)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [500/689]  eta: 0:04:57  lr: 0.000082  loss: 0.2670 (0.2684)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [510/689]  eta: 0:04:41  lr: 0.000082  loss: 0.2874 (0.2688)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [520/689]  eta: 0:04:26  lr: 0.000082  loss: 0.2702 (0.2687)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [530/689]  eta: 0:04:10  lr: 0.000082  loss: 0.2551 (0.2685)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [540/689]  eta: 0:03:54  lr: 0.000082  loss: 0.2519 (0.2687)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [550/689]  eta: 0:03:38  lr: 0.000082  loss: 0.2519 (0.2685)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [560/689]  eta: 0:03:23  lr: 0.000082  loss: 0.2580 (0.2688)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [570/689]  eta: 0:03:07  lr: 0.000082  loss: 0.2624 (0.2687)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [580/689]  eta: 0:02:51  lr: 0.000082  loss: 0.2575 (0.2686)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [590/689]  eta: 0:02:35  lr: 0.000082  loss: 0.2555 (0.2682)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [600/689]  eta: 0:02:20  lr: 0.000082  loss: 0.2548 (0.2682)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [610/689]  eta: 0:02:04  lr: 0.000082  loss: 0.2611 (0.2681)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [620/689]  eta: 0:01:48  lr: 0.000082  loss: 0.2599 (0.2680)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [630/689]  eta: 0:01:32  lr: 0.000082  loss: 0.2562 (0.2680)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [640/689]  eta: 0:01:17  lr: 0.000082  loss: 0.2504 (0.2679)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [650/689]  eta: 0:01:01  lr: 0.000082  loss: 0.2584 (0.2683)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [660/689]  eta: 0:00:45  lr: 0.000082  loss: 0.2664 (0.2682)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [670/689]  eta: 0:00:29  lr: 0.000082  loss: 0.2617 (0.2681)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [680/689]  eta: 0:00:14  lr: 0.000082  loss: 0.2617 (0.2682)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267]  [688/689]  eta: 0:00:01  lr: 0.000082  loss: 0.2593 (0.2682)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:267] Total time: 0:18:04 (1.5745 s / it)\n",
      "Averaged stats: lr: 0.000082  loss: 0.2593 (0.2682)\n",
      "Valid: [epoch:267]  [ 0/14]  eta: 0:00:14  loss: 0.2530 (0.2530)  time: 1.0204  data: 0.3888  max mem: 39763\n",
      "Valid: [epoch:267]  [13/14]  eta: 0:00:00  loss: 0.2537 (0.2569)  time: 0.1148  data: 0.0278  max mem: 39763\n",
      "Valid: [epoch:267] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.2537 (0.2569)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_267_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.257%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:268]  [  0/689]  eta: 0:12:03  lr: 0.000081  loss: 0.3629 (0.3629)  time: 1.0498  data: 0.5747  max mem: 39763\n",
      "Train: [epoch:268]  [ 10/689]  eta: 0:17:15  lr: 0.000081  loss: 0.2636 (0.2759)  time: 1.5252  data: 0.0523  max mem: 39763\n",
      "Train: [epoch:268]  [ 20/689]  eta: 0:17:16  lr: 0.000081  loss: 0.2658 (0.2767)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [ 30/689]  eta: 0:17:06  lr: 0.000081  loss: 0.2735 (0.2779)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [ 40/689]  eta: 0:16:53  lr: 0.000081  loss: 0.2711 (0.2751)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [ 50/689]  eta: 0:16:39  lr: 0.000081  loss: 0.2592 (0.2727)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [ 60/689]  eta: 0:16:24  lr: 0.000081  loss: 0.2604 (0.2730)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [ 70/689]  eta: 0:16:09  lr: 0.000081  loss: 0.2579 (0.2703)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [ 80/689]  eta: 0:15:54  lr: 0.000081  loss: 0.2610 (0.2706)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [ 90/689]  eta: 0:15:39  lr: 0.000081  loss: 0.2554 (0.2693)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [100/689]  eta: 0:15:24  lr: 0.000081  loss: 0.2557 (0.2704)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [110/689]  eta: 0:15:08  lr: 0.000081  loss: 0.2747 (0.2706)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [120/689]  eta: 0:14:53  lr: 0.000081  loss: 0.2671 (0.2703)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [130/689]  eta: 0:14:38  lr: 0.000081  loss: 0.2562 (0.2692)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [140/689]  eta: 0:14:22  lr: 0.000081  loss: 0.2491 (0.2687)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [150/689]  eta: 0:14:07  lr: 0.000081  loss: 0.2587 (0.2692)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [160/689]  eta: 0:13:51  lr: 0.000081  loss: 0.2645 (0.2692)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [170/689]  eta: 0:13:35  lr: 0.000081  loss: 0.2645 (0.2691)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [180/689]  eta: 0:13:20  lr: 0.000081  loss: 0.2496 (0.2682)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [190/689]  eta: 0:13:04  lr: 0.000081  loss: 0.2607 (0.2684)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [200/689]  eta: 0:12:49  lr: 0.000081  loss: 0.2562 (0.2672)  time: 1.5756  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:268]  [210/689]  eta: 0:12:33  lr: 0.000081  loss: 0.2479 (0.2671)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [220/689]  eta: 0:12:17  lr: 0.000081  loss: 0.2628 (0.2680)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [230/689]  eta: 0:12:02  lr: 0.000081  loss: 0.2734 (0.2679)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [240/689]  eta: 0:11:46  lr: 0.000081  loss: 0.2704 (0.2687)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [250/689]  eta: 0:11:30  lr: 0.000081  loss: 0.2589 (0.2680)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [260/689]  eta: 0:11:15  lr: 0.000081  loss: 0.2490 (0.2681)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [270/689]  eta: 0:10:59  lr: 0.000081  loss: 0.2620 (0.2682)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [280/689]  eta: 0:10:43  lr: 0.000081  loss: 0.2510 (0.2682)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [290/689]  eta: 0:10:27  lr: 0.000081  loss: 0.2499 (0.2682)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [300/689]  eta: 0:10:12  lr: 0.000081  loss: 0.2560 (0.2684)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [310/689]  eta: 0:09:56  lr: 0.000081  loss: 0.2545 (0.2681)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [320/689]  eta: 0:09:40  lr: 0.000081  loss: 0.2671 (0.2688)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [330/689]  eta: 0:09:25  lr: 0.000081  loss: 0.2671 (0.2686)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [340/689]  eta: 0:09:09  lr: 0.000081  loss: 0.2664 (0.2689)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [350/689]  eta: 0:08:53  lr: 0.000081  loss: 0.2749 (0.2695)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [360/689]  eta: 0:08:37  lr: 0.000081  loss: 0.2618 (0.2694)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [370/689]  eta: 0:08:22  lr: 0.000081  loss: 0.2561 (0.2693)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [380/689]  eta: 0:08:06  lr: 0.000081  loss: 0.2578 (0.2694)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [390/689]  eta: 0:07:50  lr: 0.000081  loss: 0.2683 (0.2698)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [400/689]  eta: 0:07:34  lr: 0.000081  loss: 0.2623 (0.2697)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [410/689]  eta: 0:07:19  lr: 0.000081  loss: 0.2549 (0.2696)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [420/689]  eta: 0:07:03  lr: 0.000081  loss: 0.2670 (0.2697)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [430/689]  eta: 0:06:47  lr: 0.000081  loss: 0.2670 (0.2695)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [440/689]  eta: 0:06:31  lr: 0.000081  loss: 0.2551 (0.2697)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [450/689]  eta: 0:06:16  lr: 0.000081  loss: 0.2534 (0.2695)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [460/689]  eta: 0:06:00  lr: 0.000081  loss: 0.2534 (0.2695)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [470/689]  eta: 0:05:44  lr: 0.000081  loss: 0.2669 (0.2697)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [480/689]  eta: 0:05:29  lr: 0.000081  loss: 0.2734 (0.2698)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [490/689]  eta: 0:05:13  lr: 0.000081  loss: 0.2518 (0.2697)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [500/689]  eta: 0:04:57  lr: 0.000081  loss: 0.2487 (0.2696)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [510/689]  eta: 0:04:41  lr: 0.000081  loss: 0.2560 (0.2695)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [520/689]  eta: 0:04:26  lr: 0.000081  loss: 0.2621 (0.2694)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [530/689]  eta: 0:04:10  lr: 0.000081  loss: 0.2621 (0.2694)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [540/689]  eta: 0:03:54  lr: 0.000081  loss: 0.2641 (0.2695)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [550/689]  eta: 0:03:38  lr: 0.000081  loss: 0.2641 (0.2696)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [560/689]  eta: 0:03:23  lr: 0.000081  loss: 0.2606 (0.2696)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [570/689]  eta: 0:03:07  lr: 0.000081  loss: 0.2517 (0.2694)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [580/689]  eta: 0:02:51  lr: 0.000081  loss: 0.2511 (0.2692)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [590/689]  eta: 0:02:35  lr: 0.000081  loss: 0.2591 (0.2693)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [600/689]  eta: 0:02:20  lr: 0.000081  loss: 0.2594 (0.2692)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [610/689]  eta: 0:02:04  lr: 0.000081  loss: 0.2537 (0.2691)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [620/689]  eta: 0:01:48  lr: 0.000081  loss: 0.2537 (0.2691)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [630/689]  eta: 0:01:32  lr: 0.000081  loss: 0.2545 (0.2689)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [640/689]  eta: 0:01:17  lr: 0.000081  loss: 0.2566 (0.2689)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [650/689]  eta: 0:01:01  lr: 0.000081  loss: 0.2589 (0.2688)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [660/689]  eta: 0:00:45  lr: 0.000081  loss: 0.2604 (0.2688)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [670/689]  eta: 0:00:29  lr: 0.000081  loss: 0.2690 (0.2690)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [680/689]  eta: 0:00:14  lr: 0.000081  loss: 0.2633 (0.2690)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268]  [688/689]  eta: 0:00:01  lr: 0.000081  loss: 0.2598 (0.2689)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:268] Total time: 0:18:04 (1.5746 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: 0.2598 (0.2689)\n",
      "Valid: [epoch:268]  [ 0/14]  eta: 0:00:14  loss: 0.2679 (0.2679)  time: 1.0248  data: 0.3826  max mem: 39763\n",
      "Valid: [epoch:268]  [13/14]  eta: 0:00:00  loss: 0.2514 (0.2546)  time: 0.1151  data: 0.0274  max mem: 39763\n",
      "Valid: [epoch:268] Total time: 0:00:01 (0.1242 s / it)\n",
      "Averaged stats: loss: 0.2514 (0.2546)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_268_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.255%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:269]  [  0/689]  eta: 0:12:23  lr: 0.000081  loss: 0.2550 (0.2550)  time: 1.0785  data: 0.6008  max mem: 39763\n",
      "Train: [epoch:269]  [ 10/689]  eta: 0:17:17  lr: 0.000081  loss: 0.2596 (0.2606)  time: 1.5277  data: 0.0547  max mem: 39763\n",
      "Train: [epoch:269]  [ 20/689]  eta: 0:17:16  lr: 0.000081  loss: 0.2493 (0.2543)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [ 30/689]  eta: 0:17:06  lr: 0.000081  loss: 0.2489 (0.2567)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [ 40/689]  eta: 0:16:53  lr: 0.000081  loss: 0.2577 (0.2583)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [ 50/689]  eta: 0:16:39  lr: 0.000081  loss: 0.2577 (0.2585)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [ 60/689]  eta: 0:16:25  lr: 0.000081  loss: 0.2544 (0.2581)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [ 70/689]  eta: 0:16:10  lr: 0.000081  loss: 0.2544 (0.2585)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [ 80/689]  eta: 0:15:55  lr: 0.000081  loss: 0.2594 (0.2621)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [ 90/689]  eta: 0:15:40  lr: 0.000081  loss: 0.2594 (0.2626)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [100/689]  eta: 0:15:24  lr: 0.000081  loss: 0.2550 (0.2612)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [110/689]  eta: 0:15:09  lr: 0.000081  loss: 0.2557 (0.2633)  time: 1.5758  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:269]  [120/689]  eta: 0:14:53  lr: 0.000081  loss: 0.2646 (0.2633)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [130/689]  eta: 0:14:38  lr: 0.000081  loss: 0.2616 (0.2646)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [140/689]  eta: 0:14:23  lr: 0.000081  loss: 0.2616 (0.2651)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [150/689]  eta: 0:14:07  lr: 0.000081  loss: 0.2671 (0.2667)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [160/689]  eta: 0:13:51  lr: 0.000081  loss: 0.2666 (0.2666)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [170/689]  eta: 0:13:36  lr: 0.000081  loss: 0.2667 (0.2673)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [180/689]  eta: 0:13:20  lr: 0.000081  loss: 0.2694 (0.2671)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [190/689]  eta: 0:13:05  lr: 0.000081  loss: 0.2569 (0.2676)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [200/689]  eta: 0:12:49  lr: 0.000081  loss: 0.2566 (0.2676)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [210/689]  eta: 0:12:33  lr: 0.000081  loss: 0.2566 (0.2678)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [220/689]  eta: 0:12:18  lr: 0.000081  loss: 0.2559 (0.2673)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [230/689]  eta: 0:12:02  lr: 0.000081  loss: 0.2654 (0.2689)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [240/689]  eta: 0:11:46  lr: 0.000081  loss: 0.2742 (0.2686)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [250/689]  eta: 0:11:31  lr: 0.000081  loss: 0.2520 (0.2683)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [260/689]  eta: 0:11:15  lr: 0.000081  loss: 0.2582 (0.2681)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [270/689]  eta: 0:10:59  lr: 0.000081  loss: 0.2677 (0.2682)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [280/689]  eta: 0:10:43  lr: 0.000081  loss: 0.2825 (0.2689)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [290/689]  eta: 0:10:28  lr: 0.000081  loss: 0.2693 (0.2692)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [300/689]  eta: 0:10:12  lr: 0.000081  loss: 0.2594 (0.2689)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [310/689]  eta: 0:09:56  lr: 0.000081  loss: 0.2551 (0.2686)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [320/689]  eta: 0:09:41  lr: 0.000081  loss: 0.2551 (0.2686)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [330/689]  eta: 0:09:25  lr: 0.000081  loss: 0.2640 (0.2692)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [340/689]  eta: 0:09:09  lr: 0.000081  loss: 0.2672 (0.2690)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [350/689]  eta: 0:08:53  lr: 0.000081  loss: 0.2627 (0.2692)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [360/689]  eta: 0:08:38  lr: 0.000081  loss: 0.2588 (0.2692)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [370/689]  eta: 0:08:22  lr: 0.000081  loss: 0.2588 (0.2692)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [380/689]  eta: 0:08:06  lr: 0.000081  loss: 0.2657 (0.2691)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [390/689]  eta: 0:07:50  lr: 0.000081  loss: 0.2699 (0.2690)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [400/689]  eta: 0:07:35  lr: 0.000081  loss: 0.2605 (0.2689)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [410/689]  eta: 0:07:19  lr: 0.000081  loss: 0.2582 (0.2689)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [420/689]  eta: 0:07:03  lr: 0.000081  loss: 0.2691 (0.2691)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [430/689]  eta: 0:06:47  lr: 0.000081  loss: 0.2740 (0.2694)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [440/689]  eta: 0:06:32  lr: 0.000081  loss: 0.2784 (0.2696)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [450/689]  eta: 0:06:16  lr: 0.000081  loss: 0.2702 (0.2694)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [460/689]  eta: 0:06:00  lr: 0.000081  loss: 0.2598 (0.2693)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [470/689]  eta: 0:05:44  lr: 0.000081  loss: 0.2604 (0.2694)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [480/689]  eta: 0:05:29  lr: 0.000081  loss: 0.2369 (0.2690)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [490/689]  eta: 0:05:13  lr: 0.000081  loss: 0.2402 (0.2686)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [500/689]  eta: 0:04:57  lr: 0.000081  loss: 0.2506 (0.2686)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [510/689]  eta: 0:04:41  lr: 0.000081  loss: 0.2557 (0.2685)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [520/689]  eta: 0:04:26  lr: 0.000081  loss: 0.2644 (0.2689)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [530/689]  eta: 0:04:10  lr: 0.000081  loss: 0.2699 (0.2691)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [540/689]  eta: 0:03:54  lr: 0.000081  loss: 0.2603 (0.2690)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [550/689]  eta: 0:03:38  lr: 0.000081  loss: 0.2600 (0.2689)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [560/689]  eta: 0:03:23  lr: 0.000081  loss: 0.2553 (0.2688)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [570/689]  eta: 0:03:07  lr: 0.000081  loss: 0.2534 (0.2687)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [580/689]  eta: 0:02:51  lr: 0.000081  loss: 0.2761 (0.2694)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [590/689]  eta: 0:02:35  lr: 0.000081  loss: 0.2759 (0.2692)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [600/689]  eta: 0:02:20  lr: 0.000081  loss: 0.2618 (0.2693)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [610/689]  eta: 0:02:04  lr: 0.000081  loss: 0.2631 (0.2691)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [620/689]  eta: 0:01:48  lr: 0.000081  loss: 0.2708 (0.2693)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [630/689]  eta: 0:01:32  lr: 0.000081  loss: 0.2742 (0.2696)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [640/689]  eta: 0:01:17  lr: 0.000081  loss: 0.2752 (0.2698)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [650/689]  eta: 0:01:01  lr: 0.000081  loss: 0.2734 (0.2696)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [660/689]  eta: 0:00:45  lr: 0.000081  loss: 0.2603 (0.2697)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [670/689]  eta: 0:00:29  lr: 0.000081  loss: 0.2777 (0.2699)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [680/689]  eta: 0:00:14  lr: 0.000081  loss: 0.2681 (0.2697)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269]  [688/689]  eta: 0:00:01  lr: 0.000081  loss: 0.2643 (0.2697)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:269] Total time: 0:18:05 (1.5754 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: 0.2643 (0.2697)\n",
      "Valid: [epoch:269]  [ 0/14]  eta: 0:00:14  loss: 0.2710 (0.2710)  time: 1.0128  data: 0.3936  max mem: 39763\n",
      "Valid: [epoch:269]  [13/14]  eta: 0:00:00  loss: 0.2524 (0.2562)  time: 0.1142  data: 0.0282  max mem: 39763\n",
      "Valid: [epoch:269] Total time: 0:00:01 (0.1215 s / it)\n",
      "Averaged stats: loss: 0.2524 (0.2562)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_269_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.256%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:270]  [  0/689]  eta: 0:12:05  lr: 0.000081  loss: 0.2432 (0.2432)  time: 1.0525  data: 0.5731  max mem: 39763\n",
      "Train: [epoch:270]  [ 10/689]  eta: 0:17:16  lr: 0.000081  loss: 0.2626 (0.2723)  time: 1.5261  data: 0.0522  max mem: 39763\n",
      "Train: [epoch:270]  [ 20/689]  eta: 0:17:16  lr: 0.000081  loss: 0.2617 (0.2643)  time: 1.5741  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:270]  [ 30/689]  eta: 0:17:06  lr: 0.000081  loss: 0.2521 (0.2664)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [ 40/689]  eta: 0:16:53  lr: 0.000081  loss: 0.2609 (0.2683)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [ 50/689]  eta: 0:16:39  lr: 0.000081  loss: 0.2607 (0.2668)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [ 60/689]  eta: 0:16:25  lr: 0.000081  loss: 0.2706 (0.2696)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [ 70/689]  eta: 0:16:10  lr: 0.000081  loss: 0.2672 (0.2687)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [ 80/689]  eta: 0:15:55  lr: 0.000081  loss: 0.2673 (0.2712)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [ 90/689]  eta: 0:15:40  lr: 0.000081  loss: 0.2711 (0.2720)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [100/689]  eta: 0:15:24  lr: 0.000081  loss: 0.2711 (0.2730)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [110/689]  eta: 0:15:09  lr: 0.000081  loss: 0.2723 (0.2748)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [120/689]  eta: 0:14:53  lr: 0.000081  loss: 0.2659 (0.2741)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [130/689]  eta: 0:14:38  lr: 0.000081  loss: 0.2697 (0.2748)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [140/689]  eta: 0:14:22  lr: 0.000081  loss: 0.2871 (0.2755)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [150/689]  eta: 0:14:07  lr: 0.000081  loss: 0.2699 (0.2754)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [160/689]  eta: 0:13:51  lr: 0.000081  loss: 0.2673 (0.2752)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [170/689]  eta: 0:13:36  lr: 0.000081  loss: 0.2719 (0.2748)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [180/689]  eta: 0:13:20  lr: 0.000081  loss: 0.2560 (0.2749)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [190/689]  eta: 0:13:04  lr: 0.000081  loss: 0.2620 (0.2741)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [200/689]  eta: 0:12:49  lr: 0.000081  loss: 0.2620 (0.2738)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [210/689]  eta: 0:12:33  lr: 0.000081  loss: 0.2715 (0.2742)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [220/689]  eta: 0:12:17  lr: 0.000081  loss: 0.2655 (0.2740)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [230/689]  eta: 0:12:02  lr: 0.000081  loss: 0.2616 (0.2739)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [240/689]  eta: 0:11:46  lr: 0.000081  loss: 0.2769 (0.2741)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [250/689]  eta: 0:11:30  lr: 0.000081  loss: 0.2839 (0.2748)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [260/689]  eta: 0:11:15  lr: 0.000081  loss: 0.2625 (0.2746)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [270/689]  eta: 0:10:59  lr: 0.000081  loss: 0.2613 (0.2746)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [280/689]  eta: 0:10:43  lr: 0.000081  loss: 0.2682 (0.2744)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [290/689]  eta: 0:10:28  lr: 0.000081  loss: 0.2560 (0.2738)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [300/689]  eta: 0:10:12  lr: 0.000081  loss: 0.2488 (0.2738)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [310/689]  eta: 0:09:56  lr: 0.000081  loss: 0.2689 (0.2746)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [320/689]  eta: 0:09:40  lr: 0.000081  loss: 0.2719 (0.2747)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [330/689]  eta: 0:09:25  lr: 0.000081  loss: 0.2674 (0.2745)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [340/689]  eta: 0:09:09  lr: 0.000081  loss: 0.2644 (0.2744)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [350/689]  eta: 0:08:53  lr: 0.000081  loss: 0.2676 (0.2746)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [360/689]  eta: 0:08:38  lr: 0.000081  loss: 0.2653 (0.2743)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [370/689]  eta: 0:08:22  lr: 0.000081  loss: 0.2563 (0.2741)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [380/689]  eta: 0:08:06  lr: 0.000081  loss: 0.2554 (0.2738)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [390/689]  eta: 0:07:50  lr: 0.000081  loss: 0.2538 (0.2735)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [400/689]  eta: 0:07:35  lr: 0.000081  loss: 0.2616 (0.2738)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [410/689]  eta: 0:07:19  lr: 0.000081  loss: 0.2617 (0.2735)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [420/689]  eta: 0:07:03  lr: 0.000081  loss: 0.2589 (0.2738)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [430/689]  eta: 0:06:47  lr: 0.000081  loss: 0.2638 (0.2740)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [440/689]  eta: 0:06:32  lr: 0.000081  loss: 0.2699 (0.2741)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [450/689]  eta: 0:06:16  lr: 0.000081  loss: 0.2763 (0.2740)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [460/689]  eta: 0:06:00  lr: 0.000081  loss: 0.2714 (0.2740)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [470/689]  eta: 0:05:44  lr: 0.000081  loss: 0.2691 (0.2738)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [480/689]  eta: 0:05:29  lr: 0.000081  loss: 0.2638 (0.2738)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [490/689]  eta: 0:05:13  lr: 0.000081  loss: 0.2663 (0.2737)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [500/689]  eta: 0:04:57  lr: 0.000081  loss: 0.2663 (0.2737)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [510/689]  eta: 0:04:41  lr: 0.000081  loss: 0.2629 (0.2736)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [520/689]  eta: 0:04:26  lr: 0.000081  loss: 0.2538 (0.2734)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [530/689]  eta: 0:04:10  lr: 0.000081  loss: 0.2462 (0.2732)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [540/689]  eta: 0:03:54  lr: 0.000081  loss: 0.2532 (0.2730)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [550/689]  eta: 0:03:38  lr: 0.000081  loss: 0.2624 (0.2728)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [560/689]  eta: 0:03:23  lr: 0.000081  loss: 0.2543 (0.2726)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [570/689]  eta: 0:03:07  lr: 0.000081  loss: 0.2555 (0.2727)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [580/689]  eta: 0:02:51  lr: 0.000081  loss: 0.2585 (0.2726)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [590/689]  eta: 0:02:35  lr: 0.000081  loss: 0.2611 (0.2727)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [600/689]  eta: 0:02:20  lr: 0.000081  loss: 0.2579 (0.2727)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [610/689]  eta: 0:02:04  lr: 0.000081  loss: 0.2569 (0.2725)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [620/689]  eta: 0:01:48  lr: 0.000081  loss: 0.2616 (0.2724)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [630/689]  eta: 0:01:32  lr: 0.000081  loss: 0.2630 (0.2723)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [640/689]  eta: 0:01:17  lr: 0.000081  loss: 0.2653 (0.2726)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [650/689]  eta: 0:01:01  lr: 0.000081  loss: 0.2559 (0.2724)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [660/689]  eta: 0:00:45  lr: 0.000081  loss: 0.2558 (0.2726)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [670/689]  eta: 0:00:29  lr: 0.000081  loss: 0.2632 (0.2726)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270]  [680/689]  eta: 0:00:14  lr: 0.000081  loss: 0.2676 (0.2726)  time: 1.5755  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:270]  [688/689]  eta: 0:00:01  lr: 0.000081  loss: 0.2633 (0.2724)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:270] Total time: 0:18:05 (1.5752 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: 0.2633 (0.2724)\n",
      "Valid: [epoch:270]  [ 0/14]  eta: 0:00:14  loss: 0.2286 (0.2286)  time: 1.0076  data: 0.3921  max mem: 39763\n",
      "Valid: [epoch:270]  [13/14]  eta: 0:00:00  loss: 0.2550 (0.2581)  time: 0.1139  data: 0.0281  max mem: 39763\n",
      "Valid: [epoch:270] Total time: 0:00:01 (0.1229 s / it)\n",
      "Averaged stats: loss: 0.2550 (0.2581)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_270_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.258%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:271]  [  0/689]  eta: 0:11:45  lr: 0.000081  loss: 0.2452 (0.2452)  time: 1.0241  data: 0.5532  max mem: 39763\n",
      "Train: [epoch:271]  [ 10/689]  eta: 0:17:14  lr: 0.000081  loss: 0.2811 (0.2848)  time: 1.5230  data: 0.0504  max mem: 39763\n",
      "Train: [epoch:271]  [ 20/689]  eta: 0:17:15  lr: 0.000081  loss: 0.2718 (0.2753)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [ 30/689]  eta: 0:17:05  lr: 0.000081  loss: 0.2656 (0.2775)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [ 40/689]  eta: 0:16:52  lr: 0.000081  loss: 0.2787 (0.2765)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [ 50/689]  eta: 0:16:39  lr: 0.000081  loss: 0.2627 (0.2765)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [ 60/689]  eta: 0:16:24  lr: 0.000081  loss: 0.2541 (0.2738)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [ 70/689]  eta: 0:16:09  lr: 0.000081  loss: 0.2537 (0.2736)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [ 80/689]  eta: 0:15:54  lr: 0.000081  loss: 0.2537 (0.2725)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [ 90/689]  eta: 0:15:39  lr: 0.000081  loss: 0.2701 (0.2748)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [100/689]  eta: 0:15:24  lr: 0.000081  loss: 0.2720 (0.2749)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [110/689]  eta: 0:15:09  lr: 0.000081  loss: 0.2616 (0.2748)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [120/689]  eta: 0:14:53  lr: 0.000081  loss: 0.2707 (0.2762)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [130/689]  eta: 0:14:38  lr: 0.000081  loss: 0.2675 (0.2751)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [140/689]  eta: 0:14:22  lr: 0.000081  loss: 0.2579 (0.2750)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [150/689]  eta: 0:14:07  lr: 0.000081  loss: 0.2654 (0.2747)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [160/689]  eta: 0:13:51  lr: 0.000081  loss: 0.2645 (0.2744)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [170/689]  eta: 0:13:36  lr: 0.000081  loss: 0.2609 (0.2744)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [180/689]  eta: 0:13:20  lr: 0.000081  loss: 0.2631 (0.2737)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [190/689]  eta: 0:13:04  lr: 0.000081  loss: 0.2596 (0.2733)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [200/689]  eta: 0:12:49  lr: 0.000081  loss: 0.2567 (0.2733)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [210/689]  eta: 0:12:33  lr: 0.000081  loss: 0.2805 (0.2737)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [220/689]  eta: 0:12:17  lr: 0.000081  loss: 0.2649 (0.2735)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [230/689]  eta: 0:12:02  lr: 0.000081  loss: 0.2649 (0.2740)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [240/689]  eta: 0:11:46  lr: 0.000081  loss: 0.2914 (0.2738)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [250/689]  eta: 0:11:30  lr: 0.000081  loss: 0.2685 (0.2742)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [260/689]  eta: 0:11:15  lr: 0.000081  loss: 0.2712 (0.2743)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [270/689]  eta: 0:10:59  lr: 0.000081  loss: 0.2609 (0.2736)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [280/689]  eta: 0:10:43  lr: 0.000081  loss: 0.2593 (0.2732)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [290/689]  eta: 0:10:27  lr: 0.000081  loss: 0.2593 (0.2729)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [300/689]  eta: 0:10:12  lr: 0.000081  loss: 0.2463 (0.2725)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [310/689]  eta: 0:09:56  lr: 0.000081  loss: 0.2530 (0.2720)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [320/689]  eta: 0:09:40  lr: 0.000081  loss: 0.2668 (0.2721)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [330/689]  eta: 0:09:25  lr: 0.000081  loss: 0.2684 (0.2718)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [340/689]  eta: 0:09:09  lr: 0.000081  loss: 0.2583 (0.2717)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [350/689]  eta: 0:08:53  lr: 0.000081  loss: 0.2614 (0.2715)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [360/689]  eta: 0:08:37  lr: 0.000081  loss: 0.2614 (0.2716)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [370/689]  eta: 0:08:22  lr: 0.000081  loss: 0.2560 (0.2713)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [380/689]  eta: 0:08:06  lr: 0.000081  loss: 0.2588 (0.2713)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [390/689]  eta: 0:07:50  lr: 0.000081  loss: 0.2688 (0.2711)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [400/689]  eta: 0:07:34  lr: 0.000081  loss: 0.2708 (0.2715)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [410/689]  eta: 0:07:19  lr: 0.000081  loss: 0.2771 (0.2717)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [420/689]  eta: 0:07:03  lr: 0.000081  loss: 0.2695 (0.2716)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [430/689]  eta: 0:06:47  lr: 0.000081  loss: 0.2571 (0.2715)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [440/689]  eta: 0:06:32  lr: 0.000081  loss: 0.2579 (0.2713)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [450/689]  eta: 0:06:16  lr: 0.000081  loss: 0.2527 (0.2711)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [460/689]  eta: 0:06:00  lr: 0.000081  loss: 0.2527 (0.2713)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [470/689]  eta: 0:05:44  lr: 0.000081  loss: 0.2631 (0.2712)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [480/689]  eta: 0:05:29  lr: 0.000081  loss: 0.2571 (0.2712)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [490/689]  eta: 0:05:13  lr: 0.000081  loss: 0.2597 (0.2712)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [500/689]  eta: 0:04:57  lr: 0.000081  loss: 0.2647 (0.2713)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [510/689]  eta: 0:04:41  lr: 0.000081  loss: 0.2665 (0.2714)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [520/689]  eta: 0:04:26  lr: 0.000081  loss: 0.2665 (0.2711)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [530/689]  eta: 0:04:10  lr: 0.000081  loss: 0.2617 (0.2712)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [540/689]  eta: 0:03:54  lr: 0.000081  loss: 0.2763 (0.2712)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [550/689]  eta: 0:03:38  lr: 0.000081  loss: 0.2596 (0.2712)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [560/689]  eta: 0:03:23  lr: 0.000081  loss: 0.2759 (0.2714)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [570/689]  eta: 0:03:07  lr: 0.000081  loss: 0.2802 (0.2716)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [580/689]  eta: 0:02:51  lr: 0.000081  loss: 0.2704 (0.2714)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [590/689]  eta: 0:02:35  lr: 0.000081  loss: 0.2618 (0.2714)  time: 1.5750  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:271]  [600/689]  eta: 0:02:20  lr: 0.000081  loss: 0.2621 (0.2714)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [610/689]  eta: 0:02:04  lr: 0.000081  loss: 0.2639 (0.2714)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [620/689]  eta: 0:01:48  lr: 0.000081  loss: 0.2574 (0.2712)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [630/689]  eta: 0:01:32  lr: 0.000081  loss: 0.2566 (0.2711)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [640/689]  eta: 0:01:17  lr: 0.000081  loss: 0.2608 (0.2711)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [650/689]  eta: 0:01:01  lr: 0.000081  loss: 0.2608 (0.2710)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [660/689]  eta: 0:00:45  lr: 0.000081  loss: 0.2658 (0.2712)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [670/689]  eta: 0:00:29  lr: 0.000081  loss: 0.2676 (0.2711)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [680/689]  eta: 0:00:14  lr: 0.000081  loss: 0.2767 (0.2713)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271]  [688/689]  eta: 0:00:01  lr: 0.000081  loss: 0.2857 (0.2715)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:271] Total time: 0:18:05 (1.5749 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: 0.2857 (0.2715)\n",
      "Valid: [epoch:271]  [ 0/14]  eta: 0:00:14  loss: 0.2688 (0.2688)  time: 1.0112  data: 0.3778  max mem: 39763\n",
      "Valid: [epoch:271]  [13/14]  eta: 0:00:00  loss: 0.2544 (0.2573)  time: 0.1141  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:271] Total time: 0:00:01 (0.1232 s / it)\n",
      "Averaged stats: loss: 0.2544 (0.2573)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_271_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.257%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:272]  [  0/689]  eta: 0:11:57  lr: 0.000081  loss: 0.2654 (0.2654)  time: 1.0417  data: 0.5722  max mem: 39763\n",
      "Train: [epoch:272]  [ 10/689]  eta: 0:17:16  lr: 0.000081  loss: 0.2783 (0.2731)  time: 1.5262  data: 0.0521  max mem: 39763\n",
      "Train: [epoch:272]  [ 20/689]  eta: 0:17:16  lr: 0.000081  loss: 0.2650 (0.2704)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [ 30/689]  eta: 0:17:06  lr: 0.000081  loss: 0.2600 (0.2746)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [ 40/689]  eta: 0:16:53  lr: 0.000081  loss: 0.2743 (0.2741)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [ 50/689]  eta: 0:16:39  lr: 0.000081  loss: 0.2766 (0.2766)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [ 60/689]  eta: 0:16:24  lr: 0.000081  loss: 0.2699 (0.2756)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [ 70/689]  eta: 0:16:10  lr: 0.000081  loss: 0.2651 (0.2758)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [ 80/689]  eta: 0:15:54  lr: 0.000081  loss: 0.2728 (0.2763)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [ 90/689]  eta: 0:15:39  lr: 0.000081  loss: 0.2728 (0.2753)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [100/689]  eta: 0:15:24  lr: 0.000081  loss: 0.2723 (0.2764)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [110/689]  eta: 0:15:08  lr: 0.000081  loss: 0.2831 (0.2772)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [120/689]  eta: 0:14:53  lr: 0.000081  loss: 0.2674 (0.2760)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [130/689]  eta: 0:14:37  lr: 0.000081  loss: 0.2575 (0.2755)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [140/689]  eta: 0:14:22  lr: 0.000081  loss: 0.2467 (0.2736)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [150/689]  eta: 0:14:06  lr: 0.000081  loss: 0.2519 (0.2725)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [160/689]  eta: 0:13:51  lr: 0.000081  loss: 0.2604 (0.2731)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [170/689]  eta: 0:13:35  lr: 0.000081  loss: 0.2669 (0.2733)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [180/689]  eta: 0:13:20  lr: 0.000081  loss: 0.2696 (0.2736)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [190/689]  eta: 0:13:04  lr: 0.000081  loss: 0.2698 (0.2741)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [200/689]  eta: 0:12:48  lr: 0.000081  loss: 0.2553 (0.2738)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [210/689]  eta: 0:12:33  lr: 0.000081  loss: 0.2603 (0.2734)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [220/689]  eta: 0:12:17  lr: 0.000081  loss: 0.2609 (0.2734)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [230/689]  eta: 0:12:01  lr: 0.000081  loss: 0.2608 (0.2728)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [240/689]  eta: 0:11:46  lr: 0.000081  loss: 0.2639 (0.2725)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [250/689]  eta: 0:11:30  lr: 0.000081  loss: 0.2507 (0.2720)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [260/689]  eta: 0:11:14  lr: 0.000081  loss: 0.2543 (0.2715)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [270/689]  eta: 0:10:58  lr: 0.000081  loss: 0.2543 (0.2712)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [280/689]  eta: 0:10:43  lr: 0.000081  loss: 0.2471 (0.2709)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [290/689]  eta: 0:10:27  lr: 0.000081  loss: 0.2608 (0.2712)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [300/689]  eta: 0:10:11  lr: 0.000081  loss: 0.2614 (0.2710)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [310/689]  eta: 0:09:56  lr: 0.000081  loss: 0.2660 (0.2709)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [320/689]  eta: 0:09:40  lr: 0.000081  loss: 0.2611 (0.2705)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [330/689]  eta: 0:09:24  lr: 0.000081  loss: 0.2618 (0.2705)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [340/689]  eta: 0:09:09  lr: 0.000081  loss: 0.2635 (0.2704)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [350/689]  eta: 0:08:53  lr: 0.000081  loss: 0.2696 (0.2711)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [360/689]  eta: 0:08:37  lr: 0.000081  loss: 0.2817 (0.2715)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [370/689]  eta: 0:08:21  lr: 0.000081  loss: 0.2764 (0.2718)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [380/689]  eta: 0:08:06  lr: 0.000081  loss: 0.2790 (0.2719)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [390/689]  eta: 0:07:50  lr: 0.000081  loss: 0.2736 (0.2717)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [400/689]  eta: 0:07:34  lr: 0.000081  loss: 0.2680 (0.2721)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [410/689]  eta: 0:07:18  lr: 0.000081  loss: 0.2680 (0.2723)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [420/689]  eta: 0:07:03  lr: 0.000081  loss: 0.2669 (0.2721)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [430/689]  eta: 0:06:47  lr: 0.000081  loss: 0.2650 (0.2720)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [440/689]  eta: 0:06:31  lr: 0.000081  loss: 0.2697 (0.2721)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [450/689]  eta: 0:06:16  lr: 0.000081  loss: 0.2697 (0.2721)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [460/689]  eta: 0:06:00  lr: 0.000081  loss: 0.2677 (0.2724)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [470/689]  eta: 0:05:44  lr: 0.000081  loss: 0.2677 (0.2724)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [480/689]  eta: 0:05:28  lr: 0.000081  loss: 0.2591 (0.2722)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [490/689]  eta: 0:05:13  lr: 0.000081  loss: 0.2696 (0.2722)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [500/689]  eta: 0:04:57  lr: 0.000081  loss: 0.2696 (0.2722)  time: 1.5739  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:272]  [510/689]  eta: 0:04:41  lr: 0.000081  loss: 0.2644 (0.2721)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [520/689]  eta: 0:04:25  lr: 0.000081  loss: 0.2621 (0.2721)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [530/689]  eta: 0:04:10  lr: 0.000081  loss: 0.2621 (0.2720)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [540/689]  eta: 0:03:54  lr: 0.000081  loss: 0.2672 (0.2723)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [550/689]  eta: 0:03:38  lr: 0.000081  loss: 0.2677 (0.2722)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [560/689]  eta: 0:03:22  lr: 0.000081  loss: 0.2634 (0.2724)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [570/689]  eta: 0:03:07  lr: 0.000081  loss: 0.2888 (0.2731)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [580/689]  eta: 0:02:51  lr: 0.000081  loss: 0.2991 (0.2732)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [590/689]  eta: 0:02:35  lr: 0.000081  loss: 0.2837 (0.2733)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [600/689]  eta: 0:02:20  lr: 0.000081  loss: 0.2823 (0.2736)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [610/689]  eta: 0:02:04  lr: 0.000081  loss: 0.2676 (0.2734)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [620/689]  eta: 0:01:48  lr: 0.000081  loss: 0.2579 (0.2733)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [630/689]  eta: 0:01:32  lr: 0.000081  loss: 0.2579 (0.2733)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [640/689]  eta: 0:01:17  lr: 0.000081  loss: 0.2624 (0.2732)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [650/689]  eta: 0:01:01  lr: 0.000081  loss: 0.2605 (0.2731)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [660/689]  eta: 0:00:45  lr: 0.000081  loss: 0.2574 (0.2728)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [670/689]  eta: 0:00:29  lr: 0.000081  loss: 0.2574 (0.2729)  time: 1.5730  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [680/689]  eta: 0:00:14  lr: 0.000081  loss: 0.2631 (0.2727)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272]  [688/689]  eta: 0:00:01  lr: 0.000081  loss: 0.2625 (0.2725)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:272] Total time: 0:18:04 (1.5739 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: 0.2625 (0.2725)\n",
      "Valid: [epoch:272]  [ 0/14]  eta: 0:00:14  loss: 0.2693 (0.2693)  time: 1.0144  data: 0.3644  max mem: 39763\n",
      "Valid: [epoch:272]  [13/14]  eta: 0:00:00  loss: 0.2551 (0.2582)  time: 0.1143  data: 0.0261  max mem: 39763\n",
      "Valid: [epoch:272] Total time: 0:00:01 (0.1237 s / it)\n",
      "Averaged stats: loss: 0.2551 (0.2582)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_272_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.258%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:273]  [  0/689]  eta: 0:11:51  lr: 0.000081  loss: 0.3080 (0.3080)  time: 1.0328  data: 0.5595  max mem: 39763\n",
      "Train: [epoch:273]  [ 10/689]  eta: 0:17:13  lr: 0.000081  loss: 0.2563 (0.2822)  time: 1.5221  data: 0.0509  max mem: 39763\n",
      "Train: [epoch:273]  [ 20/689]  eta: 0:17:14  lr: 0.000081  loss: 0.2563 (0.2790)  time: 1.5716  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [ 30/689]  eta: 0:17:04  lr: 0.000081  loss: 0.2703 (0.2795)  time: 1.5725  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [ 40/689]  eta: 0:16:51  lr: 0.000081  loss: 0.2637 (0.2748)  time: 1.5728  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [ 50/689]  eta: 0:16:38  lr: 0.000081  loss: 0.2531 (0.2732)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [ 60/689]  eta: 0:16:23  lr: 0.000081  loss: 0.2751 (0.2736)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [ 70/689]  eta: 0:16:08  lr: 0.000081  loss: 0.2772 (0.2731)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [ 80/689]  eta: 0:15:54  lr: 0.000081  loss: 0.2737 (0.2737)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [ 90/689]  eta: 0:15:38  lr: 0.000081  loss: 0.2725 (0.2744)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [100/689]  eta: 0:15:23  lr: 0.000081  loss: 0.2686 (0.2748)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [110/689]  eta: 0:15:08  lr: 0.000081  loss: 0.2642 (0.2739)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [120/689]  eta: 0:14:52  lr: 0.000081  loss: 0.2637 (0.2738)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [130/689]  eta: 0:14:37  lr: 0.000081  loss: 0.2705 (0.2746)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [140/689]  eta: 0:14:21  lr: 0.000081  loss: 0.2636 (0.2744)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [150/689]  eta: 0:14:06  lr: 0.000081  loss: 0.2654 (0.2746)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [160/689]  eta: 0:13:50  lr: 0.000081  loss: 0.2737 (0.2745)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [170/689]  eta: 0:13:35  lr: 0.000081  loss: 0.2609 (0.2743)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [180/689]  eta: 0:13:19  lr: 0.000081  loss: 0.2538 (0.2742)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [190/689]  eta: 0:13:03  lr: 0.000081  loss: 0.2617 (0.2746)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [200/689]  eta: 0:12:48  lr: 0.000081  loss: 0.2676 (0.2742)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [210/689]  eta: 0:12:32  lr: 0.000081  loss: 0.2633 (0.2740)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [220/689]  eta: 0:12:16  lr: 0.000081  loss: 0.2631 (0.2730)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [230/689]  eta: 0:12:01  lr: 0.000081  loss: 0.2528 (0.2728)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [240/689]  eta: 0:11:45  lr: 0.000081  loss: 0.2600 (0.2727)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [250/689]  eta: 0:11:30  lr: 0.000081  loss: 0.2649 (0.2724)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [260/689]  eta: 0:11:14  lr: 0.000081  loss: 0.2698 (0.2728)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [270/689]  eta: 0:10:58  lr: 0.000081  loss: 0.2650 (0.2724)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [280/689]  eta: 0:10:42  lr: 0.000081  loss: 0.2621 (0.2728)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [290/689]  eta: 0:10:27  lr: 0.000081  loss: 0.2611 (0.2721)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [300/689]  eta: 0:10:11  lr: 0.000081  loss: 0.2611 (0.2720)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [310/689]  eta: 0:09:55  lr: 0.000081  loss: 0.2567 (0.2715)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [320/689]  eta: 0:09:40  lr: 0.000081  loss: 0.2545 (0.2712)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [330/689]  eta: 0:09:24  lr: 0.000081  loss: 0.2610 (0.2710)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [340/689]  eta: 0:09:08  lr: 0.000081  loss: 0.2637 (0.2710)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [350/689]  eta: 0:08:53  lr: 0.000081  loss: 0.2567 (0.2709)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [360/689]  eta: 0:08:37  lr: 0.000081  loss: 0.2678 (0.2712)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [370/689]  eta: 0:08:21  lr: 0.000081  loss: 0.2690 (0.2711)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [380/689]  eta: 0:08:05  lr: 0.000081  loss: 0.2766 (0.2715)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [390/689]  eta: 0:07:50  lr: 0.000081  loss: 0.2735 (0.2719)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [400/689]  eta: 0:07:34  lr: 0.000081  loss: 0.2527 (0.2714)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [410/689]  eta: 0:07:18  lr: 0.000081  loss: 0.2549 (0.2716)  time: 1.5742  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:273]  [420/689]  eta: 0:07:03  lr: 0.000081  loss: 0.2665 (0.2718)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [430/689]  eta: 0:06:47  lr: 0.000081  loss: 0.2669 (0.2718)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [440/689]  eta: 0:06:31  lr: 0.000081  loss: 0.2650 (0.2717)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [450/689]  eta: 0:06:15  lr: 0.000081  loss: 0.2585 (0.2713)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [460/689]  eta: 0:06:00  lr: 0.000081  loss: 0.2614 (0.2719)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [470/689]  eta: 0:05:44  lr: 0.000081  loss: 0.2900 (0.2721)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [480/689]  eta: 0:05:28  lr: 0.000081  loss: 0.2794 (0.2722)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [490/689]  eta: 0:05:13  lr: 0.000081  loss: 0.2759 (0.2725)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [500/689]  eta: 0:04:57  lr: 0.000081  loss: 0.2759 (0.2728)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [510/689]  eta: 0:04:41  lr: 0.000081  loss: 0.2647 (0.2729)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [520/689]  eta: 0:04:25  lr: 0.000081  loss: 0.2647 (0.2733)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [530/689]  eta: 0:04:10  lr: 0.000081  loss: 0.2749 (0.2733)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [540/689]  eta: 0:03:54  lr: 0.000081  loss: 0.2705 (0.2733)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [550/689]  eta: 0:03:38  lr: 0.000081  loss: 0.2639 (0.2733)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [560/689]  eta: 0:03:22  lr: 0.000081  loss: 0.2829 (0.2736)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [570/689]  eta: 0:03:07  lr: 0.000081  loss: 0.2708 (0.2734)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [580/689]  eta: 0:02:51  lr: 0.000081  loss: 0.2620 (0.2735)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [590/689]  eta: 0:02:35  lr: 0.000081  loss: 0.2581 (0.2732)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [600/689]  eta: 0:02:20  lr: 0.000081  loss: 0.2539 (0.2730)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [610/689]  eta: 0:02:04  lr: 0.000081  loss: 0.2573 (0.2728)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [620/689]  eta: 0:01:48  lr: 0.000081  loss: 0.2683 (0.2730)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [630/689]  eta: 0:01:32  lr: 0.000081  loss: 0.2727 (0.2732)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [640/689]  eta: 0:01:17  lr: 0.000081  loss: 0.2634 (0.2731)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [650/689]  eta: 0:01:01  lr: 0.000081  loss: 0.2622 (0.2731)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [660/689]  eta: 0:00:45  lr: 0.000081  loss: 0.2711 (0.2730)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [670/689]  eta: 0:00:29  lr: 0.000081  loss: 0.2641 (0.2729)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [680/689]  eta: 0:00:14  lr: 0.000081  loss: 0.2618 (0.2729)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273]  [688/689]  eta: 0:00:01  lr: 0.000081  loss: 0.2685 (0.2729)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:273] Total time: 0:18:04 (1.5736 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: 0.2685 (0.2729)\n",
      "Valid: [epoch:273]  [ 0/14]  eta: 0:00:14  loss: 0.2548 (0.2548)  time: 1.0391  data: 0.3585  max mem: 39763\n",
      "Valid: [epoch:273]  [13/14]  eta: 0:00:00  loss: 0.2553 (0.2598)  time: 0.1161  data: 0.0257  max mem: 39763\n",
      "Valid: [epoch:273] Total time: 0:00:01 (0.1252 s / it)\n",
      "Averaged stats: loss: 0.2553 (0.2598)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_273_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.260%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:274]  [  0/689]  eta: 0:11:45  lr: 0.000081  loss: 0.2390 (0.2390)  time: 1.0232  data: 0.5468  max mem: 39763\n",
      "Train: [epoch:274]  [ 10/689]  eta: 0:17:13  lr: 0.000081  loss: 0.2644 (0.2633)  time: 1.5226  data: 0.0498  max mem: 39763\n",
      "Train: [epoch:274]  [ 20/689]  eta: 0:17:14  lr: 0.000081  loss: 0.2671 (0.2701)  time: 1.5730  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [ 30/689]  eta: 0:17:04  lr: 0.000081  loss: 0.2729 (0.2738)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [ 40/689]  eta: 0:16:52  lr: 0.000081  loss: 0.2640 (0.2747)  time: 1.5729  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [ 50/689]  eta: 0:16:38  lr: 0.000081  loss: 0.2567 (0.2697)  time: 1.5725  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [ 60/689]  eta: 0:16:23  lr: 0.000081  loss: 0.2567 (0.2716)  time: 1.5728  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [ 70/689]  eta: 0:16:08  lr: 0.000081  loss: 0.2710 (0.2720)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [ 80/689]  eta: 0:15:53  lr: 0.000081  loss: 0.2776 (0.2743)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [ 90/689]  eta: 0:15:38  lr: 0.000081  loss: 0.2668 (0.2727)  time: 1.5729  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [100/689]  eta: 0:15:23  lr: 0.000081  loss: 0.2668 (0.2745)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [110/689]  eta: 0:15:07  lr: 0.000081  loss: 0.2673 (0.2747)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [120/689]  eta: 0:14:52  lr: 0.000081  loss: 0.2679 (0.2736)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [130/689]  eta: 0:14:37  lr: 0.000081  loss: 0.2742 (0.2743)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [140/689]  eta: 0:14:21  lr: 0.000081  loss: 0.2699 (0.2734)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [150/689]  eta: 0:14:06  lr: 0.000081  loss: 0.2674 (0.2729)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [160/689]  eta: 0:13:50  lr: 0.000081  loss: 0.2689 (0.2732)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [170/689]  eta: 0:13:34  lr: 0.000081  loss: 0.2689 (0.2739)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [180/689]  eta: 0:13:19  lr: 0.000081  loss: 0.2760 (0.2746)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [190/689]  eta: 0:13:03  lr: 0.000081  loss: 0.2649 (0.2744)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [200/689]  eta: 0:12:48  lr: 0.000081  loss: 0.2632 (0.2738)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [210/689]  eta: 0:12:32  lr: 0.000081  loss: 0.2546 (0.2730)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [220/689]  eta: 0:12:16  lr: 0.000081  loss: 0.2637 (0.2742)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [230/689]  eta: 0:12:01  lr: 0.000081  loss: 0.2590 (0.2732)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [240/689]  eta: 0:11:45  lr: 0.000081  loss: 0.2443 (0.2726)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [250/689]  eta: 0:11:29  lr: 0.000081  loss: 0.2622 (0.2735)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [260/689]  eta: 0:11:14  lr: 0.000081  loss: 0.2619 (0.2727)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [270/689]  eta: 0:10:58  lr: 0.000081  loss: 0.2550 (0.2726)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [280/689]  eta: 0:10:42  lr: 0.000081  loss: 0.2540 (0.2725)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [290/689]  eta: 0:10:27  lr: 0.000081  loss: 0.2540 (0.2724)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [300/689]  eta: 0:10:11  lr: 0.000081  loss: 0.2687 (0.2727)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [310/689]  eta: 0:09:55  lr: 0.000081  loss: 0.2687 (0.2725)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [320/689]  eta: 0:09:40  lr: 0.000081  loss: 0.2698 (0.2727)  time: 1.5741  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:274]  [330/689]  eta: 0:09:24  lr: 0.000081  loss: 0.2698 (0.2728)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [340/689]  eta: 0:09:08  lr: 0.000081  loss: 0.2612 (0.2726)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [350/689]  eta: 0:08:52  lr: 0.000081  loss: 0.2742 (0.2732)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [360/689]  eta: 0:08:37  lr: 0.000081  loss: 0.2852 (0.2735)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [370/689]  eta: 0:08:21  lr: 0.000081  loss: 0.2758 (0.2736)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [380/689]  eta: 0:08:05  lr: 0.000081  loss: 0.2693 (0.2740)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [390/689]  eta: 0:07:50  lr: 0.000081  loss: 0.2953 (0.2746)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [400/689]  eta: 0:07:34  lr: 0.000081  loss: 0.2688 (0.2744)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [410/689]  eta: 0:07:18  lr: 0.000081  loss: 0.2640 (0.2747)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [420/689]  eta: 0:07:02  lr: 0.000081  loss: 0.2623 (0.2743)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [430/689]  eta: 0:06:47  lr: 0.000081  loss: 0.2561 (0.2742)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [440/689]  eta: 0:06:31  lr: 0.000081  loss: 0.2594 (0.2741)  time: 1.5729  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [450/689]  eta: 0:06:15  lr: 0.000081  loss: 0.2594 (0.2737)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [460/689]  eta: 0:06:00  lr: 0.000081  loss: 0.2632 (0.2740)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [470/689]  eta: 0:05:44  lr: 0.000081  loss: 0.2818 (0.2744)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [480/689]  eta: 0:05:28  lr: 0.000081  loss: 0.2727 (0.2744)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [490/689]  eta: 0:05:12  lr: 0.000081  loss: 0.2699 (0.2744)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [500/689]  eta: 0:04:57  lr: 0.000081  loss: 0.2703 (0.2742)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [510/689]  eta: 0:04:41  lr: 0.000081  loss: 0.2659 (0.2742)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [520/689]  eta: 0:04:25  lr: 0.000081  loss: 0.2707 (0.2743)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [530/689]  eta: 0:04:10  lr: 0.000081  loss: 0.2707 (0.2743)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [540/689]  eta: 0:03:54  lr: 0.000081  loss: 0.2649 (0.2742)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [550/689]  eta: 0:03:38  lr: 0.000081  loss: 0.2649 (0.2742)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [560/689]  eta: 0:03:22  lr: 0.000081  loss: 0.2675 (0.2742)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [570/689]  eta: 0:03:07  lr: 0.000081  loss: 0.2692 (0.2743)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [580/689]  eta: 0:02:51  lr: 0.000081  loss: 0.2761 (0.2743)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [590/689]  eta: 0:02:35  lr: 0.000081  loss: 0.2811 (0.2744)  time: 1.5725  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [600/689]  eta: 0:02:19  lr: 0.000081  loss: 0.2737 (0.2744)  time: 1.5729  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [610/689]  eta: 0:02:04  lr: 0.000081  loss: 0.2619 (0.2743)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [620/689]  eta: 0:01:48  lr: 0.000081  loss: 0.2537 (0.2741)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [630/689]  eta: 0:01:32  lr: 0.000081  loss: 0.2612 (0.2739)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [640/689]  eta: 0:01:17  lr: 0.000081  loss: 0.2652 (0.2740)  time: 1.5730  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [650/689]  eta: 0:01:01  lr: 0.000081  loss: 0.2820 (0.2743)  time: 1.5723  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [660/689]  eta: 0:00:45  lr: 0.000081  loss: 0.2729 (0.2743)  time: 1.5723  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [670/689]  eta: 0:00:29  lr: 0.000081  loss: 0.2729 (0.2744)  time: 1.5724  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [680/689]  eta: 0:00:14  lr: 0.000081  loss: 0.2780 (0.2744)  time: 1.5729  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274]  [688/689]  eta: 0:00:01  lr: 0.000081  loss: 0.2608 (0.2743)  time: 1.5731  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:274] Total time: 0:18:03 (1.5731 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: 0.2608 (0.2743)\n",
      "Valid: [epoch:274]  [ 0/14]  eta: 0:00:13  loss: 0.2296 (0.2296)  time: 0.9986  data: 0.3590  max mem: 39763\n",
      "Valid: [epoch:274]  [13/14]  eta: 0:00:00  loss: 0.2564 (0.2598)  time: 0.1132  data: 0.0257  max mem: 39763\n",
      "Valid: [epoch:274] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.2564 (0.2598)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_274_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.260%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:275]  [  0/689]  eta: 0:12:32  lr: 0.000081  loss: 0.3574 (0.3574)  time: 1.0917  data: 0.6157  max mem: 39763\n",
      "Train: [epoch:275]  [ 10/689]  eta: 0:17:16  lr: 0.000081  loss: 0.2671 (0.2773)  time: 1.5271  data: 0.0561  max mem: 39763\n",
      "Train: [epoch:275]  [ 20/689]  eta: 0:17:15  lr: 0.000081  loss: 0.2546 (0.2648)  time: 1.5712  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [ 30/689]  eta: 0:17:05  lr: 0.000081  loss: 0.2517 (0.2693)  time: 1.5722  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [ 40/689]  eta: 0:16:52  lr: 0.000081  loss: 0.2527 (0.2680)  time: 1.5726  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [ 50/689]  eta: 0:16:38  lr: 0.000081  loss: 0.2486 (0.2692)  time: 1.5723  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [ 60/689]  eta: 0:16:23  lr: 0.000081  loss: 0.2657 (0.2701)  time: 1.5724  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [ 70/689]  eta: 0:16:08  lr: 0.000081  loss: 0.2659 (0.2692)  time: 1.5729  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [ 80/689]  eta: 0:15:53  lr: 0.000081  loss: 0.2521 (0.2681)  time: 1.5731  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [ 90/689]  eta: 0:15:38  lr: 0.000081  loss: 0.2488 (0.2675)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [100/689]  eta: 0:15:23  lr: 0.000081  loss: 0.2678 (0.2700)  time: 1.5731  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [110/689]  eta: 0:15:08  lr: 0.000081  loss: 0.2694 (0.2710)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [120/689]  eta: 0:14:52  lr: 0.000081  loss: 0.2619 (0.2706)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [130/689]  eta: 0:14:37  lr: 0.000081  loss: 0.2588 (0.2700)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [140/689]  eta: 0:14:21  lr: 0.000081  loss: 0.2757 (0.2714)  time: 1.5726  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [150/689]  eta: 0:14:05  lr: 0.000081  loss: 0.2826 (0.2715)  time: 1.5725  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [160/689]  eta: 0:13:50  lr: 0.000081  loss: 0.2817 (0.2718)  time: 1.5726  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [170/689]  eta: 0:13:34  lr: 0.000081  loss: 0.2581 (0.2710)  time: 1.5728  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [180/689]  eta: 0:13:19  lr: 0.000081  loss: 0.2653 (0.2721)  time: 1.5727  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [190/689]  eta: 0:13:03  lr: 0.000081  loss: 0.2968 (0.2732)  time: 1.5725  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [200/689]  eta: 0:12:47  lr: 0.000081  loss: 0.2878 (0.2730)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [210/689]  eta: 0:12:32  lr: 0.000081  loss: 0.2605 (0.2731)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [220/689]  eta: 0:12:16  lr: 0.000081  loss: 0.2632 (0.2731)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [230/689]  eta: 0:12:00  lr: 0.000081  loss: 0.2881 (0.2739)  time: 1.5733  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:275]  [240/689]  eta: 0:11:45  lr: 0.000081  loss: 0.2767 (0.2739)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [250/689]  eta: 0:11:29  lr: 0.000081  loss: 0.2668 (0.2735)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [260/689]  eta: 0:11:14  lr: 0.000081  loss: 0.2682 (0.2740)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [270/689]  eta: 0:10:58  lr: 0.000081  loss: 0.2714 (0.2741)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [280/689]  eta: 0:10:42  lr: 0.000081  loss: 0.2773 (0.2747)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [290/689]  eta: 0:10:27  lr: 0.000081  loss: 0.2696 (0.2742)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [300/689]  eta: 0:10:11  lr: 0.000081  loss: 0.2516 (0.2736)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [310/689]  eta: 0:09:55  lr: 0.000081  loss: 0.2654 (0.2739)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [320/689]  eta: 0:09:39  lr: 0.000081  loss: 0.2610 (0.2733)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [330/689]  eta: 0:09:24  lr: 0.000081  loss: 0.2610 (0.2733)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [340/689]  eta: 0:09:08  lr: 0.000081  loss: 0.2794 (0.2733)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [350/689]  eta: 0:08:52  lr: 0.000081  loss: 0.2563 (0.2732)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [360/689]  eta: 0:08:37  lr: 0.000081  loss: 0.2597 (0.2731)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [370/689]  eta: 0:08:21  lr: 0.000081  loss: 0.2646 (0.2730)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [380/689]  eta: 0:08:05  lr: 0.000081  loss: 0.2844 (0.2731)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [390/689]  eta: 0:07:50  lr: 0.000081  loss: 0.2845 (0.2739)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [400/689]  eta: 0:07:34  lr: 0.000081  loss: 0.2869 (0.2743)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [410/689]  eta: 0:07:18  lr: 0.000081  loss: 0.2880 (0.2745)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [420/689]  eta: 0:07:02  lr: 0.000081  loss: 0.2596 (0.2741)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [430/689]  eta: 0:06:47  lr: 0.000081  loss: 0.2604 (0.2742)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [440/689]  eta: 0:06:31  lr: 0.000081  loss: 0.2604 (0.2738)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [450/689]  eta: 0:06:15  lr: 0.000081  loss: 0.2526 (0.2737)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [460/689]  eta: 0:06:00  lr: 0.000081  loss: 0.2743 (0.2740)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [470/689]  eta: 0:05:44  lr: 0.000081  loss: 0.2780 (0.2742)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [480/689]  eta: 0:05:28  lr: 0.000081  loss: 0.2688 (0.2740)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [490/689]  eta: 0:05:12  lr: 0.000081  loss: 0.2535 (0.2740)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [500/689]  eta: 0:04:57  lr: 0.000081  loss: 0.2681 (0.2740)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [510/689]  eta: 0:04:41  lr: 0.000081  loss: 0.2752 (0.2742)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [520/689]  eta: 0:04:25  lr: 0.000081  loss: 0.2738 (0.2740)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [530/689]  eta: 0:04:10  lr: 0.000081  loss: 0.2686 (0.2743)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [540/689]  eta: 0:03:54  lr: 0.000081  loss: 0.2764 (0.2747)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [550/689]  eta: 0:03:38  lr: 0.000081  loss: 0.2715 (0.2747)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [560/689]  eta: 0:03:22  lr: 0.000081  loss: 0.2666 (0.2747)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [570/689]  eta: 0:03:07  lr: 0.000081  loss: 0.2749 (0.2750)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [580/689]  eta: 0:02:51  lr: 0.000081  loss: 0.2869 (0.2748)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [590/689]  eta: 0:02:35  lr: 0.000081  loss: 0.2628 (0.2748)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [600/689]  eta: 0:02:19  lr: 0.000081  loss: 0.2692 (0.2752)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [610/689]  eta: 0:02:04  lr: 0.000081  loss: 0.2758 (0.2750)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [620/689]  eta: 0:01:48  lr: 0.000081  loss: 0.2699 (0.2752)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [630/689]  eta: 0:01:32  lr: 0.000081  loss: 0.2735 (0.2752)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [640/689]  eta: 0:01:17  lr: 0.000081  loss: 0.2618 (0.2748)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [650/689]  eta: 0:01:01  lr: 0.000081  loss: 0.2569 (0.2748)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [660/689]  eta: 0:00:45  lr: 0.000081  loss: 0.2591 (0.2747)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [670/689]  eta: 0:00:29  lr: 0.000081  loss: 0.2523 (0.2748)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [680/689]  eta: 0:00:14  lr: 0.000081  loss: 0.2714 (0.2750)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275]  [688/689]  eta: 0:00:01  lr: 0.000081  loss: 0.2788 (0.2749)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:275] Total time: 0:18:04 (1.5734 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: 0.2788 (0.2749)\n",
      "Valid: [epoch:275]  [ 0/14]  eta: 0:00:13  loss: 0.2739 (0.2739)  time: 0.9977  data: 0.3933  max mem: 39763\n",
      "Valid: [epoch:275]  [13/14]  eta: 0:00:00  loss: 0.2592 (0.2627)  time: 0.1132  data: 0.0281  max mem: 39763\n",
      "Valid: [epoch:275] Total time: 0:00:01 (0.1224 s / it)\n",
      "Averaged stats: loss: 0.2592 (0.2627)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_275_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.263%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:276]  [  0/689]  eta: 0:11:43  lr: 0.000081  loss: 0.2538 (0.2538)  time: 1.0217  data: 0.5430  max mem: 39763\n",
      "Train: [epoch:276]  [ 10/689]  eta: 0:17:13  lr: 0.000081  loss: 0.2538 (0.2712)  time: 1.5221  data: 0.0494  max mem: 39763\n",
      "Train: [epoch:276]  [ 20/689]  eta: 0:17:14  lr: 0.000081  loss: 0.2674 (0.2790)  time: 1.5730  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [ 30/689]  eta: 0:17:05  lr: 0.000081  loss: 0.2748 (0.2815)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [ 40/689]  eta: 0:16:52  lr: 0.000081  loss: 0.2698 (0.2778)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [ 50/689]  eta: 0:16:38  lr: 0.000081  loss: 0.2547 (0.2758)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [ 60/689]  eta: 0:16:23  lr: 0.000081  loss: 0.2624 (0.2755)  time: 1.5729  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [ 70/689]  eta: 0:16:09  lr: 0.000081  loss: 0.2722 (0.2754)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [ 80/689]  eta: 0:15:54  lr: 0.000081  loss: 0.2658 (0.2748)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [ 90/689]  eta: 0:15:39  lr: 0.000081  loss: 0.2650 (0.2745)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [100/689]  eta: 0:15:23  lr: 0.000081  loss: 0.2781 (0.2770)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [110/689]  eta: 0:15:08  lr: 0.000081  loss: 0.2759 (0.2772)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [120/689]  eta: 0:14:52  lr: 0.000081  loss: 0.2622 (0.2762)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [130/689]  eta: 0:14:37  lr: 0.000081  loss: 0.2654 (0.2761)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [140/689]  eta: 0:14:21  lr: 0.000081  loss: 0.2672 (0.2771)  time: 1.5742  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:276]  [150/689]  eta: 0:14:06  lr: 0.000081  loss: 0.2643 (0.2772)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [160/689]  eta: 0:13:50  lr: 0.000081  loss: 0.2624 (0.2759)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [170/689]  eta: 0:13:35  lr: 0.000081  loss: 0.2653 (0.2759)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [180/689]  eta: 0:13:19  lr: 0.000081  loss: 0.2701 (0.2757)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [190/689]  eta: 0:13:04  lr: 0.000081  loss: 0.2619 (0.2755)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [200/689]  eta: 0:12:48  lr: 0.000081  loss: 0.2698 (0.2755)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [210/689]  eta: 0:12:32  lr: 0.000081  loss: 0.2735 (0.2752)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [220/689]  eta: 0:12:17  lr: 0.000081  loss: 0.2802 (0.2759)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [230/689]  eta: 0:12:01  lr: 0.000081  loss: 0.2750 (0.2759)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [240/689]  eta: 0:11:45  lr: 0.000081  loss: 0.2689 (0.2761)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [250/689]  eta: 0:11:30  lr: 0.000081  loss: 0.2741 (0.2761)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [260/689]  eta: 0:11:14  lr: 0.000081  loss: 0.2765 (0.2768)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [270/689]  eta: 0:10:58  lr: 0.000081  loss: 0.2807 (0.2772)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [280/689]  eta: 0:10:43  lr: 0.000081  loss: 0.2661 (0.2767)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [290/689]  eta: 0:10:27  lr: 0.000081  loss: 0.2549 (0.2764)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [300/689]  eta: 0:10:11  lr: 0.000081  loss: 0.2603 (0.2759)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [310/689]  eta: 0:09:55  lr: 0.000081  loss: 0.2620 (0.2755)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [320/689]  eta: 0:09:40  lr: 0.000081  loss: 0.2581 (0.2754)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [330/689]  eta: 0:09:24  lr: 0.000081  loss: 0.2589 (0.2756)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [340/689]  eta: 0:09:08  lr: 0.000081  loss: 0.2655 (0.2757)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [350/689]  eta: 0:08:53  lr: 0.000081  loss: 0.2814 (0.2763)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [360/689]  eta: 0:08:37  lr: 0.000081  loss: 0.2719 (0.2761)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [370/689]  eta: 0:08:21  lr: 0.000081  loss: 0.2719 (0.2765)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [380/689]  eta: 0:08:06  lr: 0.000081  loss: 0.2714 (0.2759)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [390/689]  eta: 0:07:50  lr: 0.000081  loss: 0.2714 (0.2762)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [400/689]  eta: 0:07:34  lr: 0.000081  loss: 0.2824 (0.2765)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [410/689]  eta: 0:07:18  lr: 0.000081  loss: 0.2662 (0.2765)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [420/689]  eta: 0:07:03  lr: 0.000081  loss: 0.2632 (0.2762)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [430/689]  eta: 0:06:47  lr: 0.000081  loss: 0.2470 (0.2758)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [440/689]  eta: 0:06:31  lr: 0.000081  loss: 0.2648 (0.2757)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [450/689]  eta: 0:06:15  lr: 0.000081  loss: 0.2695 (0.2757)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [460/689]  eta: 0:06:00  lr: 0.000081  loss: 0.2988 (0.2765)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [470/689]  eta: 0:05:44  lr: 0.000081  loss: 0.3010 (0.2765)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [480/689]  eta: 0:05:28  lr: 0.000081  loss: 0.2595 (0.2762)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [490/689]  eta: 0:05:13  lr: 0.000081  loss: 0.2670 (0.2765)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [500/689]  eta: 0:04:57  lr: 0.000081  loss: 0.2685 (0.2762)  time: 1.5731  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [510/689]  eta: 0:04:41  lr: 0.000081  loss: 0.2589 (0.2761)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [520/689]  eta: 0:04:25  lr: 0.000081  loss: 0.2694 (0.2761)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [530/689]  eta: 0:04:10  lr: 0.000081  loss: 0.2671 (0.2759)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [540/689]  eta: 0:03:54  lr: 0.000081  loss: 0.2734 (0.2762)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [550/689]  eta: 0:03:38  lr: 0.000081  loss: 0.2763 (0.2759)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [560/689]  eta: 0:03:22  lr: 0.000081  loss: 0.2618 (0.2758)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [570/689]  eta: 0:03:07  lr: 0.000081  loss: 0.2650 (0.2757)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [580/689]  eta: 0:02:51  lr: 0.000081  loss: 0.2571 (0.2756)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [590/689]  eta: 0:02:35  lr: 0.000081  loss: 0.2541 (0.2753)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [600/689]  eta: 0:02:20  lr: 0.000081  loss: 0.2613 (0.2753)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [610/689]  eta: 0:02:04  lr: 0.000081  loss: 0.2692 (0.2751)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [620/689]  eta: 0:01:48  lr: 0.000081  loss: 0.2526 (0.2751)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [630/689]  eta: 0:01:32  lr: 0.000081  loss: 0.2588 (0.2749)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [640/689]  eta: 0:01:17  lr: 0.000081  loss: 0.2702 (0.2753)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [650/689]  eta: 0:01:01  lr: 0.000081  loss: 0.2809 (0.2755)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [660/689]  eta: 0:00:45  lr: 0.000081  loss: 0.2887 (0.2758)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [670/689]  eta: 0:00:29  lr: 0.000081  loss: 0.2789 (0.2759)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [680/689]  eta: 0:00:14  lr: 0.000081  loss: 0.2638 (0.2758)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276]  [688/689]  eta: 0:00:01  lr: 0.000081  loss: 0.2599 (0.2758)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:276] Total time: 0:18:04 (1.5735 s / it)\n",
      "Averaged stats: lr: 0.000081  loss: 0.2599 (0.2758)\n",
      "Valid: [epoch:276]  [ 0/14]  eta: 0:00:14  loss: 0.2404 (0.2404)  time: 1.0002  data: 0.3946  max mem: 39763\n",
      "Valid: [epoch:276]  [13/14]  eta: 0:00:00  loss: 0.2585 (0.2641)  time: 0.1133  data: 0.0282  max mem: 39763\n",
      "Valid: [epoch:276] Total time: 0:00:01 (0.1226 s / it)\n",
      "Averaged stats: loss: 0.2585 (0.2641)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_276_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.264%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:277]  [  0/689]  eta: 0:12:12  lr: 0.000080  loss: 0.2762 (0.2762)  time: 1.0637  data: 0.5860  max mem: 39763\n",
      "Train: [epoch:277]  [ 10/689]  eta: 0:17:15  lr: 0.000080  loss: 0.2711 (0.2766)  time: 1.5244  data: 0.0533  max mem: 39763\n",
      "Train: [epoch:277]  [ 20/689]  eta: 0:17:15  lr: 0.000080  loss: 0.2657 (0.2712)  time: 1.5714  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [ 30/689]  eta: 0:17:04  lr: 0.000080  loss: 0.2749 (0.2765)  time: 1.5723  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [ 40/689]  eta: 0:16:52  lr: 0.000080  loss: 0.2749 (0.2774)  time: 1.5722  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [ 50/689]  eta: 0:16:38  lr: 0.000080  loss: 0.2638 (0.2721)  time: 1.5722  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:277]  [ 60/689]  eta: 0:16:23  lr: 0.000080  loss: 0.2467 (0.2728)  time: 1.5724  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [ 70/689]  eta: 0:16:08  lr: 0.000080  loss: 0.2583 (0.2720)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [ 80/689]  eta: 0:15:53  lr: 0.000080  loss: 0.2701 (0.2721)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [ 90/689]  eta: 0:15:38  lr: 0.000080  loss: 0.2792 (0.2733)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [100/689]  eta: 0:15:23  lr: 0.000080  loss: 0.2753 (0.2727)  time: 1.5731  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [110/689]  eta: 0:15:07  lr: 0.000080  loss: 0.2728 (0.2731)  time: 1.5730  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [120/689]  eta: 0:14:52  lr: 0.000080  loss: 0.2728 (0.2727)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [130/689]  eta: 0:14:37  lr: 0.000080  loss: 0.2782 (0.2737)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [140/689]  eta: 0:14:21  lr: 0.000080  loss: 0.2818 (0.2747)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [150/689]  eta: 0:14:05  lr: 0.000080  loss: 0.2771 (0.2761)  time: 1.5728  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [160/689]  eta: 0:13:50  lr: 0.000080  loss: 0.2649 (0.2772)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [170/689]  eta: 0:13:34  lr: 0.000080  loss: 0.2739 (0.2778)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [180/689]  eta: 0:13:19  lr: 0.000080  loss: 0.2821 (0.2783)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [190/689]  eta: 0:13:03  lr: 0.000080  loss: 0.2951 (0.2794)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [200/689]  eta: 0:12:48  lr: 0.000080  loss: 0.2844 (0.2790)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [210/689]  eta: 0:12:32  lr: 0.000080  loss: 0.2630 (0.2783)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [220/689]  eta: 0:12:16  lr: 0.000080  loss: 0.2712 (0.2787)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [230/689]  eta: 0:12:01  lr: 0.000080  loss: 0.2790 (0.2786)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [240/689]  eta: 0:11:45  lr: 0.000080  loss: 0.2688 (0.2783)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [250/689]  eta: 0:11:29  lr: 0.000080  loss: 0.2809 (0.2789)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [260/689]  eta: 0:11:14  lr: 0.000080  loss: 0.2898 (0.2789)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [270/689]  eta: 0:10:58  lr: 0.000080  loss: 0.2667 (0.2785)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [280/689]  eta: 0:10:42  lr: 0.000080  loss: 0.2607 (0.2783)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [290/689]  eta: 0:10:27  lr: 0.000080  loss: 0.2607 (0.2780)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [300/689]  eta: 0:10:11  lr: 0.000080  loss: 0.2625 (0.2781)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [310/689]  eta: 0:09:55  lr: 0.000080  loss: 0.2539 (0.2772)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [320/689]  eta: 0:09:40  lr: 0.000080  loss: 0.2534 (0.2772)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [330/689]  eta: 0:09:24  lr: 0.000080  loss: 0.2571 (0.2768)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [340/689]  eta: 0:09:08  lr: 0.000080  loss: 0.2505 (0.2763)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [350/689]  eta: 0:08:53  lr: 0.000080  loss: 0.2539 (0.2767)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [360/689]  eta: 0:08:37  lr: 0.000080  loss: 0.2830 (0.2774)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [370/689]  eta: 0:08:21  lr: 0.000080  loss: 0.2726 (0.2773)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [380/689]  eta: 0:08:05  lr: 0.000080  loss: 0.2726 (0.2775)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [390/689]  eta: 0:07:50  lr: 0.000080  loss: 0.2684 (0.2770)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [400/689]  eta: 0:07:34  lr: 0.000080  loss: 0.2539 (0.2768)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [410/689]  eta: 0:07:18  lr: 0.000080  loss: 0.2682 (0.2772)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [420/689]  eta: 0:07:03  lr: 0.000080  loss: 0.2636 (0.2768)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [430/689]  eta: 0:06:47  lr: 0.000080  loss: 0.2604 (0.2767)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [440/689]  eta: 0:06:31  lr: 0.000080  loss: 0.2621 (0.2766)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [450/689]  eta: 0:06:15  lr: 0.000080  loss: 0.2586 (0.2764)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [460/689]  eta: 0:06:00  lr: 0.000080  loss: 0.2598 (0.2768)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [470/689]  eta: 0:05:44  lr: 0.000080  loss: 0.2635 (0.2764)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [480/689]  eta: 0:05:28  lr: 0.000080  loss: 0.2597 (0.2763)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [490/689]  eta: 0:05:12  lr: 0.000080  loss: 0.2816 (0.2768)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [500/689]  eta: 0:04:57  lr: 0.000080  loss: 0.2835 (0.2770)  time: 1.5730  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [510/689]  eta: 0:04:41  lr: 0.000080  loss: 0.2713 (0.2772)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [520/689]  eta: 0:04:25  lr: 0.000080  loss: 0.2807 (0.2774)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [530/689]  eta: 0:04:10  lr: 0.000080  loss: 0.2868 (0.2775)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [540/689]  eta: 0:03:54  lr: 0.000080  loss: 0.2700 (0.2772)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [550/689]  eta: 0:03:38  lr: 0.000080  loss: 0.2544 (0.2771)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [560/689]  eta: 0:03:22  lr: 0.000080  loss: 0.2654 (0.2770)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [570/689]  eta: 0:03:07  lr: 0.000080  loss: 0.2689 (0.2768)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [580/689]  eta: 0:02:51  lr: 0.000080  loss: 0.2605 (0.2767)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [590/689]  eta: 0:02:35  lr: 0.000080  loss: 0.2691 (0.2772)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [600/689]  eta: 0:02:19  lr: 0.000080  loss: 0.2769 (0.2773)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [610/689]  eta: 0:02:04  lr: 0.000080  loss: 0.2729 (0.2775)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [620/689]  eta: 0:01:48  lr: 0.000080  loss: 0.2699 (0.2775)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [630/689]  eta: 0:01:32  lr: 0.000080  loss: 0.2592 (0.2773)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [640/689]  eta: 0:01:17  lr: 0.000080  loss: 0.2654 (0.2774)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [650/689]  eta: 0:01:01  lr: 0.000080  loss: 0.2756 (0.2774)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [660/689]  eta: 0:00:45  lr: 0.000080  loss: 0.2651 (0.2773)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [670/689]  eta: 0:00:29  lr: 0.000080  loss: 0.2697 (0.2774)  time: 1.5730  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [680/689]  eta: 0:00:14  lr: 0.000080  loss: 0.2697 (0.2772)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277]  [688/689]  eta: 0:00:01  lr: 0.000080  loss: 0.2695 (0.2773)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:277] Total time: 0:18:04 (1.5733 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.2695 (0.2773)\n",
      "Valid: [epoch:277]  [ 0/14]  eta: 0:00:14  loss: 0.2884 (0.2884)  time: 1.0022  data: 0.3835  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:277]  [13/14]  eta: 0:00:00  loss: 0.2617 (0.2649)  time: 0.1134  data: 0.0274  max mem: 39763\n",
      "Valid: [epoch:277] Total time: 0:00:01 (0.1227 s / it)\n",
      "Averaged stats: loss: 0.2617 (0.2649)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_277_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.265%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:278]  [  0/689]  eta: 0:12:00  lr: 0.000080  loss: 0.2814 (0.2814)  time: 1.0464  data: 0.5639  max mem: 39763\n",
      "Train: [epoch:278]  [ 10/689]  eta: 0:17:14  lr: 0.000080  loss: 0.2756 (0.2788)  time: 1.5236  data: 0.0513  max mem: 39763\n",
      "Train: [epoch:278]  [ 20/689]  eta: 0:17:15  lr: 0.000080  loss: 0.2633 (0.2789)  time: 1.5725  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [ 30/689]  eta: 0:17:05  lr: 0.000080  loss: 0.2760 (0.2822)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [ 40/689]  eta: 0:16:52  lr: 0.000080  loss: 0.2760 (0.2822)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [ 50/689]  eta: 0:16:38  lr: 0.000080  loss: 0.2789 (0.2838)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [ 60/689]  eta: 0:16:24  lr: 0.000080  loss: 0.2744 (0.2821)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [ 70/689]  eta: 0:16:09  lr: 0.000080  loss: 0.2665 (0.2816)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [ 80/689]  eta: 0:15:54  lr: 0.000080  loss: 0.2665 (0.2805)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [ 90/689]  eta: 0:15:38  lr: 0.000080  loss: 0.2628 (0.2797)  time: 1.5729  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [100/689]  eta: 0:15:23  lr: 0.000080  loss: 0.2851 (0.2811)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [110/689]  eta: 0:15:08  lr: 0.000080  loss: 0.2947 (0.2819)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [120/689]  eta: 0:14:52  lr: 0.000080  loss: 0.2640 (0.2808)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [130/689]  eta: 0:14:37  lr: 0.000080  loss: 0.2617 (0.2807)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [140/689]  eta: 0:14:21  lr: 0.000080  loss: 0.2802 (0.2805)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [150/689]  eta: 0:14:06  lr: 0.000080  loss: 0.2826 (0.2817)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [160/689]  eta: 0:13:50  lr: 0.000080  loss: 0.2904 (0.2817)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [170/689]  eta: 0:13:35  lr: 0.000080  loss: 0.2689 (0.2815)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [180/689]  eta: 0:13:19  lr: 0.000080  loss: 0.2695 (0.2818)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [190/689]  eta: 0:13:03  lr: 0.000080  loss: 0.2893 (0.2829)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [200/689]  eta: 0:12:48  lr: 0.000080  loss: 0.2762 (0.2814)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [210/689]  eta: 0:12:32  lr: 0.000080  loss: 0.2584 (0.2811)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [220/689]  eta: 0:12:16  lr: 0.000080  loss: 0.2767 (0.2812)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [230/689]  eta: 0:12:01  lr: 0.000080  loss: 0.2720 (0.2803)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [240/689]  eta: 0:11:45  lr: 0.000080  loss: 0.2698 (0.2799)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [250/689]  eta: 0:11:29  lr: 0.000080  loss: 0.2725 (0.2798)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [260/689]  eta: 0:11:14  lr: 0.000080  loss: 0.2723 (0.2797)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [270/689]  eta: 0:10:58  lr: 0.000080  loss: 0.2667 (0.2792)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [280/689]  eta: 0:10:42  lr: 0.000080  loss: 0.2646 (0.2791)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [290/689]  eta: 0:10:27  lr: 0.000080  loss: 0.2646 (0.2790)  time: 1.5726  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [300/689]  eta: 0:10:11  lr: 0.000080  loss: 0.2651 (0.2784)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [310/689]  eta: 0:09:55  lr: 0.000080  loss: 0.2659 (0.2781)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [320/689]  eta: 0:09:40  lr: 0.000080  loss: 0.2721 (0.2781)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [330/689]  eta: 0:09:24  lr: 0.000080  loss: 0.2692 (0.2779)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [340/689]  eta: 0:09:08  lr: 0.000080  loss: 0.2640 (0.2776)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [350/689]  eta: 0:08:52  lr: 0.000080  loss: 0.2646 (0.2775)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [360/689]  eta: 0:08:37  lr: 0.000080  loss: 0.2656 (0.2778)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [370/689]  eta: 0:08:21  lr: 0.000080  loss: 0.2746 (0.2781)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [380/689]  eta: 0:08:05  lr: 0.000080  loss: 0.2772 (0.2785)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [390/689]  eta: 0:07:50  lr: 0.000080  loss: 0.2846 (0.2789)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [400/689]  eta: 0:07:34  lr: 0.000080  loss: 0.2846 (0.2790)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [410/689]  eta: 0:07:18  lr: 0.000080  loss: 0.2617 (0.2788)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [420/689]  eta: 0:07:03  lr: 0.000080  loss: 0.2626 (0.2785)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [430/689]  eta: 0:06:47  lr: 0.000080  loss: 0.2675 (0.2786)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [440/689]  eta: 0:06:31  lr: 0.000080  loss: 0.2789 (0.2789)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [450/689]  eta: 0:06:15  lr: 0.000080  loss: 0.2780 (0.2788)  time: 1.5727  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [460/689]  eta: 0:06:00  lr: 0.000080  loss: 0.2780 (0.2788)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [470/689]  eta: 0:05:44  lr: 0.000080  loss: 0.2786 (0.2787)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [480/689]  eta: 0:05:28  lr: 0.000080  loss: 0.2736 (0.2786)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [490/689]  eta: 0:05:12  lr: 0.000080  loss: 0.2736 (0.2787)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [500/689]  eta: 0:04:57  lr: 0.000080  loss: 0.2701 (0.2788)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [510/689]  eta: 0:04:41  lr: 0.000080  loss: 0.2683 (0.2788)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [520/689]  eta: 0:04:25  lr: 0.000080  loss: 0.2638 (0.2789)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [530/689]  eta: 0:04:10  lr: 0.000080  loss: 0.2688 (0.2787)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [540/689]  eta: 0:03:54  lr: 0.000080  loss: 0.2662 (0.2786)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [550/689]  eta: 0:03:38  lr: 0.000080  loss: 0.2831 (0.2792)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [560/689]  eta: 0:03:22  lr: 0.000080  loss: 0.2756 (0.2791)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [570/689]  eta: 0:03:07  lr: 0.000080  loss: 0.2687 (0.2792)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [580/689]  eta: 0:02:51  lr: 0.000080  loss: 0.2794 (0.2792)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [590/689]  eta: 0:02:35  lr: 0.000080  loss: 0.2663 (0.2793)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [600/689]  eta: 0:02:19  lr: 0.000080  loss: 0.2636 (0.2790)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [610/689]  eta: 0:02:04  lr: 0.000080  loss: 0.2558 (0.2788)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [620/689]  eta: 0:01:48  lr: 0.000080  loss: 0.2638 (0.2787)  time: 1.5742  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:278]  [630/689]  eta: 0:01:32  lr: 0.000080  loss: 0.2646 (0.2786)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [640/689]  eta: 0:01:17  lr: 0.000080  loss: 0.2750 (0.2787)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [650/689]  eta: 0:01:01  lr: 0.000080  loss: 0.2586 (0.2784)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [660/689]  eta: 0:00:45  lr: 0.000080  loss: 0.2560 (0.2785)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [670/689]  eta: 0:00:29  lr: 0.000080  loss: 0.2715 (0.2785)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [680/689]  eta: 0:00:14  lr: 0.000080  loss: 0.2610 (0.2783)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278]  [688/689]  eta: 0:00:01  lr: 0.000080  loss: 0.2675 (0.2784)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:278] Total time: 0:18:04 (1.5733 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.2675 (0.2784)\n",
      "Valid: [epoch:278]  [ 0/14]  eta: 0:00:14  loss: 0.2759 (0.2759)  time: 1.0051  data: 0.3592  max mem: 39763\n",
      "Valid: [epoch:278]  [13/14]  eta: 0:00:00  loss: 0.2606 (0.2654)  time: 0.1137  data: 0.0257  max mem: 39763\n",
      "Valid: [epoch:278] Total time: 0:00:01 (0.1208 s / it)\n",
      "Averaged stats: loss: 0.2606 (0.2654)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_278_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.265%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:279]  [  0/689]  eta: 0:12:54  lr: 0.000080  loss: 0.2464 (0.2464)  time: 1.1246  data: 0.6498  max mem: 39763\n",
      "Train: [epoch:279]  [ 10/689]  eta: 0:17:19  lr: 0.000080  loss: 0.2551 (0.2626)  time: 1.5304  data: 0.0591  max mem: 39763\n",
      "Train: [epoch:279]  [ 20/689]  eta: 0:17:17  lr: 0.000080  loss: 0.2646 (0.2689)  time: 1.5718  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [ 30/689]  eta: 0:17:06  lr: 0.000080  loss: 0.2709 (0.2722)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [ 40/689]  eta: 0:16:53  lr: 0.000080  loss: 0.2733 (0.2761)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [ 50/689]  eta: 0:16:39  lr: 0.000080  loss: 0.2699 (0.2772)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [ 60/689]  eta: 0:16:24  lr: 0.000080  loss: 0.2699 (0.2773)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [ 70/689]  eta: 0:16:10  lr: 0.000080  loss: 0.2698 (0.2777)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [ 80/689]  eta: 0:15:54  lr: 0.000080  loss: 0.2698 (0.2770)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [ 90/689]  eta: 0:15:39  lr: 0.000080  loss: 0.2725 (0.2762)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [100/689]  eta: 0:15:24  lr: 0.000080  loss: 0.2725 (0.2761)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [110/689]  eta: 0:15:08  lr: 0.000080  loss: 0.2770 (0.2776)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [120/689]  eta: 0:14:53  lr: 0.000080  loss: 0.2721 (0.2765)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [130/689]  eta: 0:14:37  lr: 0.000080  loss: 0.2637 (0.2757)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [140/689]  eta: 0:14:22  lr: 0.000080  loss: 0.2712 (0.2766)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [150/689]  eta: 0:14:06  lr: 0.000080  loss: 0.2753 (0.2764)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [160/689]  eta: 0:13:51  lr: 0.000080  loss: 0.2735 (0.2765)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [170/689]  eta: 0:13:35  lr: 0.000080  loss: 0.2700 (0.2763)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [180/689]  eta: 0:13:19  lr: 0.000080  loss: 0.2786 (0.2766)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [190/689]  eta: 0:13:04  lr: 0.000080  loss: 0.2766 (0.2772)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [200/689]  eta: 0:12:48  lr: 0.000080  loss: 0.2675 (0.2767)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [210/689]  eta: 0:12:32  lr: 0.000080  loss: 0.2655 (0.2764)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [220/689]  eta: 0:12:17  lr: 0.000080  loss: 0.2709 (0.2766)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [230/689]  eta: 0:12:01  lr: 0.000080  loss: 0.2812 (0.2768)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [240/689]  eta: 0:11:45  lr: 0.000080  loss: 0.2814 (0.2775)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [250/689]  eta: 0:11:30  lr: 0.000080  loss: 0.2792 (0.2774)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [260/689]  eta: 0:11:14  lr: 0.000080  loss: 0.2769 (0.2776)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [270/689]  eta: 0:10:58  lr: 0.000080  loss: 0.2756 (0.2781)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [280/689]  eta: 0:10:43  lr: 0.000080  loss: 0.2756 (0.2782)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [290/689]  eta: 0:10:27  lr: 0.000080  loss: 0.2704 (0.2781)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [300/689]  eta: 0:10:11  lr: 0.000080  loss: 0.2704 (0.2781)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [310/689]  eta: 0:09:56  lr: 0.000080  loss: 0.2727 (0.2781)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [320/689]  eta: 0:09:40  lr: 0.000080  loss: 0.2611 (0.2776)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [330/689]  eta: 0:09:24  lr: 0.000080  loss: 0.2718 (0.2781)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [340/689]  eta: 0:09:08  lr: 0.000080  loss: 0.2913 (0.2781)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [350/689]  eta: 0:08:53  lr: 0.000080  loss: 0.2794 (0.2789)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [360/689]  eta: 0:08:37  lr: 0.000080  loss: 0.2781 (0.2790)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [370/689]  eta: 0:08:21  lr: 0.000080  loss: 0.2675 (0.2790)  time: 1.5728  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [380/689]  eta: 0:08:06  lr: 0.000080  loss: 0.2672 (0.2791)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [390/689]  eta: 0:07:50  lr: 0.000080  loss: 0.2669 (0.2792)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [400/689]  eta: 0:07:34  lr: 0.000080  loss: 0.2736 (0.2791)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [410/689]  eta: 0:07:18  lr: 0.000080  loss: 0.2736 (0.2791)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [420/689]  eta: 0:07:03  lr: 0.000080  loss: 0.2717 (0.2791)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [430/689]  eta: 0:06:47  lr: 0.000080  loss: 0.2662 (0.2787)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [440/689]  eta: 0:06:31  lr: 0.000080  loss: 0.2655 (0.2784)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [450/689]  eta: 0:06:15  lr: 0.000080  loss: 0.2696 (0.2786)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [460/689]  eta: 0:06:00  lr: 0.000080  loss: 0.2744 (0.2788)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [470/689]  eta: 0:05:44  lr: 0.000080  loss: 0.2708 (0.2791)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [480/689]  eta: 0:05:28  lr: 0.000080  loss: 0.2736 (0.2791)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [490/689]  eta: 0:05:13  lr: 0.000080  loss: 0.2772 (0.2795)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [500/689]  eta: 0:04:57  lr: 0.000080  loss: 0.2764 (0.2794)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [510/689]  eta: 0:04:41  lr: 0.000080  loss: 0.2721 (0.2793)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [520/689]  eta: 0:04:25  lr: 0.000080  loss: 0.2740 (0.2795)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [530/689]  eta: 0:04:10  lr: 0.000080  loss: 0.2793 (0.2795)  time: 1.5746  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:279]  [540/689]  eta: 0:03:54  lr: 0.000080  loss: 0.2769 (0.2798)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [550/689]  eta: 0:03:38  lr: 0.000080  loss: 0.2693 (0.2797)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [560/689]  eta: 0:03:22  lr: 0.000080  loss: 0.2666 (0.2798)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [570/689]  eta: 0:03:07  lr: 0.000080  loss: 0.2571 (0.2795)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [580/689]  eta: 0:02:51  lr: 0.000080  loss: 0.2624 (0.2793)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [590/689]  eta: 0:02:35  lr: 0.000080  loss: 0.2706 (0.2794)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [600/689]  eta: 0:02:20  lr: 0.000080  loss: 0.2676 (0.2792)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [610/689]  eta: 0:02:04  lr: 0.000080  loss: 0.2564 (0.2790)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [620/689]  eta: 0:01:48  lr: 0.000080  loss: 0.2564 (0.2788)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [630/689]  eta: 0:01:32  lr: 0.000080  loss: 0.2597 (0.2787)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [640/689]  eta: 0:01:17  lr: 0.000080  loss: 0.2798 (0.2791)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [650/689]  eta: 0:01:01  lr: 0.000080  loss: 0.2866 (0.2790)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [660/689]  eta: 0:00:45  lr: 0.000080  loss: 0.2641 (0.2790)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [670/689]  eta: 0:00:29  lr: 0.000080  loss: 0.2676 (0.2790)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [680/689]  eta: 0:00:14  lr: 0.000080  loss: 0.2676 (0.2788)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279]  [688/689]  eta: 0:00:01  lr: 0.000080  loss: 0.2761 (0.2791)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:279] Total time: 0:18:04 (1.5739 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.2761 (0.2791)\n",
      "Valid: [epoch:279]  [ 0/14]  eta: 0:00:13  loss: 0.2769 (0.2769)  time: 0.9618  data: 0.4296  max mem: 39763\n",
      "Valid: [epoch:279]  [13/14]  eta: 0:00:00  loss: 0.2610 (0.2662)  time: 0.1106  data: 0.0307  max mem: 39763\n",
      "Valid: [epoch:279] Total time: 0:00:01 (0.1196 s / it)\n",
      "Averaged stats: loss: 0.2610 (0.2662)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_279_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.266%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:280]  [  0/689]  eta: 0:11:43  lr: 0.000080  loss: 0.2432 (0.2432)  time: 1.0204  data: 0.5434  max mem: 39763\n",
      "Train: [epoch:280]  [ 10/689]  eta: 0:17:13  lr: 0.000080  loss: 0.2561 (0.2555)  time: 1.5221  data: 0.0495  max mem: 39763\n",
      "Train: [epoch:280]  [ 20/689]  eta: 0:17:14  lr: 0.000080  loss: 0.2633 (0.2639)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [ 30/689]  eta: 0:17:05  lr: 0.000080  loss: 0.2749 (0.2741)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [ 40/689]  eta: 0:16:52  lr: 0.000080  loss: 0.2866 (0.2738)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [ 50/689]  eta: 0:16:38  lr: 0.000080  loss: 0.2742 (0.2747)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [ 60/689]  eta: 0:16:24  lr: 0.000080  loss: 0.2834 (0.2771)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [ 70/689]  eta: 0:16:09  lr: 0.000080  loss: 0.2746 (0.2751)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [ 80/689]  eta: 0:15:54  lr: 0.000080  loss: 0.2748 (0.2774)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [ 90/689]  eta: 0:15:39  lr: 0.000080  loss: 0.2850 (0.2777)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [100/689]  eta: 0:15:23  lr: 0.000080  loss: 0.2791 (0.2782)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [110/689]  eta: 0:15:08  lr: 0.000080  loss: 0.2791 (0.2780)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [120/689]  eta: 0:14:52  lr: 0.000080  loss: 0.2773 (0.2772)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [130/689]  eta: 0:14:37  lr: 0.000080  loss: 0.2757 (0.2784)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [140/689]  eta: 0:14:21  lr: 0.000080  loss: 0.2897 (0.2792)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [150/689]  eta: 0:14:06  lr: 0.000080  loss: 0.2813 (0.2791)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [160/689]  eta: 0:13:50  lr: 0.000080  loss: 0.2649 (0.2787)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [170/689]  eta: 0:13:35  lr: 0.000080  loss: 0.2639 (0.2786)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [180/689]  eta: 0:13:19  lr: 0.000080  loss: 0.2686 (0.2786)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [190/689]  eta: 0:13:04  lr: 0.000080  loss: 0.2686 (0.2788)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [200/689]  eta: 0:12:48  lr: 0.000080  loss: 0.2626 (0.2781)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [210/689]  eta: 0:12:32  lr: 0.000080  loss: 0.2626 (0.2775)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [220/689]  eta: 0:12:17  lr: 0.000080  loss: 0.2711 (0.2783)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [230/689]  eta: 0:12:01  lr: 0.000080  loss: 0.2777 (0.2782)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [240/689]  eta: 0:11:45  lr: 0.000080  loss: 0.2763 (0.2784)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [250/689]  eta: 0:11:30  lr: 0.000080  loss: 0.2751 (0.2787)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [260/689]  eta: 0:11:14  lr: 0.000080  loss: 0.2795 (0.2787)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [270/689]  eta: 0:10:58  lr: 0.000080  loss: 0.2795 (0.2791)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [280/689]  eta: 0:10:43  lr: 0.000080  loss: 0.2721 (0.2791)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [290/689]  eta: 0:10:27  lr: 0.000080  loss: 0.2666 (0.2784)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [300/689]  eta: 0:10:11  lr: 0.000080  loss: 0.2670 (0.2784)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [310/689]  eta: 0:09:56  lr: 0.000080  loss: 0.2756 (0.2787)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [320/689]  eta: 0:09:40  lr: 0.000080  loss: 0.2747 (0.2786)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [330/689]  eta: 0:09:24  lr: 0.000080  loss: 0.2621 (0.2778)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [340/689]  eta: 0:09:08  lr: 0.000080  loss: 0.2640 (0.2781)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [350/689]  eta: 0:08:53  lr: 0.000080  loss: 0.2749 (0.2780)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [360/689]  eta: 0:08:37  lr: 0.000080  loss: 0.2578 (0.2777)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [370/689]  eta: 0:08:21  lr: 0.000080  loss: 0.2578 (0.2773)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [380/689]  eta: 0:08:06  lr: 0.000080  loss: 0.2683 (0.2776)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [390/689]  eta: 0:07:50  lr: 0.000080  loss: 0.2792 (0.2777)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [400/689]  eta: 0:07:34  lr: 0.000080  loss: 0.2844 (0.2779)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [410/689]  eta: 0:07:18  lr: 0.000080  loss: 0.2769 (0.2779)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [420/689]  eta: 0:07:03  lr: 0.000080  loss: 0.2769 (0.2779)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [430/689]  eta: 0:06:47  lr: 0.000080  loss: 0.2728 (0.2779)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [440/689]  eta: 0:06:31  lr: 0.000080  loss: 0.2904 (0.2786)  time: 1.5743  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:280]  [450/689]  eta: 0:06:15  lr: 0.000080  loss: 0.2839 (0.2788)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [460/689]  eta: 0:06:00  lr: 0.000080  loss: 0.2904 (0.2791)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [470/689]  eta: 0:05:44  lr: 0.000080  loss: 0.2685 (0.2788)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [480/689]  eta: 0:05:28  lr: 0.000080  loss: 0.2535 (0.2788)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [490/689]  eta: 0:05:13  lr: 0.000080  loss: 0.2706 (0.2787)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [500/689]  eta: 0:04:57  lr: 0.000080  loss: 0.2851 (0.2790)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [510/689]  eta: 0:04:41  lr: 0.000080  loss: 0.2757 (0.2788)  time: 1.5734  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [520/689]  eta: 0:04:25  lr: 0.000080  loss: 0.2672 (0.2791)  time: 1.5736  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [530/689]  eta: 0:04:10  lr: 0.000080  loss: 0.2769 (0.2790)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [540/689]  eta: 0:03:54  lr: 0.000080  loss: 0.2731 (0.2791)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [550/689]  eta: 0:03:38  lr: 0.000080  loss: 0.2778 (0.2792)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [560/689]  eta: 0:03:22  lr: 0.000080  loss: 0.2810 (0.2794)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [570/689]  eta: 0:03:07  lr: 0.000080  loss: 0.2867 (0.2797)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [580/689]  eta: 0:02:51  lr: 0.000080  loss: 0.2800 (0.2797)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [590/689]  eta: 0:02:35  lr: 0.000080  loss: 0.2728 (0.2797)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [600/689]  eta: 0:02:20  lr: 0.000080  loss: 0.2732 (0.2799)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [610/689]  eta: 0:02:04  lr: 0.000080  loss: 0.2734 (0.2797)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [620/689]  eta: 0:01:48  lr: 0.000080  loss: 0.2734 (0.2797)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [630/689]  eta: 0:01:32  lr: 0.000080  loss: 0.2698 (0.2795)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [640/689]  eta: 0:01:17  lr: 0.000080  loss: 0.2665 (0.2797)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [650/689]  eta: 0:01:01  lr: 0.000080  loss: 0.2710 (0.2797)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [660/689]  eta: 0:00:45  lr: 0.000080  loss: 0.2723 (0.2799)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [670/689]  eta: 0:00:29  lr: 0.000080  loss: 0.2887 (0.2801)  time: 1.5735  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [680/689]  eta: 0:00:14  lr: 0.000080  loss: 0.2908 (0.2803)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280]  [688/689]  eta: 0:00:01  lr: 0.000080  loss: 0.2883 (0.2806)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:280] Total time: 0:18:04 (1.5737 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.2883 (0.2806)\n",
      "Valid: [epoch:280]  [ 0/14]  eta: 0:00:14  loss: 0.2623 (0.2623)  time: 1.0144  data: 0.3649  max mem: 39763\n",
      "Valid: [epoch:280]  [13/14]  eta: 0:00:00  loss: 0.2623 (0.2657)  time: 0.1143  data: 0.0261  max mem: 39763\n",
      "Valid: [epoch:280] Total time: 0:00:01 (0.1238 s / it)\n",
      "Averaged stats: loss: 0.2623 (0.2657)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_280_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.266%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:281]  [  0/689]  eta: 0:12:34  lr: 0.000080  loss: 0.2360 (0.2360)  time: 1.0947  data: 0.6200  max mem: 39763\n",
      "Train: [epoch:281]  [ 10/689]  eta: 0:17:18  lr: 0.000080  loss: 0.2685 (0.2849)  time: 1.5288  data: 0.0564  max mem: 39763\n",
      "Train: [epoch:281]  [ 20/689]  eta: 0:17:17  lr: 0.000080  loss: 0.2685 (0.2848)  time: 1.5730  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [ 30/689]  eta: 0:17:06  lr: 0.000080  loss: 0.2918 (0.2874)  time: 1.5730  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [ 40/689]  eta: 0:16:53  lr: 0.000080  loss: 0.2770 (0.2864)  time: 1.5732  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [ 50/689]  eta: 0:16:39  lr: 0.000080  loss: 0.2734 (0.2847)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [ 60/689]  eta: 0:16:24  lr: 0.000080  loss: 0.2870 (0.2862)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [ 70/689]  eta: 0:16:09  lr: 0.000080  loss: 0.2860 (0.2867)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [ 80/689]  eta: 0:15:54  lr: 0.000080  loss: 0.2723 (0.2881)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [ 90/689]  eta: 0:15:39  lr: 0.000080  loss: 0.2761 (0.2898)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [100/689]  eta: 0:15:24  lr: 0.000080  loss: 0.2693 (0.2880)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [110/689]  eta: 0:15:08  lr: 0.000080  loss: 0.2727 (0.2893)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [120/689]  eta: 0:14:53  lr: 0.000080  loss: 0.2831 (0.2885)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [130/689]  eta: 0:14:37  lr: 0.000080  loss: 0.2691 (0.2863)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [140/689]  eta: 0:14:22  lr: 0.000080  loss: 0.2688 (0.2872)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [150/689]  eta: 0:14:06  lr: 0.000080  loss: 0.2674 (0.2858)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [160/689]  eta: 0:13:51  lr: 0.000080  loss: 0.2638 (0.2852)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [170/689]  eta: 0:13:35  lr: 0.000080  loss: 0.2718 (0.2849)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [180/689]  eta: 0:13:19  lr: 0.000080  loss: 0.2736 (0.2854)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [190/689]  eta: 0:13:04  lr: 0.000080  loss: 0.2849 (0.2849)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [200/689]  eta: 0:12:48  lr: 0.000080  loss: 0.2874 (0.2856)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [210/689]  eta: 0:12:33  lr: 0.000080  loss: 0.2891 (0.2854)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [220/689]  eta: 0:12:17  lr: 0.000080  loss: 0.2784 (0.2852)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [230/689]  eta: 0:12:01  lr: 0.000080  loss: 0.2784 (0.2850)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [240/689]  eta: 0:11:46  lr: 0.000080  loss: 0.2751 (0.2845)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [250/689]  eta: 0:11:30  lr: 0.000080  loss: 0.2751 (0.2844)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [260/689]  eta: 0:11:14  lr: 0.000080  loss: 0.2803 (0.2844)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [270/689]  eta: 0:10:59  lr: 0.000080  loss: 0.2745 (0.2841)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [280/689]  eta: 0:10:43  lr: 0.000080  loss: 0.2859 (0.2846)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [290/689]  eta: 0:10:27  lr: 0.000080  loss: 0.2900 (0.2852)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [300/689]  eta: 0:10:12  lr: 0.000080  loss: 0.2792 (0.2854)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [310/689]  eta: 0:09:56  lr: 0.000080  loss: 0.2919 (0.2858)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [320/689]  eta: 0:09:40  lr: 0.000080  loss: 0.3030 (0.2867)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [330/689]  eta: 0:09:24  lr: 0.000080  loss: 0.2851 (0.2867)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [340/689]  eta: 0:09:09  lr: 0.000080  loss: 0.2740 (0.2864)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [350/689]  eta: 0:08:53  lr: 0.000080  loss: 0.2741 (0.2863)  time: 1.5754  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:281]  [360/689]  eta: 0:08:37  lr: 0.000080  loss: 0.2804 (0.2864)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [370/689]  eta: 0:08:22  lr: 0.000080  loss: 0.2903 (0.2863)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [380/689]  eta: 0:08:06  lr: 0.000080  loss: 0.2674 (0.2858)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [390/689]  eta: 0:07:50  lr: 0.000080  loss: 0.2690 (0.2856)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [400/689]  eta: 0:07:34  lr: 0.000080  loss: 0.2879 (0.2857)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [410/689]  eta: 0:07:19  lr: 0.000080  loss: 0.2874 (0.2859)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [420/689]  eta: 0:07:03  lr: 0.000080  loss: 0.2777 (0.2857)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [430/689]  eta: 0:06:47  lr: 0.000080  loss: 0.2777 (0.2857)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [440/689]  eta: 0:06:31  lr: 0.000080  loss: 0.2628 (0.2854)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [450/689]  eta: 0:06:16  lr: 0.000080  loss: 0.2726 (0.2854)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [460/689]  eta: 0:06:00  lr: 0.000080  loss: 0.2854 (0.2855)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [470/689]  eta: 0:05:44  lr: 0.000080  loss: 0.2851 (0.2854)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [480/689]  eta: 0:05:28  lr: 0.000080  loss: 0.2751 (0.2853)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [490/689]  eta: 0:05:13  lr: 0.000080  loss: 0.2777 (0.2852)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [500/689]  eta: 0:04:57  lr: 0.000080  loss: 0.2727 (0.2848)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [510/689]  eta: 0:04:41  lr: 0.000080  loss: 0.2674 (0.2848)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [520/689]  eta: 0:04:26  lr: 0.000080  loss: 0.2726 (0.2845)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [530/689]  eta: 0:04:10  lr: 0.000080  loss: 0.2760 (0.2845)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [540/689]  eta: 0:03:54  lr: 0.000080  loss: 0.2760 (0.2845)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [550/689]  eta: 0:03:38  lr: 0.000080  loss: 0.2611 (0.2841)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [560/689]  eta: 0:03:23  lr: 0.000080  loss: 0.2708 (0.2843)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [570/689]  eta: 0:03:07  lr: 0.000080  loss: 0.2863 (0.2843)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [580/689]  eta: 0:02:51  lr: 0.000080  loss: 0.2754 (0.2842)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [590/689]  eta: 0:02:35  lr: 0.000080  loss: 0.2750 (0.2844)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [600/689]  eta: 0:02:20  lr: 0.000080  loss: 0.2669 (0.2841)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [610/689]  eta: 0:02:04  lr: 0.000080  loss: 0.2628 (0.2844)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [620/689]  eta: 0:01:48  lr: 0.000080  loss: 0.2655 (0.2841)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [630/689]  eta: 0:01:32  lr: 0.000080  loss: 0.2646 (0.2840)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [640/689]  eta: 0:01:17  lr: 0.000080  loss: 0.2690 (0.2838)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [650/689]  eta: 0:01:01  lr: 0.000080  loss: 0.2655 (0.2837)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [660/689]  eta: 0:00:45  lr: 0.000080  loss: 0.2642 (0.2836)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [670/689]  eta: 0:00:29  lr: 0.000080  loss: 0.2612 (0.2836)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [680/689]  eta: 0:00:14  lr: 0.000080  loss: 0.2806 (0.2837)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281]  [688/689]  eta: 0:00:01  lr: 0.000080  loss: 0.2745 (0.2834)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:281] Total time: 0:18:05 (1.5748 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.2745 (0.2834)\n",
      "Valid: [epoch:281]  [ 0/14]  eta: 0:00:14  loss: 0.2772 (0.2772)  time: 1.0062  data: 0.3526  max mem: 39763\n",
      "Valid: [epoch:281]  [13/14]  eta: 0:00:00  loss: 0.2627 (0.2655)  time: 0.1138  data: 0.0252  max mem: 39763\n",
      "Valid: [epoch:281] Total time: 0:00:01 (0.1225 s / it)\n",
      "Averaged stats: loss: 0.2627 (0.2655)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_281_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.265%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:282]  [  0/689]  eta: 0:13:12  lr: 0.000080  loss: 0.2901 (0.2901)  time: 1.1497  data: 0.6718  max mem: 39763\n",
      "Train: [epoch:282]  [ 10/689]  eta: 0:17:22  lr: 0.000080  loss: 0.2681 (0.2730)  time: 1.5353  data: 0.0611  max mem: 39763\n",
      "Train: [epoch:282]  [ 20/689]  eta: 0:17:19  lr: 0.000080  loss: 0.2681 (0.2774)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [ 30/689]  eta: 0:17:08  lr: 0.000080  loss: 0.2816 (0.2785)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [ 40/689]  eta: 0:16:55  lr: 0.000080  loss: 0.2859 (0.2799)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [ 50/689]  eta: 0:16:41  lr: 0.000080  loss: 0.2901 (0.2842)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [ 60/689]  eta: 0:16:26  lr: 0.000080  loss: 0.2876 (0.2834)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [ 70/689]  eta: 0:16:11  lr: 0.000080  loss: 0.2780 (0.2822)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [ 80/689]  eta: 0:15:56  lr: 0.000080  loss: 0.2675 (0.2815)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [ 90/689]  eta: 0:15:40  lr: 0.000080  loss: 0.2621 (0.2805)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [100/689]  eta: 0:15:25  lr: 0.000080  loss: 0.2819 (0.2823)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [110/689]  eta: 0:15:09  lr: 0.000080  loss: 0.2866 (0.2828)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [120/689]  eta: 0:14:54  lr: 0.000080  loss: 0.2831 (0.2838)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [130/689]  eta: 0:14:38  lr: 0.000080  loss: 0.2913 (0.2843)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [140/689]  eta: 0:14:23  lr: 0.000080  loss: 0.2846 (0.2846)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [150/689]  eta: 0:14:07  lr: 0.000080  loss: 0.2846 (0.2850)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [160/689]  eta: 0:13:51  lr: 0.000080  loss: 0.2935 (0.2856)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [170/689]  eta: 0:13:36  lr: 0.000080  loss: 0.2935 (0.2865)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [180/689]  eta: 0:13:20  lr: 0.000080  loss: 0.2879 (0.2863)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [190/689]  eta: 0:13:05  lr: 0.000080  loss: 0.2752 (0.2857)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [200/689]  eta: 0:12:49  lr: 0.000080  loss: 0.2638 (0.2852)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [210/689]  eta: 0:12:33  lr: 0.000080  loss: 0.2639 (0.2853)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [220/689]  eta: 0:12:18  lr: 0.000080  loss: 0.2677 (0.2850)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [230/689]  eta: 0:12:02  lr: 0.000080  loss: 0.2677 (0.2847)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [240/689]  eta: 0:11:46  lr: 0.000080  loss: 0.2744 (0.2852)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [250/689]  eta: 0:11:30  lr: 0.000080  loss: 0.2734 (0.2846)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [260/689]  eta: 0:11:15  lr: 0.000080  loss: 0.2713 (0.2843)  time: 1.5757  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:282]  [270/689]  eta: 0:10:59  lr: 0.000080  loss: 0.2747 (0.2843)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [280/689]  eta: 0:10:43  lr: 0.000080  loss: 0.2735 (0.2838)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [290/689]  eta: 0:10:28  lr: 0.000080  loss: 0.2743 (0.2840)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [300/689]  eta: 0:10:12  lr: 0.000080  loss: 0.2722 (0.2836)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [310/689]  eta: 0:09:56  lr: 0.000080  loss: 0.2624 (0.2830)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [320/689]  eta: 0:09:40  lr: 0.000080  loss: 0.2714 (0.2832)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [330/689]  eta: 0:09:25  lr: 0.000080  loss: 0.2756 (0.2829)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [340/689]  eta: 0:09:09  lr: 0.000080  loss: 0.2778 (0.2829)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [350/689]  eta: 0:08:53  lr: 0.000080  loss: 0.2824 (0.2829)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [360/689]  eta: 0:08:38  lr: 0.000080  loss: 0.2853 (0.2832)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [370/689]  eta: 0:08:22  lr: 0.000080  loss: 0.2926 (0.2835)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [380/689]  eta: 0:08:06  lr: 0.000080  loss: 0.2738 (0.2832)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [390/689]  eta: 0:07:50  lr: 0.000080  loss: 0.2713 (0.2832)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [400/689]  eta: 0:07:35  lr: 0.000080  loss: 0.2701 (0.2832)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [410/689]  eta: 0:07:19  lr: 0.000080  loss: 0.2752 (0.2833)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [420/689]  eta: 0:07:03  lr: 0.000080  loss: 0.2819 (0.2838)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [430/689]  eta: 0:06:47  lr: 0.000080  loss: 0.2871 (0.2841)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [440/689]  eta: 0:06:32  lr: 0.000080  loss: 0.2741 (0.2838)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [450/689]  eta: 0:06:16  lr: 0.000080  loss: 0.2741 (0.2839)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [460/689]  eta: 0:06:00  lr: 0.000080  loss: 0.2827 (0.2841)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [470/689]  eta: 0:05:44  lr: 0.000080  loss: 0.2800 (0.2845)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [480/689]  eta: 0:05:29  lr: 0.000080  loss: 0.2798 (0.2843)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [490/689]  eta: 0:05:13  lr: 0.000080  loss: 0.2615 (0.2838)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [500/689]  eta: 0:04:57  lr: 0.000080  loss: 0.2782 (0.2843)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [510/689]  eta: 0:04:41  lr: 0.000080  loss: 0.2832 (0.2841)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [520/689]  eta: 0:04:26  lr: 0.000080  loss: 0.2852 (0.2844)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [530/689]  eta: 0:04:10  lr: 0.000080  loss: 0.2825 (0.2844)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [540/689]  eta: 0:03:54  lr: 0.000080  loss: 0.2724 (0.2843)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [550/689]  eta: 0:03:38  lr: 0.000080  loss: 0.2837 (0.2846)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [560/689]  eta: 0:03:23  lr: 0.000080  loss: 0.2844 (0.2845)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [570/689]  eta: 0:03:07  lr: 0.000080  loss: 0.2650 (0.2843)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [580/689]  eta: 0:02:51  lr: 0.000080  loss: 0.2732 (0.2844)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [590/689]  eta: 0:02:35  lr: 0.000080  loss: 0.2754 (0.2843)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [600/689]  eta: 0:02:20  lr: 0.000080  loss: 0.2731 (0.2841)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [610/689]  eta: 0:02:04  lr: 0.000080  loss: 0.2671 (0.2839)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [620/689]  eta: 0:01:48  lr: 0.000080  loss: 0.2566 (0.2835)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [630/689]  eta: 0:01:32  lr: 0.000080  loss: 0.2739 (0.2836)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [640/689]  eta: 0:01:17  lr: 0.000080  loss: 0.2739 (0.2833)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [650/689]  eta: 0:01:01  lr: 0.000080  loss: 0.2677 (0.2832)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [660/689]  eta: 0:00:45  lr: 0.000080  loss: 0.2784 (0.2832)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [670/689]  eta: 0:00:29  lr: 0.000080  loss: 0.2669 (0.2828)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [680/689]  eta: 0:00:14  lr: 0.000080  loss: 0.2596 (0.2826)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282]  [688/689]  eta: 0:00:01  lr: 0.000080  loss: 0.2660 (0.2828)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:282] Total time: 0:18:05 (1.5755 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.2660 (0.2828)\n",
      "Valid: [epoch:282]  [ 0/14]  eta: 0:00:13  loss: 0.2489 (0.2489)  time: 0.9768  data: 0.4259  max mem: 39763\n",
      "Valid: [epoch:282]  [13/14]  eta: 0:00:00  loss: 0.2638 (0.2666)  time: 0.1117  data: 0.0305  max mem: 39763\n",
      "Valid: [epoch:282] Total time: 0:00:01 (0.1209 s / it)\n",
      "Averaged stats: loss: 0.2638 (0.2666)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_282_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.267%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:283]  [  0/689]  eta: 0:13:18  lr: 0.000080  loss: 0.2557 (0.2557)  time: 1.1594  data: 0.6828  max mem: 39763\n",
      "Train: [epoch:283]  [ 10/689]  eta: 0:17:22  lr: 0.000080  loss: 0.2799 (0.2800)  time: 1.5360  data: 0.0621  max mem: 39763\n",
      "Train: [epoch:283]  [ 20/689]  eta: 0:17:20  lr: 0.000080  loss: 0.2640 (0.2702)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [ 30/689]  eta: 0:17:09  lr: 0.000080  loss: 0.2606 (0.2705)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [ 40/689]  eta: 0:16:55  lr: 0.000080  loss: 0.2638 (0.2734)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [ 50/689]  eta: 0:16:41  lr: 0.000080  loss: 0.2715 (0.2752)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [ 60/689]  eta: 0:16:26  lr: 0.000080  loss: 0.2715 (0.2772)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [ 70/689]  eta: 0:16:11  lr: 0.000080  loss: 0.2804 (0.2784)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [ 80/689]  eta: 0:15:56  lr: 0.000080  loss: 0.2789 (0.2785)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [ 90/689]  eta: 0:15:41  lr: 0.000080  loss: 0.2652 (0.2777)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [100/689]  eta: 0:15:25  lr: 0.000080  loss: 0.2711 (0.2779)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [110/689]  eta: 0:15:10  lr: 0.000080  loss: 0.2769 (0.2783)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [120/689]  eta: 0:14:54  lr: 0.000080  loss: 0.2801 (0.2782)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [130/689]  eta: 0:14:39  lr: 0.000080  loss: 0.2800 (0.2780)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [140/689]  eta: 0:14:23  lr: 0.000080  loss: 0.2704 (0.2778)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [150/689]  eta: 0:14:07  lr: 0.000080  loss: 0.2717 (0.2790)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [160/689]  eta: 0:13:52  lr: 0.000080  loss: 0.2822 (0.2791)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [170/689]  eta: 0:13:36  lr: 0.000080  loss: 0.2696 (0.2791)  time: 1.5764  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:283]  [180/689]  eta: 0:13:20  lr: 0.000080  loss: 0.2696 (0.2794)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [190/689]  eta: 0:13:05  lr: 0.000080  loss: 0.2772 (0.2793)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [200/689]  eta: 0:12:49  lr: 0.000080  loss: 0.2690 (0.2791)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [210/689]  eta: 0:12:33  lr: 0.000080  loss: 0.2837 (0.2802)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [220/689]  eta: 0:12:18  lr: 0.000080  loss: 0.2852 (0.2804)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [230/689]  eta: 0:12:02  lr: 0.000080  loss: 0.2817 (0.2804)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [240/689]  eta: 0:11:46  lr: 0.000080  loss: 0.2773 (0.2807)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [250/689]  eta: 0:11:31  lr: 0.000080  loss: 0.2759 (0.2807)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [260/689]  eta: 0:11:15  lr: 0.000080  loss: 0.2741 (0.2806)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [270/689]  eta: 0:10:59  lr: 0.000080  loss: 0.2642 (0.2800)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [280/689]  eta: 0:10:44  lr: 0.000080  loss: 0.2663 (0.2797)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [290/689]  eta: 0:10:28  lr: 0.000080  loss: 0.2681 (0.2800)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [300/689]  eta: 0:10:12  lr: 0.000080  loss: 0.2759 (0.2800)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [310/689]  eta: 0:09:56  lr: 0.000080  loss: 0.2835 (0.2801)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [320/689]  eta: 0:09:41  lr: 0.000080  loss: 0.2682 (0.2798)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [330/689]  eta: 0:09:25  lr: 0.000080  loss: 0.2682 (0.2796)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [340/689]  eta: 0:09:09  lr: 0.000080  loss: 0.2626 (0.2793)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [350/689]  eta: 0:08:53  lr: 0.000080  loss: 0.2626 (0.2794)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [360/689]  eta: 0:08:38  lr: 0.000080  loss: 0.2779 (0.2802)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [370/689]  eta: 0:08:22  lr: 0.000080  loss: 0.2830 (0.2805)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [380/689]  eta: 0:08:06  lr: 0.000080  loss: 0.2830 (0.2805)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [390/689]  eta: 0:07:50  lr: 0.000080  loss: 0.2898 (0.2815)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [400/689]  eta: 0:07:35  lr: 0.000080  loss: 0.2932 (0.2817)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [410/689]  eta: 0:07:19  lr: 0.000080  loss: 0.2903 (0.2822)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [420/689]  eta: 0:07:03  lr: 0.000080  loss: 0.2881 (0.2825)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [430/689]  eta: 0:06:47  lr: 0.000080  loss: 0.2841 (0.2826)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [440/689]  eta: 0:06:32  lr: 0.000080  loss: 0.2697 (0.2824)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [450/689]  eta: 0:06:16  lr: 0.000080  loss: 0.2699 (0.2822)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [460/689]  eta: 0:06:00  lr: 0.000080  loss: 0.2729 (0.2823)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [470/689]  eta: 0:05:44  lr: 0.000080  loss: 0.2756 (0.2821)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [480/689]  eta: 0:05:29  lr: 0.000080  loss: 0.2721 (0.2820)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [490/689]  eta: 0:05:13  lr: 0.000080  loss: 0.2976 (0.2826)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [500/689]  eta: 0:04:57  lr: 0.000080  loss: 0.2959 (0.2826)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [510/689]  eta: 0:04:41  lr: 0.000080  loss: 0.2712 (0.2824)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [520/689]  eta: 0:04:26  lr: 0.000080  loss: 0.2833 (0.2827)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [530/689]  eta: 0:04:10  lr: 0.000080  loss: 0.2793 (0.2824)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [540/689]  eta: 0:03:54  lr: 0.000080  loss: 0.2618 (0.2825)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [550/689]  eta: 0:03:38  lr: 0.000080  loss: 0.2618 (0.2822)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [560/689]  eta: 0:03:23  lr: 0.000080  loss: 0.2661 (0.2824)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [570/689]  eta: 0:03:07  lr: 0.000080  loss: 0.2721 (0.2822)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [580/689]  eta: 0:02:51  lr: 0.000080  loss: 0.2646 (0.2824)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [590/689]  eta: 0:02:35  lr: 0.000080  loss: 0.2646 (0.2821)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [600/689]  eta: 0:02:20  lr: 0.000080  loss: 0.2769 (0.2823)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [610/689]  eta: 0:02:04  lr: 0.000080  loss: 0.2927 (0.2824)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [620/689]  eta: 0:01:48  lr: 0.000080  loss: 0.2952 (0.2828)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [630/689]  eta: 0:01:32  lr: 0.000080  loss: 0.2880 (0.2830)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [640/689]  eta: 0:01:17  lr: 0.000080  loss: 0.2793 (0.2832)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [650/689]  eta: 0:01:01  lr: 0.000080  loss: 0.2802 (0.2834)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [660/689]  eta: 0:00:45  lr: 0.000080  loss: 0.2686 (0.2832)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [670/689]  eta: 0:00:29  lr: 0.000080  loss: 0.2704 (0.2833)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [680/689]  eta: 0:00:14  lr: 0.000080  loss: 0.2836 (0.2833)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283]  [688/689]  eta: 0:00:01  lr: 0.000080  loss: 0.2758 (0.2833)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:283] Total time: 0:18:05 (1.5758 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.2758 (0.2833)\n",
      "Valid: [epoch:283]  [ 0/14]  eta: 0:00:11  loss: 0.2643 (0.2643)  time: 0.8012  data: 0.3481  max mem: 39763\n",
      "Valid: [epoch:283]  [13/14]  eta: 0:00:00  loss: 0.2651 (0.2674)  time: 0.0991  data: 0.0249  max mem: 39763\n",
      "Valid: [epoch:283] Total time: 0:00:01 (0.1069 s / it)\n",
      "Averaged stats: loss: 0.2651 (0.2674)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_283_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.267%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:284]  [  0/689]  eta: 0:11:36  lr: 0.000080  loss: 0.2680 (0.2680)  time: 1.0103  data: 0.5327  max mem: 39763\n",
      "Train: [epoch:284]  [ 10/689]  eta: 0:17:14  lr: 0.000080  loss: 0.2715 (0.2846)  time: 1.5234  data: 0.0485  max mem: 39763\n",
      "Train: [epoch:284]  [ 20/689]  eta: 0:17:15  lr: 0.000080  loss: 0.2657 (0.2721)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [ 30/689]  eta: 0:17:06  lr: 0.000080  loss: 0.2667 (0.2759)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [ 40/689]  eta: 0:16:53  lr: 0.000080  loss: 0.2708 (0.2750)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [ 50/689]  eta: 0:16:39  lr: 0.000080  loss: 0.2681 (0.2778)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [ 60/689]  eta: 0:16:24  lr: 0.000080  loss: 0.2650 (0.2778)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [ 70/689]  eta: 0:16:10  lr: 0.000080  loss: 0.2719 (0.2786)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [ 80/689]  eta: 0:15:55  lr: 0.000080  loss: 0.2787 (0.2796)  time: 1.5760  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:284]  [ 90/689]  eta: 0:15:39  lr: 0.000080  loss: 0.2908 (0.2814)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [100/689]  eta: 0:15:24  lr: 0.000080  loss: 0.2872 (0.2816)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [110/689]  eta: 0:15:09  lr: 0.000080  loss: 0.2872 (0.2825)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [120/689]  eta: 0:14:53  lr: 0.000080  loss: 0.2918 (0.2825)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [130/689]  eta: 0:14:38  lr: 0.000080  loss: 0.2897 (0.2836)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [140/689]  eta: 0:14:22  lr: 0.000080  loss: 0.2852 (0.2835)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [150/689]  eta: 0:14:07  lr: 0.000080  loss: 0.2680 (0.2834)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [160/689]  eta: 0:13:51  lr: 0.000080  loss: 0.2718 (0.2833)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [170/689]  eta: 0:13:35  lr: 0.000080  loss: 0.2685 (0.2828)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [180/689]  eta: 0:13:20  lr: 0.000080  loss: 0.2681 (0.2833)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [190/689]  eta: 0:13:04  lr: 0.000080  loss: 0.2827 (0.2834)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [200/689]  eta: 0:12:49  lr: 0.000080  loss: 0.2849 (0.2839)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [210/689]  eta: 0:12:33  lr: 0.000080  loss: 0.2841 (0.2831)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [220/689]  eta: 0:12:17  lr: 0.000080  loss: 0.2665 (0.2829)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [230/689]  eta: 0:12:01  lr: 0.000080  loss: 0.2665 (0.2823)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [240/689]  eta: 0:11:46  lr: 0.000080  loss: 0.2708 (0.2821)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [250/689]  eta: 0:11:30  lr: 0.000080  loss: 0.2735 (0.2821)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [260/689]  eta: 0:11:14  lr: 0.000080  loss: 0.2762 (0.2820)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [270/689]  eta: 0:10:59  lr: 0.000080  loss: 0.2762 (0.2818)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [280/689]  eta: 0:10:43  lr: 0.000080  loss: 0.2669 (0.2817)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [290/689]  eta: 0:10:27  lr: 0.000080  loss: 0.2669 (0.2811)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [300/689]  eta: 0:10:12  lr: 0.000080  loss: 0.2750 (0.2813)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [310/689]  eta: 0:09:56  lr: 0.000080  loss: 0.2816 (0.2814)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [320/689]  eta: 0:09:40  lr: 0.000080  loss: 0.2782 (0.2818)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [330/689]  eta: 0:09:25  lr: 0.000080  loss: 0.2935 (0.2824)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [340/689]  eta: 0:09:09  lr: 0.000080  loss: 0.2806 (0.2821)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [350/689]  eta: 0:08:53  lr: 0.000080  loss: 0.2780 (0.2822)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [360/689]  eta: 0:08:37  lr: 0.000080  loss: 0.2687 (0.2819)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [370/689]  eta: 0:08:22  lr: 0.000080  loss: 0.2656 (0.2821)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [380/689]  eta: 0:08:06  lr: 0.000080  loss: 0.2876 (0.2824)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [390/689]  eta: 0:07:50  lr: 0.000080  loss: 0.2761 (0.2822)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [400/689]  eta: 0:07:34  lr: 0.000080  loss: 0.2718 (0.2821)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [410/689]  eta: 0:07:19  lr: 0.000080  loss: 0.2718 (0.2823)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [420/689]  eta: 0:07:03  lr: 0.000080  loss: 0.2964 (0.2828)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [430/689]  eta: 0:06:47  lr: 0.000080  loss: 0.2835 (0.2831)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [440/689]  eta: 0:06:32  lr: 0.000080  loss: 0.2918 (0.2834)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [450/689]  eta: 0:06:16  lr: 0.000080  loss: 0.2918 (0.2835)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [460/689]  eta: 0:06:00  lr: 0.000080  loss: 0.2755 (0.2834)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [470/689]  eta: 0:05:44  lr: 0.000080  loss: 0.2755 (0.2841)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [480/689]  eta: 0:05:29  lr: 0.000080  loss: 0.2943 (0.2843)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [490/689]  eta: 0:05:13  lr: 0.000080  loss: 0.3030 (0.2848)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [500/689]  eta: 0:04:57  lr: 0.000080  loss: 0.2797 (0.2845)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [510/689]  eta: 0:04:41  lr: 0.000080  loss: 0.2717 (0.2845)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [520/689]  eta: 0:04:26  lr: 0.000080  loss: 0.2772 (0.2844)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [530/689]  eta: 0:04:10  lr: 0.000080  loss: 0.2810 (0.2846)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [540/689]  eta: 0:03:54  lr: 0.000080  loss: 0.2835 (0.2847)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [550/689]  eta: 0:03:38  lr: 0.000080  loss: 0.2700 (0.2843)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [560/689]  eta: 0:03:23  lr: 0.000080  loss: 0.2684 (0.2842)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [570/689]  eta: 0:03:07  lr: 0.000080  loss: 0.2659 (0.2842)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [580/689]  eta: 0:02:51  lr: 0.000080  loss: 0.2656 (0.2838)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [590/689]  eta: 0:02:35  lr: 0.000080  loss: 0.2676 (0.2840)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [600/689]  eta: 0:02:20  lr: 0.000080  loss: 0.2745 (0.2838)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [610/689]  eta: 0:02:04  lr: 0.000080  loss: 0.2673 (0.2834)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [620/689]  eta: 0:01:48  lr: 0.000080  loss: 0.2682 (0.2836)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [630/689]  eta: 0:01:32  lr: 0.000080  loss: 0.2832 (0.2838)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [640/689]  eta: 0:01:17  lr: 0.000080  loss: 0.2725 (0.2836)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [650/689]  eta: 0:01:01  lr: 0.000080  loss: 0.2703 (0.2836)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [660/689]  eta: 0:00:45  lr: 0.000080  loss: 0.2744 (0.2838)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [670/689]  eta: 0:00:29  lr: 0.000080  loss: 0.2710 (0.2836)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [680/689]  eta: 0:00:14  lr: 0.000080  loss: 0.2745 (0.2837)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284]  [688/689]  eta: 0:00:01  lr: 0.000080  loss: 0.2821 (0.2837)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:284] Total time: 0:18:05 (1.5753 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.2821 (0.2837)\n",
      "Valid: [epoch:284]  [ 0/14]  eta: 0:00:13  loss: 0.2792 (0.2792)  time: 0.9939  data: 0.3870  max mem: 39763\n",
      "Valid: [epoch:284]  [13/14]  eta: 0:00:00  loss: 0.2665 (0.2691)  time: 0.1129  data: 0.0277  max mem: 39763\n",
      "Valid: [epoch:284] Total time: 0:00:01 (0.1219 s / it)\n",
      "Averaged stats: loss: 0.2665 (0.2691)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_284_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.269%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:285]  [  0/689]  eta: 0:12:03  lr: 0.000080  loss: 0.2802 (0.2802)  time: 1.0496  data: 0.5717  max mem: 39763\n",
      "Train: [epoch:285]  [ 10/689]  eta: 0:17:15  lr: 0.000080  loss: 0.2802 (0.2887)  time: 1.5250  data: 0.0521  max mem: 39763\n",
      "Train: [epoch:285]  [ 20/689]  eta: 0:17:16  lr: 0.000080  loss: 0.2804 (0.2867)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [ 30/689]  eta: 0:17:06  lr: 0.000080  loss: 0.2804 (0.2868)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [ 40/689]  eta: 0:16:53  lr: 0.000080  loss: 0.2764 (0.2875)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [ 50/689]  eta: 0:16:39  lr: 0.000080  loss: 0.2764 (0.2928)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [ 60/689]  eta: 0:16:25  lr: 0.000080  loss: 0.2877 (0.2936)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [ 70/689]  eta: 0:16:10  lr: 0.000080  loss: 0.2783 (0.2931)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [ 80/689]  eta: 0:15:55  lr: 0.000080  loss: 0.2732 (0.2905)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [ 90/689]  eta: 0:15:40  lr: 0.000080  loss: 0.2704 (0.2884)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [100/689]  eta: 0:15:24  lr: 0.000080  loss: 0.2733 (0.2887)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [110/689]  eta: 0:15:09  lr: 0.000080  loss: 0.2957 (0.2915)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [120/689]  eta: 0:14:53  lr: 0.000080  loss: 0.2836 (0.2904)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [130/689]  eta: 0:14:38  lr: 0.000080  loss: 0.2746 (0.2904)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [140/689]  eta: 0:14:22  lr: 0.000080  loss: 0.2879 (0.2910)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [150/689]  eta: 0:14:07  lr: 0.000080  loss: 0.2879 (0.2911)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [160/689]  eta: 0:13:51  lr: 0.000080  loss: 0.2874 (0.2908)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [170/689]  eta: 0:13:35  lr: 0.000080  loss: 0.2799 (0.2906)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [180/689]  eta: 0:13:20  lr: 0.000080  loss: 0.2799 (0.2899)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [190/689]  eta: 0:13:04  lr: 0.000080  loss: 0.2785 (0.2897)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [200/689]  eta: 0:12:49  lr: 0.000080  loss: 0.2779 (0.2888)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [210/689]  eta: 0:12:33  lr: 0.000080  loss: 0.2699 (0.2884)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [220/689]  eta: 0:12:17  lr: 0.000080  loss: 0.2660 (0.2879)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [230/689]  eta: 0:12:02  lr: 0.000080  loss: 0.2734 (0.2881)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [240/689]  eta: 0:11:46  lr: 0.000080  loss: 0.3020 (0.2890)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [250/689]  eta: 0:11:30  lr: 0.000080  loss: 0.3044 (0.2894)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [260/689]  eta: 0:11:15  lr: 0.000080  loss: 0.2866 (0.2894)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [270/689]  eta: 0:10:59  lr: 0.000080  loss: 0.2956 (0.2896)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [280/689]  eta: 0:10:43  lr: 0.000080  loss: 0.2813 (0.2895)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [290/689]  eta: 0:10:27  lr: 0.000080  loss: 0.2732 (0.2886)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [300/689]  eta: 0:10:12  lr: 0.000080  loss: 0.2688 (0.2883)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [310/689]  eta: 0:09:56  lr: 0.000080  loss: 0.2675 (0.2879)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [320/689]  eta: 0:09:40  lr: 0.000080  loss: 0.2762 (0.2879)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [330/689]  eta: 0:09:25  lr: 0.000080  loss: 0.2825 (0.2881)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [340/689]  eta: 0:09:09  lr: 0.000080  loss: 0.2965 (0.2887)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [350/689]  eta: 0:08:53  lr: 0.000080  loss: 0.2874 (0.2884)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [360/689]  eta: 0:08:37  lr: 0.000080  loss: 0.2830 (0.2890)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [370/689]  eta: 0:08:22  lr: 0.000080  loss: 0.2843 (0.2892)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [380/689]  eta: 0:08:06  lr: 0.000080  loss: 0.2782 (0.2892)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [390/689]  eta: 0:07:50  lr: 0.000080  loss: 0.2782 (0.2896)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [400/689]  eta: 0:07:35  lr: 0.000080  loss: 0.2830 (0.2897)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [410/689]  eta: 0:07:19  lr: 0.000080  loss: 0.2836 (0.2906)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [420/689]  eta: 0:07:03  lr: 0.000080  loss: 0.2948 (0.2906)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [430/689]  eta: 0:06:47  lr: 0.000080  loss: 0.2851 (0.2907)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [440/689]  eta: 0:06:32  lr: 0.000080  loss: 0.2862 (0.2909)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [450/689]  eta: 0:06:16  lr: 0.000080  loss: 0.2990 (0.2912)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [460/689]  eta: 0:06:00  lr: 0.000080  loss: 0.2836 (0.2911)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [470/689]  eta: 0:05:44  lr: 0.000080  loss: 0.2860 (0.2917)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [480/689]  eta: 0:05:29  lr: 0.000080  loss: 0.2961 (0.2917)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [490/689]  eta: 0:05:13  lr: 0.000080  loss: 0.2683 (0.2910)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [500/689]  eta: 0:04:57  lr: 0.000080  loss: 0.2659 (0.2911)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [510/689]  eta: 0:04:41  lr: 0.000080  loss: 0.2870 (0.2909)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [520/689]  eta: 0:04:26  lr: 0.000080  loss: 0.2880 (0.2912)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [530/689]  eta: 0:04:10  lr: 0.000080  loss: 0.2919 (0.2913)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [540/689]  eta: 0:03:54  lr: 0.000080  loss: 0.2871 (0.2911)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [550/689]  eta: 0:03:38  lr: 0.000080  loss: 0.2783 (0.2908)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [560/689]  eta: 0:03:23  lr: 0.000080  loss: 0.2736 (0.2910)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [570/689]  eta: 0:03:07  lr: 0.000080  loss: 0.2676 (0.2907)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [580/689]  eta: 0:02:51  lr: 0.000080  loss: 0.2675 (0.2907)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [590/689]  eta: 0:02:35  lr: 0.000080  loss: 0.2852 (0.2906)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [600/689]  eta: 0:02:20  lr: 0.000080  loss: 0.2811 (0.2905)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [610/689]  eta: 0:02:04  lr: 0.000080  loss: 0.2755 (0.2905)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [620/689]  eta: 0:01:48  lr: 0.000080  loss: 0.2684 (0.2901)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [630/689]  eta: 0:01:32  lr: 0.000080  loss: 0.2684 (0.2898)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [640/689]  eta: 0:01:17  lr: 0.000080  loss: 0.2755 (0.2898)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [650/689]  eta: 0:01:01  lr: 0.000080  loss: 0.2792 (0.2897)  time: 1.5765  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:285]  [660/689]  eta: 0:00:45  lr: 0.000080  loss: 0.2805 (0.2897)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [670/689]  eta: 0:00:29  lr: 0.000080  loss: 0.2785 (0.2895)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [680/689]  eta: 0:00:14  lr: 0.000080  loss: 0.2785 (0.2894)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285]  [688/689]  eta: 0:00:01  lr: 0.000080  loss: 0.2804 (0.2893)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:285] Total time: 0:18:05 (1.5758 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 0.2804 (0.2893)\n",
      "Valid: [epoch:285]  [ 0/14]  eta: 0:00:14  loss: 0.2485 (0.2485)  time: 1.0019  data: 0.3662  max mem: 39763\n",
      "Valid: [epoch:285]  [13/14]  eta: 0:00:00  loss: 0.2673 (0.2701)  time: 0.1135  data: 0.0262  max mem: 39763\n",
      "Valid: [epoch:285] Total time: 0:00:01 (0.1227 s / it)\n",
      "Averaged stats: loss: 0.2673 (0.2701)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_285_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.270%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:286]  [  0/689]  eta: 0:11:52  lr: 0.000079  loss: 0.2627 (0.2627)  time: 1.0338  data: 0.5461  max mem: 39763\n",
      "Train: [epoch:286]  [ 10/689]  eta: 0:17:15  lr: 0.000079  loss: 0.2701 (0.2773)  time: 1.5252  data: 0.0497  max mem: 39763\n",
      "Train: [epoch:286]  [ 20/689]  eta: 0:17:16  lr: 0.000079  loss: 0.2909 (0.2913)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [ 30/689]  eta: 0:17:06  lr: 0.000079  loss: 0.2861 (0.2860)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [ 40/689]  eta: 0:16:53  lr: 0.000079  loss: 0.2722 (0.2860)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [ 50/689]  eta: 0:16:39  lr: 0.000079  loss: 0.2809 (0.2944)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [ 60/689]  eta: 0:16:25  lr: 0.000079  loss: 0.2775 (0.2905)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [ 70/689]  eta: 0:16:10  lr: 0.000079  loss: 0.2843 (0.2925)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [ 80/689]  eta: 0:15:55  lr: 0.000079  loss: 0.2845 (0.2911)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [ 90/689]  eta: 0:15:39  lr: 0.000079  loss: 0.2811 (0.2917)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [100/689]  eta: 0:15:24  lr: 0.000079  loss: 0.2865 (0.2929)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [110/689]  eta: 0:15:09  lr: 0.000079  loss: 0.2809 (0.2911)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [120/689]  eta: 0:14:53  lr: 0.000079  loss: 0.2747 (0.2902)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [130/689]  eta: 0:14:38  lr: 0.000079  loss: 0.2730 (0.2891)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [140/689]  eta: 0:14:22  lr: 0.000079  loss: 0.2710 (0.2892)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [150/689]  eta: 0:14:07  lr: 0.000079  loss: 0.2677 (0.2883)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [160/689]  eta: 0:13:51  lr: 0.000079  loss: 0.2661 (0.2878)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [170/689]  eta: 0:13:35  lr: 0.000079  loss: 0.2659 (0.2868)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [180/689]  eta: 0:13:20  lr: 0.000079  loss: 0.2771 (0.2868)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [190/689]  eta: 0:13:04  lr: 0.000079  loss: 0.2787 (0.2866)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [200/689]  eta: 0:12:48  lr: 0.000079  loss: 0.2740 (0.2861)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [210/689]  eta: 0:12:33  lr: 0.000079  loss: 0.2763 (0.2858)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [220/689]  eta: 0:12:17  lr: 0.000079  loss: 0.2781 (0.2860)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [230/689]  eta: 0:12:01  lr: 0.000079  loss: 0.2688 (0.2853)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [240/689]  eta: 0:11:46  lr: 0.000079  loss: 0.2820 (0.2861)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [250/689]  eta: 0:11:30  lr: 0.000079  loss: 0.2989 (0.2864)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [260/689]  eta: 0:11:14  lr: 0.000079  loss: 0.2825 (0.2865)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [270/689]  eta: 0:10:59  lr: 0.000079  loss: 0.2662 (0.2865)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [280/689]  eta: 0:10:43  lr: 0.000079  loss: 0.2662 (0.2867)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [290/689]  eta: 0:10:27  lr: 0.000079  loss: 0.2747 (0.2863)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [300/689]  eta: 0:10:12  lr: 0.000079  loss: 0.2702 (0.2860)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [310/689]  eta: 0:09:56  lr: 0.000079  loss: 0.2745 (0.2861)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [320/689]  eta: 0:09:40  lr: 0.000079  loss: 0.2805 (0.2860)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [330/689]  eta: 0:09:24  lr: 0.000079  loss: 0.2757 (0.2858)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [340/689]  eta: 0:09:09  lr: 0.000079  loss: 0.2757 (0.2854)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [350/689]  eta: 0:08:53  lr: 0.000079  loss: 0.2731 (0.2853)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [360/689]  eta: 0:08:37  lr: 0.000079  loss: 0.2773 (0.2857)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [370/689]  eta: 0:08:22  lr: 0.000079  loss: 0.2873 (0.2862)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [380/689]  eta: 0:08:06  lr: 0.000079  loss: 0.2749 (0.2857)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [390/689]  eta: 0:07:50  lr: 0.000079  loss: 0.2615 (0.2857)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [400/689]  eta: 0:07:34  lr: 0.000079  loss: 0.2819 (0.2856)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [410/689]  eta: 0:07:19  lr: 0.000079  loss: 0.2819 (0.2858)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [420/689]  eta: 0:07:03  lr: 0.000079  loss: 0.2853 (0.2859)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [430/689]  eta: 0:06:47  lr: 0.000079  loss: 0.2885 (0.2862)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [440/689]  eta: 0:06:31  lr: 0.000079  loss: 0.2837 (0.2859)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [450/689]  eta: 0:06:16  lr: 0.000079  loss: 0.2692 (0.2859)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [460/689]  eta: 0:06:00  lr: 0.000079  loss: 0.2768 (0.2859)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [470/689]  eta: 0:05:44  lr: 0.000079  loss: 0.2879 (0.2861)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [480/689]  eta: 0:05:29  lr: 0.000079  loss: 0.2740 (0.2856)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [490/689]  eta: 0:05:13  lr: 0.000079  loss: 0.2740 (0.2857)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [500/689]  eta: 0:04:57  lr: 0.000079  loss: 0.2905 (0.2861)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [510/689]  eta: 0:04:41  lr: 0.000079  loss: 0.2948 (0.2862)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [520/689]  eta: 0:04:26  lr: 0.000079  loss: 0.2844 (0.2862)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [530/689]  eta: 0:04:10  lr: 0.000079  loss: 0.2844 (0.2864)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [540/689]  eta: 0:03:54  lr: 0.000079  loss: 0.2857 (0.2861)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [550/689]  eta: 0:03:38  lr: 0.000079  loss: 0.2621 (0.2860)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [560/689]  eta: 0:03:23  lr: 0.000079  loss: 0.2797 (0.2860)  time: 1.5758  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:286]  [570/689]  eta: 0:03:07  lr: 0.000079  loss: 0.2813 (0.2860)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [580/689]  eta: 0:02:51  lr: 0.000079  loss: 0.2796 (0.2858)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [590/689]  eta: 0:02:35  lr: 0.000079  loss: 0.2796 (0.2859)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [600/689]  eta: 0:02:20  lr: 0.000079  loss: 0.2965 (0.2861)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [610/689]  eta: 0:02:04  lr: 0.000079  loss: 0.2965 (0.2863)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [620/689]  eta: 0:01:48  lr: 0.000079  loss: 0.2783 (0.2861)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [630/689]  eta: 0:01:32  lr: 0.000079  loss: 0.2736 (0.2859)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [640/689]  eta: 0:01:17  lr: 0.000079  loss: 0.2801 (0.2861)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [650/689]  eta: 0:01:01  lr: 0.000079  loss: 0.2880 (0.2861)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [660/689]  eta: 0:00:45  lr: 0.000079  loss: 0.2871 (0.2864)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [670/689]  eta: 0:00:29  lr: 0.000079  loss: 0.2871 (0.2863)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [680/689]  eta: 0:00:14  lr: 0.000079  loss: 0.2762 (0.2862)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286]  [688/689]  eta: 0:00:01  lr: 0.000079  loss: 0.2737 (0.2859)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:286] Total time: 0:18:05 (1.5749 s / it)\n",
      "Averaged stats: lr: 0.000079  loss: 0.2737 (0.2859)\n",
      "Valid: [epoch:286]  [ 0/14]  eta: 0:00:13  loss: 0.2396 (0.2396)  time: 0.9810  data: 0.4056  max mem: 39763\n",
      "Valid: [epoch:286]  [13/14]  eta: 0:00:00  loss: 0.2679 (0.2705)  time: 0.1120  data: 0.0290  max mem: 39763\n",
      "Valid: [epoch:286] Total time: 0:00:01 (0.1212 s / it)\n",
      "Averaged stats: loss: 0.2679 (0.2705)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_286_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.271%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:287]  [  0/689]  eta: 0:11:59  lr: 0.000079  loss: 0.2459 (0.2459)  time: 1.0436  data: 0.5661  max mem: 39763\n",
      "Train: [epoch:287]  [ 10/689]  eta: 0:17:15  lr: 0.000079  loss: 0.2702 (0.2864)  time: 1.5247  data: 0.0515  max mem: 39763\n",
      "Train: [epoch:287]  [ 20/689]  eta: 0:17:15  lr: 0.000079  loss: 0.2720 (0.2817)  time: 1.5730  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [ 30/689]  eta: 0:17:05  lr: 0.000079  loss: 0.2747 (0.2840)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [ 40/689]  eta: 0:16:52  lr: 0.000079  loss: 0.2785 (0.2810)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [ 50/689]  eta: 0:16:39  lr: 0.000079  loss: 0.2638 (0.2804)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [ 60/689]  eta: 0:16:24  lr: 0.000079  loss: 0.2739 (0.2808)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [ 70/689]  eta: 0:16:09  lr: 0.000079  loss: 0.2667 (0.2796)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [ 80/689]  eta: 0:15:54  lr: 0.000079  loss: 0.2667 (0.2802)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [ 90/689]  eta: 0:15:39  lr: 0.000079  loss: 0.2619 (0.2798)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [100/689]  eta: 0:15:24  lr: 0.000079  loss: 0.2754 (0.2817)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [110/689]  eta: 0:15:08  lr: 0.000079  loss: 0.2881 (0.2815)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [120/689]  eta: 0:14:53  lr: 0.000079  loss: 0.2749 (0.2816)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [130/689]  eta: 0:14:37  lr: 0.000079  loss: 0.2710 (0.2821)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [140/689]  eta: 0:14:22  lr: 0.000079  loss: 0.2669 (0.2830)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [150/689]  eta: 0:14:06  lr: 0.000079  loss: 0.2844 (0.2841)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [160/689]  eta: 0:13:51  lr: 0.000079  loss: 0.3004 (0.2854)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [170/689]  eta: 0:13:35  lr: 0.000079  loss: 0.3004 (0.2859)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [180/689]  eta: 0:13:20  lr: 0.000079  loss: 0.2852 (0.2857)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [190/689]  eta: 0:13:04  lr: 0.000079  loss: 0.2855 (0.2862)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [200/689]  eta: 0:12:48  lr: 0.000079  loss: 0.2785 (0.2850)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [210/689]  eta: 0:12:33  lr: 0.000079  loss: 0.2555 (0.2844)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [220/689]  eta: 0:12:17  lr: 0.000079  loss: 0.2776 (0.2847)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [230/689]  eta: 0:12:01  lr: 0.000079  loss: 0.2933 (0.2855)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [240/689]  eta: 0:11:46  lr: 0.000079  loss: 0.3023 (0.2859)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [250/689]  eta: 0:11:30  lr: 0.000079  loss: 0.2840 (0.2860)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [260/689]  eta: 0:11:14  lr: 0.000079  loss: 0.2774 (0.2862)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [270/689]  eta: 0:10:59  lr: 0.000079  loss: 0.2774 (0.2861)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [280/689]  eta: 0:10:43  lr: 0.000079  loss: 0.2815 (0.2862)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [290/689]  eta: 0:10:27  lr: 0.000079  loss: 0.2810 (0.2864)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [300/689]  eta: 0:10:12  lr: 0.000079  loss: 0.2765 (0.2861)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [310/689]  eta: 0:09:56  lr: 0.000079  loss: 0.2721 (0.2856)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [320/689]  eta: 0:09:40  lr: 0.000079  loss: 0.2719 (0.2856)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [330/689]  eta: 0:09:24  lr: 0.000079  loss: 0.2717 (0.2857)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [340/689]  eta: 0:09:09  lr: 0.000079  loss: 0.2898 (0.2859)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [350/689]  eta: 0:08:53  lr: 0.000079  loss: 0.2919 (0.2864)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [360/689]  eta: 0:08:37  lr: 0.000079  loss: 0.2864 (0.2862)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [370/689]  eta: 0:08:22  lr: 0.000079  loss: 0.2731 (0.2862)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [380/689]  eta: 0:08:06  lr: 0.000079  loss: 0.2676 (0.2861)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [390/689]  eta: 0:07:50  lr: 0.000079  loss: 0.2781 (0.2862)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [400/689]  eta: 0:07:34  lr: 0.000079  loss: 0.2979 (0.2864)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [410/689]  eta: 0:07:19  lr: 0.000079  loss: 0.2979 (0.2865)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [420/689]  eta: 0:07:03  lr: 0.000079  loss: 0.2768 (0.2862)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [430/689]  eta: 0:06:47  lr: 0.000079  loss: 0.2768 (0.2859)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [440/689]  eta: 0:06:31  lr: 0.000079  loss: 0.2824 (0.2863)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [450/689]  eta: 0:06:16  lr: 0.000079  loss: 0.2760 (0.2862)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [460/689]  eta: 0:06:00  lr: 0.000079  loss: 0.2810 (0.2869)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [470/689]  eta: 0:05:44  lr: 0.000079  loss: 0.2956 (0.2871)  time: 1.5757  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:287]  [480/689]  eta: 0:05:28  lr: 0.000079  loss: 0.2910 (0.2874)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [490/689]  eta: 0:05:13  lr: 0.000079  loss: 0.2910 (0.2874)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [500/689]  eta: 0:04:57  lr: 0.000079  loss: 0.2946 (0.2880)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [510/689]  eta: 0:04:41  lr: 0.000079  loss: 0.2875 (0.2878)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [520/689]  eta: 0:04:26  lr: 0.000079  loss: 0.2763 (0.2879)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [530/689]  eta: 0:04:10  lr: 0.000079  loss: 0.2635 (0.2874)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [540/689]  eta: 0:03:54  lr: 0.000079  loss: 0.2646 (0.2874)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [550/689]  eta: 0:03:38  lr: 0.000079  loss: 0.2748 (0.2876)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [560/689]  eta: 0:03:23  lr: 0.000079  loss: 0.2748 (0.2876)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [570/689]  eta: 0:03:07  lr: 0.000079  loss: 0.2701 (0.2874)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [580/689]  eta: 0:02:51  lr: 0.000079  loss: 0.2750 (0.2873)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [590/689]  eta: 0:02:35  lr: 0.000079  loss: 0.2750 (0.2873)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [600/689]  eta: 0:02:20  lr: 0.000079  loss: 0.2704 (0.2871)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [610/689]  eta: 0:02:04  lr: 0.000079  loss: 0.2720 (0.2873)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [620/689]  eta: 0:01:48  lr: 0.000079  loss: 0.2787 (0.2874)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [630/689]  eta: 0:01:32  lr: 0.000079  loss: 0.2818 (0.2876)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [640/689]  eta: 0:01:17  lr: 0.000079  loss: 0.3028 (0.2878)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [650/689]  eta: 0:01:01  lr: 0.000079  loss: 0.2923 (0.2877)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [660/689]  eta: 0:00:45  lr: 0.000079  loss: 0.2908 (0.2879)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [670/689]  eta: 0:00:29  lr: 0.000079  loss: 0.2771 (0.2878)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [680/689]  eta: 0:00:14  lr: 0.000079  loss: 0.2679 (0.2875)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287]  [688/689]  eta: 0:00:01  lr: 0.000079  loss: 0.2655 (0.2874)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:287] Total time: 0:18:05 (1.5748 s / it)\n",
      "Averaged stats: lr: 0.000079  loss: 0.2655 (0.2874)\n",
      "Valid: [epoch:287]  [ 0/14]  eta: 0:00:14  loss: 0.2835 (0.2835)  time: 1.0083  data: 0.3980  max mem: 39763\n",
      "Valid: [epoch:287]  [13/14]  eta: 0:00:00  loss: 0.2710 (0.2738)  time: 0.1139  data: 0.0285  max mem: 39763\n",
      "Valid: [epoch:287] Total time: 0:00:01 (0.1216 s / it)\n",
      "Averaged stats: loss: 0.2710 (0.2738)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_287_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.274%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:288]  [  0/689]  eta: 0:11:48  lr: 0.000079  loss: 0.2795 (0.2795)  time: 1.0286  data: 0.5517  max mem: 39763\n",
      "Train: [epoch:288]  [ 10/689]  eta: 0:17:14  lr: 0.000079  loss: 0.2804 (0.2797)  time: 1.5240  data: 0.0502  max mem: 39763\n",
      "Train: [epoch:288]  [ 20/689]  eta: 0:17:16  lr: 0.000079  loss: 0.2845 (0.2862)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [ 30/689]  eta: 0:17:06  lr: 0.000079  loss: 0.2873 (0.2901)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [ 40/689]  eta: 0:16:53  lr: 0.000079  loss: 0.2921 (0.2930)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [ 50/689]  eta: 0:16:39  lr: 0.000079  loss: 0.2962 (0.2903)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [ 60/689]  eta: 0:16:24  lr: 0.000079  loss: 0.2913 (0.2901)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [ 70/689]  eta: 0:16:10  lr: 0.000079  loss: 0.2772 (0.2881)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [ 80/689]  eta: 0:15:55  lr: 0.000079  loss: 0.2772 (0.2876)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [ 90/689]  eta: 0:15:39  lr: 0.000079  loss: 0.2799 (0.2862)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [100/689]  eta: 0:15:24  lr: 0.000079  loss: 0.3007 (0.2887)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [110/689]  eta: 0:15:09  lr: 0.000079  loss: 0.2920 (0.2879)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [120/689]  eta: 0:14:53  lr: 0.000079  loss: 0.2861 (0.2886)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [130/689]  eta: 0:14:38  lr: 0.000079  loss: 0.2919 (0.2892)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [140/689]  eta: 0:14:22  lr: 0.000079  loss: 0.2807 (0.2891)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [150/689]  eta: 0:14:06  lr: 0.000079  loss: 0.2904 (0.2905)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [160/689]  eta: 0:13:51  lr: 0.000079  loss: 0.2869 (0.2913)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [170/689]  eta: 0:13:35  lr: 0.000079  loss: 0.2767 (0.2912)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [180/689]  eta: 0:13:20  lr: 0.000079  loss: 0.2847 (0.2914)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [190/689]  eta: 0:13:04  lr: 0.000079  loss: 0.2752 (0.2904)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [200/689]  eta: 0:12:48  lr: 0.000079  loss: 0.2744 (0.2896)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [210/689]  eta: 0:12:33  lr: 0.000079  loss: 0.2547 (0.2883)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [220/689]  eta: 0:12:17  lr: 0.000079  loss: 0.2652 (0.2880)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [230/689]  eta: 0:12:01  lr: 0.000079  loss: 0.3003 (0.2892)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [240/689]  eta: 0:11:46  lr: 0.000079  loss: 0.3003 (0.2895)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [250/689]  eta: 0:11:30  lr: 0.000079  loss: 0.2866 (0.2894)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [260/689]  eta: 0:11:14  lr: 0.000079  loss: 0.2928 (0.2896)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [270/689]  eta: 0:10:59  lr: 0.000079  loss: 0.2932 (0.2900)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [280/689]  eta: 0:10:43  lr: 0.000079  loss: 0.2866 (0.2902)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [290/689]  eta: 0:10:27  lr: 0.000079  loss: 0.2795 (0.2895)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [300/689]  eta: 0:10:11  lr: 0.000079  loss: 0.2752 (0.2894)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [310/689]  eta: 0:09:56  lr: 0.000079  loss: 0.2786 (0.2889)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [320/689]  eta: 0:09:40  lr: 0.000079  loss: 0.2702 (0.2890)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [330/689]  eta: 0:09:24  lr: 0.000079  loss: 0.2755 (0.2889)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [340/689]  eta: 0:09:09  lr: 0.000079  loss: 0.2742 (0.2887)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [350/689]  eta: 0:08:53  lr: 0.000079  loss: 0.2839 (0.2888)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [360/689]  eta: 0:08:37  lr: 0.000079  loss: 0.2842 (0.2888)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [370/689]  eta: 0:08:21  lr: 0.000079  loss: 0.2743 (0.2883)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [380/689]  eta: 0:08:06  lr: 0.000079  loss: 0.2850 (0.2887)  time: 1.5750  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:288]  [390/689]  eta: 0:07:50  lr: 0.000079  loss: 0.2873 (0.2886)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [400/689]  eta: 0:07:34  lr: 0.000079  loss: 0.2754 (0.2885)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [410/689]  eta: 0:07:19  lr: 0.000079  loss: 0.2733 (0.2882)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [420/689]  eta: 0:07:03  lr: 0.000079  loss: 0.2968 (0.2886)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [430/689]  eta: 0:06:47  lr: 0.000079  loss: 0.2968 (0.2884)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [440/689]  eta: 0:06:31  lr: 0.000079  loss: 0.2702 (0.2883)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [450/689]  eta: 0:06:16  lr: 0.000079  loss: 0.2909 (0.2884)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [460/689]  eta: 0:06:00  lr: 0.000079  loss: 0.2789 (0.2885)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [470/689]  eta: 0:05:44  lr: 0.000079  loss: 0.2693 (0.2885)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [480/689]  eta: 0:05:28  lr: 0.000079  loss: 0.2694 (0.2882)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [490/689]  eta: 0:05:13  lr: 0.000079  loss: 0.2765 (0.2882)  time: 1.5742  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [500/689]  eta: 0:04:57  lr: 0.000079  loss: 0.2794 (0.2881)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [510/689]  eta: 0:04:41  lr: 0.000079  loss: 0.2833 (0.2881)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [520/689]  eta: 0:04:25  lr: 0.000079  loss: 0.2982 (0.2883)  time: 1.5739  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [530/689]  eta: 0:04:10  lr: 0.000079  loss: 0.2785 (0.2882)  time: 1.5741  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [540/689]  eta: 0:03:54  lr: 0.000079  loss: 0.2788 (0.2885)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [550/689]  eta: 0:03:38  lr: 0.000079  loss: 0.2802 (0.2884)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [560/689]  eta: 0:03:23  lr: 0.000079  loss: 0.2879 (0.2888)  time: 1.5738  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [570/689]  eta: 0:03:07  lr: 0.000079  loss: 0.2891 (0.2888)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [580/689]  eta: 0:02:51  lr: 0.000079  loss: 0.2870 (0.2888)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [590/689]  eta: 0:02:35  lr: 0.000079  loss: 0.2869 (0.2887)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [600/689]  eta: 0:02:20  lr: 0.000079  loss: 0.2766 (0.2886)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [610/689]  eta: 0:02:04  lr: 0.000079  loss: 0.2647 (0.2882)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [620/689]  eta: 0:01:48  lr: 0.000079  loss: 0.2803 (0.2884)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [630/689]  eta: 0:01:32  lr: 0.000079  loss: 0.2803 (0.2882)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [640/689]  eta: 0:01:17  lr: 0.000079  loss: 0.2879 (0.2885)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [650/689]  eta: 0:01:01  lr: 0.000079  loss: 0.2911 (0.2885)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [660/689]  eta: 0:00:45  lr: 0.000079  loss: 0.2832 (0.2886)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [670/689]  eta: 0:00:29  lr: 0.000079  loss: 0.2716 (0.2883)  time: 1.5740  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [680/689]  eta: 0:00:14  lr: 0.000079  loss: 0.2718 (0.2882)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288]  [688/689]  eta: 0:00:01  lr: 0.000079  loss: 0.2745 (0.2882)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:288] Total time: 0:18:04 (1.5742 s / it)\n",
      "Averaged stats: lr: 0.000079  loss: 0.2745 (0.2882)\n",
      "Valid: [epoch:288]  [ 0/14]  eta: 0:00:14  loss: 0.2620 (0.2620)  time: 1.0345  data: 0.4067  max mem: 39763\n",
      "Valid: [epoch:288]  [13/14]  eta: 0:00:00  loss: 0.2693 (0.2721)  time: 0.1158  data: 0.0291  max mem: 39763\n",
      "Valid: [epoch:288] Total time: 0:00:01 (0.1248 s / it)\n",
      "Averaged stats: loss: 0.2693 (0.2721)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_288_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.272%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:289]  [  0/689]  eta: 0:12:39  lr: 0.000079  loss: 0.2924 (0.2924)  time: 1.1021  data: 0.6290  max mem: 39763\n",
      "Train: [epoch:289]  [ 10/689]  eta: 0:17:18  lr: 0.000079  loss: 0.2845 (0.2947)  time: 1.5299  data: 0.0572  max mem: 39763\n",
      "Train: [epoch:289]  [ 20/689]  eta: 0:17:16  lr: 0.000079  loss: 0.2746 (0.2871)  time: 1.5724  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [ 30/689]  eta: 0:17:06  lr: 0.000079  loss: 0.2761 (0.2964)  time: 1.5733  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [ 40/689]  eta: 0:16:53  lr: 0.000079  loss: 0.2903 (0.2950)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [ 50/689]  eta: 0:16:39  lr: 0.000079  loss: 0.2910 (0.2947)  time: 1.5737  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [ 60/689]  eta: 0:16:24  lr: 0.000079  loss: 0.2792 (0.2919)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [ 70/689]  eta: 0:16:09  lr: 0.000079  loss: 0.2784 (0.2921)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [ 80/689]  eta: 0:15:54  lr: 0.000079  loss: 0.2761 (0.2899)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [ 90/689]  eta: 0:15:39  lr: 0.000079  loss: 0.2761 (0.2890)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [100/689]  eta: 0:15:24  lr: 0.000079  loss: 0.2888 (0.2907)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [110/689]  eta: 0:15:09  lr: 0.000079  loss: 0.2760 (0.2895)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [120/689]  eta: 0:14:53  lr: 0.000079  loss: 0.2750 (0.2894)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [130/689]  eta: 0:14:38  lr: 0.000079  loss: 0.2678 (0.2882)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [140/689]  eta: 0:14:22  lr: 0.000079  loss: 0.2653 (0.2880)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [150/689]  eta: 0:14:06  lr: 0.000079  loss: 0.2878 (0.2888)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [160/689]  eta: 0:13:51  lr: 0.000079  loss: 0.2976 (0.2893)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [170/689]  eta: 0:13:35  lr: 0.000079  loss: 0.2744 (0.2880)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [180/689]  eta: 0:13:20  lr: 0.000079  loss: 0.2740 (0.2888)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [190/689]  eta: 0:13:04  lr: 0.000079  loss: 0.2795 (0.2891)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [200/689]  eta: 0:12:48  lr: 0.000079  loss: 0.2926 (0.2895)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [210/689]  eta: 0:12:33  lr: 0.000079  loss: 0.2926 (0.2893)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [220/689]  eta: 0:12:17  lr: 0.000079  loss: 0.2820 (0.2891)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [230/689]  eta: 0:12:01  lr: 0.000079  loss: 0.2879 (0.2893)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [240/689]  eta: 0:11:46  lr: 0.000079  loss: 0.2772 (0.2886)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [250/689]  eta: 0:11:30  lr: 0.000079  loss: 0.2694 (0.2885)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [260/689]  eta: 0:11:14  lr: 0.000079  loss: 0.2810 (0.2885)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [270/689]  eta: 0:10:59  lr: 0.000079  loss: 0.2768 (0.2883)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [280/689]  eta: 0:10:43  lr: 0.000079  loss: 0.2780 (0.2882)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [290/689]  eta: 0:10:27  lr: 0.000079  loss: 0.2816 (0.2879)  time: 1.5758  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:289]  [300/689]  eta: 0:10:12  lr: 0.000079  loss: 0.2845 (0.2881)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [310/689]  eta: 0:09:56  lr: 0.000079  loss: 0.2794 (0.2878)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [320/689]  eta: 0:09:40  lr: 0.000079  loss: 0.2761 (0.2877)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [330/689]  eta: 0:09:24  lr: 0.000079  loss: 0.2964 (0.2885)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [340/689]  eta: 0:09:09  lr: 0.000079  loss: 0.2862 (0.2881)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [350/689]  eta: 0:08:53  lr: 0.000079  loss: 0.2730 (0.2878)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [360/689]  eta: 0:08:37  lr: 0.000079  loss: 0.2762 (0.2882)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [370/689]  eta: 0:08:22  lr: 0.000079  loss: 0.2876 (0.2881)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [380/689]  eta: 0:08:06  lr: 0.000079  loss: 0.2802 (0.2882)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [390/689]  eta: 0:07:50  lr: 0.000079  loss: 0.2751 (0.2880)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [400/689]  eta: 0:07:34  lr: 0.000079  loss: 0.2730 (0.2882)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [410/689]  eta: 0:07:19  lr: 0.000079  loss: 0.2802 (0.2882)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [420/689]  eta: 0:07:03  lr: 0.000079  loss: 0.2789 (0.2881)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [430/689]  eta: 0:06:47  lr: 0.000079  loss: 0.2752 (0.2880)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [440/689]  eta: 0:06:31  lr: 0.000079  loss: 0.2767 (0.2883)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [450/689]  eta: 0:06:16  lr: 0.000079  loss: 0.2869 (0.2883)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [460/689]  eta: 0:06:00  lr: 0.000079  loss: 0.2946 (0.2886)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [470/689]  eta: 0:05:44  lr: 0.000079  loss: 0.2946 (0.2888)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [480/689]  eta: 0:05:29  lr: 0.000079  loss: 0.2834 (0.2886)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [490/689]  eta: 0:05:13  lr: 0.000079  loss: 0.2710 (0.2884)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [500/689]  eta: 0:04:57  lr: 0.000079  loss: 0.2698 (0.2881)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [510/689]  eta: 0:04:41  lr: 0.000079  loss: 0.2758 (0.2884)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [520/689]  eta: 0:04:26  lr: 0.000079  loss: 0.2927 (0.2884)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [530/689]  eta: 0:04:10  lr: 0.000079  loss: 0.2899 (0.2884)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [540/689]  eta: 0:03:54  lr: 0.000079  loss: 0.2949 (0.2884)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [550/689]  eta: 0:03:38  lr: 0.000079  loss: 0.2950 (0.2885)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [560/689]  eta: 0:03:23  lr: 0.000079  loss: 0.2924 (0.2883)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [570/689]  eta: 0:03:07  lr: 0.000079  loss: 0.2786 (0.2884)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [580/689]  eta: 0:02:51  lr: 0.000079  loss: 0.2890 (0.2889)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [590/689]  eta: 0:02:35  lr: 0.000079  loss: 0.2843 (0.2887)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [600/689]  eta: 0:02:20  lr: 0.000079  loss: 0.2835 (0.2887)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [610/689]  eta: 0:02:04  lr: 0.000079  loss: 0.2903 (0.2888)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [620/689]  eta: 0:01:48  lr: 0.000079  loss: 0.2770 (0.2888)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [630/689]  eta: 0:01:32  lr: 0.000079  loss: 0.2787 (0.2888)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [640/689]  eta: 0:01:17  lr: 0.000079  loss: 0.3083 (0.2890)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [650/689]  eta: 0:01:01  lr: 0.000079  loss: 0.2829 (0.2889)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [660/689]  eta: 0:00:45  lr: 0.000079  loss: 0.2757 (0.2889)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [670/689]  eta: 0:00:29  lr: 0.000079  loss: 0.2620 (0.2885)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [680/689]  eta: 0:00:14  lr: 0.000079  loss: 0.2760 (0.2887)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289]  [688/689]  eta: 0:00:01  lr: 0.000079  loss: 0.2924 (0.2887)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:289] Total time: 0:18:05 (1.5751 s / it)\n",
      "Averaged stats: lr: 0.000079  loss: 0.2924 (0.2887)\n",
      "Valid: [epoch:289]  [ 0/14]  eta: 0:00:13  loss: 0.2858 (0.2858)  time: 0.9856  data: 0.3712  max mem: 39763\n",
      "Valid: [epoch:289]  [13/14]  eta: 0:00:00  loss: 0.2701 (0.2731)  time: 0.1123  data: 0.0266  max mem: 39763\n",
      "Valid: [epoch:289] Total time: 0:00:01 (0.1214 s / it)\n",
      "Averaged stats: loss: 0.2701 (0.2731)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_289_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.273%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:290]  [  0/689]  eta: 0:12:06  lr: 0.000079  loss: 0.2704 (0.2704)  time: 1.0548  data: 0.5795  max mem: 39763\n",
      "Train: [epoch:290]  [ 10/689]  eta: 0:17:17  lr: 0.000079  loss: 0.2912 (0.2920)  time: 1.5276  data: 0.0528  max mem: 39763\n",
      "Train: [epoch:290]  [ 20/689]  eta: 0:17:17  lr: 0.000079  loss: 0.2813 (0.2850)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [ 30/689]  eta: 0:17:06  lr: 0.000079  loss: 0.2690 (0.2956)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [ 40/689]  eta: 0:16:54  lr: 0.000079  loss: 0.2858 (0.2962)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [ 50/689]  eta: 0:16:39  lr: 0.000079  loss: 0.2835 (0.2956)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [ 60/689]  eta: 0:16:25  lr: 0.000079  loss: 0.2811 (0.2912)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [ 70/689]  eta: 0:16:10  lr: 0.000079  loss: 0.2811 (0.2905)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [ 80/689]  eta: 0:15:55  lr: 0.000079  loss: 0.2779 (0.2883)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [ 90/689]  eta: 0:15:40  lr: 0.000079  loss: 0.2793 (0.2893)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [100/689]  eta: 0:15:24  lr: 0.000079  loss: 0.2805 (0.2895)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [110/689]  eta: 0:15:09  lr: 0.000079  loss: 0.2792 (0.2903)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [120/689]  eta: 0:14:53  lr: 0.000079  loss: 0.2729 (0.2900)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [130/689]  eta: 0:14:38  lr: 0.000079  loss: 0.2800 (0.2898)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [140/689]  eta: 0:14:22  lr: 0.000079  loss: 0.2837 (0.2898)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [150/689]  eta: 0:14:07  lr: 0.000079  loss: 0.2893 (0.2898)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [160/689]  eta: 0:13:51  lr: 0.000079  loss: 0.2994 (0.2906)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [170/689]  eta: 0:13:36  lr: 0.000079  loss: 0.2866 (0.2898)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [180/689]  eta: 0:13:20  lr: 0.000079  loss: 0.2866 (0.2909)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [190/689]  eta: 0:13:04  lr: 0.000079  loss: 0.2976 (0.2909)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [200/689]  eta: 0:12:49  lr: 0.000079  loss: 0.2728 (0.2901)  time: 1.5754  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:290]  [210/689]  eta: 0:12:33  lr: 0.000079  loss: 0.2815 (0.2901)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [220/689]  eta: 0:12:17  lr: 0.000079  loss: 0.2876 (0.2901)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [230/689]  eta: 0:12:02  lr: 0.000079  loss: 0.2905 (0.2900)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [240/689]  eta: 0:11:46  lr: 0.000079  loss: 0.2928 (0.2909)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [250/689]  eta: 0:11:30  lr: 0.000079  loss: 0.2842 (0.2908)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [260/689]  eta: 0:11:14  lr: 0.000079  loss: 0.2797 (0.2908)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [270/689]  eta: 0:10:59  lr: 0.000079  loss: 0.2849 (0.2911)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [280/689]  eta: 0:10:43  lr: 0.000079  loss: 0.2849 (0.2911)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [290/689]  eta: 0:10:27  lr: 0.000079  loss: 0.2714 (0.2903)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [300/689]  eta: 0:10:12  lr: 0.000079  loss: 0.2725 (0.2899)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [310/689]  eta: 0:09:56  lr: 0.000079  loss: 0.2702 (0.2889)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [320/689]  eta: 0:09:40  lr: 0.000079  loss: 0.2702 (0.2891)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [330/689]  eta: 0:09:25  lr: 0.000079  loss: 0.2888 (0.2892)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [340/689]  eta: 0:09:09  lr: 0.000079  loss: 0.2852 (0.2890)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [350/689]  eta: 0:08:53  lr: 0.000079  loss: 0.2894 (0.2887)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [360/689]  eta: 0:08:37  lr: 0.000079  loss: 0.2879 (0.2887)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [370/689]  eta: 0:08:22  lr: 0.000079  loss: 0.2662 (0.2884)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [380/689]  eta: 0:08:06  lr: 0.000079  loss: 0.2694 (0.2883)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [390/689]  eta: 0:07:50  lr: 0.000079  loss: 0.2842 (0.2887)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [400/689]  eta: 0:07:34  lr: 0.000079  loss: 0.2880 (0.2887)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [410/689]  eta: 0:07:19  lr: 0.000079  loss: 0.2849 (0.2887)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [420/689]  eta: 0:07:03  lr: 0.000079  loss: 0.2809 (0.2886)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [430/689]  eta: 0:06:47  lr: 0.000079  loss: 0.2834 (0.2895)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [440/689]  eta: 0:06:31  lr: 0.000079  loss: 0.3008 (0.2895)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [450/689]  eta: 0:06:16  lr: 0.000079  loss: 0.2959 (0.2898)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [460/689]  eta: 0:06:00  lr: 0.000079  loss: 0.2834 (0.2901)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [470/689]  eta: 0:05:44  lr: 0.000079  loss: 0.2813 (0.2901)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [480/689]  eta: 0:05:29  lr: 0.000079  loss: 0.2684 (0.2897)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [490/689]  eta: 0:05:13  lr: 0.000079  loss: 0.2639 (0.2896)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [500/689]  eta: 0:04:57  lr: 0.000079  loss: 0.2866 (0.2895)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [510/689]  eta: 0:04:41  lr: 0.000079  loss: 0.2934 (0.2899)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [520/689]  eta: 0:04:26  lr: 0.000079  loss: 0.2920 (0.2900)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [530/689]  eta: 0:04:10  lr: 0.000079  loss: 0.2682 (0.2897)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [540/689]  eta: 0:03:54  lr: 0.000079  loss: 0.2713 (0.2900)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [550/689]  eta: 0:03:38  lr: 0.000079  loss: 0.2857 (0.2899)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [560/689]  eta: 0:03:23  lr: 0.000079  loss: 0.2884 (0.2902)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [570/689]  eta: 0:03:07  lr: 0.000079  loss: 0.2913 (0.2902)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [580/689]  eta: 0:02:51  lr: 0.000079  loss: 0.2809 (0.2901)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [590/689]  eta: 0:02:35  lr: 0.000079  loss: 0.2830 (0.2903)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [600/689]  eta: 0:02:20  lr: 0.000079  loss: 0.2859 (0.2902)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [610/689]  eta: 0:02:04  lr: 0.000079  loss: 0.2719 (0.2899)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [620/689]  eta: 0:01:48  lr: 0.000079  loss: 0.2725 (0.2897)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [630/689]  eta: 0:01:32  lr: 0.000079  loss: 0.2789 (0.2898)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [640/689]  eta: 0:01:17  lr: 0.000079  loss: 0.2945 (0.2898)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [650/689]  eta: 0:01:01  lr: 0.000079  loss: 0.2802 (0.2897)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [660/689]  eta: 0:00:45  lr: 0.000079  loss: 0.2836 (0.2897)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [670/689]  eta: 0:00:29  lr: 0.000079  loss: 0.2769 (0.2895)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [680/689]  eta: 0:00:14  lr: 0.000079  loss: 0.2769 (0.2896)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290]  [688/689]  eta: 0:00:01  lr: 0.000079  loss: 0.2796 (0.2896)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:290] Total time: 0:18:05 (1.5750 s / it)\n",
      "Averaged stats: lr: 0.000079  loss: 0.2796 (0.2896)\n",
      "Valid: [epoch:290]  [ 0/14]  eta: 0:00:14  loss: 0.2936 (0.2936)  time: 1.0028  data: 0.4081  max mem: 39763\n",
      "Valid: [epoch:290]  [13/14]  eta: 0:00:00  loss: 0.2711 (0.2749)  time: 0.1135  data: 0.0292  max mem: 39763\n",
      "Valid: [epoch:290] Total time: 0:00:01 (0.1225 s / it)\n",
      "Averaged stats: loss: 0.2711 (0.2749)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_290_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.275%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:291]  [  0/689]  eta: 0:11:16  lr: 0.000079  loss: 0.2591 (0.2591)  time: 0.9820  data: 0.5104  max mem: 39763\n",
      "Train: [epoch:291]  [ 10/689]  eta: 0:17:12  lr: 0.000079  loss: 0.2739 (0.2738)  time: 1.5207  data: 0.0465  max mem: 39763\n",
      "Train: [epoch:291]  [ 20/689]  eta: 0:17:14  lr: 0.000079  loss: 0.2739 (0.2825)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [ 30/689]  eta: 0:17:05  lr: 0.000079  loss: 0.2764 (0.2846)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [ 40/689]  eta: 0:16:52  lr: 0.000079  loss: 0.2816 (0.2845)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [ 50/689]  eta: 0:16:38  lr: 0.000079  loss: 0.2844 (0.2863)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [ 60/689]  eta: 0:16:24  lr: 0.000079  loss: 0.2860 (0.2883)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [ 70/689]  eta: 0:16:09  lr: 0.000079  loss: 0.2817 (0.2868)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [ 80/689]  eta: 0:15:54  lr: 0.000079  loss: 0.2781 (0.2868)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [ 90/689]  eta: 0:15:39  lr: 0.000079  loss: 0.2755 (0.2862)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [100/689]  eta: 0:15:24  lr: 0.000079  loss: 0.2766 (0.2882)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [110/689]  eta: 0:15:08  lr: 0.000079  loss: 0.3052 (0.2903)  time: 1.5758  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:291]  [120/689]  eta: 0:14:53  lr: 0.000079  loss: 0.2987 (0.2910)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [130/689]  eta: 0:14:38  lr: 0.000079  loss: 0.2922 (0.2918)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [140/689]  eta: 0:14:22  lr: 0.000079  loss: 0.2822 (0.2913)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [150/689]  eta: 0:14:06  lr: 0.000079  loss: 0.2799 (0.2929)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [160/689]  eta: 0:13:51  lr: 0.000079  loss: 0.2981 (0.2932)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [170/689]  eta: 0:13:35  lr: 0.000079  loss: 0.2924 (0.2937)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [180/689]  eta: 0:13:20  lr: 0.000079  loss: 0.2863 (0.2938)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [190/689]  eta: 0:13:04  lr: 0.000079  loss: 0.2863 (0.2938)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [200/689]  eta: 0:12:48  lr: 0.000079  loss: 0.2943 (0.2940)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [210/689]  eta: 0:12:33  lr: 0.000079  loss: 0.2705 (0.2933)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [220/689]  eta: 0:12:17  lr: 0.000079  loss: 0.2705 (0.2941)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [230/689]  eta: 0:12:01  lr: 0.000079  loss: 0.2835 (0.2935)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [240/689]  eta: 0:11:46  lr: 0.000079  loss: 0.2835 (0.2934)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [250/689]  eta: 0:11:30  lr: 0.000079  loss: 0.2906 (0.2932)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [260/689]  eta: 0:11:14  lr: 0.000079  loss: 0.2865 (0.2938)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [270/689]  eta: 0:10:59  lr: 0.000079  loss: 0.2865 (0.2938)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [280/689]  eta: 0:10:43  lr: 0.000079  loss: 0.2940 (0.2943)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [290/689]  eta: 0:10:27  lr: 0.000079  loss: 0.2819 (0.2939)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [300/689]  eta: 0:10:12  lr: 0.000079  loss: 0.2762 (0.2937)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [310/689]  eta: 0:09:56  lr: 0.000079  loss: 0.2837 (0.2937)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [320/689]  eta: 0:09:40  lr: 0.000079  loss: 0.2813 (0.2936)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [330/689]  eta: 0:09:25  lr: 0.000079  loss: 0.2721 (0.2931)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [340/689]  eta: 0:09:09  lr: 0.000079  loss: 0.2762 (0.2934)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [350/689]  eta: 0:08:53  lr: 0.000079  loss: 0.2857 (0.2932)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [360/689]  eta: 0:08:37  lr: 0.000079  loss: 0.2843 (0.2928)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [370/689]  eta: 0:08:22  lr: 0.000079  loss: 0.2701 (0.2923)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [380/689]  eta: 0:08:06  lr: 0.000079  loss: 0.2701 (0.2920)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [390/689]  eta: 0:07:50  lr: 0.000079  loss: 0.2848 (0.2922)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [400/689]  eta: 0:07:34  lr: 0.000079  loss: 0.2848 (0.2925)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [410/689]  eta: 0:07:19  lr: 0.000079  loss: 0.2932 (0.2926)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [420/689]  eta: 0:07:03  lr: 0.000079  loss: 0.2764 (0.2924)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [430/689]  eta: 0:06:47  lr: 0.000079  loss: 0.2698 (0.2919)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [440/689]  eta: 0:06:32  lr: 0.000079  loss: 0.2784 (0.2919)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [450/689]  eta: 0:06:16  lr: 0.000079  loss: 0.2896 (0.2917)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [460/689]  eta: 0:06:00  lr: 0.000079  loss: 0.2873 (0.2921)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [470/689]  eta: 0:05:44  lr: 0.000079  loss: 0.3130 (0.2927)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [480/689]  eta: 0:05:29  lr: 0.000079  loss: 0.2919 (0.2925)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [490/689]  eta: 0:05:13  lr: 0.000079  loss: 0.2800 (0.2923)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [500/689]  eta: 0:04:57  lr: 0.000079  loss: 0.2746 (0.2921)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [510/689]  eta: 0:04:41  lr: 0.000079  loss: 0.2761 (0.2920)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [520/689]  eta: 0:04:26  lr: 0.000079  loss: 0.2869 (0.2921)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [530/689]  eta: 0:04:10  lr: 0.000079  loss: 0.2812 (0.2921)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [540/689]  eta: 0:03:54  lr: 0.000079  loss: 0.2791 (0.2919)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [550/689]  eta: 0:03:38  lr: 0.000079  loss: 0.2791 (0.2919)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [560/689]  eta: 0:03:23  lr: 0.000079  loss: 0.2801 (0.2918)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [570/689]  eta: 0:03:07  lr: 0.000079  loss: 0.2696 (0.2915)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [580/689]  eta: 0:02:51  lr: 0.000079  loss: 0.2800 (0.2917)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [590/689]  eta: 0:02:35  lr: 0.000079  loss: 0.2841 (0.2915)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [600/689]  eta: 0:02:20  lr: 0.000079  loss: 0.2761 (0.2915)  time: 1.5749  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [610/689]  eta: 0:02:04  lr: 0.000079  loss: 0.2805 (0.2913)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [620/689]  eta: 0:01:48  lr: 0.000079  loss: 0.2790 (0.2912)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [630/689]  eta: 0:01:32  lr: 0.000079  loss: 0.2790 (0.2911)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [640/689]  eta: 0:01:17  lr: 0.000079  loss: 0.2816 (0.2910)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [650/689]  eta: 0:01:01  lr: 0.000079  loss: 0.2816 (0.2909)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [660/689]  eta: 0:00:45  lr: 0.000079  loss: 0.2814 (0.2909)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [670/689]  eta: 0:00:29  lr: 0.000079  loss: 0.2773 (0.2908)  time: 1.5746  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [680/689]  eta: 0:00:14  lr: 0.000079  loss: 0.2828 (0.2909)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291]  [688/689]  eta: 0:00:01  lr: 0.000079  loss: 0.2836 (0.2907)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:291] Total time: 0:18:05 (1.5750 s / it)\n",
      "Averaged stats: lr: 0.000079  loss: 0.2836 (0.2907)\n",
      "Valid: [epoch:291]  [ 0/14]  eta: 0:00:14  loss: 0.2643 (0.2643)  time: 1.0017  data: 0.3697  max mem: 39763\n",
      "Valid: [epoch:291]  [13/14]  eta: 0:00:00  loss: 0.2719 (0.2751)  time: 0.1134  data: 0.0265  max mem: 39763\n",
      "Valid: [epoch:291] Total time: 0:00:01 (0.1210 s / it)\n",
      "Averaged stats: loss: 0.2719 (0.2751)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_291_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.275%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:292]  [  0/689]  eta: 0:11:37  lr: 0.000079  loss: 0.3235 (0.3235)  time: 1.0126  data: 0.5381  max mem: 39763\n",
      "Train: [epoch:292]  [ 10/689]  eta: 0:17:14  lr: 0.000079  loss: 0.2901 (0.2976)  time: 1.5242  data: 0.0490  max mem: 39763\n",
      "Train: [epoch:292]  [ 20/689]  eta: 0:17:15  lr: 0.000079  loss: 0.2861 (0.2992)  time: 1.5754  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:292]  [ 30/689]  eta: 0:17:06  lr: 0.000079  loss: 0.2935 (0.2981)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [ 40/689]  eta: 0:16:53  lr: 0.000079  loss: 0.2804 (0.2936)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [ 50/689]  eta: 0:16:39  lr: 0.000079  loss: 0.2781 (0.2927)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [ 60/689]  eta: 0:16:25  lr: 0.000079  loss: 0.2737 (0.2903)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [ 70/689]  eta: 0:16:10  lr: 0.000079  loss: 0.2737 (0.2892)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [ 80/689]  eta: 0:15:55  lr: 0.000079  loss: 0.2772 (0.2905)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [ 90/689]  eta: 0:15:40  lr: 0.000079  loss: 0.2774 (0.2899)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [100/689]  eta: 0:15:25  lr: 0.000079  loss: 0.2895 (0.2914)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [110/689]  eta: 0:15:09  lr: 0.000079  loss: 0.2937 (0.2915)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [120/689]  eta: 0:14:54  lr: 0.000079  loss: 0.2975 (0.2926)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [130/689]  eta: 0:14:38  lr: 0.000079  loss: 0.3059 (0.2933)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [140/689]  eta: 0:14:23  lr: 0.000079  loss: 0.2944 (0.2932)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [150/689]  eta: 0:14:07  lr: 0.000079  loss: 0.2784 (0.2921)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [160/689]  eta: 0:13:52  lr: 0.000079  loss: 0.2691 (0.2911)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [170/689]  eta: 0:13:36  lr: 0.000079  loss: 0.2856 (0.2923)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [180/689]  eta: 0:13:20  lr: 0.000079  loss: 0.2934 (0.2919)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [190/689]  eta: 0:13:05  lr: 0.000079  loss: 0.2824 (0.2916)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [200/689]  eta: 0:12:49  lr: 0.000079  loss: 0.2824 (0.2919)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [210/689]  eta: 0:12:33  lr: 0.000079  loss: 0.2697 (0.2914)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [220/689]  eta: 0:12:18  lr: 0.000079  loss: 0.2739 (0.2911)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [230/689]  eta: 0:12:02  lr: 0.000079  loss: 0.2679 (0.2906)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [240/689]  eta: 0:11:46  lr: 0.000079  loss: 0.2652 (0.2902)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [250/689]  eta: 0:11:30  lr: 0.000079  loss: 0.2831 (0.2913)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [260/689]  eta: 0:11:15  lr: 0.000079  loss: 0.2899 (0.2913)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [270/689]  eta: 0:10:59  lr: 0.000079  loss: 0.2812 (0.2910)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [280/689]  eta: 0:10:43  lr: 0.000079  loss: 0.2860 (0.2916)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [290/689]  eta: 0:10:28  lr: 0.000079  loss: 0.2758 (0.2909)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [300/689]  eta: 0:10:12  lr: 0.000079  loss: 0.2717 (0.2904)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [310/689]  eta: 0:09:56  lr: 0.000079  loss: 0.2682 (0.2895)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [320/689]  eta: 0:09:40  lr: 0.000079  loss: 0.2785 (0.2896)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [330/689]  eta: 0:09:25  lr: 0.000079  loss: 0.2792 (0.2895)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [340/689]  eta: 0:09:09  lr: 0.000079  loss: 0.2785 (0.2897)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [350/689]  eta: 0:08:53  lr: 0.000079  loss: 0.2828 (0.2898)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [360/689]  eta: 0:08:38  lr: 0.000079  loss: 0.2748 (0.2897)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [370/689]  eta: 0:08:22  lr: 0.000079  loss: 0.2756 (0.2898)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [380/689]  eta: 0:08:06  lr: 0.000079  loss: 0.2817 (0.2898)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [390/689]  eta: 0:07:50  lr: 0.000079  loss: 0.2815 (0.2895)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [400/689]  eta: 0:07:35  lr: 0.000079  loss: 0.2815 (0.2900)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [410/689]  eta: 0:07:19  lr: 0.000079  loss: 0.3041 (0.2906)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [420/689]  eta: 0:07:03  lr: 0.000079  loss: 0.3006 (0.2907)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [430/689]  eta: 0:06:47  lr: 0.000079  loss: 0.2975 (0.2910)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [440/689]  eta: 0:06:32  lr: 0.000079  loss: 0.2973 (0.2911)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [450/689]  eta: 0:06:16  lr: 0.000079  loss: 0.3027 (0.2914)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [460/689]  eta: 0:06:00  lr: 0.000079  loss: 0.3067 (0.2917)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [470/689]  eta: 0:05:44  lr: 0.000079  loss: 0.2916 (0.2918)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [480/689]  eta: 0:05:29  lr: 0.000079  loss: 0.2798 (0.2916)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [490/689]  eta: 0:05:13  lr: 0.000079  loss: 0.2798 (0.2918)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [500/689]  eta: 0:04:57  lr: 0.000079  loss: 0.2844 (0.2917)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [510/689]  eta: 0:04:41  lr: 0.000079  loss: 0.2874 (0.2918)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [520/689]  eta: 0:04:26  lr: 0.000079  loss: 0.2957 (0.2923)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [530/689]  eta: 0:04:10  lr: 0.000079  loss: 0.2827 (0.2919)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [540/689]  eta: 0:03:54  lr: 0.000079  loss: 0.2671 (0.2919)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [550/689]  eta: 0:03:38  lr: 0.000079  loss: 0.2705 (0.2917)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [560/689]  eta: 0:03:23  lr: 0.000079  loss: 0.2733 (0.2916)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [570/689]  eta: 0:03:07  lr: 0.000079  loss: 0.2846 (0.2916)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [580/689]  eta: 0:02:51  lr: 0.000079  loss: 0.2878 (0.2916)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [590/689]  eta: 0:02:35  lr: 0.000079  loss: 0.3065 (0.2920)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [600/689]  eta: 0:02:20  lr: 0.000079  loss: 0.3065 (0.2919)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [610/689]  eta: 0:02:04  lr: 0.000079  loss: 0.2741 (0.2918)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [620/689]  eta: 0:01:48  lr: 0.000079  loss: 0.2783 (0.2917)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [630/689]  eta: 0:01:32  lr: 0.000079  loss: 0.2870 (0.2917)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [640/689]  eta: 0:01:17  lr: 0.000079  loss: 0.2883 (0.2916)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [650/689]  eta: 0:01:01  lr: 0.000079  loss: 0.2844 (0.2917)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [660/689]  eta: 0:00:45  lr: 0.000079  loss: 0.2807 (0.2916)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [670/689]  eta: 0:00:29  lr: 0.000079  loss: 0.2700 (0.2914)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292]  [680/689]  eta: 0:00:14  lr: 0.000079  loss: 0.2817 (0.2917)  time: 1.5767  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:292]  [688/689]  eta: 0:00:01  lr: 0.000079  loss: 0.2875 (0.2915)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:292] Total time: 0:18:05 (1.5755 s / it)\n",
      "Averaged stats: lr: 0.000079  loss: 0.2875 (0.2915)\n",
      "Valid: [epoch:292]  [ 0/14]  eta: 0:00:13  loss: 0.2921 (0.2921)  time: 0.9992  data: 0.3995  max mem: 39763\n",
      "Valid: [epoch:292]  [13/14]  eta: 0:00:00  loss: 0.2732 (0.2776)  time: 0.1133  data: 0.0286  max mem: 39763\n",
      "Valid: [epoch:292] Total time: 0:00:01 (0.1213 s / it)\n",
      "Averaged stats: loss: 0.2732 (0.2776)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_292_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.278%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:293]  [  0/689]  eta: 0:12:52  lr: 0.000079  loss: 0.2871 (0.2871)  time: 1.1215  data: 0.6440  max mem: 39763\n",
      "Train: [epoch:293]  [ 10/689]  eta: 0:17:21  lr: 0.000079  loss: 0.2817 (0.2885)  time: 1.5333  data: 0.0586  max mem: 39763\n",
      "Train: [epoch:293]  [ 20/689]  eta: 0:17:19  lr: 0.000079  loss: 0.2749 (0.2829)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [ 30/689]  eta: 0:17:08  lr: 0.000079  loss: 0.2982 (0.2930)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [ 40/689]  eta: 0:16:55  lr: 0.000079  loss: 0.3016 (0.2942)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [ 50/689]  eta: 0:16:41  lr: 0.000079  loss: 0.2916 (0.2944)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [ 60/689]  eta: 0:16:26  lr: 0.000079  loss: 0.2846 (0.2939)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [ 70/689]  eta: 0:16:11  lr: 0.000079  loss: 0.2955 (0.2939)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [ 80/689]  eta: 0:15:56  lr: 0.000079  loss: 0.2837 (0.2942)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [ 90/689]  eta: 0:15:41  lr: 0.000079  loss: 0.2825 (0.2937)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [100/689]  eta: 0:15:25  lr: 0.000079  loss: 0.2848 (0.2939)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [110/689]  eta: 0:15:10  lr: 0.000079  loss: 0.2959 (0.2948)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [120/689]  eta: 0:14:54  lr: 0.000079  loss: 0.2853 (0.2944)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [130/689]  eta: 0:14:39  lr: 0.000079  loss: 0.2712 (0.2942)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [140/689]  eta: 0:14:23  lr: 0.000079  loss: 0.2885 (0.2944)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [150/689]  eta: 0:14:08  lr: 0.000079  loss: 0.2847 (0.2934)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [160/689]  eta: 0:13:52  lr: 0.000079  loss: 0.2847 (0.2946)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [170/689]  eta: 0:13:36  lr: 0.000079  loss: 0.2990 (0.2941)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [180/689]  eta: 0:13:21  lr: 0.000079  loss: 0.2927 (0.2946)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [190/689]  eta: 0:13:05  lr: 0.000079  loss: 0.2975 (0.2952)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [200/689]  eta: 0:12:49  lr: 0.000079  loss: 0.2809 (0.2943)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [210/689]  eta: 0:12:34  lr: 0.000079  loss: 0.2773 (0.2938)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [220/689]  eta: 0:12:18  lr: 0.000079  loss: 0.2864 (0.2950)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [230/689]  eta: 0:12:02  lr: 0.000079  loss: 0.2923 (0.2954)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [240/689]  eta: 0:11:47  lr: 0.000079  loss: 0.2825 (0.2953)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [250/689]  eta: 0:11:31  lr: 0.000079  loss: 0.2762 (0.2948)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [260/689]  eta: 0:11:15  lr: 0.000079  loss: 0.2930 (0.2947)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [270/689]  eta: 0:11:00  lr: 0.000079  loss: 0.2963 (0.2945)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [280/689]  eta: 0:10:44  lr: 0.000079  loss: 0.2830 (0.2944)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [290/689]  eta: 0:10:28  lr: 0.000079  loss: 0.2724 (0.2942)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [300/689]  eta: 0:10:12  lr: 0.000079  loss: 0.2716 (0.2939)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [310/689]  eta: 0:09:57  lr: 0.000079  loss: 0.2716 (0.2934)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [320/689]  eta: 0:09:41  lr: 0.000079  loss: 0.2810 (0.2936)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [330/689]  eta: 0:09:25  lr: 0.000079  loss: 0.2935 (0.2936)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [340/689]  eta: 0:09:09  lr: 0.000079  loss: 0.2935 (0.2934)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [350/689]  eta: 0:08:54  lr: 0.000079  loss: 0.2866 (0.2934)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [360/689]  eta: 0:08:38  lr: 0.000079  loss: 0.2969 (0.2937)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [370/689]  eta: 0:08:22  lr: 0.000079  loss: 0.2816 (0.2932)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [380/689]  eta: 0:08:06  lr: 0.000079  loss: 0.2732 (0.2929)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [390/689]  eta: 0:07:51  lr: 0.000079  loss: 0.2788 (0.2932)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [400/689]  eta: 0:07:35  lr: 0.000079  loss: 0.2896 (0.2933)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [410/689]  eta: 0:07:19  lr: 0.000079  loss: 0.2896 (0.2934)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [420/689]  eta: 0:07:03  lr: 0.000079  loss: 0.2792 (0.2930)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [430/689]  eta: 0:06:48  lr: 0.000079  loss: 0.2925 (0.2933)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [440/689]  eta: 0:06:32  lr: 0.000079  loss: 0.2903 (0.2930)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [450/689]  eta: 0:06:16  lr: 0.000079  loss: 0.2856 (0.2928)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [460/689]  eta: 0:06:00  lr: 0.000079  loss: 0.2950 (0.2932)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [470/689]  eta: 0:05:45  lr: 0.000079  loss: 0.3105 (0.2936)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [480/689]  eta: 0:05:29  lr: 0.000079  loss: 0.2852 (0.2934)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [490/689]  eta: 0:05:13  lr: 0.000079  loss: 0.2736 (0.2932)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [500/689]  eta: 0:04:57  lr: 0.000079  loss: 0.2764 (0.2931)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [510/689]  eta: 0:04:42  lr: 0.000079  loss: 0.2792 (0.2930)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [520/689]  eta: 0:04:26  lr: 0.000079  loss: 0.2744 (0.2925)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [530/689]  eta: 0:04:10  lr: 0.000079  loss: 0.2768 (0.2925)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [540/689]  eta: 0:03:54  lr: 0.000079  loss: 0.2843 (0.2923)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [550/689]  eta: 0:03:39  lr: 0.000079  loss: 0.2628 (0.2919)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [560/689]  eta: 0:03:23  lr: 0.000079  loss: 0.2742 (0.2919)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [570/689]  eta: 0:03:07  lr: 0.000079  loss: 0.2760 (0.2916)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [580/689]  eta: 0:02:51  lr: 0.000079  loss: 0.2760 (0.2917)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [590/689]  eta: 0:02:36  lr: 0.000079  loss: 0.3038 (0.2921)  time: 1.5767  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:293]  [600/689]  eta: 0:02:20  lr: 0.000079  loss: 0.2956 (0.2921)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [610/689]  eta: 0:02:04  lr: 0.000079  loss: 0.2724 (0.2920)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [620/689]  eta: 0:01:48  lr: 0.000079  loss: 0.2856 (0.2919)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [630/689]  eta: 0:01:32  lr: 0.000079  loss: 0.2902 (0.2921)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [640/689]  eta: 0:01:17  lr: 0.000079  loss: 0.2902 (0.2921)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [650/689]  eta: 0:01:01  lr: 0.000079  loss: 0.2833 (0.2921)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [660/689]  eta: 0:00:45  lr: 0.000079  loss: 0.3014 (0.2923)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [670/689]  eta: 0:00:29  lr: 0.000079  loss: 0.3047 (0.2926)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [680/689]  eta: 0:00:14  lr: 0.000079  loss: 0.2998 (0.2926)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293]  [688/689]  eta: 0:00:01  lr: 0.000079  loss: 0.2973 (0.2925)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:293] Total time: 0:18:06 (1.5765 s / it)\n",
      "Averaged stats: lr: 0.000079  loss: 0.2973 (0.2925)\n",
      "Valid: [epoch:293]  [ 0/14]  eta: 0:00:13  loss: 0.2911 (0.2911)  time: 0.9998  data: 0.4476  max mem: 39763\n",
      "Valid: [epoch:293]  [13/14]  eta: 0:00:00  loss: 0.2748 (0.2802)  time: 0.1133  data: 0.0320  max mem: 39763\n",
      "Valid: [epoch:293] Total time: 0:00:01 (0.1228 s / it)\n",
      "Averaged stats: loss: 0.2748 (0.2802)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_293_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.280%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:294]  [  0/689]  eta: 0:11:28  lr: 0.000079  loss: 0.2731 (0.2731)  time: 0.9999  data: 0.5233  max mem: 39763\n",
      "Train: [epoch:294]  [ 10/689]  eta: 0:17:14  lr: 0.000079  loss: 0.2731 (0.2833)  time: 1.5239  data: 0.0476  max mem: 39763\n",
      "Train: [epoch:294]  [ 20/689]  eta: 0:17:16  lr: 0.000079  loss: 0.2787 (0.2828)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [ 30/689]  eta: 0:17:07  lr: 0.000079  loss: 0.2813 (0.2851)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [ 40/689]  eta: 0:16:54  lr: 0.000079  loss: 0.2876 (0.2848)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [ 50/689]  eta: 0:16:40  lr: 0.000079  loss: 0.2793 (0.2821)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [ 60/689]  eta: 0:16:26  lr: 0.000079  loss: 0.2821 (0.2833)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [ 70/689]  eta: 0:16:11  lr: 0.000079  loss: 0.2872 (0.2850)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [ 80/689]  eta: 0:15:56  lr: 0.000079  loss: 0.2837 (0.2839)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [ 90/689]  eta: 0:15:40  lr: 0.000079  loss: 0.2840 (0.2854)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [100/689]  eta: 0:15:25  lr: 0.000079  loss: 0.2915 (0.2857)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [110/689]  eta: 0:15:10  lr: 0.000079  loss: 0.3162 (0.2887)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [120/689]  eta: 0:14:54  lr: 0.000079  loss: 0.3131 (0.2910)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [130/689]  eta: 0:14:39  lr: 0.000079  loss: 0.3105 (0.2927)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [140/689]  eta: 0:14:23  lr: 0.000079  loss: 0.2931 (0.2922)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [150/689]  eta: 0:14:08  lr: 0.000079  loss: 0.2822 (0.2920)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [160/689]  eta: 0:13:52  lr: 0.000079  loss: 0.2913 (0.2940)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [170/689]  eta: 0:13:36  lr: 0.000079  loss: 0.2897 (0.2932)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [180/689]  eta: 0:13:21  lr: 0.000079  loss: 0.2720 (0.2929)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [190/689]  eta: 0:13:05  lr: 0.000079  loss: 0.2852 (0.2933)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [200/689]  eta: 0:12:49  lr: 0.000079  loss: 0.2839 (0.2929)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [210/689]  eta: 0:12:34  lr: 0.000079  loss: 0.2839 (0.2932)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [220/689]  eta: 0:12:18  lr: 0.000079  loss: 0.2911 (0.2935)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [230/689]  eta: 0:12:02  lr: 0.000079  loss: 0.2911 (0.2939)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [240/689]  eta: 0:11:47  lr: 0.000079  loss: 0.2927 (0.2944)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [250/689]  eta: 0:11:31  lr: 0.000079  loss: 0.2789 (0.2935)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [260/689]  eta: 0:11:15  lr: 0.000079  loss: 0.2753 (0.2936)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [270/689]  eta: 0:10:59  lr: 0.000079  loss: 0.2787 (0.2932)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [280/689]  eta: 0:10:44  lr: 0.000079  loss: 0.2789 (0.2933)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [290/689]  eta: 0:10:28  lr: 0.000079  loss: 0.2839 (0.2926)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [300/689]  eta: 0:10:12  lr: 0.000079  loss: 0.2742 (0.2925)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [310/689]  eta: 0:09:57  lr: 0.000079  loss: 0.2731 (0.2920)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [320/689]  eta: 0:09:41  lr: 0.000079  loss: 0.2855 (0.2922)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [330/689]  eta: 0:09:25  lr: 0.000079  loss: 0.2904 (0.2921)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [340/689]  eta: 0:09:09  lr: 0.000079  loss: 0.2918 (0.2927)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [350/689]  eta: 0:08:54  lr: 0.000079  loss: 0.2871 (0.2924)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [360/689]  eta: 0:08:38  lr: 0.000079  loss: 0.2842 (0.2924)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [370/689]  eta: 0:08:22  lr: 0.000079  loss: 0.2843 (0.2924)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [380/689]  eta: 0:08:06  lr: 0.000079  loss: 0.2782 (0.2923)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [390/689]  eta: 0:07:51  lr: 0.000079  loss: 0.2766 (0.2922)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [400/689]  eta: 0:07:35  lr: 0.000079  loss: 0.2860 (0.2922)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [410/689]  eta: 0:07:19  lr: 0.000079  loss: 0.2855 (0.2920)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [420/689]  eta: 0:07:03  lr: 0.000079  loss: 0.2804 (0.2918)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [430/689]  eta: 0:06:48  lr: 0.000079  loss: 0.2881 (0.2922)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [440/689]  eta: 0:06:32  lr: 0.000079  loss: 0.3051 (0.2929)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [450/689]  eta: 0:06:16  lr: 0.000079  loss: 0.2963 (0.2931)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [460/689]  eta: 0:06:00  lr: 0.000079  loss: 0.2972 (0.2934)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [470/689]  eta: 0:05:45  lr: 0.000079  loss: 0.2919 (0.2937)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [480/689]  eta: 0:05:29  lr: 0.000079  loss: 0.2846 (0.2938)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [490/689]  eta: 0:05:13  lr: 0.000079  loss: 0.2744 (0.2935)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [500/689]  eta: 0:04:57  lr: 0.000079  loss: 0.2758 (0.2936)  time: 1.5768  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:294]  [510/689]  eta: 0:04:42  lr: 0.000079  loss: 0.2813 (0.2936)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [520/689]  eta: 0:04:26  lr: 0.000079  loss: 0.2975 (0.2938)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [530/689]  eta: 0:04:10  lr: 0.000079  loss: 0.2910 (0.2935)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [540/689]  eta: 0:03:54  lr: 0.000079  loss: 0.2820 (0.2936)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [550/689]  eta: 0:03:39  lr: 0.000079  loss: 0.2867 (0.2935)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [560/689]  eta: 0:03:23  lr: 0.000079  loss: 0.2990 (0.2936)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [570/689]  eta: 0:03:07  lr: 0.000079  loss: 0.2834 (0.2937)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [580/689]  eta: 0:02:51  lr: 0.000079  loss: 0.2821 (0.2935)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [590/689]  eta: 0:02:36  lr: 0.000079  loss: 0.2766 (0.2934)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [600/689]  eta: 0:02:20  lr: 0.000079  loss: 0.2813 (0.2936)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [610/689]  eta: 0:02:04  lr: 0.000079  loss: 0.2938 (0.2935)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [620/689]  eta: 0:01:48  lr: 0.000079  loss: 0.2786 (0.2934)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [630/689]  eta: 0:01:32  lr: 0.000079  loss: 0.2910 (0.2935)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [640/689]  eta: 0:01:17  lr: 0.000079  loss: 0.2948 (0.2936)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [650/689]  eta: 0:01:01  lr: 0.000079  loss: 0.2920 (0.2937)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [660/689]  eta: 0:00:45  lr: 0.000079  loss: 0.2868 (0.2935)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [670/689]  eta: 0:00:29  lr: 0.000079  loss: 0.2786 (0.2935)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [680/689]  eta: 0:00:14  lr: 0.000079  loss: 0.2849 (0.2936)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294]  [688/689]  eta: 0:00:01  lr: 0.000079  loss: 0.2862 (0.2937)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:294] Total time: 0:18:06 (1.5765 s / it)\n",
      "Averaged stats: lr: 0.000079  loss: 0.2862 (0.2937)\n",
      "Valid: [epoch:294]  [ 0/14]  eta: 0:00:13  loss: 0.2466 (0.2466)  time: 0.9983  data: 0.3896  max mem: 39763\n",
      "Valid: [epoch:294]  [13/14]  eta: 0:00:00  loss: 0.2752 (0.2789)  time: 0.1133  data: 0.0279  max mem: 39763\n",
      "Valid: [epoch:294] Total time: 0:00:01 (0.1235 s / it)\n",
      "Averaged stats: loss: 0.2752 (0.2789)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_294_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.279%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:295]  [  0/689]  eta: 0:12:18  lr: 0.000078  loss: 0.3567 (0.3567)  time: 1.0713  data: 0.5938  max mem: 39763\n",
      "Train: [epoch:295]  [ 10/689]  eta: 0:17:18  lr: 0.000078  loss: 0.2963 (0.3048)  time: 1.5293  data: 0.0541  max mem: 39763\n",
      "Train: [epoch:295]  [ 20/689]  eta: 0:17:18  lr: 0.000078  loss: 0.2898 (0.2981)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [ 30/689]  eta: 0:17:08  lr: 0.000078  loss: 0.2898 (0.3020)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [ 40/689]  eta: 0:16:55  lr: 0.000078  loss: 0.2896 (0.2989)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [ 50/689]  eta: 0:16:41  lr: 0.000078  loss: 0.2756 (0.2948)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [ 60/689]  eta: 0:16:26  lr: 0.000078  loss: 0.2740 (0.2930)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [ 70/689]  eta: 0:16:11  lr: 0.000078  loss: 0.2914 (0.2962)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [ 80/689]  eta: 0:15:56  lr: 0.000078  loss: 0.2914 (0.2940)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [ 90/689]  eta: 0:15:41  lr: 0.000078  loss: 0.2800 (0.2944)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [100/689]  eta: 0:15:25  lr: 0.000078  loss: 0.2884 (0.2958)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [110/689]  eta: 0:15:10  lr: 0.000078  loss: 0.3203 (0.2992)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [120/689]  eta: 0:14:55  lr: 0.000078  loss: 0.3179 (0.3002)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [130/689]  eta: 0:14:39  lr: 0.000078  loss: 0.2842 (0.2998)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [140/689]  eta: 0:14:23  lr: 0.000078  loss: 0.2822 (0.2988)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [150/689]  eta: 0:14:08  lr: 0.000078  loss: 0.2817 (0.2984)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [160/689]  eta: 0:13:52  lr: 0.000078  loss: 0.2822 (0.2991)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [170/689]  eta: 0:13:37  lr: 0.000078  loss: 0.2811 (0.2987)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [180/689]  eta: 0:13:21  lr: 0.000078  loss: 0.2765 (0.2980)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [190/689]  eta: 0:13:05  lr: 0.000078  loss: 0.2883 (0.2979)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [200/689]  eta: 0:12:50  lr: 0.000078  loss: 0.2915 (0.2984)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [210/689]  eta: 0:12:34  lr: 0.000078  loss: 0.2809 (0.2972)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [220/689]  eta: 0:12:18  lr: 0.000078  loss: 0.2752 (0.2974)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [230/689]  eta: 0:12:02  lr: 0.000078  loss: 0.2816 (0.2973)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [240/689]  eta: 0:11:47  lr: 0.000078  loss: 0.2790 (0.2970)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [250/689]  eta: 0:11:31  lr: 0.000078  loss: 0.2773 (0.2965)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [260/689]  eta: 0:11:15  lr: 0.000078  loss: 0.2942 (0.2973)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [270/689]  eta: 0:11:00  lr: 0.000078  loss: 0.3052 (0.2975)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [280/689]  eta: 0:10:44  lr: 0.000078  loss: 0.2986 (0.2978)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [290/689]  eta: 0:10:28  lr: 0.000078  loss: 0.2986 (0.2974)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [300/689]  eta: 0:10:12  lr: 0.000078  loss: 0.2804 (0.2975)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [310/689]  eta: 0:09:57  lr: 0.000078  loss: 0.2787 (0.2969)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [320/689]  eta: 0:09:41  lr: 0.000078  loss: 0.2732 (0.2961)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [330/689]  eta: 0:09:25  lr: 0.000078  loss: 0.2821 (0.2964)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [340/689]  eta: 0:09:09  lr: 0.000078  loss: 0.2882 (0.2961)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [350/689]  eta: 0:08:54  lr: 0.000078  loss: 0.2801 (0.2961)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [360/689]  eta: 0:08:38  lr: 0.000078  loss: 0.2735 (0.2960)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [370/689]  eta: 0:08:22  lr: 0.000078  loss: 0.2848 (0.2961)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [380/689]  eta: 0:08:06  lr: 0.000078  loss: 0.2848 (0.2960)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [390/689]  eta: 0:07:51  lr: 0.000078  loss: 0.2750 (0.2962)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [400/689]  eta: 0:07:35  lr: 0.000078  loss: 0.2872 (0.2957)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [410/689]  eta: 0:07:19  lr: 0.000078  loss: 0.2776 (0.2956)  time: 1.5767  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:295]  [420/689]  eta: 0:07:03  lr: 0.000078  loss: 0.2821 (0.2953)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [430/689]  eta: 0:06:48  lr: 0.000078  loss: 0.2759 (0.2949)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [440/689]  eta: 0:06:32  lr: 0.000078  loss: 0.2759 (0.2954)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [450/689]  eta: 0:06:16  lr: 0.000078  loss: 0.2802 (0.2950)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [460/689]  eta: 0:06:00  lr: 0.000078  loss: 0.2802 (0.2950)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [470/689]  eta: 0:05:45  lr: 0.000078  loss: 0.2732 (0.2946)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [480/689]  eta: 0:05:29  lr: 0.000078  loss: 0.2732 (0.2944)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [490/689]  eta: 0:05:13  lr: 0.000078  loss: 0.2861 (0.2944)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [500/689]  eta: 0:04:57  lr: 0.000078  loss: 0.2898 (0.2944)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [510/689]  eta: 0:04:42  lr: 0.000078  loss: 0.2813 (0.2943)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [520/689]  eta: 0:04:26  lr: 0.000078  loss: 0.3123 (0.2948)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [530/689]  eta: 0:04:10  lr: 0.000078  loss: 0.3033 (0.2947)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [540/689]  eta: 0:03:54  lr: 0.000078  loss: 0.2857 (0.2947)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [550/689]  eta: 0:03:39  lr: 0.000078  loss: 0.2857 (0.2946)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [560/689]  eta: 0:03:23  lr: 0.000078  loss: 0.2826 (0.2948)  time: 1.5787  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [570/689]  eta: 0:03:07  lr: 0.000078  loss: 0.2826 (0.2947)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [580/689]  eta: 0:02:51  lr: 0.000078  loss: 0.2877 (0.2947)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [590/689]  eta: 0:02:36  lr: 0.000078  loss: 0.2828 (0.2946)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [600/689]  eta: 0:02:20  lr: 0.000078  loss: 0.2903 (0.2950)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [610/689]  eta: 0:02:04  lr: 0.000078  loss: 0.2903 (0.2949)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [620/689]  eta: 0:01:48  lr: 0.000078  loss: 0.2774 (0.2950)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [630/689]  eta: 0:01:32  lr: 0.000078  loss: 0.2942 (0.2953)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [640/689]  eta: 0:01:17  lr: 0.000078  loss: 0.2940 (0.2951)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [650/689]  eta: 0:01:01  lr: 0.000078  loss: 0.2803 (0.2948)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [660/689]  eta: 0:00:45  lr: 0.000078  loss: 0.2783 (0.2947)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [670/689]  eta: 0:00:29  lr: 0.000078  loss: 0.2927 (0.2946)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [680/689]  eta: 0:00:14  lr: 0.000078  loss: 0.2855 (0.2946)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295]  [688/689]  eta: 0:00:01  lr: 0.000078  loss: 0.2798 (0.2944)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:295] Total time: 0:18:06 (1.5766 s / it)\n",
      "Averaged stats: lr: 0.000078  loss: 0.2798 (0.2944)\n",
      "Valid: [epoch:295]  [ 0/14]  eta: 0:00:14  loss: 0.2781 (0.2781)  time: 1.0067  data: 0.4162  max mem: 39763\n",
      "Valid: [epoch:295]  [13/14]  eta: 0:00:00  loss: 0.2781 (0.2962)  time: 0.1138  data: 0.0298  max mem: 39763\n",
      "Valid: [epoch:295] Total time: 0:00:01 (0.1231 s / it)\n",
      "Averaged stats: loss: 0.2781 (0.2962)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_295_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.296%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:296]  [  0/689]  eta: 0:11:54  lr: 0.000078  loss: 0.2822 (0.2822)  time: 1.0371  data: 0.5586  max mem: 39763\n",
      "Train: [epoch:296]  [ 10/689]  eta: 0:17:16  lr: 0.000078  loss: 0.2879 (0.3014)  time: 1.5272  data: 0.0509  max mem: 39763\n",
      "Train: [epoch:296]  [ 20/689]  eta: 0:17:17  lr: 0.000078  loss: 0.2879 (0.2993)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [ 30/689]  eta: 0:17:07  lr: 0.000078  loss: 0.2754 (0.2904)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [ 40/689]  eta: 0:16:54  lr: 0.000078  loss: 0.2731 (0.2883)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [ 50/689]  eta: 0:16:40  lr: 0.000078  loss: 0.2893 (0.2886)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [ 60/689]  eta: 0:16:26  lr: 0.000078  loss: 0.2899 (0.2891)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [ 70/689]  eta: 0:16:11  lr: 0.000078  loss: 0.2961 (0.2917)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [ 80/689]  eta: 0:15:56  lr: 0.000078  loss: 0.2912 (0.2938)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [ 90/689]  eta: 0:15:40  lr: 0.000078  loss: 0.2959 (0.2945)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [100/689]  eta: 0:15:25  lr: 0.000078  loss: 0.2977 (0.2942)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [110/689]  eta: 0:15:10  lr: 0.000078  loss: 0.2970 (0.2960)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [120/689]  eta: 0:14:54  lr: 0.000078  loss: 0.3000 (0.2964)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [130/689]  eta: 0:14:39  lr: 0.000078  loss: 0.2812 (0.2960)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [140/689]  eta: 0:14:23  lr: 0.000078  loss: 0.2777 (0.2951)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [150/689]  eta: 0:14:07  lr: 0.000078  loss: 0.2892 (0.2965)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [160/689]  eta: 0:13:52  lr: 0.000078  loss: 0.3009 (0.2958)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [170/689]  eta: 0:13:36  lr: 0.000078  loss: 0.2892 (0.2962)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [180/689]  eta: 0:13:21  lr: 0.000078  loss: 0.2909 (0.2959)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [190/689]  eta: 0:13:05  lr: 0.000078  loss: 0.2909 (0.2966)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [200/689]  eta: 0:12:49  lr: 0.000078  loss: 0.2886 (0.2963)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [210/689]  eta: 0:12:33  lr: 0.000078  loss: 0.2780 (0.2952)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [220/689]  eta: 0:12:18  lr: 0.000078  loss: 0.2746 (0.2950)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [230/689]  eta: 0:12:02  lr: 0.000078  loss: 0.2799 (0.2951)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [240/689]  eta: 0:11:46  lr: 0.000078  loss: 0.2758 (0.2950)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [250/689]  eta: 0:11:31  lr: 0.000078  loss: 0.3054 (0.2955)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [260/689]  eta: 0:11:15  lr: 0.000078  loss: 0.3028 (0.2953)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [270/689]  eta: 0:10:59  lr: 0.000078  loss: 0.2907 (0.2949)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [280/689]  eta: 0:10:44  lr: 0.000078  loss: 0.2813 (0.2949)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [290/689]  eta: 0:10:28  lr: 0.000078  loss: 0.2813 (0.2947)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [300/689]  eta: 0:10:12  lr: 0.000078  loss: 0.2814 (0.2948)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [310/689]  eta: 0:09:56  lr: 0.000078  loss: 0.2773 (0.2943)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [320/689]  eta: 0:09:41  lr: 0.000078  loss: 0.2772 (0.2947)  time: 1.5774  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:296]  [330/689]  eta: 0:09:25  lr: 0.000078  loss: 0.2821 (0.2946)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [340/689]  eta: 0:09:09  lr: 0.000078  loss: 0.2825 (0.2947)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [350/689]  eta: 0:08:54  lr: 0.000078  loss: 0.3030 (0.2948)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [360/689]  eta: 0:08:38  lr: 0.000078  loss: 0.2837 (0.2950)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [370/689]  eta: 0:08:22  lr: 0.000078  loss: 0.2828 (0.2949)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [380/689]  eta: 0:08:06  lr: 0.000078  loss: 0.2802 (0.2946)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [390/689]  eta: 0:07:51  lr: 0.000078  loss: 0.2932 (0.2949)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [400/689]  eta: 0:07:35  lr: 0.000078  loss: 0.2967 (0.2949)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [410/689]  eta: 0:07:19  lr: 0.000078  loss: 0.2931 (0.2947)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [420/689]  eta: 0:07:03  lr: 0.000078  loss: 0.2782 (0.2947)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [430/689]  eta: 0:06:48  lr: 0.000078  loss: 0.2866 (0.2950)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [440/689]  eta: 0:06:32  lr: 0.000078  loss: 0.2866 (0.2949)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [450/689]  eta: 0:06:16  lr: 0.000078  loss: 0.2841 (0.2948)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [460/689]  eta: 0:06:00  lr: 0.000078  loss: 0.2844 (0.2951)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [470/689]  eta: 0:05:45  lr: 0.000078  loss: 0.3017 (0.2953)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [480/689]  eta: 0:05:29  lr: 0.000078  loss: 0.2802 (0.2952)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [490/689]  eta: 0:05:13  lr: 0.000078  loss: 0.2813 (0.2952)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [500/689]  eta: 0:04:57  lr: 0.000078  loss: 0.2855 (0.2951)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [510/689]  eta: 0:04:42  lr: 0.000078  loss: 0.2865 (0.2949)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [520/689]  eta: 0:04:26  lr: 0.000078  loss: 0.2865 (0.2951)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [530/689]  eta: 0:04:10  lr: 0.000078  loss: 0.2977 (0.2955)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [540/689]  eta: 0:03:54  lr: 0.000078  loss: 0.3042 (0.2959)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [550/689]  eta: 0:03:39  lr: 0.000078  loss: 0.3042 (0.2959)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [560/689]  eta: 0:03:23  lr: 0.000078  loss: 0.3057 (0.2965)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [570/689]  eta: 0:03:07  lr: 0.000078  loss: 0.3044 (0.2964)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [580/689]  eta: 0:02:51  lr: 0.000078  loss: 0.2798 (0.2962)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [590/689]  eta: 0:02:36  lr: 0.000078  loss: 0.2913 (0.2963)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [600/689]  eta: 0:02:20  lr: 0.000078  loss: 0.2924 (0.2964)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [610/689]  eta: 0:02:04  lr: 0.000078  loss: 0.3063 (0.2966)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [620/689]  eta: 0:01:48  lr: 0.000078  loss: 0.2966 (0.2966)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [630/689]  eta: 0:01:33  lr: 0.000078  loss: 0.2880 (0.2966)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [640/689]  eta: 0:01:17  lr: 0.000078  loss: 0.2875 (0.2966)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [650/689]  eta: 0:01:01  lr: 0.000078  loss: 0.2875 (0.2966)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [660/689]  eta: 0:00:45  lr: 0.000078  loss: 0.2966 (0.2969)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [670/689]  eta: 0:00:29  lr: 0.000078  loss: 0.2966 (0.2969)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [680/689]  eta: 0:00:14  lr: 0.000078  loss: 0.2935 (0.2970)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296]  [688/689]  eta: 0:00:01  lr: 0.000078  loss: 0.3041 (0.2972)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:296] Total time: 0:18:06 (1.5766 s / it)\n",
      "Averaged stats: lr: 0.000078  loss: 0.3041 (0.2972)\n",
      "Valid: [epoch:296]  [ 0/14]  eta: 0:00:13  loss: 0.2492 (0.2492)  time: 0.9939  data: 0.3730  max mem: 39763\n",
      "Valid: [epoch:296]  [13/14]  eta: 0:00:00  loss: 0.2783 (0.2816)  time: 0.1130  data: 0.0267  max mem: 39763\n",
      "Valid: [epoch:296] Total time: 0:00:01 (0.1221 s / it)\n",
      "Averaged stats: loss: 0.2783 (0.2816)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_296_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.282%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:297]  [  0/689]  eta: 0:12:32  lr: 0.000078  loss: 0.2837 (0.2837)  time: 1.0921  data: 0.6146  max mem: 39763\n",
      "Train: [epoch:297]  [ 10/689]  eta: 0:17:18  lr: 0.000078  loss: 0.2916 (0.2940)  time: 1.5301  data: 0.0560  max mem: 39763\n",
      "Train: [epoch:297]  [ 20/689]  eta: 0:17:18  lr: 0.000078  loss: 0.2964 (0.2981)  time: 1.5751  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [ 30/689]  eta: 0:17:07  lr: 0.000078  loss: 0.3098 (0.3019)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [ 40/689]  eta: 0:16:54  lr: 0.000078  loss: 0.2874 (0.2962)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [ 50/689]  eta: 0:16:40  lr: 0.000078  loss: 0.2827 (0.2953)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [ 60/689]  eta: 0:16:26  lr: 0.000078  loss: 0.2827 (0.2942)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [ 70/689]  eta: 0:16:11  lr: 0.000078  loss: 0.2851 (0.2963)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [ 80/689]  eta: 0:15:56  lr: 0.000078  loss: 0.2894 (0.2953)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [ 90/689]  eta: 0:15:40  lr: 0.000078  loss: 0.2857 (0.2952)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [100/689]  eta: 0:15:25  lr: 0.000078  loss: 0.2857 (0.2954)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [110/689]  eta: 0:15:09  lr: 0.000078  loss: 0.3058 (0.2982)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [120/689]  eta: 0:14:54  lr: 0.000078  loss: 0.3038 (0.2977)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [130/689]  eta: 0:14:38  lr: 0.000078  loss: 0.2896 (0.2981)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [140/689]  eta: 0:14:23  lr: 0.000078  loss: 0.2815 (0.2968)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [150/689]  eta: 0:14:07  lr: 0.000078  loss: 0.2746 (0.2962)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [160/689]  eta: 0:13:52  lr: 0.000078  loss: 0.2946 (0.2968)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [170/689]  eta: 0:13:36  lr: 0.000078  loss: 0.2791 (0.2962)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [180/689]  eta: 0:13:20  lr: 0.000078  loss: 0.2839 (0.2958)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [190/689]  eta: 0:13:05  lr: 0.000078  loss: 0.2896 (0.2955)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [200/689]  eta: 0:12:49  lr: 0.000078  loss: 0.2918 (0.2954)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [210/689]  eta: 0:12:33  lr: 0.000078  loss: 0.2946 (0.2954)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [220/689]  eta: 0:12:18  lr: 0.000078  loss: 0.3136 (0.2964)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [230/689]  eta: 0:12:02  lr: 0.000078  loss: 0.2913 (0.2958)  time: 1.5774  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:297]  [240/689]  eta: 0:11:46  lr: 0.000078  loss: 0.2747 (0.2952)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [250/689]  eta: 0:11:31  lr: 0.000078  loss: 0.2762 (0.2946)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [260/689]  eta: 0:11:15  lr: 0.000078  loss: 0.2870 (0.2944)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [270/689]  eta: 0:10:59  lr: 0.000078  loss: 0.2870 (0.2938)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [280/689]  eta: 0:10:44  lr: 0.000078  loss: 0.2901 (0.2949)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [290/689]  eta: 0:10:28  lr: 0.000078  loss: 0.2981 (0.2949)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [300/689]  eta: 0:10:12  lr: 0.000078  loss: 0.2769 (0.2943)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [310/689]  eta: 0:09:56  lr: 0.000078  loss: 0.2738 (0.2939)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [320/689]  eta: 0:09:41  lr: 0.000078  loss: 0.2819 (0.2943)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [330/689]  eta: 0:09:25  lr: 0.000078  loss: 0.2831 (0.2943)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [340/689]  eta: 0:09:09  lr: 0.000078  loss: 0.2976 (0.2945)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [350/689]  eta: 0:08:53  lr: 0.000078  loss: 0.2986 (0.2949)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [360/689]  eta: 0:08:38  lr: 0.000078  loss: 0.2950 (0.2951)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [370/689]  eta: 0:08:22  lr: 0.000078  loss: 0.2892 (0.2952)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [380/689]  eta: 0:08:06  lr: 0.000078  loss: 0.2927 (0.2954)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [390/689]  eta: 0:07:50  lr: 0.000078  loss: 0.2932 (0.2958)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [400/689]  eta: 0:07:35  lr: 0.000078  loss: 0.3059 (0.2962)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [410/689]  eta: 0:07:19  lr: 0.000078  loss: 0.3046 (0.2963)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [420/689]  eta: 0:07:03  lr: 0.000078  loss: 0.2981 (0.2963)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [430/689]  eta: 0:06:47  lr: 0.000078  loss: 0.2894 (0.2960)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [440/689]  eta: 0:06:32  lr: 0.000078  loss: 0.2778 (0.2958)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [450/689]  eta: 0:06:16  lr: 0.000078  loss: 0.2696 (0.2953)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [460/689]  eta: 0:06:00  lr: 0.000078  loss: 0.2875 (0.2953)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [470/689]  eta: 0:05:44  lr: 0.000078  loss: 0.2992 (0.2956)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [480/689]  eta: 0:05:29  lr: 0.000078  loss: 0.2978 (0.2955)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [490/689]  eta: 0:05:13  lr: 0.000078  loss: 0.2963 (0.2958)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [500/689]  eta: 0:04:57  lr: 0.000078  loss: 0.2957 (0.2959)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [510/689]  eta: 0:04:41  lr: 0.000078  loss: 0.2795 (0.2957)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [520/689]  eta: 0:04:26  lr: 0.000078  loss: 0.2861 (0.2958)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [530/689]  eta: 0:04:10  lr: 0.000078  loss: 0.2973 (0.2962)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [540/689]  eta: 0:03:54  lr: 0.000078  loss: 0.2926 (0.2963)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [550/689]  eta: 0:03:38  lr: 0.000078  loss: 0.3105 (0.2966)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [560/689]  eta: 0:03:23  lr: 0.000078  loss: 0.3035 (0.2967)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [570/689]  eta: 0:03:07  lr: 0.000078  loss: 0.2929 (0.2966)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [580/689]  eta: 0:02:51  lr: 0.000078  loss: 0.2960 (0.2967)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [590/689]  eta: 0:02:35  lr: 0.000078  loss: 0.2947 (0.2967)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [600/689]  eta: 0:02:20  lr: 0.000078  loss: 0.2857 (0.2966)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [610/689]  eta: 0:02:04  lr: 0.000078  loss: 0.2863 (0.2967)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [620/689]  eta: 0:01:48  lr: 0.000078  loss: 0.2863 (0.2966)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [630/689]  eta: 0:01:32  lr: 0.000078  loss: 0.2877 (0.2968)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [640/689]  eta: 0:01:17  lr: 0.000078  loss: 0.2996 (0.2970)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [650/689]  eta: 0:01:01  lr: 0.000078  loss: 0.3045 (0.2972)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [660/689]  eta: 0:00:45  lr: 0.000078  loss: 0.2962 (0.2972)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [670/689]  eta: 0:00:29  lr: 0.000078  loss: 0.2910 (0.2972)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [680/689]  eta: 0:00:14  lr: 0.000078  loss: 0.2886 (0.2971)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297]  [688/689]  eta: 0:00:01  lr: 0.000078  loss: 0.2886 (0.2969)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:297] Total time: 0:18:05 (1.5757 s / it)\n",
      "Averaged stats: lr: 0.000078  loss: 0.2886 (0.2969)\n",
      "Valid: [epoch:297]  [ 0/14]  eta: 0:00:14  loss: 0.2967 (0.2967)  time: 1.0386  data: 0.3630  max mem: 39763\n",
      "Valid: [epoch:297]  [13/14]  eta: 0:00:00  loss: 0.2777 (0.2837)  time: 0.1161  data: 0.0260  max mem: 39763\n",
      "Valid: [epoch:297] Total time: 0:00:01 (0.1253 s / it)\n",
      "Averaged stats: loss: 0.2777 (0.2837)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_297_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.284%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:298]  [  0/689]  eta: 0:11:45  lr: 0.000078  loss: 0.2729 (0.2729)  time: 1.0243  data: 0.5466  max mem: 39763\n",
      "Train: [epoch:298]  [ 10/689]  eta: 0:17:16  lr: 0.000078  loss: 0.3068 (0.3102)  time: 1.5271  data: 0.0498  max mem: 39763\n",
      "Train: [epoch:298]  [ 20/689]  eta: 0:17:17  lr: 0.000078  loss: 0.2931 (0.3013)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [ 30/689]  eta: 0:17:07  lr: 0.000078  loss: 0.2869 (0.2990)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [ 40/689]  eta: 0:16:54  lr: 0.000078  loss: 0.2881 (0.2991)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [ 50/689]  eta: 0:16:40  lr: 0.000078  loss: 0.2883 (0.2960)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [ 60/689]  eta: 0:16:26  lr: 0.000078  loss: 0.2929 (0.2979)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [ 70/689]  eta: 0:16:11  lr: 0.000078  loss: 0.2911 (0.2973)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [ 80/689]  eta: 0:15:56  lr: 0.000078  loss: 0.2877 (0.2958)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [ 90/689]  eta: 0:15:40  lr: 0.000078  loss: 0.2877 (0.2949)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [100/689]  eta: 0:15:25  lr: 0.000078  loss: 0.2983 (0.2955)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [110/689]  eta: 0:15:10  lr: 0.000078  loss: 0.2983 (0.2955)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [120/689]  eta: 0:14:54  lr: 0.000078  loss: 0.2879 (0.2962)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [130/689]  eta: 0:14:39  lr: 0.000078  loss: 0.2976 (0.2977)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [140/689]  eta: 0:14:23  lr: 0.000078  loss: 0.3022 (0.2980)  time: 1.5772  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:298]  [150/689]  eta: 0:14:08  lr: 0.000078  loss: 0.2945 (0.2988)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [160/689]  eta: 0:13:52  lr: 0.000078  loss: 0.2879 (0.2987)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [170/689]  eta: 0:13:36  lr: 0.000078  loss: 0.2964 (0.2984)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [180/689]  eta: 0:13:21  lr: 0.000078  loss: 0.2978 (0.2986)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [190/689]  eta: 0:13:05  lr: 0.000078  loss: 0.3060 (0.2988)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [200/689]  eta: 0:12:49  lr: 0.000078  loss: 0.2875 (0.2987)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [210/689]  eta: 0:12:34  lr: 0.000078  loss: 0.2786 (0.2976)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [220/689]  eta: 0:12:18  lr: 0.000078  loss: 0.2753 (0.2980)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [230/689]  eta: 0:12:02  lr: 0.000078  loss: 0.2995 (0.2980)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [240/689]  eta: 0:11:47  lr: 0.000078  loss: 0.2986 (0.2983)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [250/689]  eta: 0:11:31  lr: 0.000078  loss: 0.2986 (0.2985)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [260/689]  eta: 0:11:15  lr: 0.000078  loss: 0.3072 (0.2989)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [270/689]  eta: 0:10:59  lr: 0.000078  loss: 0.2977 (0.2982)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [280/689]  eta: 0:10:44  lr: 0.000078  loss: 0.2889 (0.2986)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [290/689]  eta: 0:10:28  lr: 0.000078  loss: 0.2882 (0.2983)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [300/689]  eta: 0:10:12  lr: 0.000078  loss: 0.2825 (0.2984)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [310/689]  eta: 0:09:57  lr: 0.000078  loss: 0.2840 (0.2979)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [320/689]  eta: 0:09:41  lr: 0.000078  loss: 0.2824 (0.2976)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [330/689]  eta: 0:09:25  lr: 0.000078  loss: 0.2924 (0.2981)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [340/689]  eta: 0:09:09  lr: 0.000078  loss: 0.3010 (0.2979)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [350/689]  eta: 0:08:54  lr: 0.000078  loss: 0.2985 (0.2981)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [360/689]  eta: 0:08:38  lr: 0.000078  loss: 0.3067 (0.2986)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [370/689]  eta: 0:08:22  lr: 0.000078  loss: 0.2977 (0.2986)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [380/689]  eta: 0:08:06  lr: 0.000078  loss: 0.2812 (0.2986)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [390/689]  eta: 0:07:51  lr: 0.000078  loss: 0.2905 (0.2984)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [400/689]  eta: 0:07:35  lr: 0.000078  loss: 0.2915 (0.2985)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [410/689]  eta: 0:07:19  lr: 0.000078  loss: 0.3042 (0.2987)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [420/689]  eta: 0:07:03  lr: 0.000078  loss: 0.2824 (0.2981)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [430/689]  eta: 0:06:48  lr: 0.000078  loss: 0.2812 (0.2981)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [440/689]  eta: 0:06:32  lr: 0.000078  loss: 0.3027 (0.2983)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [450/689]  eta: 0:06:16  lr: 0.000078  loss: 0.2953 (0.2982)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [460/689]  eta: 0:06:00  lr: 0.000078  loss: 0.2953 (0.2985)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [470/689]  eta: 0:05:45  lr: 0.000078  loss: 0.2951 (0.2983)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [480/689]  eta: 0:05:29  lr: 0.000078  loss: 0.2995 (0.2989)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [490/689]  eta: 0:05:13  lr: 0.000078  loss: 0.3069 (0.2989)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [500/689]  eta: 0:04:57  lr: 0.000078  loss: 0.3076 (0.2992)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [510/689]  eta: 0:04:42  lr: 0.000078  loss: 0.2887 (0.2988)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [520/689]  eta: 0:04:26  lr: 0.000078  loss: 0.2887 (0.2991)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [530/689]  eta: 0:04:10  lr: 0.000078  loss: 0.3102 (0.2996)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [540/689]  eta: 0:03:54  lr: 0.000078  loss: 0.3015 (0.2996)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [550/689]  eta: 0:03:39  lr: 0.000078  loss: 0.2914 (0.2994)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [560/689]  eta: 0:03:23  lr: 0.000078  loss: 0.2954 (0.2994)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [570/689]  eta: 0:03:07  lr: 0.000078  loss: 0.2852 (0.2991)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [580/689]  eta: 0:02:51  lr: 0.000078  loss: 0.2820 (0.2988)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [590/689]  eta: 0:02:36  lr: 0.000078  loss: 0.2820 (0.2987)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [600/689]  eta: 0:02:20  lr: 0.000078  loss: 0.2827 (0.2985)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [610/689]  eta: 0:02:04  lr: 0.000078  loss: 0.2827 (0.2984)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [620/689]  eta: 0:01:48  lr: 0.000078  loss: 0.2832 (0.2982)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [630/689]  eta: 0:01:32  lr: 0.000078  loss: 0.2663 (0.2976)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [640/689]  eta: 0:01:17  lr: 0.000078  loss: 0.2742 (0.2976)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [650/689]  eta: 0:01:01  lr: 0.000078  loss: 0.2974 (0.2977)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [660/689]  eta: 0:00:45  lr: 0.000078  loss: 0.2909 (0.2976)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [670/689]  eta: 0:00:29  lr: 0.000078  loss: 0.2882 (0.2973)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [680/689]  eta: 0:00:14  lr: 0.000078  loss: 0.2948 (0.2975)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298]  [688/689]  eta: 0:00:01  lr: 0.000078  loss: 0.2832 (0.2971)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:298] Total time: 0:18:06 (1.5763 s / it)\n",
      "Averaged stats: lr: 0.000078  loss: 0.2832 (0.2971)\n",
      "Valid: [epoch:298]  [ 0/14]  eta: 0:00:14  loss: 0.2779 (0.2779)  time: 1.0337  data: 0.3778  max mem: 39763\n",
      "Valid: [epoch:298]  [13/14]  eta: 0:00:00  loss: 0.2779 (0.2836)  time: 0.1157  data: 0.0270  max mem: 39763\n",
      "Valid: [epoch:298] Total time: 0:00:01 (0.1251 s / it)\n",
      "Averaged stats: loss: 0.2779 (0.2836)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_298_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.284%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:299]  [  0/689]  eta: 0:12:24  lr: 0.000078  loss: 0.3122 (0.3122)  time: 1.0811  data: 0.6038  max mem: 39763\n",
      "Train: [epoch:299]  [ 10/689]  eta: 0:17:19  lr: 0.000078  loss: 0.2983 (0.2951)  time: 1.5308  data: 0.0550  max mem: 39763\n",
      "Train: [epoch:299]  [ 20/689]  eta: 0:17:18  lr: 0.000078  loss: 0.2852 (0.2906)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [ 30/689]  eta: 0:17:08  lr: 0.000078  loss: 0.2852 (0.2974)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [ 40/689]  eta: 0:16:55  lr: 0.000078  loss: 0.2892 (0.2952)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [ 50/689]  eta: 0:16:41  lr: 0.000078  loss: 0.2785 (0.2923)  time: 1.5766  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:299]  [ 60/689]  eta: 0:16:26  lr: 0.000078  loss: 0.2802 (0.2925)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [ 70/689]  eta: 0:16:11  lr: 0.000078  loss: 0.2897 (0.2959)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [ 80/689]  eta: 0:15:56  lr: 0.000078  loss: 0.2972 (0.2959)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [ 90/689]  eta: 0:15:41  lr: 0.000078  loss: 0.2943 (0.2948)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [100/689]  eta: 0:15:25  lr: 0.000078  loss: 0.2890 (0.2945)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [110/689]  eta: 0:15:10  lr: 0.000078  loss: 0.2902 (0.2946)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [120/689]  eta: 0:14:55  lr: 0.000078  loss: 0.2920 (0.2942)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [130/689]  eta: 0:14:39  lr: 0.000078  loss: 0.2920 (0.2942)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [140/689]  eta: 0:14:23  lr: 0.000078  loss: 0.2849 (0.2942)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [150/689]  eta: 0:14:08  lr: 0.000078  loss: 0.2849 (0.2944)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [160/689]  eta: 0:13:52  lr: 0.000078  loss: 0.2821 (0.2934)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [170/689]  eta: 0:13:37  lr: 0.000078  loss: 0.2821 (0.2946)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [180/689]  eta: 0:13:21  lr: 0.000078  loss: 0.2989 (0.2951)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [190/689]  eta: 0:13:05  lr: 0.000078  loss: 0.2991 (0.2953)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [200/689]  eta: 0:12:50  lr: 0.000078  loss: 0.2987 (0.2955)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [210/689]  eta: 0:12:34  lr: 0.000078  loss: 0.2786 (0.2947)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [220/689]  eta: 0:12:18  lr: 0.000078  loss: 0.2786 (0.2953)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [230/689]  eta: 0:12:02  lr: 0.000078  loss: 0.3044 (0.2961)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [240/689]  eta: 0:11:47  lr: 0.000078  loss: 0.3114 (0.2969)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [250/689]  eta: 0:11:31  lr: 0.000078  loss: 0.3051 (0.2964)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [260/689]  eta: 0:11:15  lr: 0.000078  loss: 0.3017 (0.2967)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [270/689]  eta: 0:11:00  lr: 0.000078  loss: 0.3043 (0.2969)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [280/689]  eta: 0:10:44  lr: 0.000078  loss: 0.3077 (0.2969)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [290/689]  eta: 0:10:28  lr: 0.000078  loss: 0.2794 (0.2966)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [300/689]  eta: 0:10:12  lr: 0.000078  loss: 0.2787 (0.2965)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [310/689]  eta: 0:09:57  lr: 0.000078  loss: 0.2881 (0.2969)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [320/689]  eta: 0:09:41  lr: 0.000078  loss: 0.2919 (0.2972)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [330/689]  eta: 0:09:25  lr: 0.000078  loss: 0.2823 (0.2969)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [340/689]  eta: 0:09:09  lr: 0.000078  loss: 0.2944 (0.2971)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [350/689]  eta: 0:08:54  lr: 0.000078  loss: 0.2960 (0.2971)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [360/689]  eta: 0:08:38  lr: 0.000078  loss: 0.2978 (0.2979)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [370/689]  eta: 0:08:22  lr: 0.000078  loss: 0.3194 (0.2981)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [380/689]  eta: 0:08:06  lr: 0.000078  loss: 0.3055 (0.2983)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [390/689]  eta: 0:07:51  lr: 0.000078  loss: 0.2945 (0.2982)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [400/689]  eta: 0:07:35  lr: 0.000078  loss: 0.3029 (0.2987)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [410/689]  eta: 0:07:19  lr: 0.000078  loss: 0.3029 (0.2987)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [420/689]  eta: 0:07:03  lr: 0.000078  loss: 0.2953 (0.2987)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [430/689]  eta: 0:06:48  lr: 0.000078  loss: 0.2932 (0.2987)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [440/689]  eta: 0:06:32  lr: 0.000078  loss: 0.2917 (0.2987)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [450/689]  eta: 0:06:16  lr: 0.000078  loss: 0.2921 (0.2988)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [460/689]  eta: 0:06:00  lr: 0.000078  loss: 0.2911 (0.2986)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [470/689]  eta: 0:05:45  lr: 0.000078  loss: 0.2816 (0.2986)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [480/689]  eta: 0:05:29  lr: 0.000078  loss: 0.2810 (0.2985)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [490/689]  eta: 0:05:13  lr: 0.000078  loss: 0.2810 (0.2983)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [500/689]  eta: 0:04:57  lr: 0.000078  loss: 0.2861 (0.2985)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [510/689]  eta: 0:04:42  lr: 0.000078  loss: 0.2883 (0.2982)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [520/689]  eta: 0:04:26  lr: 0.000078  loss: 0.2790 (0.2980)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [530/689]  eta: 0:04:10  lr: 0.000078  loss: 0.2828 (0.2978)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [540/689]  eta: 0:03:54  lr: 0.000078  loss: 0.2800 (0.2976)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [550/689]  eta: 0:03:39  lr: 0.000078  loss: 0.2870 (0.2977)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [560/689]  eta: 0:03:23  lr: 0.000078  loss: 0.2933 (0.2978)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [570/689]  eta: 0:03:07  lr: 0.000078  loss: 0.3004 (0.2979)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [580/689]  eta: 0:02:51  lr: 0.000078  loss: 0.2869 (0.2975)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [590/689]  eta: 0:02:36  lr: 0.000078  loss: 0.3051 (0.2979)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [600/689]  eta: 0:02:20  lr: 0.000078  loss: 0.3227 (0.2981)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [610/689]  eta: 0:02:04  lr: 0.000078  loss: 0.2865 (0.2978)  time: 1.5788  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [620/689]  eta: 0:01:48  lr: 0.000078  loss: 0.2865 (0.2980)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [630/689]  eta: 0:01:33  lr: 0.000078  loss: 0.2885 (0.2978)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [640/689]  eta: 0:01:17  lr: 0.000078  loss: 0.2812 (0.2976)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [650/689]  eta: 0:01:01  lr: 0.000078  loss: 0.2909 (0.2978)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [660/689]  eta: 0:00:45  lr: 0.000078  loss: 0.2919 (0.2979)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [670/689]  eta: 0:00:29  lr: 0.000078  loss: 0.3095 (0.2981)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [680/689]  eta: 0:00:14  lr: 0.000078  loss: 0.2945 (0.2979)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299]  [688/689]  eta: 0:00:01  lr: 0.000078  loss: 0.2894 (0.2979)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:299] Total time: 0:18:06 (1.5768 s / it)\n",
      "Averaged stats: lr: 0.000078  loss: 0.2894 (0.2979)\n",
      "Valid: [epoch:299]  [ 0/14]  eta: 0:00:13  loss: 0.2626 (0.2626)  time: 0.9808  data: 0.4439  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:299]  [13/14]  eta: 0:00:00  loss: 0.2790 (0.2866)  time: 0.1120  data: 0.0318  max mem: 39763\n",
      "Valid: [epoch:299] Total time: 0:00:01 (0.1211 s / it)\n",
      "Averaged stats: loss: 0.2790 (0.2866)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_299_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.287%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:300]  [  0/689]  eta: 0:11:37  lr: 0.000078  loss: 0.2815 (0.2815)  time: 1.0129  data: 0.5364  max mem: 39763\n",
      "Train: [epoch:300]  [ 10/689]  eta: 0:17:15  lr: 0.000078  loss: 0.2815 (0.2902)  time: 1.5256  data: 0.0488  max mem: 39763\n",
      "Train: [epoch:300]  [ 20/689]  eta: 0:17:16  lr: 0.000078  loss: 0.2869 (0.2995)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [ 30/689]  eta: 0:17:07  lr: 0.000078  loss: 0.2934 (0.2998)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [ 40/689]  eta: 0:16:54  lr: 0.000078  loss: 0.2934 (0.2991)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [ 50/689]  eta: 0:16:40  lr: 0.000078  loss: 0.2915 (0.3001)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [ 60/689]  eta: 0:16:26  lr: 0.000078  loss: 0.2821 (0.2990)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [ 70/689]  eta: 0:16:11  lr: 0.000078  loss: 0.2893 (0.3005)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [ 80/689]  eta: 0:15:56  lr: 0.000078  loss: 0.2987 (0.3005)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [ 90/689]  eta: 0:15:40  lr: 0.000078  loss: 0.3014 (0.3024)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [100/689]  eta: 0:15:25  lr: 0.000078  loss: 0.2977 (0.3013)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [110/689]  eta: 0:15:10  lr: 0.000078  loss: 0.2974 (0.3017)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [120/689]  eta: 0:14:54  lr: 0.000078  loss: 0.3005 (0.3016)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [130/689]  eta: 0:14:39  lr: 0.000078  loss: 0.2864 (0.3001)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [140/689]  eta: 0:14:23  lr: 0.000078  loss: 0.2849 (0.2992)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [150/689]  eta: 0:14:07  lr: 0.000078  loss: 0.2887 (0.2985)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [160/689]  eta: 0:13:52  lr: 0.000078  loss: 0.2819 (0.2975)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [170/689]  eta: 0:13:36  lr: 0.000078  loss: 0.2793 (0.2972)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [180/689]  eta: 0:13:21  lr: 0.000078  loss: 0.2851 (0.2970)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [190/689]  eta: 0:13:05  lr: 0.000078  loss: 0.2968 (0.2971)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [200/689]  eta: 0:12:49  lr: 0.000078  loss: 0.2968 (0.2974)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [210/689]  eta: 0:12:34  lr: 0.000078  loss: 0.2950 (0.2971)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [220/689]  eta: 0:12:18  lr: 0.000078  loss: 0.2995 (0.2977)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [230/689]  eta: 0:12:02  lr: 0.000078  loss: 0.3022 (0.2984)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [240/689]  eta: 0:11:46  lr: 0.000078  loss: 0.3024 (0.2987)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [250/689]  eta: 0:11:31  lr: 0.000078  loss: 0.2988 (0.2986)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [260/689]  eta: 0:11:15  lr: 0.000078  loss: 0.2988 (0.2990)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [270/689]  eta: 0:10:59  lr: 0.000078  loss: 0.2830 (0.2988)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [280/689]  eta: 0:10:44  lr: 0.000078  loss: 0.2810 (0.2983)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [290/689]  eta: 0:10:28  lr: 0.000078  loss: 0.2909 (0.2984)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [300/689]  eta: 0:10:12  lr: 0.000078  loss: 0.2876 (0.2980)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [310/689]  eta: 0:09:56  lr: 0.000078  loss: 0.2830 (0.2975)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [320/689]  eta: 0:09:41  lr: 0.000078  loss: 0.2810 (0.2976)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [330/689]  eta: 0:09:25  lr: 0.000078  loss: 0.3095 (0.2983)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [340/689]  eta: 0:09:09  lr: 0.000078  loss: 0.3067 (0.2984)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [350/689]  eta: 0:08:54  lr: 0.000078  loss: 0.2965 (0.2987)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [360/689]  eta: 0:08:38  lr: 0.000078  loss: 0.2957 (0.2984)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [370/689]  eta: 0:08:22  lr: 0.000078  loss: 0.2814 (0.2981)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [380/689]  eta: 0:08:06  lr: 0.000078  loss: 0.2848 (0.2979)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [390/689]  eta: 0:07:51  lr: 0.000078  loss: 0.2956 (0.2978)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [400/689]  eta: 0:07:35  lr: 0.000078  loss: 0.2938 (0.2977)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [410/689]  eta: 0:07:19  lr: 0.000078  loss: 0.2780 (0.2975)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [420/689]  eta: 0:07:03  lr: 0.000078  loss: 0.2816 (0.2979)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [430/689]  eta: 0:06:48  lr: 0.000078  loss: 0.2972 (0.2982)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [440/689]  eta: 0:06:32  lr: 0.000078  loss: 0.2972 (0.2979)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [450/689]  eta: 0:06:16  lr: 0.000078  loss: 0.2875 (0.2977)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [460/689]  eta: 0:06:00  lr: 0.000078  loss: 0.2900 (0.2982)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [470/689]  eta: 0:05:45  lr: 0.000078  loss: 0.3001 (0.2987)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [480/689]  eta: 0:05:29  lr: 0.000078  loss: 0.2987 (0.2985)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [490/689]  eta: 0:05:13  lr: 0.000078  loss: 0.2853 (0.2987)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [500/689]  eta: 0:04:57  lr: 0.000078  loss: 0.2946 (0.2991)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [510/689]  eta: 0:04:42  lr: 0.000078  loss: 0.2956 (0.2992)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [520/689]  eta: 0:04:26  lr: 0.000078  loss: 0.2958 (0.2994)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [530/689]  eta: 0:04:10  lr: 0.000078  loss: 0.3006 (0.2994)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [540/689]  eta: 0:03:54  lr: 0.000078  loss: 0.2918 (0.2993)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [550/689]  eta: 0:03:39  lr: 0.000078  loss: 0.2918 (0.2994)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [560/689]  eta: 0:03:23  lr: 0.000078  loss: 0.2895 (0.2992)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [570/689]  eta: 0:03:07  lr: 0.000078  loss: 0.2813 (0.2990)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [580/689]  eta: 0:02:51  lr: 0.000078  loss: 0.2940 (0.2988)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [590/689]  eta: 0:02:36  lr: 0.000078  loss: 0.2897 (0.2992)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [600/689]  eta: 0:02:20  lr: 0.000078  loss: 0.3013 (0.2993)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [610/689]  eta: 0:02:04  lr: 0.000078  loss: 0.3042 (0.2994)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [620/689]  eta: 0:01:48  lr: 0.000078  loss: 0.2975 (0.2995)  time: 1.5774  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:300]  [630/689]  eta: 0:01:32  lr: 0.000078  loss: 0.2759 (0.2993)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [640/689]  eta: 0:01:17  lr: 0.000078  loss: 0.2945 (0.2994)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [650/689]  eta: 0:01:01  lr: 0.000078  loss: 0.3007 (0.2993)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [660/689]  eta: 0:00:45  lr: 0.000078  loss: 0.3025 (0.2993)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [670/689]  eta: 0:00:29  lr: 0.000078  loss: 0.2946 (0.2994)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [680/689]  eta: 0:00:14  lr: 0.000078  loss: 0.2946 (0.2993)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300]  [688/689]  eta: 0:00:01  lr: 0.000078  loss: 0.3049 (0.2994)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:300] Total time: 0:18:06 (1.5763 s / it)\n",
      "Averaged stats: lr: 0.000078  loss: 0.3049 (0.2994)\n",
      "Valid: [epoch:300]  [ 0/14]  eta: 0:00:14  loss: 0.2729 (0.2729)  time: 1.0322  data: 0.3982  max mem: 39763\n",
      "Valid: [epoch:300]  [13/14]  eta: 0:00:00  loss: 0.2804 (0.2918)  time: 0.1157  data: 0.0285  max mem: 39763\n",
      "Valid: [epoch:300] Total time: 0:00:01 (0.1250 s / it)\n",
      "Averaged stats: loss: 0.2804 (0.2918)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_300_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.292%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:301]  [  0/689]  eta: 0:12:26  lr: 0.000078  loss: 0.2860 (0.2860)  time: 1.0833  data: 0.6045  max mem: 39763\n",
      "Train: [epoch:301]  [ 10/689]  eta: 0:17:19  lr: 0.000078  loss: 0.2854 (0.2907)  time: 1.5303  data: 0.0550  max mem: 39763\n",
      "Train: [epoch:301]  [ 20/689]  eta: 0:17:18  lr: 0.000078  loss: 0.2838 (0.2886)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [ 30/689]  eta: 0:17:07  lr: 0.000078  loss: 0.2864 (0.2959)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [ 40/689]  eta: 0:16:54  lr: 0.000078  loss: 0.2861 (0.2918)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [ 50/689]  eta: 0:16:40  lr: 0.000078  loss: 0.2782 (0.2966)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [ 60/689]  eta: 0:16:26  lr: 0.000078  loss: 0.2932 (0.2957)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [ 70/689]  eta: 0:16:11  lr: 0.000078  loss: 0.2782 (0.2939)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [ 80/689]  eta: 0:15:56  lr: 0.000078  loss: 0.2782 (0.2930)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [ 90/689]  eta: 0:15:40  lr: 0.000078  loss: 0.2806 (0.2923)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [100/689]  eta: 0:15:25  lr: 0.000078  loss: 0.2863 (0.2937)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [110/689]  eta: 0:15:10  lr: 0.000078  loss: 0.3066 (0.2961)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [120/689]  eta: 0:14:54  lr: 0.000078  loss: 0.3066 (0.2981)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [130/689]  eta: 0:14:39  lr: 0.000078  loss: 0.3060 (0.2983)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [140/689]  eta: 0:14:23  lr: 0.000078  loss: 0.3060 (0.2991)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [150/689]  eta: 0:14:07  lr: 0.000078  loss: 0.3195 (0.3002)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [160/689]  eta: 0:13:52  lr: 0.000078  loss: 0.3098 (0.3005)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [170/689]  eta: 0:13:36  lr: 0.000078  loss: 0.3033 (0.3002)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [180/689]  eta: 0:13:21  lr: 0.000078  loss: 0.3066 (0.3019)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [190/689]  eta: 0:13:05  lr: 0.000078  loss: 0.3004 (0.3012)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [200/689]  eta: 0:12:49  lr: 0.000078  loss: 0.2898 (0.3012)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [210/689]  eta: 0:12:33  lr: 0.000078  loss: 0.2992 (0.3014)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [220/689]  eta: 0:12:18  lr: 0.000078  loss: 0.3017 (0.3015)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [230/689]  eta: 0:12:02  lr: 0.000078  loss: 0.3021 (0.3021)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [240/689]  eta: 0:11:46  lr: 0.000078  loss: 0.2978 (0.3017)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [250/689]  eta: 0:11:31  lr: 0.000078  loss: 0.2859 (0.3012)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [260/689]  eta: 0:11:15  lr: 0.000078  loss: 0.2837 (0.3006)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [270/689]  eta: 0:10:59  lr: 0.000078  loss: 0.2980 (0.3010)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [280/689]  eta: 0:10:44  lr: 0.000078  loss: 0.2966 (0.3006)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [290/689]  eta: 0:10:28  lr: 0.000078  loss: 0.2804 (0.3002)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [300/689]  eta: 0:10:12  lr: 0.000078  loss: 0.2732 (0.3000)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [310/689]  eta: 0:09:56  lr: 0.000078  loss: 0.2787 (0.2996)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [320/689]  eta: 0:09:41  lr: 0.000078  loss: 0.2931 (0.3004)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [330/689]  eta: 0:09:25  lr: 0.000078  loss: 0.3138 (0.3006)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [340/689]  eta: 0:09:09  lr: 0.000078  loss: 0.2836 (0.2999)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [350/689]  eta: 0:08:53  lr: 0.000078  loss: 0.2745 (0.2995)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [360/689]  eta: 0:08:38  lr: 0.000078  loss: 0.2884 (0.2994)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [370/689]  eta: 0:08:22  lr: 0.000078  loss: 0.2905 (0.2995)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [380/689]  eta: 0:08:06  lr: 0.000078  loss: 0.2865 (0.2993)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [390/689]  eta: 0:07:51  lr: 0.000078  loss: 0.2859 (0.2997)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [400/689]  eta: 0:07:35  lr: 0.000078  loss: 0.3010 (0.3000)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [410/689]  eta: 0:07:19  lr: 0.000078  loss: 0.2875 (0.2997)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [420/689]  eta: 0:07:03  lr: 0.000078  loss: 0.2876 (0.3000)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [430/689]  eta: 0:06:48  lr: 0.000078  loss: 0.3126 (0.3007)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [440/689]  eta: 0:06:32  lr: 0.000078  loss: 0.3166 (0.3010)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [450/689]  eta: 0:06:16  lr: 0.000078  loss: 0.2981 (0.3009)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [460/689]  eta: 0:06:00  lr: 0.000078  loss: 0.2973 (0.3011)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [470/689]  eta: 0:05:45  lr: 0.000078  loss: 0.3051 (0.3014)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [480/689]  eta: 0:05:29  lr: 0.000078  loss: 0.2963 (0.3012)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [490/689]  eta: 0:05:13  lr: 0.000078  loss: 0.2919 (0.3012)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [500/689]  eta: 0:04:57  lr: 0.000078  loss: 0.2872 (0.3012)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [510/689]  eta: 0:04:42  lr: 0.000078  loss: 0.2886 (0.3011)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [520/689]  eta: 0:04:26  lr: 0.000078  loss: 0.2929 (0.3011)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [530/689]  eta: 0:04:10  lr: 0.000078  loss: 0.2855 (0.3007)  time: 1.5773  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:301]  [540/689]  eta: 0:03:54  lr: 0.000078  loss: 0.2758 (0.3005)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [550/689]  eta: 0:03:39  lr: 0.000078  loss: 0.2792 (0.3004)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [560/689]  eta: 0:03:23  lr: 0.000078  loss: 0.2926 (0.3005)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [570/689]  eta: 0:03:07  lr: 0.000078  loss: 0.2919 (0.3004)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [580/689]  eta: 0:02:51  lr: 0.000078  loss: 0.2919 (0.3003)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [590/689]  eta: 0:02:36  lr: 0.000078  loss: 0.2814 (0.2999)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [600/689]  eta: 0:02:20  lr: 0.000078  loss: 0.2876 (0.3001)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [610/689]  eta: 0:02:04  lr: 0.000078  loss: 0.2992 (0.3000)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [620/689]  eta: 0:01:48  lr: 0.000078  loss: 0.2945 (0.2999)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [630/689]  eta: 0:01:33  lr: 0.000078  loss: 0.3081 (0.3002)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [640/689]  eta: 0:01:17  lr: 0.000078  loss: 0.3081 (0.3004)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [650/689]  eta: 0:01:01  lr: 0.000078  loss: 0.3029 (0.3003)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [660/689]  eta: 0:00:45  lr: 0.000078  loss: 0.2997 (0.3005)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [670/689]  eta: 0:00:29  lr: 0.000078  loss: 0.2955 (0.3004)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [680/689]  eta: 0:00:14  lr: 0.000078  loss: 0.2955 (0.3006)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301]  [688/689]  eta: 0:00:01  lr: 0.000078  loss: 0.3244 (0.3007)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:301] Total time: 0:18:06 (1.5767 s / it)\n",
      "Averaged stats: lr: 0.000078  loss: 0.3244 (0.3007)\n",
      "Valid: [epoch:301]  [ 0/14]  eta: 0:00:14  loss: 0.2574 (0.2574)  time: 1.0057  data: 0.3947  max mem: 39763\n",
      "Valid: [epoch:301]  [13/14]  eta: 0:00:00  loss: 0.2859 (0.2949)  time: 0.1137  data: 0.0282  max mem: 39763\n",
      "Valid: [epoch:301] Total time: 0:00:01 (0.1228 s / it)\n",
      "Averaged stats: loss: 0.2859 (0.2949)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_301_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.295%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:302]  [  0/689]  eta: 0:12:20  lr: 0.000078  loss: 0.2829 (0.2829)  time: 1.0745  data: 0.5970  max mem: 39763\n",
      "Train: [epoch:302]  [ 10/689]  eta: 0:17:19  lr: 0.000078  loss: 0.2816 (0.2794)  time: 1.5304  data: 0.0544  max mem: 39763\n",
      "Train: [epoch:302]  [ 20/689]  eta: 0:17:18  lr: 0.000078  loss: 0.2883 (0.2866)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [ 30/689]  eta: 0:17:08  lr: 0.000078  loss: 0.2897 (0.2947)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [ 40/689]  eta: 0:16:55  lr: 0.000078  loss: 0.2875 (0.2914)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [ 50/689]  eta: 0:16:41  lr: 0.000078  loss: 0.2967 (0.2938)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [ 60/689]  eta: 0:16:26  lr: 0.000078  loss: 0.2846 (0.2910)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [ 70/689]  eta: 0:16:11  lr: 0.000078  loss: 0.2878 (0.2962)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [ 80/689]  eta: 0:15:56  lr: 0.000078  loss: 0.2968 (0.2956)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [ 90/689]  eta: 0:15:41  lr: 0.000078  loss: 0.2825 (0.2945)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [100/689]  eta: 0:15:25  lr: 0.000078  loss: 0.2825 (0.2959)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [110/689]  eta: 0:15:10  lr: 0.000078  loss: 0.2992 (0.2976)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [120/689]  eta: 0:14:54  lr: 0.000078  loss: 0.3021 (0.2987)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [130/689]  eta: 0:14:39  lr: 0.000078  loss: 0.2845 (0.2979)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [140/689]  eta: 0:14:23  lr: 0.000078  loss: 0.2978 (0.2994)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [150/689]  eta: 0:14:07  lr: 0.000078  loss: 0.2991 (0.2991)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [160/689]  eta: 0:13:52  lr: 0.000078  loss: 0.2923 (0.2996)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [170/689]  eta: 0:13:36  lr: 0.000078  loss: 0.2928 (0.3008)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [180/689]  eta: 0:13:21  lr: 0.000078  loss: 0.2959 (0.3014)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [190/689]  eta: 0:13:05  lr: 0.000078  loss: 0.2989 (0.3018)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [200/689]  eta: 0:12:49  lr: 0.000078  loss: 0.2932 (0.3016)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [210/689]  eta: 0:12:34  lr: 0.000078  loss: 0.2798 (0.3004)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [220/689]  eta: 0:12:18  lr: 0.000078  loss: 0.2775 (0.3001)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [230/689]  eta: 0:12:02  lr: 0.000078  loss: 0.2849 (0.2999)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [240/689]  eta: 0:11:46  lr: 0.000078  loss: 0.2967 (0.3001)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [250/689]  eta: 0:11:31  lr: 0.000078  loss: 0.2982 (0.3001)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [260/689]  eta: 0:11:15  lr: 0.000078  loss: 0.2916 (0.2994)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [270/689]  eta: 0:10:59  lr: 0.000078  loss: 0.2834 (0.2996)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [280/689]  eta: 0:10:44  lr: 0.000078  loss: 0.2877 (0.2994)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [290/689]  eta: 0:10:28  lr: 0.000078  loss: 0.2739 (0.2984)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [300/689]  eta: 0:10:12  lr: 0.000078  loss: 0.2746 (0.2985)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [310/689]  eta: 0:09:56  lr: 0.000078  loss: 0.2870 (0.2986)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [320/689]  eta: 0:09:41  lr: 0.000078  loss: 0.2926 (0.2991)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [330/689]  eta: 0:09:25  lr: 0.000078  loss: 0.3071 (0.2994)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [340/689]  eta: 0:09:09  lr: 0.000078  loss: 0.3071 (0.2997)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [350/689]  eta: 0:08:54  lr: 0.000078  loss: 0.2913 (0.2993)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [360/689]  eta: 0:08:38  lr: 0.000078  loss: 0.2848 (0.2994)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [370/689]  eta: 0:08:22  lr: 0.000078  loss: 0.2933 (0.2996)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [380/689]  eta: 0:08:06  lr: 0.000078  loss: 0.2766 (0.2990)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [390/689]  eta: 0:07:51  lr: 0.000078  loss: 0.2971 (0.2994)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [400/689]  eta: 0:07:35  lr: 0.000078  loss: 0.3154 (0.2998)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [410/689]  eta: 0:07:19  lr: 0.000078  loss: 0.3023 (0.2997)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [420/689]  eta: 0:07:03  lr: 0.000078  loss: 0.3000 (0.2998)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [430/689]  eta: 0:06:48  lr: 0.000078  loss: 0.2894 (0.2996)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [440/689]  eta: 0:06:32  lr: 0.000078  loss: 0.2969 (0.2998)  time: 1.5775  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:302]  [450/689]  eta: 0:06:16  lr: 0.000078  loss: 0.3043 (0.3001)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [460/689]  eta: 0:06:00  lr: 0.000078  loss: 0.3182 (0.3008)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [470/689]  eta: 0:05:45  lr: 0.000078  loss: 0.3260 (0.3012)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [480/689]  eta: 0:05:29  lr: 0.000078  loss: 0.2884 (0.3011)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [490/689]  eta: 0:05:13  lr: 0.000078  loss: 0.2854 (0.3013)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [500/689]  eta: 0:04:57  lr: 0.000078  loss: 0.2928 (0.3014)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [510/689]  eta: 0:04:42  lr: 0.000078  loss: 0.2995 (0.3015)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [520/689]  eta: 0:04:26  lr: 0.000078  loss: 0.2984 (0.3014)  time: 1.5782  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [530/689]  eta: 0:04:10  lr: 0.000078  loss: 0.2880 (0.3012)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [540/689]  eta: 0:03:54  lr: 0.000078  loss: 0.2808 (0.3010)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [550/689]  eta: 0:03:39  lr: 0.000078  loss: 0.2822 (0.3008)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [560/689]  eta: 0:03:23  lr: 0.000078  loss: 0.2834 (0.3005)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [570/689]  eta: 0:03:07  lr: 0.000078  loss: 0.2894 (0.3003)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [580/689]  eta: 0:02:51  lr: 0.000078  loss: 0.2912 (0.3003)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [590/689]  eta: 0:02:36  lr: 0.000078  loss: 0.3133 (0.3006)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [600/689]  eta: 0:02:20  lr: 0.000078  loss: 0.3274 (0.3012)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [610/689]  eta: 0:02:04  lr: 0.000078  loss: 0.3170 (0.3013)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [620/689]  eta: 0:01:48  lr: 0.000078  loss: 0.3053 (0.3014)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [630/689]  eta: 0:01:33  lr: 0.000078  loss: 0.3030 (0.3014)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [640/689]  eta: 0:01:17  lr: 0.000078  loss: 0.2952 (0.3014)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [650/689]  eta: 0:01:01  lr: 0.000078  loss: 0.3023 (0.3015)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [660/689]  eta: 0:00:45  lr: 0.000078  loss: 0.2881 (0.3013)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [670/689]  eta: 0:00:29  lr: 0.000078  loss: 0.2881 (0.3013)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [680/689]  eta: 0:00:14  lr: 0.000078  loss: 0.2908 (0.3014)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302]  [688/689]  eta: 0:00:01  lr: 0.000078  loss: 0.2848 (0.3012)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:302] Total time: 0:18:06 (1.5767 s / it)\n",
      "Averaged stats: lr: 0.000078  loss: 0.2848 (0.3012)\n",
      "Valid: [epoch:302]  [ 0/14]  eta: 0:00:14  loss: 0.3444 (0.3444)  time: 1.0063  data: 0.3962  max mem: 39763\n",
      "Valid: [epoch:302]  [13/14]  eta: 0:00:00  loss: 0.2819 (0.3055)  time: 0.1138  data: 0.0284  max mem: 39763\n",
      "Valid: [epoch:302] Total time: 0:00:01 (0.1230 s / it)\n",
      "Averaged stats: loss: 0.2819 (0.3055)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_302_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.306%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:303]  [  0/689]  eta: 0:12:03  lr: 0.000078  loss: 0.3085 (0.3085)  time: 1.0496  data: 0.5712  max mem: 39763\n",
      "Train: [epoch:303]  [ 10/689]  eta: 0:17:16  lr: 0.000078  loss: 0.2866 (0.2941)  time: 1.5260  data: 0.0520  max mem: 39763\n",
      "Train: [epoch:303]  [ 20/689]  eta: 0:17:16  lr: 0.000078  loss: 0.2822 (0.2923)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [ 30/689]  eta: 0:17:06  lr: 0.000078  loss: 0.2862 (0.2965)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [ 40/689]  eta: 0:16:53  lr: 0.000078  loss: 0.2968 (0.2961)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [ 50/689]  eta: 0:16:40  lr: 0.000078  loss: 0.2955 (0.2943)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [ 60/689]  eta: 0:16:25  lr: 0.000078  loss: 0.2898 (0.2948)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [ 70/689]  eta: 0:16:10  lr: 0.000078  loss: 0.2876 (0.2950)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [ 80/689]  eta: 0:15:55  lr: 0.000078  loss: 0.2905 (0.2951)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [ 90/689]  eta: 0:15:40  lr: 0.000078  loss: 0.2905 (0.2967)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [100/689]  eta: 0:15:24  lr: 0.000078  loss: 0.3054 (0.2972)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [110/689]  eta: 0:15:09  lr: 0.000078  loss: 0.3054 (0.2981)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [120/689]  eta: 0:14:54  lr: 0.000078  loss: 0.3050 (0.3003)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [130/689]  eta: 0:14:38  lr: 0.000078  loss: 0.3038 (0.3012)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [140/689]  eta: 0:14:23  lr: 0.000078  loss: 0.2937 (0.3020)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [150/689]  eta: 0:14:07  lr: 0.000078  loss: 0.3040 (0.3024)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [160/689]  eta: 0:13:51  lr: 0.000078  loss: 0.3006 (0.3015)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [170/689]  eta: 0:13:36  lr: 0.000078  loss: 0.2919 (0.3020)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [180/689]  eta: 0:13:20  lr: 0.000078  loss: 0.2987 (0.3023)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [190/689]  eta: 0:13:04  lr: 0.000078  loss: 0.2961 (0.3024)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [200/689]  eta: 0:12:49  lr: 0.000078  loss: 0.3049 (0.3026)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [210/689]  eta: 0:12:33  lr: 0.000078  loss: 0.2880 (0.3017)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [220/689]  eta: 0:12:18  lr: 0.000078  loss: 0.2977 (0.3023)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [230/689]  eta: 0:12:02  lr: 0.000078  loss: 0.3020 (0.3019)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [240/689]  eta: 0:11:46  lr: 0.000078  loss: 0.2902 (0.3030)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [250/689]  eta: 0:11:31  lr: 0.000078  loss: 0.2966 (0.3030)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [260/689]  eta: 0:11:15  lr: 0.000078  loss: 0.2937 (0.3026)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [270/689]  eta: 0:10:59  lr: 0.000078  loss: 0.2738 (0.3018)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [280/689]  eta: 0:10:44  lr: 0.000078  loss: 0.2744 (0.3014)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [290/689]  eta: 0:10:28  lr: 0.000078  loss: 0.2841 (0.3009)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [300/689]  eta: 0:10:12  lr: 0.000078  loss: 0.2819 (0.3001)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [310/689]  eta: 0:09:56  lr: 0.000078  loss: 0.2899 (0.3002)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [320/689]  eta: 0:09:41  lr: 0.000078  loss: 0.3030 (0.3004)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [330/689]  eta: 0:09:25  lr: 0.000078  loss: 0.2979 (0.3010)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [340/689]  eta: 0:09:09  lr: 0.000078  loss: 0.2979 (0.3008)  time: 1.5781  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [350/689]  eta: 0:08:54  lr: 0.000078  loss: 0.2962 (0.3009)  time: 1.5778  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:303]  [360/689]  eta: 0:08:38  lr: 0.000078  loss: 0.2969 (0.3009)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [370/689]  eta: 0:08:22  lr: 0.000078  loss: 0.2981 (0.3012)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [380/689]  eta: 0:08:06  lr: 0.000078  loss: 0.2917 (0.3010)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [390/689]  eta: 0:07:51  lr: 0.000078  loss: 0.2828 (0.3012)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [400/689]  eta: 0:07:35  lr: 0.000078  loss: 0.2881 (0.3014)  time: 1.5784  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [410/689]  eta: 0:07:19  lr: 0.000078  loss: 0.2881 (0.3014)  time: 1.5783  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [420/689]  eta: 0:07:03  lr: 0.000078  loss: 0.3031 (0.3015)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [430/689]  eta: 0:06:48  lr: 0.000078  loss: 0.2936 (0.3012)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [440/689]  eta: 0:06:32  lr: 0.000078  loss: 0.2936 (0.3015)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [450/689]  eta: 0:06:16  lr: 0.000078  loss: 0.2950 (0.3012)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [460/689]  eta: 0:06:00  lr: 0.000078  loss: 0.2950 (0.3017)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [470/689]  eta: 0:05:45  lr: 0.000078  loss: 0.2984 (0.3019)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [480/689]  eta: 0:05:29  lr: 0.000078  loss: 0.2895 (0.3020)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [490/689]  eta: 0:05:13  lr: 0.000078  loss: 0.2986 (0.3021)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [500/689]  eta: 0:04:57  lr: 0.000078  loss: 0.2986 (0.3022)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [510/689]  eta: 0:04:42  lr: 0.000078  loss: 0.3076 (0.3022)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [520/689]  eta: 0:04:26  lr: 0.000078  loss: 0.2883 (0.3019)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [530/689]  eta: 0:04:10  lr: 0.000078  loss: 0.2912 (0.3020)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [540/689]  eta: 0:03:54  lr: 0.000078  loss: 0.2939 (0.3020)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [550/689]  eta: 0:03:39  lr: 0.000078  loss: 0.2777 (0.3016)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [560/689]  eta: 0:03:23  lr: 0.000078  loss: 0.2794 (0.3017)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [570/689]  eta: 0:03:07  lr: 0.000078  loss: 0.2991 (0.3017)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [580/689]  eta: 0:02:51  lr: 0.000078  loss: 0.2951 (0.3016)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [590/689]  eta: 0:02:36  lr: 0.000078  loss: 0.2898 (0.3015)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [600/689]  eta: 0:02:20  lr: 0.000078  loss: 0.2863 (0.3013)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [610/689]  eta: 0:02:04  lr: 0.000078  loss: 0.2817 (0.3011)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [620/689]  eta: 0:01:48  lr: 0.000078  loss: 0.2939 (0.3013)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [630/689]  eta: 0:01:32  lr: 0.000078  loss: 0.2948 (0.3013)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [640/689]  eta: 0:01:17  lr: 0.000078  loss: 0.2985 (0.3015)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [650/689]  eta: 0:01:01  lr: 0.000078  loss: 0.2985 (0.3015)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [660/689]  eta: 0:00:45  lr: 0.000078  loss: 0.2988 (0.3018)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [670/689]  eta: 0:00:29  lr: 0.000078  loss: 0.3137 (0.3018)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [680/689]  eta: 0:00:14  lr: 0.000078  loss: 0.2981 (0.3019)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303]  [688/689]  eta: 0:00:01  lr: 0.000078  loss: 0.3077 (0.3020)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:303] Total time: 0:18:06 (1.5764 s / it)\n",
      "Averaged stats: lr: 0.000078  loss: 0.3077 (0.3020)\n",
      "Valid: [epoch:303]  [ 0/14]  eta: 0:00:14  loss: 0.3551 (0.3551)  time: 1.0056  data: 0.3848  max mem: 39763\n",
      "Valid: [epoch:303]  [13/14]  eta: 0:00:00  loss: 0.2830 (0.2972)  time: 0.1137  data: 0.0275  max mem: 39763\n",
      "Valid: [epoch:303] Total time: 0:00:01 (0.1210 s / it)\n",
      "Averaged stats: loss: 0.2830 (0.2972)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_303_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.297%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:304]  [  0/689]  eta: 0:11:46  lr: 0.000077  loss: 0.2736 (0.2736)  time: 1.0251  data: 0.5553  max mem: 39763\n",
      "Train: [epoch:304]  [ 10/689]  eta: 0:17:15  lr: 0.000077  loss: 0.2778 (0.2846)  time: 1.5253  data: 0.0506  max mem: 39763\n",
      "Train: [epoch:304]  [ 20/689]  eta: 0:17:16  lr: 0.000077  loss: 0.2778 (0.2905)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [ 30/689]  eta: 0:17:06  lr: 0.000077  loss: 0.3113 (0.3031)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [ 40/689]  eta: 0:16:53  lr: 0.000077  loss: 0.3124 (0.3019)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [ 50/689]  eta: 0:16:39  lr: 0.000077  loss: 0.2786 (0.2984)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [ 60/689]  eta: 0:16:25  lr: 0.000077  loss: 0.2734 (0.2985)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [ 70/689]  eta: 0:16:10  lr: 0.000077  loss: 0.2746 (0.2984)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [ 80/689]  eta: 0:15:55  lr: 0.000077  loss: 0.2750 (0.2954)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [ 90/689]  eta: 0:15:40  lr: 0.000077  loss: 0.2815 (0.2968)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [100/689]  eta: 0:15:24  lr: 0.000077  loss: 0.3134 (0.3001)  time: 1.5753  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [110/689]  eta: 0:15:09  lr: 0.000077  loss: 0.2996 (0.2997)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [120/689]  eta: 0:14:53  lr: 0.000077  loss: 0.2984 (0.3013)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [130/689]  eta: 0:14:38  lr: 0.000077  loss: 0.2900 (0.3014)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [140/689]  eta: 0:14:22  lr: 0.000077  loss: 0.2891 (0.3013)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [150/689]  eta: 0:14:07  lr: 0.000077  loss: 0.2985 (0.3016)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [160/689]  eta: 0:13:51  lr: 0.000077  loss: 0.2837 (0.3004)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [170/689]  eta: 0:13:36  lr: 0.000077  loss: 0.2787 (0.3007)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [180/689]  eta: 0:13:20  lr: 0.000077  loss: 0.2870 (0.3008)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [190/689]  eta: 0:13:04  lr: 0.000077  loss: 0.2870 (0.3009)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [200/689]  eta: 0:12:49  lr: 0.000077  loss: 0.2975 (0.3014)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [210/689]  eta: 0:12:33  lr: 0.000077  loss: 0.2965 (0.3009)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [220/689]  eta: 0:12:17  lr: 0.000077  loss: 0.3001 (0.3014)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [230/689]  eta: 0:12:02  lr: 0.000077  loss: 0.3032 (0.3017)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [240/689]  eta: 0:11:46  lr: 0.000077  loss: 0.2999 (0.3024)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [250/689]  eta: 0:11:30  lr: 0.000077  loss: 0.2962 (0.3021)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [260/689]  eta: 0:11:15  lr: 0.000077  loss: 0.2990 (0.3022)  time: 1.5774  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:304]  [270/689]  eta: 0:10:59  lr: 0.000077  loss: 0.2991 (0.3026)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [280/689]  eta: 0:10:43  lr: 0.000077  loss: 0.2948 (0.3027)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [290/689]  eta: 0:10:28  lr: 0.000077  loss: 0.2906 (0.3021)  time: 1.5780  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [300/689]  eta: 0:10:12  lr: 0.000077  loss: 0.2875 (0.3020)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [310/689]  eta: 0:09:56  lr: 0.000077  loss: 0.2872 (0.3011)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [320/689]  eta: 0:09:41  lr: 0.000077  loss: 0.2935 (0.3017)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [330/689]  eta: 0:09:25  lr: 0.000077  loss: 0.3037 (0.3017)  time: 1.5775  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [340/689]  eta: 0:09:09  lr: 0.000077  loss: 0.2827 (0.3015)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [350/689]  eta: 0:08:53  lr: 0.000077  loss: 0.2844 (0.3016)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [360/689]  eta: 0:08:38  lr: 0.000077  loss: 0.2964 (0.3019)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [370/689]  eta: 0:08:22  lr: 0.000077  loss: 0.2882 (0.3013)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [380/689]  eta: 0:08:06  lr: 0.000077  loss: 0.2832 (0.3010)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [390/689]  eta: 0:07:50  lr: 0.000077  loss: 0.3012 (0.3016)  time: 1.5779  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [400/689]  eta: 0:07:35  lr: 0.000077  loss: 0.3068 (0.3014)  time: 1.5778  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [410/689]  eta: 0:07:19  lr: 0.000077  loss: 0.2903 (0.3014)  time: 1.5777  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [420/689]  eta: 0:07:03  lr: 0.000077  loss: 0.3037 (0.3018)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [430/689]  eta: 0:06:48  lr: 0.000077  loss: 0.3042 (0.3018)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [440/689]  eta: 0:06:32  lr: 0.000077  loss: 0.3094 (0.3023)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [450/689]  eta: 0:06:16  lr: 0.000077  loss: 0.2956 (0.3019)  time: 1.5773  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [460/689]  eta: 0:06:00  lr: 0.000077  loss: 0.2892 (0.3018)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [470/689]  eta: 0:05:45  lr: 0.000077  loss: 0.2885 (0.3017)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [480/689]  eta: 0:05:29  lr: 0.000077  loss: 0.2853 (0.3015)  time: 1.5767  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [490/689]  eta: 0:05:13  lr: 0.000077  loss: 0.2895 (0.3017)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [500/689]  eta: 0:04:57  lr: 0.000077  loss: 0.2947 (0.3017)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [510/689]  eta: 0:04:42  lr: 0.000077  loss: 0.2947 (0.3017)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [520/689]  eta: 0:04:26  lr: 0.000077  loss: 0.2945 (0.3015)  time: 1.5769  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [530/689]  eta: 0:04:10  lr: 0.000077  loss: 0.2855 (0.3014)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [540/689]  eta: 0:03:54  lr: 0.000077  loss: 0.2905 (0.3014)  time: 1.5770  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [550/689]  eta: 0:03:39  lr: 0.000077  loss: 0.2922 (0.3015)  time: 1.5771  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [560/689]  eta: 0:03:23  lr: 0.000077  loss: 0.2921 (0.3015)  time: 1.5774  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [570/689]  eta: 0:03:07  lr: 0.000077  loss: 0.3083 (0.3019)  time: 1.5776  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [580/689]  eta: 0:02:51  lr: 0.000077  loss: 0.3065 (0.3020)  time: 1.5772  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [590/689]  eta: 0:02:36  lr: 0.000077  loss: 0.3114 (0.3023)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [600/689]  eta: 0:02:20  lr: 0.000077  loss: 0.3108 (0.3024)  time: 1.5768  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [610/689]  eta: 0:02:04  lr: 0.000077  loss: 0.3041 (0.3026)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [620/689]  eta: 0:01:48  lr: 0.000077  loss: 0.2894 (0.3025)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [630/689]  eta: 0:01:32  lr: 0.000077  loss: 0.2894 (0.3025)  time: 1.5765  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [640/689]  eta: 0:01:17  lr: 0.000077  loss: 0.2974 (0.3025)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [650/689]  eta: 0:01:01  lr: 0.000077  loss: 0.2963 (0.3025)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [660/689]  eta: 0:00:45  lr: 0.000077  loss: 0.2925 (0.3024)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [670/689]  eta: 0:00:29  lr: 0.000077  loss: 0.2996 (0.3026)  time: 1.5750  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [680/689]  eta: 0:00:14  lr: 0.000077  loss: 0.3121 (0.3028)  time: 1.5756  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304]  [688/689]  eta: 0:00:01  lr: 0.000077  loss: 0.3077 (0.3027)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:304] Total time: 0:18:05 (1.5760 s / it)\n",
      "Averaged stats: lr: 0.000077  loss: 0.3077 (0.3027)\n",
      "Valid: [epoch:304]  [ 0/14]  eta: 0:00:14  loss: 0.2754 (0.2754)  time: 1.0288  data: 0.3616  max mem: 39763\n",
      "Valid: [epoch:304]  [13/14]  eta: 0:00:00  loss: 0.2831 (0.2927)  time: 0.1153  data: 0.0259  max mem: 39763\n",
      "Valid: [epoch:304] Total time: 0:00:01 (0.1245 s / it)\n",
      "Averaged stats: loss: 0.2831 (0.2927)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/epoch_304_input_n_20.png\n",
      "loss of the network on the 14 valid images: 0.293%\n",
      "Min loss: 0.000\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:305]  [  0/689]  eta: 0:12:41  lr: 0.000077  loss: 0.2378 (0.2378)  time: 1.1052  data: 0.6300  max mem: 39763\n",
      "Train: [epoch:305]  [ 10/689]  eta: 0:17:18  lr: 0.000077  loss: 0.2950 (0.2980)  time: 1.5298  data: 0.0573  max mem: 39763\n",
      "Train: [epoch:305]  [ 20/689]  eta: 0:17:17  lr: 0.000077  loss: 0.3025 (0.3008)  time: 1.5731  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [ 30/689]  eta: 0:17:07  lr: 0.000077  loss: 0.3044 (0.3154)  time: 1.5744  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [ 40/689]  eta: 0:16:53  lr: 0.000077  loss: 0.2971 (0.3062)  time: 1.5745  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [ 50/689]  eta: 0:16:39  lr: 0.000077  loss: 0.2790 (0.3024)  time: 1.5743  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [ 60/689]  eta: 0:16:25  lr: 0.000077  loss: 0.2923 (0.3028)  time: 1.5747  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [ 70/689]  eta: 0:16:10  lr: 0.000077  loss: 0.3046 (0.3042)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [ 80/689]  eta: 0:15:55  lr: 0.000077  loss: 0.3000 (0.3049)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [ 90/689]  eta: 0:15:40  lr: 0.000077  loss: 0.3066 (0.3072)  time: 1.5752  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [100/689]  eta: 0:15:24  lr: 0.000077  loss: 0.3076 (0.3067)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [110/689]  eta: 0:15:09  lr: 0.000077  loss: 0.3067 (0.3072)  time: 1.5748  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [120/689]  eta: 0:14:53  lr: 0.000077  loss: 0.3033 (0.3074)  time: 1.5754  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [130/689]  eta: 0:14:38  lr: 0.000077  loss: 0.3002 (0.3080)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [140/689]  eta: 0:14:22  lr: 0.000077  loss: 0.3002 (0.3079)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [150/689]  eta: 0:14:07  lr: 0.000077  loss: 0.3025 (0.3077)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [160/689]  eta: 0:13:51  lr: 0.000077  loss: 0.2874 (0.3067)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [170/689]  eta: 0:13:35  lr: 0.000077  loss: 0.2861 (0.3055)  time: 1.5756  data: 0.0001  max mem: 39763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:305]  [180/689]  eta: 0:13:20  lr: 0.000077  loss: 0.2861 (0.3056)  time: 1.5755  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [190/689]  eta: 0:13:04  lr: 0.000077  loss: 0.2986 (0.3066)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [200/689]  eta: 0:12:49  lr: 0.000077  loss: 0.2893 (0.3060)  time: 1.5762  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [210/689]  eta: 0:12:33  lr: 0.000077  loss: 0.2866 (0.3061)  time: 1.5758  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [220/689]  eta: 0:12:17  lr: 0.000077  loss: 0.3236 (0.3071)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [230/689]  eta: 0:12:02  lr: 0.000077  loss: 0.3274 (0.3074)  time: 1.5761  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [240/689]  eta: 0:11:46  lr: 0.000077  loss: 0.3165 (0.3071)  time: 1.5764  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [250/689]  eta: 0:11:30  lr: 0.000077  loss: 0.2964 (0.3067)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [260/689]  eta: 0:11:15  lr: 0.000077  loss: 0.3013 (0.3068)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [270/689]  eta: 0:10:59  lr: 0.000077  loss: 0.3127 (0.3066)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [280/689]  eta: 0:10:43  lr: 0.000077  loss: 0.2905 (0.3063)  time: 1.5759  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [290/689]  eta: 0:10:27  lr: 0.000077  loss: 0.2905 (0.3064)  time: 1.5757  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [300/689]  eta: 0:10:12  lr: 0.000077  loss: 0.2908 (0.3057)  time: 1.5760  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [310/689]  eta: 0:09:56  lr: 0.000077  loss: 0.2890 (0.3055)  time: 1.5763  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [320/689]  eta: 0:09:40  lr: 0.000077  loss: 0.3062 (0.3062)  time: 1.5766  data: 0.0001  max mem: 39763\n",
      "Train: [epoch:305]  [330/689]  eta: 0:09:25  lr: 0.000077  loss: 0.3062 (0.3060)  time: 1.5765  data: 0.0001  max mem: 39763\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--batch-size 10 \\\n",
    "--epochs 1000 \\\n",
    "--lr_scheduler \"lambda\" \\\n",
    "--lr 1e-4 \\\n",
    "--data-set 'Sinogram_DCM' \\\n",
    "--model-name 'HF_ConvMixer' \\\n",
    "--criterion 'Change L2 L1 Loss' \\\n",
    "--output_dir '/workspace/sunggu/4.Dose_img2img/model/[Ours]HF_ConvMixer' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/[Ours]HF_ConvMixer/low2high/' \\\n",
    "--validate-every 2 \\\n",
    "--num_workers 4 \\\n",
    "--criterion_mode 'not balance' \\\n",
    "--multiple_GT \"False\" \\\n",
    "--patch_training \"False\" \\\n",
    "--multi-gpu-mode 'Single' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "import functools\n",
    "import pydicom\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore') \n",
    "\n",
    "\n",
    "def list_sort_nicely(l):   \n",
    "    def tryint(s):        \n",
    "        try:            \n",
    "            return int(s)        \n",
    "        except:            \n",
    "            return s\n",
    "        \n",
    "    def alphanum_key(s):\n",
    "        return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "    l.sort(key=alphanum_key)    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_20_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/20/*/*/*.dcm')) + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/20/*/*/*.dcm'))\n",
    "n_100_imgs  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/X/*/*/*.dcm'))  + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/X/*/*/*.dcm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixels_hu(path):\n",
    "    # pydicom version...!\n",
    "    # referred from https://www.kaggle.com/gzuidhof/full-preprocessing-tutorial\n",
    "    # ref: pydicom.pixel_data_handlers.util.apply_modality_lut\n",
    "    # '''\n",
    "    # Awesome pydicom lut fuction...!\n",
    "    # ds  = pydicom.dcmread(fname)\n",
    "    # arr = ds.pixel_array\n",
    "    # hu  = apply_modality_lut(arr, ds)\n",
    "    # '''\n",
    "    dcm_image = pydicom.read_file(path)\n",
    "    image = dcm_image.pixel_array\n",
    "    image = image.astype(np.int16)\n",
    "    image[image == -2000] = 0\n",
    "\n",
    "    intercept = dcm_image.RescaleIntercept\n",
    "    slope     = dcm_image.RescaleSlope\n",
    "\n",
    "    if slope != 1:\n",
    "        image = slope * image.astype(np.float64)\n",
    "        image = image.astype(np.int16)\n",
    "\n",
    "    image += np.int16(intercept)\n",
    "    # print(image.shape) # (512, 512)\n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "def dicom_normalize(image, MIN_HU=-1024.0, MAX_HU=3071.0):   # I already check the max value is 3071.0\n",
    "   image = (image - MIN_HU) / (MAX_HU - MIN_HU)   # Range  0.0 ~ 1.0\n",
    "#    image = (image - 0.5) / 0.5                  # Range -1.0 ~ 1.0   @ We do not use -1~1 range becuase there is no Tanh act.\n",
    "   return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from monai.transforms import *\n",
    "from monai.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_20_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/20/*/*/*.dcm')) + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/20/*/*/*.dcm'))\n",
    "n_100_imgs  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Train/*/X/*/*/*.dcm'))  + list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_DCM/Valid/*/X/*/*/*.dcm'))\n",
    "\n",
    "files = [{\"n_20\": n_20, \"n_100\": n_100} for n_20, n_100 in zip(n_20_imgs, n_100_imgs)]            \n",
    "print(\"Train [Total]  number = \", len(n_20_imgs))\n",
    "\n",
    "# CT에 맞는 Augmentation\n",
    "\n",
    "transforms = Compose(\n",
    "    [\n",
    "        Lambdad(keys=[\"n_20\", \"n_100\"], func=get_pixels_hu),\n",
    "        Lambdad(keys=[\"n_20\", \"n_100\"], func=dicom_normalize),\n",
    "        AddChanneld(keys=[\"n_20\", \"n_100\"]),                 \n",
    "\n",
    "        # Crop  \n",
    "        # RandWeightedCropd(keys=[\"image\"], w_key=[\"image\"], spatial_size=(512,512,1), num_samples=1),\n",
    "        # RandSpatialCropd(keys=[\"image\"], roi_size=(512, 512), random_size=False, random_center=True),\n",
    "        # RandSpatialCropd(keys=[\"image\"], roi_size=(512,512,3), random_size=False, random_center=True),\n",
    "#         RandSpatialCropSamplesd(keys=[\"n_20\", \"n_100\"], roi_size=(64, 64), num_samples=8, random_center=True, random_size=False, meta_keys=None, allow_missing_keys=False), \n",
    "            # patch training, next(iter(loader)) output : list로 sample 만큼,,, 그 List 안에 (B, C, H, W)\n",
    "\n",
    "        # (45 degree rotation, vertical & horizontal flip & scaling)\n",
    "#         RandFlipd(keys=[\"n_20\", \"n_100\"], prob=0.1, spatial_axis=[0, 1], allow_missing_keys=False),\n",
    "#         RandRotated(keys=[\"n_20\", \"n_100\"], prob=0.1, range_x=np.pi/4, range_y=np.pi/4, range_z=0.0, keep_size=True, align_corners=False, allow_missing_keys=False),\n",
    "#         RandZoomd(keys=[\"n_20\", \"n_100\"], prob=0.1, min_zoom=0.5, max_zoom=2.0, align_corners=None, keep_size=True, allow_missing_keys=False),\n",
    "        ToTensord(keys=[\"n_20\", \"n_100\"]),\n",
    "    ]\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Dataset(data=files, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_denormalize(image, MIN_HU=-1024.0, MAX_HU=3071.0):\n",
    "    # image = (image - 0.5) / 0.5           # Range -1.0 ~ 1.0   @ We do not use -1~1 range becuase there is no Tanh act.\n",
    "    image = (MAX_HU - MIN_HU)*image + MIN_HU\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(dicom_denormalize(t[470]['n_20'].squeeze()), 'gray', vmin=0, vmax=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dicom_denormalize(t[470]['n_100'].squeeze()), 'gray', vmin=0, vmax=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(t[470]['n_100'], t[470]['n_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCoshLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "#         return torch.mean(torch.log(torch.cosh(torch.pow(ey_t, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LogCoshLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a(t[470]['n_100'], t[470]['n_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.1081e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a(t[470]['n_100'], t[470]['n_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a(t[470]['n_100'], t[470]['n_100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1000*4.1081e-06 - 1000*2.1081e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for i in range(len(lines)):\n",
    "        exec('log_list.append('+lines[i] + ')')\n",
    "    return  log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_list = read_log(path = '/workspace/sunggu/4.Dose_img2img/model/[Privious]ED_CNN/log.txt')\n",
    "\n",
    "train_lr   = [ log_list[i]['train_lr'] for i in range(len(log_list)) ]\n",
    "train_loss = [ log_list[i]['train_loss'] for i in range(len(log_list)) ]\n",
    "valid_loss = [ log_list[i]['valid_loss'] for i in range(len(log_list)) ]\n",
    "epoch      = [ log_list[i]['epoch'] for i in range(len(log_list)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(train_loss)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(valid_loss)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(np.argsort(valid_loss)[:10]) & set(np.argsort(train_loss)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py \\\n",
    "--training-mode 'sinogram' \\\n",
    "--data-set 'TEST_Sinogram_DCM' \\\n",
    "--model-name 'ED_CNN' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Test/png/[Privious]ED_CNN/epoch_999/' \\\n",
    "--num_workers 4 \\\n",
    "--pin-mem \\\n",
    "--range-minus1-plus1 'False' \\\n",
    "--teacher_forcing \"False\" \\\n",
    "--resume '/workspace/sunggu/4.Dose_img2img/model/[Privious]ED_CNN/epoch_999_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 978 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Original === \n",
    "PSNR avg: 54.4628 \n",
    "SSIM avg: 0.9956 \n",
    "RMSE avg: 7.9607\n",
    "\n",
    "\n",
    "Predictions === \n",
    "PSNR avg: 57.6190 \n",
    "SSIM avg: 0.9980 \n",
    "RMSE avg: 5.5423\n",
    "***********************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
