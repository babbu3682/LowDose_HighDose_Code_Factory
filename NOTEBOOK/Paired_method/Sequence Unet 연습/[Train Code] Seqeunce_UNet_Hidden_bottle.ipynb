{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pretrainedmodels==0.7.4\n",
    "# !pip install efficientnet-pytorch==0.6.3\n",
    "# !pip install timm==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CUDA 11.1\n",
    "# !pip install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE -> MAE Loss 꿀팁!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 26 06:17:05 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "| 37%   60C    P2   120W / 260W |  46678MiB / 48601MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     Off  | 00000000:1C:00.0 Off |                  Off |\n",
      "| 34%   53C    P2    64W / 260W |  41256MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 8000     Off  | 00000000:1D:00.0 Off |                  Off |\n",
      "| 34%   50C    P2    75W / 260W |  40412MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     Off  | 00000000:1E:00.0 Off |                  Off |\n",
      "| 36%   59C    P2   229W / 260W |  47607MiB / 48601MiB |     95%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Quadro RTX 8000     Off  | 00000000:3D:00.0 Off |                  Off |\n",
      "| 33%   53C    P2    64W / 260W |  40410MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Quadro RTX 8000     Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "| 33%   30C    P8     9W / 260W |  42318MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Quadro RTX 8000     Off  | 00000000:40:00.0 Off |                  Off |\n",
      "| 34%   51C    P2    77W / 260W |  40418MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Quadro RTX 8000     Off  | 00000000:41:00.0 Off |                  Off |\n",
      "| 32%   34C    P0    66W / 260W |      0MiB / 48601MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sunggu/4.Dose_img2img/scripts study\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/sunggu/4.Dose_img2img/scripts study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 갯수 =  64\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(\"CPU 갯수 = \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seqeunce_UNet_Hidden_bottle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "***********************************************\n",
      "Dataset Name:  Sinogram\n",
      "---------- Model ----------\n",
      "Resume From:  \n",
      "Output To:  /workspace/sunggu/4.Dose_img2img/model/Seqeunce_UNet_Hidden_bottle\n",
      "Save   To:  /workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0002\n",
      "Weight Decay:  0.05\n",
      "Batchsize:  4\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  6899\n",
      "Valid [Total]  number =  845\n",
      "Creating model: Seqeunce_UNet_Hidden_bottle\n",
      "Number of Learnable Params: 34916329\n",
      "Start training for 1000 epochs\n",
      "Train: [epoch:0]  [   0/1724]  eta: 5:07:32  lr: 0.000000  loss: 1779.3397 (1779.3397)  loss_n_40: 4.4679 (4.4679)  loss_n_60: 4.8006 (4.8006)  loss_n_80: 4.9384 (4.9384)  loss_n_100: 5.0105 (5.0105)  triple_100: 445.1714 (445.1714)  triple_80: 448.1945 (448.1945)  triple_60: 445.2329 (445.2329)  triple_40: 421.5234 (421.5234)  time: 10.7031  data: 3.2270  max mem: 46062\n",
      "Train: [epoch:0]  [  10/1724]  eta: 2:12:57  lr: 0.000000  loss: 1981.0061 (1990.2950)  loss_n_40: 4.2077 (4.1781)  loss_n_60: 4.6823 (4.5308)  loss_n_80: 4.8395 (4.6720)  loss_n_100: 4.8605 (4.7204)  triple_100: 496.1042 (498.8471)  triple_80: 498.7235 (501.4211)  triple_60: 496.0863 (498.2350)  triple_40: 471.1163 (473.6905)  time: 4.6542  data: 0.2935  max mem: 46473\n",
      "Train: [epoch:0]  [  20/1724]  eta: 2:02:04  lr: 0.000000  loss: 2012.8818 (2006.5858)  loss_n_40: 4.1949 (4.2136)  loss_n_60: 4.4789 (4.5586)  loss_n_80: 4.6083 (4.6934)  loss_n_100: 4.6626 (4.7469)  triple_100: 503.7203 (502.9186)  triple_80: 506.4340 (505.4752)  triple_60: 504.0016 (502.3111)  triple_40: 478.9103 (477.6684)  time: 3.9779  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:0]  [  30/1724]  eta: 1:57:58  lr: 0.000000  loss: 2012.8818 (2013.5157)  loss_n_40: 4.1948 (4.2084)  loss_n_60: 4.5368 (4.5779)  loss_n_80: 4.7141 (4.7157)  loss_n_100: 4.7587 (4.7617)  triple_100: 504.5898 (504.6488)  triple_80: 507.4659 (507.2994)  triple_60: 504.0016 (504.1138)  triple_40: 478.9103 (479.1900)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [  40/1724]  eta: 1:55:35  lr: 0.000000  loss: 2003.0712 (2011.1553)  loss_n_40: 4.2015 (4.2140)  loss_n_60: 4.6205 (4.5859)  loss_n_80: 4.7690 (4.7233)  loss_n_100: 4.8129 (4.7700)  triple_100: 502.4359 (504.0663)  triple_80: 505.6365 (506.7488)  triple_60: 501.7852 (503.5310)  triple_40: 474.5122 (478.5161)  time: 3.9297  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:0]  [  50/1724]  eta: 1:53:53  lr: 0.000000  loss: 2003.0712 (2011.4306)  loss_n_40: 4.2341 (4.2090)  loss_n_60: 4.6122 (4.5788)  loss_n_80: 4.7690 (4.7189)  loss_n_100: 4.8129 (4.7664)  triple_100: 502.7119 (504.1675)  triple_80: 505.6580 (506.8138)  triple_60: 501.7852 (503.5832)  triple_40: 474.5122 (478.5930)  time: 3.9330  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [  60/1724]  eta: 1:52:31  lr: 0.000000  loss: 2043.1761 (2011.5769)  loss_n_40: 4.1808 (4.2162)  loss_n_60: 4.4940 (4.5796)  loss_n_80: 4.6514 (4.7177)  loss_n_100: 4.6953 (4.7659)  triple_100: 511.0349 (504.1474)  triple_80: 513.7647 (506.7963)  triple_60: 511.5500 (503.5878)  triple_40: 484.4142 (478.7659)  time: 3.9325  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [  70/1724]  eta: 1:51:21  lr: 0.000000  loss: 2043.1761 (2015.5903)  loss_n_40: 4.2012 (4.2120)  loss_n_60: 4.4870 (4.5698)  loss_n_80: 4.6030 (4.7093)  loss_n_100: 4.6623 (4.7586)  triple_100: 513.3839 (505.1645)  triple_80: 515.9209 (507.7757)  triple_60: 511.6020 (504.5584)  triple_40: 489.3887 (479.8420)  time: 3.9302  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [  80/1724]  eta: 1:50:18  lr: 0.000000  loss: 1948.2422 (2003.0029)  loss_n_40: 4.2463 (4.2232)  loss_n_60: 4.5705 (4.5809)  loss_n_80: 4.6810 (4.7209)  loss_n_100: 4.7315 (4.7701)  triple_100: 488.9221 (502.0167)  triple_80: 491.8472 (504.6396)  triple_60: 487.7315 (501.4053)  triple_40: 460.7022 (476.6461)  time: 3.9292  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [  90/1724]  eta: 1:49:20  lr: 0.000000  loss: 1883.5403 (1997.5334)  loss_n_40: 4.2745 (4.2235)  loss_n_60: 4.6353 (4.5865)  loss_n_80: 4.7816 (4.7272)  loss_n_100: 4.8387 (4.7754)  triple_100: 472.1416 (500.6760)  triple_80: 475.2833 (503.3087)  triple_60: 471.6789 (500.0512)  triple_40: 444.9173 (475.1849)  time: 3.9284  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 100/1724]  eta: 1:48:26  lr: 0.000000  loss: 1986.5721 (2000.3407)  loss_n_40: 4.2277 (4.2201)  loss_n_60: 4.6180 (4.5850)  loss_n_80: 4.7560 (4.7262)  loss_n_100: 4.8125 (4.7742)  triple_100: 498.5892 (501.3877)  triple_80: 501.3589 (504.0062)  triple_60: 497.7445 (500.7456)  triple_40: 469.7717 (475.8957)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 110/1724]  eta: 1:47:35  lr: 0.000000  loss: 1952.1373 (1994.3279)  loss_n_40: 4.2277 (4.2286)  loss_n_60: 4.6164 (4.5933)  loss_n_80: 4.7603 (4.7342)  loss_n_100: 4.8125 (4.7818)  triple_100: 488.6717 (499.8596)  triple_80: 491.2563 (502.4981)  triple_60: 488.5892 (499.2512)  triple_40: 463.5759 (474.3811)  time: 3.9295  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 120/1724]  eta: 1:46:46  lr: 0.000000  loss: 1898.0815 (1988.1520)  loss_n_40: 4.3237 (4.2354)  loss_n_60: 4.6931 (4.6007)  loss_n_80: 4.8293 (4.7414)  loss_n_100: 4.8860 (4.7892)  triple_100: 476.0379 (498.3032)  triple_80: 478.8336 (500.9486)  triple_60: 475.2438 (497.7091)  triple_40: 449.3419 (472.8243)  time: 3.9299  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:0]  [ 130/1724]  eta: 1:45:58  lr: 0.000000  loss: 1902.8628 (1985.3782)  loss_n_40: 4.3338 (4.2388)  loss_n_60: 4.7030 (4.6052)  loss_n_80: 4.8434 (4.7461)  loss_n_100: 4.8975 (4.7935)  triple_100: 476.8989 (497.6166)  triple_80: 479.7040 (500.2656)  triple_60: 476.4285 (497.0111)  triple_40: 450.1029 (472.1013)  time: 3.9295  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:0]  [ 140/1724]  eta: 1:45:11  lr: 0.000000  loss: 1946.4417 (1986.8402)  loss_n_40: 4.2651 (4.2400)  loss_n_60: 4.5917 (4.6041)  loss_n_80: 4.7396 (4.7445)  loss_n_100: 4.7824 (4.7925)  triple_100: 488.3982 (497.9566)  triple_80: 490.8281 (500.6020)  triple_60: 487.2432 (497.3594)  triple_40: 460.6317 (472.5412)  time: 3.9290  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:0]  [ 150/1724]  eta: 1:44:25  lr: 0.000000  loss: 1988.1418 (1987.7638)  loss_n_40: 4.2427 (4.2405)  loss_n_60: 4.5897 (4.6029)  loss_n_80: 4.6952 (4.7437)  loss_n_100: 4.7338 (4.7915)  triple_100: 499.5201 (498.1892)  triple_80: 500.8133 (500.8442)  triple_60: 497.0757 (497.5928)  triple_40: 473.0513 (472.7590)  time: 3.9286  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:0]  [ 160/1724]  eta: 1:43:41  lr: 0.000000  loss: 1921.7112 (1986.6000)  loss_n_40: 4.2448 (4.2417)  loss_n_60: 4.6530 (4.6055)  loss_n_80: 4.8239 (4.7467)  loss_n_100: 4.8494 (4.7938)  triple_100: 482.0400 (497.9023)  triple_80: 485.0699 (500.5668)  triple_60: 481.3716 (497.3091)  triple_40: 454.5976 (472.4342)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 170/1724]  eta: 1:42:56  lr: 0.000000  loss: 1943.6718 (1989.4324)  loss_n_40: 4.1827 (4.2341)  loss_n_60: 4.5582 (4.5971)  loss_n_80: 4.6780 (4.7384)  loss_n_100: 4.7473 (4.7860)  triple_100: 488.3146 (498.6296)  triple_80: 491.0461 (501.2700)  triple_60: 486.9298 (498.0110)  triple_40: 458.8515 (473.1662)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 180/1724]  eta: 1:42:13  lr: 0.000000  loss: 2011.5319 (1991.2754)  loss_n_40: 4.1734 (4.2347)  loss_n_60: 4.5550 (4.5987)  loss_n_80: 4.6780 (4.7398)  loss_n_100: 4.7219 (4.7872)  triple_100: 503.6157 (499.0816)  triple_80: 506.6541 (501.7239)  triple_60: 503.7361 (498.4810)  triple_40: 478.2036 (473.6284)  time: 3.9283  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 190/1724]  eta: 1:41:29  lr: 0.000000  loss: 1970.8187 (1991.6184)  loss_n_40: 4.3214 (4.2387)  loss_n_60: 4.6028 (4.6006)  loss_n_80: 4.7522 (4.7412)  loss_n_100: 4.8278 (4.7889)  triple_100: 494.5724 (499.1472)  triple_80: 497.2220 (501.7893)  triple_60: 493.7336 (498.5596)  triple_40: 467.2474 (473.7530)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 200/1724]  eta: 1:40:46  lr: 0.000000  loss: 1943.7531 (1992.4631)  loss_n_40: 4.1516 (4.2364)  loss_n_60: 4.5410 (4.5991)  loss_n_80: 4.6909 (4.7399)  loss_n_100: 4.7315 (4.7879)  triple_100: 487.8789 (499.3810)  triple_80: 490.5901 (502.0098)  triple_60: 487.0118 (498.7772)  triple_40: 460.1770 (473.9319)  time: 3.9282  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [ 210/1724]  eta: 1:40:04  lr: 0.000000  loss: 1943.7531 (1991.6867)  loss_n_40: 4.1316 (4.2392)  loss_n_60: 4.5410 (4.6009)  loss_n_80: 4.6909 (4.7413)  loss_n_100: 4.7315 (4.7890)  triple_100: 487.8789 (499.1741)  triple_80: 490.5901 (501.8178)  triple_60: 487.0118 (498.5879)  triple_40: 460.1770 (473.7366)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 220/1724]  eta: 1:39:22  lr: 0.000000  loss: 1915.5104 (1988.6581)  loss_n_40: 4.2525 (4.2403)  loss_n_60: 4.6498 (4.6034)  loss_n_80: 4.7917 (4.7441)  loss_n_100: 4.8349 (4.7917)  triple_100: 480.0833 (498.4244)  triple_80: 482.8759 (501.0727)  triple_60: 480.0619 (497.8357)  triple_40: 453.6313 (472.9457)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 230/1724]  eta: 1:38:40  lr: 0.000000  loss: 1927.5958 (1988.6702)  loss_n_40: 4.2055 (4.2390)  loss_n_60: 4.6091 (4.6022)  loss_n_80: 4.7525 (4.7433)  loss_n_100: 4.8281 (4.7911)  triple_100: 482.2004 (498.4452)  triple_80: 485.9380 (501.0824)  triple_60: 483.2892 (497.8367)  triple_40: 456.3539 (472.9303)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 240/1724]  eta: 1:37:58  lr: 0.000000  loss: 1929.5863 (1986.1113)  loss_n_40: 4.1896 (4.2390)  loss_n_60: 4.5752 (4.6033)  loss_n_80: 4.7334 (4.7449)  loss_n_100: 4.7846 (4.7927)  triple_100: 483.7796 (497.8224)  triple_80: 486.7955 (500.4653)  triple_60: 483.2892 (497.2012)  triple_40: 457.0062 (472.2425)  time: 3.9290  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 250/1724]  eta: 1:37:17  lr: 0.000000  loss: 1945.8987 (1987.6116)  loss_n_40: 4.1896 (4.2367)  loss_n_60: 4.5752 (4.6021)  loss_n_80: 4.7276 (4.7438)  loss_n_100: 4.7846 (4.7917)  triple_100: 489.0906 (498.2027)  triple_80: 491.7456 (500.8435)  triple_60: 487.2317 (497.5785)  triple_40: 461.5798 (472.6125)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 260/1724]  eta: 1:36:35  lr: 0.000000  loss: 2033.7052 (1987.8406)  loss_n_40: 4.1841 (4.2367)  loss_n_60: 4.5511 (4.6021)  loss_n_80: 4.7202 (4.7442)  loss_n_100: 4.7625 (4.7921)  triple_100: 509.9743 (498.2676)  triple_80: 512.9749 (500.9051)  triple_60: 509.4415 (497.6311)  triple_40: 482.1692 (472.6617)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 270/1724]  eta: 1:35:54  lr: 0.000000  loss: 1971.0785 (1988.1183)  loss_n_40: 4.2196 (4.2371)  loss_n_60: 4.6495 (4.6031)  loss_n_80: 4.7937 (4.7454)  loss_n_100: 4.8439 (4.7932)  triple_100: 494.7009 (498.3390)  triple_80: 497.5276 (500.9782)  triple_60: 493.6104 (497.7035)  triple_40: 466.2719 (472.7188)  time: 3.9293  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 280/1724]  eta: 1:35:13  lr: 0.000000  loss: 1985.3962 (1987.4527)  loss_n_40: 4.3170 (4.2399)  loss_n_60: 4.6698 (4.6056)  loss_n_80: 4.8027 (4.7479)  loss_n_100: 4.8681 (4.7954)  triple_100: 496.8378 (498.1628)  triple_80: 499.3527 (500.8091)  triple_60: 496.6395 (497.5372)  triple_40: 472.9084 (472.5548)  time: 3.9293  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 290/1724]  eta: 1:34:32  lr: 0.000000  loss: 1985.3962 (1986.6300)  loss_n_40: 4.2822 (4.2405)  loss_n_60: 4.6516 (4.6052)  loss_n_80: 4.8093 (4.7474)  loss_n_100: 4.8754 (4.7952)  triple_100: 496.8378 (497.9593)  triple_80: 499.3527 (500.5969)  triple_60: 496.6395 (497.3250)  triple_40: 472.9084 (472.3605)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 300/1724]  eta: 1:33:51  lr: 0.000000  loss: 1935.1565 (1985.0651)  loss_n_40: 4.2204 (4.2397)  loss_n_60: 4.5902 (4.6057)  loss_n_80: 4.7543 (4.7481)  loss_n_100: 4.8083 (4.7960)  triple_100: 485.7036 (497.5870)  triple_80: 488.1016 (500.2245)  triple_60: 484.5042 (496.9328)  triple_40: 458.8446 (471.9314)  time: 3.9292  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 310/1724]  eta: 1:33:11  lr: 0.000000  loss: 1923.0059 (1984.0945)  loss_n_40: 4.2690 (4.2406)  loss_n_60: 4.6240 (4.6076)  loss_n_80: 4.7745 (4.7501)  loss_n_100: 4.8451 (4.7980)  triple_100: 483.7177 (497.3548)  triple_80: 485.9384 (499.9915)  triple_60: 481.5214 (496.6950)  triple_40: 456.4562 (471.6568)  time: 3.9293  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 320/1724]  eta: 1:32:30  lr: 0.000000  loss: 1956.6385 (1984.2642)  loss_n_40: 4.2690 (4.2412)  loss_n_60: 4.6279 (4.6088)  loss_n_80: 4.7807 (4.7515)  loss_n_100: 4.8451 (4.7990)  triple_100: 490.3273 (497.3972)  triple_80: 493.7822 (500.0386)  triple_60: 489.9640 (496.7380)  triple_40: 463.3453 (471.6899)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 330/1724]  eta: 1:31:49  lr: 0.000000  loss: 1957.2598 (1983.6590)  loss_n_40: 4.2434 (4.2412)  loss_n_60: 4.6279 (4.6095)  loss_n_80: 4.7634 (4.7522)  loss_n_100: 4.7957 (4.7997)  triple_100: 490.8585 (497.2479)  triple_80: 493.7822 (499.8929)  triple_60: 490.2351 (496.5894)  triple_40: 463.3993 (471.5261)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 340/1724]  eta: 1:31:09  lr: 0.000000  loss: 1961.8906 (1985.1108)  loss_n_40: 4.2258 (4.2384)  loss_n_60: 4.5677 (4.6075)  loss_n_80: 4.7216 (4.7504)  loss_n_100: 4.7826 (4.7980)  triple_100: 492.6749 (497.6264)  triple_80: 495.4664 (500.2666)  triple_60: 491.5640 (496.9552)  triple_40: 464.2620 (471.8682)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 350/1724]  eta: 1:30:28  lr: 0.000000  loss: 2032.6278 (1985.2599)  loss_n_40: 4.1485 (4.2378)  loss_n_60: 4.5061 (4.6066)  loss_n_80: 4.6121 (4.7498)  loss_n_100: 4.6752 (4.7974)  triple_100: 509.3169 (497.6657)  triple_80: 511.3918 (500.3023)  triple_60: 508.5891 (496.9883)  triple_40: 484.6404 (471.9121)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 360/1724]  eta: 1:29:48  lr: 0.000000  loss: 1996.0973 (1986.8490)  loss_n_40: 4.1823 (4.2362)  loss_n_60: 4.5075 (4.6049)  loss_n_80: 4.6122 (4.7482)  loss_n_100: 4.6830 (4.7957)  triple_100: 500.0109 (498.0633)  triple_80: 502.7432 (500.6995)  triple_60: 499.8900 (497.3849)  triple_40: 474.4176 (472.3163)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 370/1724]  eta: 1:29:08  lr: 0.000000  loss: 1996.0973 (1988.1911)  loss_n_40: 4.1823 (4.2359)  loss_n_60: 4.5182 (4.6033)  loss_n_80: 4.6593 (4.7463)  loss_n_100: 4.7240 (4.7938)  triple_100: 500.0109 (498.3910)  triple_80: 502.7432 (501.0260)  triple_60: 499.8900 (497.7165)  triple_40: 474.4176 (472.6784)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 380/1724]  eta: 1:28:27  lr: 0.000000  loss: 2031.6237 (1989.3057)  loss_n_40: 4.1687 (4.2347)  loss_n_60: 4.5101 (4.6005)  loss_n_80: 4.6564 (4.7432)  loss_n_100: 4.7164 (4.7909)  triple_100: 508.3257 (498.6579)  triple_80: 510.8528 (501.2947)  triple_60: 508.0991 (497.9951)  triple_40: 484.8071 (472.9888)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 390/1724]  eta: 1:27:47  lr: 0.000000  loss: 1985.5352 (1989.1980)  loss_n_40: 4.1697 (4.2359)  loss_n_60: 4.5125 (4.6013)  loss_n_80: 4.6526 (4.7440)  loss_n_100: 4.7137 (4.7916)  triple_100: 497.2604 (498.6253)  triple_80: 499.6935 (501.2642)  triple_60: 496.7753 (497.9651)  triple_40: 473.7010 (472.9706)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 400/1724]  eta: 1:27:07  lr: 0.000000  loss: 1923.3890 (1988.8047)  loss_n_40: 4.2749 (4.2352)  loss_n_60: 4.6067 (4.5997)  loss_n_80: 4.7299 (4.7423)  loss_n_100: 4.7630 (4.7900)  triple_100: 482.1691 (498.5231)  triple_80: 485.1640 (501.1599)  triple_60: 481.6325 (497.8653)  triple_40: 455.4152 (472.8893)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 410/1724]  eta: 1:26:27  lr: 0.000000  loss: 1955.4324 (1989.0937)  loss_n_40: 4.3112 (4.2346)  loss_n_60: 4.6921 (4.5997)  loss_n_80: 4.8359 (4.7425)  loss_n_100: 4.8809 (4.7904)  triple_100: 490.4424 (498.6016)  triple_80: 493.2266 (501.2364)  triple_60: 489.4906 (497.9377)  triple_40: 463.4848 (472.9508)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 420/1724]  eta: 1:25:47  lr: 0.000000  loss: 1974.1792 (1990.4455)  loss_n_40: 4.2155 (4.2323)  loss_n_60: 4.5893 (4.5966)  loss_n_80: 4.7276 (4.7395)  loss_n_100: 4.7765 (4.7873)  triple_100: 495.6360 (498.9374)  triple_80: 498.3660 (501.5727)  triple_60: 494.5981 (498.2741)  triple_40: 467.0219 (473.3057)  time: 3.9271  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [ 430/1724]  eta: 1:25:07  lr: 0.000000  loss: 1970.5887 (1989.8873)  loss_n_40: 4.1913 (4.2328)  loss_n_60: 4.5545 (4.5967)  loss_n_80: 4.7084 (4.7395)  loss_n_100: 4.7709 (4.7875)  triple_100: 494.4446 (498.7950)  triple_80: 497.4431 (501.4313)  triple_60: 493.3724 (498.1326)  triple_40: 466.6645 (473.1718)  time: 3.9276  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 440/1724]  eta: 1:24:27  lr: 0.000000  loss: 1923.0951 (1989.5005)  loss_n_40: 4.2431 (4.2333)  loss_n_60: 4.6614 (4.5981)  loss_n_80: 4.8141 (4.7410)  loss_n_100: 4.8638 (4.7889)  triple_100: 483.3161 (498.7060)  triple_80: 486.0119 (501.3430)  triple_60: 481.8960 (498.0366)  triple_40: 454.5225 (473.0538)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 450/1724]  eta: 1:23:47  lr: 0.000000  loss: 1942.5558 (1989.0678)  loss_n_40: 4.2466 (4.2343)  loss_n_60: 4.6614 (4.5988)  loss_n_80: 4.8050 (4.7417)  loss_n_100: 4.8531 (4.7897)  triple_100: 487.2858 (498.5981)  triple_80: 490.0448 (501.2358)  triple_60: 486.8203 (497.9307)  triple_40: 459.5812 (472.9388)  time: 3.9284  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 460/1724]  eta: 1:23:07  lr: 0.000000  loss: 1894.3369 (1988.3605)  loss_n_40: 4.3362 (4.2363)  loss_n_60: 4.6665 (4.6008)  loss_n_80: 4.8050 (4.7439)  loss_n_100: 4.8531 (4.7917)  triple_100: 473.7327 (498.4186)  triple_80: 477.4755 (501.0574)  triple_60: 474.4068 (497.7536)  triple_40: 447.9409 (472.7581)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 470/1724]  eta: 1:22:27  lr: 0.000000  loss: 1894.3369 (1988.0331)  loss_n_40: 4.3107 (4.2378)  loss_n_60: 4.6789 (4.6019)  loss_n_80: 4.8055 (4.7448)  loss_n_100: 4.8419 (4.7924)  triple_100: 473.7327 (498.3286)  triple_80: 477.4755 (500.9715)  triple_60: 474.4068 (497.6710)  triple_40: 447.9409 (472.6850)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 480/1724]  eta: 1:21:47  lr: 0.000000  loss: 1950.5277 (1987.9946)  loss_n_40: 4.2791 (4.2390)  loss_n_60: 4.6789 (4.6033)  loss_n_80: 4.8055 (4.7459)  loss_n_100: 4.8419 (4.7936)  triple_100: 487.9249 (498.3169)  triple_80: 491.2372 (500.9626)  triple_60: 488.4072 (497.6624)  triple_40: 463.0688 (472.6709)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 490/1724]  eta: 1:21:07  lr: 0.000000  loss: 1911.0249 (1986.7138)  loss_n_40: 4.2979 (4.2408)  loss_n_60: 4.7522 (4.6058)  loss_n_80: 4.8879 (4.7485)  loss_n_100: 4.9391 (4.7960)  triple_100: 479.1481 (498.0014)  triple_80: 482.0597 (500.6478)  triple_60: 478.6162 (497.3435)  triple_40: 451.7580 (472.3299)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 500/1724]  eta: 1:20:27  lr: 0.000000  loss: 1916.6416 (1988.0850)  loss_n_40: 4.2443 (4.2392)  loss_n_60: 4.5260 (4.6031)  loss_n_80: 4.6975 (4.7457)  loss_n_100: 4.7583 (4.7934)  triple_100: 480.1049 (498.3357)  triple_80: 482.9471 (500.9802)  triple_60: 480.0203 (497.6840)  triple_40: 453.8289 (472.7038)  time: 3.9292  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 510/1724]  eta: 1:19:47  lr: 0.000000  loss: 2011.1974 (1987.4079)  loss_n_40: 4.2092 (4.2399)  loss_n_60: 4.5095 (4.6034)  loss_n_80: 4.6547 (4.7459)  loss_n_100: 4.7079 (4.7936)  triple_100: 504.1061 (498.1625)  triple_80: 506.8848 (500.8070)  triple_60: 503.5959 (497.5139)  triple_40: 478.2427 (472.5415)  time: 3.9307  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 520/1724]  eta: 1:19:08  lr: 0.000000  loss: 1955.0531 (1986.8215)  loss_n_40: 4.2422 (4.2401)  loss_n_60: 4.6048 (4.6045)  loss_n_80: 4.7544 (4.7470)  loss_n_100: 4.7959 (4.7945)  triple_100: 490.0705 (498.0150)  triple_80: 493.1478 (500.6643)  triple_60: 489.6594 (497.3703)  triple_40: 463.4521 (472.3857)  time: 3.9302  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 530/1724]  eta: 1:18:28  lr: 0.000000  loss: 1957.7662 (1987.5251)  loss_n_40: 4.2381 (4.2397)  loss_n_60: 4.6473 (4.6036)  loss_n_80: 4.7544 (4.7461)  loss_n_100: 4.7959 (4.7935)  triple_100: 491.3939 (498.1864)  triple_80: 494.0242 (500.8366)  triple_60: 490.3774 (497.5445)  triple_40: 463.6606 (472.5747)  time: 3.9307  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 540/1724]  eta: 1:17:48  lr: 0.000000  loss: 1960.8181 (1987.1231)  loss_n_40: 4.1913 (4.2391)  loss_n_60: 4.6428 (4.6035)  loss_n_80: 4.7706 (4.7461)  loss_n_100: 4.8117 (4.7935)  triple_100: 491.2098 (498.0904)  triple_80: 494.4903 (500.7416)  triple_60: 491.4557 (497.4467)  triple_40: 465.1827 (472.4622)  time: 3.9307  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 550/1724]  eta: 1:17:08  lr: 0.000000  loss: 1941.9683 (1986.6031)  loss_n_40: 4.2091 (4.2393)  loss_n_60: 4.6428 (4.6044)  loss_n_80: 4.7824 (4.7471)  loss_n_100: 4.8404 (4.7945)  triple_100: 487.0928 (497.9673)  triple_80: 489.7153 (500.6184)  triple_60: 486.4282 (497.3178)  triple_40: 460.5595 (472.3144)  time: 3.9297  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:0]  [ 560/1724]  eta: 1:16:29  lr: 0.000000  loss: 1938.7634 (1986.8757)  loss_n_40: 4.2835 (4.2391)  loss_n_60: 4.6569 (4.6043)  loss_n_80: 4.8142 (4.7468)  loss_n_100: 4.8650 (4.7943)  triple_100: 486.1096 (498.0328)  triple_80: 489.0266 (500.6847)  triple_60: 485.2889 (497.3860)  triple_40: 459.8729 (472.3878)  time: 3.9298  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:0]  [ 570/1724]  eta: 1:15:49  lr: 0.000000  loss: 1954.5840 (1987.1525)  loss_n_40: 4.2518 (4.2383)  loss_n_60: 4.6425 (4.6037)  loss_n_80: 4.8020 (4.7464)  loss_n_100: 4.8522 (4.7939)  triple_100: 491.2210 (498.1087)  triple_80: 493.6152 (500.7577)  triple_60: 489.4040 (497.4542)  triple_40: 462.5533 (472.4496)  time: 3.9307  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 580/1724]  eta: 1:15:09  lr: 0.000000  loss: 1971.6532 (1987.5665)  loss_n_40: 4.1351 (4.2366)  loss_n_60: 4.5472 (4.6028)  loss_n_80: 4.7113 (4.7456)  loss_n_100: 4.7689 (4.7930)  triple_100: 494.8844 (498.2179)  triple_80: 497.5533 (500.8653)  triple_60: 493.5818 (497.5601)  triple_40: 466.9141 (472.5452)  time: 3.9302  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 590/1724]  eta: 1:14:30  lr: 0.000000  loss: 1981.1552 (1987.3852)  loss_n_40: 4.2251 (4.2369)  loss_n_60: 4.5449 (4.6027)  loss_n_80: 4.7101 (4.7455)  loss_n_100: 4.7480 (4.7929)  triple_100: 496.7181 (498.1745)  triple_80: 499.5684 (500.8204)  triple_60: 496.4232 (497.5146)  triple_40: 469.6710 (472.4977)  time: 3.9294  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 600/1724]  eta: 1:13:50  lr: 0.000000  loss: 1981.7306 (1987.9551)  loss_n_40: 4.2492 (4.2361)  loss_n_60: 4.5875 (4.6012)  loss_n_80: 4.7400 (4.7440)  loss_n_100: 4.7882 (4.7915)  triple_100: 497.7568 (498.3140)  triple_80: 500.0468 (500.9579)  triple_60: 496.1907 (497.6553)  triple_40: 469.5074 (472.6551)  time: 3.9298  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:0]  [ 610/1724]  eta: 1:13:10  lr: 0.000000  loss: 1912.6561 (1986.8261)  loss_n_40: 4.2387 (4.2379)  loss_n_60: 4.6086 (4.6033)  loss_n_80: 4.7595 (4.7461)  loss_n_100: 4.8108 (4.7934)  triple_100: 479.7268 (498.0303)  triple_80: 482.4345 (500.6775)  triple_60: 478.8218 (497.3749)  triple_40: 453.1788 (472.3628)  time: 3.9304  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:0]  [ 620/1724]  eta: 1:12:31  lr: 0.000000  loss: 1912.6561 (1987.3897)  loss_n_40: 4.2313 (4.2375)  loss_n_60: 4.6086 (4.6023)  loss_n_80: 4.7595 (4.7450)  loss_n_100: 4.8108 (4.7924)  triple_100: 479.7268 (498.1716)  triple_80: 482.4345 (500.8155)  triple_60: 478.8218 (497.5145)  triple_40: 453.1788 (472.5109)  time: 3.9304  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 630/1724]  eta: 1:11:51  lr: 0.000000  loss: 2005.8578 (1987.9594)  loss_n_40: 4.1404 (4.2372)  loss_n_60: 4.5024 (4.6021)  loss_n_80: 4.6697 (4.7448)  loss_n_100: 4.7339 (4.7922)  triple_100: 503.5273 (498.3130)  triple_80: 506.3020 (500.9556)  triple_60: 502.2792 (497.6570)  triple_40: 475.3962 (472.6575)  time: 3.9304  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 640/1724]  eta: 1:11:12  lr: 0.000000  loss: 1991.1168 (1988.2596)  loss_n_40: 4.1404 (4.2364)  loss_n_60: 4.5243 (4.6012)  loss_n_80: 4.6793 (4.7440)  loss_n_100: 4.7365 (4.7913)  triple_100: 499.5400 (498.3878)  triple_80: 501.7053 (501.0312)  triple_60: 498.6020 (497.7317)  triple_40: 473.3067 (472.7359)  time: 3.9300  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [ 650/1724]  eta: 1:10:32  lr: 0.000000  loss: 1981.5380 (1988.5180)  loss_n_40: 4.2040 (4.2357)  loss_n_60: 4.5947 (4.6006)  loss_n_80: 4.7742 (4.7434)  loss_n_100: 4.7996 (4.7907)  triple_100: 497.6984 (498.4543)  triple_80: 500.0878 (501.0977)  triple_60: 496.1876 (497.7962)  triple_40: 469.5409 (472.7995)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 660/1724]  eta: 1:09:52  lr: 0.000000  loss: 1938.0424 (1988.4016)  loss_n_40: 4.1842 (4.2347)  loss_n_60: 4.5963 (4.5997)  loss_n_80: 4.7400 (4.7426)  loss_n_100: 4.7870 (4.7899)  triple_100: 487.1279 (498.4298)  triple_80: 489.5855 (501.0734)  triple_60: 485.1808 (497.7676)  triple_40: 458.4793 (472.7639)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 670/1724]  eta: 1:09:13  lr: 0.000000  loss: 1941.6409 (1987.5988)  loss_n_40: 4.2109 (4.2361)  loss_n_60: 4.6253 (4.6017)  loss_n_80: 4.7632 (4.7445)  loss_n_100: 4.8120 (4.7917)  triple_100: 487.1279 (498.2292)  triple_80: 489.5855 (500.8775)  triple_60: 486.3170 (497.5686)  triple_40: 458.6235 (472.5495)  time: 3.9284  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 680/1724]  eta: 1:08:33  lr: 0.000000  loss: 1941.6409 (1987.7257)  loss_n_40: 4.2285 (4.2364)  loss_n_60: 4.6689 (4.6015)  loss_n_80: 4.7997 (4.7443)  loss_n_100: 4.8317 (4.7914)  triple_100: 485.3772 (498.2530)  triple_80: 489.3192 (500.9033)  triple_60: 486.3170 (497.5987)  triple_40: 459.1307 (472.5971)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 690/1724]  eta: 1:07:54  lr: 0.000000  loss: 2049.6765 (1988.7055)  loss_n_40: 4.1715 (4.2355)  loss_n_60: 4.5393 (4.5997)  loss_n_80: 4.6697 (4.7424)  loss_n_100: 4.7216 (4.7895)  triple_100: 512.5677 (498.4918)  triple_80: 515.2957 (501.1430)  triple_60: 512.4939 (497.8424)  triple_40: 490.1364 (472.8613)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 700/1724]  eta: 1:07:14  lr: 0.000000  loss: 2022.5481 (1989.0913)  loss_n_40: 4.1686 (4.2344)  loss_n_60: 4.5393 (4.5984)  loss_n_80: 4.6697 (4.7412)  loss_n_100: 4.7382 (4.7884)  triple_100: 508.4826 (498.5933)  triple_80: 511.2757 (501.2418)  triple_60: 506.7343 (497.9384)  triple_40: 477.7806 (472.9554)  time: 3.9303  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 710/1724]  eta: 1:06:35  lr: 0.000000  loss: 1989.2928 (1989.2453)  loss_n_40: 4.1779 (4.2348)  loss_n_60: 4.6034 (4.5995)  loss_n_80: 4.7679 (4.7423)  loss_n_100: 4.8017 (4.7894)  triple_100: 500.2241 (498.6331)  triple_80: 502.5620 (501.2842)  triple_60: 498.0765 (497.9781)  triple_40: 469.5934 (472.9838)  time: 3.9315  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:0]  [ 720/1724]  eta: 1:05:55  lr: 0.000000  loss: 1966.7567 (1989.0013)  loss_n_40: 4.2775 (4.2345)  loss_n_60: 4.6264 (4.5995)  loss_n_80: 4.7850 (4.7423)  loss_n_100: 4.8281 (4.7894)  triple_100: 493.4753 (498.5776)  triple_80: 496.3051 (501.2271)  triple_60: 492.4841 (497.9184)  triple_40: 466.1461 (472.9124)  time: 3.9304  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 730/1724]  eta: 1:05:15  lr: 0.000000  loss: 1960.5839 (1988.8738)  loss_n_40: 4.2140 (4.2342)  loss_n_60: 4.6189 (4.5989)  loss_n_80: 4.7406 (4.7419)  loss_n_100: 4.7843 (4.7890)  triple_100: 491.3380 (498.5453)  triple_80: 494.0049 (501.1953)  triple_60: 491.3739 (497.8864)  triple_40: 465.3494 (472.8829)  time: 3.9294  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 740/1724]  eta: 1:04:36  lr: 0.000000  loss: 1970.5646 (1989.2051)  loss_n_40: 4.2140 (4.2331)  loss_n_60: 4.5621 (4.5976)  loss_n_80: 4.7179 (4.7405)  loss_n_100: 4.7482 (4.7876)  triple_100: 494.8976 (498.6297)  triple_80: 497.5948 (501.2781)  triple_60: 493.6665 (497.9699)  triple_40: 468.2020 (472.9686)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 750/1724]  eta: 1:03:56  lr: 0.000000  loss: 1961.2113 (1989.0022)  loss_n_40: 4.2198 (4.2338)  loss_n_60: 4.5781 (4.5984)  loss_n_80: 4.7250 (4.7412)  loss_n_100: 4.7815 (4.7883)  triple_100: 492.6803 (498.5743)  triple_80: 495.1867 (501.2245)  triple_60: 490.9866 (497.9189)  triple_40: 464.4674 (472.9228)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 760/1724]  eta: 1:03:17  lr: 0.000000  loss: 1962.0105 (1989.0561)  loss_n_40: 4.1865 (4.2326)  loss_n_60: 4.5927 (4.5969)  loss_n_80: 4.7633 (4.7399)  loss_n_100: 4.7936 (4.7871)  triple_100: 492.7764 (498.5893)  triple_80: 495.7021 (501.2380)  triple_60: 491.3979 (497.9332)  triple_40: 463.8304 (472.9392)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 770/1724]  eta: 1:02:37  lr: 0.000000  loss: 1998.3301 (1989.3049)  loss_n_40: 4.1540 (4.2322)  loss_n_60: 4.5376 (4.5963)  loss_n_80: 4.7037 (4.7393)  loss_n_100: 4.7409 (4.7865)  triple_100: 501.8731 (498.6519)  triple_80: 504.2812 (501.2988)  triple_60: 500.5109 (497.9943)  triple_40: 473.6216 (473.0058)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 780/1724]  eta: 1:01:58  lr: 0.000000  loss: 1971.3269 (1988.8265)  loss_n_40: 4.2584 (4.2326)  loss_n_60: 4.6065 (4.5969)  loss_n_80: 4.7180 (4.7398)  loss_n_100: 4.7556 (4.7870)  triple_100: 493.5089 (498.5321)  triple_80: 496.2108 (501.1811)  triple_60: 493.1660 (497.8773)  triple_40: 469.3984 (472.8797)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 790/1724]  eta: 1:01:18  lr: 0.000000  loss: 1947.8796 (1988.4771)  loss_n_40: 4.2584 (4.2328)  loss_n_60: 4.6455 (4.5978)  loss_n_80: 4.7892 (4.7408)  loss_n_100: 4.8413 (4.7879)  triple_100: 488.5189 (498.4484)  triple_80: 491.3626 (501.0986)  triple_60: 488.0732 (497.7920)  triple_40: 460.4399 (472.7788)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 800/1724]  eta: 1:00:39  lr: 0.000000  loss: 1991.3743 (1989.1371)  loss_n_40: 4.2750 (4.2341)  loss_n_60: 4.6710 (4.5983)  loss_n_80: 4.7800 (4.7409)  loss_n_100: 4.7991 (4.7878)  triple_100: 499.1364 (498.5966)  triple_80: 502.2353 (501.2533)  triple_60: 498.9188 (497.9571)  triple_40: 471.7598 (472.9690)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 810/1724]  eta: 0:59:59  lr: 0.000000  loss: 2063.7971 (1989.9742)  loss_n_40: 4.2283 (4.2329)  loss_n_60: 4.5497 (4.5967)  loss_n_80: 4.6931 (4.7392)  loss_n_100: 4.7336 (4.7861)  triple_100: 515.9169 (498.8016)  triple_80: 519.1241 (501.4587)  triple_60: 516.4952 (498.1648)  triple_40: 493.8911 (473.1941)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 820/1724]  eta: 0:59:20  lr: 0.000000  loss: 2012.1888 (1989.7605)  loss_n_40: 4.1838 (4.2329)  loss_n_60: 4.5967 (4.5970)  loss_n_80: 4.7120 (4.7395)  loss_n_100: 4.7347 (4.7863)  triple_100: 504.3181 (498.7463)  triple_80: 507.1813 (501.4054)  triple_60: 503.8262 (498.1111)  triple_40: 477.9722 (473.1420)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 830/1724]  eta: 0:58:40  lr: 0.000000  loss: 1962.5558 (1990.1285)  loss_n_40: 4.2126 (4.2327)  loss_n_60: 4.6125 (4.5964)  loss_n_80: 4.7530 (4.7389)  loss_n_100: 4.7971 (4.7857)  triple_100: 491.9459 (498.8372)  triple_80: 495.2470 (501.4939)  triple_60: 491.2667 (498.2000)  triple_40: 464.7691 (473.2437)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 840/1724]  eta: 0:58:01  lr: 0.000000  loss: 2004.1462 (1990.0976)  loss_n_40: 4.1818 (4.2314)  loss_n_60: 4.5585 (4.5953)  loss_n_80: 4.6993 (4.7379)  loss_n_100: 4.7593 (4.7848)  triple_100: 502.4728 (498.8328)  triple_80: 504.5246 (501.4861)  triple_60: 501.5008 (498.1914)  triple_40: 477.2477 (473.2379)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 850/1724]  eta: 0:57:21  lr: 0.000000  loss: 2004.1462 (1990.1253)  loss_n_40: 4.1455 (4.2316)  loss_n_60: 4.5231 (4.5954)  loss_n_80: 4.6759 (4.7379)  loss_n_100: 4.7345 (4.7848)  triple_100: 502.4728 (498.8371)  triple_80: 504.5246 (501.4925)  triple_60: 501.5008 (498.1992)  triple_40: 477.2477 (473.2469)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 860/1724]  eta: 0:56:42  lr: 0.000000  loss: 1977.5465 (1989.8498)  loss_n_40: 4.1876 (4.2311)  loss_n_60: 4.5159 (4.5950)  loss_n_80: 4.7031 (4.7377)  loss_n_100: 4.7462 (4.7846)  triple_100: 495.4020 (498.7728)  triple_80: 498.6354 (501.4271)  triple_60: 495.3049 (498.1308)  triple_40: 469.6118 (473.1707)  time: 3.9253  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [ 870/1724]  eta: 0:56:02  lr: 0.000000  loss: 1940.5359 (1989.1980)  loss_n_40: 4.2321 (4.2316)  loss_n_60: 4.6852 (4.5963)  loss_n_80: 4.8343 (4.7390)  loss_n_100: 4.8686 (4.7858)  triple_100: 486.3297 (498.6129)  triple_80: 489.5112 (501.2703)  triple_60: 486.0931 (497.9704)  triple_40: 459.5973 (472.9918)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 880/1724]  eta: 0:55:23  lr: 0.000000  loss: 1937.4177 (1988.8763)  loss_n_40: 4.2341 (4.2314)  loss_n_60: 4.6784 (4.5965)  loss_n_80: 4.8266 (4.7393)  loss_n_100: 4.8648 (4.7861)  triple_100: 485.7974 (498.5351)  triple_80: 488.8268 (501.1914)  triple_60: 485.1289 (497.8913)  triple_40: 459.0521 (472.9051)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 890/1724]  eta: 0:54:43  lr: 0.000000  loss: 1974.8340 (1989.6398)  loss_n_40: 4.1782 (4.2317)  loss_n_60: 4.5858 (4.5966)  loss_n_80: 4.7314 (4.7392)  loss_n_100: 4.7851 (4.7859)  triple_100: 495.3770 (498.7200)  triple_80: 497.7841 (501.3793)  triple_60: 494.5104 (498.0843)  triple_40: 468.8576 (473.1028)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 900/1724]  eta: 0:54:04  lr: 0.000000  loss: 2020.1991 (1989.9516)  loss_n_40: 4.2153 (4.2322)  loss_n_60: 4.6045 (4.5973)  loss_n_80: 4.7292 (4.7399)  loss_n_100: 4.7851 (4.7865)  triple_100: 505.4024 (498.7967)  triple_80: 508.6849 (501.4571)  triple_60: 506.0827 (498.1622)  triple_40: 481.1816 (473.1797)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 910/1724]  eta: 0:53:24  lr: 0.000000  loss: 1954.0763 (1989.3163)  loss_n_40: 4.2394 (4.2331)  loss_n_60: 4.6495 (4.5983)  loss_n_80: 4.8086 (4.7409)  loss_n_100: 4.8574 (4.7876)  triple_100: 490.3530 (498.6357)  triple_80: 493.5280 (501.2989)  triple_60: 489.3293 (498.0034)  triple_40: 462.1920 (473.0184)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 920/1724]  eta: 0:52:45  lr: 0.000000  loss: 1925.0668 (1989.5442)  loss_n_40: 4.2394 (4.2327)  loss_n_60: 4.6720 (4.5981)  loss_n_80: 4.8253 (4.7406)  loss_n_100: 4.8635 (4.7872)  triple_100: 483.4869 (498.6925)  triple_80: 486.0790 (501.3565)  triple_60: 482.2900 (498.0602)  triple_40: 454.6683 (473.0765)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 930/1724]  eta: 0:52:05  lr: 0.000000  loss: 1935.0022 (1989.2311)  loss_n_40: 4.1945 (4.2328)  loss_n_60: 4.6416 (4.5985)  loss_n_80: 4.7795 (4.7411)  loss_n_100: 4.8181 (4.7876)  triple_100: 485.9027 (498.6173)  triple_80: 488.9010 (501.2820)  triple_60: 484.8092 (497.9830)  triple_40: 456.4278 (472.9888)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 940/1724]  eta: 0:51:26  lr: 0.000000  loss: 1934.1729 (1989.3686)  loss_n_40: 4.2388 (4.2337)  loss_n_60: 4.5999 (4.5990)  loss_n_80: 4.7795 (4.7415)  loss_n_100: 4.8181 (4.7879)  triple_100: 484.3835 (498.6461)  triple_80: 487.8160 (501.3136)  triple_60: 484.4141 (498.0162)  triple_40: 458.1475 (473.0306)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 950/1724]  eta: 0:50:46  lr: 0.000000  loss: 1992.2424 (1990.1254)  loss_n_40: 4.1777 (4.2323)  loss_n_60: 4.5273 (4.5977)  loss_n_80: 4.7137 (4.7403)  loss_n_100: 4.7541 (4.7868)  triple_100: 499.2479 (498.8359)  triple_80: 502.0576 (501.5015)  triple_60: 499.1920 (498.2058)  triple_40: 472.7489 (473.2250)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 960/1724]  eta: 0:50:07  lr: 0.000000  loss: 2015.3044 (1990.1637)  loss_n_40: 4.1333 (4.2325)  loss_n_60: 4.5454 (4.5980)  loss_n_80: 4.7137 (4.7405)  loss_n_100: 4.7541 (4.7870)  triple_100: 506.0759 (498.8425)  triple_80: 509.1015 (501.5097)  triple_60: 505.2532 (498.2160)  triple_40: 476.3189 (473.2375)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 970/1724]  eta: 0:49:27  lr: 0.000000  loss: 1962.6266 (1990.5023)  loss_n_40: 4.1904 (4.2315)  loss_n_60: 4.5871 (4.5970)  loss_n_80: 4.7230 (4.7395)  loss_n_100: 4.7656 (4.7860)  triple_100: 492.8645 (498.9286)  triple_80: 495.4301 (501.5955)  triple_60: 491.3198 (498.3010)  triple_40: 464.9118 (473.3233)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 980/1724]  eta: 0:48:48  lr: 0.000000  loss: 1968.4136 (1990.6623)  loss_n_40: 4.1411 (4.2309)  loss_n_60: 4.5758 (4.5966)  loss_n_80: 4.7230 (4.7392)  loss_n_100: 4.7656 (4.7856)  triple_100: 494.8458 (498.9740)  triple_80: 497.0597 (501.6388)  triple_60: 492.9798 (498.3421)  triple_40: 465.0091 (473.3551)  time: 3.9276  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [ 990/1724]  eta: 0:48:09  lr: 0.000000  loss: 1970.5792 (1990.9586)  loss_n_40: 4.1844 (4.2306)  loss_n_60: 4.5945 (4.5966)  loss_n_80: 4.7316 (4.7393)  loss_n_100: 4.7682 (4.7857)  triple_100: 494.8458 (499.0508)  triple_80: 497.7140 (501.7158)  triple_60: 493.8942 (498.4169)  triple_40: 467.7826 (473.4229)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1000/1724]  eta: 0:47:29  lr: 0.000000  loss: 1970.5792 (1991.0898)  loss_n_40: 4.2659 (4.2314)  loss_n_60: 4.6904 (4.5975)  loss_n_80: 4.8303 (4.7401)  loss_n_100: 4.8820 (4.7866)  triple_100: 494.6759 (499.0806)  triple_80: 497.7140 (501.7471)  triple_60: 493.8942 (498.4497)  triple_40: 467.7826 (473.4567)  time: 3.9271  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1010/1724]  eta: 0:46:50  lr: 0.000000  loss: 1928.8894 (1990.4596)  loss_n_40: 4.2792 (4.2318)  loss_n_60: 4.6564 (4.5981)  loss_n_80: 4.8098 (4.7407)  loss_n_100: 4.8440 (4.7872)  triple_100: 484.8804 (498.9261)  triple_80: 487.3811 (501.5940)  triple_60: 483.2473 (498.2936)  triple_40: 455.5943 (473.2882)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1020/1724]  eta: 0:46:10  lr: 0.000000  loss: 1928.8894 (1990.3213)  loss_n_40: 4.2711 (4.2319)  loss_n_60: 4.6454 (4.5985)  loss_n_80: 4.8020 (4.7411)  loss_n_100: 4.8333 (4.7875)  triple_100: 484.3992 (498.8928)  triple_80: 487.1945 (501.5615)  triple_60: 483.2473 (498.2605)  triple_40: 455.5943 (473.2474)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1030/1724]  eta: 0:45:31  lr: 0.000000  loss: 1929.8346 (1990.1988)  loss_n_40: 4.2506 (4.2318)  loss_n_60: 4.6262 (4.5983)  loss_n_80: 4.7669 (4.7410)  loss_n_100: 4.7979 (4.7874)  triple_100: 484.3992 (498.8650)  triple_80: 487.1945 (501.5326)  triple_60: 483.4226 (498.2300)  triple_40: 455.7234 (473.2127)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1040/1724]  eta: 0:44:51  lr: 0.000000  loss: 1941.1062 (1990.1772)  loss_n_40: 4.2088 (4.2317)  loss_n_60: 4.6324 (4.5985)  loss_n_80: 4.7857 (4.7414)  loss_n_100: 4.8239 (4.7877)  triple_100: 487.2425 (498.8623)  triple_80: 489.8384 (501.5307)  triple_60: 486.5327 (498.2264)  triple_40: 458.6588 (473.1984)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1050/1724]  eta: 0:44:12  lr: 0.000000  loss: 2015.0797 (1990.8470)  loss_n_40: 4.1573 (4.2311)  loss_n_60: 4.5455 (4.5975)  loss_n_80: 4.6787 (4.7403)  loss_n_100: 4.7303 (4.7866)  triple_100: 505.3214 (499.0289)  triple_80: 508.3557 (501.6955)  triple_60: 504.7195 (498.3923)  triple_40: 478.1419 (473.3747)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1060/1724]  eta: 0:43:33  lr: 0.000000  loss: 2033.9675 (1991.2543)  loss_n_40: 4.1562 (4.2308)  loss_n_60: 4.5182 (4.5972)  loss_n_80: 4.6749 (4.7400)  loss_n_100: 4.7267 (4.7863)  triple_100: 510.1438 (499.1288)  triple_80: 512.9196 (501.7964)  triple_60: 509.2697 (498.4944)  triple_40: 483.5931 (473.4804)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1070/1724]  eta: 0:42:53  lr: 0.000000  loss: 1953.8090 (1991.7938)  loss_n_40: 4.1562 (4.2301)  loss_n_60: 4.5534 (4.5964)  loss_n_80: 4.6749 (4.7392)  loss_n_100: 4.7267 (4.7856)  triple_100: 490.3898 (499.2626)  triple_80: 493.0124 (501.9298)  triple_60: 489.1684 (498.6299)  triple_40: 464.3018 (473.6204)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1080/1724]  eta: 0:42:14  lr: 0.000000  loss: 1983.2780 (1991.8343)  loss_n_40: 4.1457 (4.2299)  loss_n_60: 4.5534 (4.5963)  loss_n_80: 4.6817 (4.7391)  loss_n_100: 4.7217 (4.7856)  triple_100: 496.9644 (499.2743)  triple_80: 500.1654 (501.9397)  triple_60: 496.8373 (498.6397)  triple_40: 470.3564 (473.6297)  time: 3.9269  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [1090/1724]  eta: 0:41:34  lr: 0.000000  loss: 1983.2780 (1992.0548)  loss_n_40: 4.1804 (4.2301)  loss_n_60: 4.6232 (4.5964)  loss_n_80: 4.7899 (4.7391)  loss_n_100: 4.8150 (4.7856)  triple_100: 496.9644 (499.3271)  triple_80: 500.1654 (501.9927)  triple_60: 496.8373 (498.6957)  triple_40: 470.2330 (473.6881)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1100/1724]  eta: 0:40:55  lr: 0.000000  loss: 1960.6202 (1991.7443)  loss_n_40: 4.2644 (4.2307)  loss_n_60: 4.6539 (4.5972)  loss_n_80: 4.8039 (4.7400)  loss_n_100: 4.8338 (4.7863)  triple_100: 492.1219 (499.2493)  triple_80: 494.6158 (501.9159)  triple_60: 491.1400 (498.6195)  triple_40: 464.7418 (473.6054)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1110/1724]  eta: 0:40:16  lr: 0.000000  loss: 1960.6202 (1991.7442)  loss_n_40: 4.2516 (4.2303)  loss_n_60: 4.6408 (4.5971)  loss_n_80: 4.8009 (4.7400)  loss_n_100: 4.8338 (4.7863)  triple_100: 492.1219 (499.2510)  triple_80: 494.6158 (501.9163)  triple_60: 491.1400 (498.6198)  triple_40: 464.7418 (473.6035)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1120/1724]  eta: 0:39:36  lr: 0.000000  loss: 1976.5850 (1991.7895)  loss_n_40: 4.2516 (4.2303)  loss_n_60: 4.6581 (4.5971)  loss_n_80: 4.7938 (4.7398)  loss_n_100: 4.8215 (4.7862)  triple_100: 495.5605 (499.2612)  triple_80: 497.9865 (501.9263)  triple_60: 494.4929 (498.6308)  triple_40: 469.6782 (473.6179)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1130/1724]  eta: 0:38:57  lr: 0.000000  loss: 1995.9163 (1991.9401)  loss_n_40: 4.2845 (4.2313)  loss_n_60: 4.6960 (4.5982)  loss_n_80: 4.8328 (4.7409)  loss_n_100: 4.8671 (4.7872)  triple_100: 500.8349 (499.2948)  triple_80: 504.0469 (501.9624)  triple_60: 500.0326 (498.6682)  triple_40: 475.0354 (473.6572)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1140/1724]  eta: 0:38:17  lr: 0.000000  loss: 2002.5325 (1992.0504)  loss_n_40: 4.2513 (4.2309)  loss_n_60: 4.6155 (4.5976)  loss_n_80: 4.7345 (4.7402)  loss_n_100: 4.7615 (4.7865)  triple_100: 501.4763 (499.3229)  triple_80: 504.9586 (501.9897)  triple_60: 501.2024 (498.6953)  triple_40: 475.0354 (473.6872)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1150/1724]  eta: 0:37:38  lr: 0.000000  loss: 1954.7466 (1991.9639)  loss_n_40: 4.1747 (4.2306)  loss_n_60: 4.5502 (4.5973)  loss_n_80: 4.7035 (4.7400)  loss_n_100: 4.7449 (4.7864)  triple_100: 490.5730 (499.3069)  triple_80: 493.4111 (501.9710)  triple_60: 489.5072 (498.6734)  triple_40: 463.1199 (473.6583)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1160/1724]  eta: 0:36:59  lr: 0.000000  loss: 1976.9724 (1992.1247)  loss_n_40: 4.2006 (4.2305)  loss_n_60: 4.5793 (4.5972)  loss_n_80: 4.7181 (4.7399)  loss_n_100: 4.7533 (4.7863)  triple_100: 496.8056 (499.3474)  triple_80: 499.5968 (502.0112)  triple_60: 495.1241 (498.7144)  triple_40: 467.1290 (473.6978)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1170/1724]  eta: 0:36:19  lr: 0.000000  loss: 1959.1342 (1991.9047)  loss_n_40: 4.2901 (4.2314)  loss_n_60: 4.6036 (4.5979)  loss_n_80: 4.7685 (4.7406)  loss_n_100: 4.8180 (4.7870)  triple_100: 491.8037 (499.2874)  triple_80: 494.1243 (501.9527)  triple_60: 490.5487 (498.6584)  triple_40: 465.9583 (473.6494)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1180/1724]  eta: 0:35:40  lr: 0.000000  loss: 1949.1279 (1991.8179)  loss_n_40: 4.2977 (4.2317)  loss_n_60: 4.6865 (4.5984)  loss_n_80: 4.8412 (4.7411)  loss_n_100: 4.8898 (4.7874)  triple_100: 488.2940 (499.2645)  triple_80: 490.8864 (501.9313)  triple_60: 487.8330 (498.6372)  triple_40: 463.4478 (473.6262)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1190/1724]  eta: 0:35:00  lr: 0.000000  loss: 1990.0422 (1991.8680)  loss_n_40: 4.2635 (4.2317)  loss_n_60: 4.6542 (4.5987)  loss_n_80: 4.8054 (4.7414)  loss_n_100: 4.8471 (4.7878)  triple_100: 498.9238 (499.2796)  triple_80: 501.5719 (501.9458)  triple_60: 498.1811 (498.6502)  triple_40: 472.4100 (473.6329)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1200/1724]  eta: 0:34:21  lr: 0.000000  loss: 1994.7924 (1991.5258)  loss_n_40: 4.2412 (4.2326)  loss_n_60: 4.6033 (4.5996)  loss_n_80: 4.7426 (4.7423)  loss_n_100: 4.7885 (4.7887)  triple_100: 499.2387 (499.1914)  triple_80: 501.8141 (501.8587)  triple_60: 499.2676 (498.5646)  triple_40: 476.2700 (473.5479)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1210/1724]  eta: 0:33:42  lr: 0.000000  loss: 1981.4714 (1991.4796)  loss_n_40: 4.2144 (4.2322)  loss_n_60: 4.5706 (4.5994)  loss_n_80: 4.7031 (4.7421)  loss_n_100: 4.7598 (4.7885)  triple_100: 497.7310 (499.1839)  triple_80: 499.7955 (501.8502)  triple_60: 495.8112 (498.5531)  triple_40: 470.3123 (473.5301)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1220/1724]  eta: 0:33:02  lr: 0.000000  loss: 1979.6970 (1991.5296)  loss_n_40: 4.1954 (4.2323)  loss_n_60: 4.5161 (4.5994)  loss_n_80: 4.6725 (4.7421)  loss_n_100: 4.7326 (4.7884)  triple_100: 496.1826 (499.1945)  triple_80: 499.6310 (501.8617)  triple_60: 495.8112 (498.5652)  triple_40: 468.4312 (473.5460)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1230/1724]  eta: 0:32:23  lr: 0.000000  loss: 1979.6970 (1991.6120)  loss_n_40: 4.2838 (4.2324)  loss_n_60: 4.6767 (4.5995)  loss_n_80: 4.8245 (4.7422)  loss_n_100: 4.8792 (4.7885)  triple_100: 496.1826 (499.2147)  triple_80: 499.3016 (501.8819)  triple_60: 496.0136 (498.5860)  triple_40: 468.4312 (473.5667)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1240/1724]  eta: 0:31:44  lr: 0.000000  loss: 1998.3978 (1991.5246)  loss_n_40: 4.2130 (4.2323)  loss_n_60: 4.6302 (4.5997)  loss_n_80: 4.7831 (4.7424)  loss_n_100: 4.8312 (4.7887)  triple_100: 499.9515 (499.1935)  triple_80: 502.8287 (501.8596)  triple_60: 500.5860 (498.5640)  triple_40: 476.7008 (473.5444)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1250/1724]  eta: 0:31:04  lr: 0.000000  loss: 1987.5347 (1991.3612)  loss_n_40: 4.1995 (4.2322)  loss_n_60: 4.6206 (4.5997)  loss_n_80: 4.7697 (4.7425)  loss_n_100: 4.7925 (4.7889)  triple_100: 497.8543 (499.1551)  triple_80: 500.1125 (501.8209)  triple_60: 497.7087 (498.5241)  triple_40: 473.7252 (473.4978)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1260/1724]  eta: 0:30:25  lr: 0.000000  loss: 1969.6442 (1991.4202)  loss_n_40: 4.1908 (4.2324)  loss_n_60: 4.5621 (4.5999)  loss_n_80: 4.7275 (4.7426)  loss_n_100: 4.7634 (4.7890)  triple_100: 493.5442 (499.1691)  triple_80: 496.6536 (501.8338)  triple_60: 493.2613 (498.5377)  triple_40: 467.1862 (473.5157)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1270/1724]  eta: 0:29:46  lr: 0.000000  loss: 1899.1899 (1990.9508)  loss_n_40: 4.3591 (4.2332)  loss_n_60: 4.7185 (4.6009)  loss_n_80: 4.8624 (4.7437)  loss_n_100: 4.9000 (4.7900)  triple_100: 476.0234 (499.0510)  triple_80: 479.0530 (501.7187)  triple_60: 475.5093 (498.4213)  triple_40: 448.9829 (473.3920)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1280/1724]  eta: 0:29:06  lr: 0.000000  loss: 1963.7817 (1991.1342)  loss_n_40: 4.2424 (4.2326)  loss_n_60: 4.6591 (4.6003)  loss_n_80: 4.8104 (4.7430)  loss_n_100: 4.8450 (4.7893)  triple_100: 492.0703 (499.0983)  triple_80: 495.2491 (501.7644)  triple_60: 491.7685 (498.4663)  triple_40: 465.3912 (473.4401)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1290/1724]  eta: 0:28:27  lr: 0.000000  loss: 1998.1776 (1991.2075)  loss_n_40: 4.2184 (4.2320)  loss_n_60: 4.5469 (4.5998)  loss_n_80: 4.7021 (4.7425)  loss_n_100: 4.7295 (4.7889)  triple_100: 501.4118 (499.1180)  triple_80: 503.9541 (501.7827)  triple_60: 500.5186 (498.4844)  triple_40: 473.9675 (473.4592)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1300/1724]  eta: 0:27:47  lr: 0.000000  loss: 1988.3232 (1991.4894)  loss_n_40: 4.2348 (4.2319)  loss_n_60: 4.6223 (4.5997)  loss_n_80: 4.7693 (4.7424)  loss_n_100: 4.8201 (4.7888)  triple_100: 498.3903 (499.1891)  triple_80: 500.7048 (501.8520)  triple_60: 497.6848 (498.5538)  triple_40: 473.0328 (473.5317)  time: 3.9287  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [1310/1724]  eta: 0:27:08  lr: 0.000000  loss: 2009.1057 (1991.3145)  loss_n_40: 4.1856 (4.2320)  loss_n_60: 4.5771 (4.6001)  loss_n_80: 4.7168 (4.7429)  loss_n_100: 4.7639 (4.7893)  triple_100: 502.8097 (499.1478)  triple_80: 506.0918 (501.8113)  triple_60: 503.4348 (498.5107)  triple_40: 477.5263 (473.4805)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1320/1724]  eta: 0:26:29  lr: 0.000000  loss: 1974.9828 (1991.3481)  loss_n_40: 4.1893 (4.2316)  loss_n_60: 4.5441 (4.5992)  loss_n_80: 4.7147 (4.7420)  loss_n_100: 4.7639 (4.7885)  triple_100: 495.4996 (499.1555)  triple_80: 498.0236 (501.8166)  triple_60: 494.5181 (498.5175)  triple_40: 468.9613 (473.4972)  time: 3.9276  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1330/1724]  eta: 0:25:49  lr: 0.000000  loss: 1974.9828 (1991.4269)  loss_n_40: 4.0861 (4.2312)  loss_n_60: 4.4901 (4.5986)  loss_n_80: 4.6542 (4.7414)  loss_n_100: 4.7150 (4.7879)  triple_100: 495.4186 (499.1761)  triple_80: 497.6017 (501.8358)  triple_60: 494.5181 (498.5359)  triple_40: 468.9613 (473.5200)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1340/1724]  eta: 0:25:10  lr: 0.000000  loss: 1920.8647 (1991.2552)  loss_n_40: 4.1982 (4.2312)  loss_n_60: 4.5126 (4.5986)  loss_n_80: 4.6714 (4.7414)  loss_n_100: 4.7262 (4.7880)  triple_100: 483.5472 (499.1333)  triple_80: 485.6818 (501.7937)  triple_60: 480.8789 (498.4931)  triple_40: 452.8580 (473.4760)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1350/1724]  eta: 0:24:31  lr: 0.000000  loss: 1935.8688 (1990.9579)  loss_n_40: 4.2255 (4.2313)  loss_n_60: 4.6236 (4.5988)  loss_n_80: 4.7895 (4.7416)  loss_n_100: 4.8381 (4.7882)  triple_100: 484.5675 (499.0609)  triple_80: 487.6141 (501.7210)  triple_60: 484.7513 (498.4185)  triple_40: 459.7429 (473.3976)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1360/1724]  eta: 0:23:51  lr: 0.000000  loss: 1952.1786 (1991.0859)  loss_n_40: 4.1370 (4.2309)  loss_n_60: 4.5230 (4.5984)  loss_n_80: 4.6691 (4.7413)  loss_n_100: 4.7147 (4.7879)  triple_100: 488.8301 (499.0934)  triple_80: 492.0789 (501.7526)  triple_60: 488.8211 (498.4500)  triple_40: 462.4143 (473.4313)  time: 3.9292  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1370/1724]  eta: 0:23:12  lr: 0.000000  loss: 1943.2826 (1990.9355)  loss_n_40: 4.1572 (4.2312)  loss_n_60: 4.5230 (4.5985)  loss_n_80: 4.6691 (4.7413)  loss_n_100: 4.7147 (4.7879)  triple_100: 488.6439 (499.0513)  triple_80: 490.9416 (501.7130)  triple_60: 486.4475 (498.4120)  triple_40: 459.3651 (473.4002)  time: 3.9293  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1380/1724]  eta: 0:22:33  lr: 0.000000  loss: 1921.1365 (1990.8441)  loss_n_40: 4.2416 (4.2311)  loss_n_60: 4.5645 (4.5985)  loss_n_80: 4.7190 (4.7413)  loss_n_100: 4.7581 (4.7880)  triple_100: 482.5168 (499.0307)  triple_80: 485.4475 (501.6918)  triple_60: 481.1515 (498.3887)  triple_40: 453.7098 (473.3741)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1390/1724]  eta: 0:21:53  lr: 0.000000  loss: 1933.8380 (1990.5355)  loss_n_40: 4.2145 (4.2314)  loss_n_60: 4.5926 (4.5989)  loss_n_80: 4.7395 (4.7417)  loss_n_100: 4.7939 (4.7884)  triple_100: 485.7325 (498.9540)  triple_80: 488.6751 (501.6154)  triple_60: 484.6131 (498.3116)  triple_40: 456.6259 (473.2940)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1400/1724]  eta: 0:21:14  lr: 0.000000  loss: 1934.4484 (1990.5486)  loss_n_40: 4.2145 (4.2315)  loss_n_60: 4.5882 (4.5989)  loss_n_80: 4.7395 (4.7417)  loss_n_100: 4.7858 (4.7884)  triple_100: 485.2748 (498.9551)  triple_80: 488.1888 (501.6169)  triple_60: 484.1444 (498.3142)  triple_40: 457.5290 (473.3018)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1410/1724]  eta: 0:20:35  lr: 0.000000  loss: 1988.1046 (1990.7106)  loss_n_40: 4.1938 (4.2313)  loss_n_60: 4.5722 (4.5987)  loss_n_80: 4.7114 (4.7415)  loss_n_100: 4.7641 (4.7881)  triple_100: 499.5955 (498.9945)  triple_80: 501.9812 (501.6567)  triple_60: 498.3288 (498.3543)  triple_40: 470.1887 (473.3456)  time: 3.9289  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1420/1724]  eta: 0:19:55  lr: 0.000000  loss: 1975.1484 (1990.7086)  loss_n_40: 4.2376 (4.2316)  loss_n_60: 4.6132 (4.5990)  loss_n_80: 4.7719 (4.7418)  loss_n_100: 4.8329 (4.7884)  triple_100: 495.1080 (498.9956)  triple_80: 498.0920 (501.6569)  triple_60: 494.8556 (498.3538)  triple_40: 468.1907 (473.3416)  time: 3.9298  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1430/1724]  eta: 0:19:16  lr: 0.000000  loss: 1926.5892 (1990.5260)  loss_n_40: 4.2521 (4.2317)  loss_n_60: 4.6589 (4.5991)  loss_n_80: 4.8032 (4.7419)  loss_n_100: 4.8584 (4.7885)  triple_100: 483.6414 (498.9487)  triple_80: 486.2066 (501.6102)  triple_60: 482.4582 (498.3078)  triple_40: 455.7138 (473.2982)  time: 3.9298  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1440/1724]  eta: 0:18:37  lr: 0.000000  loss: 1996.2324 (1990.8721)  loss_n_40: 4.2011 (4.2311)  loss_n_60: 4.5364 (4.5980)  loss_n_80: 4.6829 (4.7408)  loss_n_100: 4.7368 (4.7875)  triple_100: 501.0216 (499.0343)  triple_80: 503.8309 (501.6950)  triple_60: 500.0305 (498.3944)  triple_40: 473.5794 (473.3909)  time: 3.9298  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1450/1724]  eta: 0:17:57  lr: 0.000000  loss: 2049.2827 (1991.2480)  loss_n_40: 4.1977 (4.2309)  loss_n_60: 4.5028 (4.5977)  loss_n_80: 4.6406 (4.7405)  loss_n_100: 4.6846 (4.7872)  triple_100: 513.5219 (499.1270)  triple_80: 515.9760 (501.7869)  triple_60: 513.1281 (498.4881)  triple_40: 488.4459 (473.4897)  time: 3.9308  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1460/1724]  eta: 0:17:18  lr: 0.000000  loss: 2011.9208 (1991.2059)  loss_n_40: 4.2233 (4.2308)  loss_n_60: 4.5521 (4.5975)  loss_n_80: 4.6732 (4.7402)  loss_n_100: 4.7229 (4.7869)  triple_100: 505.8261 (499.1172)  triple_80: 508.0722 (501.7760)  triple_60: 503.9871 (498.4782)  triple_40: 476.0420 (473.4792)  time: 3.9311  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1470/1724]  eta: 0:16:39  lr: 0.000000  loss: 1985.9561 (1990.8798)  loss_n_40: 4.2588 (4.2315)  loss_n_60: 4.6502 (4.5983)  loss_n_80: 4.7782 (4.7410)  loss_n_100: 4.8214 (4.7876)  triple_100: 497.6629 (499.0342)  triple_80: 499.8402 (501.6945)  triple_60: 496.6389 (498.3964)  triple_40: 473.8665 (473.3963)  time: 3.9315  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1480/1724]  eta: 0:15:59  lr: 0.000000  loss: 2001.7509 (1990.9745)  loss_n_40: 4.2179 (4.2312)  loss_n_60: 4.6057 (4.5979)  loss_n_80: 4.7577 (4.7406)  loss_n_100: 4.7969 (4.7873)  triple_100: 501.2042 (499.0583)  triple_80: 504.0873 (501.7180)  triple_60: 501.0165 (498.4200)  triple_40: 474.3665 (473.4212)  time: 3.9331  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1490/1724]  eta: 0:15:20  lr: 0.000000  loss: 2003.9607 (1991.1514)  loss_n_40: 4.1538 (4.2307)  loss_n_60: 4.5709 (4.5978)  loss_n_80: 4.7390 (4.7406)  loss_n_100: 4.7870 (4.7872)  triple_100: 502.9042 (499.1048)  triple_80: 505.6711 (501.7645)  triple_60: 501.8506 (498.4643)  triple_40: 474.3665 (473.4616)  time: 3.9326  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1500/1724]  eta: 0:14:41  lr: 0.000000  loss: 1992.3118 (1991.1855)  loss_n_40: 4.1765 (4.2310)  loss_n_60: 4.5796 (4.5980)  loss_n_80: 4.7549 (4.7408)  loss_n_100: 4.7945 (4.7875)  triple_100: 499.1660 (499.1120)  triple_80: 501.5828 (501.7722)  triple_60: 498.9957 (498.4732)  triple_40: 472.8553 (473.4708)  time: 3.9296  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1510/1724]  eta: 0:14:01  lr: 0.000000  loss: 1992.3118 (1991.0132)  loss_n_40: 4.3595 (4.2316)  loss_n_60: 4.7165 (4.5988)  loss_n_80: 4.8577 (4.7416)  loss_n_100: 4.9007 (4.7882)  triple_100: 499.1660 (499.0678)  triple_80: 501.5828 (501.7287)  triple_60: 498.9957 (498.4300)  triple_40: 472.8553 (473.4264)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1520/1724]  eta: 0:13:22  lr: 0.000000  loss: 2029.6315 (1991.1929)  loss_n_40: 4.2341 (4.2312)  loss_n_60: 4.6387 (4.5984)  loss_n_80: 4.7641 (4.7413)  loss_n_100: 4.7852 (4.7879)  triple_100: 509.7642 (499.1152)  triple_80: 512.0146 (501.7748)  triple_60: 508.0898 (498.4748)  triple_40: 482.0464 (473.4694)  time: 3.9287  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [1530/1724]  eta: 0:12:43  lr: 0.000000  loss: 1959.8950 (1990.9102)  loss_n_40: 4.2341 (4.2313)  loss_n_60: 4.5869 (4.5985)  loss_n_80: 4.7268 (4.7414)  loss_n_100: 4.7717 (4.7880)  triple_100: 490.7089 (499.0441)  triple_80: 493.6964 (501.7038)  triple_60: 490.8183 (498.4035)  triple_40: 465.4245 (473.3995)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1540/1724]  eta: 0:12:03  lr: 0.000000  loss: 1971.8412 (1991.1233)  loss_n_40: 4.2582 (4.2314)  loss_n_60: 4.6343 (4.5986)  loss_n_80: 4.7777 (4.7415)  loss_n_100: 4.8192 (4.7881)  triple_100: 495.7890 (499.0981)  triple_80: 498.3055 (501.7583)  triple_60: 493.6073 (498.4566)  triple_40: 465.8057 (473.4508)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1550/1724]  eta: 0:11:24  lr: 0.000000  loss: 1961.1583 (1990.8295)  loss_n_40: 4.2341 (4.2315)  loss_n_60: 4.6343 (4.5988)  loss_n_80: 4.7777 (4.7417)  loss_n_100: 4.8192 (4.7883)  triple_100: 491.9676 (499.0255)  triple_80: 494.4406 (501.6858)  triple_60: 490.8053 (498.3837)  triple_40: 465.5458 (473.3742)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1560/1724]  eta: 0:10:45  lr: 0.000000  loss: 1953.7228 (1991.1114)  loss_n_40: 4.2098 (4.2313)  loss_n_60: 4.6293 (4.5985)  loss_n_80: 4.7857 (4.7413)  loss_n_100: 4.8190 (4.7879)  triple_100: 490.3782 (499.0933)  triple_80: 492.8048 (501.7549)  triple_60: 489.4312 (498.4541)  triple_40: 463.1257 (473.4500)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1570/1724]  eta: 0:10:05  lr: 0.000000  loss: 1946.0547 (1990.8192)  loss_n_40: 4.2327 (4.2319)  loss_n_60: 4.6378 (4.5989)  loss_n_80: 4.7918 (4.7416)  loss_n_100: 4.8339 (4.7882)  triple_100: 486.4542 (499.0169)  triple_80: 489.6267 (501.6795)  triple_60: 487.2957 (498.3809)  triple_40: 463.3921 (473.3814)  time: 3.9271  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1580/1724]  eta: 0:09:26  lr: 0.000000  loss: 1946.0547 (1990.8170)  loss_n_40: 4.3373 (4.2324)  loss_n_60: 4.6820 (4.5993)  loss_n_80: 4.8118 (4.7420)  loss_n_100: 4.8441 (4.7885)  triple_100: 486.2342 (499.0121)  triple_80: 489.6267 (501.6761)  triple_60: 487.2957 (498.3795)  triple_40: 463.3921 (473.3871)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1590/1724]  eta: 0:08:47  lr: 0.000000  loss: 2017.7545 (1991.0252)  loss_n_40: 4.2786 (4.2322)  loss_n_60: 4.6056 (4.5989)  loss_n_80: 4.7579 (4.7416)  loss_n_100: 4.8094 (4.7881)  triple_100: 503.3800 (499.0633)  triple_80: 507.1226 (501.7272)  triple_60: 505.0939 (498.4311)  triple_40: 482.6004 (473.4427)  time: 3.9283  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1600/1724]  eta: 0:08:07  lr: 0.000000  loss: 2018.5582 (1991.4778)  loss_n_40: 4.1268 (4.2315)  loss_n_60: 4.5217 (4.5980)  loss_n_80: 4.6288 (4.7407)  loss_n_100: 4.6722 (4.7873)  triple_100: 505.9967 (499.1760)  triple_80: 508.0915 (501.8386)  triple_60: 505.0939 (498.5438)  triple_40: 482.6004 (473.5620)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1610/1724]  eta: 0:07:28  lr: 0.000000  loss: 2018.5582 (1991.4687)  loss_n_40: 4.1715 (4.2313)  loss_n_60: 4.5450 (4.5977)  loss_n_80: 4.6688 (4.7404)  loss_n_100: 4.7221 (4.7870)  triple_100: 505.9967 (499.1725)  triple_80: 508.0915 (501.8346)  triple_60: 504.9672 (498.5401)  triple_40: 482.0464 (473.5650)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1620/1724]  eta: 0:06:49  lr: 0.000000  loss: 2031.4668 (1991.6971)  loss_n_40: 4.1937 (4.2308)  loss_n_60: 4.5460 (4.5970)  loss_n_80: 4.6756 (4.7397)  loss_n_100: 4.7324 (4.7864)  triple_100: 509.0764 (499.2308)  triple_80: 511.7441 (501.8914)  triple_60: 508.1374 (498.5961)  triple_40: 484.5848 (473.6247)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1630/1724]  eta: 0:06:09  lr: 0.000000  loss: 2026.8613 (1991.8499)  loss_n_40: 4.1853 (4.2309)  loss_n_60: 4.5525 (4.5969)  loss_n_80: 4.7170 (4.7396)  loss_n_100: 4.7700 (4.7863)  triple_100: 508.1824 (499.2677)  triple_80: 510.8927 (501.9283)  triple_60: 507.3853 (498.6343)  triple_40: 481.9597 (473.6659)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1640/1724]  eta: 0:05:30  lr: 0.000000  loss: 2016.0699 (1991.8448)  loss_n_40: 4.1782 (4.2306)  loss_n_60: 4.5453 (4.5965)  loss_n_80: 4.7051 (4.7392)  loss_n_100: 4.7609 (4.7859)  triple_100: 504.1037 (499.2667)  triple_80: 507.1498 (501.9273)  triple_60: 504.7280 (498.6334)  triple_40: 481.0272 (473.6652)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1650/1724]  eta: 0:04:51  lr: 0.000000  loss: 1930.0673 (1991.5810)  loss_n_40: 4.2141 (4.2311)  loss_n_60: 4.5881 (4.5970)  loss_n_80: 4.7264 (4.7397)  loss_n_100: 4.7647 (4.7864)  triple_100: 483.9200 (499.2022)  triple_80: 486.8123 (501.8637)  triple_60: 483.4486 (498.5680)  triple_40: 457.0573 (473.5930)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1660/1724]  eta: 0:04:11  lr: 0.000000  loss: 1930.0673 (1991.4355)  loss_n_40: 4.2885 (4.2312)  loss_n_60: 4.6996 (4.5975)  loss_n_80: 4.8435 (4.7402)  loss_n_100: 4.8881 (4.7869)  triple_100: 483.9200 (499.1678)  triple_80: 486.8123 (501.8297)  triple_60: 483.4486 (498.5323)  triple_40: 457.0573 (473.5500)  time: 3.9298  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1670/1724]  eta: 0:03:32  lr: 0.000000  loss: 1962.3323 (1991.3649)  loss_n_40: 4.1862 (4.2308)  loss_n_60: 4.6385 (4.5972)  loss_n_80: 4.7913 (4.7400)  loss_n_100: 4.8229 (4.7868)  triple_100: 492.8094 (499.1533)  triple_80: 495.6190 (501.8148)  triple_60: 491.5358 (498.5143)  triple_40: 463.9781 (473.5276)  time: 3.9308  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1680/1724]  eta: 0:02:53  lr: 0.000000  loss: 2012.0391 (1991.7353)  loss_n_40: 4.1862 (4.2304)  loss_n_60: 4.5767 (4.5968)  loss_n_80: 4.7589 (4.7396)  loss_n_100: 4.7904 (4.7863)  triple_100: 503.8521 (499.2471)  triple_80: 506.8940 (501.9074)  triple_60: 503.6747 (498.6060)  triple_40: 479.8713 (473.6216)  time: 3.9304  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1690/1724]  eta: 0:02:13  lr: 0.000000  loss: 2011.6106 (1991.5474)  loss_n_40: 4.2074 (4.2307)  loss_n_60: 4.6078 (4.5972)  loss_n_80: 4.7589 (4.7399)  loss_n_100: 4.7904 (4.7866)  triple_100: 503.7755 (499.2006)  triple_80: 506.2352 (501.8613)  triple_60: 503.0130 (498.5591)  triple_40: 478.5472 (473.5719)  time: 3.9297  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1700/1724]  eta: 0:01:34  lr: 0.000000  loss: 1931.3734 (1991.4897)  loss_n_40: 4.1779 (4.2303)  loss_n_60: 4.5809 (4.5966)  loss_n_80: 4.7363 (4.7394)  loss_n_100: 4.7849 (4.7862)  triple_100: 484.2824 (499.1862)  triple_80: 487.3435 (501.8456)  triple_60: 483.6026 (498.5440)  triple_40: 457.6035 (473.5612)  time: 3.9293  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1710/1724]  eta: 0:00:55  lr: 0.000000  loss: 1953.7223 (1991.4156)  loss_n_40: 4.1047 (4.2299)  loss_n_60: 4.5063 (4.5961)  loss_n_80: 4.6687 (4.7389)  loss_n_100: 4.7209 (4.7857)  triple_100: 488.9741 (499.1694)  triple_80: 491.2462 (501.8278)  triple_60: 488.7722 (498.5257)  triple_40: 466.1252 (473.5422)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1720/1724]  eta: 0:00:15  lr: 0.000000  loss: 1922.8508 (1991.1302)  loss_n_40: 4.1069 (4.2296)  loss_n_60: 4.5063 (4.5960)  loss_n_80: 4.6695 (4.7389)  loss_n_100: 4.7209 (4.7857)  triple_100: 483.6787 (499.1002)  triple_80: 485.3757 (501.7583)  triple_60: 481.2212 (498.4546)  triple_40: 456.6706 (473.4670)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0]  [1723/1724]  eta: 0:00:03  lr: 0.000000  loss: 1917.2306 (1990.9047)  loss_n_40: 4.1649 (4.2301)  loss_n_60: 4.5319 (4.5965)  loss_n_80: 4.6806 (4.7394)  loss_n_100: 4.7422 (4.7862)  triple_100: 481.6541 (499.0428)  triple_80: 484.3489 (501.7019)  triple_60: 480.2959 (498.3985)  triple_40: 452.9257 (473.4093)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:0] Total time: 1:53:00 (3.9328 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 1917.2306 (1990.9047)  loss_n_40: 4.1649 (4.2301)  loss_n_60: 4.5319 (4.5965)  loss_n_80: 4.6806 (4.7394)  loss_n_100: 4.7422 (4.7862)  triple_100: 481.6541 (499.0428)  triple_80: 484.3489 (501.7019)  triple_60: 480.2959 (498.3985)  triple_40: 452.9257 (473.4093)\n",
      "Valid: [epoch:0]  [  0/845]  eta: 0:09:48  loss: 2048.4312 (2048.4312)  loss_n_40: 4.0832 (4.0832)  loss_n_60: 4.6028 (4.6028)  loss_n_80: 4.8059 (4.8059)  loss_n_100: 4.8198 (4.8198)  triple_100: 514.9633 (514.9633)  triple_80: 517.3455 (517.3455)  triple_60: 512.7903 (512.7903)  triple_40: 485.0204 (485.0204)  time: 0.6965  data: 0.3604  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:0]  [ 10/845]  eta: 0:05:06  loss: 1983.4667 (2112.6789)  loss_n_40: 4.0753 (4.0011)  loss_n_60: 4.5065 (4.3172)  loss_n_80: 4.7022 (4.4704)  loss_n_100: 4.7420 (4.5341)  triple_100: 497.5216 (530.1279)  triple_80: 500.4344 (532.0821)  triple_60: 496.8157 (528.6332)  triple_40: 469.0607 (504.5129)  time: 0.3674  data: 0.0329  max mem: 46473\n",
      "Valid: [epoch:0]  [ 20/845]  eta: 0:04:50  loss: 1903.5430 (2035.6949)  loss_n_40: 4.1224 (4.1767)  loss_n_60: 4.5202 (4.5187)  loss_n_80: 4.7022 (4.6742)  loss_n_100: 4.7420 (4.7295)  triple_100: 479.7825 (510.6011)  triple_80: 481.6069 (512.8927)  triple_60: 476.8196 (509.5216)  triple_40: 449.9191 (484.5802)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [ 30/845]  eta: 0:04:42  loss: 1871.8983 (2018.2978)  loss_n_40: 4.2509 (4.1887)  loss_n_60: 4.5510 (4.5317)  loss_n_80: 4.7380 (4.6832)  loss_n_100: 4.7782 (4.7377)  triple_100: 470.7992 (506.2460)  triple_80: 472.9684 (508.5875)  triple_60: 469.0456 (505.2558)  triple_40: 440.8134 (480.0672)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [ 40/845]  eta: 0:04:36  loss: 1899.4661 (2039.2410)  loss_n_40: 4.2003 (4.1711)  loss_n_60: 4.5446 (4.4993)  loss_n_80: 4.6938 (4.6428)  loss_n_100: 4.7646 (4.6989)  triple_100: 474.3533 (511.2559)  triple_80: 478.1764 (513.6031)  triple_60: 476.2185 (510.4574)  triple_40: 450.6957 (485.9125)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [ 50/845]  eta: 0:04:31  loss: 2058.8149 (2063.3185)  loss_n_40: 4.0251 (4.1471)  loss_n_60: 4.4776 (4.4640)  loss_n_80: 4.6287 (4.6085)  loss_n_100: 4.6970 (4.6655)  triple_100: 517.3338 (517.2034)  triple_80: 519.9362 (519.5267)  triple_60: 515.7485 (516.3726)  triple_40: 487.2383 (492.3307)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [ 60/845]  eta: 0:04:27  loss: 1996.7024 (2062.2907)  loss_n_40: 4.0345 (4.1660)  loss_n_60: 4.5259 (4.4695)  loss_n_80: 4.6997 (4.6104)  loss_n_100: 4.7222 (4.6656)  triple_100: 500.9987 (516.7820)  triple_80: 503.8869 (519.1679)  triple_60: 499.9820 (516.1199)  triple_40: 471.3739 (492.3093)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [ 70/845]  eta: 0:04:23  loss: 1871.6254 (2042.1092)  loss_n_40: 4.0882 (4.1684)  loss_n_60: 4.5639 (4.4897)  loss_n_80: 4.7370 (4.6334)  loss_n_100: 4.7751 (4.6874)  triple_100: 468.7391 (511.8966)  triple_80: 472.2984 (514.2885)  triple_60: 468.9471 (511.1029)  triple_40: 440.9450 (486.8422)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [ 80/845]  eta: 0:04:19  loss: 1855.4419 (2026.4476)  loss_n_40: 4.1666 (4.1917)  loss_n_60: 4.5706 (4.5233)  loss_n_80: 4.7427 (4.6674)  loss_n_100: 4.7888 (4.7182)  triple_100: 465.6852 (507.9911)  triple_80: 468.2371 (510.4330)  triple_60: 464.4303 (507.2197)  triple_40: 439.9268 (482.7031)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [ 90/845]  eta: 0:04:15  loss: 1860.3789 (2025.3550)  loss_n_40: 4.4062 (4.2033)  loss_n_60: 4.6779 (4.5304)  loss_n_80: 4.8097 (4.6708)  loss_n_100: 4.8765 (4.7226)  triple_100: 466.2171 (507.6070)  triple_80: 469.3856 (510.1127)  triple_60: 466.1849 (506.9545)  triple_40: 439.7834 (482.5536)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [100/845]  eta: 0:04:12  loss: 1846.7933 (2018.5911)  loss_n_40: 4.3095 (4.2061)  loss_n_60: 4.6046 (4.5291)  loss_n_80: 4.7476 (4.6702)  loss_n_100: 4.7893 (4.7219)  triple_100: 464.4240 (505.9206)  triple_80: 466.5932 (508.4409)  triple_60: 462.2752 (505.2834)  triple_40: 439.0526 (480.8188)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [110/845]  eta: 0:04:08  loss: 1911.6005 (2025.7852)  loss_n_40: 4.2334 (4.1949)  loss_n_60: 4.5427 (4.5135)  loss_n_80: 4.7040 (4.6528)  loss_n_100: 4.7581 (4.7043)  triple_100: 479.8894 (507.7071)  triple_80: 482.6041 (510.2270)  triple_60: 478.3878 (507.0845)  triple_40: 452.1700 (482.7011)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [120/845]  eta: 0:04:04  loss: 1885.4819 (2011.5603)  loss_n_40: 4.1353 (4.1977)  loss_n_60: 4.5427 (4.5222)  loss_n_80: 4.6913 (4.6626)  loss_n_100: 4.7373 (4.7138)  triple_100: 472.6382 (504.2214)  triple_80: 475.6257 (506.7469)  triple_60: 472.5628 (503.5445)  triple_40: 442.8483 (478.9512)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [130/845]  eta: 0:04:01  loss: 1879.9594 (2006.4454)  loss_n_40: 4.2236 (4.2171)  loss_n_60: 4.5840 (4.5466)  loss_n_80: 4.7456 (4.6861)  loss_n_100: 4.8101 (4.7360)  triple_100: 472.6382 (502.9077)  triple_80: 475.1817 (505.4844)  triple_60: 470.1319 (502.2776)  triple_40: 443.1267 (477.5900)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [140/845]  eta: 0:03:57  loss: 1910.0212 (2003.5410)  loss_n_40: 4.3503 (4.2182)  loss_n_60: 4.6455 (4.5552)  loss_n_80: 4.8153 (4.6952)  loss_n_100: 4.8783 (4.7438)  triple_100: 480.5817 (502.2083)  triple_80: 482.6077 (504.7995)  triple_60: 477.8752 (501.5546)  triple_40: 450.6115 (476.7663)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [150/845]  eta: 0:03:54  loss: 1910.0212 (1996.5638)  loss_n_40: 4.3591 (4.2369)  loss_n_60: 4.6930 (4.5779)  loss_n_80: 4.8732 (4.7182)  loss_n_100: 4.8978 (4.7658)  triple_100: 480.5817 (500.4598)  triple_80: 482.6077 (503.0826)  triple_60: 477.8752 (499.8184)  triple_40: 450.6115 (474.9041)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [160/845]  eta: 0:03:50  loss: 1890.0282 (1999.2396)  loss_n_40: 4.3125 (4.2237)  loss_n_60: 4.6596 (4.5624)  loss_n_80: 4.8180 (4.7035)  loss_n_100: 4.8861 (4.7529)  triple_100: 474.2975 (501.1762)  triple_80: 477.2000 (503.7423)  triple_60: 473.4354 (500.4713)  triple_40: 446.0478 (475.6072)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [170/845]  eta: 0:03:47  loss: 1890.0282 (1997.2548)  loss_n_40: 4.1171 (4.2214)  loss_n_60: 4.5181 (4.5636)  loss_n_80: 4.6886 (4.7061)  loss_n_100: 4.7447 (4.7551)  triple_100: 475.2241 (500.7178)  triple_80: 477.2000 (503.2817)  triple_60: 473.0609 (499.9770)  triple_40: 446.0478 (475.0321)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [180/845]  eta: 0:03:43  loss: 1892.1882 (1994.5436)  loss_n_40: 4.1260 (4.2266)  loss_n_60: 4.5548 (4.5729)  loss_n_80: 4.7280 (4.7151)  loss_n_100: 4.7703 (4.7630)  triple_100: 475.2241 (500.0407)  triple_80: 477.8071 (502.6271)  triple_60: 473.8872 (499.3155)  triple_40: 447.3120 (474.2826)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [190/845]  eta: 0:03:40  loss: 1891.3206 (1989.5090)  loss_n_40: 4.2403 (4.2344)  loss_n_60: 4.6204 (4.5830)  loss_n_80: 4.7781 (4.7254)  loss_n_100: 4.8353 (4.7730)  triple_100: 474.4899 (498.7907)  triple_80: 477.0891 (501.3896)  triple_60: 473.9302 (498.0632)  triple_40: 447.5408 (472.9496)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [200/845]  eta: 0:03:37  loss: 1884.5027 (1990.5232)  loss_n_40: 4.2367 (4.2344)  loss_n_60: 4.5755 (4.5866)  loss_n_80: 4.7653 (4.7290)  loss_n_100: 4.7611 (4.7760)  triple_100: 471.0329 (499.0326)  triple_80: 474.5363 (501.6495)  triple_60: 471.6218 (498.3282)  triple_40: 447.3323 (473.1868)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [210/845]  eta: 0:03:33  loss: 1944.9332 (1993.8505)  loss_n_40: 4.0413 (4.2259)  loss_n_60: 4.5351 (4.5811)  loss_n_80: 4.7100 (4.7245)  loss_n_100: 4.7556 (4.7724)  triple_100: 486.4743 (499.8956)  triple_80: 489.8636 (502.4975)  triple_60: 487.0925 (499.1623)  triple_40: 460.7567 (473.9911)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [220/845]  eta: 0:03:30  loss: 1944.9332 (1996.6238)  loss_n_40: 4.0511 (4.2251)  loss_n_60: 4.5226 (4.5790)  loss_n_80: 4.6905 (4.7224)  loss_n_100: 4.7366 (4.7703)  triple_100: 486.4743 (500.6011)  triple_80: 489.8636 (503.1924)  triple_60: 487.0925 (499.8455)  triple_40: 460.5973 (474.6880)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [230/845]  eta: 0:03:26  loss: 1924.1069 (1997.8660)  loss_n_40: 4.0580 (4.2259)  loss_n_60: 4.5296 (4.5817)  loss_n_80: 4.6988 (4.7250)  loss_n_100: 4.7366 (4.7724)  triple_100: 481.1909 (500.8924)  triple_80: 484.9615 (503.4823)  triple_60: 481.7031 (500.1487)  triple_40: 455.1182 (475.0375)  time: 0.3348  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:0]  [240/845]  eta: 0:03:23  loss: 1872.1250 (1995.1662)  loss_n_40: 4.2307 (4.2323)  loss_n_60: 4.6040 (4.5859)  loss_n_80: 4.7762 (4.7289)  loss_n_100: 4.8201 (4.7765)  triple_100: 467.1151 (500.2022)  triple_80: 470.6974 (502.7975)  triple_60: 468.8766 (499.4729)  triple_40: 444.2668 (474.3701)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [250/845]  eta: 0:03:20  loss: 1875.1837 (1991.2167)  loss_n_40: 4.2769 (4.2335)  loss_n_60: 4.6040 (4.5891)  loss_n_80: 4.7762 (4.7326)  loss_n_100: 4.8239 (4.7801)  triple_100: 470.1454 (499.2324)  triple_80: 473.7277 (501.8362)  triple_60: 469.3795 (498.4872)  triple_40: 442.2248 (473.3256)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [260/845]  eta: 0:03:16  loss: 1904.8696 (1991.5446)  loss_n_40: 4.3227 (4.2395)  loss_n_60: 4.6523 (4.5961)  loss_n_80: 4.7925 (4.7391)  loss_n_100: 4.8775 (4.7859)  triple_100: 477.7754 (499.2953)  triple_80: 480.3401 (501.9162)  triple_60: 477.0475 (498.5775)  triple_40: 451.1259 (473.3950)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [270/845]  eta: 0:03:13  loss: 1926.5813 (1994.1332)  loss_n_40: 4.3125 (4.2352)  loss_n_60: 4.5990 (4.5885)  loss_n_80: 4.7763 (4.7316)  loss_n_100: 4.8198 (4.7788)  triple_100: 482.2239 (499.9334)  triple_80: 485.7092 (502.5550)  triple_60: 482.3180 (499.2196)  triple_40: 455.6984 (474.0911)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [280/845]  eta: 0:03:09  loss: 1897.0801 (1992.7414)  loss_n_40: 4.0710 (4.2310)  loss_n_60: 4.4995 (4.5855)  loss_n_80: 4.6624 (4.7287)  loss_n_100: 4.7197 (4.7762)  triple_100: 475.3441 (499.6064)  triple_80: 478.4704 (502.2253)  triple_60: 474.7341 (498.8774)  triple_40: 449.6949 (473.7109)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [290/845]  eta: 0:03:06  loss: 1884.3542 (1990.9563)  loss_n_40: 4.1330 (4.2346)  loss_n_60: 4.5152 (4.5906)  loss_n_80: 4.6703 (4.7334)  loss_n_100: 4.7329 (4.7809)  triple_100: 472.8517 (499.1615)  triple_80: 475.6776 (501.7856)  triple_60: 472.4044 (498.4398)  triple_40: 446.6611 (473.2298)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [300/845]  eta: 0:03:03  loss: 1871.8044 (1989.8200)  loss_n_40: 4.3266 (4.2360)  loss_n_60: 4.6669 (4.5918)  loss_n_80: 4.8157 (4.7339)  loss_n_100: 4.8843 (4.7814)  triple_100: 470.7357 (498.8593)  triple_80: 472.9560 (501.4961)  triple_60: 468.6170 (498.1596)  triple_40: 444.0313 (472.9620)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [310/845]  eta: 0:02:59  loss: 1883.8408 (1990.2982)  loss_n_40: 4.2272 (4.2366)  loss_n_60: 4.5587 (4.5907)  loss_n_80: 4.7127 (4.7326)  loss_n_100: 4.7913 (4.7803)  triple_100: 471.8437 (498.9713)  triple_80: 474.5906 (501.5992)  triple_60: 471.8232 (498.2782)  triple_40: 445.5646 (473.1094)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [320/845]  eta: 0:02:56  loss: 1899.7213 (1990.2731)  loss_n_40: 4.1933 (4.2358)  loss_n_60: 4.5522 (4.5911)  loss_n_80: 4.7127 (4.7333)  loss_n_100: 4.7705 (4.7808)  triple_100: 476.2328 (498.9716)  triple_80: 479.3041 (501.6046)  triple_60: 476.0944 (498.2801)  triple_40: 447.4991 (473.0758)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [330/845]  eta: 0:02:52  loss: 1941.5389 (1989.1973)  loss_n_40: 4.1933 (4.2404)  loss_n_60: 4.5666 (4.5981)  loss_n_80: 4.7490 (4.7402)  loss_n_100: 4.7736 (4.7871)  triple_100: 485.3242 (498.6870)  triple_80: 489.0265 (501.3384)  triple_60: 486.2156 (498.0134)  triple_40: 459.4558 (472.7927)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [340/845]  eta: 0:02:49  loss: 1925.8037 (1991.1214)  loss_n_40: 4.2196 (4.2402)  loss_n_60: 4.5627 (4.5960)  loss_n_80: 4.7003 (4.7372)  loss_n_100: 4.7668 (4.7844)  triple_100: 484.4495 (499.1500)  triple_80: 487.7632 (501.8002)  triple_60: 482.1234 (498.4878)  triple_40: 452.7382 (473.3255)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [350/845]  eta: 0:02:46  loss: 1895.3405 (1993.0235)  loss_n_40: 4.1561 (4.2350)  loss_n_60: 4.5407 (4.5887)  loss_n_80: 4.7003 (4.7301)  loss_n_100: 4.7668 (4.7783)  triple_100: 476.0732 (499.6378)  triple_80: 478.8237 (502.2731)  triple_60: 474.9709 (498.9648)  triple_40: 446.3395 (473.8157)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [360/845]  eta: 0:02:42  loss: 1891.2620 (1991.4261)  loss_n_40: 4.2090 (4.2377)  loss_n_60: 4.5530 (4.5915)  loss_n_80: 4.7317 (4.7328)  loss_n_100: 4.8114 (4.7809)  triple_100: 472.4241 (499.2471)  triple_80: 476.5735 (501.8799)  triple_60: 473.4662 (498.5663)  triple_40: 446.1076 (473.3899)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [370/845]  eta: 0:02:39  loss: 1868.8848 (1992.2313)  loss_n_40: 4.1610 (4.2362)  loss_n_60: 4.5649 (4.5897)  loss_n_80: 4.7292 (4.7307)  loss_n_100: 4.7656 (4.7789)  triple_100: 470.1413 (499.4290)  triple_80: 472.3588 (502.0656)  triple_60: 468.0741 (498.7636)  triple_40: 440.0267 (473.6376)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [380/845]  eta: 0:02:36  loss: 1895.4490 (1990.2903)  loss_n_40: 4.1297 (4.2372)  loss_n_60: 4.5486 (4.5916)  loss_n_80: 4.7100 (4.7332)  loss_n_100: 4.7545 (4.7813)  triple_100: 475.2198 (498.9558)  triple_80: 478.3302 (501.6006)  triple_60: 474.8477 (498.2826)  triple_40: 447.7777 (473.1081)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [390/845]  eta: 0:02:32  loss: 1903.9219 (1991.8409)  loss_n_40: 4.1808 (4.2374)  loss_n_60: 4.5918 (4.5908)  loss_n_80: 4.7571 (4.7320)  loss_n_100: 4.7948 (4.7801)  triple_100: 478.1493 (499.3331)  triple_80: 480.9103 (501.9785)  triple_60: 476.4543 (498.6620)  triple_40: 449.7843 (473.5270)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [400/845]  eta: 0:02:29  loss: 1916.5389 (1990.8659)  loss_n_40: 4.3887 (4.2400)  loss_n_60: 4.7025 (4.5955)  loss_n_80: 4.8336 (4.7370)  loss_n_100: 4.8860 (4.7848)  triple_100: 481.8369 (499.0930)  triple_80: 484.4882 (501.7471)  triple_60: 479.8223 (498.4236)  triple_40: 452.1635 (473.2450)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [410/845]  eta: 0:02:26  loss: 1937.1610 (1992.5895)  loss_n_40: 4.1443 (4.2377)  loss_n_60: 4.5685 (4.5933)  loss_n_80: 4.7351 (4.7348)  loss_n_100: 4.7929 (4.7824)  triple_100: 488.2779 (499.5270)  triple_80: 490.2974 (502.1789)  triple_60: 484.9110 (498.8585)  triple_40: 455.8736 (473.6770)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [420/845]  eta: 0:02:22  loss: 1937.1610 (1992.9794)  loss_n_40: 4.1037 (4.2374)  loss_n_60: 4.5758 (4.5940)  loss_n_80: 4.7351 (4.7358)  loss_n_100: 4.7936 (4.7835)  triple_100: 488.2779 (499.6299)  triple_80: 490.2974 (502.2808)  triple_60: 484.9110 (498.9589)  triple_40: 455.8736 (473.7590)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [430/845]  eta: 0:02:19  loss: 1925.0256 (1992.1335)  loss_n_40: 4.4854 (4.2416)  loss_n_60: 4.7706 (4.5993)  loss_n_80: 4.8617 (4.7408)  loss_n_100: 4.8959 (4.7882)  triple_100: 480.9434 (499.4027)  triple_80: 484.5778 (502.0673)  triple_60: 482.1902 (498.7486)  triple_40: 458.2494 (473.5450)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [440/845]  eta: 0:02:15  loss: 1935.6656 (1994.3472)  loss_n_40: 4.1668 (4.2395)  loss_n_60: 4.5262 (4.5963)  loss_n_80: 4.7089 (4.7378)  loss_n_100: 4.7526 (4.7848)  triple_100: 484.1941 (499.9548)  triple_80: 488.2042 (502.6179)  triple_60: 485.1337 (499.2959)  triple_40: 458.5449 (474.1203)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [450/845]  eta: 0:02:12  loss: 1925.1523 (1994.4357)  loss_n_40: 4.0387 (4.2367)  loss_n_60: 4.5168 (4.5935)  loss_n_80: 4.6966 (4.7350)  loss_n_100: 4.7468 (4.7824)  triple_100: 483.1359 (499.9777)  triple_80: 486.2886 (502.6371)  triple_60: 482.3016 (499.3178)  triple_40: 455.1135 (474.1556)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [460/845]  eta: 0:02:09  loss: 1946.0883 (1995.2368)  loss_n_40: 4.0860 (4.2387)  loss_n_60: 4.5343 (4.5955)  loss_n_80: 4.6966 (4.7370)  loss_n_100: 4.7865 (4.7841)  triple_100: 487.3548 (500.1734)  triple_80: 490.5234 (502.8374)  triple_60: 487.3041 (499.5136)  triple_40: 460.4868 (474.3570)  time: 0.3348  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:0]  [470/845]  eta: 0:02:05  loss: 1954.3937 (1996.8621)  loss_n_40: 4.0860 (4.2365)  loss_n_60: 4.5144 (4.5924)  loss_n_80: 4.6800 (4.7342)  loss_n_100: 4.7418 (4.7814)  triple_100: 488.8310 (500.5885)  triple_80: 492.1548 (503.2489)  triple_60: 489.7065 (499.9181)  triple_40: 464.4762 (474.7620)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [480/845]  eta: 0:02:02  loss: 1954.3937 (1997.8336)  loss_n_40: 4.0426 (4.2361)  loss_n_60: 4.5144 (4.5928)  loss_n_80: 4.6885 (4.7349)  loss_n_100: 4.7402 (4.7820)  triple_100: 491.2708 (500.8358)  triple_80: 494.2058 (503.4968)  triple_60: 489.7065 (500.1598)  triple_40: 464.4762 (474.9954)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [490/845]  eta: 0:01:59  loss: 1917.3025 (1998.5189)  loss_n_40: 4.0850 (4.2370)  loss_n_60: 4.5262 (4.5939)  loss_n_80: 4.7095 (4.7357)  loss_n_100: 4.7423 (4.7825)  triple_100: 479.4295 (500.9943)  triple_80: 482.9706 (503.6642)  triple_60: 479.9711 (500.3320)  triple_40: 453.9246 (475.1793)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [500/845]  eta: 0:01:55  loss: 1898.5435 (1996.5918)  loss_n_40: 4.5260 (4.2442)  loss_n_60: 4.9219 (4.6025)  loss_n_80: 5.0261 (4.7441)  loss_n_100: 5.0637 (4.7901)  triple_100: 475.2329 (500.4921)  triple_80: 478.0367 (503.1747)  triple_60: 475.3105 (499.8482)  triple_40: 447.9133 (474.6958)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [510/845]  eta: 0:01:52  loss: 1884.6650 (1996.6794)  loss_n_40: 4.2825 (4.2418)  loss_n_60: 4.6096 (4.6003)  loss_n_80: 4.7496 (4.7419)  loss_n_100: 4.8228 (4.7881)  triple_100: 473.9313 (500.5190)  triple_80: 476.8060 (503.1961)  triple_60: 471.7636 (499.8697)  triple_40: 446.9595 (474.7225)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [520/845]  eta: 0:01:49  loss: 1884.6650 (1996.5000)  loss_n_40: 4.1802 (4.2405)  loss_n_60: 4.5311 (4.5992)  loss_n_80: 4.6972 (4.7410)  loss_n_100: 4.7393 (4.7872)  triple_100: 473.9313 (500.4774)  triple_80: 476.8060 (503.1547)  triple_60: 471.7636 (499.8234)  triple_40: 445.4445 (474.6767)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [530/845]  eta: 0:01:45  loss: 1910.0566 (1997.1434)  loss_n_40: 4.2628 (4.2441)  loss_n_60: 4.6003 (4.6008)  loss_n_80: 4.7195 (4.7421)  loss_n_100: 4.7792 (4.7879)  triple_100: 477.7372 (500.6079)  triple_80: 481.1270 (503.2969)  triple_60: 478.0012 (499.9806)  triple_40: 452.5017 (474.8831)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [540/845]  eta: 0:01:42  loss: 1866.6926 (1995.0709)  loss_n_40: 4.4151 (4.2460)  loss_n_60: 4.6935 (4.6035)  loss_n_80: 4.8183 (4.7449)  loss_n_100: 4.8849 (4.7906)  triple_100: 466.2793 (500.0894)  triple_80: 470.3475 (502.7831)  triple_60: 467.4864 (499.4646)  triple_40: 442.2630 (474.3488)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [550/845]  eta: 0:01:38  loss: 1861.1707 (1993.2942)  loss_n_40: 4.3360 (4.2485)  loss_n_60: 4.6642 (4.6062)  loss_n_80: 4.8069 (4.7475)  loss_n_100: 4.8673 (4.7931)  triple_100: 466.2793 (499.6385)  triple_80: 469.4661 (502.3417)  triple_60: 466.6673 (499.0220)  triple_40: 439.9638 (473.8966)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [560/845]  eta: 0:01:35  loss: 1897.0679 (1994.4613)  loss_n_40: 4.1076 (4.2443)  loss_n_60: 4.5042 (4.6021)  loss_n_80: 4.6660 (4.7434)  loss_n_100: 4.7093 (4.7890)  triple_100: 475.9613 (499.9351)  triple_80: 479.7115 (502.6399)  triple_60: 474.5606 (499.3164)  triple_40: 447.9846 (474.1910)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [570/845]  eta: 0:01:32  loss: 1906.6545 (1993.7055)  loss_n_40: 4.0282 (4.2420)  loss_n_60: 4.5042 (4.6003)  loss_n_80: 4.6657 (4.7419)  loss_n_100: 4.7093 (4.7876)  triple_100: 479.6302 (499.7598)  triple_80: 481.7967 (502.4616)  triple_60: 477.2623 (499.1274)  triple_40: 449.8366 (473.9850)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [580/845]  eta: 0:01:28  loss: 1888.9613 (1992.8139)  loss_n_40: 4.2499 (4.2459)  loss_n_60: 4.5995 (4.6044)  loss_n_80: 4.7647 (4.7458)  loss_n_100: 4.8264 (4.7916)  triple_100: 472.4803 (499.5267)  triple_80: 476.0115 (502.2332)  triple_60: 472.8992 (498.9049)  triple_40: 447.1502 (473.7614)  time: 0.3353  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [590/845]  eta: 0:01:25  loss: 1844.1761 (1992.9245)  loss_n_40: 4.3364 (4.2450)  loss_n_60: 4.6850 (4.6033)  loss_n_80: 4.8387 (4.7448)  loss_n_100: 4.9178 (4.7906)  triple_100: 459.2866 (499.5570)  triple_80: 463.3691 (502.2604)  triple_60: 462.0283 (498.9319)  triple_40: 439.4145 (473.7914)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [600/845]  eta: 0:01:22  loss: 1892.7820 (1994.1952)  loss_n_40: 4.2737 (4.2454)  loss_n_60: 4.5739 (4.6019)  loss_n_80: 4.7335 (4.7429)  loss_n_100: 4.8005 (4.7888)  triple_100: 475.6521 (499.8512)  triple_80: 478.3649 (502.5632)  triple_60: 473.7432 (499.2515)  triple_40: 446.8252 (474.1503)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [610/845]  eta: 0:01:18  loss: 1892.7820 (1993.8953)  loss_n_40: 4.2105 (4.2440)  loss_n_60: 4.5393 (4.6009)  loss_n_80: 4.7094 (4.7421)  loss_n_100: 4.7602 (4.7880)  triple_100: 475.6521 (499.7853)  triple_80: 478.3649 (502.4931)  triple_60: 473.7432 (499.1792)  triple_40: 446.8252 (474.0628)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [620/845]  eta: 0:01:15  loss: 1925.0453 (1993.4695)  loss_n_40: 4.1363 (4.2442)  loss_n_60: 4.5393 (4.6019)  loss_n_80: 4.7094 (4.7431)  loss_n_100: 4.7517 (4.7890)  triple_100: 484.5048 (499.6817)  triple_80: 487.1397 (502.3893)  triple_60: 481.7645 (499.0735)  triple_40: 453.3719 (473.9468)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [630/845]  eta: 0:01:12  loss: 1902.7854 (1992.9742)  loss_n_40: 4.2790 (4.2451)  loss_n_60: 4.6369 (4.6029)  loss_n_80: 4.7668 (4.7442)  loss_n_100: 4.8582 (4.7901)  triple_100: 476.6908 (499.5564)  triple_80: 479.9931 (502.2648)  triple_60: 476.4512 (498.9493)  triple_40: 450.9012 (473.8213)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [640/845]  eta: 0:01:08  loss: 1902.7854 (1993.4667)  loss_n_40: 4.2790 (4.2450)  loss_n_60: 4.6435 (4.6026)  loss_n_80: 4.7734 (4.7439)  loss_n_100: 4.8509 (4.7897)  triple_100: 475.8188 (499.6774)  triple_80: 479.6072 (502.3887)  triple_60: 476.4512 (499.0712)  triple_40: 450.9012 (473.9482)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [650/845]  eta: 0:01:05  loss: 1904.7235 (1993.1447)  loss_n_40: 4.1488 (4.2435)  loss_n_60: 4.5761 (4.6009)  loss_n_80: 4.7134 (4.7424)  loss_n_100: 4.7740 (4.7883)  triple_100: 475.8188 (499.5996)  triple_80: 479.6072 (502.3102)  triple_60: 476.9942 (498.9924)  triple_40: 451.8961 (473.8673)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [660/845]  eta: 0:01:02  loss: 1940.0459 (1992.9659)  loss_n_40: 4.1707 (4.2441)  loss_n_60: 4.5808 (4.6026)  loss_n_80: 4.7601 (4.7441)  loss_n_100: 4.7975 (4.7897)  triple_100: 486.1900 (499.5588)  triple_80: 489.8494 (502.2706)  triple_60: 486.3503 (498.9499)  triple_40: 459.0223 (473.8062)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [670/845]  eta: 0:00:58  loss: 1898.2719 (1991.3885)  loss_n_40: 4.2386 (4.2461)  loss_n_60: 4.6403 (4.6052)  loss_n_80: 4.7907 (4.7466)  loss_n_100: 4.8315 (4.7921)  triple_100: 475.8976 (499.1641)  triple_80: 479.3871 (501.8816)  triple_60: 475.3472 (498.5578)  triple_40: 449.8518 (473.3950)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [680/845]  eta: 0:00:55  loss: 1877.8416 (1993.8388)  loss_n_40: 4.1775 (4.2423)  loss_n_60: 4.5728 (4.5991)  loss_n_80: 4.7511 (4.7404)  loss_n_100: 4.7847 (4.7864)  triple_100: 472.1599 (499.7750)  triple_80: 474.2809 (502.4810)  triple_60: 470.0261 (499.1629)  triple_40: 443.3597 (474.0517)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [690/845]  eta: 0:00:51  loss: 1978.2639 (1994.5180)  loss_n_40: 4.0281 (4.2407)  loss_n_60: 4.5214 (4.5980)  loss_n_80: 4.7039 (4.7395)  loss_n_100: 4.7530 (4.7855)  triple_100: 497.3822 (499.9460)  triple_80: 500.4191 (502.6555)  triple_60: 496.0017 (499.3365)  triple_40: 466.3817 (474.2163)  time: 0.3349  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:0]  [700/845]  eta: 0:00:48  loss: 1978.2639 (1993.6160)  loss_n_40: 4.0281 (4.2407)  loss_n_60: 4.5204 (4.5991)  loss_n_80: 4.7039 (4.7410)  loss_n_100: 4.7514 (4.7867)  triple_100: 497.3822 (499.7326)  triple_80: 500.4191 (502.4411)  triple_60: 496.0017 (499.1101)  triple_40: 466.3817 (473.9647)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [710/845]  eta: 0:00:45  loss: 1899.7766 (1992.7994)  loss_n_40: 4.1073 (4.2415)  loss_n_60: 4.5204 (4.6008)  loss_n_80: 4.7006 (4.7428)  loss_n_100: 4.7514 (4.7884)  triple_100: 477.1367 (499.5303)  triple_80: 479.8874 (502.2402)  triple_60: 475.6918 (498.9055)  triple_40: 449.1291 (473.7498)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [720/845]  eta: 0:00:41  loss: 1887.5066 (1992.1571)  loss_n_40: 4.1183 (4.2413)  loss_n_60: 4.5391 (4.6003)  loss_n_80: 4.7166 (4.7425)  loss_n_100: 4.7757 (4.7882)  triple_100: 474.4641 (499.3769)  triple_80: 476.9774 (502.0816)  triple_60: 472.3991 (498.7427)  triple_40: 445.1590 (473.5836)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [730/845]  eta: 0:00:38  loss: 1853.2148 (1992.3362)  loss_n_40: 4.3143 (4.2423)  loss_n_60: 4.6138 (4.6006)  loss_n_80: 4.7493 (4.7428)  loss_n_100: 4.8408 (4.7883)  triple_100: 466.0065 (499.4143)  triple_80: 467.8632 (502.1175)  triple_60: 464.3232 (498.7855)  triple_40: 439.3358 (473.6449)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [740/845]  eta: 0:00:35  loss: 1909.0771 (1991.5980)  loss_n_40: 4.4912 (4.2445)  loss_n_60: 4.8540 (4.6037)  loss_n_80: 4.9754 (4.7458)  loss_n_100: 5.0091 (4.7912)  triple_100: 476.8587 (499.2297)  triple_80: 480.6212 (501.9389)  triple_60: 478.0379 (498.6032)  triple_40: 452.8315 (473.4411)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [750/845]  eta: 0:00:31  loss: 1902.8899 (1992.5147)  loss_n_40: 4.2419 (4.2433)  loss_n_60: 4.5777 (4.6018)  loss_n_80: 4.7379 (4.7440)  loss_n_100: 4.8122 (4.7896)  triple_100: 479.2922 (499.4654)  triple_80: 481.1325 (502.1652)  triple_60: 476.4659 (498.8287)  triple_40: 451.0361 (473.6767)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [760/845]  eta: 0:00:28  loss: 1880.7052 (1991.9442)  loss_n_40: 4.1458 (4.2438)  loss_n_60: 4.5135 (4.6025)  loss_n_80: 4.6663 (4.7444)  loss_n_100: 4.7203 (4.7901)  triple_100: 471.5750 (499.3182)  triple_80: 474.6964 (502.0180)  triple_60: 470.5478 (498.6864)  triple_40: 445.6408 (473.5409)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [770/845]  eta: 0:00:25  loss: 1862.5449 (1993.2505)  loss_n_40: 4.1458 (4.2420)  loss_n_60: 4.5223 (4.5993)  loss_n_80: 4.6611 (4.7410)  loss_n_100: 4.7270 (4.7870)  triple_100: 466.8427 (499.6427)  triple_80: 469.8609 (502.3378)  triple_60: 466.4638 (499.0092)  triple_40: 440.0382 (473.8914)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [780/845]  eta: 0:00:21  loss: 1908.0845 (1993.0978)  loss_n_40: 4.0995 (4.2413)  loss_n_60: 4.4890 (4.5990)  loss_n_80: 4.6560 (4.7409)  loss_n_100: 4.7270 (4.7869)  triple_100: 478.3715 (499.6091)  triple_80: 481.3847 (502.3043)  triple_60: 477.2514 (498.9707)  triple_40: 450.2397 (473.8456)  time: 0.3353  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [790/845]  eta: 0:00:18  loss: 1890.1575 (1992.4924)  loss_n_40: 4.1653 (4.2413)  loss_n_60: 4.4972 (4.5986)  loss_n_80: 4.6916 (4.7405)  loss_n_100: 4.7421 (4.7863)  triple_100: 474.2098 (499.4548)  triple_80: 477.1433 (502.1545)  triple_60: 472.9382 (498.8184)  triple_40: 447.7870 (473.6979)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [800/845]  eta: 0:00:15  loss: 1890.1575 (1992.3902)  loss_n_40: 4.1852 (4.2422)  loss_n_60: 4.5339 (4.5998)  loss_n_80: 4.7069 (4.7416)  loss_n_100: 4.7607 (4.7873)  triple_100: 473.8203 (499.4202)  triple_80: 477.1433 (502.1278)  triple_60: 472.9382 (498.7961)  triple_40: 447.7870 (473.6752)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [810/845]  eta: 0:00:11  loss: 1901.6798 (1994.0534)  loss_n_40: 4.2335 (4.2415)  loss_n_60: 4.5838 (4.5974)  loss_n_80: 4.7324 (4.7389)  loss_n_100: 4.8049 (4.7846)  triple_100: 475.3857 (499.8239)  triple_80: 478.5413 (502.5311)  triple_60: 476.9643 (499.2106)  triple_40: 452.3331 (474.1255)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [820/845]  eta: 0:00:08  loss: 1932.6958 (1994.9088)  loss_n_40: 4.1394 (4.2414)  loss_n_60: 4.5560 (4.5968)  loss_n_80: 4.7071 (4.7381)  loss_n_100: 4.7610 (4.7838)  triple_100: 486.3541 (500.0367)  triple_80: 488.9353 (502.7461)  triple_60: 484.0053 (499.4251)  triple_40: 457.6995 (474.3408)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [830/845]  eta: 0:00:05  loss: 1932.6958 (1995.3744)  loss_n_40: 4.1028 (4.2421)  loss_n_60: 4.5516 (4.5973)  loss_n_80: 4.7071 (4.7386)  loss_n_100: 4.7610 (4.7843)  triple_100: 486.3541 (500.1455)  triple_80: 488.9353 (502.8587)  triple_60: 484.0053 (499.5426)  triple_40: 457.6995 (474.4654)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [840/845]  eta: 0:00:01  loss: 1880.0604 (1994.9812)  loss_n_40: 4.2021 (4.2416)  loss_n_60: 4.5418 (4.5969)  loss_n_80: 4.7262 (4.7383)  loss_n_100: 4.7660 (4.7841)  triple_100: 472.7724 (500.0503)  triple_80: 475.0703 (502.7649)  triple_60: 470.0569 (499.4461)  triple_40: 444.1701 (474.3591)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0]  [844/845]  eta: 0:00:00  loss: 1907.3833 (1995.3874)  loss_n_40: 4.0736 (4.2407)  loss_n_60: 4.5128 (4.5957)  loss_n_80: 4.6995 (4.7372)  loss_n_100: 4.7404 (4.7830)  triple_100: 476.0875 (500.1576)  triple_80: 479.9372 (502.8680)  triple_60: 477.9701 (499.5476)  triple_40: 451.6701 (474.4577)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:0] Total time: 0:04:43 (0.3355 s / it)\n",
      "Averaged stats: loss: 1907.3833 (1995.3874)  loss_n_40: 4.0736 (4.2407)  loss_n_60: 4.5128 (4.5957)  loss_n_80: 4.6995 (4.7372)  loss_n_100: 4.7404 (4.7830)  triple_100: 476.0875 (500.1576)  triple_80: 479.9372 (502.8680)  triple_60: 477.9701 (499.5476)  triple_40: 451.6701 (474.4577)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_0_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 4.783%\n",
      "Min loss_n_100: 4.783\n",
      "Best Epoch: 0.000\n",
      "/home/sunggu/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Train: [epoch:1]  [   0/1724]  eta: 1:59:38  lr: 0.000000  loss: 1826.8116 (1826.8116)  loss_n_40: 4.3003 (4.3003)  loss_n_60: 4.6628 (4.6628)  loss_n_80: 4.8304 (4.8304)  loss_n_100: 4.8853 (4.8853)  triple_100: 457.7603 (457.7603)  triple_80: 460.2428 (460.2428)  triple_60: 457.3535 (457.3535)  triple_40: 432.7764 (432.7764)  time: 4.1638  data: 0.4049  max mem: 46473\n",
      "Train: [epoch:1]  [  10/1724]  eta: 1:52:51  lr: 0.000000  loss: 1916.9027 (1942.1774)  loss_n_40: 4.3003 (4.3575)  loss_n_60: 4.7043 (4.7264)  loss_n_80: 4.8572 (4.8677)  loss_n_100: 4.9095 (4.9099)  triple_100: 479.5503 (486.5132)  triple_80: 483.2103 (489.3926)  triple_60: 480.1592 (486.2130)  triple_40: 454.0991 (461.1973)  time: 3.9505  data: 0.0370  max mem: 46473\n",
      "Train: [epoch:1]  [  20/1724]  eta: 1:51:52  lr: 0.000000  loss: 2020.1606 (2001.8483)  loss_n_40: 4.1768 (4.2009)  loss_n_60: 4.5445 (4.5669)  loss_n_80: 4.7155 (4.7124)  loss_n_100: 4.7531 (4.7606)  triple_100: 506.1678 (501.7666)  triple_80: 509.1818 (504.4216)  triple_60: 505.7285 (501.1586)  triple_40: 480.9041 (476.2606)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [  30/1724]  eta: 1:51:05  lr: 0.000000  loss: 2027.6046 (2001.1015)  loss_n_40: 4.0754 (4.1921)  loss_n_60: 4.3985 (4.5619)  loss_n_80: 4.5533 (4.7075)  loss_n_100: 4.5947 (4.7571)  triple_100: 507.3213 (501.6546)  triple_80: 509.9822 (504.2611)  triple_60: 507.3047 (500.9939)  triple_40: 482.1984 (475.9733)  time: 3.9263  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [  40/1724]  eta: 1:50:23  lr: 0.000000  loss: 1995.5972 (2003.9419)  loss_n_40: 4.2083 (4.1935)  loss_n_60: 4.5692 (4.5657)  loss_n_80: 4.7071 (4.7095)  loss_n_100: 4.7883 (4.7566)  triple_100: 500.0849 (502.3240)  triple_80: 503.4031 (504.9716)  triple_60: 500.1001 (501.7110)  triple_40: 474.7449 (476.7101)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [  50/1724]  eta: 1:49:42  lr: 0.000000  loss: 1927.5259 (1987.8045)  loss_n_40: 4.1761 (4.2172)  loss_n_60: 4.5627 (4.5894)  loss_n_80: 4.7279 (4.7346)  loss_n_100: 4.7834 (4.7813)  triple_100: 482.7700 (498.2846)  triple_80: 485.7393 (500.9506)  triple_60: 482.6232 (497.6649)  triple_40: 456.0959 (472.5820)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [  60/1724]  eta: 1:49:02  lr: 0.000000  loss: 1927.4167 (1982.9374)  loss_n_40: 4.2403 (4.2236)  loss_n_60: 4.6123 (4.5975)  loss_n_80: 4.7471 (4.7436)  loss_n_100: 4.8027 (4.7894)  triple_100: 482.4983 (497.0944)  triple_80: 485.7393 (499.7699)  triple_60: 482.6232 (496.4610)  triple_40: 456.0959 (471.2580)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [  70/1724]  eta: 1:48:22  lr: 0.000000  loss: 1939.2509 (1984.6366)  loss_n_40: 4.2466 (4.2255)  loss_n_60: 4.6708 (4.5956)  loss_n_80: 4.8067 (4.7418)  loss_n_100: 4.8377 (4.7881)  triple_100: 486.6962 (497.5095)  triple_80: 489.6879 (500.1707)  triple_60: 485.8574 (496.8700)  triple_40: 458.1508 (471.7353)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [  80/1724]  eta: 1:47:42  lr: 0.000000  loss: 2026.1226 (1990.9064)  loss_n_40: 4.1292 (4.2162)  loss_n_60: 4.5095 (4.5835)  loss_n_80: 4.6874 (4.7293)  loss_n_100: 4.7304 (4.7767)  triple_100: 508.6873 (499.0988)  triple_80: 510.9593 (501.7230)  triple_60: 507.3532 (498.4064)  triple_40: 481.5504 (473.3727)  time: 3.9289  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [  90/1724]  eta: 1:47:02  lr: 0.000000  loss: 1907.6414 (1984.1578)  loss_n_40: 4.2512 (4.2285)  loss_n_60: 4.6003 (4.5979)  loss_n_80: 4.7453 (4.7420)  loss_n_100: 4.8040 (4.7889)  triple_100: 479.5036 (497.3679)  triple_80: 482.0154 (500.0250)  triple_60: 477.6327 (496.7317)  triple_40: 452.1074 (471.6758)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 100/1724]  eta: 1:46:22  lr: 0.000000  loss: 1919.6368 (1985.3590)  loss_n_40: 4.2558 (4.2297)  loss_n_60: 4.6853 (4.6028)  loss_n_80: 4.8232 (4.7468)  loss_n_100: 4.8686 (4.7931)  triple_100: 480.7602 (497.6644)  triple_80: 483.8437 (500.3310)  triple_60: 480.4955 (497.0344)  triple_40: 455.4521 (471.9566)  time: 3.9271  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 110/1724]  eta: 1:45:42  lr: 0.000000  loss: 1992.4082 (1988.7470)  loss_n_40: 4.2552 (4.2327)  loss_n_60: 4.6449 (4.6016)  loss_n_80: 4.7773 (4.7453)  loss_n_100: 4.8149 (4.7919)  triple_100: 499.1379 (498.4856)  triple_80: 501.9890 (501.1402)  triple_60: 498.7977 (497.8649)  triple_40: 474.4333 (472.8848)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 120/1724]  eta: 1:45:03  lr: 0.000000  loss: 1986.0846 (1985.8072)  loss_n_40: 4.2552 (4.2396)  loss_n_60: 4.6179 (4.6060)  loss_n_80: 4.7522 (4.7496)  loss_n_100: 4.8067 (4.7963)  triple_100: 499.1379 (497.7359)  triple_80: 501.9890 (500.3879)  triple_60: 497.4682 (497.1214)  triple_40: 469.0879 (472.1704)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 130/1724]  eta: 1:44:23  lr: 0.000000  loss: 1962.7815 (1985.4210)  loss_n_40: 4.2147 (4.2368)  loss_n_60: 4.6156 (4.6056)  loss_n_80: 4.7673 (4.7491)  loss_n_100: 4.8208 (4.7957)  triple_100: 492.6802 (497.6579)  triple_80: 495.5437 (500.3125)  triple_60: 491.7174 (497.0247)  triple_40: 463.9423 (472.0387)  time: 3.9284  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 140/1724]  eta: 1:43:44  lr: 0.000000  loss: 1992.3207 (1987.2157)  loss_n_40: 4.1911 (4.2363)  loss_n_60: 4.6156 (4.6047)  loss_n_80: 4.7673 (4.7484)  loss_n_100: 4.8208 (4.7950)  triple_100: 500.1183 (498.1196)  triple_80: 502.6205 (500.7736)  triple_60: 499.0211 (497.4751)  triple_40: 472.1959 (472.4630)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 150/1724]  eta: 1:43:05  lr: 0.000000  loss: 1938.2002 (1983.2292)  loss_n_40: 4.3101 (4.2424)  loss_n_60: 4.6925 (4.6129)  loss_n_80: 4.8263 (4.7564)  loss_n_100: 4.8716 (4.8029)  triple_100: 486.7074 (497.1515)  triple_80: 489.2131 (499.8057)  triple_60: 485.3990 (496.4780)  triple_40: 458.9588 (471.3793)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 160/1724]  eta: 1:42:25  lr: 0.000000  loss: 1928.4630 (1984.8890)  loss_n_40: 4.3262 (4.2435)  loss_n_60: 4.6925 (4.6112)  loss_n_80: 4.8263 (4.7540)  loss_n_100: 4.8716 (4.8005)  triple_100: 483.4497 (497.5435)  triple_80: 485.9830 (500.2016)  triple_60: 482.9842 (496.8983)  triple_40: 457.6314 (471.8364)  time: 3.9295  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 170/1724]  eta: 1:41:46  lr: 0.000000  loss: 2021.9336 (1984.9401)  loss_n_40: 4.2247 (4.2429)  loss_n_60: 4.5227 (4.6109)  loss_n_80: 4.6886 (4.7537)  loss_n_100: 4.7461 (4.8002)  triple_100: 507.2515 (497.5718)  triple_80: 509.2133 (500.2279)  triple_60: 505.6469 (496.9088)  triple_40: 480.9175 (471.8239)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 180/1724]  eta: 1:41:07  lr: 0.000000  loss: 1984.0134 (1984.8748)  loss_n_40: 4.2619 (4.2465)  loss_n_60: 4.6225 (4.6134)  loss_n_80: 4.7094 (4.7554)  loss_n_100: 4.7578 (4.8020)  triple_100: 496.3397 (497.5398)  triple_80: 499.5526 (500.1949)  triple_60: 496.8842 (496.8927)  triple_40: 472.1281 (471.8301)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 190/1724]  eta: 1:40:27  lr: 0.000000  loss: 1977.7516 (1984.7143)  loss_n_40: 4.3245 (4.2501)  loss_n_60: 4.6268 (4.6163)  loss_n_80: 4.7536 (4.7579)  loss_n_100: 4.8109 (4.8043)  triple_100: 496.0876 (497.4894)  triple_80: 498.0480 (500.1505)  triple_60: 494.9403 (496.8489)  triple_40: 471.0893 (471.7969)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 200/1724]  eta: 1:39:48  lr: 0.000000  loss: 1915.3370 (1985.0295)  loss_n_40: 4.2549 (4.2484)  loss_n_60: 4.6174 (4.6140)  loss_n_80: 4.7371 (4.7553)  loss_n_100: 4.7899 (4.8016)  triple_100: 480.3159 (497.5522)  triple_80: 483.5140 (500.2193)  triple_60: 479.7706 (496.9307)  triple_40: 452.2235 (471.9078)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 210/1724]  eta: 1:39:08  lr: 0.000000  loss: 1911.5389 (1983.7050)  loss_n_40: 4.2605 (4.2516)  loss_n_60: 4.6872 (4.6174)  loss_n_80: 4.8206 (4.7584)  loss_n_100: 4.8751 (4.8047)  triple_100: 480.0076 (497.2096)  triple_80: 482.9141 (499.8830)  triple_60: 478.8976 (496.6018)  triple_40: 450.8513 (471.5784)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 220/1724]  eta: 1:38:29  lr: 0.000000  loss: 1954.0905 (1984.8706)  loss_n_40: 4.2635 (4.2491)  loss_n_60: 4.6468 (4.6137)  loss_n_80: 4.7384 (4.7549)  loss_n_100: 4.7856 (4.8013)  triple_100: 489.4244 (497.5072)  triple_80: 492.4103 (500.1791)  triple_60: 489.2340 (496.8924)  triple_40: 463.6726 (471.8729)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 230/1724]  eta: 1:37:49  lr: 0.000000  loss: 2049.8435 (1987.7150)  loss_n_40: 4.2145 (4.2488)  loss_n_60: 4.5707 (4.6127)  loss_n_80: 4.6857 (4.7535)  loss_n_100: 4.7368 (4.7999)  triple_100: 513.2643 (498.2138)  triple_80: 515.9242 (500.8851)  triple_60: 512.8483 (497.5999)  triple_40: 489.5462 (472.6013)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 240/1724]  eta: 1:37:10  lr: 0.000000  loss: 2030.3718 (1988.0960)  loss_n_40: 4.2082 (4.2458)  loss_n_60: 4.6048 (4.6102)  loss_n_80: 4.6857 (4.7509)  loss_n_100: 4.7368 (4.7976)  triple_100: 508.4272 (498.3177)  triple_80: 511.2737 (500.9872)  triple_60: 508.1953 (497.6934)  triple_40: 483.8153 (472.6932)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 250/1724]  eta: 1:36:30  lr: 0.000000  loss: 1965.2917 (1988.4036)  loss_n_40: 4.2521 (4.2458)  loss_n_60: 4.6335 (4.6097)  loss_n_80: 4.7789 (4.7505)  loss_n_100: 4.8178 (4.7970)  triple_100: 493.4211 (498.3987)  triple_80: 496.1136 (501.0708)  triple_60: 492.1642 (497.7716)  triple_40: 465.6564 (472.7595)  time: 3.9257  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [ 260/1724]  eta: 1:35:51  lr: 0.000000  loss: 1960.5565 (1989.4933)  loss_n_40: 4.2090 (4.2445)  loss_n_60: 4.6063 (4.6076)  loss_n_80: 4.7175 (4.7486)  loss_n_100: 4.7729 (4.7950)  triple_100: 492.0698 (498.6640)  triple_80: 494.7997 (501.3341)  triple_60: 490.9991 (498.0392)  triple_40: 464.3430 (473.0603)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 270/1724]  eta: 1:35:11  lr: 0.000000  loss: 1958.3734 (1988.4200)  loss_n_40: 4.1835 (4.2452)  loss_n_60: 4.5696 (4.6080)  loss_n_80: 4.7091 (4.7492)  loss_n_100: 4.7619 (4.7956)  triple_100: 490.8475 (498.3977)  triple_80: 493.3010 (501.0643)  triple_60: 490.5745 (497.7697)  triple_40: 464.5360 (472.7904)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 280/1724]  eta: 1:34:32  lr: 0.000000  loss: 1984.3452 (1990.7783)  loss_n_40: 4.1844 (4.2422)  loss_n_60: 4.5674 (4.6048)  loss_n_80: 4.7222 (4.7458)  loss_n_100: 4.7705 (4.7925)  triple_100: 498.1057 (498.9866)  triple_80: 500.8956 (501.6534)  triple_60: 496.8557 (498.3595)  triple_40: 469.7436 (473.3935)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 290/1724]  eta: 1:33:53  lr: 0.000000  loss: 2017.3047 (1991.6133)  loss_n_40: 4.2406 (4.2422)  loss_n_60: 4.6047 (4.6045)  loss_n_80: 4.7319 (4.7455)  loss_n_100: 4.7788 (4.7921)  triple_100: 504.4227 (499.1901)  triple_80: 507.6068 (501.8599)  triple_60: 505.3392 (498.5660)  triple_40: 480.8317 (473.6128)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 300/1724]  eta: 1:33:13  lr: 0.000000  loss: 2000.9907 (1991.9034)  loss_n_40: 4.2367 (4.2422)  loss_n_60: 4.5021 (4.6024)  loss_n_80: 4.6343 (4.7430)  loss_n_100: 4.7019 (4.7899)  triple_100: 501.2773 (499.2400)  triple_80: 503.3798 (501.9147)  triple_60: 500.3463 (498.6343)  triple_40: 478.1576 (473.7368)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 310/1724]  eta: 1:32:34  lr: 0.000000  loss: 1971.4764 (1991.7296)  loss_n_40: 4.2367 (4.2426)  loss_n_60: 4.5021 (4.6031)  loss_n_80: 4.6343 (4.7437)  loss_n_100: 4.7019 (4.7906)  triple_100: 492.7180 (499.1998)  triple_80: 495.9696 (501.8758)  triple_60: 493.4172 (498.5917)  triple_40: 470.1821 (473.6823)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 320/1724]  eta: 1:31:55  lr: 0.000000  loss: 1965.5446 (1992.6627)  loss_n_40: 4.2315 (4.2439)  loss_n_60: 4.6436 (4.6035)  loss_n_80: 4.7868 (4.7439)  loss_n_100: 4.8177 (4.7907)  triple_100: 492.7180 (499.4183)  triple_80: 495.8574 (502.0987)  triple_60: 492.0494 (498.8263)  triple_40: 465.9795 (473.9374)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 330/1724]  eta: 1:31:15  lr: 0.000000  loss: 1958.9106 (1991.6222)  loss_n_40: 4.2315 (4.2439)  loss_n_60: 4.6436 (4.6046)  loss_n_80: 4.7868 (4.7452)  loss_n_100: 4.8177 (4.7917)  triple_100: 491.7857 (499.1610)  triple_80: 494.7416 (501.8437)  triple_60: 490.9784 (498.5690)  triple_40: 463.1566 (473.6632)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 340/1724]  eta: 1:30:36  lr: 0.000000  loss: 1958.9106 (1992.9832)  loss_n_40: 4.1663 (4.2407)  loss_n_60: 4.5757 (4.6008)  loss_n_80: 4.7337 (4.7413)  loss_n_100: 4.7850 (4.7880)  triple_100: 491.7857 (499.5088)  triple_80: 494.7416 (502.1860)  triple_60: 490.9784 (498.9096)  triple_40: 463.1566 (474.0080)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 350/1724]  eta: 1:29:57  lr: 0.000000  loss: 2014.6490 (1993.8368)  loss_n_40: 4.1663 (4.2397)  loss_n_60: 4.5388 (4.5986)  loss_n_80: 4.6839 (4.7392)  loss_n_100: 4.7220 (4.7861)  triple_100: 505.7572 (499.7185)  triple_80: 507.7587 (502.3877)  triple_60: 504.0747 (499.1169)  triple_40: 482.8794 (474.2502)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 360/1724]  eta: 1:29:17  lr: 0.000000  loss: 1954.8477 (1993.1913)  loss_n_40: 4.2197 (4.2397)  loss_n_60: 4.5978 (4.5999)  loss_n_80: 4.7452 (4.7405)  loss_n_100: 4.8224 (4.7876)  triple_100: 490.8091 (499.5651)  triple_80: 493.6462 (502.2345)  triple_60: 489.8385 (498.9595)  triple_40: 462.7112 (474.0644)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 370/1724]  eta: 1:28:38  lr: 0.000000  loss: 1950.3514 (1993.2667)  loss_n_40: 4.2167 (4.2391)  loss_n_60: 4.5978 (4.5999)  loss_n_80: 4.7678 (4.7408)  loss_n_100: 4.8319 (4.7879)  triple_100: 490.6977 (499.5880)  triple_80: 492.8634 (502.2569)  triple_60: 488.5479 (498.9787)  triple_40: 462.5427 (474.0753)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 380/1724]  eta: 1:27:59  lr: 0.000000  loss: 1979.5459 (1993.8682)  loss_n_40: 4.2167 (4.2367)  loss_n_60: 4.5731 (4.5970)  loss_n_80: 4.7264 (4.7382)  loss_n_100: 4.7755 (4.7855)  triple_100: 496.1264 (499.7435)  triple_80: 498.1162 (502.4069)  triple_60: 495.2790 (499.1269)  triple_40: 471.8568 (474.2334)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 390/1724]  eta: 1:27:19  lr: 0.000000  loss: 1993.1427 (1993.8920)  loss_n_40: 4.1748 (4.2356)  loss_n_60: 4.5372 (4.5964)  loss_n_80: 4.7046 (4.7380)  loss_n_100: 4.7383 (4.7854)  triple_100: 499.8335 (499.7569)  triple_80: 502.8441 (502.4177)  triple_60: 499.3916 (499.1336)  triple_40: 471.8568 (474.2284)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 400/1724]  eta: 1:26:40  lr: 0.000000  loss: 2039.5756 (1995.1478)  loss_n_40: 4.1708 (4.2338)  loss_n_60: 4.5372 (4.5942)  loss_n_80: 4.7047 (4.7357)  loss_n_100: 4.7583 (4.7833)  triple_100: 511.3611 (500.0748)  triple_80: 513.5468 (502.7330)  triple_60: 510.1841 (499.4457)  triple_40: 486.8431 (474.5474)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 410/1724]  eta: 1:26:01  lr: 0.000000  loss: 1939.8687 (1994.4033)  loss_n_40: 4.1532 (4.2336)  loss_n_60: 4.5326 (4.5940)  loss_n_80: 4.6877 (4.7357)  loss_n_100: 4.7434 (4.7832)  triple_100: 487.1721 (499.8986)  triple_80: 489.6400 (502.5540)  triple_60: 485.3737 (499.2606)  triple_40: 459.6364 (474.3436)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 420/1724]  eta: 1:25:21  lr: 0.000000  loss: 1914.2140 (1994.5634)  loss_n_40: 4.1482 (4.2336)  loss_n_60: 4.5631 (4.5937)  loss_n_80: 4.7243 (4.7354)  loss_n_100: 4.7616 (4.7829)  triple_100: 480.6248 (499.9331)  triple_80: 483.3394 (502.5878)  triple_60: 479.2270 (499.2979)  triple_40: 453.1775 (474.3990)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 430/1724]  eta: 1:24:42  lr: 0.000000  loss: 1938.0603 (1994.3020)  loss_n_40: 4.2867 (4.2336)  loss_n_60: 4.5631 (4.5931)  loss_n_80: 4.6934 (4.7348)  loss_n_100: 4.7368 (4.7823)  triple_100: 486.6170 (499.8595)  triple_80: 489.3478 (502.5129)  triple_60: 485.3666 (499.2321)  triple_40: 458.5459 (474.3538)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 440/1724]  eta: 1:24:03  lr: 0.000000  loss: 2000.2313 (1995.4105)  loss_n_40: 4.1852 (4.2318)  loss_n_60: 4.5496 (4.5914)  loss_n_80: 4.6865 (4.7331)  loss_n_100: 4.7331 (4.7807)  triple_100: 500.1313 (500.1403)  triple_80: 502.8939 (502.7917)  triple_60: 500.8572 (499.5097)  triple_40: 472.8716 (474.6316)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 450/1724]  eta: 1:23:24  lr: 0.000000  loss: 1987.9326 (1995.3674)  loss_n_40: 4.1852 (4.2307)  loss_n_60: 4.5806 (4.5907)  loss_n_80: 4.7303 (4.7324)  loss_n_100: 4.7797 (4.7801)  triple_100: 498.8538 (500.1377)  triple_80: 501.9076 (502.7843)  triple_60: 498.1205 (499.4984)  triple_40: 472.8716 (474.6132)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 460/1724]  eta: 1:22:44  lr: 0.000000  loss: 1984.3041 (1994.8007)  loss_n_40: 4.2211 (4.2316)  loss_n_60: 4.6255 (4.5920)  loss_n_80: 4.7804 (4.7335)  loss_n_100: 4.8205 (4.7810)  triple_100: 496.6212 (499.9868)  triple_80: 498.9577 (502.6388)  triple_60: 496.2569 (499.3576)  triple_40: 472.6368 (474.4794)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 470/1724]  eta: 1:22:05  lr: 0.000000  loss: 1933.7350 (1993.6454)  loss_n_40: 4.2302 (4.2324)  loss_n_60: 4.6491 (4.5937)  loss_n_80: 4.8076 (4.7353)  loss_n_100: 4.8396 (4.7827)  triple_100: 485.7831 (499.7003)  triple_80: 488.6490 (502.3530)  triple_60: 484.3517 (499.0724)  triple_40: 456.5254 (474.1758)  time: 3.9286  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [ 480/1724]  eta: 1:21:26  lr: 0.000000  loss: 1916.7520 (1994.1625)  loss_n_40: 4.2313 (4.2327)  loss_n_60: 4.6724 (4.5935)  loss_n_80: 4.8476 (4.7351)  loss_n_100: 4.8716 (4.7824)  triple_100: 480.7603 (499.8242)  triple_80: 483.8091 (502.4754)  triple_60: 480.0353 (499.1994)  triple_40: 453.0060 (474.3197)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 490/1724]  eta: 1:20:47  lr: 0.000000  loss: 2023.4459 (1995.0248)  loss_n_40: 4.1916 (4.2323)  loss_n_60: 4.5613 (4.5926)  loss_n_80: 4.7084 (4.7340)  loss_n_100: 4.7574 (4.7814)  triple_100: 507.1590 (500.0439)  triple_80: 510.0707 (502.6928)  triple_60: 506.7038 (499.4154)  triple_40: 480.9645 (474.5324)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 500/1724]  eta: 1:20:07  lr: 0.000000  loss: 2023.4459 (1995.6606)  loss_n_40: 4.1629 (4.2306)  loss_n_60: 4.5094 (4.5910)  loss_n_80: 4.6966 (4.7327)  loss_n_100: 4.7376 (4.7801)  triple_100: 507.1590 (500.2113)  triple_80: 510.0707 (502.8548)  triple_60: 506.7038 (499.5726)  triple_40: 480.9645 (474.6874)  time: 3.9276  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 510/1724]  eta: 1:19:28  lr: 0.000000  loss: 1987.8611 (1995.2093)  loss_n_40: 4.2086 (4.2315)  loss_n_60: 4.6011 (4.5918)  loss_n_80: 4.7637 (4.7334)  loss_n_100: 4.8181 (4.7809)  triple_100: 498.7778 (500.0906)  triple_80: 501.3084 (502.7381)  triple_60: 497.6506 (499.4590)  triple_40: 471.1392 (474.5840)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 520/1724]  eta: 1:18:49  lr: 0.000000  loss: 1956.8907 (1994.5705)  loss_n_40: 4.2569 (4.2315)  loss_n_60: 4.6136 (4.5924)  loss_n_80: 4.7873 (4.7344)  loss_n_100: 4.8260 (4.7818)  triple_100: 490.6263 (499.9375)  triple_80: 493.3098 (502.5860)  triple_60: 490.1597 (499.3017)  triple_40: 464.8286 (474.4053)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 530/1724]  eta: 1:18:09  lr: 0.000000  loss: 1963.6772 (1995.0444)  loss_n_40: 4.1550 (4.2299)  loss_n_60: 4.5659 (4.5911)  loss_n_80: 4.7242 (4.7332)  loss_n_100: 4.7629 (4.7807)  triple_100: 493.1129 (500.0607)  triple_80: 495.7081 (502.7072)  triple_60: 491.7263 (499.4197)  triple_40: 467.1218 (474.5218)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 540/1724]  eta: 1:17:30  lr: 0.000000  loss: 2039.8938 (1995.4786)  loss_n_40: 4.1450 (4.2284)  loss_n_60: 4.4894 (4.5897)  loss_n_80: 4.6692 (4.7320)  loss_n_100: 4.7258 (4.7796)  triple_100: 510.8400 (500.1744)  triple_80: 513.1288 (502.8168)  triple_60: 510.1216 (499.5276)  triple_40: 485.2476 (474.6301)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 550/1724]  eta: 1:16:51  lr: 0.000000  loss: 1962.6149 (1995.5721)  loss_n_40: 4.1450 (4.2282)  loss_n_60: 4.5646 (4.5897)  loss_n_80: 4.7143 (4.7319)  loss_n_100: 4.7628 (4.7794)  triple_100: 493.3313 (500.1938)  triple_80: 495.7090 (502.8374)  triple_60: 491.2252 (499.5500)  triple_40: 464.0075 (474.6616)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 560/1724]  eta: 1:16:12  lr: 0.000000  loss: 1915.4937 (1994.6254)  loss_n_40: 4.2010 (4.2286)  loss_n_60: 4.6526 (4.5908)  loss_n_80: 4.8045 (4.7331)  loss_n_100: 4.8404 (4.7805)  triple_100: 480.4985 (499.9606)  triple_80: 483.4168 (502.6050)  triple_60: 479.7744 (499.3128)  triple_40: 452.6519 (474.4139)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 570/1724]  eta: 1:15:32  lr: 0.000000  loss: 1934.5608 (1993.9437)  loss_n_40: 4.2282 (4.2289)  loss_n_60: 4.6020 (4.5913)  loss_n_80: 4.7412 (4.7336)  loss_n_100: 4.8040 (4.7811)  triple_100: 484.3972 (499.7947)  triple_80: 487.2547 (502.4372)  triple_60: 484.5413 (499.1411)  triple_40: 459.3302 (474.2357)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 580/1724]  eta: 1:14:53  lr: 0.000000  loss: 1931.2217 (1993.0132)  loss_n_40: 4.2086 (4.2293)  loss_n_60: 4.5407 (4.5914)  loss_n_80: 4.7069 (4.7338)  loss_n_100: 4.7597 (4.7813)  triple_100: 484.3972 (499.5606)  triple_80: 486.5065 (502.2028)  triple_60: 482.8690 (498.9057)  triple_40: 459.1692 (474.0082)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 590/1724]  eta: 1:14:14  lr: 0.000000  loss: 1922.0728 (1991.7405)  loss_n_40: 4.2445 (4.2302)  loss_n_60: 4.5511 (4.5926)  loss_n_80: 4.7069 (4.7350)  loss_n_100: 4.7628 (4.7825)  triple_100: 481.5641 (499.2451)  triple_80: 484.6535 (501.8884)  triple_60: 481.1727 (498.5870)  triple_40: 454.9035 (473.6796)  time: 3.9283  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 600/1724]  eta: 1:13:34  lr: 0.000000  loss: 1979.2563 (1992.5797)  loss_n_40: 4.2445 (4.2287)  loss_n_60: 4.5511 (4.5906)  loss_n_80: 4.7018 (4.7330)  loss_n_100: 4.7628 (4.7806)  triple_100: 496.2328 (499.4601)  triple_80: 498.8276 (502.0979)  triple_60: 495.5647 (498.7950)  triple_40: 470.5645 (473.8939)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 610/1724]  eta: 1:12:55  lr: 0.000000  loss: 1988.9569 (1992.4592)  loss_n_40: 4.1087 (4.2277)  loss_n_60: 4.4596 (4.5897)  loss_n_80: 4.5919 (4.7320)  loss_n_100: 4.6567 (4.7798)  triple_100: 498.8438 (499.4333)  triple_80: 501.1121 (502.0676)  triple_60: 498.1309 (498.7661)  triple_40: 473.5789 (473.8630)  time: 3.9276  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 620/1724]  eta: 1:12:16  lr: 0.000000  loss: 1952.5677 (1993.5083)  loss_n_40: 4.2140 (4.2283)  loss_n_60: 4.5310 (4.5897)  loss_n_80: 4.6708 (4.7320)  loss_n_100: 4.7215 (4.7796)  triple_100: 489.7030 (499.6867)  triple_80: 492.7438 (502.3243)  triple_60: 488.8315 (499.0280)  triple_40: 462.4572 (474.1398)  time: 3.9290  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 630/1724]  eta: 1:11:37  lr: 0.000000  loss: 1941.3516 (1993.4682)  loss_n_40: 4.2569 (4.2278)  loss_n_60: 4.6456 (4.5896)  loss_n_80: 4.7900 (4.7318)  loss_n_100: 4.8295 (4.7794)  triple_100: 485.8643 (499.6751)  triple_80: 488.9640 (502.3145)  triple_60: 486.0390 (499.0200)  triple_40: 461.4246 (474.1300)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 640/1724]  eta: 1:10:57  lr: 0.000000  loss: 1917.8571 (1992.7708)  loss_n_40: 4.2604 (4.2288)  loss_n_60: 4.6928 (4.5911)  loss_n_80: 4.8527 (4.7332)  loss_n_100: 4.8847 (4.7808)  triple_100: 481.3098 (499.5012)  triple_80: 484.6415 (502.1433)  triple_60: 480.6005 (498.8474)  triple_40: 453.2796 (473.9451)  time: 3.9294  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 650/1724]  eta: 1:10:18  lr: 0.000000  loss: 1931.6658 (1992.2394)  loss_n_40: 4.2639 (4.2288)  loss_n_60: 4.6928 (4.5915)  loss_n_80: 4.8527 (4.7338)  loss_n_100: 4.8847 (4.7812)  triple_100: 485.0134 (499.3709)  triple_80: 487.8620 (502.0124)  triple_60: 483.6663 (498.7150)  triple_40: 456.4336 (473.8057)  time: 3.9317  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 660/1724]  eta: 1:09:39  lr: 0.000000  loss: 1972.4243 (1992.7139)  loss_n_40: 4.2639 (4.2298)  loss_n_60: 4.6622 (4.5924)  loss_n_80: 4.8064 (4.7345)  loss_n_100: 4.8499 (4.7819)  triple_100: 494.6815 (499.4825)  triple_80: 496.9529 (502.1287)  triple_60: 493.5969 (498.8346)  triple_40: 468.9062 (473.9294)  time: 3.9326  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 670/1724]  eta: 1:09:00  lr: 0.000000  loss: 1955.8085 (1992.2643)  loss_n_40: 4.2418 (4.2306)  loss_n_60: 4.6380 (4.5928)  loss_n_80: 4.7754 (4.7348)  loss_n_100: 4.8296 (4.7822)  triple_100: 491.0368 (499.3679)  triple_80: 494.2770 (502.0151)  triple_60: 490.1907 (498.7214)  triple_40: 461.7525 (473.8195)  time: 3.9325  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 680/1724]  eta: 1:08:21  lr: 0.000000  loss: 1936.2593 (1991.8113)  loss_n_40: 4.1950 (4.2300)  loss_n_60: 4.5893 (4.5925)  loss_n_80: 4.7299 (4.7346)  loss_n_100: 4.7820 (4.7821)  triple_100: 485.5679 (499.2602)  triple_80: 488.4281 (501.9050)  triple_60: 484.5952 (498.6091)  triple_40: 458.2695 (473.6978)  time: 3.9308  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 690/1724]  eta: 1:07:41  lr: 0.000000  loss: 1951.4286 (1991.8016)  loss_n_40: 4.1829 (4.2304)  loss_n_60: 4.5269 (4.5930)  loss_n_80: 4.6960 (4.7350)  loss_n_100: 4.7461 (4.7825)  triple_100: 490.2459 (499.2571)  triple_80: 492.5400 (501.9021)  triple_60: 488.6817 (498.6068)  triple_40: 461.9187 (473.6947)  time: 3.9286  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [ 700/1724]  eta: 1:07:02  lr: 0.000000  loss: 2023.2422 (1993.1261)  loss_n_40: 4.1067 (4.2283)  loss_n_60: 4.5269 (4.5905)  loss_n_80: 4.6686 (4.7324)  loss_n_100: 4.7126 (4.7799)  triple_100: 506.7628 (499.5843)  triple_80: 509.9882 (502.2276)  triple_60: 506.7659 (498.9367)  triple_40: 481.7026 (474.0466)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 710/1724]  eta: 1:06:23  lr: 0.000000  loss: 2033.3436 (1992.9751)  loss_n_40: 4.2024 (4.2285)  loss_n_60: 4.6216 (4.5912)  loss_n_80: 4.7720 (4.7332)  loss_n_100: 4.8185 (4.7806)  triple_100: 509.3298 (499.5489)  triple_80: 511.9809 (502.1935)  triple_60: 508.7921 (498.9005)  triple_40: 484.2256 (473.9988)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 720/1724]  eta: 1:05:43  lr: 0.000000  loss: 1933.6853 (1992.6663)  loss_n_40: 4.2661 (4.2281)  loss_n_60: 4.6758 (4.5904)  loss_n_80: 4.8099 (4.7324)  loss_n_100: 4.8468 (4.7799)  triple_100: 484.0709 (499.4729)  triple_80: 487.2895 (502.1181)  triple_60: 484.4284 (498.8239)  triple_40: 459.1219 (473.9205)  time: 3.9304  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 730/1724]  eta: 1:05:04  lr: 0.000000  loss: 1917.9274 (1992.2433)  loss_n_40: 4.3241 (4.2294)  loss_n_60: 4.6835 (4.5916)  loss_n_80: 4.8283 (4.7335)  loss_n_100: 4.8814 (4.7809)  triple_100: 480.3663 (499.3644)  triple_80: 483.2449 (502.0110)  triple_60: 479.8518 (498.7176)  triple_40: 454.8893 (473.8150)  time: 3.9309  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 740/1724]  eta: 1:04:25  lr: 0.000000  loss: 1926.6257 (1992.2002)  loss_n_40: 4.2316 (4.2287)  loss_n_60: 4.6323 (4.5910)  loss_n_80: 4.7521 (4.7330)  loss_n_100: 4.8194 (4.7804)  triple_100: 482.6701 (499.3576)  triple_80: 485.0595 (502.0019)  triple_60: 481.9463 (498.7066)  triple_40: 459.4981 (473.8010)  time: 3.9307  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:1]  [ 750/1724]  eta: 1:03:46  lr: 0.000000  loss: 1980.0709 (1992.4334)  loss_n_40: 4.1927 (4.2291)  loss_n_60: 4.5049 (4.5912)  loss_n_80: 4.6488 (4.7332)  loss_n_100: 4.7103 (4.7805)  triple_100: 496.1968 (499.4108)  triple_80: 498.5968 (502.0571)  triple_60: 495.8137 (498.7649)  triple_40: 471.3239 (473.8666)  time: 3.9301  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:1]  [ 760/1724]  eta: 1:03:06  lr: 0.000000  loss: 1980.0709 (1992.4889)  loss_n_40: 4.2541 (4.2288)  loss_n_60: 4.5576 (4.5908)  loss_n_80: 4.7160 (4.7329)  loss_n_100: 4.7680 (4.7801)  triple_100: 496.1968 (499.4273)  triple_80: 498.5968 (502.0734)  triple_60: 495.8137 (498.7795)  triple_40: 470.9872 (473.8761)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 770/1724]  eta: 1:02:27  lr: 0.000000  loss: 1971.9486 (1992.3631)  loss_n_40: 4.2396 (4.2287)  loss_n_60: 4.5481 (4.5904)  loss_n_80: 4.6820 (4.7323)  loss_n_100: 4.7184 (4.7795)  triple_100: 495.1847 (499.3878)  triple_80: 497.9619 (502.0383)  triple_60: 493.7310 (498.7470)  triple_40: 467.0512 (473.8591)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 780/1724]  eta: 1:01:48  lr: 0.000000  loss: 1971.9486 (1992.9993)  loss_n_40: 4.2185 (4.2281)  loss_n_60: 4.5481 (4.5894)  loss_n_80: 4.6820 (4.7313)  loss_n_100: 4.7184 (4.7785)  triple_100: 495.1847 (499.5447)  triple_80: 497.9619 (502.1944)  triple_60: 493.7310 (498.9044)  triple_40: 467.0512 (474.0286)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 790/1724]  eta: 1:01:09  lr: 0.000000  loss: 1926.9078 (1991.9734)  loss_n_40: 4.2498 (4.2291)  loss_n_60: 4.6236 (4.5907)  loss_n_80: 4.7754 (4.7326)  loss_n_100: 4.8265 (4.7798)  triple_100: 484.3203 (499.2920)  triple_80: 487.0120 (501.9419)  triple_60: 482.5154 (498.6486)  triple_40: 454.0746 (473.7587)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 800/1724]  eta: 1:00:29  lr: 0.000000  loss: 1926.8574 (1991.4641)  loss_n_40: 4.2936 (4.2294)  loss_n_60: 4.6954 (4.5913)  loss_n_80: 4.7999 (4.7332)  loss_n_100: 4.8638 (4.7804)  triple_100: 483.2697 (499.1657)  triple_80: 485.9234 (501.8162)  triple_60: 482.3438 (498.5221)  triple_40: 456.3084 (473.6257)  time: 3.9302  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 810/1724]  eta: 0:59:50  lr: 0.000000  loss: 1945.3691 (1991.6822)  loss_n_40: 4.2246 (4.2297)  loss_n_60: 4.6457 (4.5917)  loss_n_80: 4.7667 (4.7337)  loss_n_100: 4.8116 (4.7808)  triple_100: 488.5660 (499.2207)  triple_80: 491.0174 (501.8704)  triple_60: 486.9865 (498.5770)  triple_40: 461.0924 (473.6782)  time: 3.9304  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 820/1724]  eta: 0:59:11  lr: 0.000000  loss: 1934.8447 (1991.3078)  loss_n_40: 4.2172 (4.2299)  loss_n_60: 4.6348 (4.5922)  loss_n_80: 4.7667 (4.7342)  loss_n_100: 4.8239 (4.7813)  triple_100: 486.1039 (499.1290)  triple_80: 488.4948 (501.7793)  triple_60: 484.4446 (498.4837)  triple_40: 457.0693 (473.5781)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 830/1724]  eta: 0:58:31  lr: 0.000000  loss: 1936.9800 (1991.5052)  loss_n_40: 4.2309 (4.2300)  loss_n_60: 4.6348 (4.5921)  loss_n_80: 4.7775 (4.7340)  loss_n_100: 4.8265 (4.7812)  triple_100: 487.1408 (499.1777)  triple_80: 489.4277 (501.8269)  triple_60: 484.8044 (498.5333)  triple_40: 457.0693 (473.6301)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 840/1724]  eta: 0:57:52  lr: 0.000000  loss: 1992.1448 (1990.9314)  loss_n_40: 4.2267 (4.2305)  loss_n_60: 4.6078 (4.5928)  loss_n_80: 4.7536 (4.7348)  loss_n_100: 4.8043 (4.7820)  triple_100: 499.5988 (499.0364)  triple_80: 502.4485 (501.6863)  triple_60: 499.3365 (498.3907)  triple_40: 471.6478 (473.4779)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 850/1724]  eta: 0:57:13  lr: 0.000000  loss: 1908.3815 (1990.5920)  loss_n_40: 4.2267 (4.2310)  loss_n_60: 4.6157 (4.5933)  loss_n_80: 4.7837 (4.7353)  loss_n_100: 4.8281 (4.7825)  triple_100: 478.7715 (498.9517)  triple_80: 481.4686 (501.6010)  triple_60: 477.8821 (498.3065)  triple_40: 451.8425 (473.3907)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 860/1724]  eta: 0:56:34  lr: 0.000000  loss: 1977.3490 (1990.7145)  loss_n_40: 4.2558 (4.2315)  loss_n_60: 4.6543 (4.5943)  loss_n_80: 4.8089 (4.7362)  loss_n_100: 4.8537 (4.7833)  triple_100: 495.9650 (498.9816)  triple_80: 498.4617 (501.6318)  triple_60: 494.8570 (498.3378)  triple_40: 469.5447 (473.4179)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 870/1724]  eta: 0:55:54  lr: 0.000000  loss: 2016.0719 (1990.7743)  loss_n_40: 4.2318 (4.2314)  loss_n_60: 4.6328 (4.5939)  loss_n_80: 4.7895 (4.7359)  loss_n_100: 4.8272 (4.7830)  triple_100: 506.2086 (498.9950)  triple_80: 508.3300 (501.6464)  triple_60: 504.3701 (498.3520)  triple_40: 479.0690 (473.4367)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 880/1724]  eta: 0:55:15  lr: 0.000000  loss: 2056.0735 (1991.2643)  loss_n_40: 4.1858 (4.2310)  loss_n_60: 4.5182 (4.5932)  loss_n_80: 4.6796 (4.7351)  loss_n_100: 4.7450 (4.7823)  triple_100: 514.8788 (499.1147)  triple_80: 517.4791 (501.7657)  triple_60: 515.0556 (498.4745)  triple_40: 488.3191 (473.5678)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 890/1724]  eta: 0:54:36  lr: 0.000000  loss: 1980.4351 (1990.9414)  loss_n_40: 4.2057 (4.2311)  loss_n_60: 4.6094 (4.5937)  loss_n_80: 4.7670 (4.7357)  loss_n_100: 4.7982 (4.7828)  triple_100: 496.1972 (499.0352)  triple_80: 498.9276 (501.6871)  triple_60: 495.8492 (498.3941)  triple_40: 467.9422 (473.4817)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 900/1724]  eta: 0:53:56  lr: 0.000000  loss: 1980.4351 (1991.3923)  loss_n_40: 4.2251 (4.2308)  loss_n_60: 4.6348 (4.5931)  loss_n_80: 4.7900 (4.7351)  loss_n_100: 4.8358 (4.7822)  triple_100: 496.1972 (499.1441)  triple_80: 498.9276 (501.7954)  triple_60: 495.8492 (498.5044)  triple_40: 467.9422 (473.6071)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 910/1724]  eta: 0:53:17  lr: 0.000000  loss: 2004.4177 (1991.5816)  loss_n_40: 4.1802 (4.2304)  loss_n_60: 4.5617 (4.5928)  loss_n_80: 4.6917 (4.7348)  loss_n_100: 4.7370 (4.7819)  triple_100: 503.7052 (499.1940)  triple_80: 505.6992 (501.8433)  triple_60: 501.6115 (498.5511)  triple_40: 475.3044 (473.6533)  time: 3.9275  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [ 920/1724]  eta: 0:52:38  lr: 0.000000  loss: 1924.0585 (1990.7509)  loss_n_40: 4.2518 (4.2320)  loss_n_60: 4.6667 (4.5947)  loss_n_80: 4.8226 (4.7366)  loss_n_100: 4.8704 (4.7837)  triple_100: 482.1208 (498.9848)  triple_80: 484.9930 (501.6375)  triple_60: 481.9087 (498.3449)  triple_40: 455.5065 (473.4367)  time: 3.9283  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 930/1724]  eta: 0:51:59  lr: 0.000000  loss: 1933.6565 (1990.9023)  loss_n_40: 4.2782 (4.2319)  loss_n_60: 4.6691 (4.5948)  loss_n_80: 4.8305 (4.7368)  loss_n_100: 4.8817 (4.7838)  triple_100: 484.8164 (499.0218)  triple_80: 488.0384 (501.6756)  triple_60: 484.2481 (498.3816)  triple_40: 457.9763 (473.4759)  time: 3.9289  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 940/1724]  eta: 0:51:19  lr: 0.000000  loss: 1949.9790 (1990.3975)  loss_n_40: 4.2637 (4.2334)  loss_n_60: 4.6594 (4.5961)  loss_n_80: 4.7866 (4.7380)  loss_n_100: 4.8214 (4.7850)  triple_100: 488.3936 (498.8924)  triple_80: 491.2429 (501.5486)  triple_60: 487.9555 (498.2559)  triple_40: 463.8406 (473.3482)  time: 3.9289  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 950/1724]  eta: 0:50:40  lr: 0.000000  loss: 1949.9790 (1990.3838)  loss_n_40: 4.2150 (4.2328)  loss_n_60: 4.5462 (4.5951)  loss_n_80: 4.6957 (4.7370)  loss_n_100: 4.7638 (4.7840)  triple_100: 488.3936 (498.8845)  triple_80: 491.2429 (501.5416)  triple_60: 487.9555 (498.2521)  triple_40: 463.8406 (473.3568)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 960/1724]  eta: 0:50:01  lr: 0.000000  loss: 1952.9408 (1989.9431)  loss_n_40: 4.2349 (4.2338)  loss_n_60: 4.5466 (4.5964)  loss_n_80: 4.7248 (4.7383)  loss_n_100: 4.7647 (4.7852)  triple_100: 489.0145 (498.7736)  triple_80: 491.6597 (501.4342)  triple_60: 488.5884 (498.1434)  triple_40: 464.9365 (473.2381)  time: 3.9284  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 970/1724]  eta: 0:49:21  lr: 0.000000  loss: 1931.1536 (1989.6662)  loss_n_40: 4.2591 (4.2335)  loss_n_60: 4.5726 (4.5964)  loss_n_80: 4.7452 (4.7384)  loss_n_100: 4.7776 (4.7852)  triple_100: 484.9106 (498.7064)  triple_80: 487.6358 (501.3678)  triple_60: 483.4121 (498.0756)  triple_40: 457.1890 (473.1628)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 980/1724]  eta: 0:48:42  lr: 0.000000  loss: 1926.4198 (1989.7629)  loss_n_40: 4.2075 (4.2332)  loss_n_60: 4.5726 (4.5961)  loss_n_80: 4.7298 (4.7381)  loss_n_100: 4.7776 (4.7850)  triple_100: 483.5535 (498.7328)  triple_80: 485.6147 (501.3930)  triple_60: 482.2932 (498.1003)  triple_40: 456.6873 (473.1845)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [ 990/1724]  eta: 0:48:03  lr: 0.000000  loss: 1927.1771 (1989.4792)  loss_n_40: 4.1944 (4.2336)  loss_n_60: 4.5785 (4.5970)  loss_n_80: 4.7421 (4.7390)  loss_n_100: 4.7845 (4.7857)  triple_100: 483.7864 (498.6623)  triple_80: 486.4164 (501.3241)  triple_60: 482.3162 (498.0299)  triple_40: 456.5431 (473.1076)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1000/1724]  eta: 0:47:24  lr: 0.000000  loss: 1933.7827 (1989.3998)  loss_n_40: 4.1833 (4.2333)  loss_n_60: 4.5785 (4.5968)  loss_n_80: 4.7479 (4.7389)  loss_n_100: 4.7787 (4.7856)  triple_100: 484.4930 (498.6435)  triple_80: 487.8258 (501.3047)  triple_60: 484.2349 (498.0099)  triple_40: 457.1989 (473.0871)  time: 3.9292  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1010/1724]  eta: 0:46:44  lr: 0.000000  loss: 2003.1384 (1990.2143)  loss_n_40: 4.1211 (4.2321)  loss_n_60: 4.5285 (4.5954)  loss_n_80: 4.6881 (4.7374)  loss_n_100: 4.7393 (4.7842)  triple_100: 502.3134 (498.8486)  triple_80: 505.0056 (501.5086)  triple_60: 501.7977 (498.2144)  triple_40: 475.9496 (473.2936)  time: 3.9293  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1020/1724]  eta: 0:46:05  lr: 0.000000  loss: 1950.1437 (1989.7558)  loss_n_40: 4.1425 (4.2322)  loss_n_60: 4.5595 (4.5957)  loss_n_80: 4.7097 (4.7378)  loss_n_100: 4.7741 (4.7846)  triple_100: 490.2872 (498.7371)  triple_80: 493.1384 (501.3966)  triple_60: 488.8245 (498.1006)  triple_40: 463.2484 (473.1713)  time: 3.9294  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1030/1724]  eta: 0:45:26  lr: 0.000000  loss: 1945.2693 (1989.9602)  loss_n_40: 4.1503 (4.2317)  loss_n_60: 4.5595 (4.5950)  loss_n_80: 4.7097 (4.7371)  loss_n_100: 4.7741 (4.7839)  triple_100: 488.6389 (498.7883)  triple_80: 491.5283 (501.4468)  triple_60: 487.4402 (498.1524)  triple_40: 459.2079 (473.2249)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1040/1724]  eta: 0:44:47  lr: 0.000000  loss: 1938.5509 (1989.7609)  loss_n_40: 4.1824 (4.2316)  loss_n_60: 4.4973 (4.5951)  loss_n_80: 4.6554 (4.7372)  loss_n_100: 4.7217 (4.7840)  triple_100: 486.9978 (498.7372)  triple_80: 489.3605 (501.3966)  triple_60: 485.6749 (498.1028)  triple_40: 459.1459 (473.1765)  time: 3.9290  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1050/1724]  eta: 0:44:07  lr: 0.000000  loss: 1935.8374 (1989.9712)  loss_n_40: 4.2153 (4.2317)  loss_n_60: 4.6607 (4.5950)  loss_n_80: 4.8084 (4.7371)  loss_n_100: 4.8452 (4.7838)  triple_100: 484.6685 (498.7893)  triple_80: 487.6934 (501.4492)  triple_60: 484.9915 (498.1552)  triple_40: 459.1459 (473.2300)  time: 3.9303  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1060/1724]  eta: 0:43:28  lr: 0.000000  loss: 1994.3658 (1990.0324)  loss_n_40: 4.2954 (4.2322)  loss_n_60: 4.6766 (4.5959)  loss_n_80: 4.8022 (4.7379)  loss_n_100: 4.8523 (4.7846)  triple_100: 500.4955 (498.8023)  triple_80: 503.2854 (501.4631)  triple_60: 499.5894 (498.1710)  triple_40: 474.5046 (473.2455)  time: 3.9301  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1070/1724]  eta: 0:42:49  lr: 0.000000  loss: 2013.4796 (1990.7144)  loss_n_40: 4.2265 (4.2312)  loss_n_60: 4.5626 (4.5946)  loss_n_80: 4.6977 (4.7366)  loss_n_100: 4.7421 (4.7835)  triple_100: 504.2112 (498.9738)  triple_80: 507.4931 (501.6314)  triple_60: 504.1373 (498.3403)  triple_40: 480.1559 (473.4229)  time: 3.9297  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1080/1724]  eta: 0:42:09  lr: 0.000000  loss: 2013.4796 (1990.4527)  loss_n_40: 4.2265 (4.2317)  loss_n_60: 4.5254 (4.5948)  loss_n_80: 4.6505 (4.7368)  loss_n_100: 4.7083 (4.7837)  triple_100: 504.2112 (498.9064)  triple_80: 507.4931 (501.5646)  triple_60: 504.1373 (498.2754)  triple_40: 478.5108 (473.3593)  time: 3.9307  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1090/1724]  eta: 0:41:30  lr: 0.000000  loss: 1921.7579 (1990.1199)  loss_n_40: 4.2697 (4.2318)  loss_n_60: 4.5892 (4.5952)  loss_n_80: 4.7353 (4.7373)  loss_n_100: 4.7770 (4.7841)  triple_100: 481.1894 (498.8245)  triple_80: 483.3615 (501.4833)  triple_60: 480.7883 (498.1931)  triple_40: 457.4213 (473.2706)  time: 3.9303  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1100/1724]  eta: 0:40:51  lr: 0.000000  loss: 1957.4474 (1990.3520)  loss_n_40: 4.2144 (4.2316)  loss_n_60: 4.5545 (4.5951)  loss_n_80: 4.7198 (4.7372)  loss_n_100: 4.7484 (4.7841)  triple_100: 491.0109 (498.8840)  triple_80: 493.7028 (501.5410)  triple_60: 490.3279 (498.2507)  triple_40: 463.8115 (473.3282)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1110/1724]  eta: 0:40:12  lr: 0.000000  loss: 1978.8883 (1989.8802)  loss_n_40: 4.2030 (4.2318)  loss_n_60: 4.5675 (4.5953)  loss_n_80: 4.7414 (4.7374)  loss_n_100: 4.7908 (4.7842)  triple_100: 495.8777 (498.7652)  triple_80: 498.7789 (501.4234)  triple_60: 495.5210 (498.1336)  triple_40: 470.6770 (473.2094)  time: 3.9283  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1120/1724]  eta: 0:39:32  lr: 0.000000  loss: 1935.0066 (1989.8352)  loss_n_40: 4.2670 (4.2316)  loss_n_60: 4.6176 (4.5953)  loss_n_80: 4.7793 (4.7375)  loss_n_100: 4.8071 (4.7844)  triple_100: 486.1243 (498.7555)  triple_80: 488.8523 (501.4133)  triple_60: 484.6908 (498.1231)  triple_40: 456.9810 (473.1947)  time: 3.9298  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1130/1724]  eta: 0:38:53  lr: 0.000000  loss: 1988.9008 (1989.8748)  loss_n_40: 4.2761 (4.2314)  loss_n_60: 4.6122 (4.5950)  loss_n_80: 4.7445 (4.7373)  loss_n_100: 4.7997 (4.7842)  triple_100: 498.2496 (498.7648)  triple_80: 500.7921 (501.4214)  triple_60: 497.5783 (498.1314)  triple_40: 471.7587 (473.2094)  time: 3.9307  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [1140/1724]  eta: 0:38:14  lr: 0.000000  loss: 1913.1772 (1989.5066)  loss_n_40: 4.2999 (4.2323)  loss_n_60: 4.6751 (4.5960)  loss_n_80: 4.8137 (4.7383)  loss_n_100: 4.8555 (4.7852)  triple_100: 480.1114 (498.6737)  triple_80: 482.8911 (501.3307)  triple_60: 479.1999 (498.0398)  triple_40: 451.3536 (473.1107)  time: 3.9304  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1150/1724]  eta: 0:37:34  lr: 0.000000  loss: 1930.9399 (1989.8149)  loss_n_40: 4.2920 (4.2320)  loss_n_60: 4.7048 (4.5959)  loss_n_80: 4.8551 (4.7382)  loss_n_100: 4.8937 (4.7852)  triple_100: 483.1714 (498.7528)  triple_80: 485.7761 (501.4092)  triple_60: 483.1620 (498.1168)  triple_40: 456.3021 (473.1848)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1160/1724]  eta: 0:36:55  lr: 0.000000  loss: 1997.9260 (1989.6919)  loss_n_40: 4.1500 (4.2321)  loss_n_60: 4.5015 (4.5962)  loss_n_80: 4.6646 (4.7386)  loss_n_100: 4.6996 (4.7855)  triple_100: 500.4047 (498.7242)  triple_80: 503.7577 (501.3796)  triple_60: 500.3912 (498.0863)  triple_40: 474.6229 (473.1494)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1170/1724]  eta: 0:36:16  lr: 0.000000  loss: 1965.2390 (1989.8075)  loss_n_40: 4.2221 (4.2324)  loss_n_60: 4.6470 (4.5967)  loss_n_80: 4.8190 (4.7391)  loss_n_100: 4.8519 (4.7860)  triple_100: 494.5052 (498.7558)  triple_80: 496.6977 (501.4117)  triple_60: 492.0504 (498.1165)  triple_40: 463.6199 (473.1691)  time: 3.9276  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1180/1724]  eta: 0:35:37  lr: 0.000000  loss: 1965.2390 (1989.6538)  loss_n_40: 4.2591 (4.2325)  loss_n_60: 4.6470 (4.5967)  loss_n_80: 4.7833 (4.7391)  loss_n_100: 4.8320 (4.7860)  triple_100: 491.9034 (498.7158)  triple_80: 495.2973 (501.3728)  triple_60: 492.0504 (498.0783)  triple_40: 465.5002 (473.1325)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1190/1724]  eta: 0:34:57  lr: 0.000000  loss: 1966.5188 (1990.1450)  loss_n_40: 4.2036 (4.2319)  loss_n_60: 4.5789 (4.5960)  loss_n_80: 4.7111 (4.7384)  loss_n_100: 4.7517 (4.7853)  triple_100: 491.9034 (498.8370)  triple_80: 495.2973 (501.4937)  triple_60: 492.1493 (498.2005)  triple_40: 469.8334 (473.2621)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1200/1724]  eta: 0:34:18  lr: 0.000000  loss: 1997.8959 (1990.2859)  loss_n_40: 4.1293 (4.2313)  loss_n_60: 4.4989 (4.5953)  loss_n_80: 4.6670 (4.7376)  loss_n_100: 4.7142 (4.7846)  triple_100: 499.5576 (498.8724)  triple_80: 502.9421 (501.5278)  triple_60: 500.2876 (498.2354)  triple_40: 475.2087 (473.3015)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1210/1724]  eta: 0:33:39  lr: 0.000000  loss: 1966.9620 (1990.1457)  loss_n_40: 4.1833 (4.2315)  loss_n_60: 4.5806 (4.5955)  loss_n_80: 4.7238 (4.7378)  loss_n_100: 4.7815 (4.7848)  triple_100: 492.1002 (498.8349)  triple_80: 494.9919 (501.4911)  triple_60: 492.6098 (498.2004)  triple_40: 468.9515 (473.2696)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1220/1724]  eta: 0:32:59  lr: 0.000000  loss: 1933.2192 (1989.9350)  loss_n_40: 4.2405 (4.2313)  loss_n_60: 4.5820 (4.5953)  loss_n_80: 4.7401 (4.7377)  loss_n_100: 4.7900 (4.7846)  triple_100: 484.4296 (498.7835)  triple_80: 487.5043 (501.4399)  triple_60: 483.8708 (498.1489)  triple_40: 458.9846 (473.2137)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1230/1724]  eta: 0:32:20  lr: 0.000000  loss: 1954.4292 (1990.1838)  loss_n_40: 4.1245 (4.2304)  loss_n_60: 4.5334 (4.5947)  loss_n_80: 4.6979 (4.7372)  loss_n_100: 4.7270 (4.7841)  triple_100: 490.7399 (498.8503)  triple_80: 493.8427 (501.5049)  triple_60: 489.5618 (498.2111)  triple_40: 462.2020 (473.2711)  time: 3.9302  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1240/1724]  eta: 0:31:41  lr: 0.000000  loss: 1971.6162 (1990.0552)  loss_n_40: 4.1989 (4.2307)  loss_n_60: 4.5783 (4.5953)  loss_n_80: 4.7242 (4.7378)  loss_n_100: 4.7779 (4.7847)  triple_100: 493.7719 (498.8190)  triple_80: 497.3600 (501.4746)  triple_60: 493.8523 (498.1795)  triple_40: 466.2787 (473.2337)  time: 3.9302  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1250/1724]  eta: 0:31:02  lr: 0.000000  loss: 1937.6829 (1990.2138)  loss_n_40: 4.2525 (4.2302)  loss_n_60: 4.6726 (4.5950)  loss_n_80: 4.8013 (4.7375)  loss_n_100: 4.8520 (4.7844)  triple_100: 485.9511 (498.8611)  triple_80: 488.3425 (501.5147)  triple_60: 484.9923 (498.2191)  triple_40: 459.5625 (473.2717)  time: 3.9305  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1260/1724]  eta: 0:30:22  lr: 0.000000  loss: 1969.8488 (1989.9664)  loss_n_40: 4.2143 (4.2308)  loss_n_60: 4.6726 (4.5958)  loss_n_80: 4.8013 (4.7384)  loss_n_100: 4.8520 (4.7853)  triple_100: 492.8413 (498.8014)  triple_80: 496.4389 (501.4558)  triple_60: 493.3663 (498.1583)  triple_40: 467.7411 (473.2005)  time: 3.9318  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1270/1724]  eta: 0:29:43  lr: 0.000000  loss: 1985.8286 (1990.1527)  loss_n_40: 4.2212 (4.2311)  loss_n_60: 4.5893 (4.5958)  loss_n_80: 4.7312 (4.7383)  loss_n_100: 4.7859 (4.7852)  triple_100: 499.0603 (498.8446)  triple_80: 501.4065 (501.4992)  triple_60: 496.9638 (498.2043)  triple_40: 472.8919 (473.2542)  time: 3.9320  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1280/1724]  eta: 0:29:04  lr: 0.000000  loss: 2009.1562 (1989.9784)  loss_n_40: 4.1880 (4.2308)  loss_n_60: 4.4961 (4.5956)  loss_n_80: 4.6497 (4.7382)  loss_n_100: 4.7140 (4.7852)  triple_100: 502.6436 (498.8021)  triple_80: 505.7274 (501.4564)  triple_60: 503.3093 (498.1611)  triple_40: 479.6895 (473.2090)  time: 3.9319  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1290/1724]  eta: 0:28:25  lr: 0.000000  loss: 1948.6045 (1989.6624)  loss_n_40: 4.1810 (4.2313)  loss_n_60: 4.6376 (4.5965)  loss_n_80: 4.7815 (4.7391)  loss_n_100: 4.8045 (4.7860)  triple_100: 489.6669 (498.7247)  triple_80: 492.4468 (501.3800)  triple_60: 488.2087 (498.0830)  triple_40: 461.8267 (473.1218)  time: 3.9318  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1300/1724]  eta: 0:27:45  lr: 0.000000  loss: 1937.6493 (1989.2379)  loss_n_40: 4.2579 (4.2314)  loss_n_60: 4.6648 (4.5968)  loss_n_80: 4.8285 (4.7395)  loss_n_100: 4.8661 (4.7865)  triple_100: 486.0118 (498.6218)  triple_80: 488.6755 (501.2767)  triple_60: 485.3793 (497.9775)  triple_40: 459.3806 (473.0078)  time: 3.9309  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1310/1724]  eta: 0:27:06  lr: 0.000000  loss: 1944.2783 (1989.2185)  loss_n_40: 4.2487 (4.2314)  loss_n_60: 4.6474 (4.5969)  loss_n_80: 4.8090 (4.7396)  loss_n_100: 4.8375 (4.7865)  triple_100: 487.0749 (498.6173)  triple_80: 490.2312 (501.2724)  triple_60: 486.9953 (497.9731)  triple_40: 460.5069 (473.0013)  time: 3.9295  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1320/1724]  eta: 0:26:27  lr: 0.000000  loss: 2011.3364 (1989.4719)  loss_n_40: 4.1777 (4.2309)  loss_n_60: 4.5977 (4.5962)  loss_n_80: 4.7005 (4.7389)  loss_n_100: 4.7531 (4.7859)  triple_100: 504.5203 (498.6789)  triple_80: 506.4652 (501.3336)  triple_60: 503.2551 (498.0359)  triple_40: 477.3631 (473.0716)  time: 3.9283  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1330/1724]  eta: 0:25:47  lr: 0.000000  loss: 2037.8867 (1989.7702)  loss_n_40: 4.1546 (4.2307)  loss_n_60: 4.5346 (4.5959)  loss_n_80: 4.6920 (4.7386)  loss_n_100: 4.7399 (4.7855)  triple_100: 508.8997 (498.7529)  triple_80: 512.0666 (501.4077)  triple_60: 509.8904 (498.1110)  triple_40: 485.1699 (473.1479)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1340/1724]  eta: 0:25:08  lr: 0.000000  loss: 1973.7576 (1989.7580)  loss_n_40: 4.1546 (4.2306)  loss_n_60: 4.5214 (4.5955)  loss_n_80: 4.6591 (4.7382)  loss_n_100: 4.7241 (4.7852)  triple_100: 495.4316 (498.7487)  triple_80: 498.2844 (501.4025)  triple_60: 494.5785 (498.1068)  triple_40: 466.9153 (473.1504)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1350/1724]  eta: 0:24:29  lr: 0.000000  loss: 2000.3893 (1989.8441)  loss_n_40: 4.2067 (4.2306)  loss_n_60: 4.5193 (4.5957)  loss_n_80: 4.6607 (4.7384)  loss_n_100: 4.7201 (4.7854)  triple_100: 500.2192 (498.7701)  triple_80: 503.1190 (501.4239)  triple_60: 500.5970 (498.1285)  triple_40: 477.0462 (473.1715)  time: 3.9269  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [1360/1724]  eta: 0:23:49  lr: 0.000000  loss: 2005.8512 (1989.9008)  loss_n_40: 4.2301 (4.2307)  loss_n_60: 4.5654 (4.5956)  loss_n_80: 4.7184 (4.7384)  loss_n_100: 4.7824 (4.7854)  triple_100: 502.9863 (498.7834)  triple_80: 505.5444 (501.4376)  triple_60: 502.0543 (498.1431)  triple_40: 477.2285 (473.1866)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1370/1724]  eta: 0:23:10  lr: 0.000000  loss: 1992.0946 (1989.6434)  loss_n_40: 4.2222 (4.2304)  loss_n_60: 4.5668 (4.5954)  loss_n_80: 4.7184 (4.7381)  loss_n_100: 4.7806 (4.7852)  triple_100: 499.5812 (498.7224)  triple_80: 501.6229 (501.3749)  triple_60: 498.1107 (498.0783)  triple_40: 472.6076 (473.1188)  time: 3.9282  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:1]  [1380/1724]  eta: 0:22:31  lr: 0.000000  loss: 1938.7418 (1989.6144)  loss_n_40: 4.1555 (4.2304)  loss_n_60: 4.5668 (4.5953)  loss_n_80: 4.7182 (4.7381)  loss_n_100: 4.7535 (4.7851)  triple_100: 485.1615 (498.7163)  triple_80: 487.8023 (501.3672)  triple_60: 484.9151 (498.0705)  triple_40: 461.9158 (473.1114)  time: 3.9290  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1390/1724]  eta: 0:21:52  lr: 0.000000  loss: 1951.9376 (1989.5526)  loss_n_40: 4.1796 (4.2304)  loss_n_60: 4.5975 (4.5955)  loss_n_80: 4.7407 (4.7383)  loss_n_100: 4.7940 (4.7853)  triple_100: 489.7686 (498.7024)  triple_80: 492.5765 (501.3542)  triple_60: 488.8091 (498.0559)  triple_40: 461.9158 (473.0906)  time: 3.9295  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1400/1724]  eta: 0:21:12  lr: 0.000000  loss: 1994.1241 (1989.8697)  loss_n_40: 4.2163 (4.2303)  loss_n_60: 4.5334 (4.5954)  loss_n_80: 4.6792 (4.7381)  loss_n_100: 4.7235 (4.7851)  triple_100: 500.2825 (498.7797)  triple_80: 502.8051 (501.4318)  triple_60: 499.2165 (498.1354)  triple_40: 473.8677 (473.1738)  time: 3.9308  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1410/1724]  eta: 0:20:33  lr: 0.000000  loss: 1960.3208 (1989.5236)  loss_n_40: 4.3412 (4.2312)  loss_n_60: 4.6908 (4.5962)  loss_n_80: 4.8155 (4.7389)  loss_n_100: 4.8522 (4.7859)  triple_100: 492.3579 (498.6914)  triple_80: 494.9692 (501.3447)  triple_60: 491.0102 (498.0492)  triple_40: 464.0952 (473.0862)  time: 3.9312  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1420/1724]  eta: 0:19:54  lr: 0.000000  loss: 1946.0878 (1989.6617)  loss_n_40: 4.3412 (4.2313)  loss_n_60: 4.6908 (4.5964)  loss_n_80: 4.8155 (4.7391)  loss_n_100: 4.8522 (4.7861)  triple_100: 487.6868 (498.7259)  triple_80: 490.7695 (501.3798)  triple_60: 487.5605 (498.0841)  triple_40: 460.8371 (473.1191)  time: 3.9303  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1430/1724]  eta: 0:19:15  lr: 0.000000  loss: 1990.5422 (1990.1037)  loss_n_40: 4.1816 (4.2305)  loss_n_60: 4.5349 (4.5957)  loss_n_80: 4.6781 (4.7385)  loss_n_100: 4.7265 (4.7854)  triple_100: 498.3658 (498.8380)  triple_80: 501.4171 (501.4910)  triple_60: 498.5887 (498.1953)  triple_40: 474.1088 (473.2293)  time: 3.9301  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1440/1724]  eta: 0:18:35  lr: 0.000000  loss: 1967.5658 (1990.2979)  loss_n_40: 4.1215 (4.2302)  loss_n_60: 4.5397 (4.5953)  loss_n_80: 4.7015 (4.7380)  loss_n_100: 4.7288 (4.7850)  triple_100: 493.5167 (498.8852)  triple_80: 496.1821 (501.5387)  triple_60: 492.5931 (498.2439)  triple_40: 468.4250 (473.2816)  time: 3.9313  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1450/1724]  eta: 0:17:56  lr: 0.000000  loss: 1939.3234 (1990.1715)  loss_n_40: 4.2452 (4.2301)  loss_n_60: 4.6565 (4.5955)  loss_n_80: 4.8162 (4.7383)  loss_n_100: 4.8525 (4.7852)  triple_100: 487.6973 (498.8565)  triple_80: 489.9498 (501.5095)  triple_60: 485.6488 (498.2122)  triple_40: 459.1142 (473.2443)  time: 3.9321  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1460/1724]  eta: 0:17:17  lr: 0.000000  loss: 1972.2795 (1990.1981)  loss_n_40: 4.2128 (4.2301)  loss_n_60: 4.6090 (4.5954)  loss_n_80: 4.7434 (4.7382)  loss_n_100: 4.7969 (4.7852)  triple_100: 495.3041 (498.8632)  triple_80: 497.7213 (501.5165)  triple_60: 494.5437 (498.2197)  triple_40: 466.6442 (473.2498)  time: 3.9303  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1470/1724]  eta: 0:16:37  lr: 0.000000  loss: 1986.4885 (1990.2911)  loss_n_40: 4.1547 (4.2296)  loss_n_60: 4.5186 (4.5949)  loss_n_80: 4.6713 (4.7377)  loss_n_100: 4.7217 (4.7847)  triple_100: 497.5747 (498.8867)  triple_80: 500.5265 (501.5395)  triple_60: 497.0052 (498.2429)  triple_40: 473.2402 (473.2751)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1480/1724]  eta: 0:15:58  lr: 0.000000  loss: 1947.4131 (1990.0620)  loss_n_40: 4.1819 (4.2296)  loss_n_60: 4.5644 (4.5951)  loss_n_80: 4.7413 (4.7380)  loss_n_100: 4.7773 (4.7849)  triple_100: 488.3268 (498.8321)  triple_80: 491.3776 (501.4855)  triple_60: 487.4963 (498.1864)  triple_40: 461.1395 (473.2104)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1490/1724]  eta: 0:15:19  lr: 0.000000  loss: 1960.6334 (1990.2192)  loss_n_40: 4.1819 (4.2295)  loss_n_60: 4.5735 (4.5950)  loss_n_80: 4.7378 (4.7379)  loss_n_100: 4.7698 (4.7848)  triple_100: 492.9226 (498.8723)  triple_80: 494.9681 (501.5258)  triple_60: 491.1861 (498.2256)  triple_40: 463.5495 (473.2484)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1500/1724]  eta: 0:14:40  lr: 0.000000  loss: 1985.9661 (1990.5063)  loss_n_40: 4.1856 (4.2294)  loss_n_60: 4.5840 (4.5948)  loss_n_80: 4.7168 (4.7377)  loss_n_100: 4.7698 (4.7847)  triple_100: 498.2448 (498.9432)  triple_80: 501.4327 (501.5961)  triple_60: 497.5128 (498.2959)  triple_40: 469.6548 (473.3244)  time: 3.9302  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1510/1724]  eta: 0:14:00  lr: 0.000000  loss: 1964.2271 (1990.3524)  loss_n_40: 4.2131 (4.2295)  loss_n_60: 4.6107 (4.5950)  loss_n_80: 4.7511 (4.7379)  loss_n_100: 4.8218 (4.7849)  triple_100: 491.6439 (498.9053)  triple_80: 494.4310 (501.5581)  triple_60: 491.6746 (498.2575)  triple_40: 467.4355 (473.2843)  time: 3.9298  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1520/1724]  eta: 0:13:21  lr: 0.000000  loss: 1964.2271 (1990.3075)  loss_n_40: 4.2453 (4.2296)  loss_n_60: 4.6446 (4.5952)  loss_n_80: 4.7819 (4.7381)  loss_n_100: 4.8302 (4.7851)  triple_100: 491.6439 (498.8935)  triple_80: 494.4310 (501.5469)  triple_60: 491.6746 (498.2466)  triple_40: 467.4355 (473.2725)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1530/1724]  eta: 0:12:42  lr: 0.000000  loss: 2045.2759 (1990.6765)  loss_n_40: 4.2339 (4.2292)  loss_n_60: 4.5956 (4.5946)  loss_n_80: 4.6842 (4.7374)  loss_n_100: 4.7391 (4.7844)  triple_100: 512.5850 (498.9858)  triple_80: 515.3071 (501.6385)  triple_60: 511.8375 (498.3391)  triple_40: 486.1217 (473.3674)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1540/1724]  eta: 0:12:02  lr: 0.000000  loss: 2063.8350 (1991.1041)  loss_n_40: 4.1652 (4.2294)  loss_n_60: 4.5171 (4.5947)  loss_n_80: 4.6608 (4.7375)  loss_n_100: 4.7228 (4.7844)  triple_100: 517.7267 (499.0888)  triple_80: 520.0381 (501.7426)  triple_60: 516.7366 (498.4461)  triple_40: 491.3889 (473.4807)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1550/1724]  eta: 0:11:23  lr: 0.000000  loss: 1942.7606 (1990.8396)  loss_n_40: 4.2978 (4.2297)  loss_n_60: 4.6608 (4.5951)  loss_n_80: 4.7842 (4.7378)  loss_n_100: 4.8301 (4.7847)  triple_100: 486.4493 (499.0225)  triple_80: 488.9909 (501.6770)  triple_60: 486.0628 (498.3802)  triple_40: 461.4076 (473.4124)  time: 3.9295  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1560/1724]  eta: 0:10:44  lr: 0.000000  loss: 1956.7711 (1991.2059)  loss_n_40: 4.2570 (4.2296)  loss_n_60: 4.5682 (4.5947)  loss_n_80: 4.7311 (4.7373)  loss_n_100: 4.8041 (4.7844)  triple_100: 491.7635 (499.1130)  triple_80: 494.7104 (501.7667)  triple_60: 490.1810 (498.4706)  triple_40: 462.0915 (473.5095)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1570/1724]  eta: 0:10:05  lr: 0.000000  loss: 2093.0898 (1991.9419)  loss_n_40: 4.0828 (4.2285)  loss_n_60: 4.4201 (4.5934)  loss_n_80: 4.5692 (4.7360)  loss_n_100: 4.6396 (4.7830)  triple_100: 523.8505 (499.2964)  triple_80: 526.4806 (501.9490)  triple_60: 523.7929 (498.6544)  triple_40: 501.7817 (473.7013)  time: 3.9278  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [1580/1724]  eta: 0:09:25  lr: 0.000000  loss: 2062.5269 (1991.9090)  loss_n_40: 4.1380 (4.2290)  loss_n_60: 4.4684 (4.5940)  loss_n_80: 4.6059 (4.7366)  loss_n_100: 4.6720 (4.7836)  triple_100: 516.7838 (499.2879)  triple_80: 519.4800 (501.9409)  triple_60: 516.4351 (498.6462)  triple_40: 491.2744 (473.6909)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1590/1724]  eta: 0:08:46  lr: 0.000000  loss: 1951.7910 (1991.7449)  loss_n_40: 4.2935 (4.2297)  loss_n_60: 4.6779 (4.5947)  loss_n_80: 4.8167 (4.7372)  loss_n_100: 4.8628 (4.7842)  triple_100: 489.4798 (499.2457)  triple_80: 492.3859 (501.9001)  triple_60: 488.8874 (498.6055)  triple_40: 461.5864 (473.6479)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1600/1724]  eta: 0:08:07  lr: 0.000000  loss: 1921.2196 (1991.3871)  loss_n_40: 4.3008 (4.2305)  loss_n_60: 4.6779 (4.5957)  loss_n_80: 4.8167 (4.7383)  loss_n_100: 4.8628 (4.7852)  triple_100: 482.7525 (499.1556)  triple_80: 485.6913 (501.8126)  triple_60: 481.2686 (498.5171)  triple_40: 453.7303 (473.5521)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1610/1724]  eta: 0:07:27  lr: 0.000000  loss: 1952.3281 (1991.5737)  loss_n_40: 4.2138 (4.2300)  loss_n_60: 4.6319 (4.5954)  loss_n_80: 4.7865 (4.7381)  loss_n_100: 4.8179 (4.7850)  triple_100: 488.4119 (499.2054)  triple_80: 491.7328 (501.8613)  triple_60: 488.6981 (498.5634)  triple_40: 464.0815 (473.5952)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1620/1724]  eta: 0:06:48  lr: 0.000000  loss: 1927.0336 (1991.3784)  loss_n_40: 4.2187 (4.2303)  loss_n_60: 4.6319 (4.5957)  loss_n_80: 4.7865 (4.7384)  loss_n_100: 4.8147 (4.7853)  triple_100: 482.3198 (499.1566)  triple_80: 485.1615 (501.8133)  triple_60: 482.3487 (498.5142)  triple_40: 456.5820 (473.5446)  time: 3.9289  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1630/1724]  eta: 0:06:09  lr: 0.000000  loss: 1927.0336 (1991.4314)  loss_n_40: 4.2251 (4.2299)  loss_n_60: 4.6075 (4.5953)  loss_n_80: 4.7711 (4.7380)  loss_n_100: 4.8100 (4.7849)  triple_100: 482.3198 (499.1711)  triple_80: 485.1615 (501.8270)  triple_60: 482.3487 (498.5269)  triple_40: 456.5820 (473.5582)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1640/1724]  eta: 0:05:30  lr: 0.000000  loss: 1969.8085 (1991.4222)  loss_n_40: 4.2257 (4.2298)  loss_n_60: 4.6046 (4.5953)  loss_n_80: 4.7711 (4.7380)  loss_n_100: 4.8054 (4.7849)  triple_100: 493.8334 (499.1694)  triple_80: 497.1742 (501.8251)  triple_60: 493.6109 (498.5246)  triple_40: 466.1311 (473.5550)  time: 3.9276  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1650/1724]  eta: 0:04:50  lr: 0.000000  loss: 1969.8085 (1991.3465)  loss_n_40: 4.2387 (4.2302)  loss_n_60: 4.6273 (4.5959)  loss_n_80: 4.7835 (4.7386)  loss_n_100: 4.8199 (4.7855)  triple_100: 493.8334 (499.1497)  triple_80: 497.1742 (501.8065)  triple_60: 493.6109 (498.5056)  triple_40: 466.4174 (473.5345)  time: 3.9296  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1660/1724]  eta: 0:04:11  lr: 0.000000  loss: 1991.2979 (1991.5169)  loss_n_40: 4.1861 (4.2298)  loss_n_60: 4.5969 (4.5956)  loss_n_80: 4.7335 (4.7383)  loss_n_100: 4.8042 (4.7852)  triple_100: 499.7712 (499.1932)  triple_80: 502.2177 (501.8491)  triple_60: 498.4356 (498.5483)  triple_40: 472.3469 (473.5776)  time: 3.9309  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1670/1724]  eta: 0:03:32  lr: 0.000000  loss: 2000.3903 (1991.4453)  loss_n_40: 4.2014 (4.2301)  loss_n_60: 4.5969 (4.5962)  loss_n_80: 4.7335 (4.7389)  loss_n_100: 4.8058 (4.7858)  triple_100: 502.9038 (499.1751)  triple_80: 504.6562 (501.8324)  triple_60: 500.9362 (498.5305)  triple_40: 473.2377 (473.5563)  time: 3.9320  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1680/1724]  eta: 0:02:52  lr: 0.000000  loss: 1952.3574 (1991.1626)  loss_n_40: 4.2555 (4.2302)  loss_n_60: 4.6607 (4.5964)  loss_n_80: 4.8230 (4.7392)  loss_n_100: 4.8657 (4.7860)  triple_100: 489.5169 (499.1062)  triple_80: 492.4530 (501.7643)  triple_60: 489.1142 (498.4613)  triple_40: 462.3067 (473.4790)  time: 3.9319  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1690/1724]  eta: 0:02:13  lr: 0.000000  loss: 1898.3885 (1990.7521)  loss_n_40: 4.2478 (4.2306)  loss_n_60: 4.6695 (4.5970)  loss_n_80: 4.8432 (4.7398)  loss_n_100: 4.8737 (4.7865)  triple_100: 477.0014 (499.0027)  triple_80: 479.5114 (501.6626)  triple_60: 475.4123 (498.3591)  triple_40: 449.3619 (473.3739)  time: 3.9294  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1700/1724]  eta: 0:01:34  lr: 0.000000  loss: 1927.8772 (1990.7081)  loss_n_40: 4.2309 (4.2308)  loss_n_60: 4.6370 (4.5972)  loss_n_80: 4.8097 (4.7400)  loss_n_100: 4.8763 (4.7867)  triple_100: 483.6465 (498.9917)  triple_80: 486.6230 (501.6512)  triple_60: 482.9058 (498.3482)  triple_40: 456.3053 (473.3624)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1710/1724]  eta: 0:00:55  lr: 0.000000  loss: 1969.6023 (1990.7804)  loss_n_40: 4.1858 (4.2308)  loss_n_60: 4.5728 (4.5972)  loss_n_80: 4.7413 (4.7400)  loss_n_100: 4.7936 (4.7868)  triple_100: 495.3802 (499.0102)  triple_80: 497.9666 (501.6699)  triple_60: 493.4522 (498.3667)  triple_40: 464.5838 (473.3789)  time: 3.9275  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:1]  [1720/1724]  eta: 0:00:15  lr: 0.000000  loss: 1992.6992 (1991.0112)  loss_n_40: 4.1498 (4.2302)  loss_n_60: 4.5048 (4.5966)  loss_n_80: 4.6308 (4.7394)  loss_n_100: 4.6836 (4.7862)  triple_100: 499.2910 (499.0686)  triple_80: 502.3546 (501.7273)  triple_60: 499.0233 (498.4245)  triple_40: 472.8076 (473.4383)  time: 3.9276  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1]  [1723/1724]  eta: 0:00:03  lr: 0.000000  loss: 1948.1456 (1990.8857)  loss_n_40: 4.1498 (4.2302)  loss_n_60: 4.5626 (4.5967)  loss_n_80: 4.7198 (4.7395)  loss_n_100: 4.7704 (4.7863)  triple_100: 489.3134 (499.0374)  triple_80: 491.8691 (501.6967)  triple_60: 488.0864 (498.3936)  triple_40: 462.5110 (473.4052)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:1] Total time: 1:52:53 (3.9289 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 1948.1456 (1990.8857)  loss_n_40: 4.1498 (4.2302)  loss_n_60: 4.5626 (4.5967)  loss_n_80: 4.7198 (4.7395)  loss_n_100: 4.7704 (4.7863)  triple_100: 489.3134 (499.0374)  triple_80: 491.8691 (501.6967)  triple_60: 488.0864 (498.3936)  triple_40: 462.5110 (473.4052)\n",
      "Valid: [epoch:1]  [  0/845]  eta: 0:10:23  loss: 1775.3284 (1775.3284)  loss_n_40: 4.2679 (4.2679)  loss_n_60: 4.5794 (4.5794)  loss_n_80: 4.7411 (4.7411)  loss_n_100: 4.8161 (4.8161)  triple_100: 445.2107 (445.2107)  triple_80: 447.7355 (447.7355)  triple_60: 445.1697 (445.1697)  triple_40: 418.8079 (418.8079)  time: 0.7373  data: 0.4033  max mem: 46473\n",
      "Valid: [epoch:1]  [ 10/845]  eta: 0:05:09  loss: 2012.3871 (2022.8242)  loss_n_40: 4.1467 (4.1267)  loss_n_60: 4.5339 (4.5206)  loss_n_80: 4.7089 (4.6633)  loss_n_100: 4.7543 (4.7242)  triple_100: 505.6022 (507.4121)  triple_80: 507.8506 (509.8627)  triple_60: 503.5805 (506.3826)  triple_40: 476.6764 (481.1319)  time: 0.3708  data: 0.0368  max mem: 46473\n",
      "Valid: [epoch:1]  [ 20/845]  eta: 0:04:51  loss: 1897.8064 (1983.4108)  loss_n_40: 4.1509 (4.2354)  loss_n_60: 4.5556 (4.6311)  loss_n_80: 4.7166 (4.7690)  loss_n_100: 4.7607 (4.8173)  triple_100: 477.1367 (497.2944)  triple_80: 479.4506 (499.9789)  triple_60: 475.5383 (496.5503)  triple_40: 448.8239 (471.1345)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [ 30/845]  eta: 0:04:43  loss: 1887.5066 (1981.4501)  loss_n_40: 4.1183 (4.1795)  loss_n_60: 4.5076 (4.5583)  loss_n_80: 4.6785 (4.6959)  loss_n_100: 4.7224 (4.7454)  triple_100: 474.9636 (496.9803)  triple_80: 477.0316 (499.5502)  triple_60: 472.3991 (495.9818)  triple_40: 445.1590 (470.7587)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [ 40/845]  eta: 0:04:37  loss: 1880.0604 (1955.7205)  loss_n_40: 4.1396 (4.2512)  loss_n_60: 4.5162 (4.6358)  loss_n_80: 4.6854 (4.7740)  loss_n_100: 4.7224 (4.8193)  triple_100: 472.7724 (490.3926)  triple_80: 475.0703 (493.1099)  triple_60: 470.0569 (489.6103)  triple_40: 444.1701 (464.1274)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [ 50/845]  eta: 0:04:32  loss: 1845.5336 (1940.2435)  loss_n_40: 4.3550 (4.2584)  loss_n_60: 4.6669 (4.6462)  loss_n_80: 4.8295 (4.7866)  loss_n_100: 4.8843 (4.8298)  triple_100: 461.4659 (486.5850)  triple_80: 464.3564 (489.3155)  triple_60: 462.2752 (485.7745)  triple_40: 439.0526 (460.0474)  time: 0.3345  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:1]  [ 60/845]  eta: 0:04:27  loss: 1864.7109 (1944.6016)  loss_n_40: 4.1702 (4.2530)  loss_n_60: 4.5739 (4.6352)  loss_n_80: 4.7335 (4.7760)  loss_n_100: 4.7726 (4.8204)  triple_100: 468.1911 (487.6331)  triple_80: 470.8727 (490.3635)  triple_60: 467.1064 (486.8493)  triple_40: 440.4126 (461.2710)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [ 70/845]  eta: 0:04:23  loss: 1870.8760 (1945.0830)  loss_n_40: 4.1672 (4.2428)  loss_n_60: 4.5585 (4.6230)  loss_n_80: 4.7290 (4.7660)  loss_n_100: 4.7797 (4.8111)  triple_100: 470.0632 (487.8325)  triple_80: 472.8559 (490.5341)  triple_60: 468.9736 (487.0036)  triple_40: 440.4774 (461.2698)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [ 80/845]  eta: 0:04:19  loss: 1852.6560 (1940.7210)  loss_n_40: 4.1836 (4.2488)  loss_n_60: 4.5634 (4.6197)  loss_n_80: 4.7062 (4.7599)  loss_n_100: 4.7644 (4.8050)  triple_100: 463.2102 (486.6612)  triple_80: 466.7195 (489.3859)  triple_60: 463.9275 (485.9010)  triple_40: 437.3939 (460.3394)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [ 90/845]  eta: 0:04:15  loss: 1846.7933 (1939.9953)  loss_n_40: 4.2367 (4.2578)  loss_n_60: 4.5870 (4.6292)  loss_n_80: 4.7165 (4.7703)  loss_n_100: 4.7792 (4.8170)  triple_100: 464.4240 (486.5235)  triple_80: 466.5932 (489.2338)  triple_60: 462.0233 (485.7125)  triple_40: 435.7579 (460.0513)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [100/845]  eta: 0:04:12  loss: 1903.9219 (1952.0713)  loss_n_40: 4.2779 (4.2627)  loss_n_60: 4.6286 (4.6246)  loss_n_80: 4.7570 (4.7641)  loss_n_100: 4.8246 (4.8102)  triple_100: 476.5128 (489.4555)  triple_80: 480.5687 (492.1770)  triple_60: 476.4441 (488.7346)  triple_40: 449.7843 (463.2427)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [110/845]  eta: 0:04:08  loss: 1924.0536 (1962.2647)  loss_n_40: 4.1923 (4.2523)  loss_n_60: 4.5758 (4.6067)  loss_n_80: 4.7324 (4.7460)  loss_n_100: 4.7607 (4.7930)  triple_100: 482.2883 (491.9672)  triple_80: 485.8068 (494.6560)  triple_60: 482.2633 (491.2626)  triple_40: 454.0364 (465.9810)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [120/845]  eta: 0:04:05  loss: 1877.8416 (1966.2672)  loss_n_40: 4.1923 (4.2495)  loss_n_60: 4.5567 (4.5948)  loss_n_80: 4.7324 (4.7336)  loss_n_100: 4.7607 (4.7815)  triple_100: 472.1599 (492.9387)  triple_80: 474.2809 (495.6073)  triple_60: 470.0261 (492.2694)  triple_40: 443.3597 (467.0923)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [130/845]  eta: 0:04:01  loss: 1902.8899 (1968.7815)  loss_n_40: 4.3236 (4.2534)  loss_n_60: 4.6518 (4.5963)  loss_n_80: 4.8096 (4.7349)  loss_n_100: 4.8655 (4.7827)  triple_100: 475.6949 (493.5161)  triple_80: 479.8243 (496.2071)  triple_60: 476.3225 (492.8923)  triple_40: 448.1795 (467.7987)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [140/845]  eta: 0:03:57  loss: 1905.1930 (1968.0274)  loss_n_40: 4.3646 (4.2592)  loss_n_60: 4.6572 (4.6068)  loss_n_80: 4.7778 (4.7451)  loss_n_100: 4.8570 (4.7925)  triple_100: 475.6949 (493.3147)  triple_80: 479.8243 (496.0243)  triple_60: 477.5643 (492.7060)  triple_40: 451.5496 (467.5790)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [150/845]  eta: 0:03:54  loss: 1880.7052 (1967.7860)  loss_n_40: 4.1614 (4.2511)  loss_n_60: 4.5325 (4.6001)  loss_n_80: 4.7146 (4.7395)  loss_n_100: 4.7529 (4.7874)  triple_100: 470.2285 (493.2979)  triple_80: 473.2463 (496.0056)  triple_60: 470.3952 (492.6555)  triple_40: 446.5276 (467.4487)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [160/845]  eta: 0:03:50  loss: 1891.3206 (1972.9689)  loss_n_40: 4.0644 (4.2477)  loss_n_60: 4.5325 (4.5945)  loss_n_80: 4.6937 (4.7335)  loss_n_100: 4.7491 (4.7819)  triple_100: 474.4899 (494.5615)  triple_80: 477.0891 (497.2587)  triple_60: 473.9302 (493.9497)  triple_40: 447.5408 (468.8416)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [170/845]  eta: 0:03:47  loss: 1929.6812 (1977.2077)  loss_n_40: 4.1100 (4.2405)  loss_n_60: 4.5647 (4.5876)  loss_n_80: 4.6937 (4.7273)  loss_n_100: 4.7611 (4.7756)  triple_100: 484.8617 (495.6357)  triple_80: 488.3209 (498.3254)  triple_60: 483.6890 (494.9987)  triple_40: 454.3823 (469.9169)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [180/845]  eta: 0:03:44  loss: 1952.1439 (1978.2262)  loss_n_40: 4.0882 (4.2439)  loss_n_60: 4.5042 (4.5967)  loss_n_80: 4.6708 (4.7372)  loss_n_100: 4.7455 (4.7849)  triple_100: 491.8102 (495.9128)  triple_80: 494.0809 (498.6086)  triple_60: 488.8117 (495.2650)  triple_40: 459.6702 (470.0771)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [190/845]  eta: 0:03:40  loss: 1986.0640 (1986.4135)  loss_n_40: 4.0053 (4.2355)  loss_n_60: 4.4656 (4.5845)  loss_n_80: 4.6559 (4.7242)  loss_n_100: 4.7071 (4.7724)  triple_100: 497.8353 (497.9428)  triple_80: 500.6843 (500.6316)  triple_60: 497.0493 (497.3083)  triple_40: 469.6415 (472.2142)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [200/845]  eta: 0:03:37  loss: 1947.8589 (1987.5505)  loss_n_40: 4.0053 (4.2309)  loss_n_60: 4.4824 (4.5841)  loss_n_80: 4.6559 (4.7251)  loss_n_100: 4.6743 (4.7727)  triple_100: 488.8310 (498.2568)  triple_80: 491.8179 (500.9481)  triple_60: 487.6605 (497.6007)  triple_40: 459.7243 (472.4321)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [210/845]  eta: 0:03:33  loss: 1936.9701 (1989.7001)  loss_n_40: 4.0695 (4.2269)  loss_n_60: 4.5115 (4.5818)  loss_n_80: 4.6845 (4.7234)  loss_n_100: 4.7375 (4.7713)  triple_100: 487.7422 (498.8140)  triple_80: 489.5947 (501.4931)  triple_60: 484.5287 (498.1377)  triple_40: 457.1625 (472.9519)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [220/845]  eta: 0:03:30  loss: 1950.5718 (1993.2270)  loss_n_40: 4.1305 (4.2291)  loss_n_60: 4.5597 (4.5809)  loss_n_80: 4.7139 (4.7214)  loss_n_100: 4.7705 (4.7687)  triple_100: 489.3421 (499.6420)  triple_80: 492.7215 (502.3368)  triple_60: 488.6354 (499.0243)  triple_40: 461.5767 (473.9239)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [230/845]  eta: 0:03:26  loss: 1917.3025 (1991.0226)  loss_n_40: 4.3172 (4.2377)  loss_n_60: 4.5987 (4.5912)  loss_n_80: 4.7756 (4.7315)  loss_n_100: 4.7825 (4.7781)  triple_100: 479.4295 (499.0775)  triple_80: 482.9706 (501.7830)  triple_60: 479.9711 (498.4710)  triple_40: 453.9246 (473.3528)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [240/845]  eta: 0:03:23  loss: 1900.6373 (1987.8443)  loss_n_40: 4.1885 (4.2396)  loss_n_60: 4.5530 (4.5973)  loss_n_80: 4.7176 (4.7376)  loss_n_100: 4.7935 (4.7841)  triple_100: 478.1379 (498.3128)  triple_80: 480.8217 (501.0245)  triple_60: 476.2038 (497.6760)  triple_40: 447.3312 (472.4724)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [250/845]  eta: 0:03:20  loss: 1917.4458 (1991.1849)  loss_n_40: 4.0527 (4.2379)  loss_n_60: 4.5343 (4.5966)  loss_n_80: 4.6966 (4.7365)  loss_n_100: 4.7401 (4.7825)  triple_100: 482.7546 (499.1296)  triple_80: 485.7217 (501.8548)  triple_60: 480.6652 (498.5192)  triple_40: 450.8620 (473.3277)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [260/845]  eta: 0:03:16  loss: 1953.4840 (1990.3932)  loss_n_40: 4.2825 (4.2429)  loss_n_60: 4.6096 (4.6028)  loss_n_80: 4.7496 (4.7429)  loss_n_100: 4.8228 (4.7886)  triple_100: 487.6666 (498.9185)  triple_80: 491.8601 (501.6484)  triple_60: 489.1922 (498.3178)  triple_40: 464.1044 (473.1314)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [270/845]  eta: 0:03:13  loss: 1879.1340 (1988.9536)  loss_n_40: 4.3263 (4.2432)  loss_n_60: 4.6595 (4.6037)  loss_n_80: 4.8053 (4.7441)  loss_n_100: 4.8441 (4.7896)  triple_100: 470.1454 (498.5712)  triple_80: 473.7520 (501.3017)  triple_60: 470.6920 (497.9656)  triple_40: 445.0494 (472.7343)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [280/845]  eta: 0:03:09  loss: 1847.9777 (1989.4314)  loss_n_40: 4.2223 (4.2403)  loss_n_60: 4.5223 (4.6028)  loss_n_80: 4.6899 (4.7435)  loss_n_100: 4.7270 (4.7893)  triple_100: 463.9984 (498.7098)  triple_80: 465.9385 (501.4287)  triple_60: 462.3428 (498.0939)  triple_40: 437.7245 (472.8230)  time: 0.3349  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:1]  [290/845]  eta: 0:03:06  loss: 1895.2759 (1988.9417)  loss_n_40: 4.2433 (4.2459)  loss_n_60: 4.5349 (4.6071)  loss_n_80: 4.6885 (4.7473)  loss_n_100: 4.7581 (4.7928)  triple_100: 475.2329 (498.5736)  triple_80: 477.5101 (501.3010)  triple_60: 474.7780 (497.9708)  triple_40: 447.9133 (472.7032)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [300/845]  eta: 0:03:03  loss: 1891.0868 (1985.7910)  loss_n_40: 4.3411 (4.2483)  loss_n_60: 4.6935 (4.6107)  loss_n_80: 4.8183 (4.7508)  loss_n_100: 4.8609 (4.7962)  triple_100: 475.2306 (497.7878)  triple_80: 477.5101 (500.5188)  triple_60: 473.6698 (497.1927)  triple_40: 446.6611 (471.8857)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [310/845]  eta: 0:02:59  loss: 1872.7299 (1984.6741)  loss_n_40: 4.2488 (4.2486)  loss_n_60: 4.6135 (4.6122)  loss_n_80: 4.7958 (4.7530)  loss_n_100: 4.8414 (4.7984)  triple_100: 470.4789 (497.5289)  triple_80: 473.0389 (500.2536)  triple_60: 468.6510 (496.9138)  triple_40: 442.5220 (471.5656)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [320/845]  eta: 0:02:56  loss: 1896.9222 (1985.9372)  loss_n_40: 4.1653 (4.2466)  loss_n_60: 4.5468 (4.6097)  loss_n_80: 4.7414 (4.7506)  loss_n_100: 4.7679 (4.7963)  triple_100: 475.3857 (497.8549)  triple_80: 478.4409 (500.5649)  triple_60: 474.9900 (497.2296)  triple_40: 447.7780 (471.8845)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [330/845]  eta: 0:02:52  loss: 1857.3588 (1983.3444)  loss_n_40: 4.3406 (4.2518)  loss_n_60: 4.6378 (4.6153)  loss_n_80: 4.7782 (4.7559)  loss_n_100: 4.8444 (4.8014)  triple_100: 468.3476 (497.1932)  triple_80: 469.9910 (499.9071)  triple_60: 465.0920 (496.5872)  triple_40: 436.5299 (471.2326)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [340/845]  eta: 0:02:49  loss: 1852.5277 (1982.5469)  loss_n_40: 4.3308 (4.2530)  loss_n_60: 4.6523 (4.6160)  loss_n_80: 4.7925 (4.7568)  loss_n_100: 4.8775 (4.8026)  triple_100: 464.8602 (496.9947)  triple_80: 467.7445 (499.7075)  triple_60: 464.1224 (496.3873)  triple_40: 437.4029 (471.0291)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [350/845]  eta: 0:02:46  loss: 1905.9991 (1982.8421)  loss_n_40: 4.3011 (4.2531)  loss_n_60: 4.6278 (4.6160)  loss_n_80: 4.7880 (4.7568)  loss_n_100: 4.8456 (4.8024)  triple_100: 478.3069 (497.0643)  triple_80: 480.9050 (499.7746)  triple_60: 476.9606 (496.4498)  triple_40: 451.1212 (471.1251)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [360/845]  eta: 0:02:42  loss: 1916.9922 (1983.4990)  loss_n_40: 4.1458 (4.2541)  loss_n_60: 4.5827 (4.6167)  loss_n_80: 4.7662 (4.7571)  loss_n_100: 4.7888 (4.8021)  triple_100: 479.2660 (497.2025)  triple_80: 483.4269 (499.9270)  triple_60: 480.6876 (496.6152)  triple_40: 452.9577 (471.3243)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [370/845]  eta: 0:02:39  loss: 1916.9922 (1983.3360)  loss_n_40: 4.3071 (4.2574)  loss_n_60: 4.6357 (4.6209)  loss_n_80: 4.7915 (4.7612)  loss_n_100: 4.8590 (4.8059)  triple_100: 479.2660 (497.1498)  triple_80: 483.4269 (499.8873)  triple_60: 480.6876 (496.5809)  triple_40: 452.9577 (471.2726)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [380/845]  eta: 0:02:36  loss: 1896.6938 (1983.0580)  loss_n_40: 4.0856 (4.2575)  loss_n_60: 4.5595 (4.6219)  loss_n_80: 4.7131 (4.7626)  loss_n_100: 4.7385 (4.8069)  triple_100: 476.6539 (497.0879)  triple_80: 479.5581 (499.8299)  triple_60: 475.5175 (496.5139)  triple_40: 449.1415 (471.1772)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [390/845]  eta: 0:02:32  loss: 1896.6938 (1984.6734)  loss_n_40: 4.0655 (4.2561)  loss_n_60: 4.5480 (4.6194)  loss_n_80: 4.7131 (4.7597)  loss_n_100: 4.7385 (4.8041)  triple_100: 476.9468 (497.4765)  triple_80: 479.5581 (500.2224)  triple_60: 474.6835 (496.9185)  triple_40: 447.4562 (471.6167)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [400/845]  eta: 0:02:29  loss: 1925.8037 (1986.6597)  loss_n_40: 4.2124 (4.2529)  loss_n_60: 4.5810 (4.6137)  loss_n_80: 4.7286 (4.7534)  loss_n_100: 4.7707 (4.7979)  triple_100: 485.1465 (497.9617)  triple_80: 487.7632 (500.7057)  triple_60: 482.1234 (497.4118)  triple_40: 452.7382 (472.1626)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [410/845]  eta: 0:02:26  loss: 1959.0554 (1992.4719)  loss_n_40: 3.9901 (4.2481)  loss_n_60: 4.4327 (4.6055)  loss_n_80: 4.6395 (4.7447)  loss_n_100: 4.6442 (4.7898)  triple_100: 491.1056 (499.3910)  triple_80: 493.9434 (502.1203)  triple_60: 490.7930 (498.8512)  triple_40: 463.3816 (473.7212)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [420/845]  eta: 0:02:22  loss: 1937.1610 (1994.0901)  loss_n_40: 3.9901 (4.2466)  loss_n_60: 4.4835 (4.6012)  loss_n_80: 4.6463 (4.7402)  loss_n_100: 4.6888 (4.7857)  triple_100: 488.2779 (499.7938)  triple_80: 490.2974 (502.5045)  triple_60: 484.9110 (499.2448)  triple_40: 455.8736 (474.1731)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [430/845]  eta: 0:02:19  loss: 1846.9111 (1994.0111)  loss_n_40: 4.1270 (4.2435)  loss_n_60: 4.5250 (4.5966)  loss_n_80: 4.7159 (4.7365)  loss_n_100: 4.7865 (4.7826)  triple_100: 460.6715 (499.7912)  triple_80: 464.5098 (502.4885)  triple_60: 462.4121 (499.2127)  triple_40: 438.7395 (474.1594)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [440/845]  eta: 0:02:15  loss: 1903.2863 (1995.7815)  loss_n_40: 4.1224 (4.2418)  loss_n_60: 4.5144 (4.5946)  loss_n_80: 4.7022 (4.7348)  loss_n_100: 4.7691 (4.7811)  triple_100: 479.7825 (500.2314)  triple_80: 481.6069 (502.9246)  triple_60: 476.8196 (499.6507)  triple_40: 447.0513 (474.6224)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [450/845]  eta: 0:02:12  loss: 1924.8739 (1995.2441)  loss_n_40: 4.1173 (4.2410)  loss_n_60: 4.5357 (4.5941)  loss_n_80: 4.7040 (4.7345)  loss_n_100: 4.7504 (4.7806)  triple_100: 484.4163 (500.0978)  triple_80: 486.6102 (502.7981)  triple_60: 482.1274 (499.5203)  triple_40: 453.6721 (474.4778)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [460/845]  eta: 0:02:09  loss: 1924.8739 (1997.0620)  loss_n_40: 4.1173 (4.2390)  loss_n_60: 4.5351 (4.5910)  loss_n_80: 4.6905 (4.7314)  loss_n_100: 4.7385 (4.7772)  triple_100: 483.1359 (500.5478)  triple_80: 486.2886 (503.2471)  triple_60: 482.1274 (499.9723)  triple_40: 453.7252 (474.9564)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [470/845]  eta: 0:02:05  loss: 1910.9211 (1996.5082)  loss_n_40: 4.0645 (4.2370)  loss_n_60: 4.5259 (4.5909)  loss_n_80: 4.6678 (4.7315)  loss_n_100: 4.7121 (4.7775)  triple_100: 478.0842 (500.4263)  triple_80: 481.1270 (503.1228)  triple_60: 478.0012 (499.8351)  triple_40: 453.7252 (474.7870)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [480/845]  eta: 0:02:02  loss: 1893.0446 (1996.5967)  loss_n_40: 4.0716 (4.2392)  loss_n_60: 4.5269 (4.5935)  loss_n_80: 4.6907 (4.7341)  loss_n_100: 4.7305 (4.7800)  triple_100: 475.7680 (500.4425)  triple_80: 477.6583 (503.1423)  triple_60: 473.6939 (499.8558)  triple_40: 446.9938 (474.8092)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [490/845]  eta: 0:01:59  loss: 1922.6313 (1995.3662)  loss_n_40: 4.2406 (4.2409)  loss_n_60: 4.5923 (4.5965)  loss_n_80: 4.7678 (4.7372)  loss_n_100: 4.8023 (4.7827)  triple_100: 481.7528 (500.1393)  triple_80: 485.0564 (502.8458)  triple_60: 481.7407 (499.5529)  triple_40: 453.2189 (474.4708)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [500/845]  eta: 0:01:55  loss: 1963.7427 (1997.4801)  loss_n_40: 4.1666 (4.2387)  loss_n_60: 4.5856 (4.5941)  loss_n_80: 4.7490 (4.7348)  loss_n_100: 4.7913 (4.7803)  triple_100: 491.4432 (500.6604)  triple_80: 494.8316 (503.3681)  triple_60: 491.7637 (500.0795)  triple_40: 464.9218 (475.0243)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [510/845]  eta: 0:01:52  loss: 1905.3136 (1995.2971)  loss_n_40: 4.1666 (4.2427)  loss_n_60: 4.5869 (4.5986)  loss_n_80: 4.7688 (4.7392)  loss_n_100: 4.8037 (4.7845)  triple_100: 476.0022 (500.1070)  triple_80: 479.8878 (502.8247)  triple_60: 477.4868 (499.5358)  triple_40: 452.5017 (474.4646)  time: 0.3349  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:1]  [520/845]  eta: 0:01:49  loss: 1877.4595 (1995.3228)  loss_n_40: 4.2213 (4.2449)  loss_n_60: 4.6408 (4.6004)  loss_n_80: 4.7994 (4.7407)  loss_n_100: 4.8597 (4.7856)  triple_100: 472.6453 (500.0934)  triple_80: 474.8516 (502.8219)  triple_60: 470.1155 (499.5410)  triple_40: 441.6333 (474.4948)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [530/845]  eta: 0:01:45  loss: 1871.6254 (1995.1218)  loss_n_40: 4.1411 (4.2422)  loss_n_60: 4.5393 (4.5980)  loss_n_80: 4.7057 (4.7385)  loss_n_100: 4.7504 (4.7838)  triple_100: 470.7341 (500.0538)  triple_80: 472.9492 (502.7751)  triple_60: 468.9471 (499.4937)  triple_40: 442.5163 (474.4367)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [540/845]  eta: 0:01:42  loss: 1871.6254 (1995.0646)  loss_n_40: 4.1223 (4.2425)  loss_n_60: 4.5326 (4.5988)  loss_n_80: 4.6747 (4.7393)  loss_n_100: 4.7504 (4.7845)  triple_100: 470.7341 (500.0375)  triple_80: 472.9492 (502.7577)  triple_60: 468.9471 (499.4769)  triple_40: 442.5163 (474.4275)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [550/845]  eta: 0:01:38  loss: 1938.2744 (1996.2696)  loss_n_40: 4.1076 (4.2418)  loss_n_60: 4.5639 (4.5978)  loss_n_80: 4.7262 (4.7380)  loss_n_100: 4.7929 (4.7830)  triple_100: 484.5048 (500.3284)  triple_80: 488.3758 (503.0578)  triple_60: 485.6149 (499.7785)  triple_40: 459.2362 (474.7442)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [560/845]  eta: 0:01:35  loss: 1942.6375 (1997.6891)  loss_n_40: 4.0850 (4.2402)  loss_n_60: 4.5406 (4.5961)  loss_n_80: 4.7095 (4.7364)  loss_n_100: 4.7668 (4.7813)  triple_100: 486.1247 (500.6889)  triple_80: 490.1763 (503.4143)  triple_60: 487.1548 (500.1314)  triple_40: 460.2187 (475.1005)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [570/845]  eta: 0:01:32  loss: 1981.1794 (1998.6880)  loss_n_40: 4.0847 (4.2401)  loss_n_60: 4.5616 (4.5962)  loss_n_80: 4.7095 (4.7363)  loss_n_100: 4.7668 (4.7809)  triple_100: 496.9748 (500.9253)  triple_80: 500.0795 (503.6549)  triple_60: 496.4026 (500.3858)  triple_40: 469.4141 (475.3685)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [580/845]  eta: 0:01:28  loss: 1935.6656 (1999.9911)  loss_n_40: 4.0925 (4.2389)  loss_n_60: 4.5616 (4.5938)  loss_n_80: 4.6938 (4.7339)  loss_n_100: 4.7551 (4.7788)  triple_100: 484.1941 (501.2498)  triple_80: 488.2042 (503.9745)  triple_60: 485.1337 (500.7071)  triple_40: 458.5449 (475.7142)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [590/845]  eta: 0:01:25  loss: 1960.4310 (2002.2395)  loss_n_40: 4.0925 (4.2363)  loss_n_60: 4.4805 (4.5900)  loss_n_80: 4.6575 (4.7305)  loss_n_100: 4.7093 (4.7756)  triple_100: 493.0198 (501.8273)  triple_80: 495.7574 (504.5397)  triple_60: 491.1423 (501.2635)  triple_40: 462.4413 (476.2767)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [600/845]  eta: 0:01:22  loss: 1922.7822 (2000.5478)  loss_n_40: 4.1190 (4.2390)  loss_n_60: 4.5951 (4.5930)  loss_n_80: 4.7527 (4.7335)  loss_n_100: 4.7883 (4.7785)  triple_100: 482.7420 (501.4040)  triple_80: 485.7852 (504.1180)  triple_60: 481.6707 (500.8438)  triple_40: 453.2184 (475.8380)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [610/845]  eta: 0:01:18  loss: 1844.5432 (1998.4275)  loss_n_40: 4.3807 (4.2423)  loss_n_60: 4.6872 (4.5965)  loss_n_80: 4.8387 (4.7370)  loss_n_100: 4.8871 (4.7820)  triple_100: 461.2058 (500.8700)  triple_80: 464.4615 (503.5898)  triple_60: 461.9276 (500.3160)  triple_40: 437.3568 (475.2940)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [620/845]  eta: 0:01:15  loss: 1855.4419 (1997.1379)  loss_n_40: 4.3334 (4.2436)  loss_n_60: 4.6425 (4.5994)  loss_n_80: 4.7930 (4.7401)  loss_n_100: 4.8528 (4.7848)  triple_100: 463.1109 (500.5498)  triple_80: 467.0764 (503.2752)  triple_60: 464.4303 (499.9935)  triple_40: 439.9268 (474.9514)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [630/845]  eta: 0:01:12  loss: 1895.1384 (1997.2856)  loss_n_40: 4.2737 (4.2436)  loss_n_60: 4.5777 (4.5991)  loss_n_80: 4.7347 (4.7397)  loss_n_100: 4.8005 (4.7846)  triple_100: 474.6898 (500.5823)  triple_80: 478.7321 (503.3071)  triple_60: 474.6750 (500.0298)  triple_40: 447.3268 (474.9994)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [640/845]  eta: 0:01:08  loss: 1897.9752 (1997.1123)  loss_n_40: 4.2923 (4.2447)  loss_n_60: 4.6191 (4.5996)  loss_n_80: 4.7679 (4.7400)  loss_n_100: 4.8215 (4.7850)  triple_100: 476.2191 (500.5357)  triple_80: 479.0984 (503.2593)  triple_60: 475.6312 (499.9835)  triple_40: 447.9540 (474.9645)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [650/845]  eta: 0:01:05  loss: 1911.6005 (1997.8101)  loss_n_40: 4.2923 (4.2451)  loss_n_60: 4.6382 (4.6005)  loss_n_80: 4.7833 (4.7411)  loss_n_100: 4.8422 (4.7860)  triple_100: 479.8894 (500.7172)  triple_80: 482.6041 (503.4398)  triple_60: 478.3878 (500.1572)  triple_40: 452.1700 (475.1232)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [660/845]  eta: 0:01:02  loss: 1932.6958 (1997.6559)  loss_n_40: 4.1593 (4.2448)  loss_n_60: 4.5654 (4.6001)  loss_n_80: 4.7177 (4.7407)  loss_n_100: 4.7610 (4.7856)  triple_100: 486.3541 (500.6777)  triple_80: 488.9353 (503.4007)  triple_60: 484.0053 (500.1157)  triple_40: 455.2825 (475.0906)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [670/845]  eta: 0:00:58  loss: 1866.6926 (1996.6243)  loss_n_40: 4.2569 (4.2457)  loss_n_60: 4.6066 (4.6003)  loss_n_80: 4.7533 (4.7409)  loss_n_100: 4.7997 (4.7860)  triple_100: 466.1825 (500.4212)  triple_80: 470.3475 (503.1398)  triple_60: 467.4864 (499.8535)  triple_40: 442.2630 (474.8370)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [680/845]  eta: 0:00:55  loss: 1880.3893 (1996.3246)  loss_n_40: 4.2711 (4.2462)  loss_n_60: 4.6135 (4.6010)  loss_n_80: 4.7659 (4.7417)  loss_n_100: 4.8367 (4.7868)  triple_100: 471.5750 (500.3506)  triple_80: 474.5906 (503.0708)  triple_60: 470.5478 (499.7801)  triple_40: 445.5646 (474.7475)  time: 0.3353  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [690/845]  eta: 0:00:51  loss: 1906.6938 (1997.5979)  loss_n_40: 4.1041 (4.2440)  loss_n_60: 4.5152 (4.5997)  loss_n_80: 4.7005 (4.7406)  loss_n_100: 4.7463 (4.7857)  triple_100: 480.2589 (500.6761)  triple_80: 482.3708 (503.3954)  triple_60: 477.5703 (500.0977)  triple_40: 449.2520 (475.0588)  time: 0.3354  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [700/845]  eta: 0:00:48  loss: 1914.5465 (1997.2140)  loss_n_40: 4.0747 (4.2418)  loss_n_60: 4.5142 (4.5976)  loss_n_80: 4.6818 (4.7388)  loss_n_100: 4.7344 (4.7841)  triple_100: 482.0046 (500.5911)  triple_80: 484.3426 (503.3079)  triple_60: 480.0560 (500.0026)  triple_40: 451.8835 (474.9500)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [710/845]  eta: 0:00:45  loss: 1876.3411 (1995.8323)  loss_n_40: 4.1792 (4.2435)  loss_n_60: 4.5262 (4.5999)  loss_n_80: 4.6863 (4.7410)  loss_n_100: 4.7656 (4.7862)  triple_100: 470.0092 (500.2452)  triple_80: 473.2476 (502.9661)  triple_60: 470.0199 (499.6618)  triple_40: 443.5147 (474.5885)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [720/845]  eta: 0:00:41  loss: 1876.3411 (1995.4415)  loss_n_40: 4.3592 (4.2447)  loss_n_60: 4.6538 (4.6006)  loss_n_80: 4.8127 (4.7415)  loss_n_100: 4.8836 (4.7868)  triple_100: 470.0092 (500.1418)  triple_80: 473.2476 (502.8660)  triple_60: 470.0199 (499.5654)  triple_40: 443.5147 (474.4946)  time: 0.3354  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [730/845]  eta: 0:00:38  loss: 1899.7766 (1994.9335)  loss_n_40: 4.2021 (4.2443)  loss_n_60: 4.6138 (4.6002)  loss_n_80: 4.7806 (4.7415)  loss_n_100: 4.7975 (4.7866)  triple_100: 475.0338 (500.0236)  triple_80: 478.8466 (502.7482)  triple_60: 475.6918 (499.4400)  triple_40: 448.8472 (474.3492)  time: 0.3353  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [740/845]  eta: 0:00:35  loss: 1899.7766 (1994.9093)  loss_n_40: 4.1668 (4.2439)  loss_n_60: 4.5418 (4.5999)  loss_n_80: 4.7145 (4.7412)  loss_n_100: 4.7527 (4.7862)  triple_100: 475.0338 (500.0180)  triple_80: 478.8466 (502.7452)  triple_60: 475.6918 (499.4345)  triple_40: 449.1291 (474.3405)  time: 0.3351  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:1]  [750/845]  eta: 0:00:31  loss: 1900.3879 (1993.9170)  loss_n_40: 4.2908 (4.2455)  loss_n_60: 4.6330 (4.6020)  loss_n_80: 4.7896 (4.7433)  loss_n_100: 4.8531 (4.7883)  triple_100: 476.7171 (499.7681)  triple_80: 479.8306 (502.4963)  triple_60: 476.4185 (499.1877)  triple_40: 448.5347 (474.0857)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [760/845]  eta: 0:00:28  loss: 1901.5781 (1993.9454)  loss_n_40: 4.3102 (4.2451)  loss_n_60: 4.6619 (4.6014)  loss_n_80: 4.8084 (4.7427)  loss_n_100: 4.8627 (4.7878)  triple_100: 477.6144 (499.7818)  triple_80: 480.5489 (502.5076)  triple_60: 476.4185 (499.1929)  triple_40: 448.5347 (474.0861)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [770/845]  eta: 0:00:25  loss: 1903.5430 (1993.2206)  loss_n_40: 4.1821 (4.2455)  loss_n_60: 4.5807 (4.6027)  loss_n_80: 4.7793 (4.7443)  loss_n_100: 4.8235 (4.7894)  triple_100: 478.2913 (499.6066)  triple_80: 481.0743 (502.3338)  triple_60: 476.7882 (499.0145)  triple_40: 449.9191 (473.8839)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [780/845]  eta: 0:00:21  loss: 1898.5907 (1993.7711)  loss_n_40: 4.3087 (4.2459)  loss_n_60: 4.5807 (4.6025)  loss_n_80: 4.7793 (4.7440)  loss_n_100: 4.8235 (4.7892)  triple_100: 476.7795 (499.7391)  triple_80: 479.6140 (502.4646)  triple_60: 475.3105 (499.1483)  triple_40: 450.0576 (474.0375)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [790/845]  eta: 0:00:18  loss: 1987.9045 (1995.3155)  loss_n_40: 4.3186 (4.2460)  loss_n_60: 4.5918 (4.6015)  loss_n_80: 4.7571 (4.7429)  loss_n_100: 4.7860 (4.7881)  triple_100: 498.3407 (500.1130)  triple_80: 501.2000 (502.8416)  triple_60: 497.5601 (499.5331)  triple_40: 470.0870 (474.4494)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [800/845]  eta: 0:00:15  loss: 1985.2250 (1996.5102)  loss_n_40: 4.1181 (4.2438)  loss_n_60: 4.5395 (4.5986)  loss_n_80: 4.6961 (4.7400)  loss_n_100: 4.7451 (4.7853)  triple_100: 498.3407 (500.4181)  triple_80: 501.2000 (503.1369)  triple_60: 497.1545 (499.8295)  triple_40: 468.7374 (474.7580)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [810/845]  eta: 0:00:11  loss: 1899.5197 (1997.3220)  loss_n_40: 4.0643 (4.2423)  loss_n_60: 4.5004 (4.5969)  loss_n_80: 4.6324 (4.7381)  loss_n_100: 4.7109 (4.7835)  triple_100: 477.0104 (500.6183)  triple_80: 479.5133 (503.3381)  triple_60: 475.4211 (500.0313)  triple_40: 447.0962 (474.9735)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [820/845]  eta: 0:00:08  loss: 1879.9594 (1996.9204)  loss_n_40: 4.0792 (4.2423)  loss_n_60: 4.5413 (4.5968)  loss_n_80: 4.7118 (4.7381)  loss_n_100: 4.7646 (4.7836)  triple_100: 471.1296 (500.5233)  triple_80: 474.7491 (503.2406)  triple_60: 470.1319 (499.9300)  triple_40: 444.7990 (474.8658)  time: 0.3355  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [830/845]  eta: 0:00:05  loss: 1868.2249 (1996.5755)  loss_n_40: 4.1516 (4.2408)  loss_n_60: 4.5446 (4.5950)  loss_n_80: 4.7178 (4.7364)  loss_n_100: 4.7703 (4.7822)  triple_100: 468.0928 (500.4456)  triple_80: 471.1215 (503.1538)  triple_60: 467.9182 (499.8397)  triple_40: 440.8134 (474.7819)  time: 0.3353  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [840/845]  eta: 0:00:01  loss: 1875.1837 (1995.9760)  loss_n_40: 4.1224 (4.2403)  loss_n_60: 4.5452 (4.5951)  loss_n_80: 4.7217 (4.7367)  loss_n_100: 4.7512 (4.7825)  triple_100: 471.1496 (500.3066)  triple_80: 473.6760 (503.0157)  triple_60: 469.3795 (499.6952)  triple_40: 442.2248 (474.6039)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1]  [844/845]  eta: 0:00:00  loss: 1875.1837 (1995.3874)  loss_n_40: 4.1330 (4.2407)  loss_n_60: 4.5452 (4.5957)  loss_n_80: 4.7217 (4.7372)  loss_n_100: 4.7512 (4.7830)  triple_100: 471.1496 (500.1576)  triple_80: 473.6760 (502.8680)  triple_60: 469.3795 (499.5476)  triple_40: 442.2248 (474.4577)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:1] Total time: 0:04:43 (0.3356 s / it)\n",
      "Averaged stats: loss: 1875.1837 (1995.3874)  loss_n_40: 4.1330 (4.2407)  loss_n_60: 4.5452 (4.5957)  loss_n_80: 4.7217 (4.7372)  loss_n_100: 4.7512 (4.7830)  triple_100: 471.1496 (500.1576)  triple_80: 473.6760 (502.8680)  triple_60: 469.3795 (499.5476)  triple_40: 442.2248 (474.4577)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_1_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 4.783%\n",
      "Min loss_n_100: 4.783\n",
      "Best Epoch: 0.000\n",
      "Train: [epoch:2]  [   0/1724]  eta: 2:01:17  lr: 0.000020  loss: 2049.6926 (2049.6926)  loss_n_40: 4.2900 (4.2900)  loss_n_60: 4.6675 (4.6675)  loss_n_80: 4.8229 (4.8229)  loss_n_100: 4.8600 (4.8600)  triple_100: 513.0549 (513.0549)  triple_80: 516.2518 (516.2518)  triple_60: 512.8323 (512.8323)  triple_40: 488.9135 (488.9135)  time: 4.2213  data: 0.4588  max mem: 46473\n",
      "Train: [epoch:2]  [  10/1724]  eta: 1:53:07  lr: 0.000020  loss: 2022.2627 (1974.9251)  loss_n_40: 4.2608 (4.2651)  loss_n_60: 4.6242 (4.5720)  loss_n_80: 4.7142 (4.6769)  loss_n_100: 4.7676 (4.7339)  triple_100: 506.3989 (495.3414)  triple_80: 508.9196 (497.8584)  triple_60: 506.5053 (494.9643)  triple_40: 482.4986 (468.5130)  time: 3.9600  data: 0.0419  max mem: 46473\n",
      "Train: [epoch:2]  [  20/1724]  eta: 1:52:03  lr: 0.000020  loss: 1896.9713 (1946.5817)  loss_n_40: 4.2506 (4.2534)  loss_n_60: 4.6242 (4.5864)  loss_n_80: 4.7142 (4.6674)  loss_n_100: 4.7676 (4.7320)  triple_100: 476.9805 (489.2504)  triple_80: 479.4797 (491.3889)  triple_60: 475.8506 (488.4652)  triple_40: 446.3045 (459.2382)  time: 3.9321  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [  30/1724]  eta: 1:51:14  lr: 0.000020  loss: 1891.4849 (1934.2671)  loss_n_40: 4.2262 (4.2389)  loss_n_60: 4.6192 (4.5694)  loss_n_80: 4.6398 (4.6400)  loss_n_100: 4.7294 (4.7102)  triple_100: 477.7296 (486.6267)  triple_80: 479.1340 (488.4870)  triple_60: 475.0655 (485.7397)  triple_40: 441.1112 (455.2551)  time: 3.9293  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [  40/1724]  eta: 1:50:29  lr: 0.000020  loss: 1765.3658 (1885.9058)  loss_n_40: 4.2527 (4.2640)  loss_n_60: 4.6491 (4.6160)  loss_n_80: 4.6955 (4.6874)  loss_n_100: 4.7859 (4.7545)  triple_100: 448.2486 (475.4681)  triple_80: 448.9321 (477.0262)  triple_60: 445.4961 (474.2201)  triple_40: 404.5772 (440.8695)  time: 3.9276  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [  50/1724]  eta: 1:49:47  lr: 0.000020  loss: 1740.5481 (1878.2284)  loss_n_40: 4.2527 (4.2603)  loss_n_60: 4.6341 (4.6182)  loss_n_80: 4.6871 (4.6857)  loss_n_100: 4.7359 (4.7497)  triple_100: 445.6285 (474.3823)  triple_80: 445.2582 (475.5480)  triple_60: 440.9308 (472.7218)  triple_40: 397.0665 (437.2624)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [  60/1724]  eta: 1:49:06  lr: 0.000020  loss: 1678.7847 (1828.7618)  loss_n_40: 4.2920 (4.2769)  loss_n_60: 4.7565 (4.6668)  loss_n_80: 4.8795 (4.7499)  loss_n_100: 4.8047 (4.7864)  triple_100: 430.5970 (463.3753)  triple_80: 429.4120 (464.1437)  triple_60: 425.1756 (460.8765)  triple_40: 374.5197 (421.8863)  time: 3.9284  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [  70/1724]  eta: 1:48:25  lr: 0.000020  loss: 1532.5271 (1777.5370)  loss_n_40: 4.4828 (4.3200)  loss_n_60: 5.0773 (4.7518)  loss_n_80: 5.2536 (4.8491)  loss_n_100: 5.1090 (4.8556)  triple_100: 396.9579 (451.9633)  triple_80: 396.4131 (452.5902)  triple_60: 390.1283 (448.4375)  triple_40: 326.7463 (405.7694)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [  80/1724]  eta: 1:47:45  lr: 0.000020  loss: 1367.3680 (1710.6882)  loss_n_40: 4.5658 (4.3651)  loss_n_60: 5.3274 (4.8538)  loss_n_80: 5.5492 (4.9620)  loss_n_100: 5.3558 (4.9408)  triple_100: 363.1310 (437.5907)  triple_80: 360.5161 (437.8546)  triple_60: 346.8440 (431.6480)  triple_40: 272.3669 (384.4732)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [  90/1724]  eta: 1:47:05  lr: 0.000020  loss: 1099.1736 (1635.0115)  loss_n_40: 4.7973 (4.4254)  loss_n_60: 5.8407 (4.9775)  loss_n_80: 6.1044 (5.1092)  loss_n_100: 5.7503 (5.0519)  triple_100: 298.9495 (421.2274)  triple_80: 298.9960 (421.0387)  triple_60: 275.1066 (411.3671)  triple_40: 203.8757 (361.8144)  time: 3.9302  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [ 100/1724]  eta: 1:46:25  lr: 0.000020  loss: 922.6646 (1555.9293)  loss_n_40: 4.9466 (4.4897)  loss_n_60: 6.0466 (5.1015)  loss_n_80: 6.3860 (5.2584)  loss_n_100: 6.1097 (5.1782)  triple_100: 265.1029 (403.9602)  triple_80: 260.5461 (403.2274)  triple_60: 215.1538 (388.7397)  triple_40: 149.4640 (339.9742)  time: 3.9294  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 110/1724]  eta: 1:45:46  lr: 0.000020  loss: 746.0799 (1476.4909)  loss_n_40: 5.0578 (4.5453)  loss_n_60: 6.3387 (5.2249)  loss_n_80: 6.8458 (5.4108)  loss_n_100: 6.3784 (5.2982)  triple_100: 214.8027 (385.8557)  triple_80: 219.9671 (384.9040)  triple_60: 152.1118 (365.2323)  triple_40: 114.3768 (320.0197)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 120/1724]  eta: 1:45:06  lr: 0.000020  loss: 608.7460 (1405.6365)  loss_n_40: 5.1101 (4.5977)  loss_n_60: 6.5354 (5.3519)  loss_n_80: 7.0801 (5.5658)  loss_n_100: 6.6181 (5.4224)  triple_100: 180.8643 (369.2848)  triple_80: 177.6582 (368.1748)  triple_60: 109.9865 (344.3242)  triple_40: 102.9316 (302.9150)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 130/1724]  eta: 1:44:26  lr: 0.000020  loss: 604.9573 (1343.7204)  loss_n_40: 5.2353 (4.6469)  loss_n_60: 6.9083 (5.4767)  loss_n_80: 7.4421 (5.7175)  loss_n_100: 6.9116 (5.5428)  triple_100: 172.3281 (354.0598)  triple_80: 169.8159 (352.7934)  triple_60: 112.1216 (326.5769)  triple_40: 119.0092 (288.9064)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 140/1724]  eta: 1:43:46  lr: 0.000020  loss: 557.7627 (1281.7758)  loss_n_40: 5.2469 (4.6880)  loss_n_60: 7.0135 (5.5862)  loss_n_80: 7.5964 (5.8531)  loss_n_100: 7.1126 (5.6610)  triple_100: 157.6318 (339.4836)  triple_80: 151.2307 (337.3393)  triple_60: 101.8643 (308.9123)  triple_40: 99.6339 (274.2522)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 150/1724]  eta: 1:43:07  lr: 0.000020  loss: 484.7840 (1230.4085)  loss_n_40: 5.2851 (4.7293)  loss_n_60: 7.1039 (5.6927)  loss_n_80: 7.7396 (5.9814)  loss_n_100: 7.2976 (5.7765)  triple_100: 149.2701 (327.2371)  triple_80: 137.5272 (324.4038)  triple_60: 84.8344 (294.5962)  triple_40: 87.6445 (261.9915)  time: 3.9292  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 160/1724]  eta: 1:42:27  lr: 0.000020  loss: 450.4060 (1177.7112)  loss_n_40: 5.2936 (4.7704)  loss_n_60: 7.2092 (5.7940)  loss_n_80: 7.8530 (6.1045)  loss_n_100: 7.4585 (5.8918)  triple_100: 133.0444 (314.7585)  triple_80: 120.6389 (310.8889)  triple_60: 78.0653 (279.7888)  triple_40: 70.9282 (249.7143)  time: 3.9293  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 170/1724]  eta: 1:41:48  lr: 0.000020  loss: 380.0864 (1133.5614)  loss_n_40: 5.4014 (4.8138)  loss_n_60: 7.3661 (5.8949)  loss_n_80: 8.0777 (6.2227)  loss_n_100: 7.6704 (6.0001)  triple_100: 122.2825 (304.1142)  triple_80: 105.4306 (299.6608)  triple_60: 45.2008 (267.5201)  triple_40: 50.9732 (239.3347)  time: 3.9292  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 180/1724]  eta: 1:41:08  lr: 0.000020  loss: 355.6980 (1091.0499)  loss_n_40: 5.4329 (4.8503)  loss_n_60: 7.4629 (5.9825)  loss_n_80: 8.0871 (6.3274)  loss_n_100: 7.7843 (6.1020)  triple_100: 124.2928 (294.4124)  triple_80: 106.3166 (289.0827)  triple_60: 44.9599 (255.5401)  triple_40: 41.8260 (228.7523)  time: 3.9293  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 190/1724]  eta: 1:40:29  lr: 0.000020  loss: 343.8773 (1052.4174)  loss_n_40: 5.4329 (4.8870)  loss_n_60: 7.4629 (6.0669)  loss_n_80: 8.1134 (6.4270)  loss_n_100: 7.7523 (6.1956)  triple_100: 120.3916 (285.1513)  triple_80: 101.2348 (279.3328)  triple_60: 44.9599 (244.8667)  triple_40: 41.8180 (219.4902)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 200/1724]  eta: 1:39:50  lr: 0.000020  loss: 332.4919 (1016.0251)  loss_n_40: 5.5687 (4.9244)  loss_n_60: 7.6178 (6.1491)  loss_n_80: 8.1942 (6.5195)  loss_n_100: 7.8662 (6.2811)  triple_100: 111.4543 (276.3624)  triple_80: 100.0758 (270.1880)  triple_60: 48.0567 (234.8583)  triple_40: 38.3980 (210.7422)  time: 3.9294  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 210/1724]  eta: 1:39:11  lr: 0.000020  loss: 314.1219 (981.1744)  loss_n_40: 5.7450 (4.9696)  loss_n_60: 7.8780 (6.2387)  loss_n_80: 8.4197 (6.6177)  loss_n_100: 8.0481 (6.3736)  triple_100: 106.8064 (267.9272)  triple_80: 91.8866 (261.3842)  triple_60: 44.4317 (225.3642)  triple_40: 35.7963 (202.2992)  time: 3.9318  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 220/1724]  eta: 1:38:31  lr: 0.000020  loss: 309.3629 (950.6042)  loss_n_40: 5.7754 (5.0022)  loss_n_60: 7.8655 (6.3081)  loss_n_80: 8.4197 (6.6972)  loss_n_100: 8.0969 (6.4477)  triple_100: 105.8297 (260.7339)  triple_80: 91.8177 (253.9415)  triple_60: 41.2352 (216.8825)  triple_40: 26.6505 (194.5911)  time: 3.9325  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 230/1724]  eta: 1:37:52  lr: 0.000020  loss: 259.9467 (920.0532)  loss_n_40: 5.7832 (5.0418)  loss_n_60: 7.8927 (6.3843)  loss_n_80: 8.4952 (6.7808)  loss_n_100: 8.1014 (6.5248)  triple_100: 94.1599 (253.2749)  triple_80: 82.4898 (246.3119)  triple_60: 29.6981 (208.5263)  triple_40: 18.9766 (187.2083)  time: 3.9313  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 240/1724]  eta: 1:37:13  lr: 0.000020  loss: 193.5260 (892.2493)  loss_n_40: 5.8590 (5.0765)  loss_n_60: 7.9928 (6.4530)  loss_n_80: 8.6073 (6.8579)  loss_n_100: 8.2129 (6.5946)  triple_100: 81.0614 (246.3503)  triple_80: 63.8874 (239.3214)  triple_60: 12.4881 (201.0340)  triple_40: 13.6896 (180.5614)  time: 3.9304  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 250/1724]  eta: 1:36:33  lr: 0.000020  loss: 251.0873 (866.8325)  loss_n_40: 5.9280 (5.1127)  loss_n_60: 8.1149 (6.5227)  loss_n_80: 8.6563 (6.9328)  loss_n_100: 8.1712 (6.6578)  triple_100: 79.3259 (239.7757)  triple_80: 78.3680 (232.9543)  triple_60: 32.9002 (194.2917)  triple_40: 30.9518 (174.5847)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 260/1724]  eta: 1:35:54  lr: 0.000020  loss: 251.0873 (843.1822)  loss_n_40: 5.8946 (5.1393)  loss_n_60: 8.0340 (6.5773)  loss_n_80: 8.6505 (6.9965)  loss_n_100: 8.0936 (6.7097)  triple_100: 78.0873 (233.7056)  triple_80: 78.3680 (227.1804)  triple_60: 29.1147 (187.9346)  triple_40: 30.9518 (168.9388)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 270/1724]  eta: 1:35:14  lr: 0.000020  loss: 216.7968 (820.5242)  loss_n_40: 5.8793 (5.1701)  loss_n_60: 7.9476 (6.6353)  loss_n_80: 8.6454 (7.0623)  loss_n_100: 8.0598 (6.7640)  triple_100: 74.3039 (227.7610)  triple_80: 77.2606 (221.7120)  triple_60: 16.3112 (181.8856)  triple_40: 10.3225 (163.5338)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 280/1724]  eta: 1:34:35  lr: 0.000020  loss: 166.5642 (797.3897)  loss_n_40: 5.9762 (5.1996)  loss_n_60: 8.1990 (6.6908)  loss_n_80: 8.7669 (7.1250)  loss_n_100: 8.1456 (6.8133)  triple_100: 57.7464 (221.7132)  triple_80: 57.1169 (215.9744)  triple_60: 7.7435 (175.8052)  triple_40: 6.1251 (158.0683)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 290/1724]  eta: 1:33:55  lr: 0.000020  loss: 166.5642 (777.3690)  loss_n_40: 5.9411 (5.2222)  loss_n_60: 8.0915 (6.7359)  loss_n_80: 8.8242 (7.1800)  loss_n_100: 8.1338 (6.8567)  triple_100: 60.4723 (216.4348)  triple_80: 57.4072 (211.0703)  triple_60: 8.8745 (170.5377)  triple_40: 6.6914 (153.3313)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 300/1724]  eta: 1:33:16  lr: 0.000020  loss: 177.7359 (758.7093)  loss_n_40: 5.8433 (5.2410)  loss_n_60: 7.9667 (6.7745)  loss_n_80: 8.7349 (7.2304)  loss_n_100: 8.0982 (6.8954)  triple_100: 63.4157 (211.4881)  triple_80: 65.4507 (206.4860)  triple_60: 12.9575 (165.6575)  triple_40: 12.2945 (148.9363)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 310/1724]  eta: 1:32:36  lr: 0.000020  loss: 196.1408 (741.2606)  loss_n_40: 5.7720 (5.2598)  loss_n_60: 7.8840 (6.8130)  loss_n_80: 8.7349 (7.2797)  loss_n_100: 7.9562 (6.9323)  triple_100: 63.4157 (206.7716)  triple_80: 67.4734 (202.2145)  triple_60: 21.5171 (161.1439)  triple_40: 10.2848 (144.8458)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 320/1724]  eta: 1:31:57  lr: 0.000020  loss: 202.7843 (724.4039)  loss_n_40: 5.7846 (5.2773)  loss_n_60: 7.9178 (6.8500)  loss_n_80: 8.7937 (7.3278)  loss_n_100: 8.0495 (6.9682)  triple_100: 62.2838 (202.2172)  triple_80: 69.1480 (198.0652)  triple_60: 21.7416 (156.8071)  triple_40: 10.2848 (140.8910)  time: 3.9283  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [ 330/1724]  eta: 1:31:18  lr: 0.000020  loss: 196.9863 (708.2432)  loss_n_40: 5.9672 (5.3003)  loss_n_60: 8.1669 (6.8936)  loss_n_80: 8.8712 (7.3772)  loss_n_100: 8.1863 (7.0058)  triple_100: 58.1774 (197.6357)  triple_80: 68.5695 (194.0574)  triple_60: 22.0278 (152.6908)  triple_40: 10.3321 (137.2824)  time: 3.9284  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 340/1724]  eta: 1:30:38  lr: 0.000020  loss: 164.7229 (692.7831)  loss_n_40: 6.0035 (5.3187)  loss_n_60: 8.1891 (6.9297)  loss_n_80: 8.9830 (7.4226)  loss_n_100: 8.2123 (7.0415)  triple_100: 47.6898 (193.4489)  triple_80: 57.7083 (190.1427)  triple_60: 14.9389 (148.7807)  triple_40: 14.4134 (133.6982)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 350/1724]  eta: 1:29:59  lr: 0.000020  loss: 176.7464 (679.3243)  loss_n_40: 5.8797 (5.3344)  loss_n_60: 8.1064 (6.9619)  loss_n_80: 8.9396 (7.4634)  loss_n_100: 8.0827 (7.0697)  triple_100: 52.9295 (189.5455)  triple_80: 57.3304 (186.8392)  triple_60: 18.0826 (145.3230)  triple_40: 14.4134 (130.7871)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 360/1724]  eta: 1:29:19  lr: 0.000020  loss: 221.9450 (666.5442)  loss_n_40: 5.9006 (5.3501)  loss_n_60: 8.1293 (6.9931)  loss_n_80: 8.8733 (7.5040)  loss_n_100: 8.0827 (7.1029)  triple_100: 58.6674 (185.9994)  triple_80: 74.2770 (183.7094)  triple_60: 28.1011 (142.0738)  triple_40: 24.3541 (127.8115)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 370/1724]  eta: 1:28:40  lr: 0.000020  loss: 199.0257 (653.5837)  loss_n_40: 5.9258 (5.3677)  loss_n_60: 8.1293 (7.0252)  loss_n_80: 8.9529 (7.5454)  loss_n_100: 8.2237 (7.1342)  triple_100: 57.4238 (182.2919)  triple_80: 68.4610 (180.4648)  triple_60: 25.0351 (138.7783)  triple_40: 18.3415 (124.9763)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 380/1724]  eta: 1:28:01  lr: 0.000020  loss: 144.4894 (640.1938)  loss_n_40: 6.0341 (5.3870)  loss_n_60: 8.2543 (7.0591)  loss_n_80: 9.0899 (7.5880)  loss_n_100: 8.3620 (7.1685)  triple_100: 42.9159 (178.5622)  triple_80: 54.1125 (177.1065)  triple_60: 11.5667 (135.4534)  triple_40: 4.7367 (121.8692)  time: 3.9298  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 390/1724]  eta: 1:27:21  lr: 0.000020  loss: 139.7810 (627.8244)  loss_n_40: 6.0406 (5.4034)  loss_n_60: 8.2543 (7.0894)  loss_n_80: 9.1280 (7.6272)  loss_n_100: 8.3876 (7.1966)  triple_100: 34.7074 (174.9949)  triple_80: 48.7578 (173.9722)  triple_60: 8.8851 (132.3952)  triple_40: 4.2530 (119.1456)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 400/1724]  eta: 1:26:42  lr: 0.000020  loss: 126.4656 (615.8021)  loss_n_40: 5.9662 (5.4162)  loss_n_60: 8.1583 (7.1146)  loss_n_80: 9.0827 (7.6629)  loss_n_100: 8.1751 (7.2204)  triple_100: 34.7074 (171.5625)  triple_80: 47.1008 (170.9117)  triple_60: 7.6089 (129.4115)  triple_40: 3.9529 (116.5024)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 410/1724]  eta: 1:26:03  lr: 0.000020  loss: 138.3856 (604.8849)  loss_n_40: 5.9662 (5.4317)  loss_n_60: 8.1583 (7.1421)  loss_n_80: 9.0080 (7.6978)  loss_n_100: 8.2215 (7.2493)  triple_100: 39.4411 (168.3892)  triple_80: 46.7759 (168.1142)  triple_60: 13.0113 (126.7540)  triple_40: 2.6535 (114.1066)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 420/1724]  eta: 1:25:23  lr: 0.000020  loss: 138.3856 (593.7430)  loss_n_40: 5.9882 (5.4435)  loss_n_60: 8.1853 (7.1657)  loss_n_80: 9.1295 (7.7313)  loss_n_100: 8.3330 (7.2746)  triple_100: 41.5349 (165.2299)  triple_80: 43.7049 (165.1717)  triple_60: 14.4778 (124.0837)  triple_40: 4.1064 (111.6426)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 430/1724]  eta: 1:24:44  lr: 0.000020  loss: 135.7160 (583.6649)  loss_n_40: 5.9096 (5.4569)  loss_n_60: 8.1032 (7.1906)  loss_n_80: 9.1130 (7.7633)  loss_n_100: 8.3330 (7.3031)  triple_100: 41.5349 (162.3657)  triple_80: 42.1024 (162.5036)  triple_60: 14.3221 (121.6876)  triple_40: 3.5506 (109.3940)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 440/1724]  eta: 1:24:05  lr: 0.000020  loss: 136.4148 (573.4788)  loss_n_40: 6.0005 (5.4728)  loss_n_60: 8.1968 (7.2168)  loss_n_80: 9.1362 (7.7957)  loss_n_100: 8.4955 (7.3297)  triple_100: 32.7210 (159.3722)  triple_80: 44.3564 (159.7870)  triple_60: 14.3221 (119.2532)  triple_40: 3.9790 (107.2514)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 450/1724]  eta: 1:23:25  lr: 0.000020  loss: 98.9969 (562.9691)  loss_n_40: 6.0267 (5.4848)  loss_n_60: 8.1977 (7.2389)  loss_n_80: 9.2113 (7.8269)  loss_n_100: 8.5159 (7.3563)  triple_100: 30.4934 (156.4435)  triple_80: 30.5350 (156.9033)  triple_60: 8.9975 (116.7711)  triple_40: 1.4490 (104.9442)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 460/1724]  eta: 1:22:46  lr: 0.000020  loss: 107.4893 (554.2227)  loss_n_40: 6.0655 (5.4983)  loss_n_60: 8.2595 (7.2634)  loss_n_80: 9.0806 (7.8542)  loss_n_100: 8.5304 (7.3829)  triple_100: 33.7741 (153.8456)  triple_80: 31.1832 (154.4699)  triple_60: 9.3418 (114.8094)  triple_40: 3.0387 (103.0991)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 470/1724]  eta: 1:22:07  lr: 0.000020  loss: 156.5371 (545.6916)  loss_n_40: 6.1793 (5.5114)  loss_n_60: 8.4062 (7.2865)  loss_n_80: 9.0357 (7.8794)  loss_n_100: 8.5943 (7.4101)  triple_100: 34.7216 (151.4125)  triple_80: 39.9923 (151.9948)  triple_60: 25.6932 (112.8552)  triple_40: 20.0157 (101.3416)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 480/1724]  eta: 1:21:27  lr: 0.000020  loss: 114.8174 (536.3445)  loss_n_40: 6.1050 (5.5216)  loss_n_60: 8.3031 (7.3064)  loss_n_80: 9.0658 (7.9026)  loss_n_100: 8.5943 (7.4362)  triple_100: 32.0409 (148.8924)  triple_80: 23.0730 (149.1830)  triple_60: 15.3516 (110.7506)  triple_40: 11.6402 (99.3516)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 490/1724]  eta: 1:20:48  lr: 0.000020  loss: 88.6508 (527.2484)  loss_n_40: 5.9829 (5.5332)  loss_n_60: 8.2293 (7.3275)  loss_n_80: 9.0331 (7.9247)  loss_n_100: 8.8070 (7.4661)  triple_100: 28.4072 (146.4674)  triple_80: 14.2096 (146.3868)  triple_60: 10.7267 (108.7205)  triple_40: 0.0000 (97.4221)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 500/1724]  eta: 1:20:09  lr: 0.000020  loss: 78.5843 (518.2520)  loss_n_40: 6.1124 (5.5459)  loss_n_60: 8.3822 (7.3500)  loss_n_80: 9.0388 (7.9473)  loss_n_100: 8.9148 (7.4959)  triple_100: 25.3179 (144.0013)  triple_80: 9.2854 (143.6461)  triple_60: 10.7267 (106.7258)  triple_40: 1.3668 (95.5398)  time: 3.9289  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 510/1724]  eta: 1:19:29  lr: 0.000020  loss: 75.1561 (510.2513)  loss_n_40: 6.1945 (5.5604)  loss_n_60: 8.4864 (7.3737)  loss_n_80: 9.0388 (7.9680)  loss_n_100: 8.9440 (7.5247)  triple_100: 21.6371 (141.6881)  triple_80: 8.1636 (141.1518)  triple_60: 11.4332 (105.0532)  triple_40: 1.0353 (93.9314)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 520/1724]  eta: 1:18:50  lr: 0.000020  loss: 82.2453 (502.3939)  loss_n_40: 6.2071 (5.5715)  loss_n_60: 8.5043 (7.3937)  loss_n_80: 8.8907 (7.9866)  loss_n_100: 8.9500 (7.5516)  triple_100: 24.3107 (139.4643)  triple_80: 10.4033 (138.7242)  triple_60: 15.2655 (103.4077)  triple_40: 4.5478 (92.2942)  time: 3.9271  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:2]  [ 530/1724]  eta: 1:18:11  lr: 0.000020  loss: 80.3108 (494.6044)  loss_n_40: 6.1211 (5.5841)  loss_n_60: 8.4328 (7.4142)  loss_n_80: 8.9367 (8.0054)  loss_n_100: 8.8712 (7.5763)  triple_100: 19.8814 (137.2427)  triple_80: 9.7150 (136.3457)  triple_60: 14.4269 (101.7397)  triple_40: 2.9141 (90.6963)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 540/1724]  eta: 1:17:31  lr: 0.000020  loss: 71.8885 (486.9525)  loss_n_40: 6.1453 (5.5957)  loss_n_60: 8.4328 (7.4341)  loss_n_80: 8.9916 (8.0251)  loss_n_100: 8.8377 (7.6011)  triple_100: 16.0840 (135.0300)  triple_80: 8.4999 (134.0213)  triple_60: 8.8668 (100.1251)  triple_40: 0.9776 (89.1201)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 550/1724]  eta: 1:16:52  lr: 0.000020  loss: 81.0175 (479.6091)  loss_n_40: 6.3242 (5.6083)  loss_n_60: 8.5567 (7.4543)  loss_n_80: 9.0013 (8.0444)  loss_n_100: 8.8642 (7.6259)  triple_100: 16.7434 (132.8855)  triple_80: 10.6590 (131.8033)  triple_60: 15.4421 (98.5823)  triple_40: 1.4492 (87.6050)  time: 3.9282  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [ 560/1724]  eta: 1:16:13  lr: 0.000020  loss: 66.2182 (472.4898)  loss_n_40: 6.3350 (5.6203)  loss_n_60: 8.5711 (7.4736)  loss_n_80: 9.0926 (8.0638)  loss_n_100: 8.9506 (7.6528)  triple_100: 15.2157 (130.8032)  triple_80: 7.9641 (129.6327)  triple_60: 11.5584 (97.0802)  triple_40: 1.2268 (86.1632)  time: 3.9296  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 570/1724]  eta: 1:15:33  lr: 0.000020  loss: 67.0234 (465.6398)  loss_n_40: 6.3101 (5.6319)  loss_n_60: 8.5711 (7.4927)  loss_n_80: 9.0834 (8.0809)  loss_n_100: 9.0387 (7.6776)  triple_100: 13.3435 (128.8024)  triple_80: 6.9733 (127.5319)  triple_60: 12.0148 (95.6708)  triple_40: 0.6220 (84.7515)  time: 3.9298  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 580/1724]  eta: 1:14:54  lr: 0.000020  loss: 67.0234 (458.8647)  loss_n_40: 6.2887 (5.6421)  loss_n_60: 8.5578 (7.5113)  loss_n_80: 9.0518 (8.0985)  loss_n_100: 9.0732 (7.7029)  triple_100: 13.3435 (126.7993)  triple_80: 7.6933 (125.4873)  triple_60: 13.2654 (94.2816)  triple_40: 0.4268 (83.3418)  time: 3.9295  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 590/1724]  eta: 1:14:15  lr: 0.000020  loss: 52.4572 (452.0620)  loss_n_40: 6.2005 (5.6522)  loss_n_60: 8.5204 (7.5288)  loss_n_80: 9.0003 (8.1136)  loss_n_100: 9.0798 (7.7248)  triple_100: 9.0715 (124.8061)  triple_80: 3.9562 (123.4319)  triple_60: 8.5775 (92.8443)  triple_40: 0.0000 (81.9604)  time: 3.9295  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 600/1724]  eta: 1:13:36  lr: 0.000020  loss: 45.2141 (445.7993)  loss_n_40: 6.2875 (5.6647)  loss_n_60: 8.6317 (7.5482)  loss_n_80: 9.0150 (8.1290)  loss_n_100: 9.1282 (7.7478)  triple_100: 5.4644 (122.9245)  triple_80: 1.7080 (121.5329)  triple_60: 5.0350 (91.5534)  triple_40: 0.0000 (80.6988)  time: 3.9301  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 610/1724]  eta: 1:12:56  lr: 0.000020  loss: 61.8992 (439.9216)  loss_n_40: 6.3794 (5.6761)  loss_n_60: 8.6523 (7.5661)  loss_n_80: 9.0191 (8.1421)  loss_n_100: 9.0540 (7.7693)  triple_100: 9.2073 (121.1510)  triple_80: 6.2339 (119.7190)  triple_60: 12.4034 (90.3577)  triple_40: 1.1375 (79.5404)  time: 3.9290  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 620/1724]  eta: 1:12:17  lr: 0.000020  loss: 64.2370 (434.2236)  loss_n_40: 6.3225 (5.6862)  loss_n_60: 8.5669 (7.5822)  loss_n_80: 8.9601 (8.1556)  loss_n_100: 9.0540 (7.7906)  triple_100: 8.4046 (119.4090)  triple_80: 4.0304 (117.9844)  triple_60: 13.5636 (89.1996)  triple_40: 1.5838 (78.4159)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 630/1724]  eta: 1:11:38  lr: 0.000020  loss: 64.2370 (428.5985)  loss_n_40: 6.3434 (5.6962)  loss_n_60: 8.6383 (7.5986)  loss_n_80: 8.9388 (8.1665)  loss_n_100: 8.9348 (7.8084)  triple_100: 7.1803 (117.7240)  triple_80: 4.0304 (116.2678)  triple_60: 13.5636 (88.0825)  triple_40: 0.5312 (77.2546)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 640/1724]  eta: 1:10:58  lr: 0.000020  loss: 53.6854 (422.8647)  loss_n_40: 6.3307 (5.7065)  loss_n_60: 8.5955 (7.6142)  loss_n_80: 8.8851 (8.1787)  loss_n_100: 8.8908 (7.8260)  triple_100: 6.3754 (116.0117)  triple_80: 3.3189 (114.5309)  triple_60: 10.1447 (86.8759)  triple_40: 0.2386 (76.1207)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 650/1724]  eta: 1:10:19  lr: 0.000020  loss: 48.5522 (417.2721)  loss_n_40: 6.3613 (5.7170)  loss_n_60: 8.5624 (7.6298)  loss_n_80: 8.9151 (8.1907)  loss_n_100: 8.8549 (7.8430)  triple_100: 5.0225 (114.3437)  triple_80: 2.9544 (112.8341)  triple_60: 9.1368 (85.7139)  triple_40: 0.0000 (74.9998)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 660/1724]  eta: 1:09:40  lr: 0.000020  loss: 37.3344 (411.6331)  loss_n_40: 6.3300 (5.7266)  loss_n_60: 8.5495 (7.6440)  loss_n_80: 9.0127 (8.2042)  loss_n_100: 8.9416 (7.8616)  triple_100: 1.7971 (112.6578)  triple_80: 0.0000 (111.1574)  triple_60: 3.1316 (84.4864)  triple_40: 0.0000 (73.8950)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 670/1724]  eta: 1:09:00  lr: 0.000020  loss: 35.7912 (406.1452)  loss_n_40: 6.3308 (5.7364)  loss_n_60: 8.5446 (7.6579)  loss_n_80: 9.0283 (8.2172)  loss_n_100: 9.0973 (7.8807)  triple_100: 1.0420 (111.0341)  triple_80: 0.0000 (109.5195)  triple_60: 0.6997 (83.3004)  triple_40: 0.0000 (72.7990)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 680/1724]  eta: 1:08:21  lr: 0.000020  loss: 37.8230 (401.0011)  loss_n_40: 6.3877 (5.7472)  loss_n_60: 8.5805 (7.6727)  loss_n_80: 9.0114 (8.2297)  loss_n_100: 9.1362 (7.8984)  triple_100: 3.5012 (109.4874)  triple_80: 0.0000 (107.9664)  triple_60: 2.5569 (82.2021)  triple_40: 0.0000 (71.7972)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 690/1724]  eta: 1:07:42  lr: 0.000020  loss: 53.8232 (396.1315)  loss_n_40: 6.3654 (5.7577)  loss_n_60: 8.5874 (7.6871)  loss_n_80: 8.9412 (8.2394)  loss_n_100: 8.9014 (7.9133)  triple_100: 4.4845 (108.0170)  triple_80: 3.3464 (106.4831)  triple_60: 10.1371 (81.2038)  triple_40: 0.0000 (70.8302)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 700/1724]  eta: 1:07:03  lr: 0.000020  loss: 55.0176 (391.3174)  loss_n_40: 6.3800 (5.7682)  loss_n_60: 8.6397 (7.7017)  loss_n_80: 8.9952 (8.2499)  loss_n_100: 9.0227 (7.9312)  triple_100: 8.2007 (106.5738)  triple_80: 4.2966 (105.0145)  triple_60: 10.5181 (80.2051)  triple_40: 0.0000 (69.8729)  time: 3.9284  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 710/1724]  eta: 1:06:23  lr: 0.000020  loss: 48.9452 (386.6346)  loss_n_40: 6.3631 (5.7767)  loss_n_60: 8.6052 (7.7142)  loss_n_80: 8.9329 (8.2590)  loss_n_100: 9.0437 (7.9464)  triple_100: 3.4475 (105.1458)  triple_80: 2.4021 (103.5965)  triple_60: 10.0310 (79.2493)  triple_40: 0.0000 (68.9468)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 720/1724]  eta: 1:05:44  lr: 0.000020  loss: 46.9458 (382.1497)  loss_n_40: 6.4226 (5.7870)  loss_n_60: 8.6418 (7.7282)  loss_n_80: 8.7891 (8.2674)  loss_n_100: 8.9643 (7.9621)  triple_100: 2.5009 (103.7971)  triple_80: 1.5931 (102.2325)  triple_60: 9.3780 (78.3232)  triple_40: 0.0000 (68.0522)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 730/1724]  eta: 1:05:05  lr: 0.000020  loss: 53.5867 (377.8073)  loss_n_40: 6.4728 (5.7966)  loss_n_60: 8.7284 (7.7412)  loss_n_80: 8.7829 (8.2762)  loss_n_100: 8.9842 (7.9768)  triple_100: 5.5848 (102.4762)  triple_80: 2.7853 (100.9148)  triple_60: 10.6145 (77.4385)  triple_40: 0.0000 (67.1871)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 740/1724]  eta: 1:04:25  lr: 0.000020  loss: 55.1007 (373.5117)  loss_n_40: 6.4305 (5.8063)  loss_n_60: 8.6888 (7.7540)  loss_n_80: 8.8729 (8.2834)  loss_n_100: 9.1002 (7.9915)  triple_100: 5.7685 (101.1769)  triple_80: 3.3537 (99.6059)  triple_60: 11.2792 (76.5589)  triple_40: 0.6898 (66.3349)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 750/1724]  eta: 1:03:46  lr: 0.000020  loss: 53.4797 (369.3863)  loss_n_40: 6.4369 (5.8150)  loss_n_60: 8.6932 (7.7662)  loss_n_80: 8.8729 (8.2923)  loss_n_100: 9.1002 (8.0067)  triple_100: 2.6614 (99.9051)  triple_80: 2.0218 (98.3541)  triple_60: 8.8253 (75.7274)  triple_40: 0.0000 (65.5196)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 760/1724]  eta: 1:03:07  lr: 0.000020  loss: 40.7421 (365.1175)  loss_n_40: 6.4369 (5.8238)  loss_n_60: 8.6873 (7.7779)  loss_n_80: 8.9757 (8.3014)  loss_n_100: 9.0883 (8.0219)  triple_100: 0.6333 (98.6218)  triple_80: 0.9077 (97.0798)  triple_60: 7.3697 (74.8095)  triple_40: 0.0000 (64.6814)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 770/1724]  eta: 1:02:27  lr: 0.000020  loss: 42.6697 (361.4073)  loss_n_40: 6.3900 (5.8315)  loss_n_60: 8.6711 (7.7887)  loss_n_80: 8.8152 (8.3071)  loss_n_100: 9.0174 (8.0338)  triple_100: 2.4253 (97.4712)  triple_80: 1.6988 (95.9316)  triple_60: 7.7233 (74.0727)  triple_40: 0.0000 (63.9708)  time: 3.9271  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 780/1724]  eta: 1:01:48  lr: 0.000020  loss: 72.6111 (357.7953)  loss_n_40: 6.5017 (5.8413)  loss_n_60: 8.6755 (7.8010)  loss_n_80: 8.8152 (8.3136)  loss_n_100: 8.9482 (8.0467)  triple_100: 6.5857 (96.3607)  triple_80: 6.5181 (94.8284)  triple_60: 17.2172 (73.3559)  triple_40: 4.2963 (63.2477)  time: 3.9279  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [ 790/1724]  eta: 1:01:09  lr: 0.000020  loss: 58.1388 (354.1761)  loss_n_40: 6.4918 (5.8484)  loss_n_60: 8.5989 (7.8103)  loss_n_80: 8.7703 (8.3192)  loss_n_100: 8.9793 (8.0588)  triple_100: 5.1394 (95.2443)  triple_80: 4.0731 (93.7199)  triple_60: 14.7654 (72.6303)  triple_40: 3.5665 (62.5450)  time: 3.9283  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 800/1724]  eta: 1:00:30  lr: 0.000020  loss: 49.2660 (350.3301)  loss_n_40: 6.3453 (5.8556)  loss_n_60: 8.5348 (7.8199)  loss_n_80: 8.7859 (8.3265)  loss_n_100: 8.9700 (8.0706)  triple_100: 3.0481 (94.0895)  triple_80: 2.6783 (92.5748)  triple_60: 10.2043 (71.8090)  triple_40: 0.0000 (61.7842)  time: 3.9271  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 810/1724]  eta: 0:59:50  lr: 0.000020  loss: 34.9025 (346.6461)  loss_n_40: 6.4633 (5.8632)  loss_n_60: 8.6371 (7.8299)  loss_n_80: 8.8809 (8.3340)  loss_n_100: 9.0675 (8.0834)  triple_100: 1.0139 (92.9788)  triple_80: 0.0000 (91.4768)  triple_60: 1.3822 (71.0214)  triple_40: 0.0000 (61.0586)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 820/1724]  eta: 0:59:11  lr: 0.000020  loss: 35.1361 (343.0955)  loss_n_40: 6.4633 (5.8707)  loss_n_60: 8.5829 (7.8401)  loss_n_80: 8.8001 (8.3398)  loss_n_100: 8.9689 (8.0942)  triple_100: 1.2059 (91.9019)  triple_80: 0.0000 (90.4072)  triple_60: 1.3822 (70.2747)  triple_40: 0.0000 (60.3669)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 830/1724]  eta: 0:58:32  lr: 0.000020  loss: 41.2896 (339.6858)  loss_n_40: 6.4675 (5.8776)  loss_n_60: 8.6312 (7.8498)  loss_n_80: 8.7671 (8.3453)  loss_n_100: 8.9357 (8.1045)  triple_100: 1.9777 (90.8712)  triple_80: 0.0000 (89.3749)  triple_60: 6.7014 (69.5839)  triple_40: 0.0000 (59.6785)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 840/1724]  eta: 0:57:52  lr: 0.000020  loss: 45.7906 (336.3654)  loss_n_40: 6.5110 (5.8863)  loss_n_60: 8.7336 (7.8603)  loss_n_80: 8.8174 (8.3512)  loss_n_100: 8.8602 (8.1151)  triple_100: 2.0207 (89.8544)  triple_80: 0.0000 (88.3660)  triple_60: 10.3678 (68.9005)  triple_40: 0.0000 (59.0315)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 850/1724]  eta: 0:57:13  lr: 0.000020  loss: 45.9258 (333.0424)  loss_n_40: 6.5436 (5.8933)  loss_n_60: 8.7600 (7.8699)  loss_n_80: 8.8320 (8.3578)  loss_n_100: 9.0315 (8.1263)  triple_100: 2.0207 (88.8372)  triple_80: 3.5759 (87.3770)  triple_60: 10.3678 (68.2211)  triple_40: 0.0000 (58.3598)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 860/1724]  eta: 0:56:34  lr: 0.000020  loss: 53.3066 (329.9331)  loss_n_40: 6.4729 (5.9009)  loss_n_60: 8.7129 (7.8791)  loss_n_80: 8.8371 (8.3638)  loss_n_100: 9.0315 (8.1375)  triple_100: 3.0089 (87.8768)  triple_80: 3.2087 (86.4401)  triple_60: 8.1297 (67.5586)  triple_40: 0.7724 (57.7762)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 870/1724]  eta: 0:55:54  lr: 0.000020  loss: 43.8498 (326.6450)  loss_n_40: 6.4972 (5.9075)  loss_n_60: 8.6608 (7.8874)  loss_n_80: 8.8101 (8.3694)  loss_n_100: 9.0209 (8.1483)  triple_100: 3.0089 (86.8914)  triple_80: 1.0398 (85.4614)  triple_60: 7.0267 (66.8538)  triple_40: 0.0000 (57.1257)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 880/1724]  eta: 0:55:15  lr: 0.000020  loss: 37.4950 (323.5184)  loss_n_40: 6.4694 (5.9139)  loss_n_60: 8.6103 (7.8959)  loss_n_80: 8.8191 (8.3754)  loss_n_100: 9.0863 (8.1598)  triple_100: 1.1565 (85.9522)  triple_80: 0.0000 (84.5287)  triple_60: 4.0011 (66.1853)  triple_40: 0.0000 (56.5073)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 890/1724]  eta: 0:54:36  lr: 0.000020  loss: 38.6424 (320.3483)  loss_n_40: 6.4709 (5.9206)  loss_n_60: 8.6789 (7.9048)  loss_n_80: 8.8191 (8.3807)  loss_n_100: 9.0960 (8.1706)  triple_100: 1.0922 (85.0068)  triple_80: 0.0000 (83.5890)  triple_60: 3.9210 (65.4927)  triple_40: 0.0000 (55.8831)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 900/1724]  eta: 0:53:57  lr: 0.000020  loss: 34.7187 (317.1907)  loss_n_40: 6.5359 (5.9271)  loss_n_60: 8.6789 (7.9130)  loss_n_80: 8.7695 (8.3860)  loss_n_100: 9.0945 (8.1811)  triple_100: 0.0000 (84.0689)  triple_80: 0.0000 (82.6643)  triple_60: 0.6085 (64.7792)  triple_40: 0.0000 (55.2710)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 910/1724]  eta: 0:53:17  lr: 0.000020  loss: 33.8819 (314.1959)  loss_n_40: 6.5382 (5.9331)  loss_n_60: 8.6662 (7.9211)  loss_n_80: 8.8429 (8.3903)  loss_n_100: 9.0782 (8.1906)  triple_100: 0.0000 (83.1709)  triple_80: 0.0000 (81.7781)  triple_60: 0.0000 (64.1247)  triple_40: 0.0000 (54.6870)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 920/1724]  eta: 0:52:38  lr: 0.000020  loss: 36.0207 (311.2555)  loss_n_40: 6.5607 (5.9400)  loss_n_60: 8.6824 (7.9294)  loss_n_80: 8.8429 (8.3948)  loss_n_100: 9.0753 (8.2007)  triple_100: 0.6215 (82.2879)  triple_80: 0.0000 (80.8999)  triple_60: 1.8769 (63.4821)  triple_40: 0.0000 (54.1208)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 930/1724]  eta: 0:51:59  lr: 0.000020  loss: 35.1632 (308.4347)  loss_n_40: 6.5982 (5.9465)  loss_n_60: 8.7337 (7.9376)  loss_n_80: 8.8387 (8.3999)  loss_n_100: 9.1806 (8.2120)  triple_100: 0.0829 (81.4391)  triple_80: 0.0000 (80.0614)  triple_60: 0.6668 (62.8627)  triple_40: 0.0000 (53.5755)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 940/1724]  eta: 0:51:19  lr: 0.000020  loss: 34.9064 (305.5480)  loss_n_40: 6.5728 (5.9523)  loss_n_60: 8.7287 (7.9456)  loss_n_80: 8.8308 (8.4044)  loss_n_100: 9.1679 (8.2226)  triple_100: 0.0829 (80.5838)  triple_80: 0.0000 (79.2127)  triple_60: 0.6668 (62.2192)  triple_40: 0.0000 (53.0072)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 950/1724]  eta: 0:50:40  lr: 0.000020  loss: 34.9064 (302.7665)  loss_n_40: 6.5209 (5.9593)  loss_n_60: 8.6805 (7.9543)  loss_n_80: 8.7813 (8.4095)  loss_n_100: 9.1475 (8.2327)  triple_100: 0.0000 (79.7467)  triple_80: 0.0000 (78.3912)  triple_60: 0.0000 (61.6069)  triple_40: 0.0000 (52.4661)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 960/1724]  eta: 0:50:01  lr: 0.000020  loss: 35.9143 (300.0954)  loss_n_40: 6.5851 (5.9662)  loss_n_60: 8.7254 (7.9627)  loss_n_80: 8.8156 (8.4137)  loss_n_100: 9.1467 (8.2414)  triple_100: 0.0000 (78.9434)  triple_80: 0.0000 (77.5879)  triple_60: 0.9385 (61.0475)  triple_40: 0.0000 (51.9326)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 970/1724]  eta: 0:49:21  lr: 0.000020  loss: 36.4160 (297.4446)  loss_n_40: 6.5450 (5.9716)  loss_n_60: 8.7207 (7.9696)  loss_n_80: 8.8156 (8.4170)  loss_n_100: 9.0085 (8.2494)  triple_100: 0.3287 (78.1501)  triple_80: 0.0000 (76.8018)  triple_60: 2.8062 (60.4709)  triple_40: 0.0000 (51.4141)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 980/1724]  eta: 0:48:42  lr: 0.000020  loss: 36.4160 (294.8694)  loss_n_40: 6.5591 (5.9777)  loss_n_60: 8.6788 (7.9768)  loss_n_80: 8.8166 (8.4215)  loss_n_100: 9.1676 (8.2583)  triple_100: 0.0000 (77.3799)  triple_80: 0.0000 (76.0453)  triple_60: 2.8600 (59.9094)  triple_40: 0.0000 (50.9006)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [ 990/1724]  eta: 0:48:03  lr: 0.000020  loss: 40.7207 (292.3647)  loss_n_40: 6.6306 (5.9835)  loss_n_60: 8.6788 (7.9838)  loss_n_80: 8.8166 (8.4242)  loss_n_100: 9.0378 (8.2653)  triple_100: 0.7004 (76.6229)  triple_80: 0.7478 (75.2964)  triple_60: 5.7388 (59.3834)  triple_40: 0.0000 (50.4054)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1000/1724]  eta: 0:47:24  lr: 0.000020  loss: 37.1663 (289.8297)  loss_n_40: 6.6410 (5.9891)  loss_n_60: 8.6622 (7.9907)  loss_n_80: 8.6475 (8.4283)  loss_n_100: 8.9474 (8.2730)  triple_100: 0.0316 (75.8645)  triple_80: 0.0000 (74.5534)  triple_60: 4.0674 (58.8260)  triple_40: 0.0000 (49.9048)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1010/1724]  eta: 0:46:44  lr: 0.000020  loss: 34.6600 (287.3869)  loss_n_40: 6.5189 (5.9949)  loss_n_60: 8.7103 (7.9977)  loss_n_80: 8.7944 (8.4313)  loss_n_100: 9.0721 (8.2813)  triple_100: 0.1391 (75.1333)  triple_80: 0.0000 (73.8246)  triple_60: 1.3241 (58.2972)  triple_40: 0.0000 (49.4267)  time: 3.9270  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [1020/1724]  eta: 0:46:05  lr: 0.000020  loss: 35.5327 (284.9859)  loss_n_40: 6.5189 (6.0006)  loss_n_60: 8.7103 (8.0044)  loss_n_80: 8.7989 (8.4350)  loss_n_100: 9.1307 (8.2901)  triple_100: 0.4295 (74.4140)  triple_80: 0.0000 (73.1160)  triple_60: 1.3241 (57.7658)  triple_40: 0.0000 (48.9601)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1030/1724]  eta: 0:45:26  lr: 0.000020  loss: 35.1450 (282.5903)  loss_n_40: 6.6197 (6.0070)  loss_n_60: 8.7371 (8.0116)  loss_n_80: 8.8569 (8.4396)  loss_n_100: 9.1806 (8.2991)  triple_100: 0.0000 (73.6990)  triple_80: 0.0000 (72.4129)  triple_60: 1.1620 (57.2359)  triple_40: 0.0000 (48.4852)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1040/1724]  eta: 0:44:46  lr: 0.000020  loss: 37.6337 (280.3733)  loss_n_40: 6.6197 (6.0129)  loss_n_60: 8.7481 (8.0187)  loss_n_80: 8.8569 (8.4433)  loss_n_100: 9.1703 (8.3070)  triple_100: 1.2275 (73.0229)  triple_80: 0.0000 (71.7497)  triple_60: 1.5026 (56.7662)  triple_40: 0.0000 (48.0526)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1050/1724]  eta: 0:44:07  lr: 0.000020  loss: 37.6337 (278.1414)  loss_n_40: 6.5933 (6.0184)  loss_n_60: 8.7481 (8.0254)  loss_n_80: 8.8086 (8.4471)  loss_n_100: 9.0774 (8.3147)  triple_100: 0.9161 (72.3488)  triple_80: 0.0000 (71.0880)  triple_60: 2.1890 (56.2737)  triple_40: 0.0000 (47.6253)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1060/1724]  eta: 0:43:28  lr: 0.000020  loss: 35.9748 (275.9821)  loss_n_40: 6.6401 (6.0247)  loss_n_60: 8.7478 (8.0323)  loss_n_80: 8.8242 (8.4510)  loss_n_100: 9.1609 (8.3226)  triple_100: 0.1467 (71.6815)  triple_80: 0.0000 (70.4444)  triple_60: 2.1890 (55.8101)  triple_40: 0.0000 (47.2154)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1070/1724]  eta: 0:42:49  lr: 0.000020  loss: 41.7797 (273.7987)  loss_n_40: 6.6187 (6.0292)  loss_n_60: 8.6843 (8.0380)  loss_n_80: 8.8019 (8.4536)  loss_n_100: 9.1622 (8.3304)  triple_100: 0.2801 (71.0276)  triple_80: 0.9458 (69.8017)  triple_60: 5.7142 (55.3347)  triple_40: 0.0000 (46.7836)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1080/1724]  eta: 0:42:09  lr: 0.000020  loss: 40.1694 (271.6562)  loss_n_40: 6.5399 (6.0345)  loss_n_60: 8.7143 (8.0447)  loss_n_80: 8.6836 (8.4565)  loss_n_100: 9.1565 (8.3378)  triple_100: 0.4682 (70.3884)  triple_80: 0.6434 (69.1663)  triple_60: 5.1870 (54.8721)  triple_40: 0.0000 (46.3559)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1090/1724]  eta: 0:41:30  lr: 0.000020  loss: 38.0170 (269.5338)  loss_n_40: 6.6126 (6.0398)  loss_n_60: 8.7143 (8.0507)  loss_n_80: 8.8253 (8.4602)  loss_n_100: 9.1872 (8.3458)  triple_100: 0.0907 (69.7530)  triple_80: 0.0000 (68.5405)  triple_60: 3.0860 (54.4017)  triple_40: 0.0000 (45.9421)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1100/1724]  eta: 0:40:51  lr: 0.000020  loss: 34.0781 (267.4035)  loss_n_40: 6.5591 (6.0438)  loss_n_60: 8.6805 (8.0560)  loss_n_80: 8.8318 (8.4632)  loss_n_100: 9.1040 (8.3524)  triple_100: 0.0000 (69.1222)  triple_80: 0.0000 (67.9189)  triple_60: 0.3888 (53.9223)  triple_40: 0.0000 (45.5248)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1110/1724]  eta: 0:40:11  lr: 0.000020  loss: 34.7783 (265.6327)  loss_n_40: 6.5327 (6.0493)  loss_n_60: 8.7011 (8.0627)  loss_n_80: 8.7561 (8.4659)  loss_n_100: 9.0572 (8.3590)  triple_100: 0.0841 (68.5660)  triple_80: 0.0000 (67.3868)  triple_60: 1.8610 (53.5539)  triple_40: 0.0000 (45.1890)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1120/1724]  eta: 0:39:32  lr: 0.000020  loss: 37.2006 (263.6624)  loss_n_40: 6.6370 (6.0549)  loss_n_60: 8.7561 (8.0689)  loss_n_80: 8.7810 (8.4697)  loss_n_100: 9.0618 (8.3665)  triple_100: 0.8222 (67.9732)  triple_80: 0.0000 (66.8038)  triple_60: 2.2829 (53.1184)  triple_40: 0.0000 (44.8069)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1130/1724]  eta: 0:38:53  lr: 0.000020  loss: 36.1445 (261.7164)  loss_n_40: 6.5090 (6.0588)  loss_n_60: 8.6605 (8.0736)  loss_n_80: 8.8208 (8.4722)  loss_n_100: 9.1034 (8.3727)  triple_100: 0.0000 (67.3886)  triple_80: 0.0000 (66.2313)  triple_60: 0.8442 (52.6899)  triple_40: 0.0000 (44.4292)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1140/1724]  eta: 0:38:13  lr: 0.000020  loss: 33.4721 (259.8352)  loss_n_40: 6.4922 (6.0632)  loss_n_60: 8.5939 (8.0787)  loss_n_80: 8.7103 (8.4742)  loss_n_100: 9.1020 (8.3785)  triple_100: 0.0000 (66.8239)  triple_80: 0.0000 (65.6794)  triple_60: 0.0000 (52.2780)  triple_40: 0.0000 (44.0594)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1150/1724]  eta: 0:37:34  lr: 0.000020  loss: 33.8682 (257.9217)  loss_n_40: 6.5712 (6.0680)  loss_n_60: 8.6433 (8.0843)  loss_n_80: 8.8055 (8.4781)  loss_n_100: 9.1714 (8.3858)  triple_100: 0.0000 (66.2501)  triple_80: 0.0000 (65.1166)  triple_60: 0.3932 (51.8563)  triple_40: 0.0000 (43.6826)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1160/1724]  eta: 0:36:55  lr: 0.000020  loss: 35.5642 (256.1107)  loss_n_40: 6.5971 (6.0724)  loss_n_60: 8.6639 (8.0891)  loss_n_80: 8.8055 (8.4804)  loss_n_100: 9.1348 (8.3920)  triple_100: 0.0141 (65.7044)  triple_80: 0.0000 (64.5803)  triple_60: 0.4372 (51.4566)  triple_40: 0.0000 (43.3355)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1170/1724]  eta: 0:36:16  lr: 0.000020  loss: 36.4027 (254.2560)  loss_n_40: 6.6235 (6.0766)  loss_n_60: 8.5960 (8.0941)  loss_n_80: 8.6729 (8.4826)  loss_n_100: 9.0209 (8.3979)  triple_100: 0.7383 (65.1506)  triple_80: 0.0000 (64.0350)  triple_60: 2.0096 (51.0464)  triple_40: 0.0000 (42.9728)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1180/1724]  eta: 0:35:36  lr: 0.000020  loss: 34.4169 (252.4940)  loss_n_40: 6.7795 (6.0813)  loss_n_60: 8.6487 (8.0993)  loss_n_80: 8.7257 (8.4854)  loss_n_100: 9.1182 (8.4049)  triple_100: 0.0000 (64.6203)  triple_80: 0.0000 (63.5155)  triple_60: 0.0000 (50.6473)  triple_40: 0.0000 (42.6400)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1190/1724]  eta: 0:34:57  lr: 0.000020  loss: 34.0588 (250.6766)  loss_n_40: 6.7434 (6.0860)  loss_n_60: 8.7518 (8.1046)  loss_n_80: 8.7212 (8.4875)  loss_n_100: 9.1340 (8.4114)  triple_100: 0.0000 (64.0840)  triple_80: 0.0000 (62.9854)  triple_60: 0.0000 (50.2303)  triple_40: 0.0000 (42.2873)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1200/1724]  eta: 0:34:18  lr: 0.000020  loss: 34.4535 (248.9682)  loss_n_40: 6.6875 (6.0912)  loss_n_60: 8.7964 (8.1105)  loss_n_80: 8.7275 (8.4906)  loss_n_100: 9.1615 (8.4184)  triple_100: 0.0000 (63.5690)  triple_80: 0.0000 (62.4832)  triple_60: 0.0075 (49.8554)  triple_40: 0.0000 (41.9499)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1210/1724]  eta: 0:33:38  lr: 0.000020  loss: 37.1104 (247.3123)  loss_n_40: 6.5570 (6.0950)  loss_n_60: 8.7154 (8.1148)  loss_n_80: 8.7520 (8.4924)  loss_n_100: 9.1615 (8.4238)  triple_100: 0.0000 (63.0673)  triple_80: 0.5157 (62.0002)  triple_60: 3.2694 (49.4864)  triple_40: 0.0000 (41.6325)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1220/1724]  eta: 0:32:59  lr: 0.000020  loss: 35.7411 (245.6987)  loss_n_40: 6.5277 (6.0988)  loss_n_60: 8.5681 (8.1190)  loss_n_80: 8.6773 (8.4939)  loss_n_100: 8.9813 (8.4288)  triple_100: 0.0705 (62.5737)  triple_80: 0.0000 (61.5192)  triple_60: 3.2694 (49.1481)  triple_40: 0.0000 (41.3172)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1230/1724]  eta: 0:32:20  lr: 0.000020  loss: 35.1818 (244.0053)  loss_n_40: 6.5629 (6.1034)  loss_n_60: 8.6165 (8.1241)  loss_n_80: 8.6888 (8.4970)  loss_n_100: 9.1229 (8.4359)  triple_100: 0.0000 (62.0710)  triple_80: 0.0000 (61.0245)  triple_60: 0.8552 (48.7612)  triple_40: 0.0000 (40.9882)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1240/1724]  eta: 0:31:41  lr: 0.000020  loss: 34.1289 (242.3696)  loss_n_40: 6.5629 (6.1073)  loss_n_60: 8.6554 (8.1284)  loss_n_80: 8.6643 (8.4980)  loss_n_100: 9.0455 (8.4404)  triple_100: 0.0000 (61.5827)  triple_80: 0.0000 (60.5442)  triple_60: 0.0000 (48.4001)  triple_40: 0.0000 (40.6684)  time: 3.9272  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [1250/1724]  eta: 0:31:01  lr: 0.000020  loss: 34.1289 (240.7547)  loss_n_40: 6.6738 (6.1123)  loss_n_60: 8.6554 (8.1333)  loss_n_80: 8.6260 (8.4998)  loss_n_100: 8.9876 (8.4457)  triple_100: 0.0504 (61.0994)  triple_80: 0.0000 (60.0699)  triple_60: 0.0000 (48.0364)  triple_40: 0.0000 (40.3578)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1260/1724]  eta: 0:30:22  lr: 0.000020  loss: 35.3603 (239.2196)  loss_n_40: 6.7016 (6.1174)  loss_n_60: 8.7201 (8.1381)  loss_n_80: 8.6724 (8.5016)  loss_n_100: 9.1088 (8.4514)  triple_100: 0.4758 (60.6333)  triple_80: 0.0000 (59.6107)  triple_60: 2.4249 (47.7114)  triple_40: 0.0000 (40.0557)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1270/1724]  eta: 0:29:43  lr: 0.000020  loss: 39.3939 (237.6632)  loss_n_40: 6.6803 (6.1214)  loss_n_60: 8.6319 (8.1421)  loss_n_80: 8.7864 (8.5038)  loss_n_100: 9.1529 (8.4575)  triple_100: 1.0044 (60.1681)  triple_80: 0.4467 (59.1546)  triple_60: 2.1166 (47.3636)  triple_40: 0.0000 (39.7520)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1280/1724]  eta: 0:29:03  lr: 0.000020  loss: 34.3718 (236.1025)  loss_n_40: 6.4915 (6.1247)  loss_n_60: 8.5765 (8.1457)  loss_n_80: 8.6931 (8.5053)  loss_n_100: 9.1730 (8.4632)  triple_100: 0.2853 (59.7041)  triple_80: 0.0000 (58.6998)  triple_60: 0.9610 (47.0113)  triple_40: 0.0000 (39.4485)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1290/1724]  eta: 0:28:24  lr: 0.000020  loss: 36.4760 (234.6284)  loss_n_40: 6.4635 (6.1277)  loss_n_60: 8.5143 (8.1486)  loss_n_80: 8.6402 (8.5059)  loss_n_100: 9.0710 (8.4680)  triple_100: 0.2853 (59.2586)  triple_80: 0.0000 (58.2597)  triple_60: 2.2361 (46.6899)  triple_40: 0.0000 (39.1699)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1300/1724]  eta: 0:27:45  lr: 0.000020  loss: 36.8332 (233.1356)  loss_n_40: 6.5091 (6.1306)  loss_n_60: 8.5545 (8.1515)  loss_n_80: 8.5899 (8.5067)  loss_n_100: 9.0475 (8.4727)  triple_100: 1.2441 (58.8136)  triple_80: 0.0000 (57.8213)  triple_60: 2.2361 (46.3629)  triple_40: 0.0000 (38.8764)  time: 3.9298  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1310/1724]  eta: 0:27:06  lr: 0.000020  loss: 34.3345 (231.6589)  loss_n_40: 6.6401 (6.1349)  loss_n_60: 8.6317 (8.1557)  loss_n_80: 8.6701 (8.5086)  loss_n_100: 9.1288 (8.4784)  triple_100: 0.0000 (58.3717)  triple_80: 0.0000 (57.3866)  triple_60: 0.8123 (46.0325)  triple_40: 0.0000 (38.5905)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1320/1724]  eta: 0:26:26  lr: 0.000020  loss: 35.7840 (230.2126)  loss_n_40: 6.6401 (6.1387)  loss_n_60: 8.6740 (8.1595)  loss_n_80: 8.7271 (8.5101)  loss_n_100: 9.1801 (8.4843)  triple_100: 0.0000 (57.9380)  triple_80: 0.0000 (56.9615)  triple_60: 2.7414 (45.7102)  triple_40: 0.0000 (38.3103)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1330/1724]  eta: 0:25:47  lr: 0.000020  loss: 34.7676 (228.7484)  loss_n_40: 6.6138 (6.1423)  loss_n_60: 8.6659 (8.1633)  loss_n_80: 8.8133 (8.5120)  loss_n_100: 9.2736 (8.4901)  triple_100: 0.0000 (57.5080)  triple_80: 0.0000 (56.5342)  triple_60: 0.3620 (45.3761)  triple_40: 0.0000 (38.0225)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1340/1724]  eta: 0:25:08  lr: 0.000020  loss: 34.4340 (227.3102)  loss_n_40: 6.5923 (6.1448)  loss_n_60: 8.5718 (8.1660)  loss_n_80: 8.6732 (8.5133)  loss_n_100: 9.1536 (8.4954)  triple_100: 0.0000 (57.0818)  triple_80: 0.0000 (56.1150)  triple_60: 0.1756 (45.0510)  triple_40: 0.0000 (37.7430)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1350/1724]  eta: 0:24:29  lr: 0.000020  loss: 34.4770 (225.8998)  loss_n_40: 6.5572 (6.1481)  loss_n_60: 8.5689 (8.1694)  loss_n_80: 8.6595 (8.5148)  loss_n_100: 9.1444 (8.5010)  triple_100: 0.0000 (56.6645)  triple_80: 0.0000 (55.7021)  triple_60: 0.6502 (44.7329)  triple_40: 0.0000 (37.4671)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1360/1724]  eta: 0:23:49  lr: 0.000020  loss: 36.7444 (224.5936)  loss_n_40: 6.6076 (6.1518)  loss_n_60: 8.6180 (8.1732)  loss_n_80: 8.7079 (8.5156)  loss_n_100: 9.1962 (8.5052)  triple_100: 0.4485 (56.2695)  triple_80: 0.0000 (55.3119)  triple_60: 1.6417 (44.4514)  triple_40: 0.0000 (37.2149)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1370/1724]  eta: 0:23:10  lr: 0.000020  loss: 36.1596 (223.2150)  loss_n_40: 6.6076 (6.1547)  loss_n_60: 8.6327 (8.1763)  loss_n_80: 8.7023 (8.5167)  loss_n_100: 9.0528 (8.5099)  triple_100: 0.1489 (55.8615)  triple_80: 0.0000 (54.9119)  triple_60: 1.7969 (44.1404)  triple_40: 0.0000 (36.9436)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1380/1724]  eta: 0:22:31  lr: 0.000020  loss: 34.9992 (221.9306)  loss_n_40: 6.4933 (6.1575)  loss_n_60: 8.5506 (8.1791)  loss_n_80: 8.6460 (8.5173)  loss_n_100: 9.0220 (8.5137)  triple_100: 0.0000 (55.4706)  triple_80: 0.0000 (54.5366)  triple_60: 1.7969 (43.8642)  triple_40: 0.0000 (36.6915)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1390/1724]  eta: 0:21:51  lr: 0.000020  loss: 33.1416 (220.5757)  loss_n_40: 6.4228 (6.1597)  loss_n_60: 8.4930 (8.1815)  loss_n_80: 8.5955 (8.5183)  loss_n_100: 9.0220 (8.5180)  triple_100: 0.0000 (55.0723)  triple_80: 0.0000 (54.1446)  triple_60: 0.0000 (43.5536)  triple_40: 0.0000 (36.4278)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1400/1724]  eta: 0:21:12  lr: 0.000020  loss: 32.7536 (219.2516)  loss_n_40: 6.4365 (6.1621)  loss_n_60: 8.4930 (8.1843)  loss_n_80: 8.6129 (8.5195)  loss_n_100: 9.1551 (8.5228)  triple_100: 0.0000 (54.6818)  triple_80: 0.0000 (53.7606)  triple_60: 0.0000 (43.2496)  triple_40: 0.0000 (36.1709)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1410/1724]  eta: 0:20:33  lr: 0.000020  loss: 33.6699 (217.9920)  loss_n_40: 6.5751 (6.1654)  loss_n_60: 8.5428 (8.1873)  loss_n_80: 8.6622 (8.5206)  loss_n_100: 9.1796 (8.5272)  triple_100: 0.0000 (54.3052)  triple_80: 0.0000 (53.3900)  triple_60: 0.2736 (42.9739)  triple_40: 0.0000 (35.9225)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1420/1724]  eta: 0:19:54  lr: 0.000020  loss: 34.3894 (216.7265)  loss_n_40: 6.5924 (6.1682)  loss_n_60: 8.5821 (8.1898)  loss_n_80: 8.6622 (8.5215)  loss_n_100: 9.0649 (8.5313)  triple_100: 0.0719 (53.9286)  triple_80: 0.0000 (53.0209)  triple_60: 0.5547 (42.6860)  triple_40: 0.0000 (35.6801)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1430/1724]  eta: 0:19:14  lr: 0.000020  loss: 34.0331 (215.4834)  loss_n_40: 6.4805 (6.1710)  loss_n_60: 8.5375 (8.1926)  loss_n_80: 8.6184 (8.5226)  loss_n_100: 9.0649 (8.5358)  triple_100: 0.0021 (53.5565)  triple_80: 0.0000 (52.6551)  triple_60: 0.4282 (42.4124)  triple_40: 0.0000 (35.4374)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1440/1724]  eta: 0:18:35  lr: 0.000020  loss: 35.2428 (214.2716)  loss_n_40: 6.6755 (6.1745)  loss_n_60: 8.6103 (8.1958)  loss_n_80: 8.6184 (8.5234)  loss_n_100: 9.1534 (8.5406)  triple_100: 0.0021 (53.1912)  triple_80: 0.0000 (52.2973)  triple_60: 1.6239 (42.1428)  triple_40: 0.0000 (35.2059)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1450/1724]  eta: 0:17:56  lr: 0.000020  loss: 34.4729 (213.0507)  loss_n_40: 6.6755 (6.1771)  loss_n_60: 8.6400 (8.1986)  loss_n_80: 8.5997 (8.5244)  loss_n_100: 9.1766 (8.5454)  triple_100: 0.0000 (52.8325)  triple_80: 0.0000 (51.9390)  triple_60: 1.1686 (41.8700)  triple_40: 0.0000 (34.9637)  time: 3.9256  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:2]  [1460/1724]  eta: 0:17:16  lr: 0.000020  loss: 33.9324 (211.8538)  loss_n_40: 6.6235 (6.1803)  loss_n_60: 8.6400 (8.2017)  loss_n_80: 8.6086 (8.5252)  loss_n_100: 9.2091 (8.5502)  triple_100: 0.0000 (52.4778)  triple_80: 0.0000 (51.5907)  triple_60: 0.0000 (41.6000)  triple_40: 0.0000 (34.7276)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1470/1724]  eta: 0:16:37  lr: 0.000020  loss: 33.6240 (210.6889)  loss_n_40: 6.6235 (6.1834)  loss_n_60: 8.6193 (8.2048)  loss_n_80: 8.5739 (8.5256)  loss_n_100: 9.1542 (8.5539)  triple_100: 0.0000 (52.1294)  triple_80: 0.0000 (51.2499)  triple_60: 0.0000 (41.3371)  triple_40: 0.0000 (34.5048)  time: 3.9277  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [1480/1724]  eta: 0:15:58  lr: 0.000020  loss: 33.8584 (209.5712)  loss_n_40: 6.5768 (6.1861)  loss_n_60: 8.6037 (8.2073)  loss_n_80: 8.5618 (8.5263)  loss_n_100: 9.0278 (8.5577)  triple_100: 0.0000 (51.7919)  triple_80: 0.0000 (50.9260)  triple_60: 0.0958 (41.0919)  triple_40: 0.0000 (34.2841)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1490/1724]  eta: 0:15:19  lr: 0.000020  loss: 34.2768 (208.3941)  loss_n_40: 6.5473 (6.1892)  loss_n_60: 8.6037 (8.2105)  loss_n_80: 8.5883 (8.5273)  loss_n_100: 9.1118 (8.5621)  triple_100: 0.0000 (51.4464)  triple_80: 0.0000 (50.5844)  triple_60: 0.0000 (40.8200)  triple_40: 0.0000 (34.0542)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1500/1724]  eta: 0:14:39  lr: 0.000020  loss: 34.2572 (207.2481)  loss_n_40: 6.6322 (6.1919)  loss_n_60: 8.6483 (8.2133)  loss_n_80: 8.7326 (8.5292)  loss_n_100: 9.2553 (8.5668)  triple_100: 0.0000 (51.1054)  triple_80: 0.0000 (50.2501)  triple_60: 0.0000 (40.5614)  triple_40: 0.0000 (33.8300)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1510/1724]  eta: 0:14:00  lr: 0.000020  loss: 33.8953 (206.1053)  loss_n_40: 6.5559 (6.1938)  loss_n_60: 8.5639 (8.2153)  loss_n_80: 8.7326 (8.5302)  loss_n_100: 9.2658 (8.5710)  triple_100: 0.0000 (50.7688)  triple_80: 0.0000 (49.9195)  triple_60: 0.0000 (40.2978)  triple_40: 0.0000 (33.6089)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1520/1724]  eta: 0:13:21  lr: 0.000020  loss: 33.4721 (205.0258)  loss_n_40: 6.5891 (6.1966)  loss_n_60: 8.5667 (8.2180)  loss_n_80: 8.6236 (8.5308)  loss_n_100: 9.1681 (8.5748)  triple_100: 0.0000 (50.4425)  triple_80: 0.0000 (49.6013)  triple_60: 0.0000 (40.0543)  triple_40: 0.0000 (33.4074)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1530/1724]  eta: 0:12:41  lr: 0.000020  loss: 33.5531 (203.9139)  loss_n_40: 6.6157 (6.1993)  loss_n_60: 8.6108 (8.2206)  loss_n_80: 8.5629 (8.5317)  loss_n_100: 9.1539 (8.5791)  triple_100: 0.0000 (50.1156)  triple_80: 0.0000 (49.2785)  triple_60: 0.0000 (39.7990)  triple_40: 0.0000 (33.1902)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1540/1724]  eta: 0:12:02  lr: 0.000020  loss: 33.8324 (202.8502)  loss_n_40: 6.6239 (6.2018)  loss_n_60: 8.6066 (8.2229)  loss_n_80: 8.5629 (8.5317)  loss_n_100: 9.1539 (8.5830)  triple_100: 0.0000 (49.7982)  triple_80: 0.0000 (48.9671)  triple_60: 0.0000 (39.5612)  triple_40: 0.0000 (32.9843)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1550/1724]  eta: 0:11:23  lr: 0.000020  loss: 33.2545 (201.7664)  loss_n_40: 6.5540 (6.2035)  loss_n_60: 8.5497 (8.2246)  loss_n_80: 8.4897 (8.5320)  loss_n_100: 9.1576 (8.5870)  triple_100: 0.0000 (49.4798)  triple_80: 0.0000 (48.6548)  triple_60: 0.0000 (39.3111)  triple_40: 0.0000 (32.7738)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1560/1724]  eta: 0:10:44  lr: 0.000020  loss: 33.7454 (200.7022)  loss_n_40: 6.5540 (6.2064)  loss_n_60: 8.5578 (8.2275)  loss_n_80: 8.6238 (8.5334)  loss_n_100: 9.2383 (8.5915)  triple_100: 0.0000 (49.1668)  triple_80: 0.0000 (48.3454)  triple_60: 0.0000 (39.0674)  triple_40: 0.0000 (32.5638)  time: 3.9260  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:2]  [1570/1724]  eta: 0:10:04  lr: 0.000020  loss: 34.1397 (199.6562)  loss_n_40: 6.5990 (6.2088)  loss_n_60: 8.5846 (8.2298)  loss_n_80: 8.7111 (8.5347)  loss_n_100: 9.3031 (8.5957)  triple_100: 0.0000 (48.8571)  triple_80: 0.0000 (48.0405)  triple_60: 0.0151 (38.8302)  triple_40: 0.0000 (32.3594)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1580/1724]  eta: 0:09:25  lr: 0.000020  loss: 33.8449 (198.6388)  loss_n_40: 6.6059 (6.2118)  loss_n_60: 8.5338 (8.2322)  loss_n_80: 8.6198 (8.5352)  loss_n_100: 9.2207 (8.5992)  triple_100: 0.0000 (48.5510)  triple_80: 0.0000 (47.7423)  triple_60: 0.1246 (38.6025)  triple_40: 0.0000 (32.1647)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1590/1724]  eta: 0:08:46  lr: 0.000020  loss: 33.3084 (197.6239)  loss_n_40: 6.5631 (6.2138)  loss_n_60: 8.5588 (8.2342)  loss_n_80: 8.4858 (8.5347)  loss_n_100: 9.0546 (8.6026)  triple_100: 0.0000 (48.2494)  triple_80: 0.0000 (47.4476)  triple_60: 0.0000 (38.3713)  triple_40: 0.0000 (31.9703)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1600/1724]  eta: 0:08:07  lr: 0.000020  loss: 33.6983 (196.6086)  loss_n_40: 6.5702 (6.2167)  loss_n_60: 8.5620 (8.2368)  loss_n_80: 8.5705 (8.5352)  loss_n_100: 9.1606 (8.6064)  triple_100: 0.0000 (47.9510)  triple_80: 0.0000 (47.1518)  triple_60: 0.0000 (38.1388)  triple_40: 0.0000 (31.7719)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1610/1724]  eta: 0:07:27  lr: 0.000020  loss: 34.2222 (195.6183)  loss_n_40: 6.6978 (6.2192)  loss_n_60: 8.6640 (8.2391)  loss_n_80: 8.6394 (8.5363)  loss_n_100: 9.1796 (8.6101)  triple_100: 0.0000 (47.6565)  triple_80: 0.0000 (46.8648)  triple_60: 0.0000 (37.9137)  triple_40: 0.0000 (31.5785)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1620/1724]  eta: 0:06:48  lr: 0.000020  loss: 33.9302 (194.6275)  loss_n_40: 6.5031 (6.2209)  loss_n_60: 8.5132 (8.2407)  loss_n_80: 8.5679 (8.5368)  loss_n_100: 9.1117 (8.6132)  triple_100: 0.0000 (47.3637)  triple_80: 0.0000 (46.5785)  triple_60: 0.0000 (37.6891)  triple_40: 0.0000 (31.3848)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1630/1724]  eta: 0:06:09  lr: 0.000020  loss: 33.0608 (193.6378)  loss_n_40: 6.5283 (6.2234)  loss_n_60: 8.5157 (8.2429)  loss_n_80: 8.5039 (8.5373)  loss_n_100: 9.0787 (8.6164)  triple_100: 0.0000 (47.0736)  triple_80: 0.0000 (46.2929)  triple_60: 0.0000 (37.4589)  triple_40: 0.0000 (31.1923)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1640/1724]  eta: 0:05:29  lr: 0.000020  loss: 33.8005 (192.7727)  loss_n_40: 6.6896 (6.2265)  loss_n_60: 8.6073 (8.2452)  loss_n_80: 8.5374 (8.5370)  loss_n_100: 9.0110 (8.6190)  triple_100: 0.0000 (46.8062)  triple_80: 0.0000 (46.0351)  triple_60: 0.0000 (37.2646)  triple_40: 0.0000 (31.0391)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1650/1724]  eta: 0:04:50  lr: 0.000020  loss: 33.9067 (191.8346)  loss_n_40: 6.6466 (6.2286)  loss_n_60: 8.5688 (8.2471)  loss_n_80: 8.5424 (8.5371)  loss_n_100: 9.0322 (8.6218)  triple_100: 0.1820 (46.5299)  triple_80: 0.0000 (45.7632)  triple_60: 0.4708 (37.0503)  triple_40: 0.0000 (30.8565)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1660/1724]  eta: 0:04:11  lr: 0.000020  loss: 33.1146 (190.8989)  loss_n_40: 6.5423 (6.2305)  loss_n_60: 8.5359 (8.2486)  loss_n_80: 8.5439 (8.5371)  loss_n_100: 9.0750 (8.6246)  triple_100: 0.0000 (46.2523)  triple_80: 0.0000 (45.4906)  triple_60: 0.0000 (36.8364)  triple_40: 0.0000 (30.6787)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1670/1724]  eta: 0:03:32  lr: 0.000020  loss: 33.1253 (189.9844)  loss_n_40: 6.5914 (6.2334)  loss_n_60: 8.5627 (8.2510)  loss_n_80: 8.4795 (8.5369)  loss_n_100: 9.0912 (8.6277)  triple_100: 0.0000 (45.9812)  triple_80: 0.0000 (45.2238)  triple_60: 0.0000 (36.6303)  triple_40: 0.0000 (30.5001)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1680/1724]  eta: 0:02:52  lr: 0.000020  loss: 33.4403 (189.0711)  loss_n_40: 6.6845 (6.2360)  loss_n_60: 8.6323 (8.2534)  loss_n_80: 8.4795 (8.5364)  loss_n_100: 9.1072 (8.6306)  triple_100: 0.0000 (45.7117)  triple_80: 0.0000 (44.9568)  triple_60: 0.0000 (36.4215)  triple_40: 0.0000 (30.3249)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1690/1724]  eta: 0:02:13  lr: 0.000020  loss: 33.9138 (188.1969)  loss_n_40: 6.6845 (6.2388)  loss_n_60: 8.6312 (8.2558)  loss_n_80: 8.3701 (8.5350)  loss_n_100: 9.0305 (8.6328)  triple_100: 0.5778 (45.4488)  triple_80: 0.0000 (44.6961)  triple_60: 0.3824 (36.2278)  triple_40: 0.0000 (30.1617)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1700/1724]  eta: 0:01:34  lr: 0.000020  loss: 34.1058 (187.3203)  loss_n_40: 6.6236 (6.2409)  loss_n_60: 8.6203 (8.2577)  loss_n_80: 8.3701 (8.5352)  loss_n_100: 9.0549 (8.6358)  triple_100: 0.4450 (45.1859)  triple_80: 0.0000 (44.4412)  triple_60: 0.3824 (36.0313)  triple_40: 0.0000 (29.9923)  time: 3.9257  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [1710/1724]  eta: 0:00:54  lr: 0.000020  loss: 33.1679 (186.4199)  loss_n_40: 6.5673 (6.2431)  loss_n_60: 8.5487 (8.2596)  loss_n_80: 8.4778 (8.5353)  loss_n_100: 9.0549 (8.6384)  triple_100: 0.0000 (44.9226)  triple_80: 0.0000 (44.1817)  triple_60: 0.0000 (35.8223)  triple_40: 0.0000 (29.8170)  time: 3.9252  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:2]  [1720/1724]  eta: 0:00:15  lr: 0.000020  loss: 33.2564 (185.5463)  loss_n_40: 6.4836 (6.2449)  loss_n_60: 8.5129 (8.2611)  loss_n_80: 8.4451 (8.5346)  loss_n_100: 8.9761 (8.6403)  triple_100: 0.0000 (44.6645)  triple_80: 0.0000 (43.9288)  triple_60: 0.0000 (35.6243)  triple_40: 0.0000 (29.6478)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2]  [1723/1724]  eta: 0:00:03  lr: 0.000020  loss: 33.6099 (185.2846)  loss_n_40: 6.5486 (6.2459)  loss_n_60: 8.4906 (8.2619)  loss_n_80: 8.5702 (8.5351)  loss_n_100: 9.0359 (8.6415)  triple_100: 0.0000 (44.5874)  triple_80: 0.0000 (43.8523)  triple_60: 0.0000 (35.5642)  triple_40: 0.0000 (29.5962)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:2] Total time: 1:52:51 (3.9276 s / it)\n",
      "Averaged stats: lr: 0.000020  loss: 33.6099 (185.2846)  loss_n_40: 6.5486 (6.2459)  loss_n_60: 8.4906 (8.2619)  loss_n_80: 8.5702 (8.5351)  loss_n_100: 9.0359 (8.6415)  triple_100: 0.0000 (44.5874)  triple_80: 0.0000 (43.8523)  triple_60: 0.0000 (35.5642)  triple_40: 0.0000 (29.5962)\n",
      "Valid: [epoch:2]  [  0/845]  eta: 0:09:39  loss: 31.3411 (31.3411)  loss_n_40: 5.7141 (5.7141)  loss_n_60: 8.1778 (8.1778)  loss_n_80: 8.4917 (8.4917)  loss_n_100: 8.9576 (8.9576)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.6853  data: 0.3500  max mem: 46473\n",
      "Valid: [epoch:2]  [ 10/845]  eta: 0:05:05  loss: 32.5788 (33.1551)  loss_n_40: 6.4178 (6.4280)  loss_n_60: 8.4661 (8.4608)  loss_n_80: 8.4917 (8.4365)  loss_n_100: 9.0538 (9.0468)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.7831)  triple_40: 0.0000 (0.0000)  time: 0.3663  data: 0.0319  max mem: 46473\n",
      "Valid: [epoch:2]  [ 20/845]  eta: 0:04:49  loss: 32.8624 (35.5723)  loss_n_40: 6.5952 (6.5561)  loss_n_60: 8.5953 (8.5264)  loss_n_80: 8.3804 (8.3886)  loss_n_100: 9.0027 (9.0141)  triple_100: 0.0000 (0.2095)  triple_80: 0.0000 (0.6228)  triple_60: 0.0000 (2.1131)  triple_40: 0.0000 (0.1416)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [ 30/845]  eta: 0:04:41  loss: 32.8624 (34.7644)  loss_n_40: 6.6344 (6.5604)  loss_n_60: 8.6056 (8.5477)  loss_n_80: 8.3804 (8.4250)  loss_n_100: 8.9907 (9.0360)  triple_100: 0.0000 (0.1420)  triple_80: 0.0000 (0.4219)  triple_60: 0.0000 (1.5356)  triple_40: 0.0000 (0.0960)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [ 40/845]  eta: 0:04:35  loss: 32.3854 (34.2443)  loss_n_40: 6.5344 (6.5427)  loss_n_60: 8.4743 (8.5315)  loss_n_80: 8.4231 (8.4500)  loss_n_100: 9.0147 (9.0601)  triple_100: 0.0000 (0.1073)  triple_80: 0.0000 (0.3190)  triple_60: 0.0000 (1.1611)  triple_40: 0.0000 (0.0725)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [ 50/845]  eta: 0:04:31  loss: 33.3879 (35.1128)  loss_n_40: 6.6722 (6.6263)  loss_n_60: 8.5842 (8.5858)  loss_n_80: 8.5683 (8.4824)  loss_n_100: 9.1390 (9.0966)  triple_100: 0.0000 (0.2679)  triple_80: 0.0000 (0.4651)  triple_60: 0.0000 (1.1899)  triple_40: 0.0000 (0.3989)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [ 60/845]  eta: 0:04:26  loss: 33.3879 (34.6964)  loss_n_40: 6.7511 (6.6025)  loss_n_60: 8.6179 (8.5698)  loss_n_80: 8.5683 (8.4802)  loss_n_100: 9.0750 (9.1028)  triple_100: 0.0000 (0.2240)  triple_80: 0.0000 (0.3888)  triple_60: 0.0000 (0.9948)  triple_40: 0.0000 (0.3335)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [ 70/845]  eta: 0:04:22  loss: 33.1439 (34.5234)  loss_n_40: 6.5967 (6.6121)  loss_n_60: 8.5563 (8.5736)  loss_n_80: 8.6328 (8.5090)  loss_n_100: 9.1784 (9.1265)  triple_100: 0.0000 (0.2071)  triple_80: 0.0000 (0.3341)  triple_60: 0.0000 (0.8746)  triple_40: 0.0000 (0.2865)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [ 80/845]  eta: 0:04:19  loss: 33.1580 (34.3626)  loss_n_40: 6.6817 (6.6212)  loss_n_60: 8.6142 (8.5836)  loss_n_80: 8.6345 (8.5135)  loss_n_100: 9.2572 (9.1346)  triple_100: 0.0000 (0.1815)  triple_80: 0.0000 (0.2928)  triple_60: 0.0000 (0.7666)  triple_40: 0.0000 (0.2688)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [ 90/845]  eta: 0:04:15  loss: 32.5230 (34.1709)  loss_n_40: 6.6644 (6.6061)  loss_n_60: 8.5910 (8.5734)  loss_n_80: 8.3366 (8.5121)  loss_n_100: 8.9691 (9.1354)  triple_100: 0.0000 (0.1616)  triple_80: 0.0000 (0.2606)  triple_60: 0.0000 (0.6824)  triple_40: 0.0000 (0.2393)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [100/845]  eta: 0:04:11  loss: 32.2729 (33.9941)  loss_n_40: 6.4539 (6.5920)  loss_n_60: 8.4301 (8.5608)  loss_n_80: 8.3601 (8.5038)  loss_n_100: 8.8992 (9.1259)  triple_100: 0.0000 (0.1456)  triple_80: 0.0000 (0.2348)  triple_60: 0.0000 (0.6156)  triple_40: 0.0000 (0.2156)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [110/845]  eta: 0:04:08  loss: 32.3884 (34.6762)  loss_n_40: 6.3574 (6.5964)  loss_n_60: 8.3408 (8.5559)  loss_n_80: 8.3601 (8.5063)  loss_n_100: 8.8640 (9.1293)  triple_100: 0.0000 (0.1912)  triple_80: 0.0000 (0.3209)  triple_60: 0.0000 (0.9407)  triple_40: 0.0000 (0.4356)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [120/845]  eta: 0:04:04  loss: 33.0162 (34.5594)  loss_n_40: 6.7528 (6.6044)  loss_n_60: 8.6333 (8.5617)  loss_n_80: 8.4187 (8.5201)  loss_n_100: 8.9628 (9.1344)  triple_100: 0.0000 (0.1754)  triple_80: 0.0000 (0.2943)  triple_60: 0.0000 (0.8696)  triple_40: 0.0000 (0.3996)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [130/845]  eta: 0:04:00  loss: 33.0162 (34.4781)  loss_n_40: 6.6123 (6.5991)  loss_n_60: 8.5858 (8.5611)  loss_n_80: 8.2395 (8.5267)  loss_n_100: 8.9659 (9.1359)  triple_100: 0.0000 (0.1620)  triple_80: 0.0000 (0.2719)  triple_60: 0.0000 (0.8523)  triple_40: 0.0000 (0.3691)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [140/845]  eta: 0:03:57  loss: 32.4270 (34.4034)  loss_n_40: 6.3747 (6.5953)  loss_n_60: 8.3896 (8.5597)  loss_n_80: 8.2395 (8.5342)  loss_n_100: 8.9659 (9.1352)  triple_100: 0.0000 (0.1723)  triple_80: 0.0000 (0.2718)  triple_60: 0.0000 (0.7919)  triple_40: 0.0000 (0.3429)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [150/845]  eta: 0:03:53  loss: 32.0025 (34.3344)  loss_n_40: 6.3747 (6.5958)  loss_n_60: 8.4057 (8.5601)  loss_n_80: 8.1887 (8.5188)  loss_n_100: 8.9388 (9.1225)  triple_100: 0.0000 (0.1642)  triple_80: 0.0000 (0.2538)  triple_60: 0.0000 (0.7556)  triple_40: 0.0000 (0.3635)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [160/845]  eta: 0:03:50  loss: 32.7140 (34.5117)  loss_n_40: 6.5284 (6.5852)  loss_n_60: 8.4057 (8.5539)  loss_n_80: 8.2308 (8.5231)  loss_n_100: 8.8229 (9.1254)  triple_100: 0.0000 (0.1592)  triple_80: 0.0000 (0.2499)  triple_60: 0.0000 (0.8785)  triple_40: 0.0000 (0.4365)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [170/845]  eta: 0:03:47  loss: 33.3656 (34.5885)  loss_n_40: 6.6528 (6.5937)  loss_n_60: 8.6077 (8.5596)  loss_n_80: 8.6488 (8.5366)  loss_n_100: 9.0834 (9.1369)  triple_100: 0.0000 (0.1499)  triple_80: 0.0000 (0.2426)  triple_60: 0.0000 (0.9379)  triple_40: 0.0000 (0.4312)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [180/845]  eta: 0:03:43  loss: 33.5148 (34.7277)  loss_n_40: 6.6988 (6.6019)  loss_n_60: 8.6093 (8.5635)  loss_n_80: 8.6104 (8.5352)  loss_n_100: 9.0819 (9.1361)  triple_100: 0.0000 (0.1416)  triple_80: 0.0000 (0.2442)  triple_60: 0.0000 (0.9875)  triple_40: 0.0000 (0.5176)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [190/845]  eta: 0:03:40  loss: 33.4871 (34.7095)  loss_n_40: 6.7738 (6.6169)  loss_n_60: 8.6879 (8.5773)  loss_n_80: 8.5860 (8.5504)  loss_n_100: 9.1270 (9.1492)  triple_100: 0.0000 (0.1581)  triple_80: 0.0000 (0.2314)  triple_60: 0.0000 (0.9358)  triple_40: 0.0000 (0.4905)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [200/845]  eta: 0:03:36  loss: 32.6581 (34.5774)  loss_n_40: 6.6465 (6.6045)  loss_n_60: 8.6633 (8.5697)  loss_n_80: 8.4950 (8.5398)  loss_n_100: 9.0892 (9.1379)  triple_100: 0.0000 (0.1502)  triple_80: 0.0000 (0.2199)  triple_60: 0.0000 (0.8892)  triple_40: 0.0000 (0.4661)  time: 0.3346  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:2]  [210/845]  eta: 0:03:33  loss: 32.2870 (34.5899)  loss_n_40: 6.5536 (6.6082)  loss_n_60: 8.5164 (8.5700)  loss_n_80: 8.2156 (8.5336)  loss_n_100: 8.9258 (9.1364)  triple_100: 0.0000 (0.1431)  triple_80: 0.0000 (0.2299)  triple_60: 0.0000 (0.8873)  triple_40: 0.0000 (0.4815)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [220/845]  eta: 0:03:30  loss: 32.4690 (34.6217)  loss_n_40: 6.6311 (6.6039)  loss_n_60: 8.4791 (8.5642)  loss_n_80: 8.2156 (8.5356)  loss_n_100: 8.9779 (9.1402)  triple_100: 0.0000 (0.1366)  triple_80: 0.0000 (0.2350)  triple_60: 0.0000 (0.9083)  triple_40: 0.0000 (0.4978)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [230/845]  eta: 0:03:26  loss: 33.0625 (34.6969)  loss_n_40: 6.6311 (6.6113)  loss_n_60: 8.5071 (8.5689)  loss_n_80: 8.3637 (8.5329)  loss_n_100: 9.0204 (9.1401)  triple_100: 0.0000 (0.1350)  triple_80: 0.0000 (0.2486)  triple_60: 0.0000 (0.9580)  triple_40: 0.0000 (0.5019)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [240/845]  eta: 0:03:23  loss: 33.2329 (34.8462)  loss_n_40: 6.5956 (6.6106)  loss_n_60: 8.5418 (8.5675)  loss_n_80: 8.5102 (8.5410)  loss_n_100: 9.0953 (9.1453)  triple_100: 0.0000 (0.1416)  triple_80: 0.0000 (0.2627)  triple_60: 0.0000 (1.0088)  triple_40: 0.0000 (0.5686)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [250/845]  eta: 0:03:19  loss: 33.9001 (35.0259)  loss_n_40: 6.7445 (6.6215)  loss_n_60: 8.5418 (8.5728)  loss_n_80: 8.7410 (8.5416)  loss_n_100: 9.1575 (9.1472)  triple_100: 0.0000 (0.1680)  triple_80: 0.0000 (0.2742)  triple_60: 0.0000 (1.1059)  triple_40: 0.0000 (0.5946)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [260/845]  eta: 0:03:16  loss: 33.4640 (34.9274)  loss_n_40: 6.6523 (6.6200)  loss_n_60: 8.5116 (8.5721)  loss_n_80: 8.2965 (8.5334)  loss_n_100: 8.9566 (9.1413)  triple_100: 0.0000 (0.1616)  triple_80: 0.0000 (0.2637)  triple_60: 0.0000 (1.0635)  triple_40: 0.0000 (0.5718)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [270/845]  eta: 0:03:13  loss: 32.4554 (34.9035)  loss_n_40: 6.4982 (6.6226)  loss_n_60: 8.5452 (8.5749)  loss_n_80: 8.1773 (8.5393)  loss_n_100: 8.8888 (9.1466)  triple_100: 0.0000 (0.1556)  triple_80: 0.0000 (0.2540)  triple_60: 0.0000 (1.0598)  triple_40: 0.0000 (0.5507)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [280/845]  eta: 0:03:09  loss: 33.7789 (34.8639)  loss_n_40: 6.7583 (6.6306)  loss_n_60: 8.6950 (8.5809)  loss_n_80: 8.3871 (8.5410)  loss_n_100: 8.9715 (9.1473)  triple_100: 0.0000 (0.1526)  triple_80: 0.0000 (0.2449)  triple_60: 0.0000 (1.0355)  triple_40: 0.0000 (0.5311)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [290/845]  eta: 0:03:06  loss: 34.0564 (35.3371)  loss_n_40: 6.8899 (6.6339)  loss_n_60: 8.5603 (8.5799)  loss_n_80: 8.3958 (8.5455)  loss_n_100: 9.0605 (9.1532)  triple_100: 0.0000 (0.1756)  triple_80: 0.0000 (0.3335)  triple_60: 0.0000 (1.2469)  triple_40: 0.0000 (0.6685)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [300/845]  eta: 0:03:02  loss: 34.5262 (35.3684)  loss_n_40: 6.6854 (6.6333)  loss_n_60: 8.5087 (8.5780)  loss_n_80: 8.3958 (8.5440)  loss_n_100: 9.0497 (9.1512)  triple_100: 0.0000 (0.1697)  triple_80: 0.0000 (0.3418)  triple_60: 0.0000 (1.2893)  triple_40: 0.0000 (0.6610)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [310/845]  eta: 0:02:59  loss: 33.9730 (35.3216)  loss_n_40: 6.6514 (6.6370)  loss_n_60: 8.6441 (8.5831)  loss_n_80: 8.7580 (8.5588)  loss_n_100: 9.0497 (9.1600)  triple_100: 0.0000 (0.1643)  triple_80: 0.0000 (0.3308)  triple_60: 0.0000 (1.2478)  triple_40: 0.0000 (0.6397)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [320/845]  eta: 0:02:56  loss: 33.8969 (35.2658)  loss_n_40: 6.6001 (6.6336)  loss_n_60: 8.6441 (8.5813)  loss_n_80: 8.9118 (8.5600)  loss_n_100: 9.1691 (9.1606)  triple_100: 0.0000 (0.1687)  triple_80: 0.0000 (0.3205)  triple_60: 0.0000 (1.2213)  triple_40: 0.0000 (0.6198)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [330/845]  eta: 0:02:52  loss: 32.2470 (35.2189)  loss_n_40: 6.4185 (6.6295)  loss_n_60: 8.4029 (8.5783)  loss_n_80: 8.3042 (8.5505)  loss_n_100: 8.8610 (9.1519)  triple_100: 0.0000 (0.1744)  triple_80: 0.0000 (0.3167)  triple_60: 0.0000 (1.2166)  triple_40: 0.0000 (0.6011)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [340/845]  eta: 0:02:49  loss: 32.4007 (35.1618)  loss_n_40: 6.4637 (6.6262)  loss_n_60: 8.4112 (8.5763)  loss_n_80: 8.3060 (8.5535)  loss_n_100: 8.9247 (9.1549)  triple_100: 0.0000 (0.1791)  triple_80: 0.0000 (0.3075)  triple_60: 0.0000 (1.1809)  triple_40: 0.0000 (0.5834)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [350/845]  eta: 0:02:46  loss: 32.8154 (35.2446)  loss_n_40: 6.5991 (6.6294)  loss_n_60: 8.5469 (8.5781)  loss_n_80: 8.5001 (8.5551)  loss_n_100: 9.0901 (9.1545)  triple_100: 0.0000 (0.1753)  triple_80: 0.0000 (0.3111)  triple_60: 0.0000 (1.2207)  triple_40: 0.0000 (0.6205)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [360/845]  eta: 0:02:42  loss: 33.6377 (35.1984)  loss_n_40: 6.6994 (6.6334)  loss_n_60: 8.6759 (8.5818)  loss_n_80: 8.6937 (8.5606)  loss_n_100: 9.3023 (9.1594)  triple_100: 0.0000 (0.1704)  triple_80: 0.0000 (0.3025)  triple_60: 0.0000 (1.1869)  triple_40: 0.0000 (0.6033)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [370/845]  eta: 0:02:39  loss: 32.8185 (35.1177)  loss_n_40: 6.5719 (6.6277)  loss_n_60: 8.5897 (8.5786)  loss_n_80: 8.4664 (8.5551)  loss_n_100: 8.9397 (9.1542)  triple_100: 0.0000 (0.1658)  triple_80: 0.0000 (0.2943)  triple_60: 0.0000 (1.1549)  triple_40: 0.0000 (0.5870)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [380/845]  eta: 0:02:35  loss: 32.2898 (35.0680)  loss_n_40: 6.5974 (6.6292)  loss_n_60: 8.5897 (8.5806)  loss_n_80: 8.2538 (8.5569)  loss_n_100: 8.9278 (9.1571)  triple_100: 0.0000 (0.1615)  triple_80: 0.0000 (0.2866)  triple_60: 0.0000 (1.1246)  triple_40: 0.0000 (0.5716)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [390/845]  eta: 0:02:32  loss: 32.6873 (35.0206)  loss_n_40: 6.6055 (6.6306)  loss_n_60: 8.6587 (8.5825)  loss_n_80: 8.3535 (8.5558)  loss_n_100: 8.9849 (9.1563)  triple_100: 0.0000 (0.1573)  triple_80: 0.0000 (0.2793)  triple_60: 0.0000 (1.1019)  triple_40: 0.0000 (0.5570)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [400/845]  eta: 0:02:29  loss: 33.1043 (34.9909)  loss_n_40: 6.6205 (6.6333)  loss_n_60: 8.6245 (8.5838)  loss_n_80: 8.4550 (8.5555)  loss_n_100: 8.9542 (9.1575)  triple_100: 0.0000 (0.1534)  triple_80: 0.0000 (0.2723)  triple_60: 0.0000 (1.0920)  triple_40: 0.0000 (0.5431)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [410/845]  eta: 0:02:25  loss: 33.5930 (34.9649)  loss_n_40: 6.7884 (6.6371)  loss_n_60: 8.6447 (8.5878)  loss_n_80: 8.6240 (8.5578)  loss_n_100: 9.0021 (9.1571)  triple_100: 0.0000 (0.1497)  triple_80: 0.0000 (0.2657)  triple_60: 0.0000 (1.0798)  triple_40: 0.0000 (0.5299)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [420/845]  eta: 0:02:22  loss: 33.5930 (34.9515)  loss_n_40: 6.7847 (6.6377)  loss_n_60: 8.6447 (8.5880)  loss_n_80: 8.6240 (8.5581)  loss_n_100: 9.0848 (9.1562)  triple_100: 0.0000 (0.1576)  triple_80: 0.0000 (0.2594)  triple_60: 0.0000 (1.0772)  triple_40: 0.0000 (0.5173)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [430/845]  eta: 0:02:19  loss: 34.3425 (34.9582)  loss_n_40: 6.7847 (6.6444)  loss_n_60: 8.7067 (8.5942)  loss_n_80: 8.7094 (8.5651)  loss_n_100: 9.0848 (9.1606)  triple_100: 0.0000 (0.1539)  triple_80: 0.0000 (0.2533)  triple_60: 0.0000 (1.0570)  triple_40: 0.0000 (0.5297)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [440/845]  eta: 0:02:15  loss: 32.6055 (34.8914)  loss_n_40: 6.5554 (6.6377)  loss_n_60: 8.6061 (8.5883)  loss_n_80: 8.3419 (8.5606)  loss_n_100: 8.9874 (9.1561)  triple_100: 0.0000 (0.1504)  triple_80: 0.0000 (0.2476)  triple_60: 0.0000 (1.0330)  triple_40: 0.0000 (0.5177)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [450/845]  eta: 0:02:12  loss: 32.3687 (35.1020)  loss_n_40: 6.6954 (6.6406)  loss_n_60: 8.5162 (8.5894)  loss_n_80: 8.2944 (8.5583)  loss_n_100: 8.9206 (9.1527)  triple_100: 0.0000 (0.1559)  triple_80: 0.0000 (0.2708)  triple_60: 0.0000 (1.1348)  triple_40: 0.0000 (0.5995)  time: 0.3347  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:2]  [460/845]  eta: 0:02:09  loss: 32.7528 (35.1366)  loss_n_40: 6.6954 (6.6374)  loss_n_60: 8.4564 (8.5853)  loss_n_80: 8.2719 (8.5554)  loss_n_100: 8.9750 (9.1515)  triple_100: 0.0000 (0.1525)  triple_80: 0.0000 (0.2787)  triple_60: 0.0000 (1.1580)  triple_40: 0.0000 (0.6177)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [470/845]  eta: 0:02:05  loss: 32.1562 (35.1842)  loss_n_40: 6.3918 (6.6370)  loss_n_60: 8.3204 (8.5842)  loss_n_80: 8.3373 (8.5538)  loss_n_100: 9.0117 (9.1499)  triple_100: 0.0000 (0.1493)  triple_80: 0.0000 (0.2789)  triple_60: 0.0000 (1.1730)  triple_40: 0.0000 (0.6582)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [480/845]  eta: 0:02:02  loss: 32.9618 (35.1450)  loss_n_40: 6.7614 (6.6397)  loss_n_60: 8.4848 (8.5859)  loss_n_80: 8.5521 (8.5558)  loss_n_100: 9.0647 (9.1512)  triple_100: 0.0000 (0.1462)  triple_80: 0.0000 (0.2731)  triple_60: 0.0000 (1.1486)  triple_40: 0.0000 (0.6445)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [490/845]  eta: 0:01:59  loss: 33.0846 (35.1870)  loss_n_40: 6.7655 (6.6416)  loss_n_60: 8.6390 (8.5872)  loss_n_80: 8.5521 (8.5581)  loss_n_100: 9.0190 (9.1522)  triple_100: 0.0000 (0.1515)  triple_80: 0.0000 (0.2787)  triple_60: 0.0000 (1.1864)  triple_40: 0.0000 (0.6314)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [500/845]  eta: 0:01:55  loss: 33.0497 (35.1452)  loss_n_40: 6.7057 (6.6416)  loss_n_60: 8.6390 (8.5883)  loss_n_80: 8.3736 (8.5582)  loss_n_100: 8.9726 (9.1519)  triple_100: 0.0000 (0.1485)  triple_80: 0.0000 (0.2731)  triple_60: 0.0000 (1.1650)  triple_40: 0.0000 (0.6187)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [510/845]  eta: 0:01:52  loss: 32.8491 (35.1239)  loss_n_40: 6.7651 (6.6456)  loss_n_60: 8.7269 (8.5922)  loss_n_80: 8.4089 (8.5655)  loss_n_100: 9.0873 (9.1585)  triple_100: 0.0000 (0.1456)  triple_80: 0.0000 (0.2677)  triple_60: 0.0000 (1.1422)  triple_40: 0.0000 (0.6066)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [520/845]  eta: 0:01:48  loss: 32.7337 (35.3037)  loss_n_40: 6.7425 (6.6450)  loss_n_60: 8.6533 (8.5896)  loss_n_80: 8.4089 (8.5604)  loss_n_100: 9.0682 (9.1535)  triple_100: 0.0000 (0.1513)  triple_80: 0.0000 (0.2983)  triple_60: 0.0000 (1.2371)  triple_40: 0.0000 (0.6685)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [530/845]  eta: 0:01:45  loss: 32.7337 (35.4225)  loss_n_40: 6.6177 (6.6438)  loss_n_60: 8.4316 (8.5887)  loss_n_80: 8.3511 (8.5614)  loss_n_100: 8.9587 (9.1542)  triple_100: 0.0000 (0.1676)  triple_80: 0.0000 (0.3380)  triple_60: 0.0000 (1.2721)  triple_40: 0.0000 (0.6967)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [540/845]  eta: 0:01:42  loss: 33.2771 (35.4027)  loss_n_40: 6.7685 (6.6436)  loss_n_60: 8.4864 (8.5880)  loss_n_80: 8.4530 (8.5616)  loss_n_100: 9.0151 (9.1543)  triple_100: 0.0000 (0.1801)  triple_80: 0.0000 (0.3402)  triple_60: 0.0000 (1.2510)  triple_40: 0.0000 (0.6838)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [550/845]  eta: 0:01:38  loss: 33.4656 (35.3832)  loss_n_40: 6.8056 (6.6466)  loss_n_60: 8.7014 (8.5907)  loss_n_80: 8.6169 (8.5662)  loss_n_100: 9.2554 (9.1586)  triple_100: 0.0000 (0.1769)  triple_80: 0.0000 (0.3340)  triple_60: 0.0000 (1.2387)  triple_40: 0.0000 (0.6714)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [560/845]  eta: 0:01:35  loss: 32.9712 (35.3655)  loss_n_40: 6.7169 (6.6436)  loss_n_60: 8.7001 (8.5892)  loss_n_80: 8.4709 (8.5635)  loss_n_100: 8.9480 (9.1554)  triple_100: 0.0000 (0.1737)  triple_80: 0.0000 (0.3281)  triple_60: 0.0000 (1.2383)  triple_40: 0.0000 (0.6737)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [570/845]  eta: 0:01:32  loss: 32.4475 (35.3278)  loss_n_40: 6.4278 (6.6446)  loss_n_60: 8.5382 (8.5902)  loss_n_80: 8.3144 (8.5631)  loss_n_100: 8.8710 (9.1556)  triple_100: 0.0000 (0.1707)  triple_80: 0.0000 (0.3223)  triple_60: 0.0000 (1.2193)  triple_40: 0.0000 (0.6619)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [580/845]  eta: 0:01:28  loss: 32.8642 (35.2995)  loss_n_40: 6.7123 (6.6451)  loss_n_60: 8.6596 (8.5909)  loss_n_80: 8.2766 (8.5594)  loss_n_100: 8.8710 (9.1515)  triple_100: 0.0000 (0.1710)  triple_80: 0.0000 (0.3202)  triple_60: 0.0000 (1.2109)  triple_40: 0.0000 (0.6505)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [590/845]  eta: 0:01:25  loss: 32.2147 (35.3275)  loss_n_40: 6.5377 (6.6415)  loss_n_60: 8.4620 (8.5876)  loss_n_80: 8.2524 (8.5547)  loss_n_100: 8.9486 (9.1480)  triple_100: 0.0000 (0.1711)  triple_80: 0.0000 (0.3189)  triple_60: 0.0000 (1.2424)  triple_40: 0.0000 (0.6635)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [600/845]  eta: 0:01:22  loss: 32.9104 (35.3021)  loss_n_40: 6.5981 (6.6456)  loss_n_60: 8.5853 (8.5899)  loss_n_80: 8.3787 (8.5585)  loss_n_100: 9.0212 (9.1522)  triple_100: 0.0000 (0.1683)  triple_80: 0.0000 (0.3136)  triple_60: 0.0000 (1.2217)  triple_40: 0.0000 (0.6524)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [610/845]  eta: 0:01:18  loss: 33.7080 (35.2719)  loss_n_40: 6.7117 (6.6463)  loss_n_60: 8.6598 (8.5901)  loss_n_80: 8.7747 (8.5617)  loss_n_100: 9.3224 (9.1551)  triple_100: 0.0000 (0.1667)  triple_80: 0.0000 (0.3084)  triple_60: 0.0000 (1.2017)  triple_40: 0.0000 (0.6418)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [620/845]  eta: 0:01:15  loss: 33.1181 (35.2320)  loss_n_40: 6.6894 (6.6469)  loss_n_60: 8.5662 (8.5906)  loss_n_80: 8.5992 (8.5596)  loss_n_100: 9.2364 (9.1536)  triple_100: 0.0000 (0.1640)  triple_80: 0.0000 (0.3035)  triple_60: 0.0000 (1.1823)  triple_40: 0.0000 (0.6314)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [630/845]  eta: 0:01:12  loss: 32.9962 (35.3966)  loss_n_40: 6.8137 (6.6502)  loss_n_60: 8.5662 (8.5914)  loss_n_80: 8.3304 (8.5555)  loss_n_100: 8.9977 (9.1508)  triple_100: 0.0000 (0.1815)  triple_80: 0.0000 (0.3308)  triple_60: 0.0000 (1.2683)  triple_40: 0.0000 (0.6680)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [640/845]  eta: 0:01:08  loss: 33.2426 (35.3848)  loss_n_40: 6.6132 (6.6514)  loss_n_60: 8.5321 (8.5925)  loss_n_80: 8.4729 (8.5588)  loss_n_100: 9.1336 (9.1537)  triple_100: 0.0000 (0.1787)  triple_80: 0.0000 (0.3256)  triple_60: 0.0000 (1.2665)  triple_40: 0.0000 (0.6576)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [650/845]  eta: 0:01:05  loss: 32.6805 (35.3477)  loss_n_40: 6.5997 (6.6510)  loss_n_60: 8.5321 (8.5927)  loss_n_80: 8.4689 (8.5588)  loss_n_100: 9.0310 (9.1541)  triple_100: 0.0000 (0.1760)  triple_80: 0.0000 (0.3206)  triple_60: 0.0000 (1.2470)  triple_40: 0.0000 (0.6475)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [660/845]  eta: 0:01:01  loss: 32.6232 (35.3181)  loss_n_40: 6.6548 (6.6530)  loss_n_60: 8.6965 (8.5950)  loss_n_80: 8.4689 (8.5596)  loss_n_100: 9.0158 (9.1541)  triple_100: 0.0000 (0.1733)  triple_80: 0.0000 (0.3158)  triple_60: 0.0000 (1.2297)  triple_40: 0.0000 (0.6377)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [670/845]  eta: 0:00:58  loss: 32.7460 (35.2815)  loss_n_40: 6.6258 (6.6519)  loss_n_60: 8.6679 (8.5946)  loss_n_80: 8.3989 (8.5593)  loss_n_100: 9.0158 (9.1545)  triple_100: 0.0000 (0.1707)  triple_80: 0.0000 (0.3111)  triple_60: 0.0000 (1.2113)  triple_40: 0.0000 (0.6282)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [680/845]  eta: 0:00:55  loss: 33.1168 (35.2575)  loss_n_40: 6.5789 (6.6535)  loss_n_60: 8.5688 (8.5957)  loss_n_80: 8.3689 (8.5614)  loss_n_100: 9.0997 (9.1570)  triple_100: 0.0000 (0.1685)  triple_80: 0.0000 (0.3065)  triple_60: 0.0000 (1.1960)  triple_40: 0.0000 (0.6190)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [690/845]  eta: 0:00:51  loss: 33.1168 (35.2221)  loss_n_40: 6.6210 (6.6514)  loss_n_60: 8.5101 (8.5940)  loss_n_80: 8.4866 (8.5623)  loss_n_100: 9.1852 (9.1577)  triple_100: 0.0000 (0.1660)  triple_80: 0.0000 (0.3021)  triple_60: 0.0000 (1.1787)  triple_40: 0.0000 (0.6100)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [700/845]  eta: 0:00:48  loss: 32.1648 (35.2989)  loss_n_40: 6.5086 (6.6495)  loss_n_60: 8.4943 (8.5920)  loss_n_80: 8.3753 (8.5634)  loss_n_100: 9.0415 (9.1588)  triple_100: 0.0000 (0.1735)  triple_80: 0.0000 (0.3236)  triple_60: 0.0000 (1.2077)  triple_40: 0.0000 (0.6304)  time: 0.3345  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:2]  [710/845]  eta: 0:00:45  loss: 32.3552 (35.2978)  loss_n_40: 6.5374 (6.6501)  loss_n_60: 8.4789 (8.5920)  loss_n_80: 8.3298 (8.5624)  loss_n_100: 9.0246 (9.1565)  triple_100: 0.0000 (0.1750)  triple_80: 0.0000 (0.3277)  triple_60: 0.0000 (1.2078)  triple_40: 0.0000 (0.6262)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [720/845]  eta: 0:00:41  loss: 34.0691 (35.4031)  loss_n_40: 6.7266 (6.6530)  loss_n_60: 8.5991 (8.5938)  loss_n_80: 8.3889 (8.5622)  loss_n_100: 8.9772 (9.1553)  triple_100: 0.0000 (0.1757)  triple_80: 0.0000 (0.3348)  triple_60: 0.0000 (1.2502)  triple_40: 0.0000 (0.6782)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [730/845]  eta: 0:00:38  loss: 33.8335 (35.4319)  loss_n_40: 6.8387 (6.6532)  loss_n_60: 8.6729 (8.5934)  loss_n_80: 8.3889 (8.5615)  loss_n_100: 8.9841 (9.1542)  triple_100: 0.0000 (0.1769)  triple_80: 0.0000 (0.3340)  triple_60: 0.0000 (1.2634)  triple_40: 0.0000 (0.6952)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [740/845]  eta: 0:00:35  loss: 33.1029 (35.4495)  loss_n_40: 6.5332 (6.6535)  loss_n_60: 8.4189 (8.5930)  loss_n_80: 8.2318 (8.5587)  loss_n_100: 8.9410 (9.1512)  triple_100: 0.0000 (0.1772)  triple_80: 0.0000 (0.3390)  triple_60: 0.0000 (1.2873)  triple_40: 0.0000 (0.6896)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [750/845]  eta: 0:00:31  loss: 32.7957 (35.4858)  loss_n_40: 6.5785 (6.6544)  loss_n_60: 8.4370 (8.5930)  loss_n_80: 8.1850 (8.5591)  loss_n_100: 8.8362 (9.1512)  triple_100: 0.0000 (0.1773)  triple_80: 0.0000 (0.3402)  triple_60: 0.0000 (1.3143)  triple_40: 0.0000 (0.6963)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [760/845]  eta: 0:00:28  loss: 32.8622 (35.4971)  loss_n_40: 6.8108 (6.6551)  loss_n_60: 8.5304 (8.5935)  loss_n_80: 8.3316 (8.5594)  loss_n_100: 9.0174 (9.1511)  triple_100: 0.0000 (0.1793)  triple_80: 0.0000 (0.3459)  triple_60: 0.0000 (1.3217)  triple_40: 0.0000 (0.6913)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [770/845]  eta: 0:00:25  loss: 33.9400 (35.4768)  loss_n_40: 6.8179 (6.6572)  loss_n_60: 8.6689 (8.5950)  loss_n_80: 8.7836 (8.5641)  loss_n_100: 9.1746 (9.1553)  triple_100: 0.0000 (0.1769)  triple_80: 0.0000 (0.3414)  triple_60: 0.0000 (1.3045)  triple_40: 0.0000 (0.6823)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [780/845]  eta: 0:00:21  loss: 33.3581 (35.4441)  loss_n_40: 6.6882 (6.6568)  loss_n_60: 8.6156 (8.5943)  loss_n_80: 8.5884 (8.5636)  loss_n_100: 9.1461 (9.1553)  triple_100: 0.0000 (0.1747)  triple_80: 0.0000 (0.3370)  triple_60: 0.0000 (1.2888)  triple_40: 0.0000 (0.6736)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [790/845]  eta: 0:00:18  loss: 32.6581 (35.4168)  loss_n_40: 6.6193 (6.6567)  loss_n_60: 8.5976 (8.5944)  loss_n_80: 8.3905 (8.5652)  loss_n_100: 9.1010 (9.1570)  triple_100: 0.0000 (0.1725)  triple_80: 0.0000 (0.3327)  triple_60: 0.0000 (1.2731)  triple_40: 0.0000 (0.6651)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [800/845]  eta: 0:00:15  loss: 33.5649 (35.4140)  loss_n_40: 6.6842 (6.6584)  loss_n_60: 8.6580 (8.5961)  loss_n_80: 8.3515 (8.5653)  loss_n_100: 9.0623 (9.1571)  triple_100: 0.0000 (0.1706)  triple_80: 0.0000 (0.3328)  triple_60: 0.0000 (1.2768)  triple_40: 0.0000 (0.6568)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [810/845]  eta: 0:00:11  loss: 33.4297 (35.4239)  loss_n_40: 6.6387 (6.6573)  loss_n_60: 8.5672 (8.5956)  loss_n_80: 8.3515 (8.5629)  loss_n_100: 9.0111 (9.1552)  triple_100: 0.0000 (0.1857)  triple_80: 0.0000 (0.3374)  triple_60: 0.0000 (1.2790)  triple_40: 0.0000 (0.6508)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [820/845]  eta: 0:00:08  loss: 32.9359 (35.4031)  loss_n_40: 6.6146 (6.6579)  loss_n_60: 8.5673 (8.5968)  loss_n_80: 8.4750 (8.5643)  loss_n_100: 9.0172 (9.1552)  triple_100: 0.0000 (0.1834)  triple_80: 0.0000 (0.3333)  triple_60: 0.0000 (1.2692)  triple_40: 0.0000 (0.6429)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [830/845]  eta: 0:00:05  loss: 33.5469 (35.5811)  loss_n_40: 6.8182 (6.6621)  loss_n_60: 8.6710 (8.5983)  loss_n_80: 8.5098 (8.5635)  loss_n_100: 9.1035 (9.1554)  triple_100: 0.0000 (0.1938)  triple_80: 0.0000 (0.3606)  triple_60: 0.0000 (1.3562)  triple_40: 0.0000 (0.6910)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [840/845]  eta: 0:00:01  loss: 33.7957 (35.5580)  loss_n_40: 6.8405 (6.6627)  loss_n_60: 8.6710 (8.5988)  loss_n_80: 8.5579 (8.5664)  loss_n_100: 9.2447 (9.1581)  triple_100: 0.0000 (0.1915)  triple_80: 0.0000 (0.3564)  triple_60: 0.0000 (1.3413)  triple_40: 0.0000 (0.6828)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2]  [844/845]  eta: 0:00:00  loss: 33.1089 (35.6232)  loss_n_40: 6.6643 (6.6628)  loss_n_60: 8.5822 (8.5983)  loss_n_80: 8.5177 (8.5658)  loss_n_100: 9.1266 (9.1576)  triple_100: 0.0000 (0.1928)  triple_80: 0.0000 (0.3680)  triple_60: 0.0000 (1.3674)  triple_40: 0.0000 (0.7105)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:2] Total time: 0:04:43 (0.3351 s / it)\n",
      "Averaged stats: loss: 33.1089 (35.6232)  loss_n_40: 6.6643 (6.6628)  loss_n_60: 8.5822 (8.5983)  loss_n_80: 8.5177 (8.5658)  loss_n_100: 9.1266 (9.1576)  triple_100: 0.0000 (0.1928)  triple_80: 0.0000 (0.3680)  triple_60: 0.0000 (1.3674)  triple_40: 0.0000 (0.7105)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_2_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 9.158%\n",
      "Min loss_n_100: 4.783\n",
      "Best Epoch: 0.000\n",
      "Train: [epoch:3]  [   0/1724]  eta: 2:00:31  lr: 0.000040  loss: 32.0957 (32.0957)  loss_n_40: 6.4661 (6.4661)  loss_n_60: 8.4076 (8.4076)  loss_n_80: 8.2929 (8.2929)  loss_n_100: 8.9292 (8.9292)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1944  data: 0.4327  max mem: 46473\n",
      "Train: [epoch:3]  [  10/1724]  eta: 1:52:50  lr: 0.000040  loss: 33.1908 (37.4968)  loss_n_40: 6.5823 (6.6605)  loss_n_60: 8.5764 (8.6106)  loss_n_80: 8.4164 (8.4558)  loss_n_100: 9.2029 (9.1532)  triple_100: 0.1077 (0.5891)  triple_80: 0.0000 (0.4785)  triple_60: 0.0000 (2.1904)  triple_40: 0.0000 (1.3587)  time: 3.9500  data: 0.0395  max mem: 46473\n",
      "Train: [epoch:3]  [  20/1724]  eta: 1:51:51  lr: 0.000040  loss: 33.3456 (37.0188)  loss_n_40: 6.6523 (6.6671)  loss_n_60: 8.6304 (8.6221)  loss_n_80: 8.4421 (8.4835)  loss_n_100: 9.2029 (9.1685)  triple_100: 0.1636 (0.6674)  triple_80: 0.0000 (0.3821)  triple_60: 0.0000 (1.9081)  triple_40: 0.0000 (1.1200)  time: 3.9260  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:3]  [  30/1724]  eta: 1:51:05  lr: 0.000040  loss: 34.2136 (38.1534)  loss_n_40: 6.6870 (6.6781)  loss_n_60: 8.6375 (8.6159)  loss_n_80: 8.4798 (8.4789)  loss_n_100: 9.0864 (9.1277)  triple_100: 0.2142 (0.8372)  triple_80: 0.0000 (0.8956)  triple_60: 0.7561 (2.3367)  triple_40: 0.0000 (1.1831)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [  40/1724]  eta: 1:50:22  lr: 0.000040  loss: 33.4781 (37.3397)  loss_n_40: 6.6365 (6.6629)  loss_n_60: 8.6375 (8.6060)  loss_n_80: 8.4798 (8.5061)  loss_n_100: 9.0750 (9.1378)  triple_100: 0.0000 (0.6932)  triple_80: 0.0000 (0.7900)  triple_60: 0.2115 (1.9984)  triple_40: 0.0000 (0.9453)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [  50/1724]  eta: 1:49:40  lr: 0.000040  loss: 33.3569 (36.8885)  loss_n_40: 6.6365 (6.6576)  loss_n_60: 8.6483 (8.6112)  loss_n_80: 8.4821 (8.5302)  loss_n_100: 9.2092 (9.1685)  triple_100: 0.0000 (0.5712)  triple_80: 0.0000 (0.6371)  triple_60: 0.0000 (1.8126)  triple_40: 0.0000 (0.9001)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [  60/1724]  eta: 1:48:59  lr: 0.000040  loss: 33.6875 (37.0760)  loss_n_40: 6.6626 (6.6650)  loss_n_60: 8.6592 (8.6251)  loss_n_80: 8.5482 (8.5392)  loss_n_100: 9.3552 (9.2044)  triple_100: 0.0000 (0.6265)  triple_80: 0.0000 (0.6038)  triple_60: 0.0000 (1.7678)  triple_40: 0.0000 (1.0442)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [  70/1724]  eta: 1:48:19  lr: 0.000040  loss: 33.5567 (36.8205)  loss_n_40: 6.6436 (6.6618)  loss_n_60: 8.6419 (8.6216)  loss_n_80: 8.5489 (8.5177)  loss_n_100: 9.1997 (9.1837)  triple_100: 0.0000 (0.5487)  triple_80: 0.0000 (0.5580)  triple_60: 0.0000 (1.7319)  triple_40: 0.0000 (0.9971)  time: 3.9259  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [  80/1724]  eta: 1:47:39  lr: 0.000040  loss: 33.2210 (36.3908)  loss_n_40: 6.7203 (6.6655)  loss_n_60: 8.6785 (8.6249)  loss_n_80: 8.3387 (8.4948)  loss_n_100: 8.9929 (9.1610)  triple_100: 0.0000 (0.4881)  triple_80: 0.0000 (0.5023)  triple_60: 0.0000 (1.5749)  triple_40: 0.0000 (0.8793)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [  90/1724]  eta: 1:46:59  lr: 0.000040  loss: 32.9521 (36.1182)  loss_n_40: 6.7852 (6.6704)  loss_n_60: 8.7146 (8.6277)  loss_n_80: 8.3608 (8.4823)  loss_n_100: 8.9866 (9.1462)  triple_100: 0.0000 (0.4344)  triple_80: 0.0000 (0.4512)  triple_60: 0.0000 (1.4642)  triple_40: 0.0000 (0.8418)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 100/1724]  eta: 1:46:18  lr: 0.000040  loss: 33.3669 (36.0507)  loss_n_40: 6.7832 (6.6822)  loss_n_60: 8.7029 (8.6371)  loss_n_80: 8.3758 (8.4773)  loss_n_100: 8.9901 (9.1364)  triple_100: 0.0000 (0.4411)  triple_80: 0.0000 (0.4332)  triple_60: 0.0000 (1.4379)  triple_40: 0.0000 (0.8056)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 110/1724]  eta: 1:45:39  lr: 0.000040  loss: 33.5817 (36.0740)  loss_n_40: 6.6719 (6.6800)  loss_n_60: 8.6257 (8.6317)  loss_n_80: 8.5376 (8.4876)  loss_n_100: 9.0943 (9.1387)  triple_100: 0.0000 (0.4529)  triple_80: 0.0000 (0.4381)  triple_60: 0.0000 (1.4573)  triple_40: 0.0000 (0.7875)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 120/1724]  eta: 1:44:59  lr: 0.000040  loss: 33.7624 (36.1068)  loss_n_40: 6.6625 (6.6831)  loss_n_60: 8.5781 (8.6355)  loss_n_80: 8.5376 (8.4845)  loss_n_100: 9.0943 (9.1342)  triple_100: 0.2112 (0.4867)  triple_80: 0.0000 (0.4622)  triple_60: 0.0000 (1.4407)  triple_40: 0.0000 (0.7799)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 130/1724]  eta: 1:44:20  lr: 0.000040  loss: 33.1118 (36.1411)  loss_n_40: 6.5892 (6.6750)  loss_n_60: 8.5840 (8.6324)  loss_n_80: 8.4743 (8.4886)  loss_n_100: 9.0670 (9.1376)  triple_100: 0.0000 (0.5103)  triple_80: 0.0000 (0.4690)  triple_60: 0.0000 (1.4066)  triple_40: 0.0000 (0.8218)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 140/1724]  eta: 1:43:40  lr: 0.000040  loss: 32.7984 (36.0490)  loss_n_40: 6.5824 (6.6748)  loss_n_60: 8.5484 (8.6320)  loss_n_80: 8.4398 (8.4837)  loss_n_100: 9.0642 (9.1353)  triple_100: 0.0000 (0.4892)  triple_80: 0.0000 (0.4865)  triple_60: 0.0000 (1.3610)  triple_40: 0.0000 (0.7863)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 150/1724]  eta: 1:43:01  lr: 0.000040  loss: 33.3744 (36.1110)  loss_n_40: 6.6322 (6.6818)  loss_n_60: 8.6312 (8.6350)  loss_n_80: 8.4000 (8.4814)  loss_n_100: 9.0475 (9.1311)  triple_100: 0.0000 (0.4953)  triple_80: 0.0000 (0.4987)  triple_60: 0.0000 (1.3963)  triple_40: 0.0000 (0.7914)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 160/1724]  eta: 1:42:21  lr: 0.000040  loss: 33.0741 (35.9203)  loss_n_40: 6.6322 (6.6779)  loss_n_60: 8.6275 (8.6323)  loss_n_80: 8.4050 (8.4793)  loss_n_100: 9.0102 (9.1250)  triple_100: 0.0000 (0.4664)  triple_80: 0.0000 (0.4677)  triple_60: 0.0000 (1.3166)  triple_40: 0.0000 (0.7551)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 170/1724]  eta: 1:41:42  lr: 0.000040  loss: 32.9807 (35.7864)  loss_n_40: 6.6291 (6.6761)  loss_n_60: 8.5767 (8.6305)  loss_n_80: 8.3766 (8.4738)  loss_n_100: 8.9690 (9.1165)  triple_100: 0.0000 (0.4575)  triple_80: 0.0000 (0.4482)  triple_60: 0.0000 (1.2728)  triple_40: 0.0000 (0.7110)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 180/1724]  eta: 1:41:02  lr: 0.000040  loss: 32.7059 (35.6828)  loss_n_40: 6.6198 (6.6672)  loss_n_60: 8.5327 (8.6209)  loss_n_80: 8.3766 (8.4629)  loss_n_100: 8.9407 (9.1052)  triple_100: 0.0000 (0.4386)  triple_80: 0.0000 (0.4307)  triple_60: 0.0000 (1.2465)  triple_40: 0.0000 (0.7108)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 190/1724]  eta: 1:40:23  lr: 0.000040  loss: 32.7524 (35.7185)  loss_n_40: 6.6335 (6.6730)  loss_n_60: 8.5611 (8.6250)  loss_n_80: 8.3684 (8.4608)  loss_n_100: 8.9690 (9.1027)  triple_100: 0.0000 (0.4251)  triple_80: 0.0000 (0.4487)  triple_60: 0.0000 (1.2727)  triple_40: 0.0000 (0.7106)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 200/1724]  eta: 1:39:43  lr: 0.000040  loss: 33.2299 (35.6440)  loss_n_40: 6.6688 (6.6702)  loss_n_60: 8.6010 (8.6190)  loss_n_80: 8.4180 (8.4556)  loss_n_100: 8.9562 (9.0934)  triple_100: 0.0000 (0.4190)  triple_80: 0.0000 (0.4399)  triple_60: 0.0000 (1.2593)  triple_40: 0.0000 (0.6875)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 210/1724]  eta: 1:39:04  lr: 0.000040  loss: 32.5449 (35.5100)  loss_n_40: 6.6738 (6.6713)  loss_n_60: 8.5593 (8.6163)  loss_n_80: 8.4180 (8.4566)  loss_n_100: 8.8441 (9.0878)  triple_100: 0.0000 (0.4008)  triple_80: 0.0000 (0.4190)  triple_60: 0.0000 (1.2026)  triple_40: 0.0000 (0.6557)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 220/1724]  eta: 1:38:25  lr: 0.000040  loss: 32.6183 (35.3939)  loss_n_40: 6.6781 (6.6670)  loss_n_60: 8.5361 (8.6076)  loss_n_80: 8.3484 (8.4486)  loss_n_100: 8.8441 (9.0783)  triple_100: 0.0000 (0.3875)  triple_80: 0.0000 (0.4018)  triple_60: 0.0000 (1.1678)  triple_40: 0.0000 (0.6352)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 230/1724]  eta: 1:37:45  lr: 0.000040  loss: 32.1548 (35.4028)  loss_n_40: 6.5485 (6.6642)  loss_n_60: 8.4123 (8.6025)  loss_n_80: 8.2509 (8.4396)  loss_n_100: 8.8670 (9.0686)  triple_100: 0.0000 (0.4027)  triple_80: 0.0000 (0.4171)  triple_60: 0.0000 (1.1491)  triple_40: 0.0000 (0.6590)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 240/1724]  eta: 1:37:06  lr: 0.000040  loss: 32.1915 (35.3361)  loss_n_40: 6.5808 (6.6640)  loss_n_60: 8.4453 (8.5984)  loss_n_80: 8.2564 (8.4389)  loss_n_100: 8.8670 (9.0647)  triple_100: 0.0000 (0.3862)  triple_80: 0.0000 (0.4090)  triple_60: 0.0000 (1.1293)  triple_40: 0.0000 (0.6456)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 250/1724]  eta: 1:36:27  lr: 0.000040  loss: 32.1915 (35.2201)  loss_n_40: 6.6225 (6.6610)  loss_n_60: 8.4002 (8.5914)  loss_n_80: 8.2227 (8.4290)  loss_n_100: 8.8249 (9.0546)  triple_100: 0.0000 (0.3729)  triple_80: 0.0000 (0.4006)  triple_60: 0.0000 (1.0907)  triple_40: 0.0000 (0.6199)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 260/1724]  eta: 1:35:47  lr: 0.000040  loss: 32.1036 (35.1666)  loss_n_40: 6.5082 (6.6584)  loss_n_60: 8.3777 (8.5856)  loss_n_80: 8.1897 (8.4245)  loss_n_100: 8.7870 (9.0465)  triple_100: 0.0000 (0.3598)  triple_80: 0.0000 (0.3852)  triple_60: 0.0000 (1.0891)  triple_40: 0.0000 (0.6174)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 270/1724]  eta: 1:35:08  lr: 0.000040  loss: 32.4605 (35.1390)  loss_n_40: 6.5623 (6.6589)  loss_n_60: 8.4211 (8.5830)  loss_n_80: 8.3537 (8.4187)  loss_n_100: 8.8433 (9.0382)  triple_100: 0.0000 (0.3576)  triple_80: 0.0000 (0.3850)  triple_60: 0.0000 (1.0938)  triple_40: 0.0000 (0.6039)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 280/1724]  eta: 1:34:29  lr: 0.000040  loss: 32.1255 (35.0646)  loss_n_40: 6.5623 (6.6564)  loss_n_60: 8.4678 (8.5779)  loss_n_80: 8.2866 (8.4147)  loss_n_100: 8.8407 (9.0327)  triple_100: 0.0000 (0.3452)  triple_80: 0.0000 (0.3714)  triple_60: 0.0000 (1.0757)  triple_40: 0.0000 (0.5906)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 290/1724]  eta: 1:33:49  lr: 0.000040  loss: 32.2395 (35.0124)  loss_n_40: 6.5357 (6.6551)  loss_n_60: 8.4089 (8.5742)  loss_n_80: 8.2866 (8.4107)  loss_n_100: 8.8407 (9.0273)  triple_100: 0.0000 (0.3370)  triple_80: 0.0000 (0.3612)  triple_60: 0.0000 (1.0744)  triple_40: 0.0000 (0.5725)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 300/1724]  eta: 1:33:10  lr: 0.000040  loss: 32.1399 (34.9108)  loss_n_40: 6.5082 (6.6505)  loss_n_60: 8.4089 (8.5685)  loss_n_80: 8.1889 (8.4008)  loss_n_100: 8.8339 (9.0171)  triple_100: 0.0000 (0.3287)  triple_80: 0.0000 (0.3492)  triple_60: 0.0000 (1.0426)  triple_40: 0.0000 (0.5535)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 310/1724]  eta: 1:32:31  lr: 0.000040  loss: 32.1399 (34.8533)  loss_n_40: 6.6274 (6.6522)  loss_n_60: 8.4886 (8.5679)  loss_n_80: 8.1230 (8.3944)  loss_n_100: 8.7594 (9.0098)  triple_100: 0.0000 (0.3211)  triple_80: 0.0000 (0.3379)  triple_60: 0.0000 (1.0262)  triple_40: 0.0000 (0.5438)  time: 3.9254  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [ 320/1724]  eta: 1:31:51  lr: 0.000040  loss: 32.6391 (34.8549)  loss_n_40: 6.6710 (6.6530)  loss_n_60: 8.4920 (8.5655)  loss_n_80: 8.3101 (8.3910)  loss_n_100: 8.8899 (9.0041)  triple_100: 0.0000 (0.3175)  triple_80: 0.0000 (0.3476)  triple_60: 0.0000 (1.0302)  triple_40: 0.0000 (0.5459)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 330/1724]  eta: 1:31:12  lr: 0.000040  loss: 32.2400 (34.7705)  loss_n_40: 6.5435 (6.6477)  loss_n_60: 8.3485 (8.5573)  loss_n_80: 8.3101 (8.3824)  loss_n_100: 8.8707 (8.9942)  triple_100: 0.0000 (0.3195)  triple_80: 0.0000 (0.3398)  triple_60: 0.0000 (1.0002)  triple_40: 0.0000 (0.5294)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 340/1724]  eta: 1:30:33  lr: 0.000040  loss: 31.4682 (34.8295)  loss_n_40: 6.4576 (6.6447)  loss_n_60: 8.2403 (8.5501)  loss_n_80: 8.0779 (8.3751)  loss_n_100: 8.6026 (8.9836)  triple_100: 0.0000 (0.3385)  triple_80: 0.0000 (0.3645)  triple_60: 0.0000 (1.0090)  triple_40: 0.0000 (0.5639)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 350/1724]  eta: 1:29:54  lr: 0.000040  loss: 32.0182 (34.8592)  loss_n_40: 6.5501 (6.6452)  loss_n_60: 8.3195 (8.5469)  loss_n_80: 8.2382 (8.3733)  loss_n_100: 8.7133 (8.9778)  triple_100: 0.0000 (0.3589)  triple_80: 0.0000 (0.3865)  triple_60: 0.0000 (1.0094)  triple_40: 0.0000 (0.5613)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 360/1724]  eta: 1:29:14  lr: 0.000040  loss: 32.2307 (34.7820)  loss_n_40: 6.6783 (6.6450)  loss_n_60: 8.3921 (8.5427)  loss_n_80: 8.1907 (8.3651)  loss_n_100: 8.7133 (8.9678)  triple_100: 0.0000 (0.3544)  triple_80: 0.0000 (0.3778)  triple_60: 0.0000 (0.9835)  triple_40: 0.0000 (0.5457)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 370/1724]  eta: 1:28:35  lr: 0.000040  loss: 31.8883 (34.7166)  loss_n_40: 6.6359 (6.6436)  loss_n_60: 8.3921 (8.5381)  loss_n_80: 8.0819 (8.3587)  loss_n_100: 8.6306 (8.9604)  triple_100: 0.0000 (0.3448)  triple_80: 0.0000 (0.3686)  triple_60: 0.0000 (0.9667)  triple_40: 0.0000 (0.5357)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 380/1724]  eta: 1:27:56  lr: 0.000040  loss: 31.8883 (34.6519)  loss_n_40: 6.6370 (6.6427)  loss_n_60: 8.3594 (8.5323)  loss_n_80: 8.0902 (8.3502)  loss_n_100: 8.5899 (8.9509)  triple_100: 0.0000 (0.3386)  triple_80: 0.0000 (0.3618)  triple_60: 0.0000 (0.9538)  triple_40: 0.0000 (0.5216)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 390/1724]  eta: 1:27:16  lr: 0.000040  loss: 31.5160 (34.6439)  loss_n_40: 6.5632 (6.6379)  loss_n_60: 8.2589 (8.5243)  loss_n_80: 7.9712 (8.3400)  loss_n_100: 8.5136 (8.9390)  triple_100: 0.0000 (0.3437)  triple_80: 0.0000 (0.3757)  triple_60: 0.0000 (0.9589)  triple_40: 0.0000 (0.5244)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 400/1724]  eta: 1:26:37  lr: 0.000040  loss: 31.8204 (34.5977)  loss_n_40: 6.5087 (6.6384)  loss_n_60: 8.2566 (8.5206)  loss_n_80: 7.9810 (8.3378)  loss_n_100: 8.5627 (8.9345)  triple_100: 0.0000 (0.3372)  triple_80: 0.0000 (0.3678)  triple_60: 0.0000 (0.9445)  triple_40: 0.0000 (0.5169)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 410/1724]  eta: 1:25:58  lr: 0.000040  loss: 31.7353 (34.5263)  loss_n_40: 6.5100 (6.6348)  loss_n_60: 8.2618 (8.5140)  loss_n_80: 8.0808 (8.3313)  loss_n_100: 8.5901 (8.9260)  triple_100: 0.0000 (0.3299)  triple_80: 0.0000 (0.3589)  triple_60: 0.0000 (0.9267)  triple_40: 0.0000 (0.5049)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 420/1724]  eta: 1:25:19  lr: 0.000040  loss: 31.7353 (34.4908)  loss_n_40: 6.5100 (6.6358)  loss_n_60: 8.2618 (8.5114)  loss_n_80: 8.0243 (8.3251)  loss_n_100: 8.5667 (8.9181)  triple_100: 0.0000 (0.3301)  triple_80: 0.0000 (0.3519)  triple_60: 0.0000 (0.9221)  triple_40: 0.0000 (0.4964)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 430/1724]  eta: 1:24:39  lr: 0.000040  loss: 31.9826 (34.4687)  loss_n_40: 6.6694 (6.6361)  loss_n_60: 8.4420 (8.5087)  loss_n_80: 8.0437 (8.3190)  loss_n_100: 8.5669 (8.9105)  triple_100: 0.0000 (0.3236)  triple_80: 0.0000 (0.3501)  triple_60: 0.0000 (0.9265)  triple_40: 0.0000 (0.4941)  time: 3.9236  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 440/1724]  eta: 1:24:00  lr: 0.000040  loss: 31.6998 (34.4033)  loss_n_40: 6.6044 (6.6336)  loss_n_60: 8.3633 (8.5033)  loss_n_80: 8.0846 (8.3129)  loss_n_100: 8.6158 (8.9032)  triple_100: 0.0000 (0.3174)  triple_80: 0.0000 (0.3422)  triple_60: 0.0000 (0.9077)  triple_40: 0.0000 (0.4829)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 450/1724]  eta: 1:23:21  lr: 0.000040  loss: 31.7310 (34.4155)  loss_n_40: 6.6044 (6.6358)  loss_n_60: 8.3416 (8.5009)  loss_n_80: 8.1212 (8.3100)  loss_n_100: 8.6839 (8.8981)  triple_100: 0.0000 (0.3172)  triple_80: 0.0000 (0.3464)  triple_60: 0.0000 (0.9215)  triple_40: 0.0000 (0.4855)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 460/1724]  eta: 1:22:42  lr: 0.000040  loss: 32.0195 (34.3830)  loss_n_40: 6.6937 (6.6384)  loss_n_60: 8.3825 (8.4988)  loss_n_80: 8.0979 (8.3042)  loss_n_100: 8.6422 (8.8900)  triple_100: 0.0000 (0.3148)  triple_80: 0.0000 (0.3390)  triple_60: 0.0000 (0.9172)  triple_40: 0.0000 (0.4807)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 470/1724]  eta: 1:22:02  lr: 0.000040  loss: 31.5926 (34.3240)  loss_n_40: 6.5876 (6.6364)  loss_n_60: 8.2478 (8.4923)  loss_n_80: 7.9541 (8.2990)  loss_n_100: 8.4996 (8.8821)  triple_100: 0.0000 (0.3091)  triple_80: 0.0000 (0.3318)  triple_60: 0.0000 (0.9029)  triple_40: 0.0000 (0.4704)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 480/1724]  eta: 1:21:23  lr: 0.000040  loss: 31.0912 (34.2562)  loss_n_40: 6.5047 (6.6328)  loss_n_60: 8.1763 (8.4849)  loss_n_80: 8.0938 (8.2928)  loss_n_100: 8.4610 (8.8734)  triple_100: 0.0000 (0.3027)  triple_80: 0.0000 (0.3249)  triple_60: 0.0000 (0.8841)  triple_40: 0.0000 (0.4607)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 490/1724]  eta: 1:20:44  lr: 0.000040  loss: 31.1636 (34.2532)  loss_n_40: 6.4775 (6.6299)  loss_n_60: 8.1111 (8.4784)  loss_n_80: 8.0849 (8.2879)  loss_n_100: 8.4711 (8.8665)  triple_100: 0.0000 (0.3098)  triple_80: 0.0000 (0.3342)  triple_60: 0.0000 (0.8860)  triple_40: 0.0000 (0.4604)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 500/1724]  eta: 1:20:05  lr: 0.000040  loss: 31.7126 (34.2207)  loss_n_40: 6.5855 (6.6299)  loss_n_60: 8.1822 (8.4740)  loss_n_80: 7.9667 (8.2815)  loss_n_100: 8.4962 (8.8580)  triple_100: 0.0000 (0.3101)  triple_80: 0.0000 (0.3380)  triple_60: 0.0000 (0.8781)  triple_40: 0.0000 (0.4512)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 510/1724]  eta: 1:19:25  lr: 0.000040  loss: 31.6199 (34.2033)  loss_n_40: 6.5693 (6.6269)  loss_n_60: 8.1822 (8.4678)  loss_n_80: 7.9325 (8.2746)  loss_n_100: 8.4110 (8.8484)  triple_100: 0.0000 (0.3085)  triple_80: 0.0000 (0.3362)  triple_60: 0.0000 (0.8891)  triple_40: 0.0000 (0.4517)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 520/1724]  eta: 1:18:46  lr: 0.000040  loss: 31.9671 (34.2038)  loss_n_40: 6.5467 (6.6267)  loss_n_60: 8.1876 (8.4633)  loss_n_80: 8.0014 (8.2703)  loss_n_100: 8.4242 (8.8419)  triple_100: 0.0000 (0.3099)  triple_80: 0.0000 (0.3380)  triple_60: 0.0000 (0.9022)  triple_40: 0.0000 (0.4514)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 530/1724]  eta: 1:18:07  lr: 0.000040  loss: 31.9388 (34.1773)  loss_n_40: 6.5701 (6.6272)  loss_n_60: 8.2459 (8.4600)  loss_n_80: 8.0126 (8.2630)  loss_n_100: 8.4568 (8.8333)  triple_100: 0.0000 (0.3091)  triple_80: 0.0000 (0.3323)  triple_60: 0.4041 (0.9006)  triple_40: 0.0000 (0.4518)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 540/1724]  eta: 1:17:27  lr: 0.000040  loss: 31.9070 (34.2842)  loss_n_40: 6.6388 (6.6293)  loss_n_60: 8.2958 (8.4585)  loss_n_80: 7.9500 (8.2603)  loss_n_100: 8.4563 (8.8285)  triple_100: 0.0000 (0.3359)  triple_80: 0.0000 (0.3584)  triple_60: 0.0000 (0.9134)  triple_40: 0.0000 (0.5000)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 550/1724]  eta: 1:16:48  lr: 0.000040  loss: 31.9842 (34.2407)  loss_n_40: 6.7307 (6.6294)  loss_n_60: 8.3135 (8.4550)  loss_n_80: 8.0387 (8.2558)  loss_n_100: 8.4681 (8.8214)  triple_100: 0.0000 (0.3370)  triple_80: 0.0000 (0.3520)  triple_60: 0.0000 (0.8991)  triple_40: 0.0000 (0.4909)  time: 3.9244  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [ 560/1724]  eta: 1:16:09  lr: 0.000040  loss: 31.7297 (34.2435)  loss_n_40: 6.6491 (6.6286)  loss_n_60: 8.2424 (8.4511)  loss_n_80: 8.0283 (8.2515)  loss_n_100: 8.4557 (8.8146)  triple_100: 0.0000 (0.3444)  triple_80: 0.0000 (0.3579)  triple_60: 0.0000 (0.8945)  triple_40: 0.0000 (0.5007)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 570/1724]  eta: 1:15:30  lr: 0.000040  loss: 31.7297 (34.2218)  loss_n_40: 6.6491 (6.6308)  loss_n_60: 8.2444 (8.4491)  loss_n_80: 8.0105 (8.2484)  loss_n_100: 8.5139 (8.8091)  triple_100: 0.0000 (0.3410)  triple_80: 0.0000 (0.3545)  triple_60: 0.0000 (0.8919)  triple_40: 0.0000 (0.4971)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 580/1724]  eta: 1:14:50  lr: 0.000040  loss: 31.7513 (34.1801)  loss_n_40: 6.6367 (6.6290)  loss_n_60: 8.2444 (8.4448)  loss_n_80: 8.0090 (8.2453)  loss_n_100: 8.5139 (8.8026)  triple_100: 0.0000 (0.3365)  triple_80: 0.0000 (0.3490)  triple_60: 0.0000 (0.8822)  triple_40: 0.0000 (0.4908)  time: 3.9270  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:3]  [ 590/1724]  eta: 1:14:11  lr: 0.000040  loss: 31.8727 (34.1419)  loss_n_40: 6.6458 (6.6322)  loss_n_60: 8.2666 (8.4440)  loss_n_80: 8.0629 (8.2428)  loss_n_100: 8.4298 (8.7975)  triple_100: 0.0000 (0.3308)  triple_80: 0.0000 (0.3431)  triple_60: 0.0000 (0.8672)  triple_40: 0.0000 (0.4841)  time: 3.9249  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:3]  [ 600/1724]  eta: 1:13:32  lr: 0.000040  loss: 31.7697 (34.0966)  loss_n_40: 6.6138 (6.6298)  loss_n_60: 8.2316 (8.4385)  loss_n_80: 8.0581 (8.2366)  loss_n_100: 8.3932 (8.7888)  triple_100: 0.0000 (0.3264)  triple_80: 0.0000 (0.3377)  triple_60: 0.0000 (0.8627)  triple_40: 0.0000 (0.4761)  time: 3.9256  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:3]  [ 610/1724]  eta: 1:12:53  lr: 0.000040  loss: 30.8080 (34.0616)  loss_n_40: 6.4816 (6.6306)  loss_n_60: 8.1147 (8.4352)  loss_n_80: 7.8476 (8.2297)  loss_n_100: 8.1639 (8.7795)  triple_100: 0.0000 (0.3217)  triple_80: 0.0000 (0.3322)  triple_60: 0.0000 (0.8604)  triple_40: 0.0000 (0.4723)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 620/1724]  eta: 1:12:13  lr: 0.000040  loss: 31.3055 (34.0172)  loss_n_40: 6.5438 (6.6289)  loss_n_60: 8.1577 (8.4300)  loss_n_80: 7.9207 (8.2264)  loss_n_100: 8.3342 (8.7744)  triple_100: 0.0000 (0.3174)  triple_80: 0.0000 (0.3268)  triple_60: 0.0000 (0.8486)  triple_40: 0.0000 (0.4646)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 630/1724]  eta: 1:11:34  lr: 0.000040  loss: 31.3787 (33.9812)  loss_n_40: 6.5580 (6.6290)  loss_n_60: 8.1669 (8.4270)  loss_n_80: 7.9536 (8.2207)  loss_n_100: 8.4190 (8.7675)  triple_100: 0.0000 (0.3129)  triple_80: 0.0000 (0.3216)  triple_60: 0.0000 (0.8431)  triple_40: 0.0000 (0.4594)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 640/1724]  eta: 1:10:55  lr: 0.000040  loss: 31.3787 (33.9383)  loss_n_40: 6.5853 (6.6282)  loss_n_60: 8.1897 (8.4230)  loss_n_80: 7.8945 (8.2164)  loss_n_100: 8.3309 (8.7614)  triple_100: 0.0000 (0.3081)  triple_80: 0.0000 (0.3166)  triple_60: 0.0000 (0.8324)  triple_40: 0.0000 (0.4522)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 650/1724]  eta: 1:10:15  lr: 0.000040  loss: 30.8834 (33.8936)  loss_n_40: 6.6086 (6.6281)  loss_n_60: 8.1471 (8.4189)  loss_n_80: 7.9020 (8.2116)  loss_n_100: 8.2887 (8.7545)  triple_100: 0.0000 (0.3039)  triple_80: 0.0000 (0.3117)  triple_60: 0.0000 (0.8196)  triple_40: 0.0000 (0.4453)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 660/1724]  eta: 1:09:36  lr: 0.000040  loss: 31.0327 (33.8532)  loss_n_40: 6.6372 (6.6277)  loss_n_60: 8.1468 (8.4145)  loss_n_80: 7.9020 (8.2066)  loss_n_100: 8.2625 (8.7474)  triple_100: 0.0000 (0.3024)  triple_80: 0.0000 (0.3080)  triple_60: 0.0000 (0.8079)  triple_40: 0.0000 (0.4386)  time: 3.9242  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:3]  [ 670/1724]  eta: 1:08:57  lr: 0.000040  loss: 30.5705 (33.8033)  loss_n_40: 6.4407 (6.6247)  loss_n_60: 8.0195 (8.4083)  loss_n_80: 7.8573 (8.1997)  loss_n_100: 8.2498 (8.7379)  triple_100: 0.0000 (0.2987)  triple_80: 0.0000 (0.3034)  triple_60: 0.0000 (0.7986)  triple_40: 0.0000 (0.4320)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 680/1724]  eta: 1:08:18  lr: 0.000040  loss: 30.5705 (33.7594)  loss_n_40: 6.4038 (6.6230)  loss_n_60: 8.0067 (8.4031)  loss_n_80: 7.7777 (8.1950)  loss_n_100: 8.1594 (8.7313)  triple_100: 0.0000 (0.2948)  triple_80: 0.0000 (0.2991)  triple_60: 0.0000 (0.7875)  triple_40: 0.0000 (0.4257)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 690/1724]  eta: 1:07:38  lr: 0.000040  loss: 30.9894 (33.7503)  loss_n_40: 6.5672 (6.6220)  loss_n_60: 8.0971 (8.3981)  loss_n_80: 7.7892 (8.1894)  loss_n_100: 8.1875 (8.7240)  triple_100: 0.0000 (0.2955)  triple_80: 0.0000 (0.3046)  triple_60: 0.0000 (0.7906)  triple_40: 0.0000 (0.4262)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 700/1724]  eta: 1:06:59  lr: 0.000040  loss: 31.1406 (33.7261)  loss_n_40: 6.5545 (6.6215)  loss_n_60: 8.0798 (8.3937)  loss_n_80: 7.8598 (8.1853)  loss_n_100: 8.2093 (8.7182)  triple_100: 0.0000 (0.2920)  triple_80: 0.0000 (0.3003)  triple_60: 0.0000 (0.7903)  triple_40: 0.0000 (0.4249)  time: 3.9243  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 710/1724]  eta: 1:06:20  lr: 0.000040  loss: 30.9382 (33.7077)  loss_n_40: 6.5545 (6.6208)  loss_n_60: 8.0734 (8.3887)  loss_n_80: 7.8617 (8.1808)  loss_n_100: 8.3267 (8.7123)  triple_100: 0.0000 (0.2905)  triple_80: 0.0000 (0.2996)  triple_60: 0.0000 (0.7916)  triple_40: 0.0000 (0.4233)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 720/1724]  eta: 1:05:41  lr: 0.000040  loss: 30.7972 (33.6813)  loss_n_40: 6.5399 (6.6199)  loss_n_60: 7.9829 (8.3832)  loss_n_80: 7.7890 (8.1750)  loss_n_100: 8.2492 (8.7053)  triple_100: 0.0000 (0.2870)  triple_80: 0.0000 (0.2969)  triple_60: 0.0000 (0.7930)  triple_40: 0.0000 (0.4210)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 730/1724]  eta: 1:05:01  lr: 0.000040  loss: 31.2916 (33.6546)  loss_n_40: 6.5776 (6.6199)  loss_n_60: 8.0063 (8.3795)  loss_n_80: 7.8868 (8.1720)  loss_n_100: 8.2765 (8.7008)  triple_100: 0.0000 (0.2837)  triple_80: 0.0000 (0.2929)  triple_60: 0.0000 (0.7891)  triple_40: 0.0000 (0.4167)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 740/1724]  eta: 1:04:22  lr: 0.000040  loss: 31.5967 (33.6247)  loss_n_40: 6.6703 (6.6198)  loss_n_60: 8.1362 (8.3763)  loss_n_80: 7.8868 (8.1673)  loss_n_100: 8.2765 (8.6949)  triple_100: 0.0000 (0.2818)  triple_80: 0.0000 (0.2897)  triple_60: 0.0000 (0.7838)  triple_40: 0.0000 (0.4110)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 750/1724]  eta: 1:03:43  lr: 0.000040  loss: 30.9601 (33.5875)  loss_n_40: 6.6023 (6.6182)  loss_n_60: 8.0572 (8.3714)  loss_n_80: 7.8625 (8.1640)  loss_n_100: 8.3247 (8.6904)  triple_100: 0.0000 (0.2780)  triple_80: 0.0000 (0.2859)  triple_60: 0.0000 (0.7740)  triple_40: 0.0000 (0.4056)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 760/1724]  eta: 1:03:04  lr: 0.000040  loss: 30.6525 (33.5841)  loss_n_40: 6.5087 (6.6176)  loss_n_60: 7.9492 (8.3665)  loss_n_80: 7.8194 (8.1578)  loss_n_100: 8.1733 (8.6826)  triple_100: 0.0000 (0.2781)  triple_80: 0.0000 (0.2889)  triple_60: 0.0000 (0.7809)  triple_40: 0.0000 (0.4117)  time: 3.9236  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 770/1724]  eta: 1:02:24  lr: 0.000040  loss: 30.3806 (33.5415)  loss_n_40: 6.4380 (6.6157)  loss_n_60: 7.9492 (8.3610)  loss_n_80: 7.6242 (8.1511)  loss_n_100: 8.1052 (8.6748)  triple_100: 0.0000 (0.2764)  triple_80: 0.0000 (0.2853)  triple_60: 0.0000 (0.7709)  triple_40: 0.0000 (0.4063)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 780/1724]  eta: 1:01:45  lr: 0.000040  loss: 30.1832 (33.5150)  loss_n_40: 6.4211 (6.6136)  loss_n_60: 7.9189 (8.3553)  loss_n_80: 7.6197 (8.1447)  loss_n_100: 8.0526 (8.6674)  triple_100: 0.0000 (0.2765)  triple_80: 0.0000 (0.2831)  triple_60: 0.0000 (0.7709)  triple_40: 0.0000 (0.4034)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 790/1724]  eta: 1:01:06  lr: 0.000040  loss: 30.7675 (33.5022)  loss_n_40: 6.5110 (6.6136)  loss_n_60: 8.0025 (8.3520)  loss_n_80: 7.7025 (8.1401)  loss_n_100: 8.1634 (8.6610)  triple_100: 0.0000 (0.2749)  triple_80: 0.0000 (0.2835)  triple_60: 0.0000 (0.7735)  triple_40: 0.0000 (0.4035)  time: 3.9258  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [ 800/1724]  eta: 1:00:27  lr: 0.000040  loss: 30.7907 (33.5002)  loss_n_40: 6.5953 (6.6127)  loss_n_60: 8.0244 (8.3472)  loss_n_80: 7.7101 (8.1341)  loss_n_100: 8.0947 (8.6535)  triple_100: 0.0000 (0.2781)  triple_80: 0.0000 (0.2913)  triple_60: 0.0000 (0.7789)  triple_40: 0.0000 (0.4043)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 810/1724]  eta: 0:59:47  lr: 0.000040  loss: 30.2637 (33.4651)  loss_n_40: 6.5620 (6.6113)  loss_n_60: 7.9795 (8.3426)  loss_n_80: 7.6914 (8.1307)  loss_n_100: 8.0856 (8.6484)  triple_100: 0.0000 (0.2747)  triple_80: 0.0000 (0.2877)  triple_60: 0.0000 (0.7702)  triple_40: 0.0000 (0.3993)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 820/1724]  eta: 0:59:08  lr: 0.000040  loss: 30.5254 (33.4725)  loss_n_40: 6.4369 (6.6101)  loss_n_60: 7.9046 (8.3378)  loss_n_80: 7.6905 (8.1259)  loss_n_100: 8.1508 (8.6421)  triple_100: 0.0000 (0.2819)  triple_80: 0.0000 (0.2942)  triple_60: 0.0000 (0.7808)  triple_40: 0.0000 (0.3998)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 830/1724]  eta: 0:58:29  lr: 0.000040  loss: 30.4974 (33.4397)  loss_n_40: 6.3987 (6.6085)  loss_n_60: 7.9046 (8.3330)  loss_n_80: 7.6692 (8.1202)  loss_n_100: 8.0174 (8.6343)  triple_100: 0.0000 (0.2814)  triple_80: 0.0000 (0.2914)  triple_60: 0.0000 (0.7760)  triple_40: 0.0000 (0.3950)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 840/1724]  eta: 0:57:50  lr: 0.000040  loss: 30.1710 (33.4046)  loss_n_40: 6.5078 (6.6075)  loss_n_60: 7.9239 (8.3283)  loss_n_80: 7.7397 (8.1157)  loss_n_100: 8.1101 (8.6279)  triple_100: 0.0000 (0.2780)  triple_80: 0.0000 (0.2879)  triple_60: 0.0000 (0.7690)  triple_40: 0.0000 (0.3903)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 850/1724]  eta: 0:57:10  lr: 0.000040  loss: 30.6293 (33.4264)  loss_n_40: 6.5180 (6.6072)  loss_n_60: 7.9542 (8.3241)  loss_n_80: 7.7109 (8.1118)  loss_n_100: 8.0838 (8.6223)  triple_100: 0.0000 (0.2873)  triple_80: 0.0000 (0.2930)  triple_60: 0.0000 (0.7752)  triple_40: 0.0000 (0.4053)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 860/1724]  eta: 0:56:31  lr: 0.000040  loss: 30.3960 (33.3868)  loss_n_40: 6.4830 (6.6045)  loss_n_60: 7.8770 (8.3184)  loss_n_80: 7.6843 (8.1050)  loss_n_100: 7.9831 (8.6143)  triple_100: 0.0000 (0.2849)  triple_80: 0.0000 (0.2896)  triple_60: 0.0000 (0.7694)  triple_40: 0.0000 (0.4006)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 870/1724]  eta: 0:55:52  lr: 0.000040  loss: 30.3646 (33.3535)  loss_n_40: 6.3456 (6.6032)  loss_n_60: 7.8350 (8.3135)  loss_n_80: 7.7280 (8.0999)  loss_n_100: 8.0431 (8.6089)  triple_100: 0.0000 (0.2819)  triple_80: 0.0000 (0.2863)  triple_60: 0.0000 (0.7638)  triple_40: 0.0000 (0.3960)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 880/1724]  eta: 0:55:13  lr: 0.000040  loss: 30.3646 (33.3287)  loss_n_40: 6.3351 (6.6007)  loss_n_60: 7.8273 (8.3077)  loss_n_80: 7.5557 (8.0938)  loss_n_100: 8.0431 (8.6024)  triple_100: 0.0000 (0.2812)  triple_80: 0.0000 (0.2856)  triple_60: 0.0000 (0.7634)  triple_40: 0.0000 (0.3939)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 890/1724]  eta: 0:54:33  lr: 0.000040  loss: 29.9811 (33.3324)  loss_n_40: 6.4331 (6.6002)  loss_n_60: 7.7945 (8.3036)  loss_n_80: 7.5557 (8.0890)  loss_n_100: 7.9805 (8.5967)  triple_100: 0.0000 (0.2863)  triple_80: 0.0000 (0.2884)  triple_60: 0.0000 (0.7690)  triple_40: 0.0000 (0.3992)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 900/1724]  eta: 0:53:54  lr: 0.000040  loss: 30.5933 (33.3053)  loss_n_40: 6.5577 (6.5998)  loss_n_60: 7.9106 (8.2996)  loss_n_80: 7.5861 (8.0835)  loss_n_100: 8.0430 (8.5901)  triple_100: 0.0000 (0.2860)  triple_80: 0.0000 (0.2852)  triple_60: 0.0000 (0.7664)  triple_40: 0.0000 (0.3949)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 910/1724]  eta: 0:53:15  lr: 0.000040  loss: 30.2834 (33.2707)  loss_n_40: 6.4848 (6.5979)  loss_n_60: 7.8887 (8.2941)  loss_n_80: 7.5592 (8.0777)  loss_n_100: 7.9751 (8.5836)  triple_100: 0.0000 (0.2830)  triple_80: 0.0000 (0.2823)  triple_60: 0.0000 (0.7610)  triple_40: 0.0000 (0.3912)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 920/1724]  eta: 0:52:36  lr: 0.000040  loss: 30.1947 (33.2371)  loss_n_40: 6.4912 (6.5973)  loss_n_60: 7.8762 (8.2895)  loss_n_80: 7.4894 (8.0721)  loss_n_100: 7.9751 (8.5774)  triple_100: 0.0000 (0.2800)  triple_80: 0.0000 (0.2792)  triple_60: 0.0000 (0.7547)  triple_40: 0.0000 (0.3870)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 930/1724]  eta: 0:51:56  lr: 0.000040  loss: 29.9466 (33.2098)  loss_n_40: 6.4912 (6.5956)  loss_n_60: 7.8704 (8.2842)  loss_n_80: 7.5874 (8.0673)  loss_n_100: 7.9555 (8.5716)  triple_100: 0.0000 (0.2774)  triple_80: 0.0000 (0.2771)  triple_60: 0.0000 (0.7524)  triple_40: 0.0000 (0.3842)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 940/1724]  eta: 0:51:17  lr: 0.000040  loss: 29.7386 (33.1697)  loss_n_40: 6.3685 (6.5922)  loss_n_60: 7.8046 (8.2775)  loss_n_80: 7.5801 (8.0610)  loss_n_100: 7.9555 (8.5650)  triple_100: 0.0000 (0.2749)  triple_80: 0.0000 (0.2742)  triple_60: 0.0000 (0.7447)  triple_40: 0.0000 (0.3801)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 950/1724]  eta: 0:50:38  lr: 0.000040  loss: 29.6938 (33.1370)  loss_n_40: 6.3100 (6.5902)  loss_n_60: 7.6659 (8.2723)  loss_n_80: 7.4690 (8.0554)  loss_n_100: 7.9634 (8.5589)  triple_100: 0.0000 (0.2727)  triple_80: 0.0000 (0.2713)  triple_60: 0.0000 (0.7399)  triple_40: 0.0000 (0.3764)  time: 3.9267  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:3]  [ 960/1724]  eta: 0:49:59  lr: 0.000040  loss: 29.7513 (33.1048)  loss_n_40: 6.3388 (6.5889)  loss_n_60: 7.6766 (8.2673)  loss_n_80: 7.4534 (8.0491)  loss_n_100: 7.9197 (8.5522)  triple_100: 0.0000 (0.2715)  triple_80: 0.0000 (0.2685)  triple_60: 0.0000 (0.7328)  triple_40: 0.0000 (0.3745)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 970/1724]  eta: 0:49:19  lr: 0.000040  loss: 29.7513 (33.0738)  loss_n_40: 6.4574 (6.5879)  loss_n_60: 7.7826 (8.2625)  loss_n_80: 7.4534 (8.0439)  loss_n_100: 7.8972 (8.5465)  triple_100: 0.0000 (0.2699)  triple_80: 0.0000 (0.2657)  triple_60: 0.0000 (0.7268)  triple_40: 0.0000 (0.3706)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 980/1724]  eta: 0:48:40  lr: 0.000040  loss: 29.6555 (33.0371)  loss_n_40: 6.3034 (6.5857)  loss_n_60: 7.6815 (8.2569)  loss_n_80: 7.4445 (8.0378)  loss_n_100: 7.9217 (8.5403)  triple_100: 0.0000 (0.2672)  triple_80: 0.0000 (0.2630)  triple_60: 0.0000 (0.7194)  triple_40: 0.0000 (0.3669)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [ 990/1724]  eta: 0:48:01  lr: 0.000040  loss: 29.2392 (33.0026)  loss_n_40: 6.2970 (6.5840)  loss_n_60: 7.6160 (8.2514)  loss_n_80: 7.3818 (8.0318)  loss_n_100: 7.9149 (8.5346)  triple_100: 0.0000 (0.2653)  triple_80: 0.0000 (0.2604)  triple_60: 0.0000 (0.7121)  triple_40: 0.0000 (0.3632)  time: 3.9263  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:3]  [1000/1724]  eta: 0:47:22  lr: 0.000040  loss: 29.1249 (32.9754)  loss_n_40: 6.3367 (6.5816)  loss_n_60: 7.6018 (8.2457)  loss_n_80: 7.3617 (8.0258)  loss_n_100: 7.8689 (8.5279)  triple_100: 0.0000 (0.2649)  triple_80: 0.0000 (0.2580)  triple_60: 0.0000 (0.7081)  triple_40: 0.0000 (0.3632)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1010/1724]  eta: 0:46:42  lr: 0.000040  loss: 29.4767 (32.9440)  loss_n_40: 6.3367 (6.5805)  loss_n_60: 7.6708 (8.2408)  loss_n_80: 7.3935 (8.0199)  loss_n_100: 7.8286 (8.5215)  triple_100: 0.0000 (0.2639)  triple_80: 0.0000 (0.2555)  triple_60: 0.0000 (0.7021)  triple_40: 0.0000 (0.3596)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1020/1724]  eta: 0:46:03  lr: 0.000040  loss: 29.4767 (32.9089)  loss_n_40: 6.5143 (6.5798)  loss_n_60: 7.6883 (8.2361)  loss_n_80: 7.3168 (8.0128)  loss_n_100: 7.7970 (8.5145)  triple_100: 0.0000 (0.2614)  triple_80: 0.0000 (0.2530)  triple_60: 0.0000 (0.6952)  triple_40: 0.0000 (0.3561)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1030/1724]  eta: 0:45:24  lr: 0.000040  loss: 29.8456 (32.9240)  loss_n_40: 6.6070 (6.5797)  loss_n_60: 7.7142 (8.2312)  loss_n_80: 7.3770 (8.0077)  loss_n_100: 7.7875 (8.5093)  triple_100: 0.0000 (0.2687)  triple_80: 0.0000 (0.2615)  triple_60: 0.0000 (0.7048)  triple_40: 0.0000 (0.3611)  time: 3.9270  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [1040/1724]  eta: 0:44:45  lr: 0.000040  loss: 30.1988 (32.8946)  loss_n_40: 6.5931 (6.5793)  loss_n_60: 7.7493 (8.2265)  loss_n_80: 7.3786 (8.0020)  loss_n_100: 7.8231 (8.5036)  triple_100: 0.0000 (0.2665)  triple_80: 0.0000 (0.2590)  triple_60: 0.0000 (0.7000)  triple_40: 0.0000 (0.3577)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1050/1724]  eta: 0:44:05  lr: 0.000040  loss: 29.4190 (32.8598)  loss_n_40: 6.5045 (6.5777)  loss_n_60: 7.7269 (8.2214)  loss_n_80: 7.2900 (7.9951)  loss_n_100: 7.7733 (8.4968)  triple_100: 0.0000 (0.2640)  triple_80: 0.0000 (0.2565)  triple_60: 0.0000 (0.6940)  triple_40: 0.0000 (0.3543)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1060/1724]  eta: 0:43:26  lr: 0.000040  loss: 28.7003 (32.8278)  loss_n_40: 6.4264 (6.5749)  loss_n_60: 7.6526 (8.2152)  loss_n_80: 7.1963 (7.9876)  loss_n_100: 7.6742 (8.4890)  triple_100: 0.0000 (0.2623)  triple_80: 0.0000 (0.2565)  triple_60: 0.0000 (0.6905)  triple_40: 0.0000 (0.3519)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1070/1724]  eta: 0:42:47  lr: 0.000040  loss: 28.7955 (32.7997)  loss_n_40: 6.2997 (6.5728)  loss_n_60: 7.5971 (8.2091)  loss_n_80: 7.1560 (7.9809)  loss_n_100: 7.6343 (8.4824)  triple_100: 0.0000 (0.2608)  triple_80: 0.0000 (0.2549)  triple_60: 0.0000 (0.6893)  triple_40: 0.0000 (0.3497)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1080/1724]  eta: 0:42:08  lr: 0.000040  loss: 28.8269 (32.7727)  loss_n_40: 6.3470 (6.5708)  loss_n_60: 7.5435 (8.2032)  loss_n_80: 7.1238 (7.9738)  loss_n_100: 7.6343 (8.4753)  triple_100: 0.0000 (0.2595)  triple_80: 0.0000 (0.2526)  triple_60: 0.0000 (0.6892)  triple_40: 0.0000 (0.3485)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1090/1724]  eta: 0:41:28  lr: 0.000040  loss: 29.1358 (32.7514)  loss_n_40: 6.4035 (6.5694)  loss_n_60: 7.6255 (8.1984)  loss_n_80: 7.2439 (7.9680)  loss_n_100: 7.7161 (8.4690)  triple_100: 0.0000 (0.2607)  triple_80: 0.0000 (0.2520)  triple_60: 0.0000 (0.6882)  triple_40: 0.0000 (0.3457)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1100/1724]  eta: 0:40:49  lr: 0.000040  loss: 29.6290 (32.7267)  loss_n_40: 6.4016 (6.5683)  loss_n_60: 7.6187 (8.1935)  loss_n_80: 7.3033 (7.9629)  loss_n_100: 7.8384 (8.4636)  triple_100: 0.0000 (0.2588)  triple_80: 0.0000 (0.2501)  triple_60: 0.0000 (0.6861)  triple_40: 0.0000 (0.3434)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1110/1724]  eta: 0:40:10  lr: 0.000040  loss: 29.6290 (32.7067)  loss_n_40: 6.4016 (6.5670)  loss_n_60: 7.6159 (8.1884)  loss_n_80: 7.3764 (7.9575)  loss_n_100: 7.8307 (8.4577)  triple_100: 0.0000 (0.2579)  triple_80: 0.0000 (0.2493)  triple_60: 0.0000 (0.6876)  triple_40: 0.0000 (0.3412)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1120/1724]  eta: 0:39:31  lr: 0.000040  loss: 29.6873 (32.6804)  loss_n_40: 6.4886 (6.5660)  loss_n_60: 7.6757 (8.1838)  loss_n_80: 7.3028 (7.9526)  loss_n_100: 7.7577 (8.4526)  triple_100: 0.0000 (0.2556)  triple_80: 0.0000 (0.2470)  triple_60: 0.0000 (0.6846)  triple_40: 0.0000 (0.3382)  time: 3.9265  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:3]  [1130/1724]  eta: 0:38:51  lr: 0.000040  loss: 29.4104 (32.6547)  loss_n_40: 6.4924 (6.5647)  loss_n_60: 7.7023 (8.1793)  loss_n_80: 7.3028 (7.9466)  loss_n_100: 7.7417 (8.4465)  triple_100: 0.0000 (0.2533)  triple_80: 0.0000 (0.2449)  triple_60: 0.0000 (0.6837)  triple_40: 0.0000 (0.3357)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1140/1724]  eta: 0:38:12  lr: 0.000040  loss: 29.2274 (32.6237)  loss_n_40: 6.2639 (6.5620)  loss_n_60: 7.5891 (8.1732)  loss_n_80: 7.2030 (7.9401)  loss_n_100: 7.6781 (8.4401)  triple_100: 0.0000 (0.2512)  triple_80: 0.0000 (0.2430)  triple_60: 0.0000 (0.6813)  triple_40: 0.0000 (0.3327)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1150/1724]  eta: 0:37:33  lr: 0.000040  loss: 29.0508 (32.6052)  loss_n_40: 6.3435 (6.5609)  loss_n_60: 7.5955 (8.1689)  loss_n_80: 7.1296 (7.9338)  loss_n_100: 7.6781 (8.4336)  triple_100: 0.0000 (0.2527)  triple_80: 0.0000 (0.2431)  triple_60: 0.0000 (0.6824)  triple_40: 0.0000 (0.3298)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1160/1724]  eta: 0:36:54  lr: 0.000040  loss: 29.0114 (32.5730)  loss_n_40: 6.3137 (6.5579)  loss_n_60: 7.5691 (8.1632)  loss_n_80: 7.1927 (7.9283)  loss_n_100: 7.7116 (8.4279)  triple_100: 0.0000 (0.2509)  triple_80: 0.0000 (0.2410)  triple_60: 0.0000 (0.6768)  triple_40: 0.0000 (0.3270)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1170/1724]  eta: 0:36:14  lr: 0.000040  loss: 29.1154 (32.5458)  loss_n_40: 6.3709 (6.5572)  loss_n_60: 7.6026 (8.1591)  loss_n_80: 7.2924 (7.9228)  loss_n_100: 7.7726 (8.4229)  triple_100: 0.0000 (0.2496)  triple_80: 0.0000 (0.2390)  triple_60: 0.0000 (0.6710)  triple_40: 0.0000 (0.3242)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1180/1724]  eta: 0:35:35  lr: 0.000040  loss: 29.2607 (32.5187)  loss_n_40: 6.4710 (6.5564)  loss_n_60: 7.6694 (8.1554)  loss_n_80: 7.2702 (7.9177)  loss_n_100: 7.7629 (8.4172)  triple_100: 0.0000 (0.2483)  triple_80: 0.0000 (0.2369)  triple_60: 0.0000 (0.6653)  triple_40: 0.0000 (0.3215)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1190/1724]  eta: 0:34:56  lr: 0.000040  loss: 28.8877 (32.4878)  loss_n_40: 6.3931 (6.5542)  loss_n_60: 7.6366 (8.1503)  loss_n_80: 7.1175 (7.9110)  loss_n_100: 7.6498 (8.4106)  triple_100: 0.0000 (0.2462)  triple_80: 0.0000 (0.2349)  triple_60: 0.0000 (0.6616)  triple_40: 0.0000 (0.3189)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1200/1724]  eta: 0:34:16  lr: 0.000040  loss: 28.3895 (32.4632)  loss_n_40: 6.2251 (6.5523)  loss_n_60: 7.4428 (8.1452)  loss_n_80: 7.1091 (7.9048)  loss_n_100: 7.5736 (8.4041)  triple_100: 0.0000 (0.2455)  triple_80: 0.0000 (0.2330)  triple_60: 0.0000 (0.6598)  triple_40: 0.0000 (0.3183)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1210/1724]  eta: 0:33:37  lr: 0.000040  loss: 28.7499 (32.4342)  loss_n_40: 6.2861 (6.5508)  loss_n_60: 7.4579 (8.1399)  loss_n_80: 6.9897 (7.8977)  loss_n_100: 7.5629 (8.3974)  triple_100: 0.0000 (0.2456)  triple_80: 0.0000 (0.2311)  triple_60: 0.0000 (0.6556)  triple_40: 0.0000 (0.3162)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1220/1724]  eta: 0:32:58  lr: 0.000040  loss: 28.5215 (32.4037)  loss_n_40: 6.3206 (6.5490)  loss_n_60: 7.4805 (8.1347)  loss_n_80: 7.0402 (7.8916)  loss_n_100: 7.5906 (8.3916)  triple_100: 0.0000 (0.2438)  triple_80: 0.0000 (0.2292)  triple_60: 0.0000 (0.6502)  triple_40: 0.0000 (0.3136)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1230/1724]  eta: 0:32:19  lr: 0.000040  loss: 28.5215 (32.3754)  loss_n_40: 6.2278 (6.5463)  loss_n_60: 7.4343 (8.1290)  loss_n_80: 7.1798 (7.8872)  loss_n_100: 7.7007 (8.3866)  triple_100: 0.0000 (0.2418)  triple_80: 0.0000 (0.2273)  triple_60: 0.0000 (0.6460)  triple_40: 0.0000 (0.3111)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1240/1724]  eta: 0:31:39  lr: 0.000040  loss: 28.5742 (32.3434)  loss_n_40: 6.1989 (6.5441)  loss_n_60: 7.3809 (8.1235)  loss_n_80: 7.1943 (7.8809)  loss_n_100: 7.6428 (8.3800)  triple_100: 0.0000 (0.2400)  triple_80: 0.0000 (0.2255)  triple_60: 0.0000 (0.6409)  triple_40: 0.0000 (0.3086)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1250/1724]  eta: 0:31:00  lr: 0.000040  loss: 28.1913 (32.3098)  loss_n_40: 6.3410 (6.5421)  loss_n_60: 7.4590 (8.1180)  loss_n_80: 6.9338 (7.8734)  loss_n_100: 7.4574 (8.3726)  triple_100: 0.0000 (0.2381)  triple_80: 0.0000 (0.2237)  triple_60: 0.0000 (0.6358)  triple_40: 0.0000 (0.3061)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1260/1724]  eta: 0:30:21  lr: 0.000040  loss: 28.1913 (32.2814)  loss_n_40: 6.3769 (6.5398)  loss_n_60: 7.4488 (8.1124)  loss_n_80: 6.9339 (7.8674)  loss_n_100: 7.4402 (8.3661)  triple_100: 0.0000 (0.2376)  triple_80: 0.0000 (0.2220)  triple_60: 0.0000 (0.6324)  triple_40: 0.0000 (0.3037)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1270/1724]  eta: 0:29:42  lr: 0.000040  loss: 29.0763 (32.2628)  loss_n_40: 6.3769 (6.5392)  loss_n_60: 7.4488 (8.1085)  loss_n_80: 7.1516 (7.8629)  loss_n_100: 7.6372 (8.3617)  triple_100: 0.0000 (0.2366)  triple_80: 0.0000 (0.2210)  triple_60: 0.0000 (0.6315)  triple_40: 0.0000 (0.3014)  time: 3.9259  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [1280/1724]  eta: 0:29:02  lr: 0.000040  loss: 29.2894 (32.2375)  loss_n_40: 6.3436 (6.5369)  loss_n_60: 7.4440 (8.1028)  loss_n_80: 7.1635 (7.8582)  loss_n_100: 7.7544 (8.3568)  triple_100: 0.0000 (0.2357)  triple_80: 0.0000 (0.2193)  triple_60: 0.0000 (0.6287)  triple_40: 0.0000 (0.2991)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1290/1724]  eta: 0:28:23  lr: 0.000040  loss: 28.4741 (32.2083)  loss_n_40: 6.2627 (6.5344)  loss_n_60: 7.3418 (8.0968)  loss_n_80: 7.1266 (7.8526)  loss_n_100: 7.6911 (8.3509)  triple_100: 0.0000 (0.2341)  triple_80: 0.0000 (0.2176)  triple_60: 0.0000 (0.6252)  triple_40: 0.0000 (0.2969)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1300/1724]  eta: 0:27:44  lr: 0.000040  loss: 28.3096 (32.1774)  loss_n_40: 6.1800 (6.5318)  loss_n_60: 7.2962 (8.0909)  loss_n_80: 6.9586 (7.8463)  loss_n_100: 7.4275 (8.3442)  triple_100: 0.0000 (0.2323)  triple_80: 0.0000 (0.2159)  triple_60: 0.0000 (0.6214)  triple_40: 0.0000 (0.2946)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1310/1724]  eta: 0:27:05  lr: 0.000040  loss: 28.1391 (32.1506)  loss_n_40: 6.2222 (6.5301)  loss_n_60: 7.2962 (8.0854)  loss_n_80: 7.0050 (7.8405)  loss_n_100: 7.4275 (8.3385)  triple_100: 0.0000 (0.2325)  triple_80: 0.0000 (0.2143)  triple_60: 0.0000 (0.6170)  triple_40: 0.0000 (0.2924)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1320/1724]  eta: 0:26:25  lr: 0.000040  loss: 28.3551 (32.1236)  loss_n_40: 6.1952 (6.5276)  loss_n_60: 7.2654 (8.0792)  loss_n_80: 7.0050 (7.8347)  loss_n_100: 7.5245 (8.3327)  triple_100: 0.0000 (0.2316)  triple_80: 0.0000 (0.2126)  triple_60: 0.0000 (0.6150)  triple_40: 0.0000 (0.2902)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1330/1724]  eta: 0:25:46  lr: 0.000040  loss: 28.1816 (32.0947)  loss_n_40: 6.1952 (6.5255)  loss_n_60: 7.2654 (8.0736)  loss_n_80: 6.8674 (7.8285)  loss_n_100: 7.4788 (8.3266)  triple_100: 0.0000 (0.2299)  triple_80: 0.0000 (0.2110)  triple_60: 0.0000 (0.6109)  triple_40: 0.0000 (0.2886)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1340/1724]  eta: 0:25:07  lr: 0.000040  loss: 28.4475 (32.0857)  loss_n_40: 6.2371 (6.5240)  loss_n_60: 7.3561 (8.0684)  loss_n_80: 6.9511 (7.8234)  loss_n_100: 7.4788 (8.3212)  triple_100: 0.0000 (0.2325)  triple_80: 0.0000 (0.2114)  triple_60: 0.0000 (0.6120)  triple_40: 0.0000 (0.2928)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1350/1724]  eta: 0:24:28  lr: 0.000040  loss: 28.4730 (32.0580)  loss_n_40: 6.3392 (6.5222)  loss_n_60: 7.3601 (8.0635)  loss_n_80: 7.0306 (7.8175)  loss_n_100: 7.5208 (8.3151)  triple_100: 0.0000 (0.2312)  triple_80: 0.0000 (0.2098)  triple_60: 0.0000 (0.6081)  triple_40: 0.0000 (0.2906)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1360/1724]  eta: 0:23:48  lr: 0.000040  loss: 28.2743 (32.0282)  loss_n_40: 6.2665 (6.5195)  loss_n_60: 7.3229 (8.0575)  loss_n_80: 7.0345 (7.8113)  loss_n_100: 7.5205 (8.3093)  triple_100: 0.0000 (0.2295)  triple_80: 0.0000 (0.2083)  triple_60: 0.0000 (0.6043)  triple_40: 0.0000 (0.2885)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1370/1724]  eta: 0:23:09  lr: 0.000040  loss: 28.2743 (32.0144)  loss_n_40: 6.1705 (6.5176)  loss_n_60: 7.3061 (8.0525)  loss_n_80: 7.0345 (7.8053)  loss_n_100: 7.5138 (8.3025)  triple_100: 0.0000 (0.2321)  triple_80: 0.0000 (0.2082)  triple_60: 0.0000 (0.6058)  triple_40: 0.0000 (0.2904)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1380/1724]  eta: 0:22:30  lr: 0.000040  loss: 28.2925 (31.9959)  loss_n_40: 6.2422 (6.5158)  loss_n_60: 7.3168 (8.0475)  loss_n_80: 7.0967 (7.7999)  loss_n_100: 7.5849 (8.2970)  triple_100: 0.0000 (0.2329)  triple_80: 0.0000 (0.2084)  triple_60: 0.0000 (0.6053)  triple_40: 0.0000 (0.2893)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1390/1724]  eta: 0:21:51  lr: 0.000040  loss: 28.3032 (31.9744)  loss_n_40: 6.2809 (6.5148)  loss_n_60: 7.3338 (8.0430)  loss_n_80: 7.1112 (7.7949)  loss_n_100: 7.5929 (8.2920)  triple_100: 0.0000 (0.2312)  triple_80: 0.0000 (0.2069)  triple_60: 0.0000 (0.6042)  triple_40: 0.0000 (0.2874)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1400/1724]  eta: 0:21:11  lr: 0.000040  loss: 28.1649 (31.9442)  loss_n_40: 6.2696 (6.5119)  loss_n_60: 7.3079 (8.0372)  loss_n_80: 6.8880 (7.7883)  loss_n_100: 7.4176 (8.2858)  triple_100: 0.0000 (0.2304)  triple_80: 0.0000 (0.2054)  triple_60: 0.0000 (0.5999)  triple_40: 0.0000 (0.2854)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1410/1724]  eta: 0:20:32  lr: 0.000040  loss: 27.8527 (31.9238)  loss_n_40: 6.1589 (6.5101)  loss_n_60: 7.2344 (8.0322)  loss_n_80: 6.8880 (7.7822)  loss_n_100: 7.3966 (8.2796)  triple_100: 0.0000 (0.2292)  triple_80: 0.0000 (0.2043)  triple_60: 0.0000 (0.6013)  triple_40: 0.0000 (0.2849)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1420/1724]  eta: 0:19:53  lr: 0.000040  loss: 28.0886 (31.8975)  loss_n_40: 6.1657 (6.5088)  loss_n_60: 7.2964 (8.0279)  loss_n_80: 6.9355 (7.7765)  loss_n_100: 7.3797 (8.2738)  triple_100: 0.0000 (0.2276)  triple_80: 0.0000 (0.2028)  triple_60: 0.0000 (0.5970)  triple_40: 0.0000 (0.2829)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1430/1724]  eta: 0:19:14  lr: 0.000040  loss: 27.8645 (31.8696)  loss_n_40: 6.1611 (6.5065)  loss_n_60: 7.2693 (8.0225)  loss_n_80: 6.8316 (7.7709)  loss_n_100: 7.4242 (8.2682)  triple_100: 0.0000 (0.2260)  triple_80: 0.0000 (0.2014)  triple_60: 0.0000 (0.5932)  triple_40: 0.0000 (0.2809)  time: 3.9236  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1440/1724]  eta: 0:18:34  lr: 0.000040  loss: 27.7641 (31.8488)  loss_n_40: 6.1619 (6.5053)  loss_n_60: 7.2523 (8.0177)  loss_n_80: 6.8598 (7.7645)  loss_n_100: 7.3951 (8.2620)  triple_100: 0.0000 (0.2264)  triple_80: 0.0000 (0.2015)  triple_60: 0.0000 (0.5923)  triple_40: 0.0000 (0.2791)  time: 3.9231  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1450/1724]  eta: 0:17:55  lr: 0.000040  loss: 27.6347 (31.8260)  loss_n_40: 6.1330 (6.5025)  loss_n_60: 7.1931 (8.0120)  loss_n_80: 6.8730 (7.7583)  loss_n_100: 7.3599 (8.2558)  triple_100: 0.0000 (0.2286)  triple_80: 0.0000 (0.2008)  triple_60: 0.0000 (0.5908)  triple_40: 0.0000 (0.2771)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1460/1724]  eta: 0:17:16  lr: 0.000040  loss: 27.6395 (31.8001)  loss_n_40: 6.0806 (6.5005)  loss_n_60: 7.2221 (8.0072)  loss_n_80: 6.8785 (7.7525)  loss_n_100: 7.3958 (8.2502)  triple_100: 0.0000 (0.2272)  triple_80: 0.0000 (0.1995)  triple_60: 0.0000 (0.5867)  triple_40: 0.0000 (0.2764)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1470/1724]  eta: 0:16:37  lr: 0.000040  loss: 27.6574 (31.7788)  loss_n_40: 6.1926 (6.4985)  loss_n_60: 7.2454 (8.0021)  loss_n_80: 6.9861 (7.7472)  loss_n_100: 7.5084 (8.2450)  triple_100: 0.0000 (0.2268)  triple_80: 0.0000 (0.1983)  triple_60: 0.0000 (0.5856)  triple_40: 0.0000 (0.2753)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1480/1724]  eta: 0:15:57  lr: 0.000040  loss: 27.7431 (31.7529)  loss_n_40: 6.1129 (6.4963)  loss_n_60: 7.1769 (7.9968)  loss_n_80: 6.9062 (7.7411)  loss_n_100: 7.4619 (8.2396)  triple_100: 0.0000 (0.2257)  triple_80: 0.0000 (0.1970)  triple_60: 0.0000 (0.5823)  triple_40: 0.0000 (0.2740)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1490/1724]  eta: 0:15:18  lr: 0.000040  loss: 28.0016 (31.7345)  loss_n_40: 6.0899 (6.4937)  loss_n_60: 7.1769 (7.9911)  loss_n_80: 6.9505 (7.7361)  loss_n_100: 7.4717 (8.2348)  triple_100: 0.0000 (0.2250)  triple_80: 0.0000 (0.1964)  triple_60: 0.0000 (0.5847)  triple_40: 0.0000 (0.2726)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1500/1724]  eta: 0:14:39  lr: 0.000040  loss: 28.0892 (31.7090)  loss_n_40: 5.9891 (6.4909)  loss_n_60: 7.0321 (7.9855)  loss_n_80: 6.9059 (7.7305)  loss_n_100: 7.4717 (8.2294)  triple_100: 0.0000 (0.2255)  triple_80: 0.0000 (0.1951)  triple_60: 0.0000 (0.5813)  triple_40: 0.0000 (0.2708)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1510/1724]  eta: 0:13:59  lr: 0.000040  loss: 27.3281 (31.6830)  loss_n_40: 6.0250 (6.4885)  loss_n_60: 7.1089 (7.9803)  loss_n_80: 6.7306 (7.7238)  loss_n_100: 7.3117 (8.2229)  triple_100: 0.0000 (0.2241)  triple_80: 0.0000 (0.1938)  triple_60: 0.0000 (0.5806)  triple_40: 0.0000 (0.2690)  time: 3.9226  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [1520/1724]  eta: 0:13:20  lr: 0.000040  loss: 27.4602 (31.6584)  loss_n_40: 6.0928 (6.4871)  loss_n_60: 7.1638 (7.9759)  loss_n_80: 6.7277 (7.7185)  loss_n_100: 7.3186 (8.2178)  triple_100: 0.0000 (0.2226)  triple_80: 0.0000 (0.1926)  triple_60: 0.0000 (0.5767)  triple_40: 0.0000 (0.2672)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1530/1724]  eta: 0:12:41  lr: 0.000040  loss: 27.7838 (31.6363)  loss_n_40: 6.2091 (6.4860)  loss_n_60: 7.2306 (7.9713)  loss_n_80: 6.9396 (7.7139)  loss_n_100: 7.4379 (8.2128)  triple_100: 0.0000 (0.2213)  triple_80: 0.0000 (0.1913)  triple_60: 0.0000 (0.5743)  triple_40: 0.0000 (0.2655)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1540/1724]  eta: 0:12:02  lr: 0.000040  loss: 28.2765 (31.6174)  loss_n_40: 6.2570 (6.4847)  loss_n_60: 7.3037 (7.9673)  loss_n_80: 6.8997 (7.7083)  loss_n_100: 7.2727 (8.2066)  triple_100: 0.0000 (0.2202)  triple_80: 0.0000 (0.1910)  triple_60: 0.0000 (0.5751)  triple_40: 0.0000 (0.2642)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1550/1724]  eta: 0:11:22  lr: 0.000040  loss: 28.2801 (31.5949)  loss_n_40: 6.2407 (6.4828)  loss_n_60: 7.3263 (7.9630)  loss_n_80: 6.9044 (7.7042)  loss_n_100: 7.2727 (8.2023)  triple_100: 0.0000 (0.2188)  triple_80: 0.0000 (0.1898)  triple_60: 0.0000 (0.5714)  triple_40: 0.0000 (0.2625)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1560/1724]  eta: 0:10:43  lr: 0.000040  loss: 28.0083 (31.5792)  loss_n_40: 6.2442 (6.4819)  loss_n_60: 7.3156 (7.9592)  loss_n_80: 6.9170 (7.6991)  loss_n_100: 7.4701 (8.1971)  triple_100: 0.0000 (0.2177)  triple_80: 0.0000 (0.1886)  triple_60: 0.0000 (0.5718)  triple_40: 0.0000 (0.2638)  time: 3.9236  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1570/1724]  eta: 0:10:04  lr: 0.000040  loss: 27.9251 (31.5542)  loss_n_40: 6.2369 (6.4798)  loss_n_60: 7.2363 (7.9541)  loss_n_80: 6.9100 (7.6938)  loss_n_100: 7.3488 (8.1920)  triple_100: 0.0000 (0.2165)  triple_80: 0.0000 (0.1874)  triple_60: 0.0000 (0.5681)  triple_40: 0.0000 (0.2625)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1580/1724]  eta: 0:09:25  lr: 0.000040  loss: 27.5161 (31.5304)  loss_n_40: 6.1325 (6.4776)  loss_n_60: 7.1487 (7.9489)  loss_n_80: 6.8081 (7.6886)  loss_n_100: 7.3045 (8.1866)  triple_100: 0.0000 (0.2158)  triple_80: 0.0000 (0.1862)  triple_60: 0.0000 (0.5653)  triple_40: 0.0000 (0.2613)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1590/1724]  eta: 0:08:45  lr: 0.000040  loss: 27.5161 (31.5045)  loss_n_40: 6.1050 (6.4750)  loss_n_60: 7.1024 (7.9433)  loss_n_80: 6.8162 (7.6828)  loss_n_100: 7.2893 (8.1810)  triple_100: 0.0000 (0.2145)  triple_80: 0.0000 (0.1850)  triple_60: 0.0000 (0.5630)  triple_40: 0.0000 (0.2600)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1600/1724]  eta: 0:08:06  lr: 0.000040  loss: 27.8504 (31.4944)  loss_n_40: 6.1444 (6.4731)  loss_n_60: 7.1024 (7.9384)  loss_n_80: 6.8825 (7.6783)  loss_n_100: 7.2974 (8.1762)  triple_100: 0.0000 (0.2169)  triple_80: 0.0000 (0.1848)  triple_60: 0.0000 (0.5651)  triple_40: 0.0000 (0.2617)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1610/1724]  eta: 0:07:27  lr: 0.000040  loss: 27.2938 (31.4764)  loss_n_40: 6.1400 (6.4713)  loss_n_60: 7.1492 (7.9340)  loss_n_80: 6.7670 (7.6732)  loss_n_100: 7.3762 (8.1713)  triple_100: 0.0000 (0.2179)  triple_80: 0.0000 (0.1836)  triple_60: 0.0000 (0.5640)  triple_40: 0.0000 (0.2611)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1620/1724]  eta: 0:06:48  lr: 0.000040  loss: 27.7159 (31.4690)  loss_n_40: 6.0740 (6.4699)  loss_n_60: 7.1304 (7.9296)  loss_n_80: 6.8465 (7.6696)  loss_n_100: 7.4415 (8.1679)  triple_100: 0.0000 (0.2200)  triple_80: 0.0000 (0.1851)  triple_60: 0.0000 (0.5658)  triple_40: 0.0000 (0.2611)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1630/1724]  eta: 0:06:08  lr: 0.000040  loss: 28.4022 (31.4486)  loss_n_40: 6.1438 (6.4684)  loss_n_60: 7.1640 (7.9254)  loss_n_80: 6.9209 (7.6654)  loss_n_100: 7.4415 (8.1637)  triple_100: 0.0000 (0.2189)  triple_80: 0.0000 (0.1840)  triple_60: 0.0000 (0.5633)  triple_40: 0.0000 (0.2595)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1640/1724]  eta: 0:05:29  lr: 0.000040  loss: 27.5918 (31.4239)  loss_n_40: 6.0910 (6.4659)  loss_n_60: 7.1066 (7.9203)  loss_n_80: 6.8832 (7.6604)  loss_n_100: 7.4146 (8.1589)  triple_100: 0.0000 (0.2176)  triple_80: 0.0000 (0.1829)  triple_60: 0.0000 (0.5599)  triple_40: 0.0000 (0.2579)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1650/1724]  eta: 0:04:50  lr: 0.000040  loss: 27.6872 (31.4102)  loss_n_40: 6.0866 (6.4640)  loss_n_60: 7.1350 (7.9160)  loss_n_80: 6.9463 (7.6564)  loss_n_100: 7.4794 (8.1548)  triple_100: 0.0000 (0.2187)  triple_80: 0.0000 (0.1824)  triple_60: 0.0000 (0.5605)  triple_40: 0.0000 (0.2574)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1660/1724]  eta: 0:04:11  lr: 0.000040  loss: 27.6872 (31.3844)  loss_n_40: 6.0789 (6.4615)  loss_n_60: 7.1706 (7.9113)  loss_n_80: 6.7068 (7.6502)  loss_n_100: 7.2895 (8.1490)  triple_100: 0.0000 (0.2174)  triple_80: 0.0000 (0.1813)  triple_60: 0.0000 (0.5579)  triple_40: 0.0000 (0.2559)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1670/1724]  eta: 0:03:31  lr: 0.000040  loss: 26.8415 (31.3575)  loss_n_40: 6.0491 (6.4586)  loss_n_60: 7.0535 (7.9059)  loss_n_80: 6.5966 (7.6443)  loss_n_100: 7.1765 (8.1436)  triple_100: 0.0000 (0.2161)  triple_80: 0.0000 (0.1802)  triple_60: 0.0000 (0.5545)  triple_40: 0.0000 (0.2543)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1680/1724]  eta: 0:02:52  lr: 0.000040  loss: 26.8123 (31.3303)  loss_n_40: 5.8944 (6.4552)  loss_n_60: 6.9528 (7.9001)  loss_n_80: 6.5966 (7.6381)  loss_n_100: 7.1765 (8.1380)  triple_100: 0.0000 (0.2154)  triple_80: 0.0000 (0.1791)  triple_60: 0.0000 (0.5517)  triple_40: 0.0000 (0.2528)  time: 3.9236  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1690/1724]  eta: 0:02:13  lr: 0.000040  loss: 26.8369 (31.3084)  loss_n_40: 6.0043 (6.4526)  loss_n_60: 6.9771 (7.8951)  loss_n_80: 6.6166 (7.6333)  loss_n_100: 7.1756 (8.1332)  triple_100: 0.0000 (0.2147)  triple_80: 0.0000 (0.1780)  triple_60: 0.0000 (0.5501)  triple_40: 0.0000 (0.2513)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1700/1724]  eta: 0:01:34  lr: 0.000040  loss: 27.5604 (31.2867)  loss_n_40: 6.1130 (6.4509)  loss_n_60: 7.1304 (7.8911)  loss_n_80: 6.6561 (7.6282)  loss_n_100: 7.1751 (8.1282)  triple_100: 0.0000 (0.2139)  triple_80: 0.0000 (0.1770)  triple_60: 0.0000 (0.5472)  triple_40: 0.0000 (0.2502)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1710/1724]  eta: 0:00:54  lr: 0.000040  loss: 27.6865 (31.2655)  loss_n_40: 6.1477 (6.4485)  loss_n_60: 7.1557 (7.8863)  loss_n_80: 6.8110 (7.6234)  loss_n_100: 7.2242 (8.1235)  triple_100: 0.0000 (0.2129)  triple_80: 0.0000 (0.1766)  triple_60: 0.0000 (0.5456)  triple_40: 0.0000 (0.2487)  time: 3.9250  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:3]  [1720/1724]  eta: 0:00:15  lr: 0.000040  loss: 27.6807 (31.2440)  loss_n_40: 6.0429 (6.4460)  loss_n_60: 7.0830 (7.8814)  loss_n_80: 6.8827 (7.6192)  loss_n_100: 7.3473 (8.1192)  triple_100: 0.0000 (0.2118)  triple_80: 0.0000 (0.1756)  triple_60: 0.0000 (0.5436)  triple_40: 0.0000 (0.2472)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3]  [1723/1724]  eta: 0:00:03  lr: 0.000040  loss: 27.8241 (31.2410)  loss_n_40: 6.1413 (6.4461)  loss_n_60: 7.1557 (7.8805)  loss_n_80: 6.8922 (7.6182)  loss_n_100: 7.3473 (8.1179)  triple_100: 0.0000 (0.2114)  triple_80: 0.0000 (0.1752)  triple_60: 0.0000 (0.5437)  triple_40: 0.0000 (0.2480)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:3] Total time: 1:52:47 (3.9253 s / it)\n",
      "Averaged stats: lr: 0.000040  loss: 27.8241 (31.2410)  loss_n_40: 6.1413 (6.4461)  loss_n_60: 7.1557 (7.8805)  loss_n_80: 6.8922 (7.6182)  loss_n_100: 7.3473 (8.1179)  triple_100: 0.0000 (0.2114)  triple_80: 0.0000 (0.1752)  triple_60: 0.0000 (0.5437)  triple_40: 0.0000 (0.2480)\n",
      "Valid: [epoch:3]  [  0/845]  eta: 0:12:41  loss: 25.0981 (25.0981)  loss_n_40: 5.3836 (5.3836)  loss_n_60: 6.4204 (6.4204)  loss_n_80: 6.2388 (6.2388)  loss_n_100: 7.0553 (7.0553)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.9014  data: 0.5667  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:3]  [ 10/845]  eta: 0:05:21  loss: 28.5741 (27.8229)  loss_n_40: 6.2540 (6.1667)  loss_n_60: 7.1672 (7.0975)  loss_n_80: 7.0838 (6.9958)  loss_n_100: 7.6413 (7.5155)  triple_100: 0.0000 (0.0474)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3855  data: 0.0516  max mem: 46473\n",
      "Valid: [epoch:3]  [ 20/845]  eta: 0:04:57  loss: 27.2102 (27.2162)  loss_n_40: 6.1770 (6.1334)  loss_n_60: 7.1136 (7.0896)  loss_n_80: 6.6785 (6.7012)  loss_n_100: 7.2886 (7.2671)  triple_100: 0.0000 (0.0248)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [ 30/845]  eta: 0:04:47  loss: 25.7974 (26.8625)  loss_n_40: 6.1047 (6.0749)  loss_n_60: 7.0625 (7.0430)  loss_n_80: 6.0761 (6.5726)  loss_n_100: 6.8322 (7.1552)  triple_100: 0.0000 (0.0168)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [ 40/845]  eta: 0:04:40  loss: 25.9419 (26.9598)  loss_n_40: 6.0629 (6.0859)  loss_n_60: 7.0625 (7.0583)  loss_n_80: 6.1954 (6.6100)  loss_n_100: 6.8555 (7.1929)  triple_100: 0.0000 (0.0127)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [ 50/845]  eta: 0:04:34  loss: 27.5004 (27.0286)  loss_n_40: 6.0629 (6.0813)  loss_n_60: 7.0512 (7.0339)  loss_n_80: 6.9176 (6.6605)  loss_n_100: 7.3098 (7.2161)  triple_100: 0.0000 (0.0102)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0265)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [ 60/845]  eta: 0:04:29  loss: 27.2165 (27.3482)  loss_n_40: 6.0964 (6.0773)  loss_n_60: 6.9610 (7.0208)  loss_n_80: 6.9766 (6.7008)  loss_n_100: 7.3061 (7.2203)  triple_100: 0.0000 (0.0085)  triple_80: 0.0000 (0.0635)  triple_60: 0.0000 (0.2570)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [ 70/845]  eta: 0:04:25  loss: 27.2165 (27.4712)  loss_n_40: 6.2451 (6.1048)  loss_n_60: 7.0980 (7.0351)  loss_n_80: 7.1867 (6.7688)  loss_n_100: 7.2553 (7.2798)  triple_100: 0.0000 (0.0073)  triple_80: 0.0000 (0.0545)  triple_60: 0.0000 (0.2208)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [ 80/845]  eta: 0:04:21  loss: 27.1523 (27.6488)  loss_n_40: 6.2451 (6.0997)  loss_n_60: 7.0337 (7.0261)  loss_n_80: 6.8202 (6.7483)  loss_n_100: 7.0979 (7.2485)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.1100)  triple_60: 0.0000 (0.4098)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [ 90/845]  eta: 0:04:17  loss: 28.0944 (27.6901)  loss_n_40: 6.3278 (6.1259)  loss_n_60: 7.0337 (7.0374)  loss_n_80: 6.6977 (6.7789)  loss_n_100: 7.2831 (7.2794)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0979)  triple_60: 0.0000 (0.3648)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [100/845]  eta: 0:04:13  loss: 28.0944 (27.7503)  loss_n_40: 6.3278 (6.1388)  loss_n_60: 7.0911 (7.0410)  loss_n_80: 7.1405 (6.7873)  loss_n_100: 7.4441 (7.2743)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0882)  triple_60: 0.0000 (0.4155)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [110/845]  eta: 0:04:09  loss: 27.4685 (27.7903)  loss_n_40: 5.9312 (6.1452)  loss_n_60: 7.0519 (7.0534)  loss_n_80: 6.7873 (6.7769)  loss_n_100: 7.1272 (7.2634)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0803)  triple_60: 0.0000 (0.3975)  triple_40: 0.0000 (0.0689)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [120/845]  eta: 0:04:05  loss: 27.4429 (27.7906)  loss_n_40: 6.2106 (6.1492)  loss_n_60: 7.1028 (7.0534)  loss_n_80: 6.9308 (6.7913)  loss_n_100: 7.1283 (7.2776)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0737)  triple_60: 0.0000 (0.3779)  triple_40: 0.0000 (0.0632)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [130/845]  eta: 0:04:02  loss: 27.6839 (27.7473)  loss_n_40: 6.1748 (6.1435)  loss_n_60: 7.0959 (7.0511)  loss_n_80: 6.9744 (6.7901)  loss_n_100: 7.5646 (7.2832)  triple_100: 0.0000 (0.0040)  triple_80: 0.0000 (0.0680)  triple_60: 0.0000 (0.3491)  triple_40: 0.0000 (0.0583)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [140/845]  eta: 0:03:58  loss: 27.8445 (27.7177)  loss_n_40: 6.1042 (6.1419)  loss_n_60: 7.0401 (7.0524)  loss_n_80: 7.0754 (6.7946)  loss_n_100: 7.5444 (7.2834)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0632)  triple_60: 0.0000 (0.3243)  triple_40: 0.0000 (0.0542)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [150/845]  eta: 0:03:54  loss: 25.7656 (27.6454)  loss_n_40: 6.0318 (6.1394)  loss_n_60: 6.9895 (7.0528)  loss_n_80: 6.0918 (6.7695)  loss_n_100: 6.7650 (7.2677)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0590)  triple_60: 0.0000 (0.3028)  triple_40: 0.0000 (0.0506)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [160/845]  eta: 0:03:51  loss: 25.5965 (27.5674)  loss_n_40: 6.0241 (6.1283)  loss_n_60: 6.9486 (7.0409)  loss_n_80: 6.2562 (6.7542)  loss_n_100: 6.8030 (7.2540)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0554)  triple_60: 0.0000 (0.2840)  triple_40: 0.0000 (0.0475)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [170/845]  eta: 0:03:47  loss: 26.4998 (27.5553)  loss_n_40: 6.0273 (6.1356)  loss_n_60: 6.9920 (7.0479)  loss_n_80: 6.3029 (6.7448)  loss_n_100: 6.9345 (7.2476)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0521)  triple_60: 0.0000 (0.2785)  triple_40: 0.0000 (0.0447)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [180/845]  eta: 0:03:44  loss: 26.8690 (27.8072)  loss_n_40: 6.3061 (6.1355)  loss_n_60: 7.2056 (7.0515)  loss_n_80: 6.5829 (6.7405)  loss_n_100: 6.8371 (7.2399)  triple_100: 0.0000 (0.0230)  triple_80: 0.0000 (0.1104)  triple_60: 0.0000 (0.4492)  triple_40: 0.0000 (0.0570)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [190/845]  eta: 0:03:40  loss: 27.4028 (27.8701)  loss_n_40: 6.3061 (6.1411)  loss_n_60: 7.2373 (7.0590)  loss_n_80: 6.7168 (6.7429)  loss_n_100: 7.0936 (7.2437)  triple_100: 0.0000 (0.0218)  triple_80: 0.0000 (0.1046)  triple_60: 0.0000 (0.4961)  triple_40: 0.0000 (0.0607)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [200/845]  eta: 0:03:37  loss: 27.0287 (27.7922)  loss_n_40: 6.0048 (6.1346)  loss_n_60: 7.1446 (7.0552)  loss_n_80: 6.5283 (6.7243)  loss_n_100: 7.0936 (7.2287)  triple_100: 0.0000 (0.0208)  triple_80: 0.0000 (0.0994)  triple_60: 0.0000 (0.4714)  triple_40: 0.0000 (0.0577)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [210/845]  eta: 0:03:33  loss: 25.7375 (27.7460)  loss_n_40: 6.0654 (6.1339)  loss_n_60: 7.0968 (7.0546)  loss_n_80: 6.2003 (6.7162)  loss_n_100: 6.8378 (7.2228)  triple_100: 0.0000 (0.0198)  triple_80: 0.0000 (0.0947)  triple_60: 0.0000 (0.4490)  triple_40: 0.0000 (0.0550)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [220/845]  eta: 0:03:30  loss: 26.3324 (27.7405)  loss_n_40: 6.1615 (6.1356)  loss_n_60: 7.0968 (7.0547)  loss_n_80: 6.3358 (6.7204)  loss_n_100: 6.9414 (7.2266)  triple_100: 0.0000 (0.0189)  triple_80: 0.0000 (0.0904)  triple_60: 0.0000 (0.4415)  triple_40: 0.0000 (0.0525)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [230/845]  eta: 0:03:27  loss: 26.4744 (27.6998)  loss_n_40: 6.2733 (6.1324)  loss_n_60: 6.9366 (7.0500)  loss_n_80: 6.3167 (6.7162)  loss_n_100: 7.0647 (7.2241)  triple_100: 0.0000 (0.0181)  triple_80: 0.0000 (0.0865)  triple_60: 0.0000 (0.4224)  triple_40: 0.0000 (0.0502)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [240/845]  eta: 0:03:23  loss: 26.8606 (27.6738)  loss_n_40: 6.2009 (6.1306)  loss_n_60: 6.9992 (7.0489)  loss_n_80: 6.4081 (6.7146)  loss_n_100: 7.0765 (7.2265)  triple_100: 0.0000 (0.0173)  triple_80: 0.0000 (0.0829)  triple_60: 0.0000 (0.4048)  triple_40: 0.0000 (0.0481)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [250/845]  eta: 0:03:20  loss: 27.1680 (27.6611)  loss_n_40: 6.2009 (6.1292)  loss_n_60: 7.1429 (7.0503)  loss_n_80: 6.6648 (6.7192)  loss_n_100: 7.2007 (7.2296)  triple_100: 0.0000 (0.0182)  triple_80: 0.0000 (0.0796)  triple_60: 0.0000 (0.3887)  triple_40: 0.0000 (0.0462)  time: 0.3348  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:3]  [260/845]  eta: 0:03:16  loss: 27.9687 (27.6925)  loss_n_40: 6.2307 (6.1353)  loss_n_60: 7.1980 (7.0561)  loss_n_80: 6.9737 (6.7331)  loss_n_100: 7.3673 (7.2368)  triple_100: 0.0000 (0.0175)  triple_80: 0.0000 (0.0766)  triple_60: 0.0000 (0.3828)  triple_40: 0.0000 (0.0544)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [270/845]  eta: 0:03:13  loss: 27.9687 (27.6442)  loss_n_40: 6.2302 (6.1277)  loss_n_60: 7.0390 (7.0493)  loss_n_80: 6.8239 (6.7229)  loss_n_100: 7.1914 (7.2326)  triple_100: 0.0000 (0.0169)  triple_80: 0.0000 (0.0738)  triple_60: 0.0000 (0.3687)  triple_40: 0.0000 (0.0524)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [280/845]  eta: 0:03:10  loss: 27.1355 (27.6344)  loss_n_40: 6.0573 (6.1234)  loss_n_60: 7.0156 (7.0490)  loss_n_80: 6.5027 (6.7244)  loss_n_100: 7.1914 (7.2348)  triple_100: 0.0000 (0.0163)  triple_80: 0.0000 (0.0711)  triple_60: 0.0000 (0.3649)  triple_40: 0.0000 (0.0505)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [290/845]  eta: 0:03:06  loss: 27.1355 (27.6195)  loss_n_40: 5.8664 (6.1193)  loss_n_60: 6.9503 (7.0454)  loss_n_80: 6.5717 (6.7171)  loss_n_100: 7.1815 (7.2301)  triple_100: 0.0000 (0.0157)  triple_80: 0.0000 (0.0687)  triple_60: 0.0000 (0.3523)  triple_40: 0.0000 (0.0709)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [300/845]  eta: 0:03:03  loss: 25.4644 (27.5776)  loss_n_40: 5.8595 (6.1166)  loss_n_60: 6.7958 (7.0437)  loss_n_80: 6.1287 (6.7054)  loss_n_100: 6.7494 (7.2211)  triple_100: 0.0000 (0.0152)  triple_80: 0.0000 (0.0664)  triple_60: 0.0000 (0.3406)  triple_40: 0.0000 (0.0686)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [310/845]  eta: 0:02:59  loss: 25.7574 (27.6210)  loss_n_40: 6.1142 (6.1179)  loss_n_60: 6.9271 (7.0424)  loss_n_80: 6.1287 (6.7056)  loss_n_100: 6.7410 (7.2193)  triple_100: 0.0000 (0.0155)  triple_80: 0.0000 (0.0754)  triple_60: 0.0000 (0.3786)  triple_40: 0.0000 (0.0664)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [320/845]  eta: 0:02:56  loss: 27.7741 (27.6227)  loss_n_40: 6.1520 (6.1227)  loss_n_60: 7.1480 (7.0430)  loss_n_80: 6.9792 (6.7128)  loss_n_100: 7.3146 (7.2251)  triple_100: 0.0000 (0.0150)  triple_80: 0.0000 (0.0730)  triple_60: 0.0000 (0.3668)  triple_40: 0.0000 (0.0643)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [330/845]  eta: 0:02:53  loss: 27.6453 (27.5982)  loss_n_40: 5.8224 (6.1150)  loss_n_60: 6.6443 (7.0341)  loss_n_80: 6.8860 (6.7166)  loss_n_100: 7.3851 (7.2291)  triple_100: 0.0000 (0.0145)  triple_80: 0.0000 (0.0708)  triple_60: 0.0000 (0.3558)  triple_40: 0.0000 (0.0623)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [340/845]  eta: 0:02:49  loss: 26.2572 (27.6020)  loss_n_40: 5.8014 (6.1120)  loss_n_60: 6.7531 (7.0329)  loss_n_80: 6.7103 (6.7192)  loss_n_100: 7.1889 (7.2303)  triple_100: 0.0000 (0.0141)  triple_80: 0.0000 (0.0778)  triple_60: 0.0000 (0.3552)  triple_40: 0.0000 (0.0605)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [350/845]  eta: 0:02:46  loss: 27.1214 (27.5898)  loss_n_40: 5.9328 (6.1119)  loss_n_60: 6.9591 (7.0313)  loss_n_80: 6.8123 (6.7186)  loss_n_100: 7.1146 (7.2297)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0756)  triple_60: 0.0000 (0.3450)  triple_40: 0.0000 (0.0640)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [360/845]  eta: 0:02:42  loss: 28.1436 (27.5852)  loss_n_40: 6.0635 (6.1124)  loss_n_60: 7.0475 (7.0329)  loss_n_80: 7.0196 (6.7227)  loss_n_100: 7.4552 (7.2325)  triple_100: 0.0000 (0.0133)  triple_80: 0.0000 (0.0735)  triple_60: 0.0000 (0.3355)  triple_40: 0.0000 (0.0622)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [370/845]  eta: 0:02:39  loss: 28.1750 (27.5871)  loss_n_40: 6.1520 (6.1145)  loss_n_60: 7.0779 (7.0330)  loss_n_80: 7.0976 (6.7300)  loss_n_100: 7.5278 (7.2381)  triple_100: 0.0000 (0.0130)  triple_80: 0.0000 (0.0715)  triple_60: 0.0000 (0.3264)  triple_40: 0.0000 (0.0606)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [380/845]  eta: 0:02:36  loss: 27.7267 (27.5900)  loss_n_40: 6.2383 (6.1190)  loss_n_60: 7.1765 (7.0368)  loss_n_80: 7.0976 (6.7334)  loss_n_100: 7.6060 (7.2406)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0697)  triple_60: 0.0000 (0.3179)  triple_40: 0.0000 (0.0590)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [390/845]  eta: 0:02:32  loss: 28.3959 (27.6727)  loss_n_40: 6.3938 (6.1270)  loss_n_60: 7.2445 (7.0449)  loss_n_80: 6.6856 (6.7325)  loss_n_100: 7.0815 (7.2368)  triple_100: 0.0000 (0.0292)  triple_80: 0.0000 (0.0900)  triple_60: 0.0000 (0.3543)  triple_40: 0.0000 (0.0579)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [400/845]  eta: 0:02:29  loss: 27.5105 (27.6588)  loss_n_40: 6.2705 (6.1264)  loss_n_60: 7.2126 (7.0436)  loss_n_80: 6.6856 (6.7325)  loss_n_100: 7.0815 (7.2373)  triple_100: 0.0000 (0.0292)  triple_80: 0.0000 (0.0878)  triple_60: 0.0000 (0.3455)  triple_40: 0.0000 (0.0565)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [410/845]  eta: 0:02:26  loss: 27.5881 (27.6701)  loss_n_40: 6.1721 (6.1320)  loss_n_60: 7.0771 (7.0462)  loss_n_80: 6.9439 (6.7378)  loss_n_100: 7.4343 (7.2412)  triple_100: 0.0000 (0.0285)  triple_80: 0.0000 (0.0856)  triple_60: 0.0000 (0.3437)  triple_40: 0.0000 (0.0551)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [420/845]  eta: 0:02:22  loss: 27.7285 (27.6559)  loss_n_40: 6.1701 (6.1289)  loss_n_60: 7.0771 (7.0453)  loss_n_80: 6.9681 (6.7370)  loss_n_100: 7.3762 (7.2415)  triple_100: 0.0000 (0.0303)  triple_80: 0.0000 (0.0836)  triple_60: 0.0000 (0.3355)  triple_40: 0.0000 (0.0538)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [430/845]  eta: 0:02:19  loss: 26.2912 (27.6222)  loss_n_40: 6.0403 (6.1230)  loss_n_60: 7.0349 (7.0431)  loss_n_80: 6.5620 (6.7291)  loss_n_100: 7.0628 (7.2355)  triple_100: 0.0000 (0.0296)  triple_80: 0.0000 (0.0817)  triple_60: 0.0000 (0.3277)  triple_40: 0.0000 (0.0526)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [440/845]  eta: 0:02:15  loss: 26.0922 (27.6151)  loss_n_40: 6.1730 (6.1260)  loss_n_60: 7.1152 (7.0454)  loss_n_80: 6.4772 (6.7284)  loss_n_100: 7.0628 (7.2349)  triple_100: 0.0000 (0.0290)  triple_80: 0.0000 (0.0798)  triple_60: 0.0000 (0.3203)  triple_40: 0.0000 (0.0514)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [450/845]  eta: 0:02:12  loss: 26.4233 (27.5997)  loss_n_40: 6.1730 (6.1220)  loss_n_60: 7.1034 (7.0426)  loss_n_80: 6.6633 (6.7269)  loss_n_100: 7.1157 (7.2334)  triple_100: 0.0000 (0.0334)  triple_80: 0.0000 (0.0780)  triple_60: 0.0000 (0.3132)  triple_40: 0.0000 (0.0502)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [460/845]  eta: 0:02:09  loss: 25.9723 (27.5700)  loss_n_40: 5.9902 (6.1193)  loss_n_60: 6.9326 (7.0408)  loss_n_80: 6.2401 (6.7190)  loss_n_100: 6.8972 (7.2263)  triple_100: 0.0000 (0.0327)  triple_80: 0.0000 (0.0763)  triple_60: 0.0000 (0.3064)  triple_40: 0.0000 (0.0491)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [470/845]  eta: 0:02:05  loss: 26.7449 (27.5917)  loss_n_40: 6.1332 (6.1252)  loss_n_60: 7.1094 (7.0452)  loss_n_80: 6.5267 (6.7224)  loss_n_100: 6.9360 (7.2285)  triple_100: 0.0000 (0.0320)  triple_80: 0.0000 (0.0747)  triple_60: 0.0000 (0.3098)  triple_40: 0.0000 (0.0540)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [480/845]  eta: 0:02:02  loss: 27.4897 (27.5767)  loss_n_40: 6.1580 (6.1195)  loss_n_60: 7.1013 (7.0393)  loss_n_80: 6.8984 (6.7253)  loss_n_100: 7.3197 (7.2306)  triple_100: 0.0000 (0.0325)  triple_80: 0.0000 (0.0732)  triple_60: 0.0000 (0.3034)  triple_40: 0.0000 (0.0528)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [490/845]  eta: 0:01:59  loss: 26.7705 (27.5674)  loss_n_40: 5.9970 (6.1198)  loss_n_60: 6.9398 (7.0393)  loss_n_80: 7.0167 (6.7259)  loss_n_100: 7.1834 (7.2300)  triple_100: 0.0000 (0.0319)  triple_80: 0.0000 (0.0717)  triple_60: 0.0000 (0.2972)  triple_40: 0.0000 (0.0518)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [500/845]  eta: 0:01:55  loss: 26.3115 (27.5503)  loss_n_40: 5.9970 (6.1169)  loss_n_60: 6.8995 (7.0361)  loss_n_80: 6.7343 (6.7238)  loss_n_100: 7.1667 (7.2297)  triple_100: 0.0000 (0.0315)  triple_80: 0.0000 (0.0702)  triple_60: 0.0000 (0.2913)  triple_40: 0.0000 (0.0507)  time: 0.3345  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:3]  [510/845]  eta: 0:01:52  loss: 25.6153 (27.5290)  loss_n_40: 6.0632 (6.1149)  loss_n_60: 6.8483 (7.0354)  loss_n_80: 6.1523 (6.7182)  loss_n_100: 6.7825 (7.2254)  triple_100: 0.0000 (0.0309)  triple_80: 0.0000 (0.0689)  triple_60: 0.0000 (0.2856)  triple_40: 0.0000 (0.0497)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [520/845]  eta: 0:01:49  loss: 25.8544 (27.5385)  loss_n_40: 6.0847 (6.1153)  loss_n_60: 7.0927 (7.0354)  loss_n_80: 6.3611 (6.7160)  loss_n_100: 6.9763 (7.2230)  triple_100: 0.0000 (0.0303)  triple_80: 0.0000 (0.0675)  triple_60: 0.0000 (0.2907)  triple_40: 0.0000 (0.0602)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [530/845]  eta: 0:01:45  loss: 26.5270 (27.5340)  loss_n_40: 6.0057 (6.1132)  loss_n_60: 6.9275 (7.0338)  loss_n_80: 6.7091 (6.7200)  loss_n_100: 7.2500 (7.2267)  triple_100: 0.0000 (0.0297)  triple_80: 0.0000 (0.0663)  triple_60: 0.0000 (0.2852)  triple_40: 0.0000 (0.0590)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [540/845]  eta: 0:01:42  loss: 26.7875 (27.5454)  loss_n_40: 5.9995 (6.1149)  loss_n_60: 7.0131 (7.0351)  loss_n_80: 6.8993 (6.7180)  loss_n_100: 7.3881 (7.2262)  triple_100: 0.0000 (0.0292)  triple_80: 0.0000 (0.0651)  triple_60: 0.0000 (0.2929)  triple_40: 0.0000 (0.0641)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [550/845]  eta: 0:01:38  loss: 26.9136 (27.5513)  loss_n_40: 6.1253 (6.1155)  loss_n_60: 7.0719 (7.0347)  loss_n_80: 6.4563 (6.7165)  loss_n_100: 6.9148 (7.2225)  triple_100: 0.0000 (0.0286)  triple_80: 0.0000 (0.0657)  triple_60: 0.0000 (0.3049)  triple_40: 0.0000 (0.0630)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [560/845]  eta: 0:01:35  loss: 27.0045 (27.5401)  loss_n_40: 6.1453 (6.1140)  loss_n_60: 7.1305 (7.0336)  loss_n_80: 6.5671 (6.7154)  loss_n_100: 7.0828 (7.2232)  triple_100: 0.0000 (0.0281)  triple_80: 0.0000 (0.0645)  triple_60: 0.0000 (0.2994)  triple_40: 0.0000 (0.0618)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [570/845]  eta: 0:01:32  loss: 27.4241 (27.5515)  loss_n_40: 6.1453 (6.1180)  loss_n_60: 7.1584 (7.0370)  loss_n_80: 6.7492 (6.7167)  loss_n_100: 7.1332 (7.2235)  triple_100: 0.0000 (0.0276)  triple_80: 0.0000 (0.0638)  triple_60: 0.0000 (0.3040)  triple_40: 0.0000 (0.0608)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [580/845]  eta: 0:01:28  loss: 27.4241 (27.5412)  loss_n_40: 6.1047 (6.1155)  loss_n_60: 7.0873 (7.0350)  loss_n_80: 6.8009 (6.7164)  loss_n_100: 7.3556 (7.2239)  triple_100: 0.0000 (0.0291)  triple_80: 0.0000 (0.0627)  triple_60: 0.0000 (0.2988)  triple_40: 0.0000 (0.0597)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [590/845]  eta: 0:01:25  loss: 27.1135 (27.5366)  loss_n_40: 6.0456 (6.1151)  loss_n_60: 7.0389 (7.0358)  loss_n_80: 6.8009 (6.7176)  loss_n_100: 7.3556 (7.2254)  triple_100: 0.0000 (0.0286)  triple_80: 0.0000 (0.0616)  triple_60: 0.0000 (0.2937)  triple_40: 0.0000 (0.0587)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [600/845]  eta: 0:01:22  loss: 27.4622 (27.5400)  loss_n_40: 6.3357 (6.1193)  loss_n_60: 7.3409 (7.0403)  loss_n_80: 6.8545 (6.7186)  loss_n_100: 7.3543 (7.2265)  triple_100: 0.0000 (0.0281)  triple_80: 0.0000 (0.0606)  triple_60: 0.0000 (0.2889)  triple_40: 0.0000 (0.0577)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [610/845]  eta: 0:01:18  loss: 27.3470 (27.5316)  loss_n_40: 6.4435 (6.1203)  loss_n_60: 7.2888 (7.0417)  loss_n_80: 6.4463 (6.7159)  loss_n_100: 7.0854 (7.2256)  triple_100: 0.0000 (0.0276)  triple_80: 0.0000 (0.0596)  triple_60: 0.0000 (0.2841)  triple_40: 0.0000 (0.0568)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [620/845]  eta: 0:01:15  loss: 26.1176 (27.5061)  loss_n_40: 5.8398 (6.1135)  loss_n_60: 6.8132 (7.0356)  loss_n_80: 6.3760 (6.7120)  loss_n_100: 7.0143 (7.2237)  triple_100: 0.0000 (0.0272)  triple_80: 0.0000 (0.0586)  triple_60: 0.0000 (0.2795)  triple_40: 0.0000 (0.0559)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [630/845]  eta: 0:01:12  loss: 25.7834 (27.5111)  loss_n_40: 5.7010 (6.1132)  loss_n_60: 6.6629 (7.0352)  loss_n_80: 6.3941 (6.7066)  loss_n_100: 6.9567 (7.2185)  triple_100: 0.0000 (0.0270)  triple_80: 0.0000 (0.0643)  triple_60: 0.0000 (0.2914)  triple_40: 0.0000 (0.0550)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [640/845]  eta: 0:01:08  loss: 26.1166 (27.5180)  loss_n_40: 6.1066 (6.1133)  loss_n_60: 7.0250 (7.0347)  loss_n_80: 6.4369 (6.7074)  loss_n_100: 7.0867 (7.2184)  triple_100: 0.0000 (0.0266)  triple_80: 0.0000 (0.0633)  triple_60: 0.0000 (0.2937)  triple_40: 0.0000 (0.0606)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [650/845]  eta: 0:01:05  loss: 26.8909 (27.5112)  loss_n_40: 6.2833 (6.1137)  loss_n_60: 7.1444 (7.0358)  loss_n_80: 6.4906 (6.7060)  loss_n_100: 7.1747 (7.2183)  triple_100: 0.0000 (0.0262)  triple_80: 0.0000 (0.0623)  triple_60: 0.0000 (0.2892)  triple_40: 0.0000 (0.0597)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [660/845]  eta: 0:01:02  loss: 25.7516 (27.4874)  loss_n_40: 5.9171 (6.1094)  loss_n_60: 6.9881 (7.0321)  loss_n_80: 6.3459 (6.7013)  loss_n_100: 6.9573 (7.2139)  triple_100: 0.0000 (0.0258)  triple_80: 0.0000 (0.0613)  triple_60: 0.0000 (0.2849)  triple_40: 0.0000 (0.0587)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [670/845]  eta: 0:00:58  loss: 25.7516 (27.4691)  loss_n_40: 5.9072 (6.1067)  loss_n_60: 6.8955 (7.0306)  loss_n_80: 6.0880 (6.6973)  loss_n_100: 6.6409 (7.2102)  triple_100: 0.0000 (0.0254)  triple_80: 0.0000 (0.0604)  triple_60: 0.0000 (0.2806)  triple_40: 0.0000 (0.0579)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [680/845]  eta: 0:00:55  loss: 25.9937 (27.4532)  loss_n_40: 6.0350 (6.1050)  loss_n_60: 7.0186 (7.0296)  loss_n_80: 6.0880 (6.6925)  loss_n_100: 6.7404 (7.2074)  triple_100: 0.0000 (0.0256)  triple_80: 0.0000 (0.0595)  triple_60: 0.0000 (0.2765)  triple_40: 0.0000 (0.0570)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [690/845]  eta: 0:00:51  loss: 27.1371 (27.4608)  loss_n_40: 6.2586 (6.1079)  loss_n_60: 7.1846 (7.0318)  loss_n_80: 6.5041 (6.6973)  loss_n_100: 7.0444 (7.2101)  triple_100: 0.0000 (0.0253)  triple_80: 0.0000 (0.0587)  triple_60: 0.0000 (0.2725)  triple_40: 0.0000 (0.0574)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [700/845]  eta: 0:00:48  loss: 27.8738 (27.4840)  loss_n_40: 6.3345 (6.1156)  loss_n_60: 7.2788 (7.0371)  loss_n_80: 6.9230 (6.6998)  loss_n_100: 7.3469 (7.2115)  triple_100: 0.0000 (0.0249)  triple_80: 0.0000 (0.0578)  triple_60: 0.0000 (0.2765)  triple_40: 0.0000 (0.0607)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [710/845]  eta: 0:00:45  loss: 28.2996 (27.5092)  loss_n_40: 6.5785 (6.1208)  loss_n_60: 7.3403 (7.0404)  loss_n_80: 6.9230 (6.7032)  loss_n_100: 7.3150 (7.2136)  triple_100: 0.0000 (0.0246)  triple_80: 0.0000 (0.0570)  triple_60: 0.0000 (0.2897)  triple_40: 0.0000 (0.0598)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [720/845]  eta: 0:00:41  loss: 27.6469 (27.5040)  loss_n_40: 6.1951 (6.1188)  loss_n_60: 7.1144 (7.0394)  loss_n_80: 7.0708 (6.7040)  loss_n_100: 7.3392 (7.2154)  triple_100: 0.0000 (0.0255)  triple_80: 0.0000 (0.0562)  triple_60: 0.0000 (0.2857)  triple_40: 0.0000 (0.0590)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [730/845]  eta: 0:00:38  loss: 26.9863 (27.4947)  loss_n_40: 5.9712 (6.1170)  loss_n_60: 6.8427 (7.0373)  loss_n_80: 6.7748 (6.7036)  loss_n_100: 7.3392 (7.2160)  triple_100: 0.0000 (0.0254)  triple_80: 0.0000 (0.0555)  triple_60: 0.0000 (0.2818)  triple_40: 0.0000 (0.0582)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [740/845]  eta: 0:00:35  loss: 26.0960 (27.4813)  loss_n_40: 5.8480 (6.1138)  loss_n_60: 6.7547 (7.0348)  loss_n_80: 6.4539 (6.7025)  loss_n_100: 7.1740 (7.2150)  triple_100: 0.0000 (0.0250)  triple_80: 0.0000 (0.0547)  triple_60: 0.0000 (0.2780)  triple_40: 0.0000 (0.0574)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [750/845]  eta: 0:00:31  loss: 26.0712 (27.5123)  loss_n_40: 5.9885 (6.1116)  loss_n_60: 6.7660 (7.0336)  loss_n_80: 6.4894 (6.7040)  loss_n_100: 7.0500 (7.2164)  triple_100: 0.0000 (0.0247)  triple_80: 0.0000 (0.0628)  triple_60: 0.0000 (0.3025)  triple_40: 0.0000 (0.0567)  time: 0.3346  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:3]  [760/845]  eta: 0:00:28  loss: 27.5887 (27.5132)  loss_n_40: 6.2909 (6.1136)  loss_n_60: 7.2379 (7.0350)  loss_n_80: 6.8097 (6.7058)  loss_n_100: 7.1590 (7.2180)  triple_100: 0.0000 (0.0244)  triple_80: 0.0000 (0.0620)  triple_60: 0.0000 (0.2985)  triple_40: 0.0000 (0.0559)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [770/845]  eta: 0:00:25  loss: 27.2658 (27.5097)  loss_n_40: 6.2202 (6.1161)  loss_n_60: 7.2889 (7.0376)  loss_n_80: 6.7127 (6.7040)  loss_n_100: 7.1590 (7.2169)  triple_100: 0.0000 (0.0241)  triple_80: 0.0000 (0.0612)  triple_60: 0.0000 (0.2946)  triple_40: 0.0000 (0.0552)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [780/845]  eta: 0:00:21  loss: 27.1415 (27.5193)  loss_n_40: 6.2258 (6.1190)  loss_n_60: 7.2018 (7.0407)  loss_n_80: 6.7905 (6.7074)  loss_n_100: 7.3096 (7.2202)  triple_100: 0.0000 (0.0262)  triple_80: 0.0000 (0.0604)  triple_60: 0.0000 (0.2908)  triple_40: 0.0000 (0.0545)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [790/845]  eta: 0:00:18  loss: 27.1415 (27.5109)  loss_n_40: 6.2095 (6.1176)  loss_n_60: 7.1269 (7.0386)  loss_n_80: 6.7968 (6.7076)  loss_n_100: 7.3096 (7.2206)  triple_100: 0.0000 (0.0259)  triple_80: 0.0000 (0.0596)  triple_60: 0.0000 (0.2872)  triple_40: 0.0000 (0.0538)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [800/845]  eta: 0:00:15  loss: 26.0280 (27.5048)  loss_n_40: 5.9243 (6.1161)  loss_n_60: 6.8081 (7.0371)  loss_n_80: 6.5934 (6.7081)  loss_n_100: 7.2224 (7.2222)  triple_100: 0.0000 (0.0256)  triple_80: 0.0000 (0.0589)  triple_60: 0.0000 (0.2836)  triple_40: 0.0000 (0.0531)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [810/845]  eta: 0:00:11  loss: 26.6094 (27.5014)  loss_n_40: 6.1094 (6.1160)  loss_n_60: 6.9732 (7.0375)  loss_n_80: 6.5934 (6.7093)  loss_n_100: 7.2098 (7.2226)  triple_100: 0.0000 (0.0252)  triple_80: 0.0000 (0.0582)  triple_60: 0.0000 (0.2801)  triple_40: 0.0000 (0.0525)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [820/845]  eta: 0:00:08  loss: 27.6224 (27.5010)  loss_n_40: 6.1788 (6.1179)  loss_n_60: 7.0750 (7.0388)  loss_n_80: 6.7256 (6.7102)  loss_n_100: 7.2098 (7.2229)  triple_100: 0.0000 (0.0249)  triple_80: 0.0000 (0.0575)  triple_60: 0.0000 (0.2767)  triple_40: 0.0000 (0.0521)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [830/845]  eta: 0:00:05  loss: 27.6175 (27.4978)  loss_n_40: 6.0286 (6.1185)  loss_n_60: 7.0210 (7.0386)  loss_n_80: 6.7488 (6.7108)  loss_n_100: 7.2818 (7.2238)  triple_100: 0.0000 (0.0246)  triple_80: 0.0000 (0.0568)  triple_60: 0.0000 (0.2733)  triple_40: 0.0000 (0.0515)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [840/845]  eta: 0:00:01  loss: 27.5818 (27.5037)  loss_n_40: 6.1003 (6.1207)  loss_n_60: 7.0429 (7.0405)  loss_n_80: 6.7751 (6.7135)  loss_n_100: 7.4771 (7.2263)  triple_100: 0.0000 (0.0256)  triple_80: 0.0000 (0.0561)  triple_60: 0.0000 (0.2701)  triple_40: 0.0000 (0.0509)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3]  [844/845]  eta: 0:00:00  loss: 27.5293 (27.5036)  loss_n_40: 6.1003 (6.1213)  loss_n_60: 7.0429 (7.0406)  loss_n_80: 6.7751 (6.7140)  loss_n_100: 7.4771 (7.2269)  triple_100: 0.0000 (0.0255)  triple_80: 0.0000 (0.0558)  triple_60: 0.0000 (0.2688)  triple_40: 0.0000 (0.0506)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:3] Total time: 0:04:43 (0.3353 s / it)\n",
      "Averaged stats: loss: 27.5293 (27.5036)  loss_n_40: 6.1003 (6.1213)  loss_n_60: 7.0429 (7.0406)  loss_n_80: 6.7751 (6.7140)  loss_n_100: 7.4771 (7.2269)  triple_100: 0.0000 (0.0255)  triple_80: 0.0000 (0.0558)  triple_60: 0.0000 (0.2688)  triple_40: 0.0000 (0.0506)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_3_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 7.227%\n",
      "Min loss_n_100: 4.783\n",
      "Best Epoch: 0.000\n",
      "Train: [epoch:4]  [   0/1724]  eta: 2:00:39  lr: 0.000060  loss: 27.9914 (27.9914)  loss_n_40: 6.0476 (6.0476)  loss_n_60: 7.0671 (7.0671)  loss_n_80: 7.1782 (7.1782)  loss_n_100: 7.6985 (7.6985)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1991  data: 0.4367  max mem: 46473\n",
      "Train: [epoch:4]  [  10/1724]  eta: 1:52:53  lr: 0.000060  loss: 27.8419 (27.6024)  loss_n_40: 6.0476 (6.1426)  loss_n_60: 7.0671 (7.0860)  loss_n_80: 6.9218 (6.8932)  loss_n_100: 7.2706 (7.3375)  triple_100: 0.0000 (0.0416)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1016)  triple_40: 0.0000 (0.0000)  time: 3.9518  data: 0.0399  max mem: 46473\n",
      "Train: [epoch:4]  [  20/1724]  eta: 1:51:52  lr: 0.000060  loss: 27.0478 (27.2905)  loss_n_40: 5.9826 (6.0833)  loss_n_60: 7.0004 (7.0494)  loss_n_80: 6.7348 (6.8131)  loss_n_100: 7.2332 (7.2697)  triple_100: 0.0000 (0.0218)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0532)  triple_40: 0.0000 (0.0000)  time: 3.9261  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [  30/1724]  eta: 1:51:04  lr: 0.000060  loss: 27.0421 (27.3522)  loss_n_40: 6.0023 (6.0778)  loss_n_60: 6.9678 (7.0341)  loss_n_80: 6.7124 (6.8111)  loss_n_100: 7.2332 (7.2557)  triple_100: 0.0000 (0.0502)  triple_80: 0.0000 (0.0254)  triple_60: 0.0000 (0.0977)  triple_40: 0.0000 (0.0000)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [  40/1724]  eta: 1:50:21  lr: 0.000060  loss: 27.0351 (27.3282)  loss_n_40: 6.0843 (6.0747)  loss_n_60: 6.9678 (7.0233)  loss_n_80: 6.7599 (6.8031)  loss_n_100: 7.2483 (7.2489)  triple_100: 0.0000 (0.0498)  triple_80: 0.0000 (0.0192)  triple_60: 0.0000 (0.0739)  triple_40: 0.0000 (0.0353)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [  50/1724]  eta: 1:49:38  lr: 0.000060  loss: 27.2021 (27.3310)  loss_n_40: 6.0937 (6.0642)  loss_n_60: 7.0280 (7.0253)  loss_n_80: 6.6396 (6.7610)  loss_n_100: 7.1136 (7.2219)  triple_100: 0.0000 (0.0433)  triple_80: 0.0000 (0.0355)  triple_60: 0.0000 (0.1515)  triple_40: 0.0000 (0.0284)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [  60/1724]  eta: 1:48:58  lr: 0.000060  loss: 27.3444 (27.2770)  loss_n_40: 6.0579 (6.0522)  loss_n_60: 7.0280 (7.0175)  loss_n_80: 6.5753 (6.7416)  loss_n_100: 7.0985 (7.2162)  triple_100: 0.0000 (0.0368)  triple_80: 0.0000 (0.0297)  triple_60: 0.0000 (0.1592)  triple_40: 0.0000 (0.0238)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [  70/1724]  eta: 1:48:17  lr: 0.000060  loss: 27.0943 (27.3514)  loss_n_40: 5.9357 (6.0390)  loss_n_60: 6.8824 (7.0148)  loss_n_80: 6.6367 (6.7322)  loss_n_100: 7.0941 (7.1978)  triple_100: 0.0000 (0.0344)  triple_80: 0.0000 (0.0288)  triple_60: 0.0000 (0.2548)  triple_40: 0.0000 (0.0496)  time: 3.9243  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [  80/1724]  eta: 1:47:37  lr: 0.000060  loss: 26.5992 (27.2625)  loss_n_40: 5.9013 (6.0212)  loss_n_60: 6.9121 (7.0059)  loss_n_80: 6.6096 (6.7008)  loss_n_100: 7.0328 (7.1733)  triple_100: 0.0000 (0.0454)  triple_80: 0.0000 (0.0326)  triple_60: 0.0000 (0.2260)  triple_40: 0.0000 (0.0574)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [  90/1724]  eta: 1:46:56  lr: 0.000060  loss: 26.3987 (27.1787)  loss_n_40: 5.8262 (6.0022)  loss_n_60: 6.9029 (6.9902)  loss_n_80: 6.5432 (6.6848)  loss_n_100: 7.0307 (7.1638)  triple_100: 0.0000 (0.0416)  triple_80: 0.0000 (0.0290)  triple_60: 0.0000 (0.2130)  triple_40: 0.0000 (0.0541)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 100/1724]  eta: 1:46:17  lr: 0.000060  loss: 26.8973 (27.3092)  loss_n_40: 5.9744 (6.0070)  loss_n_60: 6.9046 (6.9977)  loss_n_80: 6.5792 (6.6922)  loss_n_100: 7.1348 (7.1736)  triple_100: 0.0000 (0.0375)  triple_80: 0.0000 (0.0272)  triple_60: 0.0000 (0.2950)  triple_40: 0.0000 (0.0791)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 110/1724]  eta: 1:45:36  lr: 0.000060  loss: 27.0950 (27.2943)  loss_n_40: 6.0356 (6.0052)  loss_n_60: 7.0995 (7.0027)  loss_n_80: 6.6882 (6.6858)  loss_n_100: 7.2217 (7.1716)  triple_100: 0.0000 (0.0412)  triple_80: 0.0000 (0.0318)  triple_60: 0.0000 (0.2841)  triple_40: 0.0000 (0.0719)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 120/1724]  eta: 1:44:57  lr: 0.000060  loss: 26.8224 (27.2460)  loss_n_40: 5.9362 (5.9984)  loss_n_60: 7.0136 (7.0042)  loss_n_80: 6.5924 (6.6793)  loss_n_100: 7.1574 (7.1697)  triple_100: 0.0000 (0.0386)  triple_80: 0.0000 (0.0292)  triple_60: 0.0000 (0.2606)  triple_40: 0.0000 (0.0660)  time: 3.9222  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [ 130/1724]  eta: 1:44:17  lr: 0.000060  loss: 26.3450 (27.1380)  loss_n_40: 5.8020 (5.9737)  loss_n_60: 6.9288 (6.9890)  loss_n_80: 6.5038 (6.6562)  loss_n_100: 7.0126 (7.1498)  triple_100: 0.0000 (0.0388)  triple_80: 0.0000 (0.0270)  triple_60: 0.0000 (0.2425)  triple_40: 0.0000 (0.0609)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 140/1724]  eta: 1:43:37  lr: 0.000060  loss: 25.9917 (27.1027)  loss_n_40: 5.7628 (5.9664)  loss_n_60: 6.7855 (6.9848)  loss_n_80: 6.4459 (6.6438)  loss_n_100: 6.9322 (7.1355)  triple_100: 0.0000 (0.0361)  triple_80: 0.0000 (0.0250)  triple_60: 0.0000 (0.2253)  triple_40: 0.0000 (0.0857)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 150/1724]  eta: 1:42:58  lr: 0.000060  loss: 26.5780 (27.2003)  loss_n_40: 5.8605 (5.9721)  loss_n_60: 6.9811 (6.9877)  loss_n_80: 6.4898 (6.6477)  loss_n_100: 7.0416 (7.1384)  triple_100: 0.0000 (0.0631)  triple_80: 0.0000 (0.0328)  triple_60: 0.0000 (0.2547)  triple_40: 0.0000 (0.1039)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 160/1724]  eta: 1:42:18  lr: 0.000060  loss: 27.1990 (27.1921)  loss_n_40: 6.0035 (5.9767)  loss_n_60: 7.0405 (6.9951)  loss_n_80: 6.7023 (6.6439)  loss_n_100: 7.1864 (7.1362)  triple_100: 0.0000 (0.0591)  triple_80: 0.0000 (0.0308)  triple_60: 0.0000 (0.2527)  triple_40: 0.0000 (0.0975)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 170/1724]  eta: 1:41:39  lr: 0.000060  loss: 26.1674 (27.1263)  loss_n_40: 5.8295 (5.9613)  loss_n_60: 6.9826 (6.9827)  loss_n_80: 6.4766 (6.6333)  loss_n_100: 7.1088 (7.1301)  triple_100: 0.0000 (0.0587)  triple_80: 0.0000 (0.0290)  triple_60: 0.0000 (0.2395)  triple_40: 0.0000 (0.0918)  time: 3.9214  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [ 180/1724]  eta: 1:40:59  lr: 0.000060  loss: 26.3187 (27.1285)  loss_n_40: 5.8157 (5.9683)  loss_n_60: 6.8781 (6.9880)  loss_n_80: 6.6086 (6.6391)  loss_n_100: 7.1547 (7.1359)  triple_100: 0.0000 (0.0561)  triple_80: 0.0000 (0.0274)  triple_60: 0.0000 (0.2265)  triple_40: 0.0000 (0.0871)  time: 3.9225  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [ 190/1724]  eta: 1:40:20  lr: 0.000060  loss: 27.0593 (27.1432)  loss_n_40: 6.1016 (5.9712)  loss_n_60: 7.0242 (6.9846)  loss_n_80: 6.7480 (6.6394)  loss_n_100: 7.1789 (7.1354)  triple_100: 0.0000 (0.0595)  triple_80: 0.0000 (0.0272)  triple_60: 0.0000 (0.2422)  triple_40: 0.0000 (0.0837)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 200/1724]  eta: 1:39:40  lr: 0.000060  loss: 27.0800 (27.1317)  loss_n_40: 6.0830 (5.9737)  loss_n_60: 7.0242 (6.9842)  loss_n_80: 6.7786 (6.6440)  loss_n_100: 7.0651 (7.1343)  triple_100: 0.0000 (0.0569)  triple_80: 0.0000 (0.0258)  triple_60: 0.0000 (0.2334)  triple_40: 0.0000 (0.0795)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 210/1724]  eta: 1:39:01  lr: 0.000060  loss: 26.9505 (27.1408)  loss_n_40: 5.9698 (5.9728)  loss_n_60: 7.0171 (6.9839)  loss_n_80: 6.7231 (6.6384)  loss_n_100: 7.0379 (7.1284)  triple_100: 0.0000 (0.0554)  triple_80: 0.0000 (0.0304)  triple_60: 0.0000 (0.2556)  triple_40: 0.0000 (0.0757)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 220/1724]  eta: 1:38:21  lr: 0.000060  loss: 26.6774 (27.1679)  loss_n_40: 5.9397 (5.9711)  loss_n_60: 6.9804 (6.9828)  loss_n_80: 6.5192 (6.6390)  loss_n_100: 7.0944 (7.1298)  triple_100: 0.0000 (0.0702)  triple_80: 0.0000 (0.0302)  triple_60: 0.0000 (0.2618)  triple_40: 0.0000 (0.0830)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 230/1724]  eta: 1:37:42  lr: 0.000060  loss: 26.2186 (27.1289)  loss_n_40: 5.8693 (5.9644)  loss_n_60: 6.8633 (6.9764)  loss_n_80: 6.4967 (6.6311)  loss_n_100: 7.0548 (7.1257)  triple_100: 0.0000 (0.0697)  triple_80: 0.0000 (0.0289)  triple_60: 0.0000 (0.2533)  triple_40: 0.0000 (0.0794)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 240/1724]  eta: 1:37:02  lr: 0.000060  loss: 25.9754 (27.0781)  loss_n_40: 5.7650 (5.9564)  loss_n_60: 6.7334 (6.9675)  loss_n_80: 6.3972 (6.6214)  loss_n_100: 7.0306 (7.1187)  triple_100: 0.0000 (0.0675)  triple_80: 0.0000 (0.0277)  triple_60: 0.0000 (0.2428)  triple_40: 0.0000 (0.0761)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 250/1724]  eta: 1:36:23  lr: 0.000060  loss: 26.0300 (27.0412)  loss_n_40: 5.7256 (5.9479)  loss_n_60: 6.6959 (6.9570)  loss_n_80: 6.4373 (6.6155)  loss_n_100: 7.0394 (7.1170)  triple_100: 0.0000 (0.0653)  triple_80: 0.0000 (0.0266)  triple_60: 0.0000 (0.2388)  triple_40: 0.0000 (0.0731)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 260/1724]  eta: 1:35:44  lr: 0.000060  loss: 26.0533 (26.9955)  loss_n_40: 5.7284 (5.9435)  loss_n_60: 6.7276 (6.9522)  loss_n_80: 6.3762 (6.6028)  loss_n_100: 6.9759 (7.1035)  triple_100: 0.0000 (0.0628)  triple_80: 0.0000 (0.0256)  triple_60: 0.0000 (0.2348)  triple_40: 0.0000 (0.0703)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 270/1724]  eta: 1:35:04  lr: 0.000060  loss: 25.8572 (26.9739)  loss_n_40: 5.8305 (5.9420)  loss_n_60: 6.8382 (6.9485)  loss_n_80: 6.3762 (6.5972)  loss_n_100: 6.7732 (7.0952)  triple_100: 0.0000 (0.0618)  triple_80: 0.0000 (0.0246)  triple_60: 0.0000 (0.2320)  triple_40: 0.0000 (0.0726)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 280/1724]  eta: 1:34:25  lr: 0.000060  loss: 25.7747 (26.9196)  loss_n_40: 5.8168 (5.9352)  loss_n_60: 6.8366 (6.9421)  loss_n_80: 6.3080 (6.5827)  loss_n_100: 6.8366 (7.0823)  triple_100: 0.0000 (0.0598)  triple_80: 0.0000 (0.0238)  triple_60: 0.0000 (0.2238)  triple_40: 0.0000 (0.0700)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 290/1724]  eta: 1:33:46  lr: 0.000060  loss: 25.3064 (26.8834)  loss_n_40: 5.8386 (5.9331)  loss_n_60: 6.7771 (6.9386)  loss_n_80: 6.2756 (6.5733)  loss_n_100: 6.7875 (7.0703)  triple_100: 0.0000 (0.0584)  triple_80: 0.0000 (0.0239)  triple_60: 0.0000 (0.2183)  triple_40: 0.0000 (0.0676)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 300/1724]  eta: 1:33:06  lr: 0.000060  loss: 25.4920 (26.8467)  loss_n_40: 5.8582 (5.9297)  loss_n_60: 6.7716 (6.9317)  loss_n_80: 6.2756 (6.5669)  loss_n_100: 6.7273 (7.0610)  triple_100: 0.0000 (0.0581)  triple_80: 0.0000 (0.0231)  triple_60: 0.0000 (0.2111)  triple_40: 0.0000 (0.0654)  time: 3.9231  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 310/1724]  eta: 1:32:27  lr: 0.000060  loss: 25.4026 (26.8232)  loss_n_40: 5.8414 (5.9224)  loss_n_60: 6.7088 (6.9246)  loss_n_80: 6.2116 (6.5529)  loss_n_100: 6.6146 (7.0452)  triple_100: 0.0000 (0.0586)  triple_80: 0.0000 (0.0251)  triple_60: 0.0000 (0.2256)  triple_40: 0.0000 (0.0687)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 320/1724]  eta: 1:31:48  lr: 0.000060  loss: 25.5011 (26.8258)  loss_n_40: 5.8681 (5.9188)  loss_n_60: 6.8160 (6.9225)  loss_n_80: 6.2251 (6.5439)  loss_n_100: 6.6025 (7.0359)  triple_100: 0.0000 (0.0638)  triple_80: 0.0000 (0.0313)  triple_60: 0.0000 (0.2419)  triple_40: 0.0000 (0.0678)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 330/1724]  eta: 1:31:09  lr: 0.000060  loss: 26.1216 (26.8052)  loss_n_40: 5.9237 (5.9181)  loss_n_60: 6.9204 (6.9221)  loss_n_80: 6.2837 (6.5395)  loss_n_100: 6.8107 (7.0326)  triple_100: 0.0000 (0.0623)  triple_80: 0.0000 (0.0303)  triple_60: 0.0000 (0.2346)  triple_40: 0.0000 (0.0657)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 340/1724]  eta: 1:30:29  lr: 0.000060  loss: 26.2842 (26.7922)  loss_n_40: 5.9776 (5.9194)  loss_n_60: 6.9900 (6.9217)  loss_n_80: 6.3860 (6.5381)  loss_n_100: 6.9067 (7.0296)  triple_100: 0.0000 (0.0624)  triple_80: 0.0000 (0.0294)  triple_60: 0.0000 (0.2277)  triple_40: 0.0000 (0.0638)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 350/1724]  eta: 1:29:50  lr: 0.000060  loss: 25.6350 (26.7619)  loss_n_40: 5.8230 (5.9172)  loss_n_60: 6.7252 (6.9160)  loss_n_80: 6.3386 (6.5278)  loss_n_100: 6.7858 (7.0182)  triple_100: 0.0000 (0.0608)  triple_80: 0.0000 (0.0286)  triple_60: 0.0000 (0.2286)  triple_40: 0.0000 (0.0648)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 360/1724]  eta: 1:29:11  lr: 0.000060  loss: 25.2377 (26.7219)  loss_n_40: 5.6544 (5.9105)  loss_n_60: 6.6399 (6.9096)  loss_n_80: 6.1634 (6.5200)  loss_n_100: 6.6875 (7.0096)  triple_100: 0.0000 (0.0591)  triple_80: 0.0000 (0.0278)  triple_60: 0.0000 (0.2223)  triple_40: 0.0000 (0.0630)  time: 3.9221  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [ 370/1724]  eta: 1:28:32  lr: 0.000060  loss: 25.2377 (26.7210)  loss_n_40: 5.7119 (5.9086)  loss_n_60: 6.6665 (6.9065)  loss_n_80: 6.1362 (6.5103)  loss_n_100: 6.6849 (7.0010)  triple_100: 0.0000 (0.0679)  triple_80: 0.0000 (0.0285)  triple_60: 0.0000 (0.2281)  triple_40: 0.0000 (0.0700)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 380/1724]  eta: 1:27:52  lr: 0.000060  loss: 25.9443 (26.7089)  loss_n_40: 5.9161 (5.9061)  loss_n_60: 6.6664 (6.9007)  loss_n_80: 6.3717 (6.5061)  loss_n_100: 6.7851 (6.9978)  triple_100: 0.0000 (0.0690)  triple_80: 0.0000 (0.0305)  triple_60: 0.0000 (0.2304)  triple_40: 0.0000 (0.0682)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 390/1724]  eta: 1:27:13  lr: 0.000060  loss: 25.9443 (26.6886)  loss_n_40: 5.8863 (5.9038)  loss_n_60: 6.6664 (6.8960)  loss_n_80: 6.2725 (6.5015)  loss_n_100: 6.7851 (6.9925)  triple_100: 0.0000 (0.0685)  triple_80: 0.0000 (0.0297)  triple_60: 0.0000 (0.2301)  triple_40: 0.0000 (0.0664)  time: 3.9236  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 400/1724]  eta: 1:26:34  lr: 0.000060  loss: 25.6293 (26.6631)  loss_n_40: 5.7435 (5.8994)  loss_n_60: 6.6963 (6.8923)  loss_n_80: 6.1364 (6.4924)  loss_n_100: 6.6602 (6.9837)  triple_100: 0.0000 (0.0674)  triple_80: 0.0000 (0.0293)  triple_60: 0.0000 (0.2338)  triple_40: 0.0000 (0.0648)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 410/1724]  eta: 1:25:54  lr: 0.000060  loss: 25.7102 (26.6554)  loss_n_40: 5.7163 (5.8937)  loss_n_60: 6.6660 (6.8856)  loss_n_80: 6.1971 (6.4885)  loss_n_100: 6.7653 (6.9803)  triple_100: 0.0000 (0.0719)  triple_80: 0.0000 (0.0298)  triple_60: 0.0000 (0.2413)  triple_40: 0.0000 (0.0644)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 420/1724]  eta: 1:25:15  lr: 0.000060  loss: 25.5869 (26.6214)  loss_n_40: 5.7163 (5.8866)  loss_n_60: 6.6660 (6.8797)  loss_n_80: 6.2925 (6.4835)  loss_n_100: 6.7745 (6.9736)  triple_100: 0.0000 (0.0704)  triple_80: 0.0000 (0.0291)  triple_60: 0.0000 (0.2356)  triple_40: 0.0000 (0.0629)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 430/1724]  eta: 1:24:36  lr: 0.000060  loss: 25.3681 (26.6077)  loss_n_40: 5.7395 (5.8855)  loss_n_60: 6.7057 (6.8773)  loss_n_80: 6.2909 (6.4786)  loss_n_100: 6.7398 (6.9681)  triple_100: 0.0000 (0.0696)  triple_80: 0.0000 (0.0284)  triple_60: 0.0000 (0.2358)  triple_40: 0.0000 (0.0643)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 440/1724]  eta: 1:23:57  lr: 0.000060  loss: 25.9718 (26.5927)  loss_n_40: 5.8536 (5.8842)  loss_n_60: 6.8365 (6.8764)  loss_n_80: 6.3762 (6.4790)  loss_n_100: 6.7461 (6.9639)  triple_100: 0.0000 (0.0681)  triple_80: 0.0000 (0.0278)  triple_60: 0.0000 (0.2305)  triple_40: 0.0000 (0.0629)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 450/1724]  eta: 1:23:17  lr: 0.000060  loss: 25.5731 (26.5610)  loss_n_40: 5.7781 (5.8780)  loss_n_60: 6.6796 (6.8699)  loss_n_80: 6.3762 (6.4740)  loss_n_100: 6.7152 (6.9561)  triple_100: 0.0000 (0.0666)  triple_80: 0.0000 (0.0271)  triple_60: 0.0000 (0.2267)  triple_40: 0.0000 (0.0627)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 460/1724]  eta: 1:22:38  lr: 0.000060  loss: 25.3259 (26.5352)  loss_n_40: 5.6093 (5.8746)  loss_n_60: 6.6572 (6.8675)  loss_n_80: 6.2778 (6.4693)  loss_n_100: 6.6118 (6.9478)  triple_100: 0.0000 (0.0651)  triple_80: 0.0000 (0.0265)  triple_60: 0.0000 (0.2229)  triple_40: 0.0000 (0.0614)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 470/1724]  eta: 1:21:59  lr: 0.000060  loss: 25.5687 (26.5145)  loss_n_40: 5.7118 (5.8727)  loss_n_60: 6.6572 (6.8627)  loss_n_80: 6.3295 (6.4667)  loss_n_100: 6.6545 (6.9437)  triple_100: 0.0000 (0.0644)  triple_80: 0.0000 (0.0260)  triple_60: 0.0000 (0.2182)  triple_40: 0.0000 (0.0601)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 480/1724]  eta: 1:21:20  lr: 0.000060  loss: 25.0583 (26.4795)  loss_n_40: 5.5968 (5.8649)  loss_n_60: 6.5144 (6.8549)  loss_n_80: 6.2703 (6.4605)  loss_n_100: 6.6561 (6.9350)  triple_100: 0.0000 (0.0636)  triple_80: 0.0000 (0.0266)  triple_60: 0.0000 (0.2153)  triple_40: 0.0000 (0.0588)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 490/1724]  eta: 1:20:40  lr: 0.000060  loss: 25.0583 (26.4585)  loss_n_40: 5.5747 (5.8611)  loss_n_60: 6.5719 (6.8512)  loss_n_80: 6.2863 (6.4565)  loss_n_100: 6.6010 (6.9273)  triple_100: 0.0000 (0.0624)  triple_80: 0.0000 (0.0260)  triple_60: 0.0000 (0.2154)  triple_40: 0.0000 (0.0586)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 500/1724]  eta: 1:20:01  lr: 0.000060  loss: 25.5554 (26.4594)  loss_n_40: 5.6933 (5.8593)  loss_n_60: 6.7064 (6.8487)  loss_n_80: 6.3128 (6.4524)  loss_n_100: 6.6048 (6.9212)  triple_100: 0.0000 (0.0638)  triple_80: 0.0000 (0.0286)  triple_60: 0.0000 (0.2232)  triple_40: 0.0000 (0.0622)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 510/1724]  eta: 1:19:22  lr: 0.000060  loss: 26.1864 (26.4641)  loss_n_40: 5.7841 (5.8567)  loss_n_60: 6.7367 (6.8491)  loss_n_80: 6.4225 (6.4554)  loss_n_100: 6.7987 (6.9231)  triple_100: 0.0000 (0.0651)  triple_80: 0.0000 (0.0281)  triple_60: 0.0000 (0.2256)  triple_40: 0.0000 (0.0610)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 520/1724]  eta: 1:18:42  lr: 0.000060  loss: 26.0101 (26.4487)  loss_n_40: 5.7841 (5.8531)  loss_n_60: 6.8684 (6.8460)  loss_n_80: 6.4135 (6.4525)  loss_n_100: 6.8653 (6.9221)  triple_100: 0.0000 (0.0660)  triple_80: 0.0000 (0.0275)  triple_60: 0.0000 (0.2217)  triple_40: 0.0000 (0.0599)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 530/1724]  eta: 1:18:03  lr: 0.000060  loss: 25.8042 (26.4319)  loss_n_40: 5.7314 (5.8511)  loss_n_60: 6.6751 (6.8433)  loss_n_80: 6.3987 (6.4499)  loss_n_100: 6.8233 (6.9192)  triple_100: 0.0000 (0.0647)  triple_80: 0.0000 (0.0270)  triple_60: 0.0000 (0.2180)  triple_40: 0.0000 (0.0588)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 540/1724]  eta: 1:17:24  lr: 0.000060  loss: 25.2775 (26.4129)  loss_n_40: 5.6168 (5.8449)  loss_n_60: 6.6249 (6.8380)  loss_n_80: 5.9672 (6.4425)  loss_n_100: 6.5513 (6.9122)  triple_100: 0.0000 (0.0652)  triple_80: 0.0000 (0.0285)  triple_60: 0.0000 (0.2239)  triple_40: 0.0000 (0.0577)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 550/1724]  eta: 1:16:45  lr: 0.000060  loss: 24.4279 (26.3861)  loss_n_40: 5.5117 (5.8408)  loss_n_60: 6.5715 (6.8343)  loss_n_80: 5.9348 (6.4344)  loss_n_100: 6.5010 (6.9038)  triple_100: 0.0000 (0.0640)  triple_80: 0.0000 (0.0280)  triple_60: 0.0000 (0.2242)  triple_40: 0.0000 (0.0567)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 560/1724]  eta: 1:16:05  lr: 0.000060  loss: 25.4166 (26.3757)  loss_n_40: 5.6980 (5.8405)  loss_n_60: 6.6308 (6.8322)  loss_n_80: 6.2461 (6.4351)  loss_n_100: 6.6263 (6.9018)  triple_100: 0.0000 (0.0629)  triple_80: 0.0000 (0.0275)  triple_60: 0.0000 (0.2202)  triple_40: 0.0000 (0.0556)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 570/1724]  eta: 1:15:26  lr: 0.000060  loss: 25.2673 (26.3522)  loss_n_40: 5.6989 (5.8379)  loss_n_60: 6.6308 (6.8292)  loss_n_80: 6.2496 (6.4266)  loss_n_100: 6.6391 (6.8933)  triple_100: 0.0000 (0.0624)  triple_80: 0.0000 (0.0270)  triple_60: 0.0000 (0.2171)  triple_40: 0.0000 (0.0586)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 580/1724]  eta: 1:14:47  lr: 0.000060  loss: 24.8080 (26.3462)  loss_n_40: 5.6286 (5.8358)  loss_n_60: 6.6282 (6.8267)  loss_n_80: 6.0811 (6.4249)  loss_n_100: 6.3643 (6.8879)  triple_100: 0.0000 (0.0617)  triple_80: 0.0000 (0.0265)  triple_60: 0.0000 (0.2235)  triple_40: 0.0000 (0.0592)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 590/1724]  eta: 1:14:08  lr: 0.000060  loss: 25.3759 (26.3321)  loss_n_40: 5.6804 (5.8334)  loss_n_60: 6.6573 (6.8234)  loss_n_80: 6.4214 (6.4229)  loss_n_100: 6.6458 (6.8833)  triple_100: 0.0000 (0.0627)  triple_80: 0.0000 (0.0261)  triple_60: 0.0000 (0.2220)  triple_40: 0.0000 (0.0583)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 600/1724]  eta: 1:13:28  lr: 0.000060  loss: 25.2360 (26.3171)  loss_n_40: 5.7776 (5.8327)  loss_n_60: 6.7176 (6.8211)  loss_n_80: 6.2689 (6.4187)  loss_n_100: 6.5214 (6.8776)  triple_100: 0.0000 (0.0629)  triple_80: 0.0000 (0.0266)  triple_60: 0.0000 (0.2202)  triple_40: 0.0000 (0.0574)  time: 3.9226  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [ 610/1724]  eta: 1:12:49  lr: 0.000060  loss: 25.0262 (26.3038)  loss_n_40: 5.7184 (5.8306)  loss_n_60: 6.6431 (6.8191)  loss_n_80: 6.1101 (6.4156)  loss_n_100: 6.4523 (6.8724)  triple_100: 0.0000 (0.0634)  triple_80: 0.0000 (0.0262)  triple_60: 0.0000 (0.2202)  triple_40: 0.0000 (0.0564)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 620/1724]  eta: 1:12:10  lr: 0.000060  loss: 24.8572 (26.2840)  loss_n_40: 5.4512 (5.8254)  loss_n_60: 6.4662 (6.8137)  loss_n_80: 6.0375 (6.4095)  loss_n_100: 6.4523 (6.8670)  triple_100: 0.0000 (0.0649)  triple_80: 0.0000 (0.0259)  triple_60: 0.0000 (0.2214)  triple_40: 0.0000 (0.0562)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 630/1724]  eta: 1:11:31  lr: 0.000060  loss: 25.0069 (26.3039)  loss_n_40: 5.5894 (5.8248)  loss_n_60: 6.5106 (6.8115)  loss_n_80: 6.0423 (6.4066)  loss_n_100: 6.4984 (6.8634)  triple_100: 0.0000 (0.0746)  triple_80: 0.0000 (0.0381)  triple_60: 0.0000 (0.2286)  triple_40: 0.0000 (0.0563)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 640/1724]  eta: 1:10:51  lr: 0.000060  loss: 24.9424 (26.2759)  loss_n_40: 5.5917 (5.8203)  loss_n_60: 6.5449 (6.8063)  loss_n_80: 6.0037 (6.4003)  loss_n_100: 6.4712 (6.8575)  triple_100: 0.0000 (0.0734)  triple_80: 0.0000 (0.0375)  triple_60: 0.0000 (0.2251)  triple_40: 0.0000 (0.0555)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 650/1724]  eta: 1:10:12  lr: 0.000060  loss: 24.8397 (26.2589)  loss_n_40: 5.5917 (5.8182)  loss_n_60: 6.5305 (6.8039)  loss_n_80: 5.9555 (6.3971)  loss_n_100: 6.4712 (6.8532)  triple_100: 0.0000 (0.0726)  triple_80: 0.0000 (0.0369)  triple_60: 0.0000 (0.2226)  triple_40: 0.0000 (0.0546)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 660/1724]  eta: 1:09:33  lr: 0.000060  loss: 25.0110 (26.2364)  loss_n_40: 5.6697 (5.8156)  loss_n_60: 6.5616 (6.8006)  loss_n_80: 6.1762 (6.3927)  loss_n_100: 6.5384 (6.8467)  triple_100: 0.0000 (0.0715)  triple_80: 0.0000 (0.0363)  triple_60: 0.0000 (0.2192)  triple_40: 0.0000 (0.0538)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 670/1724]  eta: 1:08:53  lr: 0.000060  loss: 24.3481 (26.2066)  loss_n_40: 5.5674 (5.8113)  loss_n_60: 6.5136 (6.7959)  loss_n_80: 5.8847 (6.3844)  loss_n_100: 6.1948 (6.8369)  triple_100: 0.0000 (0.0704)  triple_80: 0.0000 (0.0363)  triple_60: 0.0000 (0.2169)  triple_40: 0.0000 (0.0546)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 680/1724]  eta: 1:08:14  lr: 0.000060  loss: 24.0407 (26.1770)  loss_n_40: 5.5674 (5.8066)  loss_n_60: 6.5092 (6.7911)  loss_n_80: 5.8610 (6.3771)  loss_n_100: 6.1835 (6.8284)  triple_100: 0.0000 (0.0694)  triple_80: 0.0000 (0.0357)  triple_60: 0.0000 (0.2149)  triple_40: 0.0000 (0.0538)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 690/1724]  eta: 1:07:35  lr: 0.000060  loss: 24.4524 (26.1532)  loss_n_40: 5.5377 (5.8029)  loss_n_60: 6.4633 (6.7870)  loss_n_80: 5.8975 (6.3718)  loss_n_100: 6.3496 (6.8217)  triple_100: 0.0000 (0.0689)  triple_80: 0.0000 (0.0352)  triple_60: 0.0000 (0.2126)  triple_40: 0.0000 (0.0530)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 700/1724]  eta: 1:06:56  lr: 0.000060  loss: 24.6720 (26.1362)  loss_n_40: 5.5775 (5.8016)  loss_n_60: 6.4633 (6.7849)  loss_n_80: 6.0243 (6.3681)  loss_n_100: 6.3516 (6.8155)  triple_100: 0.0000 (0.0679)  triple_80: 0.0000 (0.0347)  triple_60: 0.0000 (0.2101)  triple_40: 0.0000 (0.0534)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 710/1724]  eta: 1:06:17  lr: 0.000060  loss: 24.6720 (26.1104)  loss_n_40: 5.5308 (5.7975)  loss_n_60: 6.4587 (6.7795)  loss_n_80: 5.9122 (6.3605)  loss_n_100: 6.2742 (6.8078)  triple_100: 0.0000 (0.0670)  triple_80: 0.0000 (0.0342)  triple_60: 0.0000 (0.2107)  triple_40: 0.0000 (0.0533)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 720/1724]  eta: 1:05:37  lr: 0.000060  loss: 24.3124 (26.0876)  loss_n_40: 5.5217 (5.7940)  loss_n_60: 6.4380 (6.7751)  loss_n_80: 5.8847 (6.3556)  loss_n_100: 6.2439 (6.8012)  triple_100: 0.0000 (0.0666)  triple_80: 0.0000 (0.0340)  triple_60: 0.0000 (0.2085)  triple_40: 0.0000 (0.0525)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 730/1724]  eta: 1:04:58  lr: 0.000060  loss: 24.3840 (26.0649)  loss_n_40: 5.5477 (5.7908)  loss_n_60: 6.4539 (6.7709)  loss_n_80: 6.0212 (6.3516)  loss_n_100: 6.3099 (6.7948)  triple_100: 0.0000 (0.0659)  triple_80: 0.0000 (0.0335)  triple_60: 0.0000 (0.2056)  triple_40: 0.0000 (0.0518)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 740/1724]  eta: 1:04:19  lr: 0.000060  loss: 23.8203 (26.0374)  loss_n_40: 5.3854 (5.7865)  loss_n_60: 6.3737 (6.7658)  loss_n_80: 5.8419 (6.3441)  loss_n_100: 6.1720 (6.7861)  triple_100: 0.0000 (0.0660)  triple_80: 0.0000 (0.0331)  triple_60: 0.0000 (0.2047)  triple_40: 0.0000 (0.0511)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 750/1724]  eta: 1:03:39  lr: 0.000060  loss: 23.7888 (26.0058)  loss_n_40: 5.3683 (5.7798)  loss_n_60: 6.2748 (6.7585)  loss_n_80: 5.8041 (6.3366)  loss_n_100: 6.1686 (6.7785)  triple_100: 0.0000 (0.0659)  triple_80: 0.0000 (0.0326)  triple_60: 0.0000 (0.2024)  triple_40: 0.0000 (0.0513)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 760/1724]  eta: 1:03:00  lr: 0.000060  loss: 23.3224 (25.9734)  loss_n_40: 5.2152 (5.7740)  loss_n_60: 6.1381 (6.7527)  loss_n_80: 5.7096 (6.3291)  loss_n_100: 6.0644 (6.7689)  triple_100: 0.0000 (0.0651)  triple_80: 0.0000 (0.0327)  triple_60: 0.0000 (0.1999)  triple_40: 0.0000 (0.0510)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 770/1724]  eta: 1:02:21  lr: 0.000060  loss: 23.4244 (25.9454)  loss_n_40: 5.3025 (5.7684)  loss_n_60: 6.2245 (6.7468)  loss_n_80: 5.8151 (6.3236)  loss_n_100: 6.0644 (6.7610)  triple_100: 0.0000 (0.0642)  triple_80: 0.0000 (0.0323)  triple_60: 0.0000 (0.1977)  triple_40: 0.0000 (0.0516)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 780/1724]  eta: 1:01:42  lr: 0.000060  loss: 23.7531 (25.9230)  loss_n_40: 5.4408 (5.7661)  loss_n_60: 6.4027 (6.7437)  loss_n_80: 5.8692 (6.3169)  loss_n_100: 6.1442 (6.7528)  triple_100: 0.0000 (0.0640)  triple_80: 0.0000 (0.0336)  triple_60: 0.0000 (0.1952)  triple_40: 0.0000 (0.0509)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 790/1724]  eta: 1:01:03  lr: 0.000060  loss: 24.0092 (25.9317)  loss_n_40: 5.5249 (5.7629)  loss_n_60: 6.5416 (6.7408)  loss_n_80: 5.8718 (6.3118)  loss_n_100: 6.2205 (6.7471)  triple_100: 0.0000 (0.0722)  triple_80: 0.0000 (0.0382)  triple_60: 0.0000 (0.2046)  triple_40: 0.0000 (0.0541)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 800/1724]  eta: 1:00:23  lr: 0.000060  loss: 24.0751 (25.9131)  loss_n_40: 5.4324 (5.7589)  loss_n_60: 6.5052 (6.7374)  loss_n_80: 5.9364 (6.3069)  loss_n_100: 6.3241 (6.7428)  triple_100: 0.0000 (0.0713)  triple_80: 0.0000 (0.0377)  triple_60: 0.0000 (0.2046)  triple_40: 0.0000 (0.0534)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 810/1724]  eta: 0:59:44  lr: 0.000060  loss: 24.0220 (25.8906)  loss_n_40: 5.4099 (5.7543)  loss_n_60: 6.3671 (6.7327)  loss_n_80: 5.9848 (6.3018)  loss_n_100: 6.3638 (6.7378)  triple_100: 0.0000 (0.0710)  triple_80: 0.0000 (0.0372)  triple_60: 0.0000 (0.2021)  triple_40: 0.0000 (0.0535)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 820/1724]  eta: 0:59:05  lr: 0.000060  loss: 23.5868 (25.8609)  loss_n_40: 5.3800 (5.7496)  loss_n_60: 6.3320 (6.7283)  loss_n_80: 5.8444 (6.2939)  loss_n_100: 6.2621 (6.7294)  triple_100: 0.0000 (0.0702)  triple_80: 0.0000 (0.0368)  triple_60: 0.0000 (0.1999)  triple_40: 0.0000 (0.0529)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 830/1724]  eta: 0:58:26  lr: 0.000060  loss: 23.6372 (25.8531)  loss_n_40: 5.5141 (5.7467)  loss_n_60: 6.3824 (6.7246)  loss_n_80: 5.8444 (6.2916)  loss_n_100: 6.1850 (6.7254)  triple_100: 0.0000 (0.0715)  triple_80: 0.0000 (0.0378)  triple_60: 0.0000 (0.2010)  triple_40: 0.0000 (0.0545)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 840/1724]  eta: 0:57:46  lr: 0.000060  loss: 24.0939 (25.8387)  loss_n_40: 5.5085 (5.7437)  loss_n_60: 6.3824 (6.7212)  loss_n_80: 6.0564 (6.2885)  loss_n_100: 6.2968 (6.7208)  triple_100: 0.0000 (0.0707)  triple_80: 0.0000 (0.0374)  triple_60: 0.0000 (0.2009)  triple_40: 0.0000 (0.0556)  time: 3.9202  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [ 850/1724]  eta: 0:57:07  lr: 0.000060  loss: 23.9642 (25.8127)  loss_n_40: 5.4582 (5.7396)  loss_n_60: 6.4411 (6.7178)  loss_n_80: 5.8356 (6.2819)  loss_n_100: 6.2108 (6.7131)  triple_100: 0.0000 (0.0699)  triple_80: 0.0000 (0.0370)  triple_60: 0.0000 (0.1985)  triple_40: 0.0000 (0.0550)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 860/1724]  eta: 0:56:28  lr: 0.000060  loss: 24.0804 (25.7984)  loss_n_40: 5.5167 (5.7381)  loss_n_60: 6.4593 (6.7151)  loss_n_80: 5.8356 (6.2790)  loss_n_100: 6.1616 (6.7088)  triple_100: 0.0000 (0.0693)  triple_80: 0.0000 (0.0365)  triple_60: 0.0000 (0.1972)  triple_40: 0.0000 (0.0543)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 870/1724]  eta: 0:55:49  lr: 0.000060  loss: 24.2505 (25.7858)  loss_n_40: 5.5884 (5.7347)  loss_n_60: 6.3903 (6.7103)  loss_n_80: 5.9358 (6.2744)  loss_n_100: 6.3188 (6.7034)  triple_100: 0.0000 (0.0714)  triple_80: 0.0000 (0.0387)  triple_60: 0.0000 (0.1992)  triple_40: 0.0000 (0.0537)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 880/1724]  eta: 0:55:09  lr: 0.000060  loss: 23.6628 (25.7681)  loss_n_40: 5.4470 (5.7306)  loss_n_60: 6.2848 (6.7055)  loss_n_80: 5.8399 (6.2683)  loss_n_100: 6.0347 (6.6958)  triple_100: 0.0000 (0.0734)  triple_80: 0.0000 (0.0386)  triple_60: 0.0000 (0.2017)  triple_40: 0.0000 (0.0542)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 890/1724]  eta: 0:54:30  lr: 0.000060  loss: 23.6628 (25.7538)  loss_n_40: 5.4772 (5.7286)  loss_n_60: 6.3814 (6.7023)  loss_n_80: 5.8395 (6.2634)  loss_n_100: 6.1009 (6.6896)  triple_100: 0.0000 (0.0725)  triple_80: 0.0000 (0.0396)  triple_60: 0.0000 (0.2023)  triple_40: 0.0000 (0.0555)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 900/1724]  eta: 0:53:51  lr: 0.000060  loss: 24.3904 (25.7498)  loss_n_40: 5.5459 (5.7272)  loss_n_60: 6.4997 (6.7024)  loss_n_80: 5.9381 (6.2625)  loss_n_100: 6.2301 (6.6888)  triple_100: 0.0000 (0.0725)  triple_80: 0.0000 (0.0401)  triple_60: 0.0000 (0.2014)  triple_40: 0.0000 (0.0549)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 910/1724]  eta: 0:53:12  lr: 0.000060  loss: 24.6312 (25.7436)  loss_n_40: 5.5704 (5.7254)  loss_n_60: 6.6446 (6.7018)  loss_n_80: 6.1469 (6.2595)  loss_n_100: 6.5561 (6.6857)  triple_100: 0.0000 (0.0729)  triple_80: 0.0000 (0.0396)  triple_60: 0.0000 (0.2045)  triple_40: 0.0000 (0.0543)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 920/1724]  eta: 0:52:33  lr: 0.000060  loss: 24.2047 (25.7245)  loss_n_40: 5.4417 (5.7220)  loss_n_60: 6.5937 (6.6985)  loss_n_80: 5.8822 (6.2550)  loss_n_100: 6.3223 (6.6814)  triple_100: 0.0000 (0.0721)  triple_80: 0.0000 (0.0392)  triple_60: 0.0000 (0.2025)  triple_40: 0.0000 (0.0537)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 930/1724]  eta: 0:51:53  lr: 0.000060  loss: 23.7988 (25.7168)  loss_n_40: 5.4407 (5.7209)  loss_n_60: 6.3887 (6.6967)  loss_n_80: 5.7599 (6.2498)  loss_n_100: 6.1570 (6.6759)  triple_100: 0.0000 (0.0723)  triple_80: 0.0000 (0.0389)  triple_60: 0.0000 (0.2083)  triple_40: 0.0000 (0.0540)  time: 3.9221  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [ 940/1724]  eta: 0:51:14  lr: 0.000060  loss: 23.8098 (25.6988)  loss_n_40: 5.5689 (5.7183)  loss_n_60: 6.3887 (6.6940)  loss_n_80: 5.7840 (6.2459)  loss_n_100: 6.1366 (6.6707)  triple_100: 0.0000 (0.0716)  triple_80: 0.0000 (0.0385)  triple_60: 0.0000 (0.2064)  triple_40: 0.0000 (0.0534)  time: 3.9224  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [ 950/1724]  eta: 0:50:35  lr: 0.000060  loss: 23.8098 (25.6781)  loss_n_40: 5.3955 (5.7141)  loss_n_60: 6.3672 (6.6897)  loss_n_80: 5.8145 (6.2419)  loss_n_100: 6.1579 (6.6657)  triple_100: 0.0000 (0.0715)  triple_80: 0.0000 (0.0381)  triple_60: 0.0000 (0.2042)  triple_40: 0.0000 (0.0529)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 960/1724]  eta: 0:49:56  lr: 0.000060  loss: 23.9644 (25.6649)  loss_n_40: 5.4478 (5.7130)  loss_n_60: 6.3779 (6.6869)  loss_n_80: 5.9536 (6.2399)  loss_n_100: 6.2343 (6.6617)  triple_100: 0.0000 (0.0709)  triple_80: 0.0000 (0.0380)  triple_60: 0.0000 (0.2022)  triple_40: 0.0000 (0.0523)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 970/1724]  eta: 0:49:16  lr: 0.000060  loss: 23.8715 (25.6410)  loss_n_40: 5.3384 (5.7093)  loss_n_60: 6.3485 (6.6824)  loss_n_80: 5.9136 (6.2347)  loss_n_100: 6.1302 (6.6551)  triple_100: 0.0000 (0.0701)  triple_80: 0.0000 (0.0376)  triple_60: 0.0000 (0.2002)  triple_40: 0.0000 (0.0518)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 980/1724]  eta: 0:48:37  lr: 0.000060  loss: 23.8164 (25.6257)  loss_n_40: 5.3384 (5.7074)  loss_n_60: 6.3069 (6.6794)  loss_n_80: 5.7289 (6.2301)  loss_n_100: 6.1188 (6.6499)  triple_100: 0.0000 (0.0697)  triple_80: 0.0000 (0.0372)  triple_60: 0.0000 (0.1993)  triple_40: 0.0000 (0.0527)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [ 990/1724]  eta: 0:47:58  lr: 0.000060  loss: 23.7503 (25.6019)  loss_n_40: 5.3711 (5.7026)  loss_n_60: 6.3613 (6.6747)  loss_n_80: 5.7184 (6.2246)  loss_n_100: 6.0645 (6.6430)  triple_100: 0.0000 (0.0695)  triple_80: 0.0000 (0.0368)  triple_60: 0.0000 (0.1985)  triple_40: 0.0000 (0.0522)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1000/1724]  eta: 0:47:19  lr: 0.000060  loss: 23.4416 (25.5821)  loss_n_40: 5.3711 (5.7003)  loss_n_60: 6.3563 (6.6719)  loss_n_80: 5.6611 (6.2201)  loss_n_100: 5.9423 (6.6360)  triple_100: 0.0000 (0.0688)  triple_80: 0.0000 (0.0369)  triple_60: 0.0000 (0.1965)  triple_40: 0.0000 (0.0516)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1010/1724]  eta: 0:46:40  lr: 0.000060  loss: 23.1683 (25.5585)  loss_n_40: 5.3917 (5.6961)  loss_n_60: 6.2197 (6.6669)  loss_n_80: 5.6611 (6.2142)  loss_n_100: 5.9228 (6.6288)  triple_100: 0.0000 (0.0681)  triple_80: 0.0000 (0.0367)  triple_60: 0.0000 (0.1946)  triple_40: 0.0000 (0.0530)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1020/1724]  eta: 0:46:00  lr: 0.000060  loss: 22.7648 (25.5361)  loss_n_40: 5.1563 (5.6919)  loss_n_60: 6.1745 (6.6620)  loss_n_80: 5.5673 (6.2086)  loss_n_100: 5.9088 (6.6224)  triple_100: 0.0000 (0.0675)  triple_80: 0.0000 (0.0364)  triple_60: 0.0000 (0.1942)  triple_40: 0.0000 (0.0532)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1030/1724]  eta: 0:45:21  lr: 0.000060  loss: 22.8597 (25.5119)  loss_n_40: 5.2519 (5.6884)  loss_n_60: 6.2145 (6.6585)  loss_n_80: 5.5410 (6.2021)  loss_n_100: 5.9088 (6.6151)  triple_100: 0.0000 (0.0668)  triple_80: 0.0000 (0.0360)  triple_60: 0.0000 (0.1923)  triple_40: 0.0000 (0.0527)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1040/1724]  eta: 0:44:42  lr: 0.000060  loss: 23.5786 (25.5011)  loss_n_40: 5.3241 (5.6862)  loss_n_60: 6.2827 (6.6556)  loss_n_80: 5.6326 (6.1982)  loss_n_100: 5.9435 (6.6100)  triple_100: 0.0000 (0.0675)  triple_80: 0.0000 (0.0357)  triple_60: 0.0000 (0.1940)  triple_40: 0.0000 (0.0539)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1050/1724]  eta: 0:44:03  lr: 0.000060  loss: 22.8355 (25.4763)  loss_n_40: 5.3548 (5.6810)  loss_n_60: 6.1296 (6.6507)  loss_n_80: 5.6174 (6.1915)  loss_n_100: 5.9786 (6.6031)  triple_100: 0.0000 (0.0669)  triple_80: 0.0000 (0.0368)  triple_60: 0.0000 (0.1930)  triple_40: 0.0000 (0.0534)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1060/1724]  eta: 0:43:23  lr: 0.000060  loss: 22.7041 (25.4535)  loss_n_40: 5.0614 (5.6772)  loss_n_60: 6.0638 (6.6469)  loss_n_80: 5.4636 (6.1852)  loss_n_100: 5.8497 (6.5964)  triple_100: 0.0000 (0.0662)  triple_80: 0.0000 (0.0364)  triple_60: 0.0000 (0.1923)  triple_40: 0.0000 (0.0529)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [1070/1724]  eta: 0:42:44  lr: 0.000060  loss: 23.2042 (25.4446)  loss_n_40: 5.3162 (5.6749)  loss_n_60: 6.3152 (6.6452)  loss_n_80: 5.4898 (6.1820)  loss_n_100: 5.8661 (6.5914)  triple_100: 0.0000 (0.0659)  triple_80: 0.0000 (0.0361)  triple_60: 0.0000 (0.1957)  triple_40: 0.0000 (0.0534)  time: 3.9198  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [1080/1724]  eta: 0:42:05  lr: 0.000060  loss: 23.4489 (25.4259)  loss_n_40: 5.2372 (5.6700)  loss_n_60: 6.3688 (6.6423)  loss_n_80: 5.8452 (6.1791)  loss_n_100: 6.0512 (6.5865)  triple_100: 0.0000 (0.0655)  triple_80: 0.0000 (0.0357)  triple_60: 0.0000 (0.1939)  triple_40: 0.0000 (0.0529)  time: 3.9205  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [1090/1724]  eta: 0:41:26  lr: 0.000060  loss: 23.1309 (25.4015)  loss_n_40: 5.1687 (5.6654)  loss_n_60: 6.2093 (6.6383)  loss_n_80: 5.6928 (6.1733)  loss_n_100: 5.9014 (6.5785)  triple_100: 0.0000 (0.0649)  triple_80: 0.0000 (0.0365)  triple_60: 0.0000 (0.1921)  triple_40: 0.0000 (0.0524)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1100/1724]  eta: 0:40:47  lr: 0.000060  loss: 22.7301 (25.3806)  loss_n_40: 5.2267 (5.6616)  loss_n_60: 6.1750 (6.6342)  loss_n_80: 5.3101 (6.1656)  loss_n_100: 5.6499 (6.5706)  triple_100: 0.0000 (0.0699)  triple_80: 0.0000 (0.0361)  triple_60: 0.0000 (0.1904)  triple_40: 0.0000 (0.0521)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1110/1724]  eta: 0:40:07  lr: 0.000060  loss: 23.0165 (25.3608)  loss_n_40: 5.2831 (5.6582)  loss_n_60: 6.2576 (6.6302)  loss_n_80: 5.5520 (6.1616)  loss_n_100: 5.7552 (6.5645)  triple_100: 0.0000 (0.0693)  triple_80: 0.0000 (0.0364)  triple_60: 0.0000 (0.1889)  triple_40: 0.0000 (0.0517)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1120/1724]  eta: 0:39:28  lr: 0.000060  loss: 23.0165 (25.3335)  loss_n_40: 5.1756 (5.6527)  loss_n_60: 5.9989 (6.6250)  loss_n_80: 5.6424 (6.1557)  loss_n_100: 5.7297 (6.5567)  triple_100: 0.0000 (0.0687)  triple_80: 0.0000 (0.0361)  triple_60: 0.0000 (0.1876)  triple_40: 0.0000 (0.0512)  time: 3.9213  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [1130/1724]  eta: 0:38:49  lr: 0.000060  loss: 22.3108 (25.3106)  loss_n_40: 5.0514 (5.6485)  loss_n_60: 5.9760 (6.6206)  loss_n_80: 5.4376 (6.1502)  loss_n_100: 5.7175 (6.5502)  triple_100: 0.0000 (0.0681)  triple_80: 0.0000 (0.0359)  triple_60: 0.0000 (0.1864)  triple_40: 0.0000 (0.0507)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1140/1724]  eta: 0:38:10  lr: 0.000060  loss: 22.4479 (25.2874)  loss_n_40: 5.1313 (5.6449)  loss_n_60: 6.1356 (6.6166)  loss_n_80: 5.4580 (6.1447)  loss_n_100: 5.7023 (6.5431)  triple_100: 0.0000 (0.0675)  triple_80: 0.0000 (0.0356)  triple_60: 0.0000 (0.1848)  triple_40: 0.0000 (0.0503)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1150/1724]  eta: 0:37:30  lr: 0.000060  loss: 22.9778 (25.2753)  loss_n_40: 5.1313 (5.6420)  loss_n_60: 6.1348 (6.6128)  loss_n_80: 5.5923 (6.1407)  loss_n_100: 5.7925 (6.5373)  triple_100: 0.0000 (0.0678)  triple_80: 0.0000 (0.0353)  triple_60: 0.0000 (0.1879)  triple_40: 0.0000 (0.0515)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1160/1724]  eta: 0:36:51  lr: 0.000060  loss: 23.2711 (25.2552)  loss_n_40: 5.1893 (5.6386)  loss_n_60: 6.1348 (6.6094)  loss_n_80: 5.5849 (6.1356)  loss_n_100: 5.8381 (6.5307)  triple_100: 0.0000 (0.0672)  triple_80: 0.0000 (0.0350)  triple_60: 0.0000 (0.1863)  triple_40: 0.0000 (0.0525)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1170/1724]  eta: 0:36:12  lr: 0.000060  loss: 23.1891 (25.2378)  loss_n_40: 5.2285 (5.6359)  loss_n_60: 6.1914 (6.6063)  loss_n_80: 5.4715 (6.1301)  loss_n_100: 5.7895 (6.5246)  triple_100: 0.0000 (0.0666)  triple_80: 0.0000 (0.0347)  triple_60: 0.0000 (0.1872)  triple_40: 0.0000 (0.0523)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1180/1724]  eta: 0:35:33  lr: 0.000060  loss: 23.1409 (25.2199)  loss_n_40: 5.3144 (5.6332)  loss_n_60: 6.2638 (6.6031)  loss_n_80: 5.4715 (6.1246)  loss_n_100: 5.8259 (6.5186)  triple_100: 0.0000 (0.0669)  triple_80: 0.0000 (0.0344)  triple_60: 0.0000 (0.1872)  triple_40: 0.0000 (0.0518)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1190/1724]  eta: 0:34:54  lr: 0.000060  loss: 22.6039 (25.2035)  loss_n_40: 5.3144 (5.6303)  loss_n_60: 6.2499 (6.6000)  loss_n_80: 5.5031 (6.1203)  loss_n_100: 5.8259 (6.5123)  triple_100: 0.0000 (0.0667)  triple_80: 0.0000 (0.0343)  triple_60: 0.0000 (0.1881)  triple_40: 0.0000 (0.0514)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1200/1724]  eta: 0:34:14  lr: 0.000060  loss: 22.7681 (25.1823)  loss_n_40: 5.2480 (5.6267)  loss_n_60: 6.1743 (6.5959)  loss_n_80: 5.5928 (6.1157)  loss_n_100: 5.7462 (6.5058)  triple_100: 0.0000 (0.0662)  triple_80: 0.0000 (0.0340)  triple_60: 0.0000 (0.1868)  triple_40: 0.0000 (0.0512)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1210/1724]  eta: 0:33:35  lr: 0.000060  loss: 22.7692 (25.1639)  loss_n_40: 5.3170 (5.6242)  loss_n_60: 6.2070 (6.5936)  loss_n_80: 5.5928 (6.1117)  loss_n_100: 5.7327 (6.4988)  triple_100: 0.0000 (0.0656)  triple_80: 0.0000 (0.0337)  triple_60: 0.0000 (0.1856)  triple_40: 0.0000 (0.0508)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1220/1724]  eta: 0:32:56  lr: 0.000060  loss: 22.5260 (25.1428)  loss_n_40: 5.1916 (5.6201)  loss_n_60: 6.1318 (6.5890)  loss_n_80: 5.4203 (6.1050)  loss_n_100: 5.5440 (6.4906)  triple_100: 0.0000 (0.0653)  triple_80: 0.0000 (0.0334)  triple_60: 0.0000 (0.1881)  triple_40: 0.0000 (0.0512)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1230/1724]  eta: 0:32:17  lr: 0.000060  loss: 22.1092 (25.1314)  loss_n_40: 5.1348 (5.6174)  loss_n_60: 6.0540 (6.5861)  loss_n_80: 5.3645 (6.1006)  loss_n_100: 5.5052 (6.4841)  triple_100: 0.0000 (0.0659)  triple_80: 0.0000 (0.0361)  triple_60: 0.0000 (0.1895)  triple_40: 0.0000 (0.0517)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1240/1724]  eta: 0:31:37  lr: 0.000060  loss: 23.1703 (25.1165)  loss_n_40: 5.1330 (5.6132)  loss_n_60: 6.0107 (6.5815)  loss_n_80: 5.5953 (6.0962)  loss_n_100: 5.8804 (6.4802)  triple_100: 0.0000 (0.0662)  triple_80: 0.0000 (0.0363)  triple_60: 0.0000 (0.1901)  triple_40: 0.0000 (0.0529)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1250/1724]  eta: 0:30:58  lr: 0.000060  loss: 23.1600 (25.1017)  loss_n_40: 5.2262 (5.6106)  loss_n_60: 6.1757 (6.5792)  loss_n_80: 5.5392 (6.0919)  loss_n_100: 5.9275 (6.4763)  triple_100: 0.0000 (0.0665)  triple_80: 0.0000 (0.0360)  triple_60: 0.0000 (0.1885)  triple_40: 0.0000 (0.0525)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1260/1724]  eta: 0:30:19  lr: 0.000060  loss: 22.7898 (25.0802)  loss_n_40: 5.2282 (5.6068)  loss_n_60: 6.1869 (6.5758)  loss_n_80: 5.4362 (6.0855)  loss_n_100: 5.8748 (6.4712)  triple_100: 0.0000 (0.0660)  triple_80: 0.0000 (0.0358)  triple_60: 0.0000 (0.1871)  triple_40: 0.0000 (0.0521)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1270/1724]  eta: 0:29:40  lr: 0.000060  loss: 22.0648 (25.0574)  loss_n_40: 5.0532 (5.6028)  loss_n_60: 5.9796 (6.5714)  loss_n_80: 5.1606 (6.0786)  loss_n_100: 5.7356 (6.4648)  triple_100: 0.0000 (0.0670)  triple_80: 0.0000 (0.0355)  triple_60: 0.0000 (0.1856)  triple_40: 0.0000 (0.0518)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1280/1724]  eta: 0:29:01  lr: 0.000060  loss: 21.8912 (25.0346)  loss_n_40: 4.9795 (5.5988)  loss_n_60: 5.9477 (6.5674)  loss_n_80: 5.1977 (6.0730)  loss_n_100: 5.5618 (6.4577)  triple_100: 0.0000 (0.0664)  triple_80: 0.0000 (0.0352)  triple_60: 0.0000 (0.1847)  triple_40: 0.0000 (0.0514)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1290/1724]  eta: 0:28:21  lr: 0.000060  loss: 21.5872 (25.0112)  loss_n_40: 4.9448 (5.5945)  loss_n_60: 5.9432 (6.5633)  loss_n_80: 5.2141 (6.0661)  loss_n_100: 5.3806 (6.4486)  triple_100: 0.0000 (0.0662)  triple_80: 0.0000 (0.0349)  triple_60: 0.0000 (0.1852)  triple_40: 0.0000 (0.0523)  time: 3.9213  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [1300/1724]  eta: 0:27:42  lr: 0.000060  loss: 21.0782 (24.9809)  loss_n_40: 4.9391 (5.5889)  loss_n_60: 5.9432 (6.5582)  loss_n_80: 5.0669 (6.0590)  loss_n_100: 5.2331 (6.4389)  triple_100: 0.0000 (0.0657)  triple_80: 0.0000 (0.0347)  triple_60: 0.0000 (0.1838)  triple_40: 0.0000 (0.0519)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1310/1724]  eta: 0:27:03  lr: 0.000060  loss: 21.1710 (24.9569)  loss_n_40: 4.9039 (5.5838)  loss_n_60: 5.9091 (6.5536)  loss_n_80: 5.1279 (6.0529)  loss_n_100: 5.2439 (6.4303)  triple_100: 0.0000 (0.0659)  triple_80: 0.0000 (0.0344)  triple_60: 0.0000 (0.1844)  triple_40: 0.0000 (0.0515)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1320/1724]  eta: 0:26:24  lr: 0.000060  loss: 21.5168 (24.9301)  loss_n_40: 4.9621 (5.5793)  loss_n_60: 5.9317 (6.5489)  loss_n_80: 5.1333 (6.0462)  loss_n_100: 5.3435 (6.4218)  triple_100: 0.0000 (0.0657)  triple_80: 0.0000 (0.0343)  triple_60: 0.0000 (0.1830)  triple_40: 0.0000 (0.0511)  time: 3.9226  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [1330/1724]  eta: 0:25:45  lr: 0.000060  loss: 21.4514 (24.9123)  loss_n_40: 5.0358 (5.5754)  loss_n_60: 5.9801 (6.5453)  loss_n_80: 5.1333 (6.0397)  loss_n_100: 5.3212 (6.4133)  triple_100: 0.0000 (0.0657)  triple_80: 0.0000 (0.0351)  triple_60: 0.0000 (0.1866)  triple_40: 0.0000 (0.0513)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1340/1724]  eta: 0:25:05  lr: 0.000060  loss: 21.1181 (24.8849)  loss_n_40: 4.9872 (5.5703)  loss_n_60: 5.9801 (6.5407)  loss_n_80: 5.1232 (6.0329)  loss_n_100: 5.2134 (6.4049)  triple_100: 0.0000 (0.0652)  triple_80: 0.0000 (0.0349)  triple_60: 0.0000 (0.1852)  triple_40: 0.0000 (0.0509)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1350/1724]  eta: 0:24:26  lr: 0.000060  loss: 20.9448 (24.8576)  loss_n_40: 4.8606 (5.5656)  loss_n_60: 5.8227 (6.5357)  loss_n_80: 5.0828 (6.0255)  loss_n_100: 5.2134 (6.3966)  triple_100: 0.0000 (0.0652)  triple_80: 0.0000 (0.0346)  triple_60: 0.0000 (0.1838)  triple_40: 0.0000 (0.0505)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1360/1724]  eta: 0:23:47  lr: 0.000060  loss: 21.3694 (24.8339)  loss_n_40: 5.0078 (5.5623)  loss_n_60: 5.9516 (6.5323)  loss_n_80: 5.1218 (6.0193)  loss_n_100: 5.2740 (6.3884)  triple_100: 0.0000 (0.0647)  triple_80: 0.0000 (0.0343)  triple_60: 0.0000 (0.1825)  triple_40: 0.0000 (0.0501)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1370/1724]  eta: 0:23:08  lr: 0.000060  loss: 21.5795 (24.8085)  loss_n_40: 5.1682 (5.5586)  loss_n_60: 6.0313 (6.5283)  loss_n_80: 5.1815 (6.0122)  loss_n_100: 5.2069 (6.3797)  triple_100: 0.0000 (0.0642)  triple_80: 0.0000 (0.0341)  triple_60: 0.0000 (0.1816)  triple_40: 0.0000 (0.0498)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1380/1724]  eta: 0:22:28  lr: 0.000060  loss: 21.1767 (24.7848)  loss_n_40: 5.0659 (5.5555)  loss_n_60: 6.0465 (6.5250)  loss_n_80: 5.0121 (6.0056)  loss_n_100: 5.1913 (6.3712)  triple_100: 0.0000 (0.0640)  triple_80: 0.0000 (0.0338)  triple_60: 0.0000 (0.1803)  triple_40: 0.0000 (0.0495)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1390/1724]  eta: 0:21:49  lr: 0.000060  loss: 21.8922 (24.7667)  loss_n_40: 5.2126 (5.5533)  loss_n_60: 6.0745 (6.5225)  loss_n_80: 5.1335 (5.9997)  loss_n_100: 5.2460 (6.3637)  triple_100: 0.0000 (0.0656)  triple_80: 0.0000 (0.0336)  triple_60: 0.0000 (0.1790)  triple_40: 0.0000 (0.0493)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1400/1724]  eta: 0:21:10  lr: 0.000060  loss: 21.7843 (24.7427)  loss_n_40: 5.1249 (5.5490)  loss_n_60: 6.0745 (6.5181)  loss_n_80: 5.2070 (5.9944)  loss_n_100: 5.3864 (6.3559)  triple_100: 0.0000 (0.0651)  triple_80: 0.0000 (0.0335)  triple_60: 0.0000 (0.1778)  triple_40: 0.0000 (0.0490)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1410/1724]  eta: 0:20:31  lr: 0.000060  loss: 21.7843 (24.7238)  loss_n_40: 5.0408 (5.5462)  loss_n_60: 6.0018 (6.5150)  loss_n_80: 5.2025 (5.9889)  loss_n_100: 5.4314 (6.3491)  triple_100: 0.0000 (0.0653)  triple_80: 0.0000 (0.0342)  triple_60: 0.0000 (0.1765)  triple_40: 0.0000 (0.0486)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1420/1724]  eta: 0:19:52  lr: 0.000060  loss: 22.1523 (24.7118)  loss_n_40: 5.1003 (5.5434)  loss_n_60: 6.1064 (6.5127)  loss_n_80: 5.1990 (5.9847)  loss_n_100: 5.4867 (6.3435)  triple_100: 0.0000 (0.0658)  triple_80: 0.0000 (0.0354)  triple_60: 0.0000 (0.1769)  triple_40: 0.0000 (0.0494)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1430/1724]  eta: 0:19:12  lr: 0.000060  loss: 22.8404 (24.6980)  loss_n_40: 5.0171 (5.5390)  loss_n_60: 6.1403 (6.5099)  loss_n_80: 5.4574 (5.9819)  loss_n_100: 5.8536 (6.3416)  triple_100: 0.0000 (0.0659)  triple_80: 0.0000 (0.0352)  triple_60: 0.0000 (0.1757)  triple_40: 0.0000 (0.0490)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1440/1724]  eta: 0:18:33  lr: 0.000060  loss: 22.5062 (24.6826)  loss_n_40: 4.9506 (5.5358)  loss_n_60: 6.1205 (6.5076)  loss_n_80: 5.4448 (5.9774)  loss_n_100: 6.0308 (6.3379)  triple_100: 0.0000 (0.0656)  triple_80: 0.0000 (0.0349)  triple_60: 0.0000 (0.1746)  triple_40: 0.0000 (0.0487)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1450/1724]  eta: 0:17:54  lr: 0.000060  loss: 21.7113 (24.6585)  loss_n_40: 4.9506 (5.5312)  loss_n_60: 6.0765 (6.5035)  loss_n_80: 5.1081 (5.9706)  loss_n_100: 5.5671 (6.3308)  triple_100: 0.0000 (0.0655)  triple_80: 0.0000 (0.0347)  triple_60: 0.0000 (0.1738)  triple_40: 0.0000 (0.0484)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1460/1724]  eta: 0:17:15  lr: 0.000060  loss: 21.4670 (24.6535)  loss_n_40: 4.9672 (5.5279)  loss_n_60: 6.0121 (6.4999)  loss_n_80: 5.0696 (5.9648)  loss_n_100: 5.3363 (6.3243)  triple_100: 0.0000 (0.0704)  triple_80: 0.0000 (0.0355)  triple_60: 0.0000 (0.1794)  triple_40: 0.0000 (0.0515)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1470/1724]  eta: 0:16:35  lr: 0.000060  loss: 21.3560 (24.6298)  loss_n_40: 4.9301 (5.5235)  loss_n_60: 5.9737 (6.4956)  loss_n_80: 5.1404 (5.9588)  loss_n_100: 5.3125 (6.3175)  triple_100: 0.0000 (0.0699)  triple_80: 0.0000 (0.0352)  triple_60: 0.0000 (0.1782)  triple_40: 0.0000 (0.0511)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1480/1724]  eta: 0:15:56  lr: 0.000060  loss: 21.0097 (24.6097)  loss_n_40: 4.8934 (5.5200)  loss_n_60: 5.8899 (6.4923)  loss_n_80: 5.1189 (5.9537)  loss_n_100: 5.2892 (6.3110)  triple_100: 0.0000 (0.0694)  triple_80: 0.0000 (0.0352)  triple_60: 0.0000 (0.1772)  triple_40: 0.0000 (0.0508)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1490/1724]  eta: 0:15:17  lr: 0.000060  loss: 21.1057 (24.5878)  loss_n_40: 4.8187 (5.5152)  loss_n_60: 5.8122 (6.4875)  loss_n_80: 4.9274 (5.9472)  loss_n_100: 5.0781 (6.3034)  triple_100: 0.0000 (0.0690)  triple_80: 0.0000 (0.0349)  triple_60: 0.0000 (0.1795)  triple_40: 0.0000 (0.0509)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1500/1724]  eta: 0:14:38  lr: 0.000060  loss: 20.7374 (24.5647)  loss_n_40: 4.6907 (5.5113)  loss_n_60: 5.7457 (6.4844)  loss_n_80: 4.9314 (5.9410)  loss_n_100: 5.0781 (6.2959)  triple_100: 0.0000 (0.0685)  triple_80: 0.0000 (0.0347)  triple_60: 0.0000 (0.1783)  triple_40: 0.0000 (0.0506)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1510/1724]  eta: 0:13:59  lr: 0.000060  loss: 20.8201 (24.5421)  loss_n_40: 4.9699 (5.5081)  loss_n_60: 5.9608 (6.4811)  loss_n_80: 4.9779 (5.9346)  loss_n_100: 5.0696 (6.2883)  triple_100: 0.0000 (0.0681)  triple_80: 0.0000 (0.0345)  triple_60: 0.0000 (0.1772)  triple_40: 0.0000 (0.0503)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1520/1724]  eta: 0:13:19  lr: 0.000060  loss: 20.5336 (24.5136)  loss_n_40: 4.9007 (5.5034)  loss_n_60: 5.8398 (6.4763)  loss_n_80: 4.7715 (5.9266)  loss_n_100: 4.9713 (6.2792)  triple_100: 0.0000 (0.0677)  triple_80: 0.0000 (0.0342)  triple_60: 0.0000 (0.1760)  triple_40: 0.0000 (0.0501)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1530/1724]  eta: 0:12:40  lr: 0.000060  loss: 20.6510 (24.4948)  loss_n_40: 4.8103 (5.4986)  loss_n_60: 5.7813 (6.4717)  loss_n_80: 4.8560 (5.9210)  loss_n_100: 5.0886 (6.2734)  triple_100: 0.0000 (0.0683)  triple_80: 0.0000 (0.0353)  triple_60: 0.0000 (0.1760)  triple_40: 0.0000 (0.0504)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1540/1724]  eta: 0:12:01  lr: 0.000060  loss: 21.8811 (24.4767)  loss_n_40: 4.9130 (5.4950)  loss_n_60: 5.9669 (6.4691)  loss_n_80: 5.0486 (5.9161)  loss_n_100: 5.4430 (6.2682)  triple_100: 0.0000 (0.0683)  triple_80: 0.0000 (0.0351)  triple_60: 0.0000 (0.1750)  triple_40: 0.0000 (0.0500)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1550/1724]  eta: 0:11:22  lr: 0.000060  loss: 21.3126 (24.4511)  loss_n_40: 4.9059 (5.4904)  loss_n_60: 5.9816 (6.4648)  loss_n_80: 4.9885 (5.9096)  loss_n_100: 5.0299 (6.2593)  triple_100: 0.0000 (0.0679)  triple_80: 0.0000 (0.0348)  triple_60: 0.0000 (0.1745)  triple_40: 0.0000 (0.0497)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1560/1724]  eta: 0:10:43  lr: 0.000060  loss: 20.2837 (24.4282)  loss_n_40: 4.6974 (5.4866)  loss_n_60: 5.7067 (6.4606)  loss_n_80: 4.8879 (5.9034)  loss_n_100: 4.8037 (6.2507)  triple_100: 0.0000 (0.0680)  triple_80: 0.0000 (0.0348)  triple_60: 0.0000 (0.1746)  triple_40: 0.0000 (0.0496)  time: 3.9210  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [1570/1724]  eta: 0:10:03  lr: 0.000060  loss: 20.5453 (24.4056)  loss_n_40: 4.8713 (5.4831)  loss_n_60: 5.8272 (6.4568)  loss_n_80: 4.8879 (5.8970)  loss_n_100: 4.8042 (6.2421)  triple_100: 0.0000 (0.0676)  triple_80: 0.0000 (0.0347)  triple_60: 0.0000 (0.1736)  triple_40: 0.0000 (0.0506)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1580/1724]  eta: 0:09:24  lr: 0.000060  loss: 20.5559 (24.3833)  loss_n_40: 4.8713 (5.4789)  loss_n_60: 5.8147 (6.4531)  loss_n_80: 4.8281 (5.8908)  loss_n_100: 4.9573 (6.2355)  triple_100: 0.0000 (0.0674)  triple_80: 0.0000 (0.0345)  triple_60: 0.0000 (0.1729)  triple_40: 0.0000 (0.0503)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1590/1724]  eta: 0:08:45  lr: 0.000060  loss: 20.6692 (24.3639)  loss_n_40: 4.6768 (5.4745)  loss_n_60: 5.6917 (6.4485)  loss_n_80: 4.9589 (5.8852)  loss_n_100: 5.3381 (6.2295)  triple_100: 0.0000 (0.0669)  triple_80: 0.0000 (0.0348)  triple_60: 0.0000 (0.1744)  triple_40: 0.0000 (0.0500)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1600/1724]  eta: 0:08:06  lr: 0.000060  loss: 20.6692 (24.3388)  loss_n_40: 4.6768 (5.4704)  loss_n_60: 5.7007 (6.4446)  loss_n_80: 4.9589 (5.8786)  loss_n_100: 5.0432 (6.2210)  triple_100: 0.0000 (0.0665)  triple_80: 0.0000 (0.0346)  triple_60: 0.0000 (0.1733)  triple_40: 0.0000 (0.0497)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1610/1724]  eta: 0:07:27  lr: 0.000060  loss: 20.8762 (24.3210)  loss_n_40: 4.9494 (5.4670)  loss_n_60: 5.7959 (6.4404)  loss_n_80: 4.9163 (5.8727)  loss_n_100: 4.9131 (6.2139)  triple_100: 0.0000 (0.0671)  triple_80: 0.0000 (0.0344)  triple_60: 0.0000 (0.1757)  triple_40: 0.0000 (0.0498)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1620/1724]  eta: 0:06:47  lr: 0.000060  loss: 20.8762 (24.3027)  loss_n_40: 4.9174 (5.4628)  loss_n_60: 5.7959 (6.4363)  loss_n_80: 4.9497 (5.8675)  loss_n_100: 4.9184 (6.2060)  triple_100: 0.0000 (0.0684)  triple_80: 0.0000 (0.0358)  triple_60: 0.0000 (0.1764)  triple_40: 0.0000 (0.0495)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1630/1724]  eta: 0:06:08  lr: 0.000060  loss: 20.3475 (24.2815)  loss_n_40: 4.7751 (5.4584)  loss_n_60: 5.7796 (6.4318)  loss_n_80: 4.9451 (5.8616)  loss_n_100: 5.0034 (6.1988)  triple_100: 0.0000 (0.0688)  triple_80: 0.0000 (0.0359)  triple_60: 0.0000 (0.1761)  triple_40: 0.0000 (0.0500)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1640/1724]  eta: 0:05:29  lr: 0.000060  loss: 20.4716 (24.2603)  loss_n_40: 4.7534 (5.4549)  loss_n_60: 5.6714 (6.4280)  loss_n_80: 4.9200 (5.8564)  loss_n_100: 4.9644 (6.1918)  triple_100: 0.0000 (0.0684)  triple_80: 0.0000 (0.0360)  triple_60: 0.0000 (0.1751)  triple_40: 0.0000 (0.0497)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1650/1724]  eta: 0:04:50  lr: 0.000060  loss: 20.6413 (24.2384)  loss_n_40: 4.8387 (5.4517)  loss_n_60: 5.8255 (6.4246)  loss_n_80: 4.9394 (5.8509)  loss_n_100: 4.8643 (6.1837)  triple_100: 0.0000 (0.0680)  triple_80: 0.0000 (0.0358)  triple_60: 0.0000 (0.1743)  triple_40: 0.0000 (0.0494)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1660/1724]  eta: 0:04:10  lr: 0.000060  loss: 20.0871 (24.2145)  loss_n_40: 4.7247 (5.4470)  loss_n_60: 5.7363 (6.4199)  loss_n_80: 4.8450 (5.8444)  loss_n_100: 4.6653 (6.1746)  triple_100: 0.0000 (0.0676)  triple_80: 0.0000 (0.0358)  triple_60: 0.0000 (0.1756)  triple_40: 0.0000 (0.0497)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [1670/1724]  eta: 0:03:31  lr: 0.000060  loss: 20.0168 (24.1932)  loss_n_40: 4.6695 (5.4434)  loss_n_60: 5.6707 (6.4164)  loss_n_80: 4.7739 (5.8392)  loss_n_100: 4.6700 (6.1673)  triple_100: 0.0000 (0.0671)  triple_80: 0.0000 (0.0359)  triple_60: 0.0000 (0.1746)  triple_40: 0.0000 (0.0494)  time: 3.9193  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [1680/1724]  eta: 0:02:52  lr: 0.000060  loss: 20.3917 (24.1695)  loss_n_40: 4.7124 (5.4398)  loss_n_60: 5.7503 (6.4124)  loss_n_80: 4.8837 (5.8329)  loss_n_100: 4.8016 (6.1590)  triple_100: 0.0000 (0.0668)  triple_80: 0.0000 (0.0357)  triple_60: 0.0000 (0.1735)  triple_40: 0.0000 (0.0494)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1690/1724]  eta: 0:02:13  lr: 0.000060  loss: 20.4496 (24.1446)  loss_n_40: 4.8988 (5.4359)  loss_n_60: 5.7648 (6.4084)  loss_n_80: 4.8175 (5.8265)  loss_n_100: 4.7681 (6.1501)  triple_100: 0.0000 (0.0664)  triple_80: 0.0000 (0.0356)  triple_60: 0.0000 (0.1725)  triple_40: 0.0000 (0.0491)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1700/1724]  eta: 0:01:34  lr: 0.000060  loss: 20.4496 (24.1210)  loss_n_40: 4.7281 (5.4318)  loss_n_60: 5.7841 (6.4045)  loss_n_80: 4.7528 (5.8203)  loss_n_100: 4.6366 (6.1414)  triple_100: 0.0000 (0.0662)  triple_80: 0.0000 (0.0354)  triple_60: 0.0000 (0.1720)  triple_40: 0.0000 (0.0495)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4]  [1710/1724]  eta: 0:00:54  lr: 0.000060  loss: 19.8943 (24.0949)  loss_n_40: 4.7193 (5.4269)  loss_n_60: 5.7721 (6.3997)  loss_n_80: 4.7528 (5.8141)  loss_n_100: 4.6114 (6.1330)  triple_100: 0.0000 (0.0658)  triple_80: 0.0000 (0.0352)  triple_60: 0.0000 (0.1710)  triple_40: 0.0000 (0.0492)  time: 3.9202  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [1720/1724]  eta: 0:00:15  lr: 0.000060  loss: 19.7864 (24.0700)  loss_n_40: 4.6708 (5.4222)  loss_n_60: 5.5247 (6.3948)  loss_n_80: 4.6823 (5.8076)  loss_n_100: 4.6854 (6.1248)  triple_100: 0.0000 (0.0655)  triple_80: 0.0000 (0.0351)  triple_60: 0.0000 (0.1706)  triple_40: 0.0000 (0.0494)  time: 3.9183  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:4]  [1723/1724]  eta: 0:00:03  lr: 0.000060  loss: 19.6509 (24.0610)  loss_n_40: 4.6708 (5.4204)  loss_n_60: 5.5247 (6.3932)  loss_n_80: 4.6457 (5.8054)  loss_n_100: 4.6114 (6.1219)  triple_100: 0.0000 (0.0654)  triple_80: 0.0000 (0.0350)  triple_60: 0.0000 (0.1703)  triple_40: 0.0000 (0.0493)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:4] Total time: 1:52:40 (3.9213 s / it)\n",
      "Averaged stats: lr: 0.000060  loss: 19.6509 (24.0610)  loss_n_40: 4.6708 (5.4204)  loss_n_60: 5.5247 (6.3932)  loss_n_80: 4.6457 (5.8054)  loss_n_100: 4.6114 (6.1219)  triple_100: 0.0000 (0.0654)  triple_80: 0.0000 (0.0350)  triple_60: 0.0000 (0.1703)  triple_40: 0.0000 (0.0493)\n",
      "Valid: [epoch:4]  [  0/845]  eta: 0:11:11  loss: 21.0390 (21.0390)  loss_n_40: 5.1742 (5.1742)  loss_n_60: 6.3149 (6.3149)  loss_n_80: 4.9105 (4.9105)  loss_n_100: 4.6393 (4.6393)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7952  data: 0.4585  max mem: 46473\n",
      "Valid: [epoch:4]  [ 10/845]  eta: 0:05:13  loss: 18.1659 (18.4688)  loss_n_40: 3.9701 (4.3331)  loss_n_60: 5.0397 (5.3468)  loss_n_80: 4.2058 (4.4020)  loss_n_100: 4.1849 (4.3870)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3758  data: 0.0418  max mem: 46473\n",
      "Valid: [epoch:4]  [ 20/845]  eta: 0:04:53  loss: 18.5497 (18.8249)  loss_n_40: 4.2806 (4.4169)  loss_n_60: 5.2167 (5.4333)  loss_n_80: 4.2058 (4.4623)  loss_n_100: 4.1849 (4.4290)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0834)  triple_40: 0.0000 (0.0000)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [ 30/845]  eta: 0:04:44  loss: 19.0352 (19.1985)  loss_n_40: 4.5119 (4.4657)  loss_n_60: 5.4236 (5.4304)  loss_n_80: 4.5938 (4.5429)  loss_n_100: 4.4685 (4.5741)  triple_100: 0.0000 (0.0993)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0862)  triple_40: 0.0000 (0.0000)  time: 0.3336  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [ 40/845]  eta: 0:04:37  loss: 19.0352 (19.3673)  loss_n_40: 4.5707 (4.4919)  loss_n_60: 5.2770 (5.4335)  loss_n_80: 4.6550 (4.5643)  loss_n_100: 4.9103 (4.5747)  triple_100: 0.0000 (0.1243)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1786)  triple_40: 0.0000 (0.0000)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [ 50/845]  eta: 0:04:32  loss: 19.3279 (19.8710)  loss_n_40: 4.7067 (4.5840)  loss_n_60: 5.5750 (5.5157)  loss_n_80: 4.6550 (4.6153)  loss_n_100: 4.6160 (4.6273)  triple_100: 0.0000 (0.1869)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.3418)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:4]  [ 60/845]  eta: 0:04:27  loss: 20.0572 (20.0975)  loss_n_40: 4.7435 (4.5904)  loss_n_60: 5.7533 (5.5292)  loss_n_80: 4.5839 (4.6361)  loss_n_100: 4.4829 (4.6316)  triple_100: 0.0000 (0.1563)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.4761)  triple_40: 0.0000 (0.0780)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [ 70/845]  eta: 0:04:23  loss: 19.0087 (20.2281)  loss_n_40: 4.5794 (4.5749)  loss_n_60: 5.4287 (5.5090)  loss_n_80: 4.4984 (4.6332)  loss_n_100: 4.3929 (4.6380)  triple_100: 0.0000 (0.2529)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.5531)  triple_40: 0.0000 (0.0670)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [ 80/845]  eta: 0:04:19  loss: 18.8393 (20.0869)  loss_n_40: 4.5131 (4.5732)  loss_n_60: 5.4397 (5.5216)  loss_n_80: 4.3837 (4.6178)  loss_n_100: 4.2851 (4.6090)  triple_100: 0.0000 (0.2217)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.4848)  triple_40: 0.0000 (0.0587)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [ 90/845]  eta: 0:04:15  loss: 19.2719 (20.0277)  loss_n_40: 4.6275 (4.5801)  loss_n_60: 5.5381 (5.5270)  loss_n_80: 4.4136 (4.6192)  loss_n_100: 4.3126 (4.6179)  triple_100: 0.0000 (0.1998)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.4315)  triple_40: 0.0000 (0.0523)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [100/845]  eta: 0:04:12  loss: 19.2719 (19.9849)  loss_n_40: 4.5523 (4.5715)  loss_n_60: 5.1649 (5.5042)  loss_n_80: 4.4884 (4.6220)  loss_n_100: 4.4729 (4.6376)  triple_100: 0.0000 (0.1893)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.4134)  triple_40: 0.0000 (0.0471)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [110/845]  eta: 0:04:08  loss: 18.7450 (19.9509)  loss_n_40: 4.4448 (4.5709)  loss_n_60: 5.4366 (5.5100)  loss_n_80: 4.4703 (4.6238)  loss_n_100: 4.4729 (4.6421)  triple_100: 0.0000 (0.1851)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.3761)  triple_40: 0.0000 (0.0428)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [120/845]  eta: 0:04:04  loss: 19.3978 (19.9848)  loss_n_40: 4.5276 (4.5978)  loss_n_60: 5.5856 (5.5259)  loss_n_80: 4.6146 (4.6339)  loss_n_100: 4.6856 (4.6669)  triple_100: 0.0000 (0.1759)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.3451)  triple_40: 0.0000 (0.0393)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [130/845]  eta: 0:04:01  loss: 21.3021 (20.0659)  loss_n_40: 5.0342 (4.6278)  loss_n_60: 5.8636 (5.5578)  loss_n_80: 5.0056 (4.6614)  loss_n_100: 4.8702 (4.6932)  triple_100: 0.0000 (0.1706)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.3187)  triple_40: 0.0000 (0.0363)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [140/845]  eta: 0:03:57  loss: 20.8284 (20.0558)  loss_n_40: 5.0311 (4.6198)  loss_n_60: 5.8636 (5.5536)  loss_n_80: 4.8719 (4.6713)  loss_n_100: 4.8515 (4.7018)  triple_100: 0.0000 (0.1796)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.2961)  triple_40: 0.0000 (0.0337)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [150/845]  eta: 0:03:54  loss: 19.3792 (20.0271)  loss_n_40: 4.6579 (4.6160)  loss_n_60: 5.6701 (5.5597)  loss_n_80: 4.5769 (4.6690)  loss_n_100: 4.5040 (4.6917)  triple_100: 0.0000 (0.1828)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.2765)  triple_40: 0.0000 (0.0315)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [160/845]  eta: 0:03:50  loss: 19.0861 (19.9727)  loss_n_40: 4.6509 (4.6137)  loss_n_60: 5.7687 (5.5613)  loss_n_80: 4.4694 (4.6623)  loss_n_100: 4.4917 (4.6751)  triple_100: 0.0000 (0.1714)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.2593)  triple_40: 0.0000 (0.0295)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [170/845]  eta: 0:03:47  loss: 19.0831 (19.9300)  loss_n_40: 4.5101 (4.6086)  loss_n_60: 5.4843 (5.5609)  loss_n_80: 4.4694 (4.6557)  loss_n_100: 4.3270 (4.6663)  triple_100: 0.0000 (0.1665)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.2442)  triple_40: 0.0000 (0.0278)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [180/845]  eta: 0:03:43  loss: 19.1994 (19.8875)  loss_n_40: 4.4789 (4.5979)  loss_n_60: 5.4143 (5.5516)  loss_n_80: 4.5797 (4.6507)  loss_n_100: 4.3789 (4.6615)  triple_100: 0.0000 (0.1689)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.2307)  triple_40: 0.0000 (0.0263)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [190/845]  eta: 0:03:40  loss: 18.9597 (19.8338)  loss_n_40: 4.4789 (4.5893)  loss_n_60: 5.3776 (5.5490)  loss_n_80: 4.3633 (4.6444)  loss_n_100: 4.3095 (4.6475)  triple_100: 0.0000 (0.1600)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.2186)  triple_40: 0.0000 (0.0249)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [200/845]  eta: 0:03:36  loss: 18.7135 (19.7991)  loss_n_40: 4.4410 (4.5839)  loss_n_60: 5.6018 (5.5522)  loss_n_80: 4.4478 (4.6432)  loss_n_100: 4.2133 (4.6363)  triple_100: 0.0000 (0.1521)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.2077)  triple_40: 0.0000 (0.0237)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [210/845]  eta: 0:03:33  loss: 18.9746 (19.7691)  loss_n_40: 4.6610 (4.5854)  loss_n_60: 5.7142 (5.5586)  loss_n_80: 4.4478 (4.6351)  loss_n_100: 4.2610 (4.6246)  triple_100: 0.0000 (0.1449)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1979)  triple_40: 0.0000 (0.0225)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [220/845]  eta: 0:03:30  loss: 18.5136 (19.7552)  loss_n_40: 4.4605 (4.5865)  loss_n_60: 5.5681 (5.5633)  loss_n_80: 4.2753 (4.6315)  loss_n_100: 4.2590 (4.6209)  triple_100: 0.0000 (0.1425)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1889)  triple_40: 0.0000 (0.0215)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [230/845]  eta: 0:03:26  loss: 18.5016 (19.7277)  loss_n_40: 4.4453 (4.5799)  loss_n_60: 5.4833 (5.5540)  loss_n_80: 4.3387 (4.6318)  loss_n_100: 4.3615 (4.6244)  triple_100: 0.0000 (0.1363)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1807)  triple_40: 0.0000 (0.0206)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [240/845]  eta: 0:03:23  loss: 18.7029 (19.7341)  loss_n_40: 4.5110 (4.5813)  loss_n_60: 5.4329 (5.5584)  loss_n_80: 4.6506 (4.6370)  loss_n_100: 4.4050 (4.6263)  triple_100: 0.0000 (0.1381)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1732)  triple_40: 0.0000 (0.0197)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [250/845]  eta: 0:03:19  loss: 19.3542 (19.6985)  loss_n_40: 4.6538 (4.5763)  loss_n_60: 5.5282 (5.5585)  loss_n_80: 4.6506 (4.6315)  loss_n_100: 4.3633 (4.6143)  triple_100: 0.0000 (0.1326)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1663)  triple_40: 0.0000 (0.0189)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [260/845]  eta: 0:03:16  loss: 19.1712 (19.6923)  loss_n_40: 4.4676 (4.5759)  loss_n_60: 5.5282 (5.5564)  loss_n_80: 4.4931 (4.6301)  loss_n_100: 4.4601 (4.6179)  triple_100: 0.0000 (0.1339)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1600)  triple_40: 0.0000 (0.0182)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [270/845]  eta: 0:03:13  loss: 19.1712 (19.6691)  loss_n_40: 4.4676 (4.5726)  loss_n_60: 5.5861 (5.5594)  loss_n_80: 4.4286 (4.6275)  loss_n_100: 4.3893 (4.6090)  triple_100: 0.0000 (0.1289)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1541)  triple_40: 0.0000 (0.0175)  time: 0.3336  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [280/845]  eta: 0:03:09  loss: 19.0857 (19.6387)  loss_n_40: 4.5606 (4.5676)  loss_n_60: 5.5200 (5.5543)  loss_n_80: 4.3222 (4.6233)  loss_n_100: 4.3141 (4.6036)  triple_100: 0.0000 (0.1243)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1486)  triple_40: 0.0000 (0.0169)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [290/845]  eta: 0:03:06  loss: 19.0857 (19.6468)  loss_n_40: 4.6738 (4.5746)  loss_n_60: 5.7993 (5.5635)  loss_n_80: 4.3142 (4.6223)  loss_n_100: 4.3366 (4.6008)  triple_100: 0.0000 (0.1201)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1492)  triple_40: 0.0000 (0.0163)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [300/845]  eta: 0:03:02  loss: 18.9309 (19.6719)  loss_n_40: 4.5819 (4.5760)  loss_n_60: 5.6280 (5.5651)  loss_n_80: 4.2981 (4.6203)  loss_n_100: 4.2036 (4.6005)  triple_100: 0.0000 (0.1161)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1782)  triple_40: 0.0000 (0.0158)  time: 0.3345  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:4]  [310/845]  eta: 0:02:59  loss: 18.7031 (19.6911)  loss_n_40: 4.5454 (4.5851)  loss_n_60: 5.5136 (5.5765)  loss_n_80: 4.1892 (4.6262)  loss_n_100: 4.2090 (4.6031)  triple_100: 0.0000 (0.1123)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1725)  triple_40: 0.0000 (0.0153)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [320/845]  eta: 0:02:56  loss: 19.1339 (19.6800)  loss_n_40: 4.6581 (4.5844)  loss_n_60: 5.7763 (5.5795)  loss_n_80: 4.5347 (4.6257)  loss_n_100: 4.3921 (4.5989)  triple_100: 0.0000 (0.1096)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1671)  triple_40: 0.0000 (0.0148)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [330/845]  eta: 0:02:52  loss: 19.4878 (19.7069)  loss_n_40: 4.7034 (4.5954)  loss_n_60: 5.8110 (5.5915)  loss_n_80: 4.5828 (4.6309)  loss_n_100: 4.5049 (4.6064)  triple_100: 0.0000 (0.1063)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1621)  triple_40: 0.0000 (0.0144)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [340/845]  eta: 0:02:49  loss: 19.4878 (19.6834)  loss_n_40: 4.6823 (4.5906)  loss_n_60: 5.8108 (5.5897)  loss_n_80: 4.5278 (4.6270)  loss_n_100: 4.3980 (4.6016)  triple_100: 0.0000 (0.1032)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1573)  triple_40: 0.0000 (0.0139)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [350/845]  eta: 0:02:46  loss: 18.9983 (19.7053)  loss_n_40: 4.5627 (4.5997)  loss_n_60: 5.6852 (5.5959)  loss_n_80: 4.4204 (4.6317)  loss_n_100: 4.3370 (4.6089)  triple_100: 0.0000 (0.1002)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1553)  triple_40: 0.0000 (0.0135)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [360/845]  eta: 0:02:42  loss: 19.0088 (19.6951)  loss_n_40: 4.5627 (4.6003)  loss_n_60: 5.6852 (5.5975)  loss_n_80: 4.7364 (4.6315)  loss_n_100: 4.3938 (4.6041)  triple_100: 0.0000 (0.0974)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1510)  triple_40: 0.0000 (0.0132)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [370/845]  eta: 0:02:39  loss: 19.1332 (19.7137)  loss_n_40: 4.6267 (4.6065)  loss_n_60: 5.7131 (5.6011)  loss_n_80: 4.6615 (4.6337)  loss_n_100: 4.3938 (4.6093)  triple_100: 0.0000 (0.1035)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1469)  triple_40: 0.0000 (0.0128)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [380/845]  eta: 0:02:35  loss: 19.8957 (19.7286)  loss_n_40: 4.8364 (4.6124)  loss_n_60: 5.8877 (5.6105)  loss_n_80: 4.7219 (4.6390)  loss_n_100: 4.5101 (4.6104)  triple_100: 0.0000 (0.1008)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1431)  triple_40: 0.0000 (0.0125)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [390/845]  eta: 0:02:32  loss: 20.0865 (19.7644)  loss_n_40: 4.8775 (4.6207)  loss_n_60: 6.0051 (5.6176)  loss_n_80: 4.9024 (4.6447)  loss_n_100: 4.6222 (4.6174)  triple_100: 0.0000 (0.0982)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1537)  triple_40: 0.0000 (0.0122)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [400/845]  eta: 0:02:29  loss: 19.5111 (19.7396)  loss_n_40: 4.6774 (4.6150)  loss_n_60: 5.7127 (5.6140)  loss_n_80: 4.6650 (4.6412)  loss_n_100: 4.4870 (4.6118)  triple_100: 0.0000 (0.0957)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1499)  triple_40: 0.0000 (0.0119)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [410/845]  eta: 0:02:25  loss: 19.4495 (19.7554)  loss_n_40: 4.5850 (4.6151)  loss_n_60: 5.5698 (5.6149)  loss_n_80: 4.5804 (4.6475)  loss_n_100: 4.4873 (4.6160)  triple_100: 0.0000 (0.0997)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1508)  triple_40: 0.0000 (0.0116)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [420/845]  eta: 0:02:22  loss: 19.3863 (19.7866)  loss_n_40: 4.6618 (4.6186)  loss_n_60: 5.7498 (5.6180)  loss_n_80: 4.5072 (4.6466)  loss_n_100: 4.4873 (4.6146)  triple_100: 0.0000 (0.1089)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1685)  triple_40: 0.0000 (0.0113)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [430/845]  eta: 0:02:19  loss: 19.2407 (19.7922)  loss_n_40: 4.7600 (4.6200)  loss_n_60: 5.8847 (5.6198)  loss_n_80: 4.4947 (4.6512)  loss_n_100: 4.4073 (4.6179)  triple_100: 0.0000 (0.1075)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1646)  triple_40: 0.0000 (0.0110)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [440/845]  eta: 0:02:15  loss: 20.4466 (19.8216)  loss_n_40: 4.8961 (4.6266)  loss_n_60: 5.9351 (5.6267)  loss_n_80: 4.8986 (4.6555)  loss_n_100: 4.6310 (4.6248)  triple_100: 0.0000 (0.1051)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1722)  triple_40: 0.0000 (0.0108)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [450/845]  eta: 0:02:12  loss: 19.7512 (19.8212)  loss_n_40: 4.8036 (4.6289)  loss_n_60: 5.7558 (5.6285)  loss_n_80: 4.6792 (4.6564)  loss_n_100: 4.3971 (4.6257)  triple_100: 0.0000 (0.1027)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1684)  triple_40: 0.0000 (0.0105)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [460/845]  eta: 0:02:08  loss: 18.3423 (19.8054)  loss_n_40: 4.4945 (4.6246)  loss_n_60: 5.5690 (5.6273)  loss_n_80: 4.3272 (4.6561)  loss_n_100: 4.2642 (4.6220)  triple_100: 0.0000 (0.1005)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1647)  triple_40: 0.0000 (0.0103)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [470/845]  eta: 0:02:05  loss: 19.2810 (19.8125)  loss_n_40: 4.8090 (4.6278)  loss_n_60: 5.9807 (5.6329)  loss_n_80: 4.3666 (4.6604)  loss_n_100: 4.3056 (4.6218)  triple_100: 0.0000 (0.0984)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1612)  triple_40: 0.0000 (0.0101)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [480/845]  eta: 0:02:02  loss: 19.7657 (19.8476)  loss_n_40: 4.9848 (4.6317)  loss_n_60: 5.8732 (5.6344)  loss_n_80: 4.8442 (4.6645)  loss_n_100: 4.6517 (4.6299)  triple_100: 0.0000 (0.1114)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1659)  triple_40: 0.0000 (0.0099)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [490/845]  eta: 0:01:58  loss: 19.7657 (19.8524)  loss_n_40: 4.6957 (4.6331)  loss_n_60: 5.6994 (5.6364)  loss_n_80: 4.8442 (4.6659)  loss_n_100: 4.6073 (4.6289)  triple_100: 0.0000 (0.1111)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1672)  triple_40: 0.0000 (0.0097)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [500/845]  eta: 0:01:55  loss: 18.8244 (19.8268)  loss_n_40: 4.3989 (4.6256)  loss_n_60: 5.5503 (5.6275)  loss_n_80: 4.3053 (4.6628)  loss_n_100: 4.3086 (4.6273)  triple_100: 0.0000 (0.1103)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1639)  triple_40: 0.0000 (0.0095)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [510/845]  eta: 0:01:52  loss: 19.3147 (19.8283)  loss_n_40: 4.4450 (4.6270)  loss_n_60: 5.5503 (5.6294)  loss_n_80: 4.5825 (4.6647)  loss_n_100: 4.5056 (4.6290)  triple_100: 0.0000 (0.1082)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1607)  triple_40: 0.0000 (0.0093)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [520/845]  eta: 0:01:48  loss: 19.5946 (19.8252)  loss_n_40: 4.7887 (4.6286)  loss_n_60: 5.9008 (5.6333)  loss_n_80: 4.3894 (4.6630)  loss_n_100: 4.3723 (4.6274)  triple_100: 0.0000 (0.1061)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1576)  triple_40: 0.0000 (0.0091)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [530/845]  eta: 0:01:45  loss: 18.7572 (19.8102)  loss_n_40: 4.5074 (4.6217)  loss_n_60: 5.6208 (5.6263)  loss_n_80: 4.2051 (4.6604)  loss_n_100: 4.2023 (4.6240)  triple_100: 0.0000 (0.1041)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1648)  triple_40: 0.0000 (0.0090)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [540/845]  eta: 0:01:42  loss: 18.7217 (19.8167)  loss_n_40: 4.5074 (4.6247)  loss_n_60: 5.6208 (5.6268)  loss_n_80: 4.2051 (4.6622)  loss_n_100: 4.2251 (4.6300)  triple_100: 0.0000 (0.1024)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1617)  triple_40: 0.0000 (0.0088)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [550/845]  eta: 0:01:38  loss: 19.5288 (19.8378)  loss_n_40: 4.7017 (4.6283)  loss_n_60: 5.8056 (5.6287)  loss_n_80: 4.7968 (4.6682)  loss_n_100: 4.7897 (4.6357)  triple_100: 0.0000 (0.1025)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1658)  triple_40: 0.0000 (0.0086)  time: 0.3340  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:4]  [560/845]  eta: 0:01:35  loss: 19.4465 (19.8228)  loss_n_40: 4.4909 (4.6249)  loss_n_60: 5.5628 (5.6266)  loss_n_80: 4.7100 (4.6672)  loss_n_100: 4.5714 (4.6322)  triple_100: 0.0000 (0.1006)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1628)  triple_40: 0.0000 (0.0085)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [570/845]  eta: 0:01:32  loss: 19.8357 (19.8184)  loss_n_40: 4.4120 (4.6226)  loss_n_60: 5.5628 (5.6258)  loss_n_80: 4.7100 (4.6674)  loss_n_100: 4.5365 (4.6297)  triple_100: 0.0000 (0.1046)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1600)  triple_40: 0.0000 (0.0083)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [580/845]  eta: 0:01:28  loss: 19.4666 (19.8094)  loss_n_40: 4.3910 (4.6220)  loss_n_60: 5.4808 (5.6271)  loss_n_80: 4.6329 (4.6653)  loss_n_100: 4.5365 (4.6266)  triple_100: 0.0000 (0.1028)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1572)  triple_40: 0.0000 (0.0082)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [590/845]  eta: 0:01:25  loss: 19.4666 (19.8297)  loss_n_40: 4.4545 (4.6235)  loss_n_60: 5.4808 (5.6281)  loss_n_80: 4.8341 (4.6696)  loss_n_100: 4.5649 (4.6315)  triple_100: 0.0000 (0.1014)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1676)  triple_40: 0.0000 (0.0080)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [600/845]  eta: 0:01:22  loss: 20.2705 (19.8347)  loss_n_40: 4.8119 (4.6254)  loss_n_60: 5.9022 (5.6322)  loss_n_80: 4.8870 (4.6733)  loss_n_100: 4.7152 (4.6314)  triple_100: 0.0000 (0.0997)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1648)  triple_40: 0.0000 (0.0079)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [610/845]  eta: 0:01:18  loss: 20.1696 (19.8287)  loss_n_40: 4.6676 (4.6250)  loss_n_60: 5.8524 (5.6333)  loss_n_80: 4.6825 (4.6728)  loss_n_100: 4.5706 (4.6292)  triple_100: 0.0000 (0.0981)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1624)  triple_40: 0.0000 (0.0078)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [620/845]  eta: 0:01:15  loss: 20.0675 (19.8276)  loss_n_40: 4.6676 (4.6241)  loss_n_60: 5.7008 (5.6323)  loss_n_80: 4.7872 (4.6749)  loss_n_100: 4.6799 (4.6302)  triple_100: 0.0000 (0.0986)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1598)  triple_40: 0.0000 (0.0077)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [630/845]  eta: 0:01:11  loss: 19.2420 (19.8353)  loss_n_40: 4.6442 (4.6263)  loss_n_60: 5.7101 (5.6333)  loss_n_80: 4.6275 (4.6743)  loss_n_100: 4.4658 (4.6331)  triple_100: 0.0000 (0.1035)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1572)  triple_40: 0.0000 (0.0075)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [640/845]  eta: 0:01:08  loss: 18.7668 (19.8236)  loss_n_40: 4.5644 (4.6256)  loss_n_60: 5.7631 (5.6338)  loss_n_80: 4.3139 (4.6702)  loss_n_100: 4.2611 (4.6300)  triple_100: 0.0000 (0.1019)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1548)  triple_40: 0.0000 (0.0074)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [650/845]  eta: 0:01:05  loss: 19.4315 (19.8219)  loss_n_40: 4.6615 (4.6243)  loss_n_60: 5.6777 (5.6316)  loss_n_80: 4.3139 (4.6679)  loss_n_100: 4.2354 (4.6317)  triple_100: 0.0000 (0.1068)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1524)  triple_40: 0.0000 (0.0073)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [660/845]  eta: 0:01:01  loss: 19.6330 (19.8208)  loss_n_40: 4.8535 (4.6250)  loss_n_60: 5.6777 (5.6323)  loss_n_80: 4.4169 (4.6690)  loss_n_100: 4.3503 (4.6320)  triple_100: 0.0000 (0.1052)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1501)  triple_40: 0.0000 (0.0072)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [670/845]  eta: 0:00:58  loss: 20.6365 (19.8339)  loss_n_40: 4.9208 (4.6286)  loss_n_60: 6.0331 (5.6365)  loss_n_80: 4.9423 (4.6743)  loss_n_100: 4.7057 (4.6360)  triple_100: 0.0000 (0.1036)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1479)  triple_40: 0.0000 (0.0071)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [680/845]  eta: 0:00:55  loss: 21.0448 (19.8465)  loss_n_40: 4.9542 (4.6307)  loss_n_60: 6.0591 (5.6388)  loss_n_80: 4.9567 (4.6774)  loss_n_100: 4.7506 (4.6408)  triple_100: 0.0000 (0.1060)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1457)  triple_40: 0.0000 (0.0070)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [690/845]  eta: 0:00:51  loss: 19.3176 (19.8436)  loss_n_40: 4.6201 (4.6307)  loss_n_60: 5.8228 (5.6413)  loss_n_80: 4.6952 (4.6777)  loss_n_100: 4.6036 (4.6389)  triple_100: 0.0000 (0.1044)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1436)  triple_40: 0.0000 (0.0069)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [700/845]  eta: 0:00:48  loss: 20.1188 (19.8570)  loss_n_40: 4.6811 (4.6346)  loss_n_60: 5.8228 (5.6446)  loss_n_80: 4.7897 (4.6810)  loss_n_100: 4.6485 (4.6427)  triple_100: 0.0000 (0.1034)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1439)  triple_40: 0.0000 (0.0068)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [710/845]  eta: 0:00:45  loss: 19.8525 (19.8425)  loss_n_40: 4.6811 (4.6318)  loss_n_60: 5.8136 (5.6429)  loss_n_80: 4.6759 (4.6765)  loss_n_100: 4.5452 (4.6381)  triple_100: 0.0000 (0.1047)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1419)  triple_40: 0.0000 (0.0067)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [720/845]  eta: 0:00:41  loss: 19.8241 (19.8740)  loss_n_40: 4.6835 (4.6375)  loss_n_60: 5.8677 (5.6484)  loss_n_80: 4.6759 (4.6800)  loss_n_100: 4.4787 (4.6417)  triple_100: 0.0000 (0.1193)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1405)  triple_40: 0.0000 (0.0066)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [730/845]  eta: 0:00:38  loss: 20.8442 (19.8783)  loss_n_40: 4.9929 (4.6401)  loss_n_60: 5.8871 (5.6510)  loss_n_80: 4.8722 (4.6809)  loss_n_100: 4.6476 (4.6436)  triple_100: 0.0000 (0.1177)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1386)  triple_40: 0.0000 (0.0065)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [740/845]  eta: 0:00:35  loss: 18.3961 (19.8544)  loss_n_40: 4.5625 (4.6339)  loss_n_60: 5.3670 (5.6434)  loss_n_80: 4.1728 (4.6759)  loss_n_100: 4.1556 (4.6409)  triple_100: 0.0000 (0.1170)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1367)  triple_40: 0.0000 (0.0064)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [750/845]  eta: 0:00:31  loss: 18.2398 (19.8398)  loss_n_40: 4.2343 (4.6288)  loss_n_60: 5.1295 (5.6366)  loss_n_80: 4.1728 (4.6729)  loss_n_100: 4.0803 (4.6392)  triple_100: 0.0000 (0.1155)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1404)  triple_40: 0.0000 (0.0063)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [760/845]  eta: 0:00:28  loss: 18.8794 (19.8440)  loss_n_40: 4.2754 (4.6279)  loss_n_60: 5.2269 (5.6341)  loss_n_80: 4.6689 (4.6742)  loss_n_100: 4.4294 (4.6444)  triple_100: 0.0000 (0.1185)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1386)  triple_40: 0.0000 (0.0062)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [770/845]  eta: 0:00:25  loss: 19.0325 (19.8526)  loss_n_40: 4.6013 (4.6276)  loss_n_60: 5.6943 (5.6320)  loss_n_80: 4.4151 (4.6737)  loss_n_100: 4.3364 (4.6461)  triple_100: 0.0000 (0.1266)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1404)  triple_40: 0.0000 (0.0062)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [780/845]  eta: 0:00:21  loss: 18.3992 (19.8321)  loss_n_40: 4.5457 (4.6242)  loss_n_60: 5.6253 (5.6299)  loss_n_80: 4.1695 (4.6685)  loss_n_100: 4.1123 (4.6399)  triple_100: 0.0000 (0.1250)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1386)  triple_40: 0.0000 (0.0061)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [790/845]  eta: 0:00:18  loss: 18.0793 (19.8138)  loss_n_40: 4.4120 (4.6192)  loss_n_60: 5.4677 (5.6257)  loss_n_80: 4.1658 (4.6656)  loss_n_100: 4.1123 (4.6364)  triple_100: 0.0000 (0.1241)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1369)  triple_40: 0.0000 (0.0060)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [800/845]  eta: 0:00:15  loss: 18.0793 (19.8065)  loss_n_40: 4.1637 (4.6180)  loss_n_60: 5.1975 (5.6248)  loss_n_80: 4.3951 (4.6647)  loss_n_100: 4.3125 (4.6353)  triple_100: 0.0000 (0.1226)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1352)  triple_40: 0.0000 (0.0059)  time: 0.3343  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:4]  [810/845]  eta: 0:00:11  loss: 19.3972 (19.8051)  loss_n_40: 4.6800 (4.6197)  loss_n_60: 5.6967 (5.6265)  loss_n_80: 4.4651 (4.6637)  loss_n_100: 4.3661 (4.6348)  triple_100: 0.0000 (0.1211)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1335)  triple_40: 0.0000 (0.0059)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [820/845]  eta: 0:00:08  loss: 19.9971 (19.8224)  loss_n_40: 4.8197 (4.6234)  loss_n_60: 5.9862 (5.6308)  loss_n_80: 4.7218 (4.6694)  loss_n_100: 4.8651 (4.6406)  triple_100: 0.0000 (0.1206)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.1319)  triple_40: 0.0000 (0.0058)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [830/845]  eta: 0:00:05  loss: 21.0366 (19.8697)  loss_n_40: 4.8197 (4.6226)  loss_n_60: 5.9406 (5.6294)  loss_n_80: 5.2615 (4.6681)  loss_n_100: 4.8415 (4.6392)  triple_100: 0.0000 (0.1241)  triple_80: 0.0000 (0.0004)  triple_60: 0.0000 (0.1642)  triple_40: 0.0000 (0.0215)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [840/845]  eta: 0:00:01  loss: 19.8062 (19.8688)  loss_n_40: 4.6973 (4.6218)  loss_n_60: 5.7501 (5.6281)  loss_n_80: 4.7315 (4.6685)  loss_n_100: 4.5155 (4.6394)  triple_100: 0.0000 (0.1271)  triple_80: 0.0000 (0.0004)  triple_60: 0.0000 (0.1623)  triple_40: 0.0000 (0.0213)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4]  [844/845]  eta: 0:00:00  loss: 19.8062 (19.8609)  loss_n_40: 4.3173 (4.6205)  loss_n_60: 5.3984 (5.6271)  loss_n_80: 4.7315 (4.6667)  loss_n_100: 4.5155 (4.6370)  triple_100: 0.0000 (0.1265)  triple_80: 0.0000 (0.0004)  triple_60: 0.0000 (0.1615)  triple_40: 0.0000 (0.0212)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:4] Total time: 0:04:42 (0.3349 s / it)\n",
      "Averaged stats: loss: 19.8062 (19.8609)  loss_n_40: 4.3173 (4.6205)  loss_n_60: 5.3984 (5.6271)  loss_n_80: 4.7315 (4.6667)  loss_n_100: 4.5155 (4.6370)  triple_100: 0.0000 (0.1265)  triple_80: 0.0000 (0.0004)  triple_60: 0.0000 (0.1615)  triple_40: 0.0000 (0.0212)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_4_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 4.637%\n",
      "Min loss_n_100: 4.637\n",
      "Best Epoch: 4.000\n",
      "Train: [epoch:5]  [   0/1724]  eta: 1:59:11  lr: 0.000080  loss: 18.6479 (18.6479)  loss_n_40: 4.4401 (4.4401)  loss_n_60: 5.4799 (5.4799)  loss_n_80: 4.3930 (4.3930)  loss_n_100: 4.3350 (4.3350)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1482  data: 0.3956  max mem: 46473\n",
      "Train: [epoch:5]  [  10/1724]  eta: 1:52:33  lr: 0.000080  loss: 19.6331 (19.3887)  loss_n_40: 4.5758 (4.5568)  loss_n_60: 5.5816 (5.5865)  loss_n_80: 4.6717 (4.6574)  loss_n_100: 4.3350 (4.4322)  triple_100: 0.0000 (0.0358)  triple_80: 0.0000 (0.0467)  triple_60: 0.0000 (0.0733)  triple_40: 0.0000 (0.0000)  time: 3.9405  data: 0.0361  max mem: 46473\n",
      "Train: [epoch:5]  [  20/1724]  eta: 1:51:38  lr: 0.000080  loss: 20.0435 (20.4821)  loss_n_40: 4.7200 (4.6507)  loss_n_60: 5.7452 (5.6933)  loss_n_80: 4.7589 (4.7720)  loss_n_100: 4.6599 (4.6754)  triple_100: 0.0000 (0.0483)  triple_80: 0.0000 (0.0245)  triple_60: 0.0000 (0.4200)  triple_40: 0.0000 (0.1979)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [  30/1724]  eta: 1:50:53  lr: 0.000080  loss: 20.5708 (20.4211)  loss_n_40: 4.7453 (4.6467)  loss_n_60: 5.8137 (5.6973)  loss_n_80: 4.8402 (4.7913)  loss_n_100: 4.8612 (4.7735)  triple_100: 0.0000 (0.0683)  triple_80: 0.0000 (0.0166)  triple_60: 0.0000 (0.2933)  triple_40: 0.0000 (0.1341)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [  40/1724]  eta: 1:50:10  lr: 0.000080  loss: 20.1655 (20.2884)  loss_n_40: 4.6494 (4.6409)  loss_n_60: 5.6196 (5.6677)  loss_n_80: 4.8527 (4.8002)  loss_n_100: 4.5647 (4.7250)  triple_100: 0.0000 (0.0516)  triple_80: 0.0000 (0.0367)  triple_60: 0.0000 (0.2649)  triple_40: 0.0000 (0.1014)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [  50/1724]  eta: 1:49:28  lr: 0.000080  loss: 19.7572 (20.2339)  loss_n_40: 4.6269 (4.6579)  loss_n_60: 5.5999 (5.6570)  loss_n_80: 4.8103 (4.7965)  loss_n_100: 4.5427 (4.7245)  triple_100: 0.0000 (0.0415)  triple_80: 0.0000 (0.0366)  triple_60: 0.0000 (0.2281)  triple_40: 0.0000 (0.0918)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [  60/1724]  eta: 1:48:47  lr: 0.000080  loss: 19.7054 (20.1299)  loss_n_40: 4.7159 (4.6637)  loss_n_60: 5.6642 (5.6705)  loss_n_80: 4.7338 (4.7823)  loss_n_100: 4.5427 (4.6806)  triple_100: 0.0000 (0.0347)  triple_80: 0.0000 (0.0306)  triple_60: 0.0000 (0.1907)  triple_40: 0.0000 (0.0767)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [  70/1724]  eta: 1:48:07  lr: 0.000080  loss: 19.7054 (20.0463)  loss_n_40: 4.6646 (4.6428)  loss_n_60: 5.5719 (5.6333)  loss_n_80: 4.7042 (4.7545)  loss_n_100: 4.4960 (4.6555)  triple_100: 0.0000 (0.0298)  triple_80: 0.0000 (0.0480)  triple_60: 0.0000 (0.2156)  triple_40: 0.0000 (0.0670)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [  80/1724]  eta: 1:47:27  lr: 0.000080  loss: 19.1900 (19.9459)  loss_n_40: 4.4407 (4.6112)  loss_n_60: 5.3566 (5.6017)  loss_n_80: 4.6032 (4.7275)  loss_n_100: 4.4960 (4.6534)  triple_100: 0.0000 (0.0619)  triple_80: 0.0000 (0.0420)  triple_60: 0.0000 (0.1890)  triple_40: 0.0000 (0.0591)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [  90/1724]  eta: 1:46:47  lr: 0.000080  loss: 19.9991 (20.2467)  loss_n_40: 4.3485 (4.5936)  loss_n_60: 5.3914 (5.6038)  loss_n_80: 4.8724 (4.8118)  loss_n_100: 5.4026 (4.8087)  triple_100: 0.0000 (0.1239)  triple_80: 0.0000 (0.0447)  triple_60: 0.0000 (0.2076)  triple_40: 0.0000 (0.0526)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 100/1724]  eta: 1:46:08  lr: 0.000080  loss: 22.3490 (20.6040)  loss_n_40: 4.5216 (4.5996)  loss_n_60: 5.6807 (5.6228)  loss_n_80: 5.5899 (4.8984)  loss_n_100: 6.0187 (4.9323)  triple_100: 0.0000 (0.1283)  triple_80: 0.0000 (0.0793)  triple_60: 0.0000 (0.2510)  triple_40: 0.0000 (0.0923)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 110/1724]  eta: 1:45:28  lr: 0.000080  loss: 21.8729 (20.7055)  loss_n_40: 4.5216 (4.5863)  loss_n_60: 5.6807 (5.6215)  loss_n_80: 5.6292 (4.9793)  loss_n_100: 5.9666 (5.0171)  triple_100: 0.0000 (0.1167)  triple_80: 0.0000 (0.0721)  triple_60: 0.0000 (0.2284)  triple_40: 0.0000 (0.0840)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 120/1724]  eta: 1:44:49  lr: 0.000080  loss: 21.1750 (20.7027)  loss_n_40: 4.4822 (4.5788)  loss_n_60: 5.6160 (5.6162)  loss_n_80: 5.4536 (4.9963)  loss_n_100: 5.5751 (5.0490)  triple_100: 0.0000 (0.1071)  triple_80: 0.0000 (0.0662)  triple_60: 0.0000 (0.2121)  triple_40: 0.0000 (0.0770)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 130/1724]  eta: 1:44:09  lr: 0.000080  loss: 20.6828 (20.7412)  loss_n_40: 4.5109 (4.5830)  loss_n_60: 5.6160 (5.6124)  loss_n_80: 5.0360 (5.0048)  loss_n_100: 5.3023 (5.0666)  triple_100: 0.0000 (0.0992)  triple_80: 0.0000 (0.0611)  triple_60: 0.0000 (0.2143)  triple_40: 0.0000 (0.0998)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 140/1724]  eta: 1:43:30  lr: 0.000080  loss: 21.0847 (20.8563)  loss_n_40: 4.5960 (4.5743)  loss_n_60: 5.6331 (5.6031)  loss_n_80: 5.1842 (5.0463)  loss_n_100: 5.7040 (5.1736)  triple_100: 0.0000 (0.1098)  triple_80: 0.0000 (0.0575)  triple_60: 0.0000 (0.1991)  triple_40: 0.0000 (0.0927)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 150/1724]  eta: 1:42:51  lr: 0.000080  loss: 21.3782 (20.9094)  loss_n_40: 4.6181 (4.5797)  loss_n_60: 5.6618 (5.6063)  loss_n_80: 5.2836 (5.0565)  loss_n_100: 6.1555 (5.2165)  triple_100: 0.0000 (0.1034)  triple_80: 0.0000 (0.0592)  triple_60: 0.0000 (0.1867)  triple_40: 0.0000 (0.1011)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 160/1724]  eta: 1:42:12  lr: 0.000080  loss: 19.9494 (20.8249)  loss_n_40: 4.5004 (4.5664)  loss_n_60: 5.5419 (5.5916)  loss_n_80: 5.0080 (5.0347)  loss_n_100: 5.1551 (5.1994)  triple_100: 0.0000 (0.1008)  triple_80: 0.0000 (0.0555)  triple_60: 0.0000 (0.1818)  triple_40: 0.0000 (0.0948)  time: 3.9210  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [ 170/1724]  eta: 1:41:32  lr: 0.000080  loss: 19.5398 (20.7780)  loss_n_40: 4.4034 (4.5617)  loss_n_60: 5.3769 (5.5847)  loss_n_80: 4.7985 (5.0260)  loss_n_100: 4.8128 (5.1769)  triple_100: 0.0000 (0.0977)  triple_80: 0.0000 (0.0544)  triple_60: 0.0000 (0.1811)  triple_40: 0.0000 (0.0956)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 180/1724]  eta: 1:40:53  lr: 0.000080  loss: 19.4925 (20.7130)  loss_n_40: 4.4287 (4.5513)  loss_n_60: 5.3769 (5.5711)  loss_n_80: 4.8313 (5.0068)  loss_n_100: 4.7759 (5.1586)  triple_100: 0.0000 (0.0948)  triple_80: 0.0000 (0.0514)  triple_60: 0.0000 (0.1803)  triple_40: 0.0000 (0.0987)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 190/1724]  eta: 1:40:14  lr: 0.000080  loss: 19.4925 (20.6621)  loss_n_40: 4.4789 (4.5499)  loss_n_60: 5.4848 (5.5704)  loss_n_80: 4.7692 (4.9950)  loss_n_100: 4.7466 (5.1409)  triple_100: 0.0000 (0.0901)  triple_80: 0.0000 (0.0515)  triple_60: 0.0000 (0.1709)  triple_40: 0.0000 (0.0935)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 200/1724]  eta: 1:39:35  lr: 0.000080  loss: 20.1092 (20.6444)  loss_n_40: 4.5405 (4.5494)  loss_n_60: 5.6054 (5.5694)  loss_n_80: 4.8251 (4.9833)  loss_n_100: 4.8316 (5.1240)  triple_100: 0.0000 (0.0930)  triple_80: 0.0000 (0.0549)  triple_60: 0.0000 (0.1799)  triple_40: 0.0000 (0.0904)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 210/1724]  eta: 1:38:55  lr: 0.000080  loss: 20.1364 (20.6095)  loss_n_40: 4.5538 (4.5501)  loss_n_60: 5.6054 (5.5702)  loss_n_80: 4.8601 (4.9752)  loss_n_100: 4.7222 (5.1021)  triple_100: 0.0000 (0.0925)  triple_80: 0.0000 (0.0543)  triple_60: 0.0000 (0.1790)  triple_40: 0.0000 (0.0861)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 220/1724]  eta: 1:38:16  lr: 0.000080  loss: 19.9028 (20.5544)  loss_n_40: 4.5995 (4.5471)  loss_n_60: 5.5121 (5.5649)  loss_n_80: 4.8021 (4.9598)  loss_n_100: 4.5498 (5.0776)  triple_100: 0.0000 (0.0897)  triple_80: 0.0000 (0.0562)  triple_60: 0.0000 (0.1738)  triple_40: 0.0000 (0.0853)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 230/1724]  eta: 1:37:37  lr: 0.000080  loss: 19.1789 (20.4772)  loss_n_40: 4.3762 (4.5356)  loss_n_60: 5.3690 (5.5498)  loss_n_80: 4.5483 (4.9409)  loss_n_100: 4.4901 (5.0515)  triple_100: 0.0000 (0.0876)  triple_80: 0.0000 (0.0537)  triple_60: 0.0000 (0.1765)  triple_40: 0.0000 (0.0816)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 240/1724]  eta: 1:36:57  lr: 0.000080  loss: 18.3984 (20.3814)  loss_n_40: 4.3030 (4.5247)  loss_n_60: 5.2677 (5.5359)  loss_n_80: 4.3488 (4.9174)  loss_n_100: 4.4057 (5.0194)  triple_100: 0.0000 (0.0845)  triple_80: 0.0000 (0.0521)  triple_60: 0.0000 (0.1692)  triple_40: 0.0000 (0.0782)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 250/1724]  eta: 1:36:18  lr: 0.000080  loss: 18.3655 (20.3039)  loss_n_40: 4.3567 (4.5188)  loss_n_60: 5.2251 (5.5265)  loss_n_80: 4.3181 (4.8988)  loss_n_100: 4.2082 (4.9873)  triple_100: 0.0000 (0.0827)  triple_80: 0.0000 (0.0501)  triple_60: 0.0000 (0.1647)  triple_40: 0.0000 (0.0751)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 260/1724]  eta: 1:35:39  lr: 0.000080  loss: 18.4472 (20.2462)  loss_n_40: 4.3967 (4.5146)  loss_n_60: 5.3280 (5.5176)  loss_n_80: 4.3905 (4.8812)  loss_n_100: 4.2526 (4.9596)  triple_100: 0.0000 (0.0796)  triple_80: 0.0000 (0.0501)  triple_60: 0.0000 (0.1664)  triple_40: 0.0000 (0.0773)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 270/1724]  eta: 1:35:00  lr: 0.000080  loss: 18.6878 (20.2305)  loss_n_40: 4.3306 (4.5039)  loss_n_60: 5.1748 (5.5029)  loss_n_80: 4.5035 (4.8692)  loss_n_100: 4.3581 (4.9495)  triple_100: 0.0000 (0.0939)  triple_80: 0.0000 (0.0524)  triple_60: 0.0000 (0.1813)  triple_40: 0.0000 (0.0773)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 280/1724]  eta: 1:34:20  lr: 0.000080  loss: 19.2556 (20.2323)  loss_n_40: 4.3306 (4.5032)  loss_n_60: 5.2397 (5.5019)  loss_n_80: 4.6218 (4.8707)  loss_n_100: 4.9463 (4.9594)  triple_100: 0.0000 (0.0905)  triple_80: 0.0000 (0.0505)  triple_60: 0.0000 (0.1749)  triple_40: 0.0000 (0.0812)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 290/1724]  eta: 1:33:41  lr: 0.000080  loss: 19.3857 (20.1997)  loss_n_40: 4.4310 (4.4996)  loss_n_60: 5.3690 (5.4953)  loss_n_80: 4.7621 (4.8640)  loss_n_100: 4.8898 (4.9484)  triple_100: 0.0000 (0.0888)  triple_80: 0.0000 (0.0520)  triple_60: 0.0000 (0.1733)  triple_40: 0.0000 (0.0784)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 300/1724]  eta: 1:33:02  lr: 0.000080  loss: 18.9385 (20.1644)  loss_n_40: 4.4278 (4.4981)  loss_n_60: 5.3729 (5.4911)  loss_n_80: 4.6440 (4.8580)  loss_n_100: 4.5752 (4.9349)  triple_100: 0.0000 (0.0858)  triple_80: 0.0000 (0.0532)  triple_60: 0.0000 (0.1675)  triple_40: 0.0000 (0.0758)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 310/1724]  eta: 1:32:23  lr: 0.000080  loss: 19.0454 (20.1212)  loss_n_40: 4.2781 (4.4845)  loss_n_60: 5.2517 (5.4805)  loss_n_80: 4.6308 (4.8508)  loss_n_100: 4.4496 (4.9193)  triple_100: 0.0000 (0.0831)  triple_80: 0.0000 (0.0675)  triple_60: 0.0000 (0.1622)  triple_40: 0.0000 (0.0733)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 320/1724]  eta: 1:31:44  lr: 0.000080  loss: 19.0454 (20.0853)  loss_n_40: 4.1695 (4.4803)  loss_n_60: 5.2016 (5.4759)  loss_n_80: 4.4865 (4.8424)  loss_n_100: 4.4496 (4.9057)  triple_100: 0.0000 (0.0820)  triple_80: 0.0000 (0.0654)  triple_60: 0.0000 (0.1601)  triple_40: 0.0000 (0.0735)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 330/1724]  eta: 1:31:04  lr: 0.000080  loss: 18.0117 (20.0024)  loss_n_40: 4.1324 (4.4663)  loss_n_60: 5.1272 (5.4602)  loss_n_80: 4.4157 (4.8249)  loss_n_100: 4.1896 (4.8804)  triple_100: 0.0000 (0.0795)  triple_80: 0.0000 (0.0635)  triple_60: 0.0000 (0.1564)  triple_40: 0.0000 (0.0713)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 340/1724]  eta: 1:30:25  lr: 0.000080  loss: 17.2761 (19.9654)  loss_n_40: 4.0351 (4.4616)  loss_n_60: 4.9550 (5.4515)  loss_n_80: 4.3518 (4.8129)  loss_n_100: 4.1257 (4.8644)  triple_100: 0.0000 (0.0784)  triple_80: 0.0000 (0.0616)  triple_60: 0.0000 (0.1611)  triple_40: 0.0000 (0.0740)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 350/1724]  eta: 1:29:46  lr: 0.000080  loss: 17.8227 (19.8918)  loss_n_40: 4.1539 (4.4521)  loss_n_60: 5.0678 (5.4401)  loss_n_80: 4.2425 (4.7945)  loss_n_100: 4.0225 (4.8384)  triple_100: 0.0000 (0.0785)  triple_80: 0.0000 (0.0599)  triple_60: 0.0000 (0.1565)  triple_40: 0.0000 (0.0719)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 360/1724]  eta: 1:29:07  lr: 0.000080  loss: 17.8227 (19.8570)  loss_n_40: 4.1260 (4.4472)  loss_n_60: 5.0527 (5.4323)  loss_n_80: 4.2904 (4.7851)  loss_n_100: 4.0934 (4.8244)  triple_100: 0.0000 (0.0764)  triple_80: 0.0000 (0.0603)  triple_60: 0.0000 (0.1593)  triple_40: 0.0000 (0.0719)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 370/1724]  eta: 1:28:27  lr: 0.000080  loss: 18.2908 (19.8379)  loss_n_40: 4.1774 (4.4428)  loss_n_60: 5.1125 (5.4263)  loss_n_80: 4.4705 (4.7782)  loss_n_100: 4.2634 (4.8106)  triple_100: 0.0000 (0.0764)  triple_80: 0.0000 (0.0587)  triple_60: 0.0000 (0.1685)  triple_40: 0.0000 (0.0764)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 380/1724]  eta: 1:27:48  lr: 0.000080  loss: 17.9674 (19.7812)  loss_n_40: 4.1507 (4.4340)  loss_n_60: 5.1573 (5.4170)  loss_n_80: 4.4312 (4.7652)  loss_n_100: 4.2634 (4.7906)  triple_100: 0.0000 (0.0787)  triple_80: 0.0000 (0.0571)  triple_60: 0.0000 (0.1640)  triple_40: 0.0000 (0.0744)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 390/1724]  eta: 1:27:09  lr: 0.000080  loss: 17.8328 (19.7243)  loss_n_40: 4.1067 (4.4242)  loss_n_60: 4.9498 (5.4039)  loss_n_80: 4.3004 (4.7517)  loss_n_100: 4.1332 (4.7759)  triple_100: 0.0000 (0.0789)  triple_80: 0.0000 (0.0573)  triple_60: 0.0000 (0.1600)  triple_40: 0.0000 (0.0725)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 400/1724]  eta: 1:26:30  lr: 0.000080  loss: 17.1826 (19.6616)  loss_n_40: 3.9971 (4.4154)  loss_n_60: 4.8384 (5.3930)  loss_n_80: 4.1667 (4.7379)  loss_n_100: 3.9533 (4.7549)  triple_100: 0.0000 (0.0769)  triple_80: 0.0000 (0.0565)  triple_60: 0.0000 (0.1563)  triple_40: 0.0000 (0.0707)  time: 3.9188  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [ 410/1724]  eta: 1:25:50  lr: 0.000080  loss: 16.8710 (19.6013)  loss_n_40: 3.9989 (4.4070)  loss_n_60: 4.8902 (5.3822)  loss_n_80: 4.1661 (4.7245)  loss_n_100: 3.8419 (4.7331)  triple_100: 0.0000 (0.0758)  triple_80: 0.0000 (0.0572)  triple_60: 0.0000 (0.1525)  triple_40: 0.0000 (0.0690)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 420/1724]  eta: 1:25:11  lr: 0.000080  loss: 17.3717 (19.5928)  loss_n_40: 4.1108 (4.4026)  loss_n_60: 4.9339 (5.3753)  loss_n_80: 4.2960 (4.7190)  loss_n_100: 3.9327 (4.7238)  triple_100: 0.0000 (0.0740)  triple_80: 0.0000 (0.0601)  triple_60: 0.0000 (0.1578)  triple_40: 0.0000 (0.0802)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 430/1724]  eta: 1:24:32  lr: 0.000080  loss: 18.3308 (19.5568)  loss_n_40: 4.1629 (4.3978)  loss_n_60: 5.1478 (5.3705)  loss_n_80: 4.4449 (4.7105)  loss_n_100: 4.2310 (4.7123)  triple_100: 0.0000 (0.0732)  triple_80: 0.0000 (0.0587)  triple_60: 0.0000 (0.1553)  triple_40: 0.0000 (0.0783)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 440/1724]  eta: 1:23:53  lr: 0.000080  loss: 17.6017 (19.5191)  loss_n_40: 4.1472 (4.3920)  loss_n_60: 5.1071 (5.3640)  loss_n_80: 4.2000 (4.6996)  loss_n_100: 3.9973 (4.6956)  triple_100: 0.0000 (0.0723)  triple_80: 0.0000 (0.0580)  triple_60: 0.0000 (0.1577)  triple_40: 0.0000 (0.0799)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 450/1724]  eta: 1:23:14  lr: 0.000080  loss: 17.0199 (19.4654)  loss_n_40: 4.0367 (4.3846)  loss_n_60: 5.0391 (5.3569)  loss_n_80: 4.1231 (4.6855)  loss_n_100: 3.7805 (4.6747)  triple_100: 0.0000 (0.0721)  triple_80: 0.0000 (0.0567)  triple_60: 0.0000 (0.1568)  triple_40: 0.0000 (0.0781)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 460/1724]  eta: 1:22:35  lr: 0.000080  loss: 17.0199 (19.4211)  loss_n_40: 4.0315 (4.3791)  loss_n_60: 5.0353 (5.3494)  loss_n_80: 4.0553 (4.6749)  loss_n_100: 3.7908 (4.6619)  triple_100: 0.0000 (0.0705)  triple_80: 0.0000 (0.0555)  triple_60: 0.0000 (0.1534)  triple_40: 0.0000 (0.0764)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 470/1724]  eta: 1:21:55  lr: 0.000080  loss: 17.1145 (19.3728)  loss_n_40: 4.1209 (4.3720)  loss_n_60: 5.0555 (5.3406)  loss_n_80: 4.1620 (4.6640)  loss_n_100: 3.8712 (4.6457)  triple_100: 0.0000 (0.0695)  triple_80: 0.0000 (0.0551)  triple_60: 0.0000 (0.1512)  triple_40: 0.0000 (0.0748)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 480/1724]  eta: 1:21:16  lr: 0.000080  loss: 17.0148 (19.3286)  loss_n_40: 4.0336 (4.3656)  loss_n_60: 4.9425 (5.3316)  loss_n_80: 4.1438 (4.6538)  loss_n_100: 3.9229 (4.6302)  triple_100: 0.0000 (0.0719)  triple_80: 0.0000 (0.0542)  triple_60: 0.0000 (0.1480)  triple_40: 0.0000 (0.0733)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 490/1724]  eta: 1:20:37  lr: 0.000080  loss: 17.3279 (19.3008)  loss_n_40: 3.9922 (4.3597)  loss_n_60: 4.9175 (5.3243)  loss_n_80: 4.2350 (4.6460)  loss_n_100: 4.0910 (4.6232)  triple_100: 0.0000 (0.0728)  triple_80: 0.0000 (0.0531)  triple_60: 0.0000 (0.1473)  triple_40: 0.0000 (0.0744)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 500/1724]  eta: 1:19:58  lr: 0.000080  loss: 17.3872 (19.2627)  loss_n_40: 4.0164 (4.3530)  loss_n_60: 4.9345 (5.3165)  loss_n_80: 4.2609 (4.6378)  loss_n_100: 4.2571 (4.6138)  triple_100: 0.0000 (0.0714)  triple_80: 0.0000 (0.0521)  triple_60: 0.0000 (0.1453)  triple_40: 0.0000 (0.0729)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 510/1724]  eta: 1:19:19  lr: 0.000080  loss: 17.0972 (19.2212)  loss_n_40: 3.9955 (4.3464)  loss_n_60: 4.8833 (5.3091)  loss_n_80: 4.2053 (4.6291)  loss_n_100: 3.9768 (4.6014)  triple_100: 0.0000 (0.0700)  triple_80: 0.0000 (0.0512)  triple_60: 0.0000 (0.1425)  triple_40: 0.0000 (0.0717)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 520/1724]  eta: 1:18:39  lr: 0.000080  loss: 17.0515 (19.1743)  loss_n_40: 3.9822 (4.3397)  loss_n_60: 4.8750 (5.3001)  loss_n_80: 4.1099 (4.6174)  loss_n_100: 3.9483 (4.5870)  triple_100: 0.0000 (0.0695)  triple_80: 0.0000 (0.0504)  triple_60: 0.0000 (0.1397)  triple_40: 0.0000 (0.0705)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:5]  [ 530/1724]  eta: 1:18:00  lr: 0.000080  loss: 17.0657 (19.1443)  loss_n_40: 3.9822 (4.3331)  loss_n_60: 4.8107 (5.2904)  loss_n_80: 4.1099 (4.6072)  loss_n_100: 3.9845 (4.5751)  triple_100: 0.0000 (0.0738)  triple_80: 0.0000 (0.0543)  triple_60: 0.0000 (0.1406)  triple_40: 0.0000 (0.0699)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 540/1724]  eta: 1:17:21  lr: 0.000080  loss: 17.0657 (19.1139)  loss_n_40: 3.9477 (4.3266)  loss_n_60: 4.7531 (5.2813)  loss_n_80: 4.1535 (4.5988)  loss_n_100: 4.1450 (4.5676)  triple_100: 0.0000 (0.0737)  triple_80: 0.0000 (0.0533)  triple_60: 0.0000 (0.1380)  triple_40: 0.0000 (0.0747)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 550/1724]  eta: 1:16:42  lr: 0.000080  loss: 16.3604 (19.0652)  loss_n_40: 3.7273 (4.3172)  loss_n_60: 4.6909 (5.2729)  loss_n_80: 4.0332 (4.5882)  loss_n_100: 3.9923 (4.5526)  triple_100: 0.0000 (0.0728)  triple_80: 0.0000 (0.0526)  triple_60: 0.0000 (0.1355)  triple_40: 0.0000 (0.0733)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 560/1724]  eta: 1:16:02  lr: 0.000080  loss: 16.0285 (19.0101)  loss_n_40: 3.6903 (4.3062)  loss_n_60: 4.6683 (5.2615)  loss_n_80: 3.9107 (4.5760)  loss_n_100: 3.6822 (4.5368)  triple_100: 0.0000 (0.0715)  triple_80: 0.0000 (0.0517)  triple_60: 0.0000 (0.1344)  triple_40: 0.0000 (0.0720)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 570/1724]  eta: 1:15:23  lr: 0.000080  loss: 16.0821 (18.9689)  loss_n_40: 3.7399 (4.2974)  loss_n_60: 4.6592 (5.2517)  loss_n_80: 3.9107 (4.5644)  loss_n_100: 3.6923 (4.5218)  triple_100: 0.0000 (0.0725)  triple_80: 0.0000 (0.0508)  triple_60: 0.0000 (0.1363)  triple_40: 0.0000 (0.0740)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 580/1724]  eta: 1:14:44  lr: 0.000080  loss: 17.3478 (18.9560)  loss_n_40: 3.8434 (4.2908)  loss_n_60: 4.7004 (5.2442)  loss_n_80: 4.3008 (4.5636)  loss_n_100: 4.1942 (4.5270)  triple_100: 0.0000 (0.0727)  triple_80: 0.0000 (0.0509)  triple_60: 0.0000 (0.1340)  triple_40: 0.0000 (0.0727)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 590/1724]  eta: 1:14:05  lr: 0.000080  loss: 17.3478 (18.9235)  loss_n_40: 3.9072 (4.2850)  loss_n_60: 4.8103 (5.2359)  loss_n_80: 4.3300 (4.5563)  loss_n_100: 4.4372 (4.5203)  triple_100: 0.0000 (0.0715)  triple_80: 0.0000 (0.0511)  triple_60: 0.0000 (0.1317)  triple_40: 0.0000 (0.0716)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 600/1724]  eta: 1:13:26  lr: 0.000080  loss: 16.8171 (18.8901)  loss_n_40: 3.9071 (4.2799)  loss_n_60: 4.8188 (5.2292)  loss_n_80: 4.1840 (4.5491)  loss_n_100: 4.0839 (4.5108)  triple_100: 0.0000 (0.0709)  triple_80: 0.0000 (0.0502)  triple_60: 0.0000 (0.1295)  triple_40: 0.0000 (0.0704)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 610/1724]  eta: 1:12:46  lr: 0.000080  loss: 16.4997 (18.8525)  loss_n_40: 3.9182 (4.2734)  loss_n_60: 4.8811 (5.2212)  loss_n_80: 4.0367 (4.5413)  loss_n_100: 3.8621 (4.5005)  triple_100: 0.0000 (0.0698)  triple_80: 0.0000 (0.0494)  triple_60: 0.0000 (0.1275)  triple_40: 0.0000 (0.0693)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 620/1724]  eta: 1:12:07  lr: 0.000080  loss: 16.4342 (18.8180)  loss_n_40: 3.8151 (4.2655)  loss_n_60: 4.6745 (5.2124)  loss_n_80: 4.1403 (4.5334)  loss_n_100: 3.8621 (4.4883)  triple_100: 0.0000 (0.0687)  triple_80: 0.0000 (0.0486)  triple_60: 0.0000 (0.1302)  triple_40: 0.0000 (0.0708)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 630/1724]  eta: 1:11:28  lr: 0.000080  loss: 16.0381 (18.7730)  loss_n_40: 3.7521 (4.2574)  loss_n_60: 4.6518 (5.2035)  loss_n_80: 4.0018 (4.5239)  loss_n_100: 3.7272 (4.4748)  triple_100: 0.0000 (0.0676)  triple_80: 0.0000 (0.0478)  triple_60: 0.0000 (0.1282)  triple_40: 0.0000 (0.0697)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 640/1724]  eta: 1:10:49  lr: 0.000080  loss: 16.5429 (18.7395)  loss_n_40: 3.7817 (4.2521)  loss_n_60: 4.6733 (5.1964)  loss_n_80: 4.0018 (4.5169)  loss_n_100: 3.8274 (4.4644)  triple_100: 0.0000 (0.0670)  triple_80: 0.0000 (0.0479)  triple_60: 0.0000 (0.1262)  triple_40: 0.0000 (0.0686)  time: 3.9210  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [ 650/1724]  eta: 1:10:10  lr: 0.000080  loss: 16.5429 (18.7085)  loss_n_40: 3.7498 (4.2434)  loss_n_60: 4.6911 (5.1868)  loss_n_80: 4.1413 (4.5093)  loss_n_100: 3.8818 (4.4525)  triple_100: 0.0000 (0.0695)  triple_80: 0.0000 (0.0519)  triple_60: 0.0000 (0.1268)  triple_40: 0.0000 (0.0683)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 660/1724]  eta: 1:09:30  lr: 0.000080  loss: 16.5182 (18.6814)  loss_n_40: 3.7498 (4.2375)  loss_n_60: 4.6911 (5.1800)  loss_n_80: 4.0289 (4.5019)  loss_n_100: 3.9099 (4.4446)  triple_100: 0.0000 (0.0719)  triple_80: 0.0000 (0.0512)  triple_60: 0.0000 (0.1252)  triple_40: 0.0000 (0.0690)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 670/1724]  eta: 1:08:51  lr: 0.000080  loss: 16.3993 (18.6460)  loss_n_40: 3.7792 (4.2310)  loss_n_60: 4.6909 (5.1734)  loss_n_80: 3.9469 (4.4913)  loss_n_100: 3.7676 (4.4332)  triple_100: 0.0000 (0.0720)  triple_80: 0.0000 (0.0504)  triple_60: 0.0000 (0.1267)  triple_40: 0.0000 (0.0681)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 680/1724]  eta: 1:08:12  lr: 0.000080  loss: 15.5861 (18.6002)  loss_n_40: 3.6811 (4.2225)  loss_n_60: 4.6289 (5.1649)  loss_n_80: 3.7707 (4.4808)  loss_n_100: 3.5502 (4.4196)  triple_100: 0.0000 (0.0709)  triple_80: 0.0000 (0.0496)  triple_60: 0.0000 (0.1248)  triple_40: 0.0000 (0.0671)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 690/1724]  eta: 1:07:33  lr: 0.000080  loss: 15.5861 (18.5587)  loss_n_40: 3.6442 (4.2146)  loss_n_60: 4.6289 (5.1566)  loss_n_80: 3.7872 (4.4709)  loss_n_100: 3.4346 (4.4074)  triple_100: 0.0000 (0.0699)  triple_80: 0.0000 (0.0492)  triple_60: 0.0000 (0.1230)  triple_40: 0.0000 (0.0671)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 700/1724]  eta: 1:06:54  lr: 0.000080  loss: 15.1454 (18.5197)  loss_n_40: 3.5505 (4.2065)  loss_n_60: 4.4138 (5.1475)  loss_n_80: 3.6573 (4.4615)  loss_n_100: 3.4362 (4.3963)  triple_100: 0.0000 (0.0697)  triple_80: 0.0000 (0.0488)  triple_60: 0.0000 (0.1224)  triple_40: 0.0000 (0.0668)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 710/1724]  eta: 1:06:14  lr: 0.000080  loss: 15.1728 (18.4761)  loss_n_40: 3.5370 (4.1976)  loss_n_60: 4.4026 (5.1391)  loss_n_80: 3.7178 (4.4518)  loss_n_100: 3.4362 (4.3839)  triple_100: 0.0000 (0.0687)  triple_80: 0.0000 (0.0481)  triple_60: 0.0000 (0.1209)  triple_40: 0.0000 (0.0659)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 720/1724]  eta: 1:05:35  lr: 0.000080  loss: 15.1728 (18.4301)  loss_n_40: 3.5370 (4.1887)  loss_n_60: 4.4367 (5.1304)  loss_n_80: 3.6855 (4.4401)  loss_n_100: 3.3535 (4.3700)  triple_100: 0.0000 (0.0679)  triple_80: 0.0000 (0.0476)  triple_60: 0.0000 (0.1205)  triple_40: 0.0000 (0.0649)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 730/1724]  eta: 1:04:56  lr: 0.000080  loss: 15.3199 (18.3875)  loss_n_40: 3.5789 (4.1808)  loss_n_60: 4.5461 (5.1215)  loss_n_80: 3.6766 (4.4298)  loss_n_100: 3.3350 (4.3561)  triple_100: 0.0000 (0.0669)  triple_80: 0.0000 (0.0486)  triple_60: 0.0000 (0.1197)  triple_40: 0.0000 (0.0641)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 740/1724]  eta: 1:04:17  lr: 0.000080  loss: 15.0998 (18.3485)  loss_n_40: 3.5789 (4.1733)  loss_n_60: 4.3951 (5.1123)  loss_n_80: 3.6690 (4.4192)  loss_n_100: 3.3350 (4.3433)  triple_100: 0.0000 (0.0660)  triple_80: 0.0000 (0.0479)  triple_60: 0.0000 (0.1209)  triple_40: 0.0000 (0.0656)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 750/1724]  eta: 1:03:38  lr: 0.000080  loss: 15.0777 (18.3104)  loss_n_40: 3.5420 (4.1654)  loss_n_60: 4.3948 (5.1028)  loss_n_80: 3.6092 (4.4087)  loss_n_100: 3.3926 (4.3324)  triple_100: 0.0000 (0.0691)  triple_80: 0.0000 (0.0476)  triple_60: 0.0000 (0.1193)  triple_40: 0.0000 (0.0651)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 760/1724]  eta: 1:02:58  lr: 0.000080  loss: 15.0517 (18.2705)  loss_n_40: 3.4992 (4.1574)  loss_n_60: 4.3340 (5.0926)  loss_n_80: 3.6283 (4.3994)  loss_n_100: 3.4472 (4.3214)  triple_100: 0.0000 (0.0704)  triple_80: 0.0000 (0.0470)  triple_60: 0.0000 (0.1178)  triple_40: 0.0000 (0.0646)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 770/1724]  eta: 1:02:19  lr: 0.000080  loss: 15.0517 (18.2352)  loss_n_40: 3.4992 (4.1517)  loss_n_60: 4.3038 (5.0840)  loss_n_80: 3.6748 (4.3909)  loss_n_100: 3.5513 (4.3106)  triple_100: 0.0000 (0.0704)  triple_80: 0.0000 (0.0463)  triple_60: 0.0000 (0.1173)  triple_40: 0.0000 (0.0639)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 780/1724]  eta: 1:01:40  lr: 0.000080  loss: 14.4519 (18.1924)  loss_n_40: 3.4740 (4.1439)  loss_n_60: 4.3109 (5.0745)  loss_n_80: 3.5396 (4.3808)  loss_n_100: 3.1549 (4.2986)  triple_100: 0.0000 (0.0695)  triple_80: 0.0000 (0.0462)  triple_60: 0.0000 (0.1158)  triple_40: 0.0000 (0.0631)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 790/1724]  eta: 1:01:01  lr: 0.000080  loss: 14.3195 (18.1485)  loss_n_40: 3.4386 (4.1359)  loss_n_60: 4.2837 (5.0643)  loss_n_80: 3.5273 (4.3700)  loss_n_100: 3.0919 (4.2852)  triple_100: 0.0000 (0.0694)  triple_80: 0.0000 (0.0456)  triple_60: 0.0000 (0.1151)  triple_40: 0.0000 (0.0629)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 800/1724]  eta: 1:00:22  lr: 0.000080  loss: 14.4896 (18.1022)  loss_n_40: 3.3797 (4.1270)  loss_n_60: 4.1821 (5.0525)  loss_n_80: 3.5127 (4.3591)  loss_n_100: 3.2460 (4.2722)  triple_100: 0.0000 (0.0694)  triple_80: 0.0000 (0.0456)  triple_60: 0.0000 (0.1142)  triple_40: 0.0000 (0.0622)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 810/1724]  eta: 0:59:43  lr: 0.000080  loss: 14.4896 (18.0579)  loss_n_40: 3.3230 (4.1187)  loss_n_60: 4.0548 (5.0424)  loss_n_80: 3.5027 (4.3485)  loss_n_100: 3.2213 (4.2594)  triple_100: 0.0000 (0.0686)  triple_80: 0.0000 (0.0451)  triple_60: 0.0000 (0.1138)  triple_40: 0.0000 (0.0614)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 820/1724]  eta: 0:59:03  lr: 0.000080  loss: 14.3708 (18.0141)  loss_n_40: 3.3495 (4.1106)  loss_n_60: 4.2324 (5.0323)  loss_n_80: 3.5027 (4.3382)  loss_n_100: 3.2213 (4.2477)  triple_100: 0.0000 (0.0677)  triple_80: 0.0000 (0.0445)  triple_60: 0.0000 (0.1124)  triple_40: 0.0000 (0.0607)  time: 3.9222  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:5]  [ 830/1724]  eta: 0:58:24  lr: 0.000080  loss: 14.8772 (17.9789)  loss_n_40: 3.4571 (4.1040)  loss_n_60: 4.1673 (5.0230)  loss_n_80: 3.5306 (4.3287)  loss_n_100: 3.4379 (4.2392)  triple_100: 0.0000 (0.0682)  triple_80: 0.0000 (0.0444)  triple_60: 0.0000 (0.1113)  triple_40: 0.0000 (0.0600)  time: 3.9221  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:5]  [ 840/1724]  eta: 0:57:45  lr: 0.000080  loss: 15.1236 (17.9496)  loss_n_40: 3.4667 (4.0986)  loss_n_60: 4.2636 (5.0151)  loss_n_80: 3.6675 (4.3215)  loss_n_100: 3.4490 (4.2315)  triple_100: 0.0000 (0.0685)  triple_80: 0.0000 (0.0439)  triple_60: 0.0000 (0.1100)  triple_40: 0.0000 (0.0606)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 850/1724]  eta: 0:57:06  lr: 0.000080  loss: 15.1077 (17.9084)  loss_n_40: 3.5150 (4.0906)  loss_n_60: 4.2507 (5.0047)  loss_n_80: 3.6794 (4.3119)  loss_n_100: 3.4102 (4.2212)  triple_100: 0.0000 (0.0679)  triple_80: 0.0000 (0.0434)  triple_60: 0.0000 (0.1087)  triple_40: 0.0000 (0.0601)  time: 3.9224  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:5]  [ 860/1724]  eta: 0:56:27  lr: 0.000080  loss: 14.2954 (17.8627)  loss_n_40: 3.3160 (4.0811)  loss_n_60: 4.0780 (4.9941)  loss_n_80: 3.4665 (4.3016)  loss_n_100: 3.3064 (4.2085)  triple_100: 0.0000 (0.0671)  triple_80: 0.0000 (0.0435)  triple_60: 0.0000 (0.1075)  triple_40: 0.0000 (0.0594)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 870/1724]  eta: 0:55:47  lr: 0.000080  loss: 13.9127 (17.8227)  loss_n_40: 3.2467 (4.0725)  loss_n_60: 4.0627 (4.9836)  loss_n_80: 3.4359 (4.2921)  loss_n_100: 3.2195 (4.1989)  triple_100: 0.0000 (0.0676)  triple_80: 0.0000 (0.0430)  triple_60: 0.0000 (0.1062)  triple_40: 0.0000 (0.0587)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 880/1724]  eta: 0:55:08  lr: 0.000080  loss: 13.8209 (17.7768)  loss_n_40: 3.1638 (4.0629)  loss_n_60: 4.0177 (4.9722)  loss_n_80: 3.3651 (4.2811)  loss_n_100: 3.1565 (4.1858)  triple_100: 0.0000 (0.0669)  triple_80: 0.0000 (0.0425)  triple_60: 0.0000 (0.1073)  triple_40: 0.0000 (0.0581)  time: 3.9216  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [ 890/1724]  eta: 0:54:29  lr: 0.000080  loss: 13.7669 (17.7364)  loss_n_40: 3.1531 (4.0551)  loss_n_60: 3.9792 (4.9621)  loss_n_80: 3.3906 (4.2716)  loss_n_100: 3.1491 (4.1745)  triple_100: 0.0000 (0.0661)  triple_80: 0.0000 (0.0420)  triple_60: 0.0000 (0.1069)  triple_40: 0.0000 (0.0580)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 900/1724]  eta: 0:53:50  lr: 0.000080  loss: 13.2914 (17.6892)  loss_n_40: 3.1620 (4.0464)  loss_n_60: 3.8772 (4.9503)  loss_n_80: 3.2577 (4.2599)  loss_n_100: 2.8897 (4.1616)  triple_100: 0.0000 (0.0664)  triple_80: 0.0000 (0.0416)  triple_60: 0.0000 (0.1057)  triple_40: 0.0000 (0.0573)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 910/1724]  eta: 0:53:11  lr: 0.000080  loss: 13.2361 (17.6437)  loss_n_40: 3.1620 (4.0376)  loss_n_60: 3.8772 (4.9391)  loss_n_80: 3.1995 (4.2487)  loss_n_100: 2.8402 (4.1491)  triple_100: 0.0000 (0.0663)  triple_80: 0.0000 (0.0413)  triple_60: 0.0000 (0.1046)  triple_40: 0.0000 (0.0570)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 920/1724]  eta: 0:52:32  lr: 0.000080  loss: 13.5179 (17.6106)  loss_n_40: 3.1715 (4.0305)  loss_n_60: 3.8622 (4.9294)  loss_n_80: 3.3012 (4.2401)  loss_n_100: 3.1161 (4.1403)  triple_100: 0.0000 (0.0667)  triple_80: 0.0000 (0.0417)  triple_60: 0.0000 (0.1039)  triple_40: 0.0000 (0.0580)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 930/1724]  eta: 0:51:52  lr: 0.000080  loss: 14.1463 (17.5682)  loss_n_40: 3.2370 (4.0211)  loss_n_60: 3.9018 (4.9175)  loss_n_80: 3.3505 (4.2307)  loss_n_100: 3.2152 (4.1302)  triple_100: 0.0000 (0.0666)  triple_80: 0.0000 (0.0412)  triple_60: 0.0000 (0.1028)  triple_40: 0.0000 (0.0580)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 940/1724]  eta: 0:51:13  lr: 0.000080  loss: 13.2839 (17.5225)  loss_n_40: 3.0657 (4.0112)  loss_n_60: 3.8255 (4.9059)  loss_n_80: 3.3505 (4.2203)  loss_n_100: 2.9840 (4.1183)  triple_100: 0.0000 (0.0659)  triple_80: 0.0000 (0.0408)  triple_60: 0.0000 (0.1019)  triple_40: 0.0000 (0.0580)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 950/1724]  eta: 0:50:34  lr: 0.000080  loss: 13.4497 (17.4865)  loss_n_40: 3.1096 (4.0046)  loss_n_60: 3.8681 (4.8966)  loss_n_80: 3.2932 (4.2119)  loss_n_100: 3.0038 (4.1093)  triple_100: 0.0000 (0.0656)  triple_80: 0.0000 (0.0404)  triple_60: 0.0000 (0.1008)  triple_40: 0.0000 (0.0575)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 960/1724]  eta: 0:49:55  lr: 0.000080  loss: 13.5640 (17.4498)  loss_n_40: 3.2681 (3.9957)  loss_n_60: 3.9472 (4.8863)  loss_n_80: 3.2872 (4.2021)  loss_n_100: 3.1457 (4.1009)  triple_100: 0.0000 (0.0651)  triple_80: 0.0000 (0.0418)  triple_60: 0.0000 (0.1007)  triple_40: 0.0000 (0.0573)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 970/1724]  eta: 0:49:16  lr: 0.000080  loss: 14.0386 (17.4190)  loss_n_40: 3.2635 (3.9889)  loss_n_60: 3.9666 (4.8776)  loss_n_80: 3.2795 (4.1946)  loss_n_100: 3.2765 (4.0950)  triple_100: 0.0000 (0.0644)  triple_80: 0.0000 (0.0413)  triple_60: 0.0000 (0.0997)  triple_40: 0.0000 (0.0576)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 980/1724]  eta: 0:48:36  lr: 0.000080  loss: 14.3335 (17.3895)  loss_n_40: 3.3089 (3.9823)  loss_n_60: 3.9808 (4.8686)  loss_n_80: 3.4210 (4.1870)  loss_n_100: 3.4783 (4.0885)  triple_100: 0.0000 (0.0655)  triple_80: 0.0000 (0.0409)  triple_60: 0.0000 (0.0997)  triple_40: 0.0000 (0.0570)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [ 990/1724]  eta: 0:47:57  lr: 0.000080  loss: 14.0150 (17.3495)  loss_n_40: 3.1715 (3.9737)  loss_n_60: 3.8996 (4.8579)  loss_n_80: 3.3684 (4.1779)  loss_n_100: 3.2944 (4.0788)  triple_100: 0.0000 (0.0649)  triple_80: 0.0000 (0.0405)  triple_60: 0.0000 (0.0987)  triple_40: 0.0000 (0.0573)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1000/1724]  eta: 0:47:18  lr: 0.000080  loss: 13.8511 (17.3217)  loss_n_40: 3.2310 (3.9674)  loss_n_60: 3.9609 (4.8496)  loss_n_80: 3.3603 (4.1703)  loss_n_100: 3.1441 (4.0702)  triple_100: 0.0000 (0.0642)  triple_80: 0.0000 (0.0404)  triple_60: 0.0000 (0.1017)  triple_40: 0.0000 (0.0580)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1010/1724]  eta: 0:46:39  lr: 0.000080  loss: 13.0566 (17.2762)  loss_n_40: 3.1211 (3.9584)  loss_n_60: 3.7558 (4.8382)  loss_n_80: 3.1697 (4.1594)  loss_n_100: 2.8910 (4.0576)  triple_100: 0.0000 (0.0644)  triple_80: 0.0000 (0.0400)  triple_60: 0.0000 (0.1007)  triple_40: 0.0000 (0.0574)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1020/1724]  eta: 0:46:00  lr: 0.000080  loss: 12.6516 (17.2329)  loss_n_40: 3.0077 (3.9497)  loss_n_60: 3.7108 (4.8278)  loss_n_80: 3.1108 (4.1498)  loss_n_100: 2.8010 (4.0456)  triple_100: 0.0000 (0.0638)  triple_80: 0.0000 (0.0396)  triple_60: 0.0000 (0.0999)  triple_40: 0.0000 (0.0568)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1030/1724]  eta: 0:45:20  lr: 0.000080  loss: 12.8492 (17.2006)  loss_n_40: 2.9674 (3.9419)  loss_n_60: 3.7080 (4.8178)  loss_n_80: 3.1984 (4.1415)  loss_n_100: 2.9149 (4.0365)  triple_100: 0.0000 (0.0632)  triple_80: 0.0000 (0.0392)  triple_60: 0.0000 (0.1036)  triple_40: 0.0000 (0.0570)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1040/1724]  eta: 0:44:41  lr: 0.000080  loss: 13.1554 (17.1694)  loss_n_40: 3.0734 (3.9352)  loss_n_60: 3.7080 (4.8082)  loss_n_80: 3.2066 (4.1340)  loss_n_100: 3.0793 (4.0311)  triple_100: 0.0000 (0.0627)  triple_80: 0.0000 (0.0392)  triple_60: 0.0000 (0.1026)  triple_40: 0.0000 (0.0564)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1050/1724]  eta: 0:44:02  lr: 0.000080  loss: 13.9335 (17.1418)  loss_n_40: 3.1624 (3.9279)  loss_n_60: 3.8694 (4.7999)  loss_n_80: 3.3875 (4.1276)  loss_n_100: 3.3348 (4.0235)  triple_100: 0.0000 (0.0645)  triple_80: 0.0000 (0.0409)  triple_60: 0.0000 (0.1016)  triple_40: 0.0000 (0.0559)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1060/1724]  eta: 0:43:23  lr: 0.000080  loss: 13.6510 (17.1103)  loss_n_40: 3.1795 (3.9213)  loss_n_60: 3.9487 (4.7920)  loss_n_80: 3.3832 (4.1208)  loss_n_100: 3.1361 (4.0154)  triple_100: 0.0000 (0.0643)  triple_80: 0.0000 (0.0405)  triple_60: 0.0000 (0.1007)  triple_40: 0.0000 (0.0553)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1070/1724]  eta: 0:42:43  lr: 0.000080  loss: 13.0997 (17.0671)  loss_n_40: 3.1445 (3.9125)  loss_n_60: 3.7244 (4.7807)  loss_n_80: 3.1495 (4.1105)  loss_n_100: 2.9216 (4.0040)  triple_100: 0.0000 (0.0637)  triple_80: 0.0000 (0.0401)  triple_60: 0.0000 (0.0997)  triple_40: 0.0000 (0.0560)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1080/1724]  eta: 0:42:04  lr: 0.000080  loss: 12.6233 (17.0324)  loss_n_40: 2.9713 (3.9045)  loss_n_60: 3.6234 (4.7710)  loss_n_80: 2.9729 (4.1010)  loss_n_100: 2.7356 (3.9931)  triple_100: 0.0000 (0.0634)  triple_80: 0.0000 (0.0398)  triple_60: 0.0000 (0.1033)  triple_40: 0.0000 (0.0563)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1090/1724]  eta: 0:41:25  lr: 0.000080  loss: 12.7502 (16.9947)  loss_n_40: 2.9635 (3.8954)  loss_n_60: 3.6097 (4.7598)  loss_n_80: 3.0622 (4.0917)  loss_n_100: 2.8664 (3.9849)  triple_100: 0.0000 (0.0643)  triple_80: 0.0000 (0.0394)  triple_60: 0.0000 (0.1033)  triple_40: 0.0000 (0.0558)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1100/1724]  eta: 0:40:46  lr: 0.000080  loss: 12.2912 (16.9514)  loss_n_40: 2.8814 (3.8871)  loss_n_60: 3.5240 (4.7489)  loss_n_80: 3.0173 (4.0811)  loss_n_100: 2.8386 (3.9737)  triple_100: 0.0000 (0.0637)  triple_80: 0.0000 (0.0390)  triple_60: 0.0000 (0.1024)  triple_40: 0.0000 (0.0553)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1110/1724]  eta: 0:40:07  lr: 0.000080  loss: 12.2912 (16.9177)  loss_n_40: 2.9054 (3.8796)  loss_n_60: 3.5407 (4.7387)  loss_n_80: 3.0173 (4.0718)  loss_n_100: 2.7606 (3.9644)  triple_100: 0.0000 (0.0646)  triple_80: 0.0000 (0.0390)  triple_60: 0.0000 (0.1028)  triple_40: 0.0000 (0.0568)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1120/1724]  eta: 0:39:27  lr: 0.000080  loss: 12.6881 (16.8847)  loss_n_40: 2.9676 (3.8722)  loss_n_60: 3.5190 (4.7289)  loss_n_80: 3.0366 (4.0629)  loss_n_100: 2.9524 (3.9565)  triple_100: 0.0000 (0.0659)  triple_80: 0.0000 (0.0402)  triple_60: 0.0000 (0.1018)  triple_40: 0.0000 (0.0563)  time: 3.9196  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [1130/1724]  eta: 0:38:48  lr: 0.000080  loss: 13.0317 (16.8585)  loss_n_40: 3.0728 (3.8664)  loss_n_60: 3.6682 (4.7206)  loss_n_80: 3.0547 (4.0559)  loss_n_100: 3.1496 (3.9519)  triple_100: 0.0000 (0.0656)  triple_80: 0.0000 (0.0413)  triple_60: 0.0000 (0.1009)  triple_40: 0.0000 (0.0558)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1140/1724]  eta: 0:38:09  lr: 0.000080  loss: 12.9060 (16.8203)  loss_n_40: 3.0728 (3.8586)  loss_n_60: 3.6682 (4.7107)  loss_n_80: 3.0752 (4.0466)  loss_n_100: 3.1217 (3.9426)  triple_100: 0.0000 (0.0654)  triple_80: 0.0000 (0.0410)  triple_60: 0.0000 (0.1001)  triple_40: 0.0000 (0.0553)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1150/1724]  eta: 0:37:30  lr: 0.000080  loss: 11.7742 (16.7731)  loss_n_40: 2.8446 (3.8490)  loss_n_60: 3.4603 (4.6989)  loss_n_80: 2.8117 (4.0347)  loss_n_100: 2.5894 (3.9300)  triple_100: 0.0000 (0.0654)  triple_80: 0.0000 (0.0406)  triple_60: 0.0000 (0.0992)  triple_40: 0.0000 (0.0552)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1160/1724]  eta: 0:36:51  lr: 0.000080  loss: 11.9227 (16.7408)  loss_n_40: 2.8573 (3.8421)  loss_n_60: 3.4270 (4.6893)  loss_n_80: 2.8117 (4.0257)  loss_n_100: 2.4434 (3.9210)  triple_100: 0.0000 (0.0669)  triple_80: 0.0000 (0.0413)  triple_60: 0.0000 (0.0998)  triple_40: 0.0000 (0.0547)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1170/1724]  eta: 0:36:11  lr: 0.000080  loss: 12.3703 (16.7011)  loss_n_40: 2.9436 (3.8338)  loss_n_60: 3.5063 (4.6786)  loss_n_80: 2.9836 (4.0162)  loss_n_100: 2.8097 (3.9113)  triple_100: 0.0000 (0.0664)  triple_80: 0.0000 (0.0417)  triple_60: 0.0000 (0.0989)  triple_40: 0.0000 (0.0542)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1180/1724]  eta: 0:35:32  lr: 0.000080  loss: 12.0768 (16.6645)  loss_n_40: 2.8147 (3.8257)  loss_n_60: 3.3728 (4.6679)  loss_n_80: 2.8549 (4.0070)  loss_n_100: 2.9060 (3.9026)  triple_100: 0.0000 (0.0658)  triple_80: 0.0000 (0.0413)  triple_60: 0.0000 (0.0994)  triple_40: 0.0000 (0.0549)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1190/1724]  eta: 0:34:53  lr: 0.000080  loss: 11.7093 (16.6267)  loss_n_40: 2.8147 (3.8183)  loss_n_60: 3.3717 (4.6578)  loss_n_80: 2.8443 (3.9979)  loss_n_100: 2.7788 (3.8935)  triple_100: 0.0000 (0.0652)  triple_80: 0.0000 (0.0410)  triple_60: 0.0000 (0.0986)  triple_40: 0.0000 (0.0544)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1200/1724]  eta: 0:34:14  lr: 0.000080  loss: 11.6431 (16.5846)  loss_n_40: 2.7370 (3.8091)  loss_n_60: 3.3034 (4.6461)  loss_n_80: 2.7870 (3.9874)  loss_n_100: 2.6360 (3.8821)  triple_100: 0.0000 (0.0676)  triple_80: 0.0000 (0.0406)  triple_60: 0.0000 (0.0978)  triple_40: 0.0000 (0.0540)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1210/1724]  eta: 0:33:35  lr: 0.000080  loss: 11.2909 (16.5466)  loss_n_40: 2.6872 (3.8009)  loss_n_60: 3.2341 (4.6355)  loss_n_80: 2.7285 (3.9777)  loss_n_100: 2.4378 (3.8709)  triple_100: 0.0000 (0.0675)  triple_80: 0.0000 (0.0406)  triple_60: 0.0000 (0.0999)  triple_40: 0.0000 (0.0535)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1220/1724]  eta: 0:32:55  lr: 0.000080  loss: 11.4061 (16.5089)  loss_n_40: 2.6959 (3.7935)  loss_n_60: 3.2567 (4.6249)  loss_n_80: 2.9101 (3.9687)  loss_n_100: 2.6127 (3.8615)  triple_100: 0.0000 (0.0670)  triple_80: 0.0000 (0.0410)  triple_60: 0.0000 (0.0991)  triple_40: 0.0000 (0.0531)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1230/1724]  eta: 0:32:16  lr: 0.000080  loss: 11.5868 (16.4712)  loss_n_40: 2.8709 (3.7856)  loss_n_60: 3.3124 (4.6139)  loss_n_80: 2.8925 (3.9593)  loss_n_100: 2.6650 (3.8517)  triple_100: 0.0000 (0.0664)  triple_80: 0.0000 (0.0407)  triple_60: 0.0000 (0.0999)  triple_40: 0.0000 (0.0536)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1240/1724]  eta: 0:31:37  lr: 0.000080  loss: 11.2390 (16.4297)  loss_n_40: 2.6846 (3.7772)  loss_n_60: 3.1414 (4.6026)  loss_n_80: 2.7182 (3.9490)  loss_n_100: 2.4960 (3.8413)  triple_100: 0.0000 (0.0667)  triple_80: 0.0000 (0.0406)  triple_60: 0.0000 (0.0991)  triple_40: 0.0000 (0.0532)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1250/1724]  eta: 0:30:58  lr: 0.000080  loss: 10.6316 (16.3896)  loss_n_40: 2.5156 (3.7678)  loss_n_60: 3.0567 (4.5906)  loss_n_80: 2.5781 (3.9377)  loss_n_100: 2.3266 (3.8298)  triple_100: 0.0000 (0.0670)  triple_80: 0.0000 (0.0405)  triple_60: 0.0000 (0.1016)  triple_40: 0.0000 (0.0547)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1260/1724]  eta: 0:30:19  lr: 0.000080  loss: 11.0346 (16.3576)  loss_n_40: 2.5221 (3.7601)  loss_n_60: 3.0805 (4.5801)  loss_n_80: 2.7151 (3.9292)  loss_n_100: 2.6960 (3.8236)  triple_100: 0.0000 (0.0676)  triple_80: 0.0000 (0.0419)  triple_60: 0.0000 (0.1008)  triple_40: 0.0000 (0.0543)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1270/1724]  eta: 0:29:39  lr: 0.000080  loss: 11.6390 (16.3216)  loss_n_40: 2.6755 (3.7531)  loss_n_60: 3.2382 (4.5706)  loss_n_80: 2.8125 (3.9201)  loss_n_100: 2.8149 (3.8152)  triple_100: 0.0000 (0.0670)  triple_80: 0.0000 (0.0416)  triple_60: 0.0000 (0.1000)  triple_40: 0.0000 (0.0540)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1280/1724]  eta: 0:29:00  lr: 0.000080  loss: 11.2598 (16.2857)  loss_n_40: 2.6561 (3.7459)  loss_n_60: 3.2024 (4.5608)  loss_n_80: 2.6072 (3.9108)  loss_n_100: 2.6718 (3.8065)  triple_100: 0.0000 (0.0670)  triple_80: 0.0000 (0.0417)  triple_60: 0.0000 (0.0995)  triple_40: 0.0000 (0.0536)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1290/1724]  eta: 0:28:21  lr: 0.000080  loss: 10.4360 (16.2441)  loss_n_40: 2.5954 (3.7374)  loss_n_60: 3.1394 (4.5496)  loss_n_80: 2.4734 (3.9006)  loss_n_100: 2.3126 (3.7966)  triple_100: 0.0000 (0.0664)  triple_80: 0.0000 (0.0414)  triple_60: 0.0000 (0.0989)  triple_40: 0.0000 (0.0531)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1300/1724]  eta: 0:27:42  lr: 0.000080  loss: 10.4697 (16.2080)  loss_n_40: 2.5303 (3.7298)  loss_n_60: 3.0935 (4.5392)  loss_n_80: 2.5074 (3.8916)  loss_n_100: 2.3835 (3.7876)  triple_100: 0.0000 (0.0664)  triple_80: 0.0000 (0.0424)  triple_60: 0.0000 (0.0981)  triple_40: 0.0000 (0.0529)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1310/1724]  eta: 0:27:02  lr: 0.000080  loss: 11.3624 (16.1741)  loss_n_40: 2.6548 (3.7216)  loss_n_60: 3.1810 (4.5287)  loss_n_80: 2.5949 (3.8811)  loss_n_100: 2.6314 (3.7776)  triple_100: 0.0000 (0.0662)  triple_80: 0.0000 (0.0421)  triple_60: 0.0000 (0.1032)  triple_40: 0.0000 (0.0536)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1320/1724]  eta: 0:26:23  lr: 0.000080  loss: 11.3705 (16.1398)  loss_n_40: 2.6922 (3.7150)  loss_n_60: 3.1804 (4.5195)  loss_n_80: 2.4918 (3.8717)  loss_n_100: 2.3893 (3.7689)  triple_100: 0.0000 (0.0657)  triple_80: 0.0000 (0.0433)  triple_60: 0.0000 (0.1024)  triple_40: 0.0000 (0.0532)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1330/1724]  eta: 0:25:44  lr: 0.000080  loss: 10.8656 (16.0999)  loss_n_40: 2.6497 (3.7064)  loss_n_60: 3.1047 (4.5082)  loss_n_80: 2.4517 (3.8617)  loss_n_100: 2.4163 (3.7595)  triple_100: 0.0000 (0.0667)  triple_80: 0.0000 (0.0430)  triple_60: 0.0000 (0.1017)  triple_40: 0.0000 (0.0528)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1340/1724]  eta: 0:25:05  lr: 0.000080  loss: 10.8111 (16.0641)  loss_n_40: 2.5456 (3.6985)  loss_n_60: 3.0033 (4.4980)  loss_n_80: 2.4981 (3.8530)  loss_n_100: 2.5082 (3.7516)  triple_100: 0.0000 (0.0662)  triple_80: 0.0000 (0.0429)  triple_60: 0.0000 (0.1014)  triple_40: 0.0000 (0.0525)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1350/1724]  eta: 0:24:26  lr: 0.000080  loss: 10.6938 (16.0313)  loss_n_40: 2.4887 (3.6907)  loss_n_60: 2.9844 (4.4877)  loss_n_80: 2.5511 (3.8433)  loss_n_100: 2.5082 (3.7420)  triple_100: 0.0000 (0.0657)  triple_80: 0.0000 (0.0436)  triple_60: 0.0000 (0.1036)  triple_40: 0.0000 (0.0545)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1360/1724]  eta: 0:23:46  lr: 0.000080  loss: 10.7519 (16.0003)  loss_n_40: 2.5442 (3.6836)  loss_n_60: 3.0182 (4.4782)  loss_n_80: 2.6269 (3.8360)  loss_n_100: 2.5348 (3.7355)  triple_100: 0.0000 (0.0662)  triple_80: 0.0000 (0.0434)  triple_60: 0.0000 (0.1031)  triple_40: 0.0000 (0.0541)  time: 3.9200  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [1370/1724]  eta: 0:23:07  lr: 0.000080  loss: 10.7519 (15.9614)  loss_n_40: 2.5442 (3.6751)  loss_n_60: 3.0182 (4.4667)  loss_n_80: 2.5826 (3.8264)  loss_n_100: 2.5023 (3.7262)  triple_100: 0.0000 (0.0669)  triple_80: 0.0000 (0.0431)  triple_60: 0.0000 (0.1035)  triple_40: 0.0000 (0.0537)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1380/1724]  eta: 0:22:28  lr: 0.000080  loss: 10.6568 (15.9288)  loss_n_40: 2.5933 (3.6680)  loss_n_60: 2.9142 (4.4562)  loss_n_80: 2.5531 (3.8186)  loss_n_100: 2.4494 (3.7178)  triple_100: 0.0000 (0.0673)  triple_80: 0.0000 (0.0438)  triple_60: 0.0000 (0.1027)  triple_40: 0.0000 (0.0543)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1390/1724]  eta: 0:21:49  lr: 0.000080  loss: 10.8446 (15.8948)  loss_n_40: 2.5933 (3.6609)  loss_n_60: 2.9792 (4.4465)  loss_n_80: 2.6320 (3.8105)  loss_n_100: 2.4732 (3.7094)  triple_100: 0.0000 (0.0676)  triple_80: 0.0000 (0.0436)  triple_60: 0.0000 (0.1021)  triple_40: 0.0000 (0.0542)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1400/1724]  eta: 0:21:10  lr: 0.000080  loss: 10.6950 (15.8573)  loss_n_40: 2.4594 (3.6526)  loss_n_60: 2.9792 (4.4364)  loss_n_80: 2.4846 (3.8015)  loss_n_100: 2.4326 (3.7009)  triple_100: 0.0000 (0.0671)  triple_80: 0.0000 (0.0435)  triple_60: 0.0000 (0.1014)  triple_40: 0.0000 (0.0539)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1410/1724]  eta: 0:20:30  lr: 0.000080  loss: 10.2392 (15.8169)  loss_n_40: 2.4551 (3.6440)  loss_n_60: 2.9599 (4.4254)  loss_n_80: 2.4256 (3.7914)  loss_n_100: 2.3193 (3.6912)  triple_100: 0.0000 (0.0673)  triple_80: 0.0000 (0.0432)  triple_60: 0.0000 (0.1009)  triple_40: 0.0000 (0.0535)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1420/1724]  eta: 0:19:51  lr: 0.000080  loss: 9.1506 (15.7728)  loss_n_40: 2.3168 (3.6352)  loss_n_60: 2.7033 (4.4137)  loss_n_80: 2.1651 (3.7803)  loss_n_100: 2.1159 (3.6803)  triple_100: 0.0000 (0.0670)  triple_80: 0.0000 (0.0429)  triple_60: 0.0000 (0.1002)  triple_40: 0.0000 (0.0532)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1430/1724]  eta: 0:19:12  lr: 0.000080  loss: 8.8209 (15.7294)  loss_n_40: 2.2348 (3.6261)  loss_n_60: 2.6393 (4.4021)  loss_n_80: 2.0888 (3.7699)  loss_n_100: 1.9806 (3.6694)  triple_100: 0.0000 (0.0668)  triple_80: 0.0000 (0.0426)  triple_60: 0.0000 (0.0995)  triple_40: 0.0000 (0.0529)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1440/1724]  eta: 0:18:33  lr: 0.000080  loss: 9.3935 (15.6879)  loss_n_40: 2.3388 (3.6176)  loss_n_60: 2.7357 (4.3910)  loss_n_80: 2.1901 (3.7593)  loss_n_100: 1.9916 (3.6584)  triple_100: 0.0000 (0.0673)  triple_80: 0.0000 (0.0424)  triple_60: 0.0000 (0.0991)  triple_40: 0.0000 (0.0526)  time: 3.9173  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:5]  [1450/1724]  eta: 0:17:54  lr: 0.000080  loss: 9.6793 (15.6518)  loss_n_40: 2.3922 (3.6107)  loss_n_60: 2.8592 (4.3814)  loss_n_80: 2.2728 (3.7507)  loss_n_100: 2.0845 (3.6494)  triple_100: 0.0000 (0.0669)  triple_80: 0.0000 (0.0421)  triple_60: 0.0000 (0.0985)  triple_40: 0.0000 (0.0523)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1460/1724]  eta: 0:17:14  lr: 0.000080  loss: 9.5160 (15.6098)  loss_n_40: 2.3719 (3.6022)  loss_n_60: 2.8109 (4.3699)  loss_n_80: 2.2468 (3.7405)  loss_n_100: 2.1187 (3.6392)  triple_100: 0.0000 (0.0664)  triple_80: 0.0000 (0.0420)  triple_60: 0.0000 (0.0978)  triple_40: 0.0000 (0.0519)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1470/1724]  eta: 0:16:35  lr: 0.000080  loss: 9.7453 (15.5751)  loss_n_40: 2.3719 (3.5953)  loss_n_60: 2.8001 (4.3600)  loss_n_80: 2.3447 (3.7314)  loss_n_100: 2.1976 (3.6300)  triple_100: 0.0000 (0.0660)  triple_80: 0.0000 (0.0417)  triple_60: 0.0000 (0.0990)  triple_40: 0.0000 (0.0517)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1480/1724]  eta: 0:15:56  lr: 0.000080  loss: 9.5125 (15.5319)  loss_n_40: 2.3350 (3.5867)  loss_n_60: 2.7479 (4.3486)  loss_n_80: 2.3027 (3.7209)  loss_n_100: 2.0797 (3.6189)  triple_100: 0.0000 (0.0656)  triple_80: 0.0000 (0.0415)  triple_60: 0.0000 (0.0983)  triple_40: 0.0000 (0.0514)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1490/1724]  eta: 0:15:17  lr: 0.000080  loss: 9.2338 (15.4943)  loss_n_40: 2.3350 (3.5795)  loss_n_60: 2.6646 (4.3383)  loss_n_80: 2.2426 (3.7112)  loss_n_100: 2.0525 (3.6093)  triple_100: 0.0000 (0.0652)  triple_80: 0.0000 (0.0412)  triple_60: 0.0000 (0.0977)  triple_40: 0.0000 (0.0520)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1500/1724]  eta: 0:14:38  lr: 0.000080  loss: 8.9614 (15.4521)  loss_n_40: 2.2668 (3.5709)  loss_n_60: 2.5611 (4.3267)  loss_n_80: 2.1207 (3.7007)  loss_n_100: 1.9980 (3.5989)  triple_100: 0.0000 (0.0650)  triple_80: 0.0000 (0.0410)  triple_60: 0.0000 (0.0972)  triple_40: 0.0000 (0.0516)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1510/1724]  eta: 0:13:58  lr: 0.000080  loss: 8.6175 (15.4082)  loss_n_40: 2.1513 (3.5622)  loss_n_60: 2.4800 (4.3150)  loss_n_80: 2.0312 (3.6898)  loss_n_100: 1.9099 (3.5878)  triple_100: 0.0000 (0.0648)  triple_80: 0.0000 (0.0407)  triple_60: 0.0000 (0.0965)  triple_40: 0.0000 (0.0513)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1520/1724]  eta: 0:13:19  lr: 0.000080  loss: 9.0453 (15.3756)  loss_n_40: 2.2735 (3.5548)  loss_n_60: 2.6107 (4.3049)  loss_n_80: 2.0954 (3.6803)  loss_n_100: 1.9976 (3.5788)  triple_100: 0.0000 (0.0644)  triple_80: 0.0000 (0.0406)  triple_60: 0.0000 (0.1004)  triple_40: 0.0000 (0.0514)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1530/1724]  eta: 0:12:40  lr: 0.000080  loss: 9.9847 (15.3405)  loss_n_40: 2.4546 (3.5475)  loss_n_60: 2.7439 (4.2946)  loss_n_80: 2.2322 (3.6715)  loss_n_100: 2.3297 (3.5714)  triple_100: 0.0000 (0.0640)  triple_80: 0.0000 (0.0403)  triple_60: 0.0000 (0.0997)  triple_40: 0.0000 (0.0514)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1540/1724]  eta: 0:12:01  lr: 0.000080  loss: 9.9922 (15.3072)  loss_n_40: 2.3235 (3.5408)  loss_n_60: 2.6709 (4.2850)  loss_n_80: 2.2581 (3.6627)  loss_n_100: 2.3402 (3.5636)  triple_100: 0.0000 (0.0646)  triple_80: 0.0000 (0.0401)  triple_60: 0.0000 (0.0991)  triple_40: 0.0000 (0.0515)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1550/1724]  eta: 0:11:22  lr: 0.000080  loss: 9.9922 (15.2690)  loss_n_40: 2.3235 (3.5337)  loss_n_60: 2.6562 (4.2747)  loss_n_80: 2.2362 (3.6527)  loss_n_100: 2.2877 (3.5542)  triple_100: 0.0000 (0.0641)  triple_80: 0.0000 (0.0398)  triple_60: 0.0000 (0.0984)  triple_40: 0.0000 (0.0513)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1560/1724]  eta: 0:10:42  lr: 0.000080  loss: 8.7188 (15.2295)  loss_n_40: 2.1314 (3.5254)  loss_n_60: 2.4695 (4.2635)  loss_n_80: 2.1218 (3.6426)  loss_n_100: 2.0075 (3.5443)  triple_100: 0.0000 (0.0648)  triple_80: 0.0000 (0.0400)  triple_60: 0.0000 (0.0978)  triple_40: 0.0000 (0.0509)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1570/1724]  eta: 0:10:03  lr: 0.000080  loss: 8.3341 (15.1890)  loss_n_40: 2.0993 (3.5170)  loss_n_60: 2.4124 (4.2523)  loss_n_80: 2.0572 (3.6333)  loss_n_100: 1.9637 (3.5340)  triple_100: 0.0000 (0.0644)  triple_80: 0.0000 (0.0398)  triple_60: 0.0000 (0.0972)  triple_40: 0.0000 (0.0510)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1580/1724]  eta: 0:09:24  lr: 0.000080  loss: 8.1289 (15.1459)  loss_n_40: 2.0915 (3.5078)  loss_n_60: 2.3949 (4.2403)  loss_n_80: 2.0167 (3.6231)  loss_n_100: 1.7272 (3.5227)  triple_100: 0.0000 (0.0640)  triple_80: 0.0000 (0.0397)  triple_60: 0.0000 (0.0976)  triple_40: 0.0000 (0.0507)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1590/1724]  eta: 0:08:45  lr: 0.000080  loss: 8.0994 (15.1056)  loss_n_40: 2.0164 (3.4994)  loss_n_60: 2.3323 (4.2292)  loss_n_80: 2.0021 (3.6138)  loss_n_100: 1.7272 (3.5125)  triple_100: 0.0000 (0.0636)  triple_80: 0.0000 (0.0396)  triple_60: 0.0000 (0.0971)  triple_40: 0.0000 (0.0504)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1600/1724]  eta: 0:08:06  lr: 0.000080  loss: 8.0994 (15.0698)  loss_n_40: 2.0354 (3.4914)  loss_n_60: 2.3335 (4.2186)  loss_n_80: 1.9860 (3.6044)  loss_n_100: 1.7958 (3.5025)  triple_100: 0.0000 (0.0632)  triple_80: 0.0000 (0.0393)  triple_60: 0.0000 (0.0989)  triple_40: 0.0000 (0.0515)  time: 3.9180  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [1610/1724]  eta: 0:07:26  lr: 0.000080  loss: 9.3406 (15.0408)  loss_n_40: 2.3311 (3.4857)  loss_n_60: 2.6513 (4.2097)  loss_n_80: 2.2613 (3.5968)  loss_n_100: 2.1066 (3.4952)  triple_100: 0.0000 (0.0628)  triple_80: 0.0000 (0.0391)  triple_60: 0.0000 (0.0983)  triple_40: 0.0000 (0.0532)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1620/1724]  eta: 0:06:47  lr: 0.000080  loss: 9.4722 (15.0061)  loss_n_40: 2.4196 (3.4791)  loss_n_60: 2.6513 (4.2000)  loss_n_80: 2.2613 (3.5882)  loss_n_100: 2.2121 (3.4869)  triple_100: 0.0000 (0.0625)  triple_80: 0.0000 (0.0389)  triple_60: 0.0000 (0.0977)  triple_40: 0.0000 (0.0528)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1630/1724]  eta: 0:06:08  lr: 0.000080  loss: 9.6020 (14.9781)  loss_n_40: 2.3846 (3.4744)  loss_n_60: 2.7124 (4.1918)  loss_n_80: 2.2824 (3.5806)  loss_n_100: 2.2377 (3.4802)  triple_100: 0.0000 (0.0623)  triple_80: 0.0000 (0.0386)  triple_60: 0.0000 (0.0971)  triple_40: 0.0000 (0.0530)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1640/1724]  eta: 0:05:29  lr: 0.000080  loss: 9.6020 (14.9424)  loss_n_40: 2.3138 (3.4673)  loss_n_60: 2.6284 (4.1817)  loss_n_80: 2.2672 (3.5716)  loss_n_100: 2.1997 (3.4712)  triple_100: 0.0000 (0.0619)  triple_80: 0.0000 (0.0384)  triple_60: 0.0000 (0.0976)  triple_40: 0.0000 (0.0527)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1650/1724]  eta: 0:04:50  lr: 0.000080  loss: 9.0273 (14.9084)  loss_n_40: 2.2049 (3.4602)  loss_n_60: 2.4118 (4.1716)  loss_n_80: 2.1888 (3.5630)  loss_n_100: 2.0760 (3.4629)  triple_100: 0.0000 (0.0616)  triple_80: 0.0000 (0.0384)  triple_60: 0.0000 (0.0970)  triple_40: 0.0000 (0.0536)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1660/1724]  eta: 0:04:10  lr: 0.000080  loss: 7.9199 (14.8668)  loss_n_40: 2.0142 (3.4516)  loss_n_60: 2.2790 (4.1603)  loss_n_80: 1.8884 (3.5527)  loss_n_100: 1.7575 (3.4530)  triple_100: 0.0000 (0.0613)  triple_80: 0.0000 (0.0383)  triple_60: 0.0000 (0.0964)  triple_40: 0.0000 (0.0532)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1670/1724]  eta: 0:03:31  lr: 0.000080  loss: 7.6989 (14.8297)  loss_n_40: 1.9986 (3.4441)  loss_n_60: 2.2654 (4.1502)  loss_n_80: 1.8173 (3.5434)  loss_n_100: 1.7185 (3.4439)  triple_100: 0.0000 (0.0613)  triple_80: 0.0000 (0.0381)  triple_60: 0.0000 (0.0958)  triple_40: 0.0000 (0.0529)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1680/1724]  eta: 0:02:52  lr: 0.000080  loss: 8.2760 (14.7946)  loss_n_40: 2.0662 (3.4369)  loss_n_60: 2.3826 (4.1404)  loss_n_80: 1.9845 (3.5347)  loss_n_100: 1.9139 (3.4349)  triple_100: 0.0000 (0.0610)  triple_80: 0.0000 (0.0388)  triple_60: 0.0000 (0.0953)  triple_40: 0.0000 (0.0527)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1690/1724]  eta: 0:02:13  lr: 0.000080  loss: 8.3071 (14.7610)  loss_n_40: 2.0127 (3.4299)  loss_n_60: 2.4311 (4.1314)  loss_n_80: 1.9845 (3.5262)  loss_n_100: 1.9116 (3.4261)  triple_100: 0.0000 (0.0606)  triple_80: 0.0000 (0.0385)  triple_60: 0.0000 (0.0958)  triple_40: 0.0000 (0.0525)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1700/1724]  eta: 0:01:34  lr: 0.000080  loss: 10.6741 (14.7448)  loss_n_40: 2.0709 (3.4228)  loss_n_60: 2.5873 (4.1233)  loss_n_80: 2.2888 (3.5224)  loss_n_100: 2.3645 (3.4244)  triple_100: 0.0000 (0.0636)  triple_80: 0.0000 (0.0409)  triple_60: 0.0000 (0.0953)  triple_40: 0.0000 (0.0522)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1710/1724]  eta: 0:00:54  lr: 0.000080  loss: 11.1379 (14.7210)  loss_n_40: 2.3066 (3.4173)  loss_n_60: 2.8081 (4.1165)  loss_n_80: 2.7251 (3.5174)  loss_n_100: 2.7746 (3.4192)  triple_100: 0.0000 (0.0632)  triple_80: 0.0000 (0.0407)  triple_60: 0.0000 (0.0947)  triple_40: 0.0000 (0.0520)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:5]  [1720/1724]  eta: 0:00:15  lr: 0.000080  loss: 10.2598 (14.6987)  loss_n_40: 2.3917 (3.4124)  loss_n_60: 2.8546 (4.1101)  loss_n_80: 2.4933 (3.5112)  loss_n_100: 2.1963 (3.4125)  triple_100: 0.0000 (0.0628)  triple_80: 0.0000 (0.0414)  triple_60: 0.0000 (0.0955)  triple_40: 0.0000 (0.0528)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5]  [1723/1724]  eta: 0:00:03  lr: 0.000080  loss: 9.5335 (14.6880)  loss_n_40: 2.3225 (3.4100)  loss_n_60: 2.8369 (4.1074)  loss_n_80: 2.2954 (3.5088)  loss_n_100: 2.1407 (3.4098)  triple_100: 0.0000 (0.0627)  triple_80: 0.0000 (0.0413)  triple_60: 0.0000 (0.0954)  triple_40: 0.0000 (0.0527)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:5] Total time: 1:52:38 (3.9200 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 9.5335 (14.6880)  loss_n_40: 2.3225 (3.4100)  loss_n_60: 2.8369 (4.1074)  loss_n_80: 2.2954 (3.5088)  loss_n_100: 2.1407 (3.4098)  triple_100: 0.0000 (0.0627)  triple_80: 0.0000 (0.0413)  triple_60: 0.0000 (0.0954)  triple_40: 0.0000 (0.0527)\n",
      "Valid: [epoch:5]  [  0/845]  eta: 0:09:53  loss: 8.7167 (8.7167)  loss_n_40: 1.9437 (1.9437)  loss_n_60: 2.5184 (2.5184)  loss_n_80: 2.1748 (2.1748)  loss_n_100: 2.0797 (2.0797)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7026  data: 0.3678  max mem: 46473\n",
      "Valid: [epoch:5]  [ 10/845]  eta: 0:05:06  loss: 9.7896 (12.0249)  loss_n_40: 2.1768 (2.7347)  loss_n_60: 2.6719 (3.1180)  loss_n_80: 2.3593 (2.7864)  loss_n_100: 2.3097 (2.8160)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.1871)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.3827)  time: 0.3675  data: 0.0335  max mem: 46473\n",
      "Valid: [epoch:5]  [ 20/845]  eta: 0:04:50  loss: 9.7896 (11.4804)  loss_n_40: 2.1126 (2.5382)  loss_n_60: 2.6270 (2.9804)  loss_n_80: 2.3593 (2.7554)  loss_n_100: 2.3097 (2.9080)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0980)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.2005)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [ 30/845]  eta: 0:04:41  loss: 10.3117 (11.6556)  loss_n_40: 2.2269 (2.6040)  loss_n_60: 2.7564 (3.0321)  loss_n_80: 2.4648 (2.8086)  loss_n_100: 2.6126 (3.0086)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0664)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.1358)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [ 40/845]  eta: 0:04:36  loss: 10.3117 (11.2372)  loss_n_40: 2.2051 (2.4846)  loss_n_60: 2.7564 (2.9394)  loss_n_80: 2.4648 (2.7114)  loss_n_100: 2.4352 (2.9002)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0502)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.1515)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [ 50/845]  eta: 0:04:31  loss: 8.4518 (10.7963)  loss_n_40: 2.0241 (2.3936)  loss_n_60: 2.5449 (2.8685)  loss_n_80: 2.1052 (2.6199)  loss_n_100: 1.9076 (2.7522)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0403)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.1218)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [ 60/845]  eta: 0:04:26  loss: 9.6717 (11.0600)  loss_n_40: 2.1161 (2.4609)  loss_n_60: 2.7126 (2.9243)  loss_n_80: 2.4440 (2.6701)  loss_n_100: 2.3909 (2.8050)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0576)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.1422)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [ 70/845]  eta: 0:04:22  loss: 10.5800 (11.2135)  loss_n_40: 2.3110 (2.5092)  loss_n_60: 2.8501 (2.9599)  loss_n_80: 2.4885 (2.7093)  loss_n_100: 2.4550 (2.8553)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0575)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.1222)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [ 80/845]  eta: 0:04:19  loss: 9.3704 (10.9760)  loss_n_40: 2.0236 (2.4580)  loss_n_60: 2.6343 (2.9208)  loss_n_80: 2.4411 (2.6574)  loss_n_100: 2.2968 (2.7823)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0504)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.1071)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [ 90/845]  eta: 0:04:15  loss: 8.6842 (10.7894)  loss_n_40: 1.9982 (2.4125)  loss_n_60: 2.5058 (2.8837)  loss_n_80: 2.0947 (2.6168)  loss_n_100: 1.8732 (2.7270)  triple_100: 0.0000 (0.0091)  triple_80: 0.0000 (0.0449)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0953)  time: 0.3340  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:5]  [100/845]  eta: 0:04:11  loss: 8.5404 (10.8871)  loss_n_40: 1.9444 (2.4398)  loss_n_60: 2.4441 (2.9033)  loss_n_80: 2.0662 (2.6321)  loss_n_100: 1.9244 (2.7423)  triple_100: 0.0000 (0.0082)  triple_80: 0.0000 (0.0754)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0859)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [110/845]  eta: 0:04:07  loss: 8.9230 (10.7741)  loss_n_40: 2.0254 (2.4107)  loss_n_60: 2.6079 (2.8823)  loss_n_80: 2.2229 (2.6037)  loss_n_100: 2.1233 (2.7042)  triple_100: 0.0000 (0.0075)  triple_80: 0.0000 (0.0686)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0971)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [120/845]  eta: 0:04:04  loss: 9.2661 (10.7114)  loss_n_40: 2.0211 (2.4035)  loss_n_60: 2.5321 (2.8769)  loss_n_80: 2.2934 (2.5903)  loss_n_100: 2.2050 (2.6819)  triple_100: 0.0000 (0.0069)  triple_80: 0.0000 (0.0630)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0891)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [130/845]  eta: 0:04:00  loss: 9.4087 (10.6743)  loss_n_40: 2.0535 (2.3914)  loss_n_60: 2.5389 (2.8690)  loss_n_80: 2.3195 (2.5859)  loss_n_100: 2.4371 (2.6812)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0582)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0823)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [140/845]  eta: 0:03:57  loss: 9.4696 (10.5802)  loss_n_40: 2.0194 (2.3709)  loss_n_60: 2.5701 (2.8523)  loss_n_80: 2.3344 (2.5668)  loss_n_100: 2.4371 (2.6539)  triple_100: 0.0000 (0.0059)  triple_80: 0.0000 (0.0540)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0764)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [150/845]  eta: 0:03:53  loss: 9.0046 (10.6139)  loss_n_40: 1.9995 (2.3716)  loss_n_60: 2.5358 (2.8524)  loss_n_80: 2.3070 (2.5617)  loss_n_100: 2.2565 (2.6385)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0588)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.1254)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [160/845]  eta: 0:03:50  loss: 9.0046 (10.5735)  loss_n_40: 2.0992 (2.3665)  loss_n_60: 2.6721 (2.8493)  loss_n_80: 2.3070 (2.5554)  loss_n_100: 2.2565 (2.6243)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0552)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.1176)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [170/845]  eta: 0:03:46  loss: 8.5746 (10.5112)  loss_n_40: 2.0001 (2.3597)  loss_n_60: 2.5281 (2.8400)  loss_n_80: 2.1290 (2.5411)  loss_n_100: 1.9226 (2.6028)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0519)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.1108)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [180/845]  eta: 0:03:43  loss: 8.3613 (10.4971)  loss_n_40: 1.9853 (2.3614)  loss_n_60: 2.4820 (2.8384)  loss_n_80: 2.0796 (2.5398)  loss_n_100: 1.8632 (2.5992)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0491)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.1046)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [190/845]  eta: 0:03:40  loss: 8.4450 (10.4371)  loss_n_40: 1.9940 (2.3460)  loss_n_60: 2.4848 (2.8248)  loss_n_80: 2.1387 (2.5282)  loss_n_100: 1.9955 (2.5882)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0465)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0992)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [200/845]  eta: 0:03:36  loss: 8.5721 (10.3953)  loss_n_40: 1.9660 (2.3405)  loss_n_60: 2.4848 (2.8192)  loss_n_80: 2.1607 (2.5201)  loss_n_100: 2.0444 (2.5730)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0442)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0942)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [210/845]  eta: 0:03:33  loss: 8.5721 (10.4130)  loss_n_40: 1.9586 (2.3478)  loss_n_60: 2.5304 (2.8251)  loss_n_80: 2.1607 (2.5249)  loss_n_100: 2.0444 (2.5794)  triple_100: 0.0000 (0.0039)  triple_80: 0.0000 (0.0421)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0898)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [220/845]  eta: 0:03:29  loss: 8.4987 (10.3294)  loss_n_40: 1.9706 (2.3296)  loss_n_60: 2.5086 (2.8102)  loss_n_80: 2.0774 (2.5079)  loss_n_100: 1.9980 (2.5521)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0402)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0857)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [230/845]  eta: 0:03:26  loss: 8.7907 (10.3797)  loss_n_40: 1.9857 (2.3440)  loss_n_60: 2.5216 (2.8212)  loss_n_80: 2.1468 (2.5217)  loss_n_100: 1.9173 (2.5688)  triple_100: 0.0000 (0.0036)  triple_80: 0.0000 (0.0384)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0820)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [240/845]  eta: 0:03:23  loss: 8.9942 (10.3316)  loss_n_40: 2.0141 (2.3321)  loss_n_60: 2.5409 (2.8115)  loss_n_80: 2.2425 (2.5121)  loss_n_100: 2.2721 (2.5570)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0368)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0786)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [250/845]  eta: 0:03:19  loss: 8.5872 (10.3156)  loss_n_40: 1.9539 (2.3268)  loss_n_60: 2.4914 (2.8088)  loss_n_80: 2.1945 (2.5120)  loss_n_100: 2.0955 (2.5539)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0354)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0755)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [260/845]  eta: 0:03:16  loss: 8.6317 (10.2926)  loss_n_40: 1.9551 (2.3194)  loss_n_60: 2.4699 (2.8036)  loss_n_80: 2.1945 (2.5081)  loss_n_100: 2.0844 (2.5517)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0340)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0726)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [270/845]  eta: 0:03:12  loss: 8.5886 (10.2701)  loss_n_40: 1.9702 (2.3161)  loss_n_60: 2.4691 (2.8009)  loss_n_80: 2.1615 (2.5035)  loss_n_100: 2.0787 (2.5439)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0328)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0699)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [280/845]  eta: 0:03:09  loss: 8.5886 (10.3154)  loss_n_40: 1.9874 (2.3255)  loss_n_60: 2.5165 (2.8105)  loss_n_80: 2.1615 (2.5162)  loss_n_100: 2.0787 (2.5612)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0316)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0674)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [290/845]  eta: 0:03:06  loss: 10.2036 (10.3400)  loss_n_40: 2.2548 (2.3297)  loss_n_60: 2.8280 (2.8149)  loss_n_80: 2.5575 (2.5238)  loss_n_100: 2.5047 (2.5731)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0305)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0651)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [300/845]  eta: 0:03:02  loss: 9.2300 (10.3240)  loss_n_40: 2.0138 (2.3234)  loss_n_60: 2.6276 (2.8112)  loss_n_80: 2.3990 (2.5217)  loss_n_100: 2.3333 (2.5726)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0295)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0629)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [310/845]  eta: 0:02:59  loss: 8.6659 (10.3037)  loss_n_40: 1.9891 (2.3194)  loss_n_60: 2.5128 (2.8079)  loss_n_80: 2.2464 (2.5175)  loss_n_100: 2.0123 (2.5633)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0286)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0643)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [320/845]  eta: 0:02:56  loss: 8.4843 (10.2605)  loss_n_40: 1.9804 (2.3100)  loss_n_60: 2.4906 (2.8003)  loss_n_80: 2.1206 (2.5089)  loss_n_100: 1.9048 (2.5487)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0277)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0623)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [330/845]  eta: 0:02:52  loss: 8.5845 (10.2328)  loss_n_40: 1.9532 (2.3032)  loss_n_60: 2.4906 (2.7966)  loss_n_80: 2.2021 (2.5038)  loss_n_100: 2.0091 (2.5389)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0268)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0609)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [340/845]  eta: 0:02:49  loss: 8.7147 (10.2337)  loss_n_40: 1.9691 (2.3045)  loss_n_60: 2.5108 (2.7968)  loss_n_80: 2.1299 (2.5051)  loss_n_100: 1.9921 (2.5397)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0260)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0592)  time: 0.3343  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:5]  [350/845]  eta: 0:02:45  loss: 8.4010 (10.1871)  loss_n_40: 1.9625 (2.2955)  loss_n_60: 2.4497 (2.7878)  loss_n_80: 2.0878 (2.4947)  loss_n_100: 1.9134 (2.5239)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0253)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0575)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [360/845]  eta: 0:02:42  loss: 8.4245 (10.1522)  loss_n_40: 1.9421 (2.2870)  loss_n_60: 2.4346 (2.7806)  loss_n_80: 2.0770 (2.4876)  loss_n_100: 1.9134 (2.5143)  triple_100: 0.0000 (0.0023)  triple_80: 0.0000 (0.0246)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0559)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [370/845]  eta: 0:02:39  loss: 8.5434 (10.1669)  loss_n_40: 1.9655 (2.2911)  loss_n_60: 2.4851 (2.7845)  loss_n_80: 2.1274 (2.4913)  loss_n_100: 1.9862 (2.5196)  triple_100: 0.0000 (0.0022)  triple_80: 0.0000 (0.0239)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0544)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [380/845]  eta: 0:02:35  loss: 9.6552 (10.2458)  loss_n_40: 2.0983 (2.3108)  loss_n_60: 2.6359 (2.8005)  loss_n_80: 2.4151 (2.5077)  loss_n_100: 2.4122 (2.5448)  triple_100: 0.0000 (0.0022)  triple_80: 0.0000 (0.0262)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0537)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [390/845]  eta: 0:02:32  loss: 10.3876 (10.2757)  loss_n_40: 2.2449 (2.3191)  loss_n_60: 2.8768 (2.8088)  loss_n_80: 2.5751 (2.5150)  loss_n_100: 2.5674 (2.5528)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0255)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0524)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [400/845]  eta: 0:02:29  loss: 10.2543 (10.3330)  loss_n_40: 2.2336 (2.3367)  loss_n_60: 2.9444 (2.8210)  loss_n_80: 2.5448 (2.5257)  loss_n_100: 2.4710 (2.5648)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0318)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0511)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [410/845]  eta: 0:02:25  loss: 9.5470 (10.3518)  loss_n_40: 1.9739 (2.3405)  loss_n_60: 2.5685 (2.8246)  loss_n_80: 2.4237 (2.5312)  loss_n_100: 2.2539 (2.5726)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0310)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0498)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [420/845]  eta: 0:02:22  loss: 8.9164 (10.3710)  loss_n_40: 2.0409 (2.3441)  loss_n_60: 2.5685 (2.8282)  loss_n_80: 2.2206 (2.5367)  loss_n_100: 2.1671 (2.5812)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0302)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0486)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [430/845]  eta: 0:02:19  loss: 9.4829 (10.3603)  loss_n_40: 2.0529 (2.3387)  loss_n_60: 2.6327 (2.8246)  loss_n_80: 2.4387 (2.5364)  loss_n_100: 2.3381 (2.5817)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0295)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0475)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [440/845]  eta: 0:02:15  loss: 9.4829 (10.3486)  loss_n_40: 2.0332 (2.3333)  loss_n_60: 2.6250 (2.8210)  loss_n_80: 2.2973 (2.5315)  loss_n_100: 2.2723 (2.5750)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0289)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0571)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [450/845]  eta: 0:02:12  loss: 8.9301 (10.3619)  loss_n_40: 2.0246 (2.3382)  loss_n_60: 2.5517 (2.8250)  loss_n_80: 2.2595 (2.5341)  loss_n_100: 2.2337 (2.5783)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0282)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0562)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [460/845]  eta: 0:02:08  loss: 8.9301 (10.3936)  loss_n_40: 2.0246 (2.3455)  loss_n_60: 2.5804 (2.8307)  loss_n_80: 2.2595 (2.5413)  loss_n_100: 2.2491 (2.5894)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0276)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0574)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [470/845]  eta: 0:02:05  loss: 8.7034 (10.3957)  loss_n_40: 1.9987 (2.3500)  loss_n_60: 2.5804 (2.8335)  loss_n_80: 2.1454 (2.5406)  loss_n_100: 2.0777 (2.5867)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0270)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0561)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [480/845]  eta: 0:02:02  loss: 8.5825 (10.3858)  loss_n_40: 1.9835 (2.3489)  loss_n_60: 2.5195 (2.8317)  loss_n_80: 2.0829 (2.5368)  loss_n_100: 1.9326 (2.5808)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0265)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0594)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [490/845]  eta: 0:01:58  loss: 8.8286 (10.3959)  loss_n_40: 1.9815 (2.3541)  loss_n_60: 2.5133 (2.8354)  loss_n_80: 2.2229 (2.5385)  loss_n_100: 2.1039 (2.5809)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0272)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0582)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [500/845]  eta: 0:01:55  loss: 9.0203 (10.4044)  loss_n_40: 2.0015 (2.3560)  loss_n_60: 2.5681 (2.8374)  loss_n_80: 2.2826 (2.5404)  loss_n_100: 2.2264 (2.5830)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0289)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0570)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [510/845]  eta: 0:01:52  loss: 9.2214 (10.4031)  loss_n_40: 2.0239 (2.3531)  loss_n_60: 2.5836 (2.8357)  loss_n_80: 2.3561 (2.5401)  loss_n_100: 2.3186 (2.5853)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0283)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0559)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [520/845]  eta: 0:01:48  loss: 8.6669 (10.3879)  loss_n_40: 1.9714 (2.3503)  loss_n_60: 2.5101 (2.8328)  loss_n_80: 2.1998 (2.5373)  loss_n_100: 2.0427 (2.5804)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0278)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0548)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [530/845]  eta: 0:01:45  loss: 8.5998 (10.3769)  loss_n_40: 1.9714 (2.3488)  loss_n_60: 2.5125 (2.8315)  loss_n_80: 2.1268 (2.5343)  loss_n_100: 1.9958 (2.5749)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0273)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0557)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [540/845]  eta: 0:01:42  loss: 9.3540 (10.4404)  loss_n_40: 2.0761 (2.3652)  loss_n_60: 2.6149 (2.8445)  loss_n_80: 2.3418 (2.5459)  loss_n_100: 2.2618 (2.5898)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0359)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0547)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [550/845]  eta: 0:01:38  loss: 9.5195 (10.4454)  loss_n_40: 2.0761 (2.3645)  loss_n_60: 2.6531 (2.8452)  loss_n_80: 2.4201 (2.5482)  loss_n_100: 2.3130 (2.5943)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0353)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0537)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [560/845]  eta: 0:01:35  loss: 8.5861 (10.4371)  loss_n_40: 1.9771 (2.3636)  loss_n_60: 2.5221 (2.8439)  loss_n_80: 2.1592 (2.5449)  loss_n_100: 1.9557 (2.5887)  triple_100: 0.0000 (0.0042)  triple_80: 0.0000 (0.0391)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0527)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [570/845]  eta: 0:01:32  loss: 8.5903 (10.4579)  loss_n_40: 2.0414 (2.3701)  loss_n_60: 2.5116 (2.8489)  loss_n_80: 2.1592 (2.5493)  loss_n_100: 2.0583 (2.5934)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0402)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0518)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [580/845]  eta: 0:01:28  loss: 9.3268 (10.4699)  loss_n_40: 2.0574 (2.3729)  loss_n_60: 2.5801 (2.8503)  loss_n_80: 2.3552 (2.5529)  loss_n_100: 2.2668 (2.5993)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0396)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0509)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [590/845]  eta: 0:01:25  loss: 9.3268 (10.4945)  loss_n_40: 2.0574 (2.3793)  loss_n_60: 2.5302 (2.8547)  loss_n_80: 2.3296 (2.5579)  loss_n_100: 2.2842 (2.6078)  triple_100: 0.0000 (0.0059)  triple_80: 0.0000 (0.0389)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0500)  time: 0.3342  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:5]  [600/845]  eta: 0:01:22  loss: 9.1000 (10.4704)  loss_n_40: 1.9648 (2.3727)  loss_n_60: 2.5061 (2.8493)  loss_n_80: 2.3296 (2.5533)  loss_n_100: 2.2842 (2.6019)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0382)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0492)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [610/845]  eta: 0:01:18  loss: 9.2136 (10.4873)  loss_n_40: 1.9648 (2.3745)  loss_n_60: 2.5282 (2.8514)  loss_n_80: 2.3586 (2.5587)  loss_n_100: 2.2933 (2.6089)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0376)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0504)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [620/845]  eta: 0:01:15  loss: 10.0569 (10.4910)  loss_n_40: 2.1351 (2.3749)  loss_n_60: 2.6876 (2.8517)  loss_n_80: 2.4563 (2.5596)  loss_n_100: 2.3799 (2.6110)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0370)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0512)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [630/845]  eta: 0:01:11  loss: 10.5284 (10.5300)  loss_n_40: 2.2899 (2.3859)  loss_n_60: 2.8491 (2.8599)  loss_n_80: 2.4940 (2.5664)  loss_n_100: 2.5328 (2.6199)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0419)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0504)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [640/845]  eta: 0:01:08  loss: 10.7971 (10.5476)  loss_n_40: 2.3980 (2.3921)  loss_n_60: 2.9578 (2.8645)  loss_n_80: 2.5088 (2.5698)  loss_n_100: 2.6171 (2.6248)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0412)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0496)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [650/845]  eta: 0:01:05  loss: 8.3486 (10.5318)  loss_n_40: 2.0667 (2.3886)  loss_n_60: 2.5308 (2.8618)  loss_n_80: 2.1122 (2.5661)  loss_n_100: 1.9768 (2.6205)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0406)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0489)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [660/845]  eta: 0:01:01  loss: 8.5629 (10.5438)  loss_n_40: 2.0667 (2.3922)  loss_n_60: 2.5209 (2.8642)  loss_n_80: 2.0785 (2.5687)  loss_n_100: 2.0177 (2.6250)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0400)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0484)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [670/845]  eta: 0:00:58  loss: 8.5629 (10.5554)  loss_n_40: 2.0355 (2.3964)  loss_n_60: 2.4722 (2.8672)  loss_n_80: 2.1249 (2.5714)  loss_n_100: 2.0177 (2.6280)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0394)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0477)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [680/845]  eta: 0:00:55  loss: 8.5058 (10.5539)  loss_n_40: 1.9801 (2.3955)  loss_n_60: 2.4722 (2.8664)  loss_n_80: 2.1249 (2.5699)  loss_n_100: 1.9078 (2.6259)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0392)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0519)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [690/845]  eta: 0:00:51  loss: 8.4861 (10.5389)  loss_n_40: 1.9921 (2.3917)  loss_n_60: 2.4786 (2.8632)  loss_n_80: 2.1222 (2.5672)  loss_n_100: 1.8949 (2.6221)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0386)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0511)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [700/845]  eta: 0:00:48  loss: 9.2748 (10.5432)  loss_n_40: 2.0046 (2.3916)  loss_n_60: 2.4848 (2.8632)  loss_n_80: 2.2974 (2.5685)  loss_n_100: 2.3502 (2.6262)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0381)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0504)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [710/845]  eta: 0:00:45  loss: 8.5021 (10.5294)  loss_n_40: 1.9563 (2.3882)  loss_n_60: 2.5047 (2.8602)  loss_n_80: 2.1653 (2.5657)  loss_n_100: 1.9669 (2.6229)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0375)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0497)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [720/845]  eta: 0:00:41  loss: 8.5349 (10.5320)  loss_n_40: 1.9716 (2.3895)  loss_n_60: 2.5047 (2.8608)  loss_n_80: 2.1653 (2.5662)  loss_n_100: 1.9669 (2.6241)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0374)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0490)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [730/845]  eta: 0:00:38  loss: 9.0259 (10.5436)  loss_n_40: 2.0147 (2.3925)  loss_n_60: 2.5627 (2.8626)  loss_n_80: 2.2658 (2.5678)  loss_n_100: 2.2170 (2.6264)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0410)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0483)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [740/845]  eta: 0:00:35  loss: 9.3088 (10.5550)  loss_n_40: 2.0226 (2.3955)  loss_n_60: 2.5627 (2.8651)  loss_n_80: 2.3873 (2.5707)  loss_n_100: 2.3360 (2.6307)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0404)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0477)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [750/845]  eta: 0:00:31  loss: 9.5800 (10.5711)  loss_n_40: 2.1065 (2.3987)  loss_n_60: 2.5863 (2.8675)  loss_n_80: 2.4339 (2.5738)  loss_n_100: 2.3971 (2.6353)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0439)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0470)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [760/845]  eta: 0:00:28  loss: 8.9862 (10.5579)  loss_n_40: 1.9908 (2.3960)  loss_n_60: 2.5341 (2.8647)  loss_n_80: 2.3344 (2.5712)  loss_n_100: 2.2666 (2.6315)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0433)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0464)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [770/845]  eta: 0:00:25  loss: 8.8132 (10.5547)  loss_n_40: 1.9844 (2.3951)  loss_n_60: 2.5189 (2.8643)  loss_n_80: 2.1660 (2.5700)  loss_n_100: 2.1387 (2.6299)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0428)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0479)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [780/845]  eta: 0:00:21  loss: 8.8729 (10.5535)  loss_n_40: 2.0211 (2.3946)  loss_n_60: 2.5406 (2.8637)  loss_n_80: 2.1990 (2.5702)  loss_n_100: 2.1470 (2.6309)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0422)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0472)  time: 0.3353  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [790/845]  eta: 0:00:18  loss: 8.8729 (10.5590)  loss_n_40: 2.0278 (2.3957)  loss_n_60: 2.5187 (2.8644)  loss_n_80: 2.1719 (2.5720)  loss_n_100: 2.0621 (2.6339)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0417)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0466)  time: 0.3353  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [800/845]  eta: 0:00:15  loss: 8.7835 (10.5473)  loss_n_40: 1.9752 (2.3920)  loss_n_60: 2.4675 (2.8611)  loss_n_80: 2.2205 (2.5702)  loss_n_100: 2.1744 (2.6314)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0412)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0468)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [810/845]  eta: 0:00:11  loss: 9.2222 (10.5523)  loss_n_40: 1.9414 (2.3933)  loss_n_60: 2.5498 (2.8624)  loss_n_80: 2.4062 (2.5713)  loss_n_100: 2.2986 (2.6333)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0406)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0467)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [820/845]  eta: 0:00:08  loss: 9.7456 (10.5618)  loss_n_40: 2.0894 (2.3952)  loss_n_60: 2.7495 (2.8641)  loss_n_80: 2.4711 (2.5742)  loss_n_100: 2.3869 (2.6375)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0402)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0461)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [830/845]  eta: 0:00:05  loss: 9.0874 (10.5599)  loss_n_40: 1.9619 (2.3950)  loss_n_60: 2.4788 (2.8639)  loss_n_80: 2.4099 (2.5737)  loss_n_100: 2.2986 (2.6368)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0407)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0456)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5]  [840/845]  eta: 0:00:01  loss: 9.6911 (10.5586)  loss_n_40: 2.0912 (2.3931)  loss_n_60: 2.5646 (2.8632)  loss_n_80: 2.4290 (2.5737)  loss_n_100: 2.4185 (2.6390)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0402)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0450)  time: 0.3343  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:5]  [844/845]  eta: 0:00:00  loss: 10.4039 (10.5582)  loss_n_40: 2.2059 (2.3921)  loss_n_60: 2.8594 (2.8627)  loss_n_80: 2.4992 (2.5741)  loss_n_100: 2.5475 (2.6402)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0400)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0448)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:5] Total time: 0:04:42 (0.3348 s / it)\n",
      "Averaged stats: loss: 10.4039 (10.5582)  loss_n_40: 2.2059 (2.3921)  loss_n_60: 2.8594 (2.8627)  loss_n_80: 2.4992 (2.5741)  loss_n_100: 2.5475 (2.6402)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0400)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0448)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_5_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 2.640%\n",
      "Min loss_n_100: 2.640\n",
      "Best Epoch: 5.000\n",
      "Train: [epoch:6]  [   0/1724]  eta: 2:00:00  lr: 0.000100  loss: 8.7549 (8.7549)  loss_n_40: 2.0619 (2.0619)  loss_n_60: 2.5562 (2.5562)  loss_n_80: 2.0936 (2.0936)  loss_n_100: 2.0431 (2.0431)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1765  data: 0.4264  max mem: 46473\n",
      "Train: [epoch:6]  [  10/1724]  eta: 1:52:39  lr: 0.000100  loss: 10.5539 (11.2076)  loss_n_40: 2.1926 (2.4066)  loss_n_60: 2.6733 (2.8236)  loss_n_80: 2.6158 (2.6098)  loss_n_100: 3.1657 (3.0708)  triple_100: 0.0000 (0.0270)  triple_80: 0.0000 (0.0390)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.2307)  time: 3.9438  data: 0.0389  max mem: 46473\n",
      "Train: [epoch:6]  [  20/1724]  eta: 1:51:39  lr: 0.000100  loss: 11.3837 (11.0257)  loss_n_40: 2.4848 (2.4433)  loss_n_60: 2.8571 (2.8439)  loss_n_80: 2.6271 (2.5643)  loss_n_100: 3.0722 (2.9332)  triple_100: 0.0000 (0.0483)  triple_80: 0.0000 (0.0719)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.1209)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [  30/1724]  eta: 1:50:53  lr: 0.000100  loss: 10.2348 (10.7600)  loss_n_40: 2.3551 (2.4031)  loss_n_60: 2.8191 (2.8153)  loss_n_80: 2.3737 (2.5131)  loss_n_100: 2.5507 (2.7965)  triple_100: 0.0000 (0.0744)  triple_80: 0.0000 (0.0489)  triple_60: 0.0000 (0.0269)  triple_40: 0.0000 (0.0819)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [  40/1724]  eta: 1:50:10  lr: 0.000100  loss: 9.7945 (10.3626)  loss_n_40: 2.2094 (2.3438)  loss_n_60: 2.6269 (2.7478)  loss_n_80: 2.3499 (2.4284)  loss_n_100: 2.4053 (2.6536)  triple_100: 0.0000 (0.0562)  triple_80: 0.0000 (0.0370)  triple_60: 0.0000 (0.0203)  triple_40: 0.0000 (0.0755)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [  50/1724]  eta: 1:49:29  lr: 0.000100  loss: 8.3086 (9.9761)  loss_n_40: 2.0857 (2.2981)  loss_n_60: 2.3800 (2.6741)  loss_n_80: 1.9125 (2.3227)  loss_n_100: 1.9053 (2.4946)  triple_100: 0.0000 (0.0549)  triple_80: 0.0000 (0.0301)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0853)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [  60/1724]  eta: 1:48:48  lr: 0.000100  loss: 8.0019 (9.7491)  loss_n_40: 1.9468 (2.2764)  loss_n_60: 2.2500 (2.6214)  loss_n_80: 1.7893 (2.2736)  loss_n_100: 1.7183 (2.4105)  triple_100: 0.0000 (0.0493)  triple_80: 0.0000 (0.0294)  triple_60: 0.0000 (0.0172)  triple_40: 0.0000 (0.0713)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [  70/1724]  eta: 1:48:08  lr: 0.000100  loss: 9.1890 (9.6902)  loss_n_40: 2.0327 (2.2768)  loss_n_60: 2.2960 (2.6033)  loss_n_80: 2.0503 (2.2539)  loss_n_100: 2.0597 (2.3715)  triple_100: 0.0000 (0.0677)  triple_80: 0.0000 (0.0409)  triple_60: 0.0000 (0.0148)  triple_40: 0.0000 (0.0613)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [  80/1724]  eta: 1:47:27  lr: 0.000100  loss: 9.2340 (9.6154)  loss_n_40: 2.0647 (2.2704)  loss_n_60: 2.2960 (2.5836)  loss_n_80: 2.0503 (2.2365)  loss_n_100: 2.0597 (2.3335)  triple_100: 0.0000 (0.0687)  triple_80: 0.0000 (0.0359)  triple_60: 0.0000 (0.0333)  triple_40: 0.0000 (0.0537)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [  90/1724]  eta: 1:46:47  lr: 0.000100  loss: 8.6354 (9.5176)  loss_n_40: 2.0647 (2.2577)  loss_n_60: 2.2762 (2.5604)  loss_n_80: 1.9205 (2.2207)  loss_n_100: 1.9199 (2.2914)  triple_100: 0.0000 (0.0611)  triple_80: 0.0000 (0.0395)  triple_60: 0.0000 (0.0314)  triple_40: 0.0000 (0.0553)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 100/1724]  eta: 1:46:08  lr: 0.000100  loss: 7.8683 (9.3568)  loss_n_40: 1.9286 (2.2346)  loss_n_60: 2.2021 (2.5278)  loss_n_80: 1.8804 (2.1867)  loss_n_100: 1.7290 (2.2365)  triple_100: 0.0000 (0.0575)  triple_80: 0.0000 (0.0356)  triple_60: 0.0000 (0.0283)  triple_40: 0.0000 (0.0499)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 110/1724]  eta: 1:45:28  lr: 0.000100  loss: 7.8683 (9.2985)  loss_n_40: 2.0081 (2.2297)  loss_n_60: 2.2536 (2.5119)  loss_n_80: 1.8631 (2.1681)  loss_n_100: 1.7230 (2.2108)  triple_100: 0.0000 (0.0525)  triple_80: 0.0000 (0.0421)  triple_60: 0.0000 (0.0277)  triple_40: 0.0000 (0.0557)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 120/1724]  eta: 1:44:49  lr: 0.000100  loss: 7.4315 (9.1827)  loss_n_40: 1.8969 (2.2061)  loss_n_60: 2.1963 (2.4918)  loss_n_80: 1.8506 (2.1469)  loss_n_100: 1.6839 (2.1746)  triple_100: 0.0000 (0.0482)  triple_80: 0.0000 (0.0386)  triple_60: 0.0000 (0.0254)  triple_40: 0.0000 (0.0511)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 130/1724]  eta: 1:44:10  lr: 0.000100  loss: 7.3281 (9.0931)  loss_n_40: 1.8969 (2.1924)  loss_n_60: 2.1563 (2.4752)  loss_n_80: 1.7728 (2.1257)  loss_n_100: 1.5805 (2.1461)  triple_100: 0.0000 (0.0456)  triple_80: 0.0000 (0.0360)  triple_60: 0.0000 (0.0234)  triple_40: 0.0000 (0.0488)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 140/1724]  eta: 1:43:30  lr: 0.000100  loss: 7.2400 (9.0001)  loss_n_40: 1.8433 (2.1743)  loss_n_60: 2.1013 (2.4536)  loss_n_80: 1.7066 (2.0994)  loss_n_100: 1.5866 (2.1118)  triple_100: 0.0000 (0.0452)  triple_80: 0.0000 (0.0335)  triple_60: 0.0000 (0.0297)  triple_40: 0.0000 (0.0525)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 150/1724]  eta: 1:42:51  lr: 0.000100  loss: 7.3090 (8.9725)  loss_n_40: 1.8125 (2.1562)  loss_n_60: 2.0963 (2.4384)  loss_n_80: 1.9102 (2.1050)  loss_n_100: 1.6784 (2.1134)  triple_100: 0.0000 (0.0515)  triple_80: 0.0000 (0.0312)  triple_60: 0.0000 (0.0278)  triple_40: 0.0000 (0.0490)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 160/1724]  eta: 1:42:11  lr: 0.000100  loss: 7.7384 (8.9431)  loss_n_40: 1.9178 (2.1483)  loss_n_60: 2.1756 (2.4255)  loss_n_80: 1.9554 (2.1057)  loss_n_100: 1.8227 (2.1092)  triple_100: 0.0000 (0.0513)  triple_80: 0.0000 (0.0293)  triple_60: 0.0000 (0.0260)  triple_40: 0.0000 (0.0477)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 170/1724]  eta: 1:41:32  lr: 0.000100  loss: 7.3976 (8.8888)  loss_n_40: 1.8832 (2.1346)  loss_n_60: 2.1059 (2.4092)  loss_n_80: 1.7636 (2.0890)  loss_n_100: 1.6950 (2.0942)  triple_100: 0.0000 (0.0553)  triple_80: 0.0000 (0.0315)  triple_60: 0.0000 (0.0245)  triple_40: 0.0000 (0.0506)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 180/1724]  eta: 1:40:52  lr: 0.000100  loss: 7.4159 (8.8689)  loss_n_40: 1.9215 (2.1372)  loss_n_60: 2.1431 (2.4081)  loss_n_80: 1.7185 (2.0828)  loss_n_100: 1.7176 (2.0859)  triple_100: 0.0000 (0.0525)  triple_80: 0.0000 (0.0298)  triple_60: 0.0000 (0.0232)  triple_40: 0.0000 (0.0495)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 190/1724]  eta: 1:40:13  lr: 0.000100  loss: 8.3900 (8.8688)  loss_n_40: 2.0827 (2.1469)  loss_n_60: 2.3656 (2.4106)  loss_n_80: 1.9627 (2.0806)  loss_n_100: 1.9653 (2.0823)  triple_100: 0.0000 (0.0500)  triple_80: 0.0000 (0.0282)  triple_60: 0.0000 (0.0219)  triple_40: 0.0000 (0.0483)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 200/1724]  eta: 1:39:34  lr: 0.000100  loss: 8.6559 (8.8553)  loss_n_40: 2.0668 (2.1425)  loss_n_60: 2.3944 (2.4054)  loss_n_80: 1.9366 (2.0775)  loss_n_100: 1.9306 (2.0722)  triple_100: 0.0000 (0.0479)  triple_80: 0.0000 (0.0284)  triple_60: 0.0000 (0.0273)  triple_40: 0.0000 (0.0541)  time: 3.9196  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [ 210/1724]  eta: 1:38:54  lr: 0.000100  loss: 7.7402 (8.7992)  loss_n_40: 1.9393 (2.1292)  loss_n_60: 2.1809 (2.3931)  loss_n_80: 1.8771 (2.0696)  loss_n_100: 1.7240 (2.0563)  triple_100: 0.0000 (0.0456)  triple_80: 0.0000 (0.0275)  triple_60: 0.0000 (0.0260)  triple_40: 0.0000 (0.0521)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 220/1724]  eta: 1:38:15  lr: 0.000100  loss: 6.9633 (8.7131)  loss_n_40: 1.7287 (2.1106)  loss_n_60: 1.9993 (2.3726)  loss_n_80: 1.7278 (2.0505)  loss_n_100: 1.6097 (2.0338)  triple_100: 0.0000 (0.0435)  triple_80: 0.0000 (0.0262)  triple_60: 0.0000 (0.0248)  triple_40: 0.0000 (0.0511)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 230/1724]  eta: 1:37:36  lr: 0.000100  loss: 6.5287 (8.6607)  loss_n_40: 1.6739 (2.0985)  loss_n_60: 1.8930 (2.3556)  loss_n_80: 1.5384 (2.0362)  loss_n_100: 1.3905 (2.0164)  triple_100: 0.0000 (0.0469)  triple_80: 0.0000 (0.0342)  triple_60: 0.0000 (0.0241)  triple_40: 0.0000 (0.0489)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 240/1724]  eta: 1:36:56  lr: 0.000100  loss: 7.0485 (8.6306)  loss_n_40: 1.7348 (2.0944)  loss_n_60: 1.9716 (2.3487)  loss_n_80: 1.6287 (2.0308)  loss_n_100: 1.5569 (2.0091)  triple_100: 0.0000 (0.0450)  triple_80: 0.0000 (0.0328)  triple_60: 0.0000 (0.0231)  triple_40: 0.0000 (0.0469)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 250/1724]  eta: 1:36:17  lr: 0.000100  loss: 7.6617 (8.6116)  loss_n_40: 1.8337 (2.0878)  loss_n_60: 2.1022 (2.3405)  loss_n_80: 1.7916 (2.0202)  loss_n_100: 1.7951 (1.9985)  triple_100: 0.0000 (0.0450)  triple_80: 0.0000 (0.0315)  triple_60: 0.0000 (0.0315)  triple_40: 0.0000 (0.0566)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 260/1724]  eta: 1:35:38  lr: 0.000100  loss: 7.2438 (8.5720)  loss_n_40: 1.7449 (2.0787)  loss_n_60: 2.0557 (2.3311)  loss_n_80: 1.6756 (2.0086)  loss_n_100: 1.5509 (1.9844)  triple_100: 0.0000 (0.0433)  triple_80: 0.0000 (0.0310)  triple_60: 0.0000 (0.0378)  triple_40: 0.0000 (0.0572)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 270/1724]  eta: 1:34:58  lr: 0.000100  loss: 8.2580 (8.5929)  loss_n_40: 1.8632 (2.0804)  loss_n_60: 2.1357 (2.3304)  loss_n_80: 1.8649 (2.0110)  loss_n_100: 1.8448 (1.9941)  triple_100: 0.0000 (0.0557)  triple_80: 0.0000 (0.0298)  triple_60: 0.0000 (0.0364)  triple_40: 0.0000 (0.0551)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 280/1724]  eta: 1:34:19  lr: 0.000100  loss: 8.5111 (8.5859)  loss_n_40: 1.9804 (2.0791)  loss_n_60: 2.1658 (2.3263)  loss_n_80: 1.9935 (2.0120)  loss_n_100: 2.1156 (1.9972)  triple_100: 0.0000 (0.0537)  triple_80: 0.0000 (0.0293)  triple_60: 0.0000 (0.0351)  triple_40: 0.0000 (0.0531)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 290/1724]  eta: 1:33:40  lr: 0.000100  loss: 7.8710 (8.5352)  loss_n_40: 1.8212 (2.0668)  loss_n_60: 2.0155 (2.3121)  loss_n_80: 1.9221 (2.0031)  loss_n_100: 1.8816 (1.9876)  triple_100: 0.0000 (0.0518)  triple_80: 0.0000 (0.0283)  triple_60: 0.0000 (0.0339)  triple_40: 0.0000 (0.0517)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 300/1724]  eta: 1:33:00  lr: 0.000100  loss: 6.8057 (8.4911)  loss_n_40: 1.7159 (2.0580)  loss_n_60: 1.9177 (2.3012)  loss_n_80: 1.6583 (1.9932)  loss_n_100: 1.5847 (1.9769)  triple_100: 0.0000 (0.0501)  triple_80: 0.0000 (0.0290)  triple_60: 0.0000 (0.0328)  triple_40: 0.0000 (0.0499)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 310/1724]  eta: 1:32:21  lr: 0.000100  loss: 6.9694 (8.4526)  loss_n_40: 1.6557 (2.0476)  loss_n_60: 1.8703 (2.2883)  loss_n_80: 1.6453 (1.9816)  loss_n_100: 1.6221 (1.9655)  triple_100: 0.0000 (0.0564)  triple_80: 0.0000 (0.0296)  triple_60: 0.0000 (0.0317)  triple_40: 0.0000 (0.0519)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 320/1724]  eta: 1:31:42  lr: 0.000100  loss: 6.7856 (8.4125)  loss_n_40: 1.5779 (2.0394)  loss_n_60: 1.8504 (2.2780)  loss_n_80: 1.6453 (1.9717)  loss_n_100: 1.6478 (1.9556)  triple_100: 0.0000 (0.0546)  triple_80: 0.0000 (0.0287)  triple_60: 0.0000 (0.0319)  triple_40: 0.0000 (0.0526)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 330/1724]  eta: 1:31:03  lr: 0.000100  loss: 6.9035 (8.3878)  loss_n_40: 1.7183 (2.0350)  loss_n_60: 1.8823 (2.2707)  loss_n_80: 1.6735 (1.9650)  loss_n_100: 1.6728 (1.9478)  triple_100: 0.0000 (0.0551)  triple_80: 0.0000 (0.0322)  triple_60: 0.0000 (0.0310)  triple_40: 0.0000 (0.0510)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 340/1724]  eta: 1:30:23  lr: 0.000100  loss: 7.1369 (8.3504)  loss_n_40: 1.8080 (2.0282)  loss_n_60: 1.9375 (2.2609)  loss_n_80: 1.6735 (1.9571)  loss_n_100: 1.6728 (1.9397)  triple_100: 0.0000 (0.0535)  triple_80: 0.0000 (0.0313)  triple_60: 0.0000 (0.0301)  triple_40: 0.0000 (0.0495)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 350/1724]  eta: 1:29:44  lr: 0.000100  loss: 7.3423 (8.3399)  loss_n_40: 1.8080 (2.0250)  loss_n_60: 2.0285 (2.2554)  loss_n_80: 1.7657 (1.9548)  loss_n_100: 1.7276 (1.9370)  triple_100: 0.0000 (0.0567)  triple_80: 0.0000 (0.0332)  triple_60: 0.0000 (0.0294)  triple_40: 0.0000 (0.0485)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 360/1724]  eta: 1:29:05  lr: 0.000100  loss: 6.9293 (8.2982)  loss_n_40: 1.7222 (2.0165)  loss_n_60: 1.9102 (2.2450)  loss_n_80: 1.7550 (1.9471)  loss_n_100: 1.6954 (1.9265)  triple_100: 0.0000 (0.0551)  triple_80: 0.0000 (0.0322)  triple_60: 0.0000 (0.0286)  triple_40: 0.0000 (0.0471)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 370/1724]  eta: 1:28:26  lr: 0.000100  loss: 6.6795 (8.2627)  loss_n_40: 1.5439 (2.0075)  loss_n_60: 1.8145 (2.2346)  loss_n_80: 1.6574 (1.9370)  loss_n_100: 1.4450 (1.9145)  triple_100: 0.0000 (0.0555)  triple_80: 0.0000 (0.0321)  triple_60: 0.0000 (0.0308)  triple_40: 0.0000 (0.0506)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 380/1724]  eta: 1:27:46  lr: 0.000100  loss: 8.0360 (8.2841)  loss_n_40: 1.9269 (2.0173)  loss_n_60: 2.1905 (2.2443)  loss_n_80: 1.8343 (1.9411)  loss_n_100: 1.7180 (1.9169)  triple_100: 0.0000 (0.0540)  triple_80: 0.0000 (0.0313)  triple_60: 0.0000 (0.0300)  triple_40: 0.0000 (0.0493)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 390/1724]  eta: 1:27:07  lr: 0.000100  loss: 8.1539 (8.2652)  loss_n_40: 2.0900 (2.0176)  loss_n_60: 2.3371 (2.2422)  loss_n_80: 1.8748 (1.9340)  loss_n_100: 1.7370 (1.9084)  triple_100: 0.0000 (0.0526)  triple_80: 0.0000 (0.0305)  triple_60: 0.0000 (0.0318)  triple_40: 0.0000 (0.0480)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 400/1724]  eta: 1:26:28  lr: 0.000100  loss: 6.4898 (8.2225)  loss_n_40: 1.7735 (2.0100)  loss_n_60: 1.9026 (2.2324)  loss_n_80: 1.4606 (1.9230)  loss_n_100: 1.3973 (1.8973)  triple_100: 0.0000 (0.0513)  triple_80: 0.0000 (0.0297)  triple_60: 0.0000 (0.0310)  triple_40: 0.0000 (0.0478)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 410/1724]  eta: 1:25:49  lr: 0.000100  loss: 6.4057 (8.1961)  loss_n_40: 1.6959 (2.0073)  loss_n_60: 1.8448 (2.2254)  loss_n_80: 1.4606 (1.9165)  loss_n_100: 1.3973 (1.8896)  triple_100: 0.0000 (0.0501)  triple_80: 0.0000 (0.0290)  triple_60: 0.0000 (0.0303)  triple_40: 0.0000 (0.0480)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 420/1724]  eta: 1:25:09  lr: 0.000100  loss: 6.2197 (8.1536)  loss_n_40: 1.6102 (1.9977)  loss_n_60: 1.7596 (2.2131)  loss_n_80: 1.5225 (1.9061)  loss_n_100: 1.3401 (1.8781)  triple_100: 0.0000 (0.0497)  triple_80: 0.0000 (0.0283)  triple_60: 0.0000 (0.0310)  triple_40: 0.0000 (0.0496)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 430/1724]  eta: 1:24:30  lr: 0.000100  loss: 6.5759 (8.1232)  loss_n_40: 1.5371 (1.9916)  loss_n_60: 1.7170 (2.2042)  loss_n_80: 1.5900 (1.8998)  loss_n_100: 1.3356 (1.8699)  triple_100: 0.0000 (0.0494)  triple_80: 0.0000 (0.0294)  triple_60: 0.0000 (0.0303)  triple_40: 0.0000 (0.0484)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 440/1724]  eta: 1:23:51  lr: 0.000100  loss: 6.4341 (8.0814)  loss_n_40: 1.4896 (1.9831)  loss_n_60: 1.5942 (2.1930)  loss_n_80: 1.4289 (1.8901)  loss_n_100: 1.2727 (1.8583)  triple_100: 0.0000 (0.0493)  triple_80: 0.0000 (0.0295)  triple_60: 0.0000 (0.0296)  triple_40: 0.0000 (0.0485)  time: 3.9171  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [ 450/1724]  eta: 1:23:12  lr: 0.000100  loss: 5.4148 (8.0291)  loss_n_40: 1.3771 (1.9720)  loss_n_60: 1.5871 (2.1808)  loss_n_80: 1.2918 (1.8784)  loss_n_100: 1.2005 (1.8444)  triple_100: 0.0000 (0.0482)  triple_80: 0.0000 (0.0288)  triple_60: 0.0000 (0.0290)  triple_40: 0.0000 (0.0475)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 460/1724]  eta: 1:22:33  lr: 0.000100  loss: 5.4148 (7.9838)  loss_n_40: 1.4171 (1.9640)  loss_n_60: 1.5880 (2.1704)  loss_n_80: 1.2818 (1.8679)  loss_n_100: 1.1675 (1.8314)  triple_100: 0.0000 (0.0472)  triple_80: 0.0000 (0.0282)  triple_60: 0.0000 (0.0283)  triple_40: 0.0000 (0.0464)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 470/1724]  eta: 1:21:53  lr: 0.000100  loss: 5.6408 (7.9465)  loss_n_40: 1.4288 (1.9548)  loss_n_60: 1.5748 (2.1593)  loss_n_80: 1.3588 (1.8581)  loss_n_100: 1.2857 (1.8204)  triple_100: 0.0000 (0.0493)  triple_80: 0.0000 (0.0284)  triple_60: 0.0000 (0.0293)  triple_40: 0.0000 (0.0468)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 480/1724]  eta: 1:21:14  lr: 0.000100  loss: 6.0003 (7.9196)  loss_n_40: 1.3947 (1.9474)  loss_n_60: 1.5822 (2.1507)  loss_n_80: 1.4584 (1.8538)  loss_n_100: 1.3887 (1.8167)  triple_100: 0.0000 (0.0486)  triple_80: 0.0000 (0.0278)  triple_60: 0.0000 (0.0287)  triple_40: 0.0000 (0.0459)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 490/1724]  eta: 1:20:35  lr: 0.000100  loss: 6.4175 (7.9045)  loss_n_40: 1.5279 (1.9449)  loss_n_60: 1.6524 (2.1445)  loss_n_80: 1.6087 (1.8509)  loss_n_100: 1.6115 (1.8146)  triple_100: 0.0000 (0.0476)  triple_80: 0.0000 (0.0272)  triple_60: 0.0000 (0.0281)  triple_40: 0.0000 (0.0467)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 500/1724]  eta: 1:19:56  lr: 0.000100  loss: 5.2140 (7.8635)  loss_n_40: 1.3603 (1.9338)  loss_n_60: 1.4388 (2.1311)  loss_n_80: 1.3180 (1.8407)  loss_n_100: 1.1704 (1.8028)  triple_100: 0.0000 (0.0536)  triple_80: 0.0000 (0.0267)  triple_60: 0.0000 (0.0276)  triple_40: 0.0000 (0.0472)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 510/1724]  eta: 1:19:17  lr: 0.000100  loss: 5.2140 (7.8455)  loss_n_40: 1.3393 (1.9317)  loss_n_60: 1.4388 (2.1252)  loss_n_80: 1.3180 (1.8356)  loss_n_100: 1.1751 (1.7962)  triple_100: 0.0000 (0.0536)  triple_80: 0.0000 (0.0262)  triple_60: 0.0000 (0.0289)  triple_40: 0.0000 (0.0480)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 520/1724]  eta: 1:18:37  lr: 0.000100  loss: 6.3079 (7.8204)  loss_n_40: 1.5430 (1.9267)  loss_n_60: 1.6503 (2.1172)  loss_n_80: 1.5342 (1.8308)  loss_n_100: 1.3891 (1.7904)  triple_100: 0.0000 (0.0529)  triple_80: 0.0000 (0.0257)  triple_60: 0.0000 (0.0285)  triple_40: 0.0000 (0.0482)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 530/1724]  eta: 1:17:58  lr: 0.000100  loss: 6.3079 (7.7907)  loss_n_40: 1.4866 (1.9190)  loss_n_60: 1.5979 (2.1071)  loss_n_80: 1.4798 (1.8232)  loss_n_100: 1.2956 (1.7808)  triple_100: 0.0000 (0.0556)  triple_80: 0.0000 (0.0284)  triple_60: 0.0000 (0.0289)  triple_40: 0.0000 (0.0478)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 540/1724]  eta: 1:17:19  lr: 0.000100  loss: 5.4242 (7.7677)  loss_n_40: 1.3978 (1.9135)  loss_n_60: 1.4708 (2.0986)  loss_n_80: 1.3268 (1.8162)  loss_n_100: 1.2710 (1.7749)  triple_100: 0.0000 (0.0583)  triple_80: 0.0000 (0.0310)  triple_60: 0.0000 (0.0283)  triple_40: 0.0000 (0.0469)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 550/1724]  eta: 1:16:40  lr: 0.000100  loss: 5.8245 (7.7437)  loss_n_40: 1.4621 (1.9089)  loss_n_60: 1.5395 (2.0928)  loss_n_80: 1.3721 (1.8111)  loss_n_100: 1.4508 (1.7692)  triple_100: 0.0000 (0.0574)  triple_80: 0.0000 (0.0304)  triple_60: 0.0000 (0.0278)  triple_40: 0.0000 (0.0460)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 560/1724]  eta: 1:16:00  lr: 0.000100  loss: 6.6369 (7.7201)  loss_n_40: 1.6167 (1.9033)  loss_n_60: 1.8304 (2.0866)  loss_n_80: 1.5525 (1.8067)  loss_n_100: 1.4920 (1.7643)  triple_100: 0.0000 (0.0563)  triple_80: 0.0000 (0.0299)  triple_60: 0.0000 (0.0278)  triple_40: 0.0000 (0.0452)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 570/1724]  eta: 1:15:21  lr: 0.000100  loss: 5.3652 (7.6822)  loss_n_40: 1.2993 (1.8939)  loss_n_60: 1.4949 (2.0763)  loss_n_80: 1.3234 (1.7990)  loss_n_100: 1.2899 (1.7559)  triple_100: 0.0000 (0.0553)  triple_80: 0.0000 (0.0294)  triple_60: 0.0000 (0.0273)  triple_40: 0.0000 (0.0452)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 580/1724]  eta: 1:14:42  lr: 0.000100  loss: 4.8979 (7.6536)  loss_n_40: 1.2503 (1.8875)  loss_n_60: 1.3654 (2.0682)  loss_n_80: 1.2277 (1.7938)  loss_n_100: 1.1380 (1.7494)  triple_100: 0.0000 (0.0544)  triple_80: 0.0000 (0.0290)  triple_60: 0.0000 (0.0268)  triple_40: 0.0000 (0.0444)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 590/1724]  eta: 1:14:03  lr: 0.000100  loss: 4.8288 (7.6118)  loss_n_40: 1.2356 (1.8777)  loss_n_60: 1.2893 (2.0572)  loss_n_80: 1.2047 (1.7847)  loss_n_100: 1.0937 (1.7396)  triple_100: 0.0000 (0.0535)  triple_80: 0.0000 (0.0285)  triple_60: 0.0000 (0.0264)  triple_40: 0.0000 (0.0442)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 600/1724]  eta: 1:13:24  lr: 0.000100  loss: 4.9988 (7.5952)  loss_n_40: 1.2356 (1.8707)  loss_n_60: 1.2731 (2.0486)  loss_n_80: 1.1906 (1.7776)  loss_n_100: 1.1152 (1.7320)  triple_100: 0.0000 (0.0547)  triple_80: 0.0000 (0.0281)  triple_60: 0.0000 (0.0380)  triple_40: 0.0000 (0.0455)  time: 3.9158  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 610/1724]  eta: 1:12:44  lr: 0.000100  loss: 6.3343 (7.5912)  loss_n_40: 1.4291 (1.8703)  loss_n_60: 1.6287 (2.0464)  loss_n_80: 1.3085 (1.7765)  loss_n_100: 1.2557 (1.7320)  triple_100: 0.0000 (0.0539)  triple_80: 0.0000 (0.0276)  triple_60: 0.0000 (0.0374)  triple_40: 0.0000 (0.0471)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 620/1724]  eta: 1:12:05  lr: 0.000100  loss: 5.6458 (7.5560)  loss_n_40: 1.4539 (1.8629)  loss_n_60: 1.4898 (2.0370)  loss_n_80: 1.3605 (1.7695)  loss_n_100: 1.1869 (1.7233)  triple_100: 0.0000 (0.0530)  triple_80: 0.0000 (0.0272)  triple_60: 0.0000 (0.0368)  triple_40: 0.0000 (0.0464)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 630/1724]  eta: 1:11:26  lr: 0.000100  loss: 5.3206 (7.5425)  loss_n_40: 1.4349 (1.8618)  loss_n_60: 1.4878 (2.0326)  loss_n_80: 1.2875 (1.7658)  loss_n_100: 1.1892 (1.7212)  triple_100: 0.0000 (0.0521)  triple_80: 0.0000 (0.0267)  triple_60: 0.0000 (0.0362)  triple_40: 0.0000 (0.0460)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 640/1724]  eta: 1:10:47  lr: 0.000100  loss: 7.2447 (7.5545)  loss_n_40: 2.0041 (1.8639)  loss_n_60: 1.9108 (2.0341)  loss_n_80: 1.5788 (1.7665)  loss_n_100: 1.5377 (1.7220)  triple_100: 0.0000 (0.0553)  triple_80: 0.0000 (0.0284)  triple_60: 0.0000 (0.0370)  triple_40: 0.0000 (0.0474)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 650/1724]  eta: 1:10:08  lr: 0.000100  loss: 7.5415 (7.5581)  loss_n_40: 2.0041 (1.8655)  loss_n_60: 2.0169 (2.0352)  loss_n_80: 1.7872 (1.7680)  loss_n_100: 1.7202 (1.7240)  triple_100: 0.0000 (0.0544)  triple_80: 0.0000 (0.0279)  triple_60: 0.0000 (0.0364)  triple_40: 0.0000 (0.0467)  time: 3.9153  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 660/1724]  eta: 1:09:28  lr: 0.000100  loss: 6.8287 (7.5437)  loss_n_40: 1.6919 (1.8626)  loss_n_60: 1.8611 (2.0312)  loss_n_80: 1.6024 (1.7643)  loss_n_100: 1.6219 (1.7210)  triple_100: 0.0000 (0.0536)  triple_80: 0.0000 (0.0282)  triple_60: 0.0000 (0.0359)  triple_40: 0.0000 (0.0470)  time: 3.9155  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 670/1724]  eta: 1:08:49  lr: 0.000100  loss: 6.2013 (7.5169)  loss_n_40: 1.4660 (1.8567)  loss_n_60: 1.6274 (2.0247)  loss_n_80: 1.4072 (1.7579)  loss_n_100: 1.3051 (1.7153)  triple_100: 0.0000 (0.0528)  triple_80: 0.0000 (0.0278)  triple_60: 0.0000 (0.0353)  triple_40: 0.0000 (0.0463)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 680/1724]  eta: 1:08:10  lr: 0.000100  loss: 6.2013 (7.5115)  loss_n_40: 1.4660 (1.8574)  loss_n_60: 1.6616 (2.0232)  loss_n_80: 1.4716 (1.7561)  loss_n_100: 1.4548 (1.7141)  triple_100: 0.0000 (0.0522)  triple_80: 0.0000 (0.0275)  triple_60: 0.0000 (0.0354)  triple_40: 0.0000 (0.0457)  time: 3.9168  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [ 690/1724]  eta: 1:07:31  lr: 0.000100  loss: 6.4484 (7.4922)  loss_n_40: 1.7300 (1.8541)  loss_n_60: 1.7732 (2.0173)  loss_n_80: 1.5492 (1.7516)  loss_n_100: 1.4924 (1.7094)  triple_100: 0.0000 (0.0517)  triple_80: 0.0000 (0.0279)  triple_60: 0.0000 (0.0352)  triple_40: 0.0000 (0.0450)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 700/1724]  eta: 1:06:51  lr: 0.000100  loss: 5.7580 (7.4683)  loss_n_40: 1.3882 (1.8488)  loss_n_60: 1.6063 (2.0118)  loss_n_80: 1.3215 (1.7457)  loss_n_100: 1.3252 (1.7036)  triple_100: 0.0000 (0.0512)  triple_80: 0.0000 (0.0281)  triple_60: 0.0000 (0.0347)  triple_40: 0.0000 (0.0444)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 710/1724]  eta: 1:06:12  lr: 0.000100  loss: 5.6910 (7.4553)  loss_n_40: 1.4004 (1.8460)  loss_n_60: 1.6479 (2.0091)  loss_n_80: 1.3215 (1.7433)  loss_n_100: 1.2664 (1.7007)  triple_100: 0.0000 (0.0505)  triple_80: 0.0000 (0.0277)  triple_60: 0.0000 (0.0342)  triple_40: 0.0000 (0.0437)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 720/1724]  eta: 1:05:33  lr: 0.000100  loss: 5.6910 (7.4379)  loss_n_40: 1.3964 (1.8405)  loss_n_60: 1.6243 (2.0031)  loss_n_80: 1.3604 (1.7382)  loss_n_100: 1.2608 (1.6954)  triple_100: 0.0000 (0.0509)  triple_80: 0.0000 (0.0280)  triple_60: 0.0000 (0.0376)  triple_40: 0.0000 (0.0442)  time: 3.9179  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:6]  [ 730/1724]  eta: 1:04:54  lr: 0.000100  loss: 6.6313 (7.4634)  loss_n_40: 1.4778 (1.8418)  loss_n_60: 1.6747 (2.0065)  loss_n_80: 1.5765 (1.7431)  loss_n_100: 1.5048 (1.7014)  triple_100: 0.0000 (0.0585)  triple_80: 0.0000 (0.0297)  triple_60: 0.0000 (0.0377)  triple_40: 0.0000 (0.0447)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 740/1724]  eta: 1:04:15  lr: 0.000100  loss: 7.6820 (7.4676)  loss_n_40: 1.6664 (1.8436)  loss_n_60: 1.9974 (2.0091)  loss_n_80: 1.7166 (1.7445)  loss_n_100: 1.6363 (1.7020)  triple_100: 0.0000 (0.0577)  triple_80: 0.0000 (0.0293)  triple_60: 0.0000 (0.0372)  triple_40: 0.0000 (0.0443)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 750/1724]  eta: 1:03:36  lr: 0.000100  loss: 8.7765 (7.4949)  loss_n_40: 1.8048 (1.8437)  loss_n_60: 2.1761 (2.0120)  loss_n_80: 2.0013 (1.7520)  loss_n_100: 1.8084 (1.7124)  triple_100: 0.0000 (0.0591)  triple_80: 0.0000 (0.0293)  triple_60: 0.0000 (0.0388)  triple_40: 0.0000 (0.0476)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 760/1724]  eta: 1:02:56  lr: 0.000100  loss: 9.3330 (7.5185)  loss_n_40: 1.8555 (1.8438)  loss_n_60: 2.2697 (2.0159)  loss_n_80: 2.5043 (1.7606)  loss_n_100: 2.8322 (1.7253)  triple_100: 0.0000 (0.0586)  triple_80: 0.0000 (0.0289)  triple_60: 0.0000 (0.0383)  triple_40: 0.0000 (0.0470)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 770/1724]  eta: 1:02:17  lr: 0.000100  loss: 8.0295 (7.5170)  loss_n_40: 1.7129 (1.8416)  loss_n_60: 2.1444 (2.0158)  loss_n_80: 2.0952 (1.7617)  loss_n_100: 2.1290 (1.7272)  triple_100: 0.0000 (0.0578)  triple_80: 0.0000 (0.0285)  triple_60: 0.0000 (0.0378)  triple_40: 0.0000 (0.0464)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 780/1724]  eta: 1:01:38  lr: 0.000100  loss: 6.1110 (7.5026)  loss_n_40: 1.4164 (1.8379)  loss_n_60: 1.6691 (2.0122)  loss_n_80: 1.5356 (1.7587)  loss_n_100: 1.5001 (1.7241)  triple_100: 0.0000 (0.0571)  triple_80: 0.0000 (0.0282)  triple_60: 0.0000 (0.0387)  triple_40: 0.0000 (0.0458)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 790/1724]  eta: 1:00:59  lr: 0.000100  loss: 5.6741 (7.4830)  loss_n_40: 1.3278 (1.8329)  loss_n_60: 1.5519 (2.0071)  loss_n_80: 1.3182 (1.7540)  loss_n_100: 1.3760 (1.7201)  triple_100: 0.0000 (0.0570)  triple_80: 0.0000 (0.0278)  triple_60: 0.0000 (0.0388)  triple_40: 0.0000 (0.0452)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 800/1724]  eta: 1:00:20  lr: 0.000100  loss: 5.3476 (7.4590)  loss_n_40: 1.2914 (1.8270)  loss_n_60: 1.4795 (2.0008)  loss_n_80: 1.2771 (1.7484)  loss_n_100: 1.3142 (1.7155)  triple_100: 0.0000 (0.0563)  triple_80: 0.0000 (0.0275)  triple_60: 0.0000 (0.0383)  triple_40: 0.0000 (0.0452)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 810/1724]  eta: 0:59:40  lr: 0.000100  loss: 5.1924 (7.4346)  loss_n_40: 1.2112 (1.8216)  loss_n_60: 1.4531 (1.9951)  loss_n_80: 1.2809 (1.7423)  loss_n_100: 1.2915 (1.7104)  triple_100: 0.0000 (0.0556)  triple_80: 0.0000 (0.0272)  triple_60: 0.0000 (0.0379)  triple_40: 0.0000 (0.0446)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 820/1724]  eta: 0:59:01  lr: 0.000100  loss: 5.1775 (7.4155)  loss_n_40: 1.1985 (1.8165)  loss_n_60: 1.4531 (1.9898)  loss_n_80: 1.2623 (1.7368)  loss_n_100: 1.2434 (1.7061)  triple_100: 0.0000 (0.0580)  triple_80: 0.0000 (0.0269)  triple_60: 0.0000 (0.0374)  triple_40: 0.0000 (0.0441)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 830/1724]  eta: 0:58:22  lr: 0.000100  loss: 5.8807 (7.3953)  loss_n_40: 1.2067 (1.8121)  loss_n_60: 1.4818 (1.9845)  loss_n_80: 1.2623 (1.7318)  loss_n_100: 1.2360 (1.7001)  triple_100: 0.0000 (0.0573)  triple_80: 0.0000 (0.0286)  triple_60: 0.0000 (0.0370)  triple_40: 0.0000 (0.0439)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 840/1724]  eta: 0:57:43  lr: 0.000100  loss: 6.4327 (7.4072)  loss_n_40: 1.3833 (1.8115)  loss_n_60: 1.7412 (1.9846)  loss_n_80: 1.4810 (1.7321)  loss_n_100: 1.3839 (1.7017)  triple_100: 0.0000 (0.0580)  triple_80: 0.0000 (0.0292)  triple_60: 0.0000 (0.0425)  triple_40: 0.0000 (0.0475)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 850/1724]  eta: 0:57:04  lr: 0.000100  loss: 6.6037 (7.3986)  loss_n_40: 1.3916 (1.8081)  loss_n_60: 1.7993 (1.9825)  loss_n_80: 1.5612 (1.7299)  loss_n_100: 1.6626 (1.7021)  triple_100: 0.0000 (0.0578)  triple_80: 0.0000 (0.0289)  triple_60: 0.0000 (0.0420)  triple_40: 0.0000 (0.0472)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 860/1724]  eta: 0:56:25  lr: 0.000100  loss: 5.6767 (7.3783)  loss_n_40: 1.2178 (1.8029)  loss_n_60: 1.5181 (1.9768)  loss_n_80: 1.3511 (1.7250)  loss_n_100: 1.4782 (1.6981)  triple_100: 0.0000 (0.0571)  triple_80: 0.0000 (0.0286)  triple_60: 0.0000 (0.0415)  triple_40: 0.0000 (0.0483)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 870/1724]  eta: 0:55:45  lr: 0.000100  loss: 4.7111 (7.3538)  loss_n_40: 1.1757 (1.7971)  loss_n_60: 1.2857 (1.9705)  loss_n_80: 1.1531 (1.7198)  loss_n_100: 1.1072 (1.6929)  triple_100: 0.0000 (0.0565)  triple_80: 0.0000 (0.0282)  triple_60: 0.0000 (0.0410)  triple_40: 0.0000 (0.0478)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 880/1724]  eta: 0:55:06  lr: 0.000100  loss: 4.6933 (7.3305)  loss_n_40: 1.1441 (1.7919)  loss_n_60: 1.2827 (1.9644)  loss_n_80: 1.1437 (1.7141)  loss_n_100: 1.1454 (1.6881)  triple_100: 0.0000 (0.0563)  triple_80: 0.0000 (0.0279)  triple_60: 0.0000 (0.0406)  triple_40: 0.0000 (0.0472)  time: 3.9170  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:6]  [ 890/1724]  eta: 0:54:27  lr: 0.000100  loss: 4.3830 (7.3031)  loss_n_40: 1.1038 (1.7856)  loss_n_60: 1.2057 (1.9568)  loss_n_80: 1.0185 (1.7063)  loss_n_100: 1.0581 (1.6809)  triple_100: 0.0000 (0.0575)  triple_80: 0.0000 (0.0279)  triple_60: 0.0000 (0.0411)  triple_40: 0.0000 (0.0470)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 900/1724]  eta: 0:53:48  lr: 0.000100  loss: 4.3748 (7.2788)  loss_n_40: 1.1266 (1.7811)  loss_n_60: 1.2728 (1.9514)  loss_n_80: 1.0280 (1.7004)  loss_n_100: 0.9620 (1.6744)  triple_100: 0.0000 (0.0568)  triple_80: 0.0000 (0.0276)  triple_60: 0.0000 (0.0406)  triple_40: 0.0000 (0.0465)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 910/1724]  eta: 0:53:09  lr: 0.000100  loss: 4.4712 (7.2575)  loss_n_40: 1.1849 (1.7766)  loss_n_60: 1.3241 (1.9460)  loss_n_80: 1.0796 (1.6952)  loss_n_100: 0.9634 (1.6684)  triple_100: 0.0000 (0.0562)  triple_80: 0.0000 (0.0273)  triple_60: 0.0000 (0.0408)  triple_40: 0.0000 (0.0470)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 920/1724]  eta: 0:52:29  lr: 0.000100  loss: 4.6337 (7.2345)  loss_n_40: 1.1823 (1.7708)  loss_n_60: 1.2672 (1.9397)  loss_n_80: 1.1220 (1.6897)  loss_n_100: 1.0928 (1.6646)  triple_100: 0.0000 (0.0558)  triple_80: 0.0000 (0.0270)  triple_60: 0.0000 (0.0404)  triple_40: 0.0000 (0.0465)  time: 3.9172  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [ 930/1724]  eta: 0:51:50  lr: 0.000100  loss: 4.6199 (7.2155)  loss_n_40: 1.1011 (1.7672)  loss_n_60: 1.2480 (1.9344)  loss_n_80: 1.0983 (1.6850)  loss_n_100: 1.1532 (1.6609)  triple_100: 0.0000 (0.0552)  triple_80: 0.0000 (0.0269)  triple_60: 0.0000 (0.0400)  triple_40: 0.0000 (0.0460)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 940/1724]  eta: 0:51:11  lr: 0.000100  loss: 4.4467 (7.1922)  loss_n_40: 1.1058 (1.7623)  loss_n_60: 1.1445 (1.9275)  loss_n_80: 1.0585 (1.6797)  loss_n_100: 1.1681 (1.6558)  triple_100: 0.0000 (0.0552)  triple_80: 0.0000 (0.0266)  triple_60: 0.0000 (0.0395)  triple_40: 0.0000 (0.0455)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 950/1724]  eta: 0:50:32  lr: 0.000100  loss: 4.4467 (7.1653)  loss_n_40: 1.0117 (1.7564)  loss_n_60: 1.0873 (1.9199)  loss_n_80: 0.9727 (1.6736)  loss_n_100: 0.9445 (1.6496)  triple_100: 0.0000 (0.0546)  triple_80: 0.0000 (0.0263)  triple_60: 0.0000 (0.0391)  triple_40: 0.0000 (0.0458)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 960/1724]  eta: 0:49:53  lr: 0.000100  loss: 4.5796 (7.1428)  loss_n_40: 1.1139 (1.7517)  loss_n_60: 1.2702 (1.9140)  loss_n_80: 1.0546 (1.6683)  loss_n_100: 0.9942 (1.6443)  triple_100: 0.0000 (0.0541)  triple_80: 0.0000 (0.0260)  triple_60: 0.0000 (0.0387)  triple_40: 0.0000 (0.0457)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 970/1724]  eta: 0:49:14  lr: 0.000100  loss: 4.7955 (7.1195)  loss_n_40: 1.1878 (1.7470)  loss_n_60: 1.2940 (1.9081)  loss_n_80: 1.1190 (1.6624)  loss_n_100: 1.0760 (1.6386)  triple_100: 0.0000 (0.0541)  triple_80: 0.0000 (0.0258)  triple_60: 0.0000 (0.0383)  triple_40: 0.0000 (0.0452)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 980/1724]  eta: 0:48:34  lr: 0.000100  loss: 4.0999 (7.0943)  loss_n_40: 1.0856 (1.7410)  loss_n_60: 1.1095 (1.9006)  loss_n_80: 0.9465 (1.6557)  loss_n_100: 0.9584 (1.6320)  triple_100: 0.0000 (0.0536)  triple_80: 0.0000 (0.0265)  triple_60: 0.0000 (0.0390)  triple_40: 0.0000 (0.0460)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [ 990/1724]  eta: 0:47:55  lr: 0.000100  loss: 4.6414 (7.0797)  loss_n_40: 1.2156 (1.7380)  loss_n_60: 1.2507 (1.8959)  loss_n_80: 1.1116 (1.6523)  loss_n_100: 1.1182 (1.6286)  triple_100: 0.0000 (0.0530)  triple_80: 0.0000 (0.0262)  triple_60: 0.0000 (0.0396)  triple_40: 0.0000 (0.0461)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1000/1724]  eta: 0:47:16  lr: 0.000100  loss: 5.0247 (7.0611)  loss_n_40: 1.3603 (1.7348)  loss_n_60: 1.3523 (1.8909)  loss_n_80: 1.2363 (1.6480)  loss_n_100: 1.2160 (1.6240)  triple_100: 0.0000 (0.0525)  triple_80: 0.0000 (0.0259)  triple_60: 0.0000 (0.0394)  triple_40: 0.0000 (0.0457)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1010/1724]  eta: 0:46:37  lr: 0.000100  loss: 4.4681 (7.0402)  loss_n_40: 1.3548 (1.7312)  loss_n_60: 1.2572 (1.8850)  loss_n_80: 1.0852 (1.6431)  loss_n_100: 0.9878 (1.6188)  triple_100: 0.0000 (0.0520)  triple_80: 0.0000 (0.0257)  triple_60: 0.0000 (0.0390)  triple_40: 0.0000 (0.0455)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1020/1724]  eta: 0:45:58  lr: 0.000100  loss: 4.4186 (7.0238)  loss_n_40: 1.2148 (1.7286)  loss_n_60: 1.1861 (1.8804)  loss_n_80: 1.0209 (1.6393)  loss_n_100: 0.9829 (1.6147)  triple_100: 0.0000 (0.0516)  triple_80: 0.0000 (0.0254)  triple_60: 0.0000 (0.0386)  triple_40: 0.0000 (0.0451)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1030/1724]  eta: 0:45:19  lr: 0.000100  loss: 4.2019 (7.0069)  loss_n_40: 1.1560 (1.7260)  loss_n_60: 1.1644 (1.8754)  loss_n_80: 1.0209 (1.6356)  loss_n_100: 0.9415 (1.6108)  triple_100: 0.0000 (0.0511)  triple_80: 0.0000 (0.0252)  triple_60: 0.0000 (0.0382)  triple_40: 0.0000 (0.0446)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1040/1724]  eta: 0:44:39  lr: 0.000100  loss: 4.6210 (6.9866)  loss_n_40: 1.1560 (1.7220)  loss_n_60: 1.1655 (1.8693)  loss_n_80: 1.0236 (1.6309)  loss_n_100: 0.9415 (1.6055)  triple_100: 0.0000 (0.0510)  triple_80: 0.0000 (0.0256)  triple_60: 0.0000 (0.0378)  triple_40: 0.0000 (0.0444)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1050/1724]  eta: 0:44:00  lr: 0.000100  loss: 4.3482 (6.9633)  loss_n_40: 1.1101 (1.7168)  loss_n_60: 1.1655 (1.8633)  loss_n_80: 1.0041 (1.6253)  loss_n_100: 0.9165 (1.5996)  triple_100: 0.0000 (0.0506)  triple_80: 0.0000 (0.0253)  triple_60: 0.0000 (0.0377)  triple_40: 0.0000 (0.0446)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1060/1724]  eta: 0:43:21  lr: 0.000100  loss: 4.0177 (6.9411)  loss_n_40: 1.0429 (1.7122)  loss_n_60: 1.1419 (1.8574)  loss_n_80: 0.9629 (1.6204)  loss_n_100: 0.9088 (1.5943)  triple_100: 0.0000 (0.0501)  triple_80: 0.0000 (0.0251)  triple_60: 0.0000 (0.0374)  triple_40: 0.0000 (0.0442)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1070/1724]  eta: 0:42:42  lr: 0.000100  loss: 3.7724 (6.9166)  loss_n_40: 1.0165 (1.7072)  loss_n_60: 1.0319 (1.8507)  loss_n_80: 0.9273 (1.6148)  loss_n_100: 0.8709 (1.5882)  triple_100: 0.0000 (0.0497)  triple_80: 0.0000 (0.0249)  triple_60: 0.0000 (0.0370)  triple_40: 0.0000 (0.0440)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1080/1724]  eta: 0:42:03  lr: 0.000100  loss: 3.9514 (6.9002)  loss_n_40: 1.0364 (1.7041)  loss_n_60: 1.1097 (1.8460)  loss_n_80: 0.9379 (1.6111)  loss_n_100: 0.9102 (1.5838)  triple_100: 0.0000 (0.0499)  triple_80: 0.0000 (0.0246)  triple_60: 0.0000 (0.0367)  triple_40: 0.0000 (0.0439)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1090/1724]  eta: 0:41:23  lr: 0.000100  loss: 4.0716 (6.8753)  loss_n_40: 1.0982 (1.6984)  loss_n_60: 1.1097 (1.8394)  loss_n_80: 0.9592 (1.6055)  loss_n_100: 0.8458 (1.5774)  triple_100: 0.0000 (0.0495)  triple_80: 0.0000 (0.0244)  triple_60: 0.0000 (0.0372)  triple_40: 0.0000 (0.0435)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1100/1724]  eta: 0:40:44  lr: 0.000100  loss: 4.0868 (6.8572)  loss_n_40: 1.1029 (1.6960)  loss_n_60: 1.0952 (1.8349)  loss_n_80: 0.9592 (1.6007)  loss_n_100: 0.8691 (1.5722)  triple_100: 0.0000 (0.0492)  triple_80: 0.0000 (0.0242)  triple_60: 0.0000 (0.0369)  triple_40: 0.0000 (0.0431)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1110/1724]  eta: 0:40:05  lr: 0.000100  loss: 4.2305 (6.8353)  loss_n_40: 1.1289 (1.6920)  loss_n_60: 1.0863 (1.8290)  loss_n_80: 0.9491 (1.5956)  loss_n_100: 0.9103 (1.5665)  triple_100: 0.0000 (0.0487)  triple_80: 0.0000 (0.0240)  triple_60: 0.0000 (0.0366)  triple_40: 0.0000 (0.0428)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1120/1724]  eta: 0:39:26  lr: 0.000100  loss: 3.8161 (6.8107)  loss_n_40: 0.9976 (1.6868)  loss_n_60: 1.0101 (1.8224)  loss_n_80: 0.9205 (1.5901)  loss_n_100: 0.8879 (1.5606)  triple_100: 0.0000 (0.0483)  triple_80: 0.0000 (0.0238)  triple_60: 0.0000 (0.0362)  triple_40: 0.0000 (0.0425)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1130/1724]  eta: 0:38:47  lr: 0.000100  loss: 4.1510 (6.7908)  loss_n_40: 1.1254 (1.6834)  loss_n_60: 1.1088 (1.8171)  loss_n_80: 1.0043 (1.5853)  loss_n_100: 0.8980 (1.5552)  triple_100: 0.0000 (0.0479)  triple_80: 0.0000 (0.0236)  triple_60: 0.0000 (0.0359)  triple_40: 0.0000 (0.0425)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1140/1724]  eta: 0:38:08  lr: 0.000100  loss: 4.4731 (6.7732)  loss_n_40: 1.2856 (1.6810)  loss_n_60: 1.2698 (1.8126)  loss_n_80: 1.0377 (1.5806)  loss_n_100: 0.9098 (1.5501)  triple_100: 0.0000 (0.0475)  triple_80: 0.0000 (0.0233)  triple_60: 0.0000 (0.0359)  triple_40: 0.0000 (0.0422)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1150/1724]  eta: 0:37:28  lr: 0.000100  loss: 4.5492 (6.7589)  loss_n_40: 1.3033 (1.6791)  loss_n_60: 1.2425 (1.8086)  loss_n_80: 1.0386 (1.5772)  loss_n_100: 0.9423 (1.5464)  triple_100: 0.0000 (0.0471)  triple_80: 0.0000 (0.0231)  triple_60: 0.0000 (0.0356)  triple_40: 0.0000 (0.0419)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1160/1724]  eta: 0:36:49  lr: 0.000100  loss: 4.2871 (6.7392)  loss_n_40: 1.0747 (1.6750)  loss_n_60: 1.0714 (1.8029)  loss_n_80: 0.9475 (1.5725)  loss_n_100: 0.9558 (1.5414)  triple_100: 0.0000 (0.0467)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0353)  triple_40: 0.0000 (0.0423)  time: 3.9172  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [1170/1724]  eta: 0:36:10  lr: 0.000100  loss: 5.1870 (6.7519)  loss_n_40: 1.1907 (1.6737)  loss_n_60: 1.3683 (1.8017)  loss_n_80: 1.2408 (1.5747)  loss_n_100: 1.1599 (1.5442)  triple_100: 0.0000 (0.0477)  triple_80: 0.0000 (0.0247)  triple_60: 0.0000 (0.0417)  triple_40: 0.0000 (0.0434)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1180/1724]  eta: 0:35:31  lr: 0.000100  loss: 6.4114 (6.7498)  loss_n_40: 1.4723 (1.6731)  loss_n_60: 1.6276 (1.8007)  loss_n_80: 1.5683 (1.5745)  loss_n_100: 1.7481 (1.5453)  triple_100: 0.0000 (0.0473)  triple_80: 0.0000 (0.0245)  triple_60: 0.0000 (0.0413)  triple_40: 0.0000 (0.0430)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1190/1724]  eta: 0:34:52  lr: 0.000100  loss: 6.4114 (6.7553)  loss_n_40: 1.6331 (1.6742)  loss_n_60: 1.6931 (1.8004)  loss_n_80: 1.5679 (1.5751)  loss_n_100: 1.7040 (1.5472)  triple_100: 0.0000 (0.0472)  triple_80: 0.0000 (0.0244)  triple_60: 0.0000 (0.0424)  triple_40: 0.0000 (0.0443)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1200/1724]  eta: 0:34:12  lr: 0.000100  loss: 6.7110 (6.7529)  loss_n_40: 1.4087 (1.6718)  loss_n_60: 1.5455 (1.7975)  loss_n_80: 1.6613 (1.5755)  loss_n_100: 1.7040 (1.5478)  triple_100: 0.0000 (0.0478)  triple_80: 0.0000 (0.0249)  triple_60: 0.0000 (0.0436)  triple_40: 0.0000 (0.0440)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1210/1724]  eta: 0:33:33  lr: 0.000100  loss: 6.1369 (6.7818)  loss_n_40: 1.2643 (1.6693)  loss_n_60: 1.3564 (1.7935)  loss_n_80: 1.4205 (1.5745)  loss_n_100: 1.6359 (1.5496)  triple_100: 0.0000 (0.0647)  triple_80: 0.0000 (0.0374)  triple_60: 0.0000 (0.0453)  triple_40: 0.0000 (0.0474)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1220/1724]  eta: 0:32:54  lr: 0.000100  loss: 12.3133 (6.8503)  loss_n_40: 1.7284 (1.6811)  loss_n_60: 1.6458 (1.8096)  loss_n_80: 2.1232 (1.5931)  loss_n_100: 2.4214 (1.5697)  triple_100: 0.0000 (0.0676)  triple_80: 0.0000 (0.0372)  triple_60: 0.0000 (0.0450)  triple_40: 0.0000 (0.0470)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1230/1724]  eta: 0:32:15  lr: 0.000100  loss: 16.4270 (6.9429)  loss_n_40: 3.6171 (1.6997)  loss_n_60: 4.1752 (1.8311)  loss_n_80: 4.1338 (1.6151)  loss_n_100: 4.1431 (1.5925)  triple_100: 0.0000 (0.0690)  triple_80: 0.0000 (0.0409)  triple_60: 0.0000 (0.0446)  triple_40: 0.0000 (0.0499)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1240/1724]  eta: 0:31:36  lr: 0.000100  loss: 16.4540 (7.0142)  loss_n_40: 3.6171 (1.7136)  loss_n_60: 4.2042 (1.8491)  loss_n_80: 4.1795 (1.6357)  loss_n_100: 4.1027 (1.6117)  triple_100: 0.0000 (0.0692)  triple_80: 0.0000 (0.0406)  triple_60: 0.0000 (0.0449)  triple_40: 0.0000 (0.0495)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1250/1724]  eta: 0:30:57  lr: 0.000100  loss: 14.9297 (7.0689)  loss_n_40: 3.2086 (1.7238)  loss_n_60: 3.8508 (1.8617)  loss_n_80: 4.0133 (1.6532)  loss_n_100: 3.7779 (1.6273)  triple_100: 0.0000 (0.0686)  triple_80: 0.0000 (0.0403)  triple_60: 0.0000 (0.0445)  triple_40: 0.0000 (0.0496)  time: 3.9183  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:6]  [1260/1724]  eta: 0:30:17  lr: 0.000100  loss: 13.6608 (7.1271)  loss_n_40: 2.8431 (1.7319)  loss_n_60: 3.1999 (1.8713)  loss_n_80: 3.4949 (1.6682)  loss_n_100: 3.4687 (1.6415)  triple_100: 0.0000 (0.0717)  triple_80: 0.0000 (0.0446)  triple_60: 0.0000 (0.0481)  triple_40: 0.0000 (0.0498)  time: 3.9181  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:6]  [1270/1724]  eta: 0:29:38  lr: 0.000100  loss: 15.8501 (7.1982)  loss_n_40: 3.0649 (1.7443)  loss_n_60: 3.5628 (1.8896)  loss_n_80: 4.0737 (1.6889)  loss_n_100: 3.9301 (1.6612)  triple_100: 0.0000 (0.0711)  triple_80: 0.0000 (0.0457)  triple_60: 0.0000 (0.0479)  triple_40: 0.0000 (0.0494)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1280/1724]  eta: 0:28:59  lr: 0.000100  loss: 15.8501 (7.2651)  loss_n_40: 3.1994 (1.7551)  loss_n_60: 3.9489 (1.9041)  loss_n_80: 4.1218 (1.7068)  loss_n_100: 3.9229 (1.6768)  triple_100: 0.0000 (0.0706)  triple_80: 0.0000 (0.0476)  triple_60: 0.0000 (0.0551)  triple_40: 0.0000 (0.0490)  time: 3.9220  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:6]  [1290/1724]  eta: 0:28:20  lr: 0.000100  loss: 13.3407 (7.3035)  loss_n_40: 2.8818 (1.7619)  loss_n_60: 3.2522 (1.9109)  loss_n_80: 3.7563 (1.7200)  loss_n_100: 3.4314 (1.6892)  triple_100: 0.0000 (0.0700)  triple_80: 0.0000 (0.0473)  triple_60: 0.0000 (0.0556)  triple_40: 0.0000 (0.0487)  time: 3.9245  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:6]  [1300/1724]  eta: 0:27:41  lr: 0.000100  loss: 11.3011 (7.3307)  loss_n_40: 2.4721 (1.7661)  loss_n_60: 2.4377 (1.9141)  loss_n_80: 3.0615 (1.7297)  loss_n_100: 3.1485 (1.6998)  triple_100: 0.0000 (0.0695)  triple_80: 0.0000 (0.0477)  triple_60: 0.0000 (0.0551)  triple_40: 0.0000 (0.0488)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1310/1724]  eta: 0:27:02  lr: 0.000100  loss: 10.0022 (7.3464)  loss_n_40: 2.1940 (1.7695)  loss_n_60: 2.2079 (1.9158)  loss_n_80: 2.6742 (1.7351)  loss_n_100: 2.8464 (1.7066)  triple_100: 0.0000 (0.0689)  triple_80: 0.0000 (0.0473)  triple_60: 0.0000 (0.0547)  triple_40: 0.0000 (0.0484)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1320/1724]  eta: 0:26:22  lr: 0.000100  loss: 8.9204 (7.3549)  loss_n_40: 1.9756 (1.7705)  loss_n_60: 2.0263 (1.9159)  loss_n_80: 2.3274 (1.7388)  loss_n_100: 2.4600 (1.7120)  triple_100: 0.0000 (0.0684)  triple_80: 0.0000 (0.0469)  triple_60: 0.0000 (0.0543)  triple_40: 0.0000 (0.0480)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1330/1724]  eta: 0:25:43  lr: 0.000100  loss: 7.9781 (7.3584)  loss_n_40: 1.7557 (1.7699)  loss_n_60: 1.8096 (1.9149)  loss_n_80: 2.0974 (1.7410)  loss_n_100: 2.3233 (1.7161)  triple_100: 0.0000 (0.0680)  triple_80: 0.0000 (0.0466)  triple_60: 0.0000 (0.0542)  triple_40: 0.0000 (0.0477)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1340/1724]  eta: 0:25:04  lr: 0.000100  loss: 7.3828 (7.3562)  loss_n_40: 1.6079 (1.7686)  loss_n_60: 1.6880 (1.9127)  loss_n_80: 1.9223 (1.7415)  loss_n_100: 2.1474 (1.7185)  triple_100: 0.0000 (0.0675)  triple_80: 0.0000 (0.0462)  triple_60: 0.0000 (0.0538)  triple_40: 0.0000 (0.0473)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1350/1724]  eta: 0:24:25  lr: 0.000100  loss: 7.2434 (7.3846)  loss_n_40: 1.6430 (1.7706)  loss_n_60: 1.6768 (1.9153)  loss_n_80: 1.8223 (1.7479)  loss_n_100: 2.0284 (1.7258)  triple_100: 0.0000 (0.0685)  triple_80: 0.0000 (0.0508)  triple_60: 0.0000 (0.0577)  triple_40: 0.0000 (0.0480)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1360/1724]  eta: 0:23:46  lr: 0.000100  loss: 13.5711 (7.4374)  loss_n_40: 2.8776 (1.7803)  loss_n_60: 3.3896 (1.9281)  loss_n_80: 3.5517 (1.7635)  loss_n_100: 3.4167 (1.7404)  triple_100: 0.0000 (0.0680)  triple_80: 0.0000 (0.0504)  triple_60: 0.0000 (0.0574)  triple_40: 0.0000 (0.0493)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1370/1724]  eta: 0:23:07  lr: 0.000100  loss: 13.6238 (7.4785)  loss_n_40: 2.9883 (1.7876)  loss_n_60: 3.4218 (1.9379)  loss_n_80: 3.7447 (1.7772)  loss_n_100: 3.5336 (1.7524)  triple_100: 0.0000 (0.0675)  triple_80: 0.0000 (0.0501)  triple_60: 0.0000 (0.0570)  triple_40: 0.0000 (0.0489)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1380/1724]  eta: 0:22:27  lr: 0.000100  loss: 12.1579 (7.5090)  loss_n_40: 2.6831 (1.7933)  loss_n_60: 3.0439 (1.9447)  loss_n_80: 3.4243 (1.7873)  loss_n_100: 3.1762 (1.7613)  triple_100: 0.0000 (0.0670)  triple_80: 0.0000 (0.0500)  triple_60: 0.0000 (0.0566)  triple_40: 0.0000 (0.0488)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1390/1724]  eta: 0:21:48  lr: 0.000100  loss: 11.4017 (7.5338)  loss_n_40: 2.3828 (1.7963)  loss_n_60: 2.8384 (1.9506)  loss_n_80: 3.1693 (1.7968)  loss_n_100: 2.9878 (1.7692)  triple_100: 0.0000 (0.0665)  triple_80: 0.0000 (0.0497)  triple_60: 0.0000 (0.0562)  triple_40: 0.0000 (0.0484)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1400/1724]  eta: 0:21:09  lr: 0.000100  loss: 10.5743 (7.5541)  loss_n_40: 2.1475 (1.7991)  loss_n_60: 2.6237 (1.9550)  loss_n_80: 2.9784 (1.8045)  loss_n_100: 2.8544 (1.7762)  triple_100: 0.0000 (0.0660)  triple_80: 0.0000 (0.0493)  triple_60: 0.0000 (0.0558)  triple_40: 0.0000 (0.0481)  time: 3.9288  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [1410/1724]  eta: 0:20:30  lr: 0.000100  loss: 9.6671 (7.5674)  loss_n_40: 2.0041 (1.8003)  loss_n_60: 2.3753 (1.9576)  loss_n_80: 2.7049 (1.8104)  loss_n_100: 2.6405 (1.7815)  triple_100: 0.0000 (0.0656)  triple_80: 0.0000 (0.0490)  triple_60: 0.0000 (0.0554)  triple_40: 0.0000 (0.0477)  time: 3.9299  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1420/1724]  eta: 0:19:51  lr: 0.000100  loss: 9.3179 (7.5792)  loss_n_40: 1.9753 (1.8019)  loss_n_60: 2.2368 (1.9592)  loss_n_80: 2.5201 (1.8149)  loss_n_100: 2.5230 (1.7862)  triple_100: 0.0000 (0.0651)  triple_80: 0.0000 (0.0486)  triple_60: 0.0000 (0.0550)  triple_40: 0.0000 (0.0483)  time: 3.9292  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1430/1724]  eta: 0:19:12  lr: 0.000100  loss: 9.1335 (7.5898)  loss_n_40: 1.9206 (1.8023)  loss_n_60: 2.1612 (1.9606)  loss_n_80: 2.4536 (1.8196)  loss_n_100: 2.5119 (1.7917)  triple_100: 0.0000 (0.0647)  triple_80: 0.0000 (0.0483)  triple_60: 0.0000 (0.0546)  triple_40: 0.0000 (0.0480)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1440/1724]  eta: 0:18:33  lr: 0.000100  loss: 8.6942 (7.5963)  loss_n_40: 1.8054 (1.8024)  loss_n_60: 2.0796 (1.9608)  loss_n_80: 2.3503 (1.8226)  loss_n_100: 2.5156 (1.7961)  triple_100: 0.0000 (0.0642)  triple_80: 0.0000 (0.0483)  triple_60: 0.0000 (0.0542)  triple_40: 0.0000 (0.0477)  time: 3.9298  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1450/1724]  eta: 0:17:53  lr: 0.000100  loss: 8.1928 (7.5986)  loss_n_40: 1.7417 (1.8020)  loss_n_60: 1.9088 (1.9602)  loss_n_80: 2.1791 (1.8243)  loss_n_100: 2.3710 (1.7992)  triple_100: 0.0000 (0.0638)  triple_80: 0.0000 (0.0480)  triple_60: 0.0000 (0.0538)  triple_40: 0.0000 (0.0473)  time: 3.9292  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1460/1724]  eta: 0:17:14  lr: 0.000100  loss: 7.3811 (7.5952)  loss_n_40: 1.6788 (1.8015)  loss_n_60: 1.8206 (1.9586)  loss_n_80: 1.8758 (1.8239)  loss_n_100: 2.0156 (1.7997)  triple_100: 0.0000 (0.0633)  triple_80: 0.0000 (0.0476)  triple_60: 0.0000 (0.0535)  triple_40: 0.0000 (0.0470)  time: 3.9283  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1470/1724]  eta: 0:16:35  lr: 0.000100  loss: 7.1468 (7.5918)  loss_n_40: 1.5268 (1.7994)  loss_n_60: 1.6660 (1.9562)  loss_n_80: 1.8010 (1.8243)  loss_n_100: 1.9590 (1.8019)  triple_100: 0.0000 (0.0629)  triple_80: 0.0000 (0.0473)  triple_60: 0.0000 (0.0531)  triple_40: 0.0000 (0.0467)  time: 3.9295  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1480/1724]  eta: 0:15:56  lr: 0.000100  loss: 6.8773 (7.5847)  loss_n_40: 1.4319 (1.7971)  loss_n_60: 1.5855 (1.9533)  loss_n_80: 1.7867 (1.8235)  loss_n_100: 1.9977 (1.8023)  triple_100: 0.0000 (0.0625)  triple_80: 0.0000 (0.0470)  triple_60: 0.0000 (0.0527)  triple_40: 0.0000 (0.0464)  time: 3.9312  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:6]  [1490/1724]  eta: 0:15:17  lr: 0.000100  loss: 6.0099 (7.5733)  loss_n_40: 1.3038 (1.7936)  loss_n_60: 1.4031 (1.9491)  loss_n_80: 1.5738 (1.8217)  loss_n_100: 1.7668 (1.8017)  triple_100: 0.0000 (0.0621)  triple_80: 0.0000 (0.0467)  triple_60: 0.0000 (0.0524)  triple_40: 0.0000 (0.0461)  time: 3.9318  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:6]  [1500/1724]  eta: 0:14:37  lr: 0.000100  loss: 5.8290 (7.5623)  loss_n_40: 1.2262 (1.7900)  loss_n_60: 1.3152 (1.9448)  loss_n_80: 1.5330 (1.8197)  loss_n_100: 1.7284 (1.8009)  triple_100: 0.0000 (0.0623)  triple_80: 0.0000 (0.0464)  triple_60: 0.0000 (0.0525)  triple_40: 0.0000 (0.0458)  time: 3.9311  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1510/1724]  eta: 0:13:58  lr: 0.000100  loss: 5.8729 (7.5527)  loss_n_40: 1.2464 (1.7877)  loss_n_60: 1.3567 (1.9414)  loss_n_80: 1.5227 (1.8179)  loss_n_100: 1.6157 (1.7999)  triple_100: 0.0000 (0.0619)  triple_80: 0.0000 (0.0461)  triple_60: 0.0000 (0.0521)  triple_40: 0.0000 (0.0458)  time: 3.9302  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1520/1724]  eta: 0:13:19  lr: 0.000100  loss: 5.7779 (7.5396)  loss_n_40: 1.2256 (1.7843)  loss_n_60: 1.2969 (1.9367)  loss_n_80: 1.4954 (1.8155)  loss_n_100: 1.6157 (1.7986)  triple_100: 0.0000 (0.0615)  triple_80: 0.0000 (0.0458)  triple_60: 0.0000 (0.0518)  triple_40: 0.0000 (0.0455)  time: 3.9307  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1530/1724]  eta: 0:12:40  lr: 0.000100  loss: 5.4819 (7.5260)  loss_n_40: 1.2174 (1.7809)  loss_n_60: 1.2319 (1.9324)  loss_n_80: 1.4609 (1.8129)  loss_n_100: 1.5831 (1.7967)  triple_100: 0.0000 (0.0611)  triple_80: 0.0000 (0.0455)  triple_60: 0.0000 (0.0515)  triple_40: 0.0000 (0.0452)  time: 3.9312  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1540/1724]  eta: 0:12:01  lr: 0.000100  loss: 5.1309 (7.5094)  loss_n_40: 1.1360 (1.7765)  loss_n_60: 1.1864 (1.9272)  loss_n_80: 1.3589 (1.8096)  loss_n_100: 1.4234 (1.7943)  triple_100: 0.0000 (0.0607)  triple_80: 0.0000 (0.0452)  triple_60: 0.0000 (0.0511)  triple_40: 0.0000 (0.0449)  time: 3.9312  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1550/1724]  eta: 0:11:22  lr: 0.000100  loss: 5.0775 (7.4959)  loss_n_40: 1.0966 (1.7725)  loss_n_60: 1.1450 (1.9226)  loss_n_80: 1.3342 (1.8068)  loss_n_100: 1.4544 (1.7924)  triple_100: 0.0000 (0.0603)  triple_80: 0.0000 (0.0449)  triple_60: 0.0000 (0.0508)  triple_40: 0.0000 (0.0456)  time: 3.9311  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1560/1724]  eta: 0:10:42  lr: 0.000100  loss: 5.0135 (7.4805)  loss_n_40: 1.0961 (1.7685)  loss_n_60: 1.1493 (1.9176)  loss_n_80: 1.3322 (1.8035)  loss_n_100: 1.4743 (1.7900)  triple_100: 0.0000 (0.0599)  triple_80: 0.0000 (0.0446)  triple_60: 0.0000 (0.0505)  triple_40: 0.0000 (0.0459)  time: 3.9314  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1570/1724]  eta: 0:10:03  lr: 0.000100  loss: 4.9583 (7.4647)  loss_n_40: 1.1084 (1.7647)  loss_n_60: 1.1211 (1.9128)  loss_n_80: 1.2891 (1.8002)  loss_n_100: 1.4034 (1.7874)  triple_100: 0.0000 (0.0595)  triple_80: 0.0000 (0.0443)  triple_60: 0.0000 (0.0501)  triple_40: 0.0000 (0.0456)  time: 3.9310  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1580/1724]  eta: 0:09:24  lr: 0.000100  loss: 4.9466 (7.4510)  loss_n_40: 1.1070 (1.7614)  loss_n_60: 1.1323 (1.9082)  loss_n_80: 1.2631 (1.7968)  loss_n_100: 1.3359 (1.7846)  triple_100: 0.0000 (0.0599)  triple_80: 0.0000 (0.0440)  triple_60: 0.0000 (0.0504)  triple_40: 0.0000 (0.0457)  time: 3.9303  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1590/1724]  eta: 0:08:45  lr: 0.000100  loss: 4.9466 (7.4353)  loss_n_40: 1.0770 (1.7576)  loss_n_60: 1.1323 (1.9034)  loss_n_80: 1.2478 (1.7935)  loss_n_100: 1.3359 (1.7820)  triple_100: 0.0000 (0.0595)  triple_80: 0.0000 (0.0438)  triple_60: 0.0000 (0.0501)  triple_40: 0.0000 (0.0454)  time: 3.9304  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1600/1724]  eta: 0:08:06  lr: 0.000100  loss: 4.5694 (7.4163)  loss_n_40: 0.9719 (1.7528)  loss_n_60: 1.0592 (1.8978)  loss_n_80: 1.2252 (1.7894)  loss_n_100: 1.3144 (1.7787)  triple_100: 0.0000 (0.0591)  triple_80: 0.0000 (0.0435)  triple_60: 0.0000 (0.0498)  triple_40: 0.0000 (0.0452)  time: 3.9304  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1610/1724]  eta: 0:07:26  lr: 0.000100  loss: 4.3549 (7.3984)  loss_n_40: 0.9586 (1.7486)  loss_n_60: 0.9964 (1.8925)  loss_n_80: 1.1163 (1.7854)  loss_n_100: 1.2238 (1.7754)  triple_100: 0.0000 (0.0588)  triple_80: 0.0000 (0.0432)  triple_60: 0.0000 (0.0497)  triple_40: 0.0000 (0.0449)  time: 3.9305  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:6]  [1620/1724]  eta: 0:06:47  lr: 0.000100  loss: 4.5624 (7.3828)  loss_n_40: 1.0546 (1.7451)  loss_n_60: 1.0679 (1.8879)  loss_n_80: 1.1468 (1.7818)  loss_n_100: 1.2742 (1.7726)  triple_100: 0.0000 (0.0584)  triple_80: 0.0000 (0.0430)  triple_60: 0.0000 (0.0494)  triple_40: 0.0000 (0.0446)  time: 3.9309  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1630/1724]  eta: 0:06:08  lr: 0.000100  loss: 4.5768 (7.3647)  loss_n_40: 1.0322 (1.7407)  loss_n_60: 1.0783 (1.8828)  loss_n_80: 1.1241 (1.7776)  loss_n_100: 1.2742 (1.7694)  triple_100: 0.0000 (0.0581)  triple_80: 0.0000 (0.0427)  triple_60: 0.0000 (0.0491)  triple_40: 0.0000 (0.0443)  time: 3.9317  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1640/1724]  eta: 0:05:29  lr: 0.000100  loss: 4.3556 (7.3455)  loss_n_40: 0.9939 (1.7362)  loss_n_60: 1.0130 (1.8774)  loss_n_80: 1.0803 (1.7732)  loss_n_100: 1.2301 (1.7658)  triple_100: 0.0000 (0.0577)  triple_80: 0.0000 (0.0424)  triple_60: 0.0000 (0.0488)  triple_40: 0.0000 (0.0441)  time: 3.9320  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [1650/1724]  eta: 0:04:50  lr: 0.000100  loss: 4.2708 (7.3272)  loss_n_40: 0.9830 (1.7321)  loss_n_60: 1.0057 (1.8722)  loss_n_80: 1.0489 (1.7688)  loss_n_100: 1.2073 (1.7623)  triple_100: 0.0000 (0.0574)  triple_80: 0.0000 (0.0422)  triple_60: 0.0000 (0.0485)  triple_40: 0.0000 (0.0438)  time: 3.9302  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1660/1724]  eta: 0:04:10  lr: 0.000100  loss: 4.2517 (7.3092)  loss_n_40: 0.9477 (1.7279)  loss_n_60: 1.0137 (1.8672)  loss_n_80: 1.0589 (1.7646)  loss_n_100: 1.2073 (1.7588)  triple_100: 0.0000 (0.0570)  triple_80: 0.0000 (0.0419)  triple_60: 0.0000 (0.0482)  triple_40: 0.0000 (0.0435)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1670/1724]  eta: 0:03:31  lr: 0.000100  loss: 4.1508 (7.2898)  loss_n_40: 0.9387 (1.7232)  loss_n_60: 0.9652 (1.8617)  loss_n_80: 1.0157 (1.7599)  loss_n_100: 1.1447 (1.7547)  triple_100: 0.0000 (0.0572)  triple_80: 0.0000 (0.0417)  triple_60: 0.0000 (0.0481)  triple_40: 0.0000 (0.0433)  time: 3.9301  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1680/1724]  eta: 0:02:52  lr: 0.000100  loss: 4.1534 (7.2757)  loss_n_40: 0.9622 (1.7195)  loss_n_60: 0.9610 (1.8572)  loss_n_80: 0.9859 (1.7563)  loss_n_100: 1.1391 (1.7519)  triple_100: 0.0000 (0.0572)  triple_80: 0.0000 (0.0419)  triple_60: 0.0000 (0.0478)  triple_40: 0.0000 (0.0438)  time: 3.9294  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1690/1724]  eta: 0:02:13  lr: 0.000100  loss: 4.8055 (7.2617)  loss_n_40: 0.9942 (1.7156)  loss_n_60: 1.1564 (1.8530)  loss_n_80: 1.1641 (1.7529)  loss_n_100: 1.3404 (1.7498)  triple_100: 0.0000 (0.0574)  triple_80: 0.0000 (0.0417)  triple_60: 0.0000 (0.0478)  triple_40: 0.0000 (0.0435)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1700/1724]  eta: 0:01:34  lr: 0.000100  loss: 4.6080 (7.2455)  loss_n_40: 1.0172 (1.7119)  loss_n_60: 1.0913 (1.8484)  loss_n_80: 1.1545 (1.7490)  loss_n_100: 1.3332 (1.7467)  triple_100: 0.0000 (0.0571)  triple_80: 0.0000 (0.0414)  triple_60: 0.0000 (0.0475)  triple_40: 0.0000 (0.0433)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6]  [1710/1724]  eta: 0:00:54  lr: 0.000100  loss: 4.2948 (7.2259)  loss_n_40: 0.9870 (1.7073)  loss_n_60: 1.0180 (1.8431)  loss_n_80: 1.0342 (1.7444)  loss_n_100: 1.1539 (1.7429)  triple_100: 0.0000 (0.0568)  triple_80: 0.0000 (0.0412)  triple_60: 0.0000 (0.0473)  triple_40: 0.0000 (0.0430)  time: 3.9287  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:6]  [1720/1724]  eta: 0:00:15  lr: 0.000100  loss: 4.1335 (7.2088)  loss_n_40: 0.9432 (1.7037)  loss_n_60: 1.0019 (1.8384)  loss_n_80: 1.0040 (1.7402)  loss_n_100: 1.1026 (1.7394)  triple_100: 0.0000 (0.0564)  triple_80: 0.0000 (0.0409)  triple_60: 0.0000 (0.0470)  triple_40: 0.0000 (0.0428)  time: 3.9295  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:6]  [1723/1724]  eta: 0:00:03  lr: 0.000100  loss: 4.1335 (7.2039)  loss_n_40: 0.9624 (1.7025)  loss_n_60: 1.0038 (1.8370)  loss_n_80: 1.0379 (1.7390)  loss_n_100: 1.1026 (1.7385)  triple_100: 0.0000 (0.0563)  triple_80: 0.0000 (0.0409)  triple_60: 0.0000 (0.0469)  triple_40: 0.0000 (0.0427)  time: 3.9300  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:6] Total time: 1:52:39 (3.9211 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 4.1335 (7.2039)  loss_n_40: 0.9624 (1.7025)  loss_n_60: 1.0038 (1.8370)  loss_n_80: 1.0379 (1.7390)  loss_n_100: 1.1026 (1.7385)  triple_100: 0.0000 (0.0563)  triple_80: 0.0000 (0.0409)  triple_60: 0.0000 (0.0469)  triple_40: 0.0000 (0.0427)\n",
      "Valid: [epoch:6]  [  0/845]  eta: 0:10:50  loss: 3.2264 (3.2264)  loss_n_40: 0.7693 (0.7693)  loss_n_60: 0.7735 (0.7735)  loss_n_80: 0.7919 (0.7919)  loss_n_100: 0.8918 (0.8918)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7698  data: 0.4317  max mem: 46473\n",
      "Valid: [epoch:6]  [ 10/845]  eta: 0:05:12  loss: 4.0081 (3.9288)  loss_n_40: 0.7925 (0.9207)  loss_n_60: 0.9254 (0.9311)  loss_n_80: 1.0086 (0.9553)  loss_n_100: 1.1500 (1.1218)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3745  data: 0.0394  max mem: 46473\n",
      "Valid: [epoch:6]  [ 20/845]  eta: 0:04:53  loss: 3.7850 (4.1189)  loss_n_40: 0.7925 (1.0430)  loss_n_60: 0.8621 (0.9894)  loss_n_80: 0.9093 (0.9846)  loss_n_100: 1.1040 (1.1018)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [ 30/845]  eta: 0:04:44  loss: 3.5861 (4.0090)  loss_n_40: 0.7869 (0.9952)  loss_n_60: 0.8923 (0.9634)  loss_n_80: 0.8770 (0.9665)  loss_n_100: 1.0478 (1.0840)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [ 40/845]  eta: 0:04:38  loss: 3.7803 (3.9602)  loss_n_40: 0.7752 (0.9604)  loss_n_60: 0.9316 (0.9542)  loss_n_80: 0.9727 (0.9611)  loss_n_100: 1.0152 (1.0845)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [ 50/845]  eta: 0:04:32  loss: 3.9469 (4.0882)  loss_n_40: 0.7671 (1.0321)  loss_n_60: 0.9710 (0.9810)  loss_n_80: 0.9298 (0.9630)  loss_n_100: 1.0373 (1.0742)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0379)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [ 60/845]  eta: 0:04:28  loss: 3.3472 (3.9902)  loss_n_40: 0.7671 (0.9942)  loss_n_60: 0.8262 (0.9563)  loss_n_80: 0.8160 (0.9458)  loss_n_100: 0.9499 (1.0621)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0317)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [ 70/845]  eta: 0:04:24  loss: 3.1880 (3.9147)  loss_n_40: 0.7342 (0.9649)  loss_n_60: 0.7561 (0.9361)  loss_n_80: 0.8092 (0.9313)  loss_n_100: 0.9400 (1.0552)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0272)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [ 80/845]  eta: 0:04:20  loss: 3.3543 (3.8749)  loss_n_40: 0.7585 (0.9513)  loss_n_60: 0.7888 (0.9244)  loss_n_80: 0.8423 (0.9235)  loss_n_100: 0.9955 (1.0518)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0239)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [ 90/845]  eta: 0:04:16  loss: 3.5758 (3.9260)  loss_n_40: 0.7995 (0.9763)  loss_n_60: 0.8745 (0.9363)  loss_n_80: 0.8698 (0.9331)  loss_n_100: 1.0352 (1.0591)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0212)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [100/845]  eta: 0:04:12  loss: 3.7293 (3.8877)  loss_n_40: 0.7817 (0.9586)  loss_n_60: 0.8752 (0.9275)  loss_n_80: 0.9225 (0.9280)  loss_n_100: 1.0430 (1.0544)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0191)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [110/845]  eta: 0:04:09  loss: 3.3995 (3.9173)  loss_n_40: 0.7796 (0.9730)  loss_n_60: 0.8426 (0.9363)  loss_n_80: 0.8872 (0.9341)  loss_n_100: 1.0011 (1.0564)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0174)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [120/845]  eta: 0:04:05  loss: 3.9463 (3.9418)  loss_n_40: 0.7880 (0.9867)  loss_n_60: 0.9026 (0.9417)  loss_n_80: 0.9268 (0.9367)  loss_n_100: 1.0806 (1.0608)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0160)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [130/845]  eta: 0:04:01  loss: 3.4849 (3.9200)  loss_n_40: 0.7412 (0.9739)  loss_n_60: 0.8613 (0.9360)  loss_n_80: 0.8744 (0.9345)  loss_n_100: 1.0806 (1.0609)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0148)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [140/845]  eta: 0:03:58  loss: 3.5816 (3.9280)  loss_n_40: 0.7721 (0.9717)  loss_n_60: 0.8227 (0.9373)  loss_n_80: 0.8791 (0.9382)  loss_n_100: 1.1258 (1.0670)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0137)  time: 0.3350  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:6]  [150/845]  eta: 0:03:54  loss: 4.1119 (3.9397)  loss_n_40: 0.8806 (0.9768)  loss_n_60: 0.9130 (0.9399)  loss_n_80: 0.9572 (0.9396)  loss_n_100: 1.1258 (1.0706)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0128)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [160/845]  eta: 0:03:51  loss: 3.7045 (3.9457)  loss_n_40: 0.7907 (0.9834)  loss_n_60: 0.9130 (0.9424)  loss_n_80: 0.9447 (0.9395)  loss_n_100: 1.0250 (1.0684)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0120)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [170/845]  eta: 0:03:47  loss: 3.7045 (3.9781)  loss_n_40: 0.7907 (0.9994)  loss_n_60: 0.8491 (0.9519)  loss_n_80: 0.9067 (0.9440)  loss_n_100: 1.0250 (1.0714)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0113)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [180/845]  eta: 0:03:44  loss: 3.5762 (3.9781)  loss_n_40: 0.8034 (0.9996)  loss_n_60: 0.8491 (0.9524)  loss_n_80: 0.8974 (0.9447)  loss_n_100: 1.0262 (1.0708)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0107)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [190/845]  eta: 0:03:40  loss: 3.4562 (3.9881)  loss_n_40: 0.8067 (1.0051)  loss_n_60: 0.8666 (0.9550)  loss_n_80: 0.8835 (0.9450)  loss_n_100: 0.9752 (1.0678)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0153)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [200/845]  eta: 0:03:37  loss: 3.6938 (3.9881)  loss_n_40: 0.8169 (1.0034)  loss_n_60: 0.8841 (0.9533)  loss_n_80: 0.9161 (0.9451)  loss_n_100: 0.9954 (1.0717)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0145)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [210/845]  eta: 0:03:33  loss: 3.6767 (3.9947)  loss_n_40: 0.8129 (1.0079)  loss_n_60: 0.9412 (0.9562)  loss_n_80: 0.8880 (0.9453)  loss_n_100: 1.0819 (1.0715)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0138)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [220/845]  eta: 0:03:30  loss: 3.5626 (3.9785)  loss_n_40: 0.7764 (0.9987)  loss_n_60: 0.8596 (0.9520)  loss_n_80: 0.8804 (0.9435)  loss_n_100: 1.0160 (1.0711)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0132)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [230/845]  eta: 0:03:27  loss: 3.5712 (3.9718)  loss_n_40: 0.7846 (0.9946)  loss_n_60: 0.8596 (0.9494)  loss_n_80: 0.9244 (0.9425)  loss_n_100: 1.0160 (1.0727)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0126)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [240/845]  eta: 0:03:23  loss: 3.9237 (3.9685)  loss_n_40: 0.8245 (0.9916)  loss_n_60: 0.9261 (0.9472)  loss_n_80: 0.9454 (0.9423)  loss_n_100: 1.1516 (1.0752)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0121)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [250/845]  eta: 0:03:20  loss: 3.7976 (3.9661)  loss_n_40: 0.8354 (0.9938)  loss_n_60: 0.8950 (0.9467)  loss_n_80: 0.9645 (0.9413)  loss_n_100: 1.0483 (1.0726)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0116)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [260/845]  eta: 0:03:16  loss: 3.7976 (3.9714)  loss_n_40: 0.7714 (0.9956)  loss_n_60: 0.9509 (0.9486)  loss_n_80: 0.9704 (0.9427)  loss_n_100: 1.0422 (1.0734)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0112)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [270/845]  eta: 0:03:13  loss: 3.9684 (3.9721)  loss_n_40: 0.8570 (0.9923)  loss_n_60: 0.9605 (0.9486)  loss_n_80: 1.0162 (0.9440)  loss_n_100: 1.1382 (1.0764)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0108)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [280/845]  eta: 0:03:10  loss: 4.0805 (3.9747)  loss_n_40: 0.8570 (0.9895)  loss_n_60: 0.9617 (0.9497)  loss_n_80: 1.0213 (0.9457)  loss_n_100: 1.0498 (1.0794)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0104)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [290/845]  eta: 0:03:06  loss: 3.6161 (3.9668)  loss_n_40: 0.8031 (0.9848)  loss_n_60: 0.8720 (0.9474)  loss_n_80: 0.9313 (0.9448)  loss_n_100: 1.0661 (1.0798)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0100)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [300/845]  eta: 0:03:03  loss: 3.8579 (3.9764)  loss_n_40: 0.8034 (0.9882)  loss_n_60: 0.8806 (0.9501)  loss_n_80: 0.9568 (0.9473)  loss_n_100: 1.0699 (1.0812)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0097)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [310/845]  eta: 0:02:59  loss: 4.1681 (3.9779)  loss_n_40: 0.8702 (0.9858)  loss_n_60: 0.9609 (0.9503)  loss_n_80: 1.0247 (0.9484)  loss_n_100: 1.1370 (1.0840)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0094)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [320/845]  eta: 0:02:56  loss: 3.7929 (3.9693)  loss_n_40: 0.8364 (0.9820)  loss_n_60: 0.9393 (0.9482)  loss_n_80: 0.9393 (0.9470)  loss_n_100: 1.1141 (1.0831)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0091)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [330/845]  eta: 0:02:53  loss: 3.8309 (3.9706)  loss_n_40: 0.8067 (0.9797)  loss_n_60: 0.9393 (0.9485)  loss_n_80: 0.9217 (0.9485)  loss_n_100: 1.0966 (1.0851)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0088)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [340/845]  eta: 0:02:49  loss: 3.9765 (3.9946)  loss_n_40: 0.8047 (0.9874)  loss_n_60: 0.9600 (0.9540)  loss_n_80: 0.9584 (0.9502)  loss_n_100: 1.1216 (1.0858)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0172)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [350/845]  eta: 0:02:46  loss: 4.0160 (4.0050)  loss_n_40: 0.8295 (0.9930)  loss_n_60: 0.9499 (0.9569)  loss_n_80: 0.9999 (0.9518)  loss_n_100: 1.1216 (1.0866)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0167)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [360/845]  eta: 0:02:42  loss: 3.9228 (4.0114)  loss_n_40: 0.8295 (0.9950)  loss_n_60: 0.9241 (0.9590)  loss_n_80: 0.9872 (0.9537)  loss_n_100: 1.0212 (1.0874)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0162)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [370/845]  eta: 0:02:39  loss: 3.8925 (4.0184)  loss_n_40: 0.8038 (0.9979)  loss_n_60: 0.9172 (0.9613)  loss_n_80: 0.9804 (0.9546)  loss_n_100: 1.0405 (1.0888)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0158)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [380/845]  eta: 0:02:36  loss: 4.1390 (4.0221)  loss_n_40: 0.8555 (0.9968)  loss_n_60: 0.9559 (0.9617)  loss_n_80: 0.9972 (0.9564)  loss_n_100: 1.2175 (1.0919)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0154)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [390/845]  eta: 0:02:32  loss: 4.1851 (4.0341)  loss_n_40: 0.8555 (0.9998)  loss_n_60: 0.9812 (0.9654)  loss_n_80: 1.0256 (0.9597)  loss_n_100: 1.2339 (1.0943)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0150)  time: 0.3347  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:6]  [400/845]  eta: 0:02:29  loss: 4.0321 (4.0332)  loss_n_40: 0.8032 (1.0011)  loss_n_60: 0.9966 (0.9648)  loss_n_80: 0.9914 (0.9593)  loss_n_100: 1.0633 (1.0934)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0146)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [410/845]  eta: 0:02:26  loss: 3.5513 (4.0317)  loss_n_40: 0.8007 (1.0000)  loss_n_60: 0.8462 (0.9640)  loss_n_80: 0.9071 (0.9592)  loss_n_100: 1.0380 (1.0942)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0142)  time: 0.3349  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:6]  [420/845]  eta: 0:02:22  loss: 3.5180 (4.0243)  loss_n_40: 0.7569 (0.9964)  loss_n_60: 0.8462 (0.9620)  loss_n_80: 0.9071 (0.9579)  loss_n_100: 1.0253 (1.0941)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0139)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [430/845]  eta: 0:02:19  loss: 3.4168 (4.0208)  loss_n_40: 0.7772 (0.9962)  loss_n_60: 0.8341 (0.9615)  loss_n_80: 0.8657 (0.9569)  loss_n_100: 0.9937 (1.0926)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0136)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [440/845]  eta: 0:02:15  loss: 3.4494 (4.0170)  loss_n_40: 0.7974 (0.9952)  loss_n_60: 0.8451 (0.9603)  loss_n_80: 0.8657 (0.9564)  loss_n_100: 0.9937 (1.0918)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0133)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [450/845]  eta: 0:02:12  loss: 3.8445 (4.0214)  loss_n_40: 0.8342 (0.9969)  loss_n_60: 0.8683 (0.9616)  loss_n_80: 0.9215 (0.9572)  loss_n_100: 1.0674 (1.0928)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0130)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [460/845]  eta: 0:02:09  loss: 4.2941 (4.0341)  loss_n_40: 0.8436 (0.9997)  loss_n_60: 1.0152 (0.9643)  loss_n_80: 1.0468 (0.9602)  loss_n_100: 1.2206 (1.0972)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0127)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [470/845]  eta: 0:02:05  loss: 4.2034 (4.0259)  loss_n_40: 0.8055 (0.9981)  loss_n_60: 0.9931 (0.9627)  loss_n_80: 1.0059 (0.9580)  loss_n_100: 1.1151 (1.0947)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0124)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [480/845]  eta: 0:02:02  loss: 3.7063 (4.0364)  loss_n_40: 0.7901 (1.0028)  loss_n_60: 0.8719 (0.9657)  loss_n_80: 0.9406 (0.9600)  loss_n_100: 1.0546 (1.0958)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0122)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [490/845]  eta: 0:01:59  loss: 4.1775 (4.0384)  loss_n_40: 1.0180 (1.0029)  loss_n_60: 0.9871 (0.9662)  loss_n_80: 1.0233 (0.9609)  loss_n_100: 1.1586 (1.0964)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0119)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [500/845]  eta: 0:01:55  loss: 4.0623 (4.0368)  loss_n_40: 0.8323 (1.0006)  loss_n_60: 0.9573 (0.9659)  loss_n_80: 1.0233 (0.9613)  loss_n_100: 1.1227 (1.0972)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0117)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [510/845]  eta: 0:01:52  loss: 3.8763 (4.0326)  loss_n_40: 0.7853 (0.9981)  loss_n_60: 0.9005 (0.9646)  loss_n_80: 0.9808 (0.9606)  loss_n_100: 1.0826 (1.0979)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0115)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [520/845]  eta: 0:01:49  loss: 3.4373 (4.0319)  loss_n_40: 0.7900 (0.9980)  loss_n_60: 0.8664 (0.9644)  loss_n_80: 0.8561 (0.9607)  loss_n_100: 1.0149 (1.0975)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0112)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [530/845]  eta: 0:01:45  loss: 4.2422 (4.0520)  loss_n_40: 0.8285 (1.0056)  loss_n_60: 1.0084 (0.9705)  loss_n_80: 1.0302 (0.9641)  loss_n_100: 1.1732 (1.1007)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0110)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [540/845]  eta: 0:01:42  loss: 3.9109 (4.0483)  loss_n_40: 0.8125 (1.0046)  loss_n_60: 0.9783 (0.9699)  loss_n_80: 1.0014 (0.9636)  loss_n_100: 1.1483 (1.0994)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0108)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [550/845]  eta: 0:01:38  loss: 3.4474 (4.0407)  loss_n_40: 0.7628 (1.0015)  loss_n_60: 0.8188 (0.9681)  loss_n_80: 0.8526 (0.9627)  loss_n_100: 0.9781 (1.0978)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0106)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [560/845]  eta: 0:01:35  loss: 3.3867 (4.0363)  loss_n_40: 0.7923 (1.0003)  loss_n_60: 0.8085 (0.9669)  loss_n_80: 0.8390 (0.9619)  loss_n_100: 1.0010 (1.0968)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0104)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [570/845]  eta: 0:01:32  loss: 3.5077 (4.0456)  loss_n_40: 0.8216 (1.0052)  loss_n_60: 0.8607 (0.9697)  loss_n_80: 0.8937 (0.9634)  loss_n_100: 1.0207 (1.0970)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0102)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [580/845]  eta: 0:01:28  loss: 3.5806 (4.0394)  loss_n_40: 0.8216 (1.0030)  loss_n_60: 0.8631 (0.9684)  loss_n_80: 0.8937 (0.9619)  loss_n_100: 1.0471 (1.0960)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0101)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [590/845]  eta: 0:01:25  loss: 3.7494 (4.0389)  loss_n_40: 0.8080 (1.0015)  loss_n_60: 0.9058 (0.9683)  loss_n_80: 0.9163 (0.9624)  loss_n_100: 1.0228 (1.0967)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0099)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [600/845]  eta: 0:01:22  loss: 3.8069 (4.0371)  loss_n_40: 0.8336 (1.0018)  loss_n_60: 0.9411 (0.9680)  loss_n_80: 0.9615 (0.9617)  loss_n_100: 1.0310 (1.0959)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0097)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [610/845]  eta: 0:01:18  loss: 3.3188 (4.0316)  loss_n_40: 0.8014 (0.9996)  loss_n_60: 0.8339 (0.9665)  loss_n_80: 0.8411 (0.9606)  loss_n_100: 0.9737 (1.0953)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0096)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [620/845]  eta: 0:01:15  loss: 3.6274 (4.0301)  loss_n_40: 0.7747 (0.9990)  loss_n_60: 0.8692 (0.9664)  loss_n_80: 0.8815 (0.9602)  loss_n_100: 1.0286 (1.0951)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0094)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [630/845]  eta: 0:01:12  loss: 3.8272 (4.0340)  loss_n_40: 0.8167 (0.9997)  loss_n_60: 0.9628 (0.9683)  loss_n_80: 0.9317 (0.9615)  loss_n_100: 1.0295 (1.0953)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0093)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [640/845]  eta: 0:01:08  loss: 3.8181 (4.0292)  loss_n_40: 0.8130 (0.9973)  loss_n_60: 0.9189 (0.9672)  loss_n_80: 0.9160 (0.9608)  loss_n_100: 1.0295 (1.0948)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0091)  time: 0.3350  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:6]  [650/845]  eta: 0:01:05  loss: 3.4485 (4.0237)  loss_n_40: 0.7794 (0.9945)  loss_n_60: 0.8208 (0.9657)  loss_n_80: 0.8538 (0.9601)  loss_n_100: 1.0382 (1.0945)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0090)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [660/845]  eta: 0:01:02  loss: 3.6076 (4.0241)  loss_n_40: 0.7830 (0.9931)  loss_n_60: 0.8966 (0.9657)  loss_n_80: 0.8941 (0.9605)  loss_n_100: 1.0394 (1.0959)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0089)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [670/845]  eta: 0:00:58  loss: 3.7245 (4.0228)  loss_n_40: 0.8194 (0.9934)  loss_n_60: 0.8981 (0.9660)  loss_n_80: 0.9221 (0.9602)  loss_n_100: 1.0647 (1.0945)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0087)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [680/845]  eta: 0:00:55  loss: 3.4786 (4.0210)  loss_n_40: 0.7880 (0.9934)  loss_n_60: 0.8637 (0.9659)  loss_n_80: 0.8694 (0.9599)  loss_n_100: 0.9523 (1.0933)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0086)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [690/845]  eta: 0:00:51  loss: 3.5036 (4.0205)  loss_n_40: 0.7882 (0.9933)  loss_n_60: 0.8699 (0.9656)  loss_n_80: 0.9171 (0.9598)  loss_n_100: 1.0389 (1.0934)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0085)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [700/845]  eta: 0:00:48  loss: 3.4957 (4.0197)  loss_n_40: 0.7854 (0.9931)  loss_n_60: 0.8626 (0.9655)  loss_n_80: 0.8792 (0.9598)  loss_n_100: 1.0363 (1.0930)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0083)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [710/845]  eta: 0:00:45  loss: 3.4279 (4.0188)  loss_n_40: 0.7624 (0.9930)  loss_n_60: 0.8253 (0.9650)  loss_n_80: 0.8370 (0.9597)  loss_n_100: 1.0130 (1.0928)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0082)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [720/845]  eta: 0:00:41  loss: 3.4598 (4.0241)  loss_n_40: 0.7761 (0.9965)  loss_n_60: 0.8196 (0.9660)  loss_n_80: 0.8819 (0.9596)  loss_n_100: 0.9761 (1.0916)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0104)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [730/845]  eta: 0:00:38  loss: 3.5491 (4.0221)  loss_n_40: 0.7993 (0.9961)  loss_n_60: 0.8677 (0.9653)  loss_n_80: 0.8982 (0.9589)  loss_n_100: 1.0556 (1.0916)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0103)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [740/845]  eta: 0:00:35  loss: 3.7045 (4.0201)  loss_n_40: 0.8729 (0.9947)  loss_n_60: 0.8832 (0.9648)  loss_n_80: 0.9002 (0.9586)  loss_n_100: 1.0880 (1.0919)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0101)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [750/845]  eta: 0:00:31  loss: 3.4578 (4.0150)  loss_n_40: 0.7941 (0.9930)  loss_n_60: 0.8086 (0.9636)  loss_n_80: 0.8596 (0.9574)  loss_n_100: 0.9882 (1.0910)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0100)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [760/845]  eta: 0:00:28  loss: 3.4798 (4.0124)  loss_n_40: 0.7677 (0.9919)  loss_n_60: 0.8041 (0.9631)  loss_n_80: 0.8596 (0.9571)  loss_n_100: 0.9654 (1.0906)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0099)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [770/845]  eta: 0:00:25  loss: 3.6678 (4.0096)  loss_n_40: 0.8064 (0.9908)  loss_n_60: 0.8740 (0.9623)  loss_n_80: 0.9515 (0.9566)  loss_n_100: 1.0219 (1.0902)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0098)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [780/845]  eta: 0:00:21  loss: 3.6599 (4.0075)  loss_n_40: 0.8381 (0.9897)  loss_n_60: 0.8415 (0.9616)  loss_n_80: 0.8817 (0.9564)  loss_n_100: 1.0511 (1.0902)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0096)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [790/845]  eta: 0:00:18  loss: 3.5744 (4.0040)  loss_n_40: 0.7743 (0.9876)  loss_n_60: 0.8357 (0.9606)  loss_n_80: 0.8953 (0.9561)  loss_n_100: 1.0888 (1.0902)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0095)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [800/845]  eta: 0:00:15  loss: 4.0237 (4.0103)  loss_n_40: 0.7813 (0.9888)  loss_n_60: 0.9568 (0.9619)  loss_n_80: 1.0263 (0.9577)  loss_n_100: 1.1969 (1.0924)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0094)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [810/845]  eta: 0:00:11  loss: 3.9158 (4.0051)  loss_n_40: 0.7730 (0.9868)  loss_n_60: 0.9281 (0.9608)  loss_n_80: 0.9737 (0.9568)  loss_n_100: 1.1735 (1.0914)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0093)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [820/845]  eta: 0:00:08  loss: 3.4772 (4.0063)  loss_n_40: 0.7693 (0.9882)  loss_n_60: 0.8878 (0.9612)  loss_n_80: 0.8433 (0.9569)  loss_n_100: 0.9746 (1.0908)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0092)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [830/845]  eta: 0:00:05  loss: 3.4898 (4.0014)  loss_n_40: 0.7775 (0.9861)  loss_n_60: 0.8455 (0.9598)  loss_n_80: 0.8684 (0.9561)  loss_n_100: 0.9991 (1.0903)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0090)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [840/845]  eta: 0:00:01  loss: 4.0226 (4.0144)  loss_n_40: 0.8556 (0.9918)  loss_n_60: 0.9692 (0.9640)  loss_n_80: 0.9776 (0.9583)  loss_n_100: 1.1386 (1.0913)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0089)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6]  [844/845]  eta: 0:00:00  loss: 4.0226 (4.0127)  loss_n_40: 0.8413 (0.9908)  loss_n_60: 0.9692 (0.9637)  loss_n_80: 0.9776 (0.9581)  loss_n_100: 1.0834 (1.0913)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0089)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:6] Total time: 0:04:43 (0.3354 s / it)\n",
      "Averaged stats: loss: 4.0226 (4.0127)  loss_n_40: 0.8413 (0.9908)  loss_n_60: 0.9692 (0.9637)  loss_n_80: 0.9776 (0.9581)  loss_n_100: 1.0834 (1.0913)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0089)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_6_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 1.091%\n",
      "Min loss_n_100: 1.091\n",
      "Best Epoch: 6.000\n",
      "Train: [epoch:7]  [   0/1724]  eta: 1:59:51  lr: 0.000120  loss: 4.6795 (4.6795)  loss_n_40: 1.2074 (1.2074)  loss_n_60: 1.1646 (1.1646)  loss_n_80: 1.0717 (1.0717)  loss_n_100: 1.2359 (1.2359)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1714  data: 0.4133  max mem: 46473\n",
      "Train: [epoch:7]  [  10/1724]  eta: 1:52:45  lr: 0.000120  loss: 3.8423 (3.8334)  loss_n_40: 0.8877 (0.9267)  loss_n_60: 0.9224 (0.9179)  loss_n_80: 0.8998 (0.9305)  loss_n_100: 1.0496 (1.0582)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9474  data: 0.0377  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [  20/1724]  eta: 1:51:47  lr: 0.000120  loss: 3.6357 (3.7143)  loss_n_40: 0.8348 (0.8715)  loss_n_60: 0.8730 (0.8951)  loss_n_80: 0.8794 (0.9131)  loss_n_100: 1.0134 (1.0346)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [  30/1724]  eta: 1:51:01  lr: 0.000120  loss: 3.7141 (3.7844)  loss_n_40: 0.8287 (0.9119)  loss_n_60: 0.8953 (0.9101)  loss_n_80: 0.9042 (0.9288)  loss_n_100: 1.0154 (1.0336)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [  40/1724]  eta: 1:50:18  lr: 0.000120  loss: 3.8738 (3.8946)  loss_n_40: 0.9394 (0.9600)  loss_n_60: 0.9135 (0.9318)  loss_n_80: 0.9395 (0.9422)  loss_n_100: 1.0154 (1.0374)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0029)  triple_60: 0.0000 (0.0019)  triple_40: 0.0000 (0.0183)  time: 3.9235  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [  50/1724]  eta: 1:49:36  lr: 0.000120  loss: 4.5550 (4.2020)  loss_n_40: 0.9780 (0.9883)  loss_n_60: 1.0276 (0.9771)  loss_n_80: 1.0683 (1.0366)  loss_n_100: 1.1514 (1.1291)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0024)  triple_60: 0.0000 (0.0122)  triple_40: 0.0000 (0.0516)  time: 3.9226  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [  60/1724]  eta: 1:48:56  lr: 0.000120  loss: 5.4763 (4.4291)  loss_n_40: 1.1556 (1.0409)  loss_n_60: 1.2466 (1.0254)  loss_n_80: 1.4009 (1.1003)  loss_n_100: 1.5513 (1.2031)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0020)  triple_60: 0.0000 (0.0102)  triple_40: 0.0000 (0.0431)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [  70/1724]  eta: 1:48:16  lr: 0.000120  loss: 5.2101 (4.4876)  loss_n_40: 1.1735 (1.0544)  loss_n_60: 1.1900 (1.0393)  loss_n_80: 1.3154 (1.1209)  loss_n_100: 1.4884 (1.2220)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0017)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0370)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [  80/1724]  eta: 1:47:36  lr: 0.000120  loss: 4.6674 (4.4925)  loss_n_40: 1.0785 (1.0613)  loss_n_60: 1.1204 (1.0458)  loss_n_80: 1.1586 (1.1206)  loss_n_100: 1.2377 (1.2201)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0015)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0325)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [  90/1724]  eta: 1:46:56  lr: 0.000120  loss: 4.2313 (4.4372)  loss_n_40: 0.9962 (1.0514)  loss_n_60: 1.0119 (1.0364)  loss_n_80: 1.0097 (1.1039)  loss_n_100: 1.1566 (1.2057)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0013)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0289)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 100/1724]  eta: 1:46:16  lr: 0.000120  loss: 3.9484 (4.3845)  loss_n_40: 0.9110 (1.0420)  loss_n_60: 0.9477 (1.0253)  loss_n_80: 0.9741 (1.0903)  loss_n_100: 1.0736 (1.1910)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0012)  triple_60: 0.0000 (0.0062)  triple_40: 0.0000 (0.0260)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 110/1724]  eta: 1:45:37  lr: 0.000120  loss: 3.8230 (4.3296)  loss_n_40: 0.8429 (1.0299)  loss_n_60: 0.8918 (1.0126)  loss_n_80: 0.9414 (1.0760)  loss_n_100: 1.0633 (1.1785)  triple_100: 0.0000 (0.0022)  triple_80: 0.0000 (0.0011)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0237)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 120/1724]  eta: 1:44:57  lr: 0.000120  loss: 4.1729 (4.4448)  loss_n_40: 0.9535 (1.0403)  loss_n_60: 0.9590 (1.0271)  loss_n_80: 1.0226 (1.0831)  loss_n_100: 1.1039 (1.1836)  triple_100: 0.0000 (0.0219)  triple_80: 0.0000 (0.0198)  triple_60: 0.0000 (0.0472)  triple_40: 0.0000 (0.0217)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 130/1724]  eta: 1:44:18  lr: 0.000120  loss: 4.5946 (4.4600)  loss_n_40: 1.1097 (1.0540)  loss_n_60: 1.1068 (1.0312)  loss_n_80: 1.1380 (1.0850)  loss_n_100: 1.2645 (1.1876)  triple_100: 0.0000 (0.0202)  triple_80: 0.0000 (0.0183)  triple_60: 0.0000 (0.0436)  triple_40: 0.0000 (0.0201)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 140/1724]  eta: 1:43:39  lr: 0.000120  loss: 4.4245 (4.4519)  loss_n_40: 1.1097 (1.0622)  loss_n_60: 1.0347 (1.0287)  loss_n_80: 1.0360 (1.0815)  loss_n_100: 1.1727 (1.1845)  triple_100: 0.0000 (0.0188)  triple_80: 0.0000 (0.0170)  triple_60: 0.0000 (0.0405)  triple_40: 0.0000 (0.0186)  time: 3.9252  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [ 150/1724]  eta: 1:42:59  lr: 0.000120  loss: 3.9920 (4.4333)  loss_n_40: 1.0728 (1.0661)  loss_n_60: 0.9341 (1.0246)  loss_n_80: 0.9645 (1.0759)  loss_n_100: 1.0942 (1.1780)  triple_100: 0.0000 (0.0176)  triple_80: 0.0000 (0.0159)  triple_60: 0.0000 (0.0379)  triple_40: 0.0000 (0.0174)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 160/1724]  eta: 1:42:20  lr: 0.000120  loss: 3.8743 (4.3848)  loss_n_40: 0.9797 (1.0574)  loss_n_60: 0.8750 (1.0131)  loss_n_80: 0.9115 (1.0635)  loss_n_100: 1.0251 (1.1676)  triple_100: 0.0000 (0.0165)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0355)  triple_40: 0.0000 (0.0163)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 170/1724]  eta: 1:41:40  lr: 0.000120  loss: 3.6668 (4.3455)  loss_n_40: 0.9352 (1.0518)  loss_n_60: 0.8364 (1.0035)  loss_n_80: 0.8564 (1.0533)  loss_n_100: 1.0058 (1.1585)  triple_100: 0.0000 (0.0155)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0334)  triple_40: 0.0000 (0.0154)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 180/1724]  eta: 1:41:01  lr: 0.000120  loss: 3.6564 (4.3087)  loss_n_40: 0.9544 (1.0460)  loss_n_60: 0.8381 (0.9961)  loss_n_80: 0.8511 (1.0442)  loss_n_100: 0.9719 (1.1484)  triple_100: 0.0000 (0.0147)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0316)  triple_40: 0.0000 (0.0145)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 190/1724]  eta: 1:40:22  lr: 0.000120  loss: 3.4958 (4.2735)  loss_n_40: 0.8827 (1.0385)  loss_n_60: 0.8163 (0.9872)  loss_n_80: 0.8647 (1.0349)  loss_n_100: 0.9574 (1.1381)  triple_100: 0.0000 (0.0139)  triple_80: 0.0000 (0.0126)  triple_60: 0.0000 (0.0299)  triple_40: 0.0000 (0.0184)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 200/1724]  eta: 1:39:42  lr: 0.000120  loss: 3.4678 (4.2505)  loss_n_40: 0.8732 (1.0334)  loss_n_60: 0.8163 (0.9826)  loss_n_80: 0.8647 (1.0291)  loss_n_100: 0.9440 (1.1313)  triple_100: 0.0000 (0.0159)  triple_80: 0.0000 (0.0119)  triple_60: 0.0000 (0.0289)  triple_40: 0.0000 (0.0175)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 210/1724]  eta: 1:39:03  lr: 0.000120  loss: 4.7326 (4.7076)  loss_n_40: 1.0204 (1.0703)  loss_n_60: 1.0688 (1.0372)  loss_n_80: 1.0725 (1.1015)  loss_n_100: 1.1817 (1.2067)  triple_100: 0.0000 (0.1006)  triple_80: 0.0000 (0.0779)  triple_60: 0.0000 (0.0359)  triple_40: 0.0000 (0.0775)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 220/1724]  eta: 1:38:24  lr: 0.000120  loss: 20.8551 (5.5246)  loss_n_40: 4.0229 (1.2429)  loss_n_60: 4.3733 (1.2278)  loss_n_80: 4.6990 (1.3000)  loss_n_100: 4.8368 (1.3976)  triple_100: 0.0000 (0.1094)  triple_80: 0.0000 (0.1318)  triple_60: 0.0000 (0.0411)  triple_40: 0.0000 (0.0740)  time: 3.9236  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 230/1724]  eta: 1:37:44  lr: 0.000120  loss: 22.3654 (6.2662)  loss_n_40: 5.2248 (1.4239)  loss_n_60: 5.5982 (1.4244)  loss_n_80: 5.6742 (1.4952)  loss_n_100: 5.5274 (1.5818)  triple_100: 0.0000 (0.1046)  triple_80: 0.0000 (0.1261)  triple_60: 0.0000 (0.0393)  triple_40: 0.0000 (0.0708)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 240/1724]  eta: 1:37:05  lr: 0.000120  loss: 21.9776 (6.8770)  loss_n_40: 5.2524 (1.5720)  loss_n_60: 5.6767 (1.5888)  loss_n_80: 5.6446 (1.6556)  loss_n_100: 5.4039 (1.7316)  triple_100: 0.0000 (0.1003)  triple_80: 0.0000 (0.1209)  triple_60: 0.0000 (0.0392)  triple_40: 0.0000 (0.0686)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 250/1724]  eta: 1:36:25  lr: 0.000120  loss: 20.0077 (7.3553)  loss_n_40: 4.6335 (1.6880)  loss_n_60: 4.9131 (1.7122)  loss_n_80: 5.2275 (1.7861)  loss_n_100: 5.0538 (1.8531)  triple_100: 0.0000 (0.0963)  triple_80: 0.0000 (0.1161)  triple_60: 0.0000 (0.0376)  triple_40: 0.0000 (0.0658)  time: 3.9217  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [ 260/1724]  eta: 1:35:46  lr: 0.000120  loss: 18.1037 (7.7478)  loss_n_40: 4.2093 (1.7758)  loss_n_60: 4.3638 (1.8070)  loss_n_80: 4.7287 (1.8942)  loss_n_100: 4.6653 (1.9590)  triple_100: 0.0000 (0.0926)  triple_80: 0.0000 (0.1165)  triple_60: 0.0000 (0.0394)  triple_40: 0.0000 (0.0633)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 270/1724]  eta: 1:35:06  lr: 0.000120  loss: 16.4064 (8.0163)  loss_n_40: 3.6722 (1.8345)  loss_n_60: 3.9318 (1.8732)  loss_n_80: 4.4024 (1.9712)  loss_n_100: 4.4128 (2.0369)  triple_100: 0.0000 (0.0892)  triple_80: 0.0000 (0.1124)  triple_60: 0.0000 (0.0380)  triple_40: 0.0000 (0.0610)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 280/1724]  eta: 1:34:27  lr: 0.000120  loss: 13.2995 (8.1720)  loss_n_40: 3.0206 (1.8678)  loss_n_60: 3.1219 (1.9073)  loss_n_80: 3.5745 (2.0201)  loss_n_100: 3.6923 (2.0846)  triple_100: 0.0000 (0.0860)  triple_80: 0.0000 (0.1084)  triple_60: 0.0000 (0.0366)  triple_40: 0.0000 (0.0610)  time: 3.9238  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [ 290/1724]  eta: 1:33:48  lr: 0.000120  loss: 12.5262 (8.4199)  loss_n_40: 2.6825 (1.8920)  loss_n_60: 2.8033 (1.9390)  loss_n_80: 3.2444 (2.0592)  loss_n_100: 3.3770 (2.1249)  triple_100: 0.0000 (0.1031)  triple_80: 0.0000 (0.1157)  triple_60: 0.0000 (0.0369)  triple_40: 0.0000 (0.1490)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 300/1724]  eta: 1:33:08  lr: 0.000120  loss: 14.1900 (8.6169)  loss_n_40: 2.8034 (1.9272)  loss_n_60: 3.1355 (1.9917)  loss_n_80: 3.3198 (2.1161)  loss_n_100: 3.3823 (2.1722)  triple_100: 0.0000 (0.0997)  triple_80: 0.0000 (0.1127)  triple_60: 0.0000 (0.0529)  triple_40: 0.0000 (0.1444)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 310/1724]  eta: 1:32:29  lr: 0.000120  loss: 14.6610 (8.8617)  loss_n_40: 2.9535 (1.9621)  loss_n_60: 3.2291 (2.0295)  loss_n_80: 3.9781 (2.1847)  loss_n_100: 3.8112 (2.2439)  triple_100: 0.0000 (0.1143)  triple_80: 0.0000 (0.1159)  triple_60: 0.0000 (0.0547)  triple_40: 0.0000 (0.1566)  time: 3.9238  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [ 320/1724]  eta: 1:31:50  lr: 0.000120  loss: 16.0757 (9.1188)  loss_n_40: 2.8623 (1.9877)  loss_n_60: 3.2291 (2.0736)  loss_n_80: 4.2241 (2.2488)  loss_n_100: 4.4144 (2.3110)  triple_100: 0.0000 (0.1107)  triple_80: 0.0000 (0.1350)  triple_60: 0.0000 (0.0626)  triple_40: 0.0000 (0.1894)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 330/1724]  eta: 1:31:10  lr: 0.000120  loss: 14.4929 (9.2809)  loss_n_40: 2.7198 (2.0105)  loss_n_60: 3.4528 (2.1145)  loss_n_80: 3.9994 (2.2982)  loss_n_100: 4.1537 (2.3648)  triple_100: 0.0000 (0.1176)  triple_80: 0.0000 (0.1309)  triple_60: 0.0000 (0.0607)  triple_40: 0.0000 (0.1837)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 340/1724]  eta: 1:30:31  lr: 0.000120  loss: 13.8585 (9.3994)  loss_n_40: 2.6411 (2.0278)  loss_n_60: 3.2301 (2.1399)  loss_n_80: 3.8316 (2.3391)  loss_n_100: 3.9516 (2.4059)  triple_100: 0.0000 (0.1216)  triple_80: 0.0000 (0.1278)  triple_60: 0.0000 (0.0589)  triple_40: 0.0000 (0.1783)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 350/1724]  eta: 1:29:52  lr: 0.000120  loss: 13.4628 (9.5129)  loss_n_40: 2.5836 (2.0473)  loss_n_60: 3.0243 (2.1667)  loss_n_80: 3.6478 (2.3741)  loss_n_100: 3.6368 (2.4365)  triple_100: 0.0000 (0.1261)  triple_80: 0.0000 (0.1242)  triple_60: 0.0000 (0.0648)  triple_40: 0.0000 (0.1732)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 360/1724]  eta: 1:29:12  lr: 0.000120  loss: 14.2944 (9.7059)  loss_n_40: 2.9427 (2.0873)  loss_n_60: 3.2570 (2.2066)  loss_n_80: 3.7780 (2.4188)  loss_n_100: 3.6902 (2.4774)  triple_100: 0.0000 (0.1226)  triple_80: 0.0000 (0.1207)  triple_60: 0.0000 (0.0630)  triple_40: 0.0000 (0.2096)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 370/1724]  eta: 1:28:33  lr: 0.000120  loss: 16.9826 (9.9111)  loss_n_40: 3.7833 (2.1375)  loss_n_60: 4.0526 (2.2642)  loss_n_80: 4.1995 (2.4717)  loss_n_100: 4.0936 (2.5256)  triple_100: 0.0000 (0.1193)  triple_80: 0.0000 (0.1175)  triple_60: 0.0000 (0.0613)  triple_40: 0.0000 (0.2140)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 380/1724]  eta: 1:27:54  lr: 0.000120  loss: 16.7363 (10.0655)  loss_n_40: 3.8562 (2.1748)  loss_n_60: 4.3147 (2.3138)  loss_n_80: 4.1976 (2.5116)  loss_n_100: 4.1701 (2.5652)  triple_100: 0.0000 (0.1162)  triple_80: 0.0000 (0.1144)  triple_60: 0.0000 (0.0597)  triple_40: 0.0000 (0.2098)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 390/1724]  eta: 1:27:14  lr: 0.000120  loss: 15.0802 (10.1801)  loss_n_40: 3.3094 (2.2010)  loss_n_60: 3.9231 (2.3486)  loss_n_80: 3.8591 (2.5431)  loss_n_100: 3.8950 (2.5960)  triple_100: 0.0000 (0.1133)  triple_80: 0.0000 (0.1115)  triple_60: 0.0000 (0.0582)  triple_40: 0.0000 (0.2083)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 400/1724]  eta: 1:26:35  lr: 0.000120  loss: 13.8580 (10.2653)  loss_n_40: 3.1176 (2.2229)  loss_n_60: 3.5734 (2.3780)  loss_n_80: 3.5517 (2.5678)  loss_n_100: 3.5560 (2.6176)  triple_100: 0.0000 (0.1104)  triple_80: 0.0000 (0.1087)  triple_60: 0.0000 (0.0567)  triple_40: 0.0000 (0.2031)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 410/1724]  eta: 1:25:56  lr: 0.000120  loss: 12.9114 (10.3214)  loss_n_40: 2.9043 (2.2371)  loss_n_60: 3.3589 (2.3992)  loss_n_80: 3.4576 (2.5862)  loss_n_100: 3.3324 (2.6316)  triple_100: 0.0000 (0.1078)  triple_80: 0.0000 (0.1061)  triple_60: 0.0000 (0.0553)  triple_40: 0.0000 (0.1982)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 420/1724]  eta: 1:25:17  lr: 0.000120  loss: 12.0398 (10.3547)  loss_n_40: 2.7784 (2.2479)  loss_n_60: 3.1074 (2.4114)  loss_n_80: 3.1767 (2.5982)  loss_n_100: 2.9842 (2.6384)  triple_100: 0.0000 (0.1052)  triple_80: 0.0000 (0.1035)  triple_60: 0.0000 (0.0540)  triple_40: 0.0000 (0.1961)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 430/1724]  eta: 1:24:37  lr: 0.000120  loss: 11.1979 (10.3648)  loss_n_40: 2.6119 (2.2541)  loss_n_60: 2.8131 (2.4168)  loss_n_80: 3.0003 (2.6046)  loss_n_100: 2.8507 (2.6411)  triple_100: 0.0000 (0.1028)  triple_80: 0.0000 (0.1011)  triple_60: 0.0000 (0.0528)  triple_40: 0.0000 (0.1915)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 440/1724]  eta: 1:23:58  lr: 0.000120  loss: 10.8308 (10.3829)  loss_n_40: 2.5156 (2.2627)  loss_n_60: 2.6427 (2.4229)  loss_n_80: 2.8621 (2.6125)  loss_n_100: 2.7658 (2.6463)  triple_100: 0.0000 (0.1004)  triple_80: 0.0000 (0.0988)  triple_60: 0.0000 (0.0516)  triple_40: 0.0000 (0.1876)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 450/1724]  eta: 1:23:19  lr: 0.000120  loss: 10.6562 (10.3848)  loss_n_40: 2.4911 (2.2676)  loss_n_60: 2.6100 (2.4247)  loss_n_80: 2.8041 (2.6159)  loss_n_100: 2.7295 (2.6480)  triple_100: 0.0000 (0.0982)  triple_80: 0.0000 (0.0967)  triple_60: 0.0000 (0.0504)  triple_40: 0.0000 (0.1834)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 460/1724]  eta: 1:22:39  lr: 0.000120  loss: 9.6858 (10.3636)  loss_n_40: 2.3048 (2.2675)  loss_n_60: 2.3078 (2.4212)  loss_n_80: 2.5720 (2.6124)  loss_n_100: 2.5684 (2.6430)  triple_100: 0.0000 (0.0961)  triple_80: 0.0000 (0.0946)  triple_60: 0.0000 (0.0493)  triple_40: 0.0000 (0.1795)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 470/1724]  eta: 1:22:00  lr: 0.000120  loss: 9.4014 (10.3513)  loss_n_40: 2.2157 (2.2687)  loss_n_60: 2.2191 (2.4194)  loss_n_80: 2.4378 (2.6112)  loss_n_100: 2.4741 (2.6416)  triple_100: 0.0000 (0.0940)  triple_80: 0.0000 (0.0925)  triple_60: 0.0000 (0.0483)  triple_40: 0.0000 (0.1757)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 480/1724]  eta: 1:21:21  lr: 0.000120  loss: 9.4014 (10.3386)  loss_n_40: 2.3021 (2.2684)  loss_n_60: 2.2191 (2.4165)  loss_n_80: 2.4685 (2.6101)  loss_n_100: 2.4741 (2.6385)  triple_100: 0.0000 (0.0921)  triple_80: 0.0000 (0.0906)  triple_60: 0.0000 (0.0473)  triple_40: 0.0000 (0.1751)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 490/1724]  eta: 1:20:42  lr: 0.000120  loss: 9.3828 (10.3248)  loss_n_40: 2.2982 (2.2688)  loss_n_60: 2.1852 (2.4133)  loss_n_80: 2.4741 (2.6072)  loss_n_100: 2.3905 (2.6353)  triple_100: 0.0000 (0.0902)  triple_80: 0.0000 (0.0921)  triple_60: 0.0000 (0.0463)  triple_40: 0.0000 (0.1716)  time: 3.9235  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [ 500/1724]  eta: 1:20:02  lr: 0.000120  loss: 9.3823 (10.3025)  loss_n_40: 2.2675 (2.2677)  loss_n_60: 2.1717 (2.4083)  loss_n_80: 2.3936 (2.6022)  loss_n_100: 2.4170 (2.6300)  triple_100: 0.0000 (0.0884)  triple_80: 0.0000 (0.0913)  triple_60: 0.0000 (0.0454)  triple_40: 0.0000 (0.1692)  time: 3.9243  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [ 510/1724]  eta: 1:19:23  lr: 0.000120  loss: 8.9568 (10.2709)  loss_n_40: 2.0950 (2.2640)  loss_n_60: 2.0620 (2.4015)  loss_n_80: 2.3219 (2.5957)  loss_n_100: 2.3439 (2.6232)  triple_100: 0.0000 (0.0867)  triple_80: 0.0000 (0.0896)  triple_60: 0.0000 (0.0445)  triple_40: 0.0000 (0.1659)  time: 3.9244  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [ 520/1724]  eta: 1:18:44  lr: 0.000120  loss: 8.8344 (10.2483)  loss_n_40: 2.0494 (2.2610)  loss_n_60: 2.0466 (2.3971)  loss_n_80: 2.3130 (2.5918)  loss_n_100: 2.3439 (2.6192)  triple_100: 0.0000 (0.0850)  triple_80: 0.0000 (0.0878)  triple_60: 0.0000 (0.0436)  triple_40: 0.0000 (0.1627)  time: 3.9243  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 530/1724]  eta: 1:18:05  lr: 0.000120  loss: 8.8269 (10.2152)  loss_n_40: 2.0765 (2.2579)  loss_n_60: 2.0773 (2.3899)  loss_n_80: 2.3073 (2.5843)  loss_n_100: 2.3799 (2.6110)  triple_100: 0.0000 (0.0834)  triple_80: 0.0000 (0.0862)  triple_60: 0.0000 (0.0428)  triple_40: 0.0000 (0.1596)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 540/1724]  eta: 1:17:26  lr: 0.000120  loss: 8.4839 (10.1861)  loss_n_40: 2.0156 (2.2532)  loss_n_60: 1.9896 (2.3831)  loss_n_80: 2.2696 (2.5790)  loss_n_100: 2.2919 (2.6056)  triple_100: 0.0000 (0.0819)  triple_80: 0.0000 (0.0846)  triple_60: 0.0000 (0.0420)  triple_40: 0.0000 (0.1567)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 550/1724]  eta: 1:16:46  lr: 0.000120  loss: 8.6837 (10.1745)  loss_n_40: 1.9960 (2.2503)  loss_n_60: 2.0096 (2.3784)  loss_n_80: 2.3752 (2.5768)  loss_n_100: 2.3145 (2.6012)  triple_100: 0.0000 (0.0804)  triple_80: 0.0000 (0.0898)  triple_60: 0.0000 (0.0438)  triple_40: 0.0000 (0.1538)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 560/1724]  eta: 1:16:07  lr: 0.000120  loss: 8.6684 (10.1456)  loss_n_40: 2.0551 (2.2464)  loss_n_60: 2.0072 (2.3715)  loss_n_80: 2.3509 (2.5720)  loss_n_100: 2.2894 (2.5944)  triple_100: 0.0000 (0.0789)  triple_80: 0.0000 (0.0882)  triple_60: 0.0000 (0.0431)  triple_40: 0.0000 (0.1511)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 570/1724]  eta: 1:15:28  lr: 0.000120  loss: 8.2487 (10.1182)  loss_n_40: 2.0409 (2.2433)  loss_n_60: 1.9452 (2.3643)  loss_n_80: 2.2477 (2.5674)  loss_n_100: 2.1427 (2.5883)  triple_100: 0.0000 (0.0776)  triple_80: 0.0000 (0.0867)  triple_60: 0.0000 (0.0423)  triple_40: 0.0000 (0.1484)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 580/1724]  eta: 1:14:49  lr: 0.000120  loss: 8.2487 (10.0957)  loss_n_40: 1.9555 (2.2382)  loss_n_60: 1.8525 (2.3553)  loss_n_80: 2.1796 (2.5605)  loss_n_100: 2.1025 (2.5806)  triple_100: 0.0000 (0.0805)  triple_80: 0.0000 (0.0852)  triple_60: 0.0000 (0.0430)  triple_40: 0.0000 (0.1524)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 590/1724]  eta: 1:14:09  lr: 0.000120  loss: 8.4288 (10.0761)  loss_n_40: 1.9756 (2.2361)  loss_n_60: 1.9140 (2.3500)  loss_n_80: 2.2523 (2.5582)  loss_n_100: 2.1675 (2.5767)  triple_100: 0.0000 (0.0791)  triple_80: 0.0000 (0.0837)  triple_60: 0.0000 (0.0423)  triple_40: 0.0000 (0.1499)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 600/1724]  eta: 1:13:30  lr: 0.000120  loss: 8.6079 (10.0571)  loss_n_40: 2.0901 (2.2339)  loss_n_60: 2.0095 (2.3450)  loss_n_80: 2.3023 (2.5559)  loss_n_100: 2.2655 (2.5732)  triple_100: 0.0000 (0.0778)  triple_80: 0.0000 (0.0823)  triple_60: 0.0000 (0.0416)  triple_40: 0.0000 (0.1474)  time: 3.9243  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 610/1724]  eta: 1:12:51  lr: 0.000120  loss: 8.3813 (10.0264)  loss_n_40: 2.0023 (2.2294)  loss_n_60: 1.9366 (2.3370)  loss_n_80: 2.2841 (2.5501)  loss_n_100: 2.1902 (2.5666)  triple_100: 0.0000 (0.0765)  triple_80: 0.0000 (0.0810)  triple_60: 0.0000 (0.0409)  triple_40: 0.0000 (0.1449)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 620/1724]  eta: 1:12:12  lr: 0.000120  loss: 8.0052 (9.9946)  loss_n_40: 1.9191 (2.2247)  loss_n_60: 1.8062 (2.3288)  loss_n_80: 2.1243 (2.5431)  loss_n_100: 2.1323 (2.5599)  triple_100: 0.0000 (0.0753)  triple_80: 0.0000 (0.0797)  triple_60: 0.0000 (0.0403)  triple_40: 0.0000 (0.1428)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 630/1724]  eta: 1:11:33  lr: 0.000120  loss: 7.8734 (9.9584)  loss_n_40: 1.9171 (2.2198)  loss_n_60: 1.7836 (2.3199)  loss_n_80: 2.0555 (2.5350)  loss_n_100: 2.0981 (2.5511)  triple_100: 0.0000 (0.0741)  triple_80: 0.0000 (0.0784)  triple_60: 0.0000 (0.0396)  triple_40: 0.0000 (0.1405)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 640/1724]  eta: 1:10:53  lr: 0.000120  loss: 7.7336 (9.9268)  loss_n_40: 1.8602 (2.2155)  loss_n_60: 1.7697 (2.3119)  loss_n_80: 2.0433 (2.5276)  loss_n_100: 1.9819 (2.5435)  triple_100: 0.0000 (0.0729)  triple_80: 0.0000 (0.0781)  triple_60: 0.0000 (0.0390)  triple_40: 0.0000 (0.1383)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 650/1724]  eta: 1:10:14  lr: 0.000120  loss: 7.6147 (9.8915)  loss_n_40: 1.8186 (2.2099)  loss_n_60: 1.7073 (2.3032)  loss_n_80: 1.9686 (2.5194)  loss_n_100: 1.9952 (2.5353)  triple_100: 0.0000 (0.0718)  triple_80: 0.0000 (0.0772)  triple_60: 0.0000 (0.0384)  triple_40: 0.0000 (0.1362)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 660/1724]  eta: 1:09:35  lr: 0.000120  loss: 7.4251 (9.8549)  loss_n_40: 1.8030 (2.2051)  loss_n_60: 1.6789 (2.2944)  loss_n_80: 1.9212 (2.5103)  loss_n_100: 1.9457 (2.5252)  triple_100: 0.0000 (0.0707)  triple_80: 0.0000 (0.0768)  triple_60: 0.0000 (0.0382)  triple_40: 0.0000 (0.1341)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 670/1724]  eta: 1:08:56  lr: 0.000120  loss: 7.2282 (9.8174)  loss_n_40: 1.8298 (2.1996)  loss_n_60: 1.6151 (2.2845)  loss_n_80: 1.8931 (2.5010)  loss_n_100: 1.8146 (2.5155)  triple_100: 0.0000 (0.0697)  triple_80: 0.0000 (0.0756)  triple_60: 0.0000 (0.0376)  triple_40: 0.0000 (0.1337)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 680/1724]  eta: 1:08:16  lr: 0.000120  loss: 7.2229 (9.7821)  loss_n_40: 1.8298 (2.1948)  loss_n_60: 1.6255 (2.2760)  loss_n_80: 1.8558 (2.4924)  loss_n_100: 1.8287 (2.5069)  triple_100: 0.0000 (0.0687)  triple_80: 0.0000 (0.0745)  triple_60: 0.0000 (0.0371)  triple_40: 0.0000 (0.1318)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 690/1724]  eta: 1:07:37  lr: 0.000120  loss: 7.0067 (9.7409)  loss_n_40: 1.7980 (2.1887)  loss_n_60: 1.6166 (2.2662)  loss_n_80: 1.8156 (2.4825)  loss_n_100: 1.8450 (2.4961)  triple_100: 0.0000 (0.0677)  triple_80: 0.0000 (0.0734)  triple_60: 0.0000 (0.0365)  triple_40: 0.0000 (0.1299)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 700/1724]  eta: 1:06:58  lr: 0.000120  loss: 7.0067 (9.7033)  loss_n_40: 1.7895 (2.1836)  loss_n_60: 1.6166 (2.2572)  loss_n_80: 1.8156 (2.4727)  loss_n_100: 1.8279 (2.4858)  triple_100: 0.0000 (0.0667)  triple_80: 0.0000 (0.0724)  triple_60: 0.0000 (0.0360)  triple_40: 0.0000 (0.1288)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 710/1724]  eta: 1:06:19  lr: 0.000120  loss: 7.2712 (9.6685)  loss_n_40: 1.8013 (2.1785)  loss_n_60: 1.6281 (2.2486)  loss_n_80: 1.8327 (2.4638)  loss_n_100: 1.8296 (2.4771)  triple_100: 0.0000 (0.0658)  triple_80: 0.0000 (0.0714)  triple_60: 0.0000 (0.0355)  triple_40: 0.0000 (0.1278)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 720/1724]  eta: 1:05:39  lr: 0.000120  loss: 6.8647 (9.6293)  loss_n_40: 1.7692 (2.1719)  loss_n_60: 1.5910 (2.2392)  loss_n_80: 1.8010 (2.4546)  loss_n_100: 1.7433 (2.4673)  triple_100: 0.0000 (0.0649)  triple_80: 0.0000 (0.0704)  triple_60: 0.0000 (0.0350)  triple_40: 0.0000 (0.1260)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 730/1724]  eta: 1:05:00  lr: 0.000120  loss: 6.8193 (9.5914)  loss_n_40: 1.6949 (2.1657)  loss_n_60: 1.5187 (2.2297)  loss_n_80: 1.7943 (2.4452)  loss_n_100: 1.7298 (2.4574)  triple_100: 0.0000 (0.0640)  triple_80: 0.0000 (0.0694)  triple_60: 0.0000 (0.0356)  triple_40: 0.0000 (0.1243)  time: 3.9229  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [ 740/1724]  eta: 1:04:21  lr: 0.000120  loss: 6.7858 (9.5506)  loss_n_40: 1.6949 (2.1591)  loss_n_60: 1.4981 (2.2198)  loss_n_80: 1.6853 (2.4351)  loss_n_100: 1.6885 (2.4473)  triple_100: 0.0000 (0.0631)  triple_80: 0.0000 (0.0685)  triple_60: 0.0000 (0.0351)  triple_40: 0.0000 (0.1226)  time: 3.9237  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [ 750/1724]  eta: 1:03:42  lr: 0.000120  loss: 6.6948 (9.5138)  loss_n_40: 1.7495 (2.1535)  loss_n_60: 1.5371 (2.2108)  loss_n_80: 1.6571 (2.4253)  loss_n_100: 1.6577 (2.4368)  triple_100: 0.0000 (0.0623)  triple_80: 0.0000 (0.0690)  triple_60: 0.0000 (0.0347)  triple_40: 0.0000 (0.1215)  time: 3.9237  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [ 760/1724]  eta: 1:03:02  lr: 0.000120  loss: 6.8897 (9.4810)  loss_n_40: 1.7720 (2.1488)  loss_n_60: 1.5672 (2.2030)  loss_n_80: 1.7285 (2.4173)  loss_n_100: 1.6577 (2.4283)  triple_100: 0.0000 (0.0614)  triple_80: 0.0000 (0.0681)  triple_60: 0.0000 (0.0342)  triple_40: 0.0000 (0.1199)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 770/1724]  eta: 1:02:23  lr: 0.000120  loss: 6.5893 (9.4427)  loss_n_40: 1.7539 (2.1430)  loss_n_60: 1.5075 (2.1941)  loss_n_80: 1.6791 (2.4074)  loss_n_100: 1.6272 (2.4182)  triple_100: 0.0000 (0.0606)  triple_80: 0.0000 (0.0672)  triple_60: 0.0000 (0.0338)  triple_40: 0.0000 (0.1184)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 780/1724]  eta: 1:01:44  lr: 0.000120  loss: 6.5842 (9.4137)  loss_n_40: 1.7202 (2.1388)  loss_n_60: 1.5075 (2.1870)  loss_n_80: 1.6757 (2.4003)  loss_n_100: 1.6272 (2.4109)  triple_100: 0.0000 (0.0599)  triple_80: 0.0000 (0.0664)  triple_60: 0.0000 (0.0333)  triple_40: 0.0000 (0.1172)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 790/1724]  eta: 1:01:05  lr: 0.000120  loss: 6.8454 (9.3808)  loss_n_40: 1.7202 (2.1337)  loss_n_60: 1.5385 (2.1788)  loss_n_80: 1.7503 (2.3923)  loss_n_100: 1.7630 (2.4027)  triple_100: 0.0000 (0.0591)  triple_80: 0.0000 (0.0655)  triple_60: 0.0000 (0.0329)  triple_40: 0.0000 (0.1157)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 800/1724]  eta: 1:00:25  lr: 0.000120  loss: 6.5510 (9.3447)  loss_n_40: 1.6679 (2.1282)  loss_n_60: 1.4745 (2.1702)  loss_n_80: 1.6685 (2.3832)  loss_n_100: 1.6472 (2.3932)  triple_100: 0.0000 (0.0584)  triple_80: 0.0000 (0.0647)  triple_60: 0.0000 (0.0325)  triple_40: 0.0000 (0.1143)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 810/1724]  eta: 0:59:46  lr: 0.000120  loss: 6.2880 (9.3081)  loss_n_40: 1.6084 (2.1219)  loss_n_60: 1.4138 (2.1614)  loss_n_80: 1.6217 (2.3738)  loss_n_100: 1.6219 (2.3836)  triple_100: 0.0000 (0.0577)  triple_80: 0.0000 (0.0639)  triple_60: 0.0000 (0.0321)  triple_40: 0.0000 (0.1139)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 820/1724]  eta: 0:59:07  lr: 0.000120  loss: 6.3750 (9.2809)  loss_n_40: 1.6084 (2.1170)  loss_n_60: 1.4726 (2.1539)  loss_n_80: 1.6217 (2.3663)  loss_n_100: 1.6108 (2.3759)  triple_100: 0.0000 (0.0570)  triple_80: 0.0000 (0.0659)  triple_60: 0.0000 (0.0317)  triple_40: 0.0000 (0.1132)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 830/1724]  eta: 0:58:28  lr: 0.000120  loss: 6.9845 (9.2604)  loss_n_40: 1.7948 (2.1148)  loss_n_60: 1.5724 (2.1489)  loss_n_80: 1.7976 (2.3615)  loss_n_100: 1.7377 (2.3705)  triple_100: 0.0000 (0.0563)  triple_80: 0.0000 (0.0651)  triple_60: 0.0000 (0.0313)  triple_40: 0.0000 (0.1119)  time: 3.9231  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 840/1724]  eta: 0:57:48  lr: 0.000120  loss: 6.9845 (9.2278)  loss_n_40: 1.7980 (2.1103)  loss_n_60: 1.6056 (2.1408)  loss_n_80: 1.8116 (2.3534)  loss_n_100: 1.7111 (2.3611)  triple_100: 0.0000 (0.0556)  triple_80: 0.0000 (0.0644)  triple_60: 0.0000 (0.0310)  triple_40: 0.0000 (0.1113)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 850/1724]  eta: 0:57:09  lr: 0.000120  loss: 6.4854 (9.1965)  loss_n_40: 1.6624 (2.1050)  loss_n_60: 1.4631 (2.1332)  loss_n_80: 1.7014 (2.3457)  loss_n_100: 1.6168 (2.3529)  triple_100: 0.0000 (0.0549)  triple_80: 0.0000 (0.0636)  triple_60: 0.0000 (0.0306)  triple_40: 0.0000 (0.1105)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 860/1724]  eta: 0:56:30  lr: 0.000120  loss: 6.9039 (9.1781)  loss_n_40: 1.6521 (2.1007)  loss_n_60: 1.5240 (2.1267)  loss_n_80: 1.7588 (2.3392)  loss_n_100: 1.7090 (2.3465)  triple_100: 0.0000 (0.0543)  triple_80: 0.0000 (0.0662)  triple_60: 0.0000 (0.0302)  triple_40: 0.0000 (0.1143)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 870/1724]  eta: 0:55:51  lr: 0.000120  loss: 6.7100 (9.1516)  loss_n_40: 1.6465 (2.0956)  loss_n_60: 1.5357 (2.1208)  loss_n_80: 1.7588 (2.3331)  loss_n_100: 1.7229 (2.3400)  triple_100: 0.0000 (0.0537)  triple_80: 0.0000 (0.0655)  triple_60: 0.0000 (0.0299)  triple_40: 0.0000 (0.1130)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 880/1724]  eta: 0:55:11  lr: 0.000120  loss: 6.7000 (9.1254)  loss_n_40: 1.6630 (2.0910)  loss_n_60: 1.5749 (2.1153)  loss_n_80: 1.7582 (2.3271)  loss_n_100: 1.7023 (2.3330)  triple_100: 0.0000 (0.0531)  triple_80: 0.0000 (0.0647)  triple_60: 0.0000 (0.0296)  triple_40: 0.0000 (0.1117)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 890/1724]  eta: 0:54:32  lr: 0.000120  loss: 6.7000 (9.0963)  loss_n_40: 1.6571 (2.0859)  loss_n_60: 1.5749 (2.1091)  loss_n_80: 1.7335 (2.3203)  loss_n_100: 1.6575 (2.3248)  triple_100: 0.0000 (0.0525)  triple_80: 0.0000 (0.0640)  triple_60: 0.0000 (0.0292)  triple_40: 0.0000 (0.1104)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 900/1724]  eta: 0:53:53  lr: 0.000120  loss: 6.2292 (9.0638)  loss_n_40: 1.5760 (2.0801)  loss_n_60: 1.4838 (2.1018)  loss_n_80: 1.6574 (2.3124)  loss_n_100: 1.5442 (2.3163)  triple_100: 0.0000 (0.0519)  triple_80: 0.0000 (0.0633)  triple_60: 0.0000 (0.0289)  triple_40: 0.0000 (0.1092)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 910/1724]  eta: 0:53:14  lr: 0.000120  loss: 6.1505 (9.0342)  loss_n_40: 1.5323 (2.0750)  loss_n_60: 1.4629 (2.0952)  loss_n_80: 1.6247 (2.3050)  loss_n_100: 1.5270 (2.3085)  triple_100: 0.0000 (0.0513)  triple_80: 0.0000 (0.0626)  triple_60: 0.0000 (0.0286)  triple_40: 0.0000 (0.1080)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 920/1724]  eta: 0:52:35  lr: 0.000120  loss: 6.2300 (9.0036)  loss_n_40: 1.5308 (2.0683)  loss_n_60: 1.4440 (2.0875)  loss_n_80: 1.5872 (2.2969)  loss_n_100: 1.5446 (2.3000)  triple_100: 0.0000 (0.0508)  triple_80: 0.0000 (0.0641)  triple_60: 0.0000 (0.0291)  triple_40: 0.0000 (0.1068)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 930/1724]  eta: 0:51:55  lr: 0.000120  loss: 6.2300 (8.9749)  loss_n_40: 1.5402 (2.0634)  loss_n_60: 1.4440 (2.0808)  loss_n_80: 1.5857 (2.2888)  loss_n_100: 1.5446 (2.2917)  triple_100: 0.0000 (0.0502)  triple_80: 0.0000 (0.0635)  triple_60: 0.0000 (0.0309)  triple_40: 0.0000 (0.1057)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 940/1724]  eta: 0:51:16  lr: 0.000120  loss: 6.1688 (8.9465)  loss_n_40: 1.5220 (2.0577)  loss_n_60: 1.4580 (2.0742)  loss_n_80: 1.5962 (2.2817)  loss_n_100: 1.5916 (2.2847)  triple_100: 0.0000 (0.0497)  triple_80: 0.0000 (0.0632)  triple_60: 0.0000 (0.0306)  triple_40: 0.0000 (0.1047)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 950/1724]  eta: 0:50:37  lr: 0.000120  loss: 6.1354 (8.9165)  loss_n_40: 1.4714 (2.0514)  loss_n_60: 1.3942 (2.0672)  loss_n_80: 1.5879 (2.2740)  loss_n_100: 1.5916 (2.2774)  triple_100: 0.0000 (0.0492)  triple_80: 0.0000 (0.0626)  triple_60: 0.0000 (0.0303)  triple_40: 0.0000 (0.1046)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 960/1724]  eta: 0:49:58  lr: 0.000120  loss: 6.0769 (8.8856)  loss_n_40: 1.4420 (2.0453)  loss_n_60: 1.3864 (2.0600)  loss_n_80: 1.5264 (2.2659)  loss_n_100: 1.5565 (2.2694)  triple_100: 0.0000 (0.0487)  triple_80: 0.0000 (0.0622)  triple_60: 0.0000 (0.0299)  triple_40: 0.0000 (0.1042)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 970/1724]  eta: 0:49:18  lr: 0.000120  loss: 5.8791 (8.8557)  loss_n_40: 1.4153 (2.0394)  loss_n_60: 1.3547 (2.0532)  loss_n_80: 1.5281 (2.2586)  loss_n_100: 1.5271 (2.2621)  triple_100: 0.0000 (0.0482)  triple_80: 0.0000 (0.0615)  triple_60: 0.0000 (0.0296)  triple_40: 0.0000 (0.1031)  time: 3.9244  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [ 980/1724]  eta: 0:48:39  lr: 0.000120  loss: 5.7735 (8.8223)  loss_n_40: 1.4096 (2.0330)  loss_n_60: 1.3547 (2.0457)  loss_n_80: 1.5073 (2.2502)  loss_n_100: 1.5166 (2.2534)  triple_100: 0.0000 (0.0477)  triple_80: 0.0000 (0.0609)  triple_60: 0.0000 (0.0293)  triple_40: 0.0000 (0.1020)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [ 990/1724]  eta: 0:48:00  lr: 0.000120  loss: 5.6016 (8.7892)  loss_n_40: 1.4061 (2.0266)  loss_n_60: 1.3077 (2.0380)  loss_n_80: 1.4415 (2.2418)  loss_n_100: 1.3909 (2.2449)  triple_100: 0.0000 (0.0472)  triple_80: 0.0000 (0.0603)  triple_60: 0.0000 (0.0295)  triple_40: 0.0000 (0.1010)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1000/1724]  eta: 0:47:21  lr: 0.000120  loss: 5.3252 (8.7547)  loss_n_40: 1.3235 (2.0194)  loss_n_60: 1.2606 (2.0299)  loss_n_80: 1.3893 (2.2334)  loss_n_100: 1.3695 (2.2363)  triple_100: 0.0000 (0.0467)  triple_80: 0.0000 (0.0597)  triple_60: 0.0000 (0.0292)  triple_40: 0.0000 (0.1000)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1010/1724]  eta: 0:46:41  lr: 0.000120  loss: 5.3131 (8.7206)  loss_n_40: 1.3168 (2.0124)  loss_n_60: 1.2273 (2.0221)  loss_n_80: 1.3893 (2.2250)  loss_n_100: 1.3886 (2.2278)  triple_100: 0.0000 (0.0463)  triple_80: 0.0000 (0.0591)  triple_60: 0.0000 (0.0289)  triple_40: 0.0000 (0.0990)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1020/1724]  eta: 0:46:02  lr: 0.000120  loss: 5.0336 (8.6839)  loss_n_40: 1.2572 (2.0049)  loss_n_60: 1.1723 (2.0136)  loss_n_80: 1.3065 (2.2158)  loss_n_100: 1.2803 (2.2185)  triple_100: 0.0000 (0.0458)  triple_80: 0.0000 (0.0585)  triple_60: 0.0000 (0.0286)  triple_40: 0.0000 (0.0981)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1030/1724]  eta: 0:45:23  lr: 0.000120  loss: 5.0336 (8.6529)  loss_n_40: 1.2414 (1.9989)  loss_n_60: 1.1354 (2.0068)  loss_n_80: 1.3065 (2.2077)  loss_n_100: 1.3054 (2.2106)  triple_100: 0.0000 (0.0454)  triple_80: 0.0000 (0.0580)  triple_60: 0.0000 (0.0284)  triple_40: 0.0000 (0.0973)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1040/1724]  eta: 0:44:44  lr: 0.000120  loss: 5.2506 (8.6230)  loss_n_40: 1.3452 (1.9924)  loss_n_60: 1.2559 (1.9994)  loss_n_80: 1.3551 (2.1996)  loss_n_100: 1.3604 (2.2024)  triple_100: 0.0000 (0.0449)  triple_80: 0.0000 (0.0574)  triple_60: 0.0000 (0.0301)  triple_40: 0.0000 (0.0968)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1050/1724]  eta: 0:44:04  lr: 0.000120  loss: 5.3913 (8.5954)  loss_n_40: 1.3509 (1.9872)  loss_n_60: 1.2724 (1.9932)  loss_n_80: 1.3808 (2.1922)  loss_n_100: 1.3669 (2.1948)  triple_100: 0.0000 (0.0445)  triple_80: 0.0000 (0.0578)  triple_60: 0.0000 (0.0298)  triple_40: 0.0000 (0.0959)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1060/1724]  eta: 0:43:25  lr: 0.000120  loss: 5.3790 (8.5666)  loss_n_40: 1.4274 (1.9818)  loss_n_60: 1.2900 (1.9868)  loss_n_80: 1.3904 (2.1848)  loss_n_100: 1.3739 (2.1874)  triple_100: 0.0000 (0.0441)  triple_80: 0.0000 (0.0572)  triple_60: 0.0000 (0.0295)  triple_40: 0.0000 (0.0949)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1070/1724]  eta: 0:42:46  lr: 0.000120  loss: 5.3572 (8.5364)  loss_n_40: 1.3359 (1.9759)  loss_n_60: 1.2733 (1.9801)  loss_n_80: 1.3627 (2.1770)  loss_n_100: 1.3791 (2.1798)  triple_100: 0.0000 (0.0437)  triple_80: 0.0000 (0.0567)  triple_60: 0.0000 (0.0292)  triple_40: 0.0000 (0.0941)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1080/1724]  eta: 0:42:07  lr: 0.000120  loss: 5.1747 (8.5048)  loss_n_40: 1.2622 (1.9691)  loss_n_60: 1.2099 (1.9729)  loss_n_80: 1.3410 (2.1691)  loss_n_100: 1.3781 (2.1722)  triple_100: 0.0000 (0.0433)  triple_80: 0.0000 (0.0562)  triple_60: 0.0000 (0.0290)  triple_40: 0.0000 (0.0932)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1090/1724]  eta: 0:41:27  lr: 0.000120  loss: 5.1266 (8.4764)  loss_n_40: 1.2355 (1.9626)  loss_n_60: 1.1775 (1.9656)  loss_n_80: 1.3211 (2.1612)  loss_n_100: 1.3576 (2.1647)  triple_100: 0.0000 (0.0442)  triple_80: 0.0000 (0.0561)  triple_60: 0.0000 (0.0287)  triple_40: 0.0000 (0.0933)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1100/1724]  eta: 0:40:48  lr: 0.000120  loss: 5.6951 (8.4524)  loss_n_40: 1.3516 (1.9577)  loss_n_60: 1.3085 (1.9603)  loss_n_80: 1.4258 (2.1552)  loss_n_100: 1.4465 (2.1586)  triple_100: 0.0000 (0.0438)  triple_80: 0.0000 (0.0558)  triple_60: 0.0000 (0.0284)  triple_40: 0.0000 (0.0925)  time: 3.9214  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1110/1724]  eta: 0:40:09  lr: 0.000120  loss: 5.3723 (8.4224)  loss_n_40: 1.3394 (1.9513)  loss_n_60: 1.2927 (1.9532)  loss_n_80: 1.4053 (2.1478)  loss_n_100: 1.4393 (2.1516)  triple_100: 0.0000 (0.0434)  triple_80: 0.0000 (0.0553)  triple_60: 0.0000 (0.0282)  triple_40: 0.0000 (0.0916)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1120/1724]  eta: 0:39:30  lr: 0.000120  loss: 4.9266 (8.3899)  loss_n_40: 1.2184 (1.9448)  loss_n_60: 1.1207 (1.9457)  loss_n_80: 1.2533 (2.1395)  loss_n_100: 1.2878 (2.1433)  triple_100: 0.0000 (0.0430)  triple_80: 0.0000 (0.0548)  triple_60: 0.0000 (0.0279)  triple_40: 0.0000 (0.0908)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1130/1724]  eta: 0:38:50  lr: 0.000120  loss: 4.5365 (8.3549)  loss_n_40: 1.1026 (1.9370)  loss_n_60: 1.0321 (1.9375)  loss_n_80: 1.1746 (2.1309)  loss_n_100: 1.2217 (2.1349)  triple_100: 0.0000 (0.0427)  triple_80: 0.0000 (0.0543)  triple_60: 0.0000 (0.0277)  triple_40: 0.0000 (0.0900)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1140/1724]  eta: 0:38:11  lr: 0.000120  loss: 4.3471 (8.3194)  loss_n_40: 1.0294 (1.9291)  loss_n_60: 1.0123 (1.9291)  loss_n_80: 1.1127 (2.1219)  loss_n_100: 1.1858 (2.1264)  triple_100: 0.0000 (0.0423)  triple_80: 0.0000 (0.0539)  triple_60: 0.0000 (0.0274)  triple_40: 0.0000 (0.0892)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1150/1724]  eta: 0:37:32  lr: 0.000120  loss: 4.0170 (8.2822)  loss_n_40: 0.9836 (1.9210)  loss_n_60: 0.9135 (1.9204)  loss_n_80: 1.0195 (2.1124)  loss_n_100: 1.1101 (2.1174)  triple_100: 0.0000 (0.0419)  triple_80: 0.0000 (0.0534)  triple_60: 0.0000 (0.0272)  triple_40: 0.0000 (0.0884)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1160/1724]  eta: 0:36:53  lr: 0.000120  loss: 4.0422 (8.2622)  loss_n_40: 0.9576 (1.9154)  loss_n_60: 0.9347 (1.9149)  loss_n_80: 1.0282 (2.1068)  loss_n_100: 1.1117 (2.1119)  triple_100: 0.0000 (0.0434)  triple_80: 0.0000 (0.0537)  triple_60: 0.0000 (0.0281)  triple_40: 0.0000 (0.0882)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1170/1724]  eta: 0:36:13  lr: 0.000120  loss: 9.1809 (8.2858)  loss_n_40: 1.7388 (1.9175)  loss_n_60: 2.0817 (1.9198)  loss_n_80: 2.5571 (2.1155)  loss_n_100: 2.4358 (2.1202)  triple_100: 0.0000 (0.0430)  triple_80: 0.0000 (0.0545)  triple_60: 0.0000 (0.0278)  triple_40: 0.0000 (0.0874)  time: 3.9245  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1180/1724]  eta: 0:35:34  lr: 0.000120  loss: 10.2175 (8.2997)  loss_n_40: 2.0493 (1.9186)  loss_n_60: 2.2556 (1.9218)  loss_n_80: 2.9732 (2.1221)  loss_n_100: 2.9228 (2.1262)  triple_100: 0.0000 (0.0426)  triple_80: 0.0000 (0.0540)  triple_60: 0.0000 (0.0276)  triple_40: 0.0000 (0.0867)  time: 3.9250  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1190/1724]  eta: 0:34:55  lr: 0.000120  loss: 8.6574 (8.2973)  loss_n_40: 1.9771 (1.9185)  loss_n_60: 1.8386 (1.9201)  loss_n_80: 2.4481 (2.1229)  loss_n_100: 2.3691 (2.1265)  triple_100: 0.0000 (0.0426)  triple_80: 0.0000 (0.0536)  triple_60: 0.0000 (0.0274)  triple_40: 0.0000 (0.0859)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1200/1724]  eta: 0:34:16  lr: 0.000120  loss: 7.5290 (8.2879)  loss_n_40: 1.7406 (1.9159)  loss_n_60: 1.6457 (1.9174)  loss_n_80: 2.0432 (2.1217)  loss_n_100: 2.0315 (2.1252)  triple_100: 0.0000 (0.0422)  triple_80: 0.0000 (0.0531)  triple_60: 0.0000 (0.0271)  triple_40: 0.0000 (0.0852)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1210/1724]  eta: 0:33:37  lr: 0.000120  loss: 6.8012 (8.2774)  loss_n_40: 1.4967 (1.9123)  loss_n_60: 1.6003 (1.9147)  loss_n_80: 1.8655 (2.1189)  loss_n_100: 1.9361 (2.1228)  triple_100: 0.0000 (0.0419)  triple_80: 0.0000 (0.0527)  triple_60: 0.0000 (0.0297)  triple_40: 0.0000 (0.0845)  time: 3.9249  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [1220/1724]  eta: 0:32:57  lr: 0.000120  loss: 6.4468 (8.2583)  loss_n_40: 1.3550 (1.9079)  loss_n_60: 1.4915 (1.9104)  loss_n_80: 1.6782 (2.1143)  loss_n_100: 1.6657 (2.1187)  triple_100: 0.0000 (0.0415)  triple_80: 0.0000 (0.0523)  triple_60: 0.0000 (0.0294)  triple_40: 0.0000 (0.0838)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1230/1724]  eta: 0:32:18  lr: 0.000120  loss: 5.7345 (8.2376)  loss_n_40: 1.2399 (1.9025)  loss_n_60: 1.2893 (1.9054)  loss_n_80: 1.4731 (2.1089)  loss_n_100: 1.5437 (2.1139)  triple_100: 0.0000 (0.0412)  triple_80: 0.0000 (0.0518)  triple_60: 0.0000 (0.0292)  triple_40: 0.0000 (0.0847)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1240/1724]  eta: 0:31:39  lr: 0.000120  loss: 5.3250 (8.2116)  loss_n_40: 1.1418 (1.8965)  loss_n_60: 1.2340 (1.8993)  loss_n_80: 1.3860 (2.1027)  loss_n_100: 1.4543 (2.1079)  triple_100: 0.0000 (0.0408)  triple_80: 0.0000 (0.0514)  triple_60: 0.0000 (0.0289)  triple_40: 0.0000 (0.0840)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1250/1724]  eta: 0:31:00  lr: 0.000120  loss: 4.8012 (8.1841)  loss_n_40: 1.1001 (1.8898)  loss_n_60: 1.0817 (1.8929)  loss_n_80: 1.3286 (2.0964)  loss_n_100: 1.2966 (2.1014)  triple_100: 0.0000 (0.0405)  triple_80: 0.0000 (0.0510)  triple_60: 0.0000 (0.0288)  triple_40: 0.0000 (0.0833)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1260/1724]  eta: 0:30:20  lr: 0.000120  loss: 5.0614 (8.1632)  loss_n_40: 1.2332 (1.8861)  loss_n_60: 1.1085 (1.8876)  loss_n_80: 1.3488 (2.0910)  loss_n_100: 1.3212 (2.0961)  triple_100: 0.0000 (0.0402)  triple_80: 0.0000 (0.0506)  triple_60: 0.0000 (0.0289)  triple_40: 0.0000 (0.0827)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1270/1724]  eta: 0:29:41  lr: 0.000120  loss: 5.0844 (8.1372)  loss_n_40: 1.3193 (1.8806)  loss_n_60: 1.1532 (1.8816)  loss_n_80: 1.3258 (2.0842)  loss_n_100: 1.3017 (2.0897)  triple_100: 0.0000 (0.0399)  triple_80: 0.0000 (0.0502)  triple_60: 0.0000 (0.0287)  triple_40: 0.0000 (0.0822)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1280/1724]  eta: 0:29:02  lr: 0.000120  loss: 4.7964 (8.1119)  loss_n_40: 1.0863 (1.8750)  loss_n_60: 1.0888 (1.8754)  loss_n_80: 1.2443 (2.0776)  loss_n_100: 1.2699 (2.0832)  triple_100: 0.0000 (0.0396)  triple_80: 0.0000 (0.0505)  triple_60: 0.0000 (0.0285)  triple_40: 0.0000 (0.0822)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1290/1724]  eta: 0:28:23  lr: 0.000120  loss: 4.7964 (8.0887)  loss_n_40: 1.1761 (1.8694)  loss_n_60: 1.0845 (1.8701)  loss_n_80: 1.2443 (2.0717)  loss_n_100: 1.2657 (2.0781)  triple_100: 0.0000 (0.0393)  triple_80: 0.0000 (0.0503)  triple_60: 0.0000 (0.0282)  triple_40: 0.0000 (0.0816)  time: 3.9241  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1300/1724]  eta: 0:27:43  lr: 0.000120  loss: 6.4117 (8.1033)  loss_n_40: 1.3327 (1.8686)  loss_n_60: 1.4718 (1.8709)  loss_n_80: 1.7833 (2.0735)  loss_n_100: 1.9240 (2.0811)  triple_100: 0.0000 (0.0403)  triple_80: 0.0000 (0.0528)  triple_60: 0.0000 (0.0350)  triple_40: 0.0000 (0.0809)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1310/1724]  eta: 0:27:04  lr: 0.000120  loss: 10.1325 (8.1203)  loss_n_40: 2.0321 (1.8726)  loss_n_60: 2.2731 (1.8759)  loss_n_80: 2.5570 (2.0785)  loss_n_100: 2.6244 (2.0859)  triple_100: 0.0000 (0.0400)  triple_80: 0.0000 (0.0524)  triple_60: 0.0000 (0.0348)  triple_40: 0.0000 (0.0803)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1320/1724]  eta: 0:26:25  lr: 0.000120  loss: 10.1325 (8.1300)  loss_n_40: 2.2041 (1.8736)  loss_n_60: 2.4183 (1.8786)  loss_n_80: 2.5732 (2.0809)  loss_n_100: 2.5929 (2.0875)  triple_100: 0.0000 (0.0397)  triple_80: 0.0000 (0.0542)  triple_60: 0.0000 (0.0359)  triple_40: 0.0000 (0.0797)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1330/1724]  eta: 0:25:46  lr: 0.000120  loss: 8.8453 (8.1327)  loss_n_40: 1.9287 (1.8740)  loss_n_60: 2.2132 (1.8807)  loss_n_80: 2.2873 (2.0821)  loss_n_100: 2.3048 (2.0880)  triple_100: 0.0000 (0.0394)  triple_80: 0.0000 (0.0538)  triple_60: 0.0000 (0.0356)  triple_40: 0.0000 (0.0791)  time: 3.9220  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1340/1724]  eta: 0:25:06  lr: 0.000120  loss: 8.2062 (8.1289)  loss_n_40: 1.8299 (1.8729)  loss_n_60: 2.1386 (1.8812)  loss_n_80: 2.1680 (2.0813)  loss_n_100: 2.1121 (2.0872)  triple_100: 0.0000 (0.0391)  triple_80: 0.0000 (0.0534)  triple_60: 0.0000 (0.0353)  triple_40: 0.0000 (0.0785)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1350/1724]  eta: 0:24:27  lr: 0.000120  loss: 7.1673 (8.1201)  loss_n_40: 1.6733 (1.8715)  loss_n_60: 1.8482 (1.8805)  loss_n_80: 1.8789 (2.0792)  loss_n_100: 1.6998 (2.0842)  triple_100: 0.0000 (0.0388)  triple_80: 0.0000 (0.0530)  triple_60: 0.0000 (0.0351)  triple_40: 0.0000 (0.0779)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1360/1724]  eta: 0:23:48  lr: 0.000120  loss: 6.5518 (8.1061)  loss_n_40: 1.5292 (1.8681)  loss_n_60: 1.6810 (1.8781)  loss_n_80: 1.6985 (2.0757)  loss_n_100: 1.6266 (2.0807)  triple_100: 0.0000 (0.0385)  triple_80: 0.0000 (0.0528)  triple_60: 0.0000 (0.0348)  triple_40: 0.0000 (0.0774)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1370/1724]  eta: 0:23:09  lr: 0.000120  loss: 5.8856 (8.0884)  loss_n_40: 1.3351 (1.8641)  loss_n_60: 1.4774 (1.8746)  loss_n_80: 1.4862 (2.0711)  loss_n_100: 1.4747 (2.0765)  triple_100: 0.0000 (0.0382)  triple_80: 0.0000 (0.0524)  triple_60: 0.0000 (0.0346)  triple_40: 0.0000 (0.0768)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1380/1724]  eta: 0:22:29  lr: 0.000120  loss: 5.3671 (8.0674)  loss_n_40: 1.2536 (1.8598)  loss_n_60: 1.3006 (1.8701)  loss_n_80: 1.3169 (2.0654)  loss_n_100: 1.4686 (2.0716)  triple_100: 0.0000 (0.0380)  triple_80: 0.0000 (0.0520)  triple_60: 0.0000 (0.0343)  triple_40: 0.0000 (0.0763)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1390/1724]  eta: 0:21:50  lr: 0.000120  loss: 4.9876 (8.0442)  loss_n_40: 1.1487 (1.8546)  loss_n_60: 1.1729 (1.8652)  loss_n_80: 1.2291 (2.0592)  loss_n_100: 1.3666 (2.0661)  triple_100: 0.0000 (0.0377)  triple_80: 0.0000 (0.0516)  triple_60: 0.0000 (0.0341)  triple_40: 0.0000 (0.0757)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1400/1724]  eta: 0:21:11  lr: 0.000120  loss: 4.8285 (8.0261)  loss_n_40: 1.1104 (1.8500)  loss_n_60: 1.1684 (1.8608)  loss_n_80: 1.1909 (2.0542)  loss_n_100: 1.3627 (2.0618)  triple_100: 0.0000 (0.0386)  triple_80: 0.0000 (0.0516)  triple_60: 0.0000 (0.0338)  triple_40: 0.0000 (0.0752)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1410/1724]  eta: 0:20:32  lr: 0.000120  loss: 5.9454 (8.0181)  loss_n_40: 1.3727 (1.8476)  loss_n_60: 1.4908 (1.8591)  loss_n_80: 1.5904 (2.0528)  loss_n_100: 1.5949 (2.0607)  triple_100: 0.0000 (0.0383)  triple_80: 0.0000 (0.0513)  triple_60: 0.0000 (0.0336)  triple_40: 0.0000 (0.0747)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1420/1724]  eta: 0:19:52  lr: 0.000120  loss: 6.4423 (8.0051)  loss_n_40: 1.4302 (1.8440)  loss_n_60: 1.5097 (1.8565)  loss_n_80: 1.6421 (2.0496)  loss_n_100: 1.7207 (2.0578)  triple_100: 0.0000 (0.0380)  triple_80: 0.0000 (0.0509)  triple_60: 0.0000 (0.0339)  triple_40: 0.0000 (0.0743)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1430/1724]  eta: 0:19:13  lr: 0.000120  loss: 6.1361 (7.9896)  loss_n_40: 1.2499 (1.8405)  loss_n_60: 1.4449 (1.8535)  loss_n_80: 1.5036 (2.0456)  loss_n_100: 1.5853 (2.0543)  triple_100: 0.0000 (0.0378)  triple_80: 0.0000 (0.0506)  triple_60: 0.0000 (0.0336)  triple_40: 0.0000 (0.0738)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1440/1724]  eta: 0:18:34  lr: 0.000120  loss: 5.3630 (7.9704)  loss_n_40: 1.1493 (1.8356)  loss_n_60: 1.3042 (1.8495)  loss_n_80: 1.3763 (2.0408)  loss_n_100: 1.4579 (2.0502)  triple_100: 0.0000 (0.0375)  triple_80: 0.0000 (0.0502)  triple_60: 0.0000 (0.0334)  triple_40: 0.0000 (0.0733)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1450/1724]  eta: 0:17:55  lr: 0.000120  loss: 5.5484 (7.9573)  loss_n_40: 1.1263 (1.8316)  loss_n_60: 1.3583 (1.8468)  loss_n_80: 1.4476 (2.0377)  loss_n_100: 1.5134 (2.0476)  triple_100: 0.0000 (0.0376)  triple_80: 0.0000 (0.0501)  triple_60: 0.0000 (0.0332)  triple_40: 0.0000 (0.0728)  time: 3.9247  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [1460/1724]  eta: 0:17:15  lr: 0.000120  loss: 5.9353 (7.9413)  loss_n_40: 1.2367 (1.8281)  loss_n_60: 1.4262 (1.8436)  loss_n_80: 1.5299 (2.0337)  loss_n_100: 1.5134 (2.0436)  triple_100: 0.0000 (0.0373)  triple_80: 0.0000 (0.0498)  triple_60: 0.0000 (0.0329)  triple_40: 0.0000 (0.0723)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1470/1724]  eta: 0:16:36  lr: 0.000120  loss: 5.1822 (7.9220)  loss_n_40: 1.1662 (1.8235)  loss_n_60: 1.2501 (1.8395)  loss_n_80: 1.3504 (2.0289)  loss_n_100: 1.3835 (2.0390)  triple_100: 0.0000 (0.0371)  triple_80: 0.0000 (0.0494)  triple_60: 0.0000 (0.0327)  triple_40: 0.0000 (0.0718)  time: 3.9241  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1480/1724]  eta: 0:15:57  lr: 0.000120  loss: 4.8611 (7.9013)  loss_n_40: 1.0101 (1.8181)  loss_n_60: 1.1853 (1.8351)  loss_n_80: 1.3144 (2.0239)  loss_n_100: 1.3618 (2.0343)  triple_100: 0.0000 (0.0368)  triple_80: 0.0000 (0.0492)  triple_60: 0.0000 (0.0326)  triple_40: 0.0000 (0.0713)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1490/1724]  eta: 0:15:18  lr: 0.000120  loss: 4.8582 (7.8806)  loss_n_40: 1.0131 (1.8129)  loss_n_60: 1.1853 (1.8306)  loss_n_80: 1.3068 (2.0190)  loss_n_100: 1.3335 (2.0296)  triple_100: 0.0000 (0.0366)  triple_80: 0.0000 (0.0489)  triple_60: 0.0000 (0.0323)  triple_40: 0.0000 (0.0708)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1500/1724]  eta: 0:14:38  lr: 0.000120  loss: 4.8233 (7.8626)  loss_n_40: 1.0153 (1.8084)  loss_n_60: 1.1846 (1.8268)  loss_n_80: 1.3055 (2.0144)  loss_n_100: 1.3335 (2.0251)  triple_100: 0.0000 (0.0363)  triple_80: 0.0000 (0.0488)  triple_60: 0.0000 (0.0323)  triple_40: 0.0000 (0.0704)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1510/1724]  eta: 0:13:59  lr: 0.000120  loss: 4.8233 (7.8442)  loss_n_40: 0.9694 (1.8035)  loss_n_60: 1.1637 (1.8225)  loss_n_80: 1.2971 (2.0097)  loss_n_100: 1.3465 (2.0208)  triple_100: 0.0000 (0.0367)  triple_80: 0.0000 (0.0489)  triple_60: 0.0000 (0.0321)  triple_40: 0.0000 (0.0699)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1520/1724]  eta: 0:13:20  lr: 0.000120  loss: 4.8781 (7.8252)  loss_n_40: 0.9903 (1.7991)  loss_n_60: 1.1378 (1.8185)  loss_n_80: 1.3208 (2.0049)  loss_n_100: 1.3465 (2.0162)  triple_100: 0.0000 (0.0365)  triple_80: 0.0000 (0.0486)  triple_60: 0.0000 (0.0319)  triple_40: 0.0000 (0.0694)  time: 3.9236  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1530/1724]  eta: 0:12:41  lr: 0.000120  loss: 4.5141 (7.8044)  loss_n_40: 0.9462 (1.7935)  loss_n_60: 1.1123 (1.8140)  loss_n_80: 1.1967 (1.9994)  loss_n_100: 1.2495 (2.0111)  triple_100: 0.0000 (0.0368)  triple_80: 0.0000 (0.0486)  triple_60: 0.0000 (0.0320)  triple_40: 0.0000 (0.0690)  time: 3.9243  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1540/1724]  eta: 0:12:02  lr: 0.000120  loss: 4.3911 (7.7844)  loss_n_40: 0.9817 (1.7890)  loss_n_60: 1.1123 (1.8100)  loss_n_80: 1.1157 (1.9943)  loss_n_100: 1.2144 (2.0060)  triple_100: 0.0000 (0.0365)  triple_80: 0.0000 (0.0483)  triple_60: 0.0000 (0.0318)  triple_40: 0.0000 (0.0685)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1550/1724]  eta: 0:11:22  lr: 0.000120  loss: 4.3714 (7.7622)  loss_n_40: 0.9975 (1.7839)  loss_n_60: 1.1425 (1.8054)  loss_n_80: 1.1079 (1.9885)  loss_n_100: 1.1775 (2.0005)  triple_100: 0.0000 (0.0363)  triple_80: 0.0000 (0.0480)  triple_60: 0.0000 (0.0316)  triple_40: 0.0000 (0.0681)  time: 3.9242  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1560/1724]  eta: 0:10:43  lr: 0.000120  loss: 4.1242 (7.7398)  loss_n_40: 0.8956 (1.7785)  loss_n_60: 1.0520 (1.8007)  loss_n_80: 1.0516 (1.9829)  loss_n_100: 1.1231 (1.9949)  triple_100: 0.0000 (0.0361)  triple_80: 0.0000 (0.0477)  triple_60: 0.0000 (0.0314)  triple_40: 0.0000 (0.0677)  time: 3.9253  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1570/1724]  eta: 0:10:04  lr: 0.000120  loss: 3.9763 (7.7156)  loss_n_40: 0.8548 (1.7730)  loss_n_60: 1.0040 (1.7956)  loss_n_80: 1.0155 (1.9768)  loss_n_100: 1.0515 (1.9886)  triple_100: 0.0000 (0.0358)  triple_80: 0.0000 (0.0474)  triple_60: 0.0000 (0.0312)  triple_40: 0.0000 (0.0672)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1580/1724]  eta: 0:09:25  lr: 0.000120  loss: 3.8140 (7.6913)  loss_n_40: 0.8144 (1.7671)  loss_n_60: 0.9466 (1.7903)  loss_n_80: 1.0052 (1.9707)  loss_n_100: 1.0488 (1.9828)  triple_100: 0.0000 (0.0356)  triple_80: 0.0000 (0.0471)  triple_60: 0.0000 (0.0310)  triple_40: 0.0000 (0.0668)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1590/1724]  eta: 0:08:45  lr: 0.000120  loss: 4.5154 (7.7368)  loss_n_40: 0.8423 (1.7705)  loss_n_60: 1.0153 (1.7937)  loss_n_80: 1.1034 (1.9745)  loss_n_100: 1.1418 (1.9856)  triple_100: 0.0000 (0.0394)  triple_80: 0.0000 (0.0689)  triple_60: 0.0000 (0.0367)  triple_40: 0.0000 (0.0675)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1600/1724]  eta: 0:08:06  lr: 0.000120  loss: 14.2672 (7.7767)  loss_n_40: 3.5441 (1.7828)  loss_n_60: 3.2862 (1.8041)  loss_n_80: 3.3036 (1.9841)  loss_n_100: 3.0531 (1.9930)  triple_100: 0.0000 (0.0392)  triple_80: 0.0000 (0.0700)  triple_60: 0.0000 (0.0364)  triple_40: 0.0000 (0.0671)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1610/1724]  eta: 0:07:27  lr: 0.000120  loss: 13.8088 (7.8125)  loss_n_40: 3.6953 (1.7943)  loss_n_60: 3.3389 (1.8133)  loss_n_80: 3.4771 (1.9928)  loss_n_100: 3.2060 (2.0003)  triple_100: 0.0000 (0.0389)  triple_80: 0.0000 (0.0695)  triple_60: 0.0000 (0.0362)  triple_40: 0.0000 (0.0671)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1620/1724]  eta: 0:06:48  lr: 0.000120  loss: 12.5648 (7.8382)  loss_n_40: 3.2611 (1.8017)  loss_n_60: 3.1320 (1.8205)  loss_n_80: 3.2070 (1.9994)  loss_n_100: 3.0497 (2.0058)  triple_100: 0.0000 (0.0387)  triple_80: 0.0000 (0.0691)  triple_60: 0.0000 (0.0360)  triple_40: 0.0000 (0.0671)  time: 3.9231  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1630/1724]  eta: 0:06:08  lr: 0.000120  loss: 10.9657 (7.8556)  loss_n_40: 2.7057 (1.8063)  loss_n_60: 2.6957 (1.8256)  loss_n_80: 2.8988 (2.0046)  loss_n_100: 2.7587 (2.0096)  triple_100: 0.0000 (0.0384)  triple_80: 0.0000 (0.0687)  triple_60: 0.0000 (0.0358)  triple_40: 0.0000 (0.0666)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1640/1724]  eta: 0:05:29  lr: 0.000120  loss: 10.7229 (7.8698)  loss_n_40: 2.4245 (1.8096)  loss_n_60: 2.6247 (1.8299)  loss_n_80: 2.8607 (2.0092)  loss_n_100: 2.6053 (2.0129)  triple_100: 0.0000 (0.0382)  triple_80: 0.0000 (0.0683)  triple_60: 0.0000 (0.0355)  triple_40: 0.0000 (0.0662)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1650/1724]  eta: 0:04:50  lr: 0.000120  loss: 9.7921 (7.8812)  loss_n_40: 2.3120 (1.8122)  loss_n_60: 2.4801 (1.8333)  loss_n_80: 2.7034 (2.0131)  loss_n_100: 2.3800 (2.0153)  triple_100: 0.0000 (0.0380)  triple_80: 0.0000 (0.0681)  triple_60: 0.0000 (0.0353)  triple_40: 0.0000 (0.0658)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1660/1724]  eta: 0:04:11  lr: 0.000120  loss: 10.2151 (7.8959)  loss_n_40: 2.3708 (1.8157)  loss_n_60: 2.4854 (1.8374)  loss_n_80: 2.7034 (2.0179)  loss_n_100: 2.5123 (2.0190)  triple_100: 0.0000 (0.0377)  triple_80: 0.0000 (0.0677)  triple_60: 0.0000 (0.0351)  triple_40: 0.0000 (0.0654)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1670/1724]  eta: 0:03:31  lr: 0.000120  loss: 10.2345 (7.9066)  loss_n_40: 2.3390 (1.8179)  loss_n_60: 2.4288 (1.8402)  loss_n_80: 2.7862 (2.0218)  loss_n_100: 2.5936 (2.0219)  triple_100: 0.0000 (0.0375)  triple_80: 0.0000 (0.0673)  triple_60: 0.0000 (0.0349)  triple_40: 0.0000 (0.0650)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1680/1724]  eta: 0:02:52  lr: 0.000120  loss: 9.6061 (7.9176)  loss_n_40: 2.1655 (1.8204)  loss_n_60: 2.2274 (1.8430)  loss_n_80: 2.5837 (2.0254)  loss_n_100: 2.4640 (2.0249)  triple_100: 0.0000 (0.0373)  triple_80: 0.0000 (0.0672)  triple_60: 0.0000 (0.0347)  triple_40: 0.0000 (0.0647)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1690/1724]  eta: 0:02:13  lr: 0.000120  loss: 9.2318 (7.9245)  loss_n_40: 2.1018 (1.8220)  loss_n_60: 2.1999 (1.8449)  loss_n_80: 2.4870 (2.0278)  loss_n_100: 2.4229 (2.0271)  triple_100: 0.0000 (0.0371)  triple_80: 0.0000 (0.0668)  triple_60: 0.0000 (0.0345)  triple_40: 0.0000 (0.0643)  time: 3.9243  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [1700/1724]  eta: 0:01:34  lr: 0.000120  loss: 8.6510 (7.9299)  loss_n_40: 2.0240 (1.8235)  loss_n_60: 2.0714 (1.8464)  loss_n_80: 2.2982 (2.0297)  loss_n_100: 2.3377 (2.0287)  triple_100: 0.0000 (0.0369)  triple_80: 0.0000 (0.0665)  triple_60: 0.0000 (0.0343)  triple_40: 0.0000 (0.0639)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:7]  [1710/1724]  eta: 0:00:54  lr: 0.000120  loss: 8.6205 (7.9349)  loss_n_40: 2.0240 (1.8250)  loss_n_60: 2.0191 (1.8479)  loss_n_80: 2.2982 (2.0316)  loss_n_100: 2.2256 (2.0301)  triple_100: 0.0000 (0.0366)  triple_80: 0.0000 (0.0661)  triple_60: 0.0000 (0.0341)  triple_40: 0.0000 (0.0635)  time: 3.9244  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1720/1724]  eta: 0:00:15  lr: 0.000120  loss: 8.5247 (7.9398)  loss_n_40: 1.9927 (1.8264)  loss_n_60: 2.0636 (1.8495)  loss_n_80: 2.3242 (2.0334)  loss_n_100: 2.2256 (2.0313)  triple_100: 0.0000 (0.0364)  triple_80: 0.0000 (0.0657)  triple_60: 0.0000 (0.0339)  triple_40: 0.0000 (0.0632)  time: 3.9247  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7]  [1723/1724]  eta: 0:00:03  lr: 0.000120  loss: 8.4081 (7.9414)  loss_n_40: 1.9808 (1.8267)  loss_n_60: 2.0191 (1.8501)  loss_n_80: 2.2784 (2.0341)  loss_n_100: 2.2016 (2.0317)  triple_100: 0.0000 (0.0364)  triple_80: 0.0000 (0.0656)  triple_60: 0.0000 (0.0338)  triple_40: 0.0000 (0.0631)  time: 3.9246  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:7] Total time: 1:52:45 (3.9242 s / it)\n",
      "Averaged stats: lr: 0.000120  loss: 8.4081 (7.9414)  loss_n_40: 1.9808 (1.8267)  loss_n_60: 2.0191 (1.8501)  loss_n_80: 2.2784 (2.0341)  loss_n_100: 2.2016 (2.0317)  triple_100: 0.0000 (0.0364)  triple_80: 0.0000 (0.0656)  triple_60: 0.0000 (0.0338)  triple_40: 0.0000 (0.0631)\n",
      "Valid: [epoch:7]  [  0/845]  eta: 0:11:11  loss: 9.2058 (9.2058)  loss_n_40: 2.1161 (2.1161)  loss_n_60: 2.1553 (2.1553)  loss_n_80: 2.5099 (2.5099)  loss_n_100: 2.4245 (2.4245)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7941  data: 0.4608  max mem: 46473\n",
      "Valid: [epoch:7]  [ 10/845]  eta: 0:05:14  loss: 8.0530 (8.6926)  loss_n_40: 1.9587 (2.0282)  loss_n_60: 1.8970 (2.0545)  loss_n_80: 2.1709 (2.3440)  loss_n_100: 2.0906 (2.2659)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3764  data: 0.0420  max mem: 46473\n",
      "Valid: [epoch:7]  [ 20/845]  eta: 0:04:54  loss: 7.9947 (8.5959)  loss_n_40: 1.9169 (1.9882)  loss_n_60: 1.8598 (2.0494)  loss_n_80: 2.1546 (2.3225)  loss_n_100: 2.0880 (2.2357)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [ 30/845]  eta: 0:04:44  loss: 7.8512 (8.6978)  loss_n_40: 1.8621 (1.9973)  loss_n_60: 1.8415 (2.0816)  loss_n_80: 2.1968 (2.3524)  loss_n_100: 2.0891 (2.2666)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [ 40/845]  eta: 0:04:38  loss: 7.5887 (8.3643)  loss_n_40: 1.8996 (1.9608)  loss_n_60: 1.8033 (2.0052)  loss_n_80: 2.0875 (2.2530)  loss_n_100: 2.0372 (2.1453)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [ 50/845]  eta: 0:04:33  loss: 7.6465 (8.3825)  loss_n_40: 1.8990 (1.9586)  loss_n_60: 1.8033 (2.0108)  loss_n_80: 2.0858 (2.2566)  loss_n_100: 2.0254 (2.1565)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:7]  [ 60/845]  eta: 0:04:28  loss: 7.9242 (8.3879)  loss_n_40: 1.7722 (1.9501)  loss_n_60: 1.8561 (2.0103)  loss_n_80: 2.1483 (2.2593)  loss_n_100: 2.0503 (2.1682)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [ 70/845]  eta: 0:04:24  loss: 8.0722 (8.4202)  loss_n_40: 1.9047 (1.9563)  loss_n_60: 1.8789 (2.0176)  loss_n_80: 2.1543 (2.2665)  loss_n_100: 2.1522 (2.1799)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [ 80/845]  eta: 0:04:20  loss: 7.7578 (8.3531)  loss_n_40: 1.8643 (1.9513)  loss_n_60: 1.8286 (2.0042)  loss_n_80: 2.0790 (2.2471)  loss_n_100: 2.0542 (2.1505)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [ 90/845]  eta: 0:04:16  loss: 7.5644 (8.3660)  loss_n_40: 1.8259 (1.9506)  loss_n_60: 1.8236 (2.0101)  loss_n_80: 2.0697 (2.2513)  loss_n_100: 2.0144 (2.1539)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [100/845]  eta: 0:04:12  loss: 7.7183 (8.3627)  loss_n_40: 1.8259 (1.9510)  loss_n_60: 1.8701 (2.0095)  loss_n_80: 2.1071 (2.2503)  loss_n_100: 2.0683 (2.1519)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [110/845]  eta: 0:04:08  loss: 7.8992 (8.4185)  loss_n_40: 1.9132 (1.9624)  loss_n_60: 1.9162 (2.0246)  loss_n_80: 2.1338 (2.2652)  loss_n_100: 2.0920 (2.1663)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [120/845]  eta: 0:04:05  loss: 8.2593 (8.3961)  loss_n_40: 1.9663 (1.9619)  loss_n_60: 1.9162 (2.0211)  loss_n_80: 2.1787 (2.2561)  loss_n_100: 2.1800 (2.1570)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [130/845]  eta: 0:04:01  loss: 7.3904 (8.3720)  loss_n_40: 1.7286 (1.9565)  loss_n_60: 1.7913 (2.0187)  loss_n_80: 2.0402 (2.2494)  loss_n_100: 1.9487 (2.1474)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:7]  [140/845]  eta: 0:03:58  loss: 7.6362 (8.3564)  loss_n_40: 1.8530 (1.9539)  loss_n_60: 1.8144 (2.0161)  loss_n_80: 2.0402 (2.2444)  loss_n_100: 1.9672 (2.1420)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [150/845]  eta: 0:03:54  loss: 8.0103 (8.3849)  loss_n_40: 1.9371 (1.9570)  loss_n_60: 1.8873 (2.0240)  loss_n_80: 2.1168 (2.2528)  loss_n_100: 2.0781 (2.1511)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [160/845]  eta: 0:03:51  loss: 7.7859 (8.3303)  loss_n_40: 1.8193 (1.9438)  loss_n_60: 1.9203 (2.0130)  loss_n_80: 2.0924 (2.2373)  loss_n_100: 2.0365 (2.1362)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [170/845]  eta: 0:03:47  loss: 7.5347 (8.2802)  loss_n_40: 1.7172 (1.9326)  loss_n_60: 1.7849 (1.9994)  loss_n_80: 2.0197 (2.2242)  loss_n_100: 1.9490 (2.1241)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3360  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:7]  [180/845]  eta: 0:03:44  loss: 7.5252 (8.2835)  loss_n_40: 1.7756 (1.9329)  loss_n_60: 1.8259 (2.0052)  loss_n_80: 2.0397 (2.2223)  loss_n_100: 1.9474 (2.1230)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3361  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [190/845]  eta: 0:03:40  loss: 9.2606 (8.3394)  loss_n_40: 2.0921 (1.9444)  loss_n_60: 2.1897 (2.0184)  loss_n_80: 2.4906 (2.2372)  loss_n_100: 2.3436 (2.1394)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:7]  [200/845]  eta: 0:03:37  loss: 7.8886 (8.3225)  loss_n_40: 1.9306 (1.9403)  loss_n_60: 1.9369 (2.0154)  loss_n_80: 2.1406 (2.2335)  loss_n_100: 2.0849 (2.1333)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [210/845]  eta: 0:03:33  loss: 7.7588 (8.2899)  loss_n_40: 1.8249 (1.9346)  loss_n_60: 1.8753 (2.0086)  loss_n_80: 2.0629 (2.2231)  loss_n_100: 2.0319 (2.1236)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [220/845]  eta: 0:03:30  loss: 7.4522 (8.2777)  loss_n_40: 1.8690 (1.9357)  loss_n_60: 1.8113 (2.0053)  loss_n_80: 2.0629 (2.2179)  loss_n_100: 1.9587 (2.1187)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [230/845]  eta: 0:03:27  loss: 7.3140 (8.2456)  loss_n_40: 1.7449 (1.9285)  loss_n_60: 1.8198 (1.9998)  loss_n_80: 1.9381 (2.2081)  loss_n_100: 1.9230 (2.1091)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [240/845]  eta: 0:03:23  loss: 7.8350 (8.2534)  loss_n_40: 1.8377 (1.9316)  loss_n_60: 1.8495 (2.0001)  loss_n_80: 2.0840 (2.2097)  loss_n_100: 2.0873 (2.1120)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [250/845]  eta: 0:03:20  loss: 7.8350 (8.2370)  loss_n_40: 1.8787 (1.9295)  loss_n_60: 1.8281 (1.9951)  loss_n_80: 2.0840 (2.2047)  loss_n_100: 2.0873 (2.1077)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [260/845]  eta: 0:03:16  loss: 7.6000 (8.2238)  loss_n_40: 1.8300 (1.9264)  loss_n_60: 1.8445 (1.9932)  loss_n_80: 2.0663 (2.2010)  loss_n_100: 1.9955 (2.1033)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [270/845]  eta: 0:03:13  loss: 7.6620 (8.2232)  loss_n_40: 1.7793 (1.9255)  loss_n_60: 1.8445 (1.9930)  loss_n_80: 2.1005 (2.2006)  loss_n_100: 2.0608 (2.1040)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [280/845]  eta: 0:03:09  loss: 7.8511 (8.2448)  loss_n_40: 1.8469 (1.9286)  loss_n_60: 1.9460 (1.9973)  loss_n_80: 2.1242 (2.2071)  loss_n_100: 2.1184 (2.1119)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [290/845]  eta: 0:03:06  loss: 8.7966 (8.2551)  loss_n_40: 2.0219 (1.9296)  loss_n_60: 2.0976 (1.9991)  loss_n_80: 2.3644 (2.2103)  loss_n_100: 2.2306 (2.1161)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [300/845]  eta: 0:03:03  loss: 7.9743 (8.2563)  loss_n_40: 1.8827 (1.9286)  loss_n_60: 1.8749 (1.9989)  loss_n_80: 2.1297 (2.2121)  loss_n_100: 2.1092 (2.1167)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [310/845]  eta: 0:02:59  loss: 7.3037 (8.2332)  loss_n_40: 1.6710 (1.9257)  loss_n_60: 1.7260 (1.9928)  loss_n_80: 2.0515 (2.2062)  loss_n_100: 1.9242 (2.1085)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [320/845]  eta: 0:02:56  loss: 7.1724 (8.2244)  loss_n_40: 1.8118 (1.9259)  loss_n_60: 1.7372 (1.9905)  loss_n_80: 2.0250 (2.2032)  loss_n_100: 1.8507 (2.1048)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [330/845]  eta: 0:02:53  loss: 7.4182 (8.2068)  loss_n_40: 1.8118 (1.9234)  loss_n_60: 1.7372 (1.9858)  loss_n_80: 2.0464 (2.1981)  loss_n_100: 1.9592 (2.0994)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [340/845]  eta: 0:02:49  loss: 7.4683 (8.2077)  loss_n_40: 1.9068 (1.9249)  loss_n_60: 1.7869 (1.9858)  loss_n_80: 2.0535 (2.1980)  loss_n_100: 1.9751 (2.0989)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [350/845]  eta: 0:02:46  loss: 7.5711 (8.1822)  loss_n_40: 1.7868 (1.9207)  loss_n_60: 1.8050 (1.9794)  loss_n_80: 2.0726 (2.1918)  loss_n_100: 1.9751 (2.0904)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [360/845]  eta: 0:02:42  loss: 7.3563 (8.1901)  loss_n_40: 1.7371 (1.9224)  loss_n_60: 1.7643 (1.9805)  loss_n_80: 2.0364 (2.1938)  loss_n_100: 1.9256 (2.0934)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [370/845]  eta: 0:02:39  loss: 7.4457 (8.1713)  loss_n_40: 1.7374 (1.9187)  loss_n_60: 1.7989 (1.9763)  loss_n_80: 2.0219 (2.1880)  loss_n_100: 1.9471 (2.0883)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [380/845]  eta: 0:02:36  loss: 7.4457 (8.1703)  loss_n_40: 1.7208 (1.9177)  loss_n_60: 1.7989 (1.9760)  loss_n_80: 2.0219 (2.1881)  loss_n_100: 1.9970 (2.0885)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [390/845]  eta: 0:02:32  loss: 7.5285 (8.1633)  loss_n_40: 1.7618 (1.9152)  loss_n_60: 1.7968 (1.9744)  loss_n_80: 2.0346 (2.1868)  loss_n_100: 1.9970 (2.0870)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [400/845]  eta: 0:02:29  loss: 7.7286 (8.1733)  loss_n_40: 1.7817 (1.9163)  loss_n_60: 1.8230 (1.9769)  loss_n_80: 2.1096 (2.1897)  loss_n_100: 2.0381 (2.0905)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [410/845]  eta: 0:02:26  loss: 7.7286 (8.1667)  loss_n_40: 1.7849 (1.9153)  loss_n_60: 1.8230 (1.9755)  loss_n_80: 2.1376 (2.1881)  loss_n_100: 2.0921 (2.0879)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [420/845]  eta: 0:02:22  loss: 7.8925 (8.1681)  loss_n_40: 1.7849 (1.9151)  loss_n_60: 1.8251 (1.9756)  loss_n_80: 2.1025 (2.1882)  loss_n_100: 2.0858 (2.0892)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [430/845]  eta: 0:02:19  loss: 7.7063 (8.1623)  loss_n_40: 1.8304 (1.9148)  loss_n_60: 1.8023 (1.9732)  loss_n_80: 2.0783 (2.1868)  loss_n_100: 2.0623 (2.0877)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [440/845]  eta: 0:02:15  loss: 7.5839 (8.1746)  loss_n_40: 1.8304 (1.9168)  loss_n_60: 1.7790 (1.9759)  loss_n_80: 2.0372 (2.1903)  loss_n_100: 2.0364 (2.0916)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:7]  [450/845]  eta: 0:02:12  loss: 7.7347 (8.1640)  loss_n_40: 1.8001 (1.9145)  loss_n_60: 1.8179 (1.9733)  loss_n_80: 2.0887 (2.1871)  loss_n_100: 2.0593 (2.0891)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [460/845]  eta: 0:02:09  loss: 7.6896 (8.1643)  loss_n_40: 1.7708 (1.9136)  loss_n_60: 1.7895 (1.9733)  loss_n_80: 2.0656 (2.1876)  loss_n_100: 2.0386 (2.0898)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [470/845]  eta: 0:02:05  loss: 7.7030 (8.1603)  loss_n_40: 1.7446 (1.9125)  loss_n_60: 1.7961 (1.9727)  loss_n_80: 2.0452 (2.1867)  loss_n_100: 2.0562 (2.0885)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [480/845]  eta: 0:02:02  loss: 7.7030 (8.1711)  loss_n_40: 1.7934 (1.9144)  loss_n_60: 1.8898 (1.9752)  loss_n_80: 2.0464 (2.1896)  loss_n_100: 2.1176 (2.0919)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [490/845]  eta: 0:01:59  loss: 7.6737 (8.1714)  loss_n_40: 1.8125 (1.9145)  loss_n_60: 1.8613 (1.9756)  loss_n_80: 2.0753 (2.1899)  loss_n_100: 1.9992 (2.0914)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [500/845]  eta: 0:01:55  loss: 7.7029 (8.1675)  loss_n_40: 1.8285 (1.9157)  loss_n_60: 1.8383 (1.9744)  loss_n_80: 2.0753 (2.1881)  loss_n_100: 2.0547 (2.0892)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [510/845]  eta: 0:01:52  loss: 7.7029 (8.1720)  loss_n_40: 1.8285 (1.9161)  loss_n_60: 1.8121 (1.9748)  loss_n_80: 2.0737 (2.1895)  loss_n_100: 2.0547 (2.0916)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [520/845]  eta: 0:01:49  loss: 7.6885 (8.1682)  loss_n_40: 1.7706 (1.9155)  loss_n_60: 1.8160 (1.9739)  loss_n_80: 2.0737 (2.1879)  loss_n_100: 2.0743 (2.0908)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [530/845]  eta: 0:01:45  loss: 7.6360 (8.1653)  loss_n_40: 1.7439 (1.9141)  loss_n_60: 1.8160 (1.9728)  loss_n_80: 2.0633 (2.1879)  loss_n_100: 2.0642 (2.0905)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [540/845]  eta: 0:01:42  loss: 7.5933 (8.1685)  loss_n_40: 1.7439 (1.9142)  loss_n_60: 1.7924 (1.9734)  loss_n_80: 2.0663 (2.1889)  loss_n_100: 2.0436 (2.0920)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [550/845]  eta: 0:01:38  loss: 7.6332 (8.1597)  loss_n_40: 1.7658 (1.9119)  loss_n_60: 1.7948 (1.9722)  loss_n_80: 2.0663 (2.1865)  loss_n_100: 2.0246 (2.0891)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [560/845]  eta: 0:01:35  loss: 7.3205 (8.1351)  loss_n_40: 1.6865 (1.9064)  loss_n_60: 1.7308 (1.9664)  loss_n_80: 1.9877 (2.1802)  loss_n_100: 1.8999 (2.0821)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [570/845]  eta: 0:01:32  loss: 7.3265 (8.1392)  loss_n_40: 1.6229 (1.9067)  loss_n_60: 1.7177 (1.9682)  loss_n_80: 2.0372 (2.1815)  loss_n_100: 1.9117 (2.0827)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [580/845]  eta: 0:01:28  loss: 7.9319 (8.1396)  loss_n_40: 1.8078 (1.9071)  loss_n_60: 1.8986 (1.9684)  loss_n_80: 2.1271 (2.1815)  loss_n_100: 2.0829 (2.0826)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [590/845]  eta: 0:01:25  loss: 7.9319 (8.1465)  loss_n_40: 1.8129 (1.9079)  loss_n_60: 1.9512 (1.9706)  loss_n_80: 2.1448 (2.1832)  loss_n_100: 2.0829 (2.0848)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [600/845]  eta: 0:01:22  loss: 7.7249 (8.1519)  loss_n_40: 1.8129 (1.9087)  loss_n_60: 1.8681 (1.9718)  loss_n_80: 2.1075 (2.1852)  loss_n_100: 2.0568 (2.0862)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [610/845]  eta: 0:01:18  loss: 7.7006 (8.1489)  loss_n_40: 1.8089 (1.9089)  loss_n_60: 1.8196 (1.9711)  loss_n_80: 2.0695 (2.1841)  loss_n_100: 2.0325 (2.0847)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [620/845]  eta: 0:01:15  loss: 7.7017 (8.1484)  loss_n_40: 1.7583 (1.9077)  loss_n_60: 1.7891 (1.9711)  loss_n_80: 2.0671 (2.1841)  loss_n_100: 2.0481 (2.0855)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [630/845]  eta: 0:01:12  loss: 7.6402 (8.1363)  loss_n_40: 1.7583 (1.9069)  loss_n_60: 1.7891 (1.9672)  loss_n_80: 2.0398 (2.1804)  loss_n_100: 2.0177 (2.0817)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [640/845]  eta: 0:01:08  loss: 7.5299 (8.1347)  loss_n_40: 1.8670 (1.9066)  loss_n_60: 1.8037 (1.9659)  loss_n_80: 2.0729 (2.1801)  loss_n_100: 2.0031 (2.0820)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [650/845]  eta: 0:01:05  loss: 7.8754 (8.1357)  loss_n_40: 1.8809 (1.9066)  loss_n_60: 1.8482 (1.9668)  loss_n_80: 2.1053 (2.1800)  loss_n_100: 2.1060 (2.0823)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [660/845]  eta: 0:01:02  loss: 7.7504 (8.1419)  loss_n_40: 1.8358 (1.9078)  loss_n_60: 1.9264 (1.9682)  loss_n_80: 2.0806 (2.1819)  loss_n_100: 2.0213 (2.0840)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [670/845]  eta: 0:00:58  loss: 7.7504 (8.1451)  loss_n_40: 1.8358 (1.9085)  loss_n_60: 1.9287 (1.9695)  loss_n_80: 2.1132 (2.1832)  loss_n_100: 2.0213 (2.0839)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [680/845]  eta: 0:00:55  loss: 7.8055 (8.1474)  loss_n_40: 1.8737 (1.9086)  loss_n_60: 1.8520 (1.9702)  loss_n_80: 2.1094 (2.1841)  loss_n_100: 2.0582 (2.0844)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [690/845]  eta: 0:00:51  loss: 9.0069 (8.1655)  loss_n_40: 2.1641 (1.9122)  loss_n_60: 2.0818 (1.9746)  loss_n_80: 2.4123 (2.1891)  loss_n_100: 2.3222 (2.0896)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:7]  [700/845]  eta: 0:00:48  loss: 9.3525 (8.1712)  loss_n_40: 2.1907 (1.9135)  loss_n_60: 2.2089 (1.9766)  loss_n_80: 2.5503 (2.1900)  loss_n_100: 2.4450 (2.0911)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [710/845]  eta: 0:00:45  loss: 9.2299 (8.1817)  loss_n_40: 2.1919 (1.9175)  loss_n_60: 2.2089 (1.9789)  loss_n_80: 2.4690 (2.1919)  loss_n_100: 2.3944 (2.0934)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [720/845]  eta: 0:00:41  loss: 9.5899 (8.1895)  loss_n_40: 2.2318 (1.9190)  loss_n_60: 2.3010 (1.9813)  loss_n_80: 2.5393 (2.1943)  loss_n_100: 2.4370 (2.0949)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [730/845]  eta: 0:00:38  loss: 7.7008 (8.1896)  loss_n_40: 1.8192 (1.9184)  loss_n_60: 1.8142 (1.9812)  loss_n_80: 2.1400 (2.1947)  loss_n_100: 2.0193 (2.0951)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [740/845]  eta: 0:00:35  loss: 7.5750 (8.1928)  loss_n_40: 1.7390 (1.9193)  loss_n_60: 1.8142 (1.9822)  loss_n_80: 2.0613 (2.1952)  loss_n_100: 1.9590 (2.0961)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [750/845]  eta: 0:00:31  loss: 7.6525 (8.1821)  loss_n_40: 1.7517 (1.9172)  loss_n_60: 1.8277 (1.9798)  loss_n_80: 2.0411 (2.1923)  loss_n_100: 1.9577 (2.0928)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [760/845]  eta: 0:00:28  loss: 7.7826 (8.1883)  loss_n_40: 1.8414 (1.9197)  loss_n_60: 1.8592 (1.9812)  loss_n_80: 2.0866 (2.1934)  loss_n_100: 2.0585 (2.0940)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [770/845]  eta: 0:00:25  loss: 7.8754 (8.1858)  loss_n_40: 1.9898 (1.9208)  loss_n_60: 1.8343 (1.9807)  loss_n_80: 2.0866 (2.1923)  loss_n_100: 2.0784 (2.0920)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [780/845]  eta: 0:00:21  loss: 7.6068 (8.1896)  loss_n_40: 1.9783 (1.9211)  loss_n_60: 1.8399 (1.9820)  loss_n_80: 2.0738 (2.1935)  loss_n_100: 1.9508 (2.0929)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [790/845]  eta: 0:00:18  loss: 7.9094 (8.1906)  loss_n_40: 1.8913 (1.9220)  loss_n_60: 1.8573 (1.9819)  loss_n_80: 2.1185 (2.1938)  loss_n_100: 2.1124 (2.0929)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [800/845]  eta: 0:00:15  loss: 7.9094 (8.1913)  loss_n_40: 1.9054 (1.9219)  loss_n_60: 1.8750 (1.9824)  loss_n_80: 2.1185 (2.1936)  loss_n_100: 2.1124 (2.0934)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [810/845]  eta: 0:00:11  loss: 7.8013 (8.1903)  loss_n_40: 1.8905 (1.9216)  loss_n_60: 1.8796 (1.9823)  loss_n_80: 2.0720 (2.1931)  loss_n_100: 2.1028 (2.0933)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [820/845]  eta: 0:00:08  loss: 7.6890 (8.1871)  loss_n_40: 1.7230 (1.9206)  loss_n_60: 1.8054 (1.9819)  loss_n_80: 2.0823 (2.1924)  loss_n_100: 2.0416 (2.0922)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [830/845]  eta: 0:00:05  loss: 7.7398 (8.1918)  loss_n_40: 1.7559 (1.9213)  loss_n_60: 1.8478 (1.9832)  loss_n_80: 2.1358 (2.1936)  loss_n_100: 2.0480 (2.0938)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [840/845]  eta: 0:00:01  loss: 7.6606 (8.1922)  loss_n_40: 1.7869 (1.9217)  loss_n_60: 1.9072 (1.9832)  loss_n_80: 2.0504 (2.1935)  loss_n_100: 2.0678 (2.0939)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7]  [844/845]  eta: 0:00:00  loss: 7.6606 (8.1978)  loss_n_40: 1.7869 (1.9225)  loss_n_60: 1.9072 (1.9845)  loss_n_80: 2.0504 (2.1952)  loss_n_100: 2.0678 (2.0956)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:7] Total time: 0:04:43 (0.3353 s / it)\n",
      "Averaged stats: loss: 7.6606 (8.1978)  loss_n_40: 1.7869 (1.9225)  loss_n_60: 1.9072 (1.9845)  loss_n_80: 2.0504 (2.1952)  loss_n_100: 2.0678 (2.0956)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_7_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 2.096%\n",
      "Min loss_n_100: 1.091\n",
      "Best Epoch: 6.000\n",
      "Train: [epoch:8]  [   0/1724]  eta: 2:00:45  lr: 0.000140  loss: 8.4393 (8.4393)  loss_n_40: 1.9149 (1.9149)  loss_n_60: 2.0581 (2.0581)  loss_n_80: 2.2583 (2.2583)  loss_n_100: 2.2081 (2.2081)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.2029  data: 0.4465  max mem: 46473\n",
      "Train: [epoch:8]  [  10/1724]  eta: 1:52:50  lr: 0.000140  loss: 8.0083 (8.1100)  loss_n_40: 1.9149 (1.8751)  loss_n_60: 1.9108 (1.9203)  loss_n_80: 2.1565 (2.1289)  loss_n_100: 2.0193 (2.0329)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.1015)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0513)  time: 3.9501  data: 0.0407  max mem: 46473\n",
      "Train: [epoch:8]  [  20/1724]  eta: 1:51:49  lr: 0.000140  loss: 7.7556 (7.9431)  loss_n_40: 1.8234 (1.8443)  loss_n_60: 1.9105 (1.9176)  loss_n_80: 2.0932 (2.1026)  loss_n_100: 2.0100 (1.9987)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0532)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0269)  time: 3.9242  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [  30/1724]  eta: 1:51:02  lr: 0.000140  loss: 7.6297 (7.9030)  loss_n_40: 1.8038 (1.8388)  loss_n_60: 1.9100 (1.9187)  loss_n_80: 2.0831 (2.1009)  loss_n_100: 1.9363 (1.9904)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0360)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0182)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [  40/1724]  eta: 1:50:19  lr: 0.000140  loss: 7.5516 (7.8103)  loss_n_40: 1.7689 (1.8095)  loss_n_60: 1.9002 (1.9132)  loss_n_80: 2.0344 (2.0791)  loss_n_100: 1.9039 (1.9674)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0272)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0138)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [  50/1724]  eta: 1:49:37  lr: 0.000140  loss: 7.3604 (7.7069)  loss_n_40: 1.7230 (1.7961)  loss_n_60: 1.8380 (1.8886)  loss_n_80: 1.9801 (2.0513)  loss_n_100: 1.8498 (1.9380)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0219)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0111)  time: 3.9237  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [  60/1724]  eta: 1:48:57  lr: 0.000140  loss: 7.0563 (7.6323)  loss_n_40: 1.7054 (1.7830)  loss_n_60: 1.7363 (1.8668)  loss_n_80: 1.8905 (2.0317)  loss_n_100: 1.7684 (1.9232)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0183)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0092)  time: 3.9243  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [  70/1724]  eta: 1:48:17  lr: 0.000140  loss: 6.8226 (7.5244)  loss_n_40: 1.6832 (1.7664)  loss_n_60: 1.6919 (1.8438)  loss_n_80: 1.8201 (1.9987)  loss_n_100: 1.7218 (1.8917)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0157)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0079)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [  80/1724]  eta: 1:47:36  lr: 0.000140  loss: 6.6872 (7.4122)  loss_n_40: 1.5874 (1.7468)  loss_n_60: 1.6366 (1.8193)  loss_n_80: 1.7582 (1.9665)  loss_n_100: 1.6483 (1.8588)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0070)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [  90/1724]  eta: 1:46:57  lr: 0.000140  loss: 6.7725 (7.3406)  loss_n_40: 1.6103 (1.7320)  loss_n_60: 1.6963 (1.8019)  loss_n_80: 1.7884 (1.9460)  loss_n_100: 1.7035 (1.8424)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0123)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0062)  time: 3.9235  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [ 100/1724]  eta: 1:46:17  lr: 0.000140  loss: 6.9448 (7.2912)  loss_n_40: 1.6441 (1.7254)  loss_n_60: 1.6985 (1.7864)  loss_n_80: 1.8290 (1.9316)  loss_n_100: 1.7803 (1.8311)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0111)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0056)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 110/1724]  eta: 1:45:37  lr: 0.000140  loss: 6.8385 (7.1903)  loss_n_40: 1.6210 (1.7123)  loss_n_60: 1.5892 (1.7579)  loss_n_80: 1.7724 (1.8995)  loss_n_100: 1.6339 (1.8003)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0101)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0102)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 120/1724]  eta: 1:44:57  lr: 0.000140  loss: 6.3950 (7.1453)  loss_n_40: 1.6003 (1.7100)  loss_n_60: 1.5205 (1.7466)  loss_n_80: 1.6604 (1.8850)  loss_n_100: 1.6038 (1.7851)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0092)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0093)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 130/1724]  eta: 1:44:18  lr: 0.000140  loss: 6.3487 (7.0881)  loss_n_40: 1.6819 (1.7075)  loss_n_60: 1.5381 (1.7340)  loss_n_80: 1.6149 (1.8657)  loss_n_100: 1.4885 (1.7636)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0085)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0086)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 140/1724]  eta: 1:43:38  lr: 0.000140  loss: 6.1877 (7.0184)  loss_n_40: 1.5804 (1.6970)  loss_n_60: 1.4788 (1.7168)  loss_n_80: 1.5912 (1.8454)  loss_n_100: 1.4434 (1.7433)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0079)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0080)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 150/1724]  eta: 1:42:59  lr: 0.000140  loss: 6.3017 (6.9881)  loss_n_40: 1.5560 (1.6891)  loss_n_60: 1.5329 (1.7088)  loss_n_80: 1.6436 (1.8323)  loss_n_100: 1.5310 (1.7300)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0064)  triple_40: 0.0000 (0.0075)  time: 3.9256  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [ 160/1724]  eta: 1:42:20  lr: 0.000140  loss: 6.3672 (6.9443)  loss_n_40: 1.5560 (1.6808)  loss_n_60: 1.5272 (1.6950)  loss_n_80: 1.6493 (1.8174)  loss_n_100: 1.5449 (1.7157)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0142)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 170/1724]  eta: 1:41:40  lr: 0.000140  loss: 6.2526 (6.9003)  loss_n_40: 1.5176 (1.6723)  loss_n_60: 1.4628 (1.6827)  loss_n_80: 1.6124 (1.8041)  loss_n_100: 1.5391 (1.7079)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0134)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 180/1724]  eta: 1:41:01  lr: 0.000140  loss: 6.0895 (6.8503)  loss_n_40: 1.4851 (1.6594)  loss_n_60: 1.4954 (1.6712)  loss_n_80: 1.5836 (1.7904)  loss_n_100: 1.5391 (1.6980)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0123)  triple_60: 0.0000 (0.0065)  triple_40: 0.0000 (0.0127)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 190/1724]  eta: 1:40:21  lr: 0.000140  loss: 5.8463 (6.7937)  loss_n_40: 1.4328 (1.6463)  loss_n_60: 1.4458 (1.6578)  loss_n_80: 1.4755 (1.7741)  loss_n_100: 1.4834 (1.6857)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0116)  triple_60: 0.0000 (0.0062)  triple_40: 0.0000 (0.0120)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 200/1724]  eta: 1:39:42  lr: 0.000140  loss: 5.6251 (6.7257)  loss_n_40: 1.3891 (1.6326)  loss_n_60: 1.3855 (1.6423)  loss_n_80: 1.4454 (1.7542)  loss_n_100: 1.4299 (1.6682)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0111)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0114)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 210/1724]  eta: 1:39:02  lr: 0.000140  loss: 5.3073 (6.6544)  loss_n_40: 1.3250 (1.6177)  loss_n_60: 1.3350 (1.6255)  loss_n_80: 1.3460 (1.7339)  loss_n_100: 1.3192 (1.6503)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0109)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 220/1724]  eta: 1:38:23  lr: 0.000140  loss: 5.0696 (6.5970)  loss_n_40: 1.2485 (1.5993)  loss_n_60: 1.2066 (1.6041)  loss_n_80: 1.2681 (1.7108)  loss_n_100: 1.2570 (1.6310)  triple_100: 0.0000 (0.0061)  triple_80: 0.0000 (0.0192)  triple_60: 0.0000 (0.0161)  triple_40: 0.0000 (0.0104)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 230/1724]  eta: 1:37:44  lr: 0.000140  loss: 4.6542 (6.5198)  loss_n_40: 1.2202 (1.5824)  loss_n_60: 1.1230 (1.5845)  loss_n_80: 1.1723 (1.6874)  loss_n_100: 1.2367 (1.6159)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0184)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0099)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 240/1724]  eta: 1:37:05  lr: 0.000140  loss: 4.5620 (6.4294)  loss_n_40: 1.1709 (1.5617)  loss_n_60: 1.0894 (1.5611)  loss_n_80: 1.0843 (1.6606)  loss_n_100: 1.1928 (1.5943)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0176)  triple_60: 0.0000 (0.0189)  triple_40: 0.0000 (0.0095)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 250/1724]  eta: 1:36:25  lr: 0.000140  loss: 4.2929 (6.3499)  loss_n_40: 1.1103 (1.5450)  loss_n_60: 1.0626 (1.5418)  loss_n_80: 1.0596 (1.6381)  loss_n_100: 1.0688 (1.5754)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0169)  triple_60: 0.0000 (0.0181)  triple_40: 0.0000 (0.0092)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 260/1724]  eta: 1:35:46  lr: 0.000140  loss: 4.2654 (6.2651)  loss_n_40: 1.0237 (1.5243)  loss_n_60: 1.0041 (1.5199)  loss_n_80: 1.0761 (1.6155)  loss_n_100: 1.1263 (1.5578)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0163)  triple_60: 0.0000 (0.0174)  triple_40: 0.0000 (0.0088)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 270/1724]  eta: 1:35:07  lr: 0.000140  loss: 4.0337 (6.1780)  loss_n_40: 0.9692 (1.5034)  loss_n_60: 0.9386 (1.4985)  loss_n_80: 0.9768 (1.5914)  loss_n_100: 1.0675 (1.5388)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0157)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0085)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 280/1724]  eta: 1:34:28  lr: 0.000140  loss: 3.6714 (6.0889)  loss_n_40: 0.8973 (1.4816)  loss_n_60: 0.9092 (1.4762)  loss_n_80: 0.9106 (1.5670)  loss_n_100: 1.0110 (1.5198)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0082)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 290/1724]  eta: 1:33:48  lr: 0.000140  loss: 3.5472 (5.9996)  loss_n_40: 0.8524 (1.4604)  loss_n_60: 0.8469 (1.4533)  loss_n_80: 0.8698 (1.5421)  loss_n_100: 0.9737 (1.5011)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0146)  triple_60: 0.0000 (0.0156)  triple_40: 0.0000 (0.0079)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 300/1724]  eta: 1:33:09  lr: 0.000140  loss: 3.3629 (5.9095)  loss_n_40: 0.8196 (1.4395)  loss_n_60: 0.7742 (1.4307)  loss_n_80: 0.8076 (1.5177)  loss_n_100: 0.9144 (1.4802)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0151)  triple_40: 0.0000 (0.0076)  time: 3.9249  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [ 310/1724]  eta: 1:32:30  lr: 0.000140  loss: 3.3935 (5.8326)  loss_n_40: 0.8196 (1.4212)  loss_n_60: 0.7989 (1.4122)  loss_n_80: 0.8149 (1.4963)  loss_n_100: 0.8911 (1.4629)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0146)  triple_40: 0.0000 (0.0074)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 320/1724]  eta: 1:31:51  lr: 0.000140  loss: 3.4206 (5.7702)  loss_n_40: 0.8744 (1.4060)  loss_n_60: 0.8417 (1.3953)  loss_n_80: 0.8282 (1.4764)  loss_n_100: 0.9117 (1.4465)  triple_100: 0.0000 (0.0042)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0120)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 330/1724]  eta: 1:31:11  lr: 0.000140  loss: 3.5267 (5.7070)  loss_n_40: 0.8569 (1.3877)  loss_n_60: 0.8524 (1.3793)  loss_n_80: 0.8733 (1.4595)  loss_n_100: 0.9619 (1.4338)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0146)  triple_40: 0.0000 (0.0128)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 340/1724]  eta: 1:30:32  lr: 0.000140  loss: 3.5187 (5.6402)  loss_n_40: 0.8107 (1.3707)  loss_n_60: 0.8465 (1.3631)  loss_n_80: 0.8838 (1.4413)  loss_n_100: 0.9630 (1.4196)  triple_100: 0.0000 (0.0039)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0124)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 350/1724]  eta: 1:29:53  lr: 0.000140  loss: 3.2982 (5.5753)  loss_n_40: 0.8002 (1.3551)  loss_n_60: 0.8171 (1.3472)  loss_n_80: 0.8019 (1.4234)  loss_n_100: 0.9264 (1.4054)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0138)  triple_40: 0.0000 (0.0121)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 360/1724]  eta: 1:29:13  lr: 0.000140  loss: 3.2672 (5.5108)  loss_n_40: 0.7666 (1.3401)  loss_n_60: 0.7817 (1.3322)  loss_n_80: 0.7886 (1.4059)  loss_n_100: 0.8413 (1.3897)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0134)  triple_40: 0.0000 (0.0118)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 370/1724]  eta: 1:28:34  lr: 0.000140  loss: 3.1415 (5.4451)  loss_n_40: 0.7443 (1.3240)  loss_n_60: 0.7756 (1.3162)  loss_n_80: 0.7645 (1.3883)  loss_n_100: 0.8088 (1.3749)  triple_100: 0.0000 (0.0036)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0130)  triple_40: 0.0000 (0.0114)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 380/1724]  eta: 1:27:55  lr: 0.000140  loss: 3.4297 (5.3961)  loss_n_40: 0.7327 (1.3093)  loss_n_60: 0.7756 (1.3028)  loss_n_80: 0.8359 (1.3747)  loss_n_100: 0.9198 (1.3648)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0127)  triple_40: 0.0000 (0.0148)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 390/1724]  eta: 1:27:16  lr: 0.000140  loss: 3.4810 (5.3484)  loss_n_40: 0.7327 (1.2966)  loss_n_60: 0.8056 (1.2911)  loss_n_80: 0.8978 (1.3620)  loss_n_100: 1.0116 (1.3552)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0124)  triple_40: 0.0000 (0.0145)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 400/1724]  eta: 1:26:36  lr: 0.000140  loss: 3.3277 (5.2990)  loss_n_40: 0.7363 (1.2846)  loss_n_60: 0.8068 (1.2799)  loss_n_80: 0.8376 (1.3486)  loss_n_100: 0.9180 (1.3434)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0121)  triple_40: 0.0000 (0.0141)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 410/1724]  eta: 1:25:57  lr: 0.000140  loss: 3.1377 (5.2433)  loss_n_40: 0.7355 (1.2712)  loss_n_60: 0.7633 (1.2668)  loss_n_80: 0.7600 (1.3338)  loss_n_100: 0.8139 (1.3301)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0127)  triple_60: 0.0000 (0.0118)  triple_40: 0.0000 (0.0137)  time: 3.9253  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [ 420/1724]  eta: 1:25:18  lr: 0.000140  loss: 3.1377 (5.2335)  loss_n_40: 0.7420 (1.2650)  loss_n_60: 0.7633 (1.2619)  loss_n_80: 0.7600 (1.3295)  loss_n_100: 0.8731 (1.3286)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0148)  triple_40: 0.0000 (0.0134)  time: 3.9251  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [ 430/1724]  eta: 1:24:39  lr: 0.000140  loss: 4.7164 (5.2248)  loss_n_40: 1.0152 (1.2610)  loss_n_60: 1.0802 (1.2586)  loss_n_80: 1.1652 (1.3276)  loss_n_100: 1.2446 (1.3303)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0144)  triple_40: 0.0000 (0.0131)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 440/1724]  eta: 1:23:59  lr: 0.000140  loss: 4.5296 (5.1961)  loss_n_40: 1.0102 (1.2538)  loss_n_60: 1.0163 (1.2519)  loss_n_80: 1.0917 (1.3195)  loss_n_100: 1.2286 (1.3247)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0146)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0128)  time: 3.9251  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [ 450/1724]  eta: 1:23:20  lr: 0.000140  loss: 3.6503 (5.1582)  loss_n_40: 0.8125 (1.2438)  loss_n_60: 0.9110 (1.2431)  loss_n_80: 0.8741 (1.3094)  loss_n_100: 0.9820 (1.3167)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0138)  triple_40: 0.0000 (0.0125)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 460/1724]  eta: 1:22:41  lr: 0.000140  loss: 3.2635 (5.1184)  loss_n_40: 0.7714 (1.2344)  loss_n_60: 0.7969 (1.2335)  loss_n_80: 0.7999 (1.2983)  loss_n_100: 0.9399 (1.3080)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0135)  triple_40: 0.0000 (0.0123)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 470/1724]  eta: 1:22:02  lr: 0.000140  loss: 3.0706 (5.0737)  loss_n_40: 0.7342 (1.2244)  loss_n_60: 0.7437 (1.2227)  loss_n_80: 0.7477 (1.2863)  loss_n_100: 0.8538 (1.2970)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0132)  triple_40: 0.0000 (0.0120)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 480/1724]  eta: 1:21:22  lr: 0.000140  loss: 2.8932 (5.0276)  loss_n_40: 0.6435 (1.2127)  loss_n_60: 0.6963 (1.2114)  loss_n_80: 0.7043 (1.2744)  loss_n_100: 0.7665 (1.2867)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0134)  triple_60: 0.0000 (0.0129)  triple_40: 0.0000 (0.0117)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 490/1724]  eta: 1:20:43  lr: 0.000140  loss: 2.8396 (4.9863)  loss_n_40: 0.6276 (1.2020)  loss_n_60: 0.6695 (1.2010)  loss_n_80: 0.7077 (1.2636)  loss_n_100: 0.7665 (1.2773)  triple_100: 0.0000 (0.0042)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0127)  triple_40: 0.0000 (0.0123)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 500/1724]  eta: 1:20:04  lr: 0.000140  loss: 2.6710 (4.9441)  loss_n_40: 0.6276 (1.1915)  loss_n_60: 0.6308 (1.1900)  loss_n_80: 0.6954 (1.2523)  loss_n_100: 0.7612 (1.2669)  triple_100: 0.0000 (0.0042)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0129)  triple_40: 0.0000 (0.0121)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 510/1724]  eta: 1:19:25  lr: 0.000140  loss: 2.6710 (4.9075)  loss_n_40: 0.6276 (1.1840)  loss_n_60: 0.6471 (1.1809)  loss_n_80: 0.6920 (1.2422)  loss_n_100: 0.7617 (1.2573)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0127)  triple_40: 0.0000 (0.0124)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 520/1724]  eta: 1:18:45  lr: 0.000140  loss: 2.8193 (4.8690)  loss_n_40: 0.6603 (1.1740)  loss_n_60: 0.6916 (1.1713)  loss_n_80: 0.7138 (1.2325)  loss_n_100: 0.7728 (1.2489)  triple_100: 0.0000 (0.0040)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0124)  triple_40: 0.0000 (0.0121)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 530/1724]  eta: 1:18:06  lr: 0.000140  loss: 2.8114 (4.8311)  loss_n_40: 0.6547 (1.1653)  loss_n_60: 0.6566 (1.1617)  loss_n_80: 0.7049 (1.2228)  loss_n_100: 0.7790 (1.2398)  triple_100: 0.0000 (0.0039)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0122)  triple_40: 0.0000 (0.0119)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 540/1724]  eta: 1:17:27  lr: 0.000140  loss: 2.7744 (4.7967)  loss_n_40: 0.6465 (1.1566)  loss_n_60: 0.6498 (1.1531)  loss_n_80: 0.7140 (1.2141)  loss_n_100: 0.7670 (1.2319)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0120)  triple_40: 0.0000 (0.0117)  time: 3.9259  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [ 550/1724]  eta: 1:16:48  lr: 0.000140  loss: 2.6719 (4.7586)  loss_n_40: 0.6286 (1.1475)  loss_n_60: 0.6427 (1.1436)  loss_n_80: 0.6705 (1.2043)  loss_n_100: 0.7413 (1.2231)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0117)  triple_40: 0.0000 (0.0115)  time: 3.9265  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [ 560/1724]  eta: 1:16:08  lr: 0.000140  loss: 2.6428 (4.7214)  loss_n_40: 0.6346 (1.1390)  loss_n_60: 0.6224 (1.1342)  loss_n_80: 0.6581 (1.1946)  loss_n_100: 0.7413 (1.2141)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0115)  triple_40: 0.0000 (0.0113)  time: 3.9266  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [ 570/1724]  eta: 1:15:29  lr: 0.000140  loss: 2.6294 (4.6842)  loss_n_40: 0.6052 (1.1301)  loss_n_60: 0.6083 (1.1249)  loss_n_80: 0.6581 (1.1848)  loss_n_100: 0.7418 (1.2052)  triple_100: 0.0000 (0.0036)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0113)  triple_40: 0.0000 (0.0111)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 580/1724]  eta: 1:14:50  lr: 0.000140  loss: 2.5473 (4.6477)  loss_n_40: 0.6004 (1.1212)  loss_n_60: 0.5912 (1.1157)  loss_n_80: 0.6174 (1.1754)  loss_n_100: 0.7131 (1.1967)  triple_100: 0.0000 (0.0036)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0109)  time: 3.9271  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 590/1724]  eta: 1:14:11  lr: 0.000140  loss: 2.5342 (4.6111)  loss_n_40: 0.6099 (1.1125)  loss_n_60: 0.5865 (1.1066)  loss_n_80: 0.6201 (1.1659)  loss_n_100: 0.7005 (1.1881)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0109)  triple_40: 0.0000 (0.0107)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 600/1724]  eta: 1:13:32  lr: 0.000140  loss: 2.4773 (4.5789)  loss_n_40: 0.6036 (1.1057)  loss_n_60: 0.5819 (1.0987)  loss_n_80: 0.6193 (1.1574)  loss_n_100: 0.7005 (1.1798)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0126)  triple_60: 0.0000 (0.0108)  triple_40: 0.0000 (0.0105)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 610/1724]  eta: 1:12:52  lr: 0.000140  loss: 2.4815 (4.5501)  loss_n_40: 0.6073 (1.0990)  loss_n_60: 0.5960 (1.0909)  loss_n_80: 0.6280 (1.1490)  loss_n_100: 0.6589 (1.1716)  triple_100: 0.0000 (0.0039)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0112)  triple_40: 0.0000 (0.0103)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 620/1724]  eta: 1:12:13  lr: 0.000140  loss: 2.6544 (4.5208)  loss_n_40: 0.6338 (1.0924)  loss_n_60: 0.6348 (1.0838)  loss_n_80: 0.6418 (1.1416)  loss_n_100: 0.6589 (1.1641)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0102)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 630/1724]  eta: 1:11:34  lr: 0.000140  loss: 2.7570 (4.5070)  loss_n_40: 0.6669 (1.0859)  loss_n_60: 0.6476 (1.0778)  loss_n_80: 0.7089 (1.1362)  loss_n_100: 0.7597 (1.1594)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0145)  triple_40: 0.0000 (0.0127)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 640/1724]  eta: 1:10:55  lr: 0.000140  loss: 6.0150 (4.5757)  loss_n_40: 0.9852 (1.0919)  loss_n_60: 0.9844 (1.0876)  loss_n_80: 1.1838 (1.1494)  loss_n_100: 1.3456 (1.1741)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0316)  triple_60: 0.0000 (0.0242)  triple_40: 0.0000 (0.0125)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 650/1724]  eta: 1:10:15  lr: 0.000140  loss: 6.8020 (4.6016)  loss_n_40: 1.4462 (1.0973)  loss_n_60: 1.5539 (1.0944)  loss_n_80: 1.8279 (1.1567)  loss_n_100: 1.7974 (1.1816)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0312)  triple_60: 0.0000 (0.0238)  triple_40: 0.0000 (0.0123)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 660/1724]  eta: 1:09:36  lr: 0.000140  loss: 5.8417 (4.6146)  loss_n_40: 1.3951 (1.1013)  loss_n_60: 1.3740 (1.0973)  loss_n_80: 1.4402 (1.1600)  loss_n_100: 1.4970 (1.1855)  triple_100: 0.0000 (0.0042)  triple_80: 0.0000 (0.0307)  triple_60: 0.0000 (0.0234)  triple_40: 0.0000 (0.0122)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 670/1724]  eta: 1:08:57  lr: 0.000140  loss: 5.1438 (4.6199)  loss_n_40: 1.3060 (1.1048)  loss_n_60: 1.2026 (1.0979)  loss_n_80: 1.2995 (1.1610)  loss_n_100: 1.3314 (1.1867)  triple_100: 0.0000 (0.0042)  triple_80: 0.0000 (0.0302)  triple_60: 0.0000 (0.0231)  triple_40: 0.0000 (0.0120)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 680/1724]  eta: 1:08:18  lr: 0.000140  loss: 4.6872 (4.6188)  loss_n_40: 1.2374 (1.1055)  loss_n_60: 1.1110 (1.0976)  loss_n_80: 1.1693 (1.1606)  loss_n_100: 1.2092 (1.1867)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0298)  triple_60: 0.0000 (0.0227)  triple_40: 0.0000 (0.0118)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 690/1724]  eta: 1:07:38  lr: 0.000140  loss: 4.2738 (4.6130)  loss_n_40: 1.0305 (1.1043)  loss_n_60: 0.9850 (1.0957)  loss_n_80: 1.0257 (1.1583)  loss_n_100: 1.1456 (1.1859)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0294)  triple_60: 0.0000 (0.0224)  triple_40: 0.0000 (0.0129)  time: 3.9243  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 700/1724]  eta: 1:06:59  lr: 0.000140  loss: 3.9413 (4.6053)  loss_n_40: 0.9661 (1.1030)  loss_n_60: 0.9167 (1.0938)  loss_n_80: 0.9863 (1.1562)  loss_n_100: 1.0941 (1.1846)  triple_100: 0.0000 (0.0040)  triple_80: 0.0000 (0.0289)  triple_60: 0.0000 (0.0221)  triple_40: 0.0000 (0.0127)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 710/1724]  eta: 1:06:20  lr: 0.000140  loss: 3.6098 (4.5880)  loss_n_40: 0.8901 (1.0993)  loss_n_60: 0.8430 (1.0898)  loss_n_80: 0.8991 (1.1514)  loss_n_100: 0.9932 (1.1808)  triple_100: 0.0000 (0.0039)  triple_80: 0.0000 (0.0285)  triple_60: 0.0000 (0.0218)  triple_40: 0.0000 (0.0125)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 720/1724]  eta: 1:05:41  lr: 0.000140  loss: 3.4087 (4.5724)  loss_n_40: 0.7984 (1.0959)  loss_n_60: 0.8001 (1.0863)  loss_n_80: 0.8322 (1.1472)  loss_n_100: 0.9078 (1.1771)  triple_100: 0.0000 (0.0039)  triple_80: 0.0000 (0.0281)  triple_60: 0.0000 (0.0215)  triple_40: 0.0000 (0.0123)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 730/1724]  eta: 1:05:01  lr: 0.000140  loss: 3.3488 (4.5555)  loss_n_40: 0.7984 (1.0925)  loss_n_60: 0.8131 (1.0825)  loss_n_80: 0.8322 (1.1429)  loss_n_100: 0.8943 (1.1728)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0278)  triple_60: 0.0000 (0.0212)  triple_40: 0.0000 (0.0122)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 740/1724]  eta: 1:04:22  lr: 0.000140  loss: 3.0135 (4.5364)  loss_n_40: 0.7441 (1.0883)  loss_n_60: 0.7534 (1.0780)  loss_n_80: 0.7586 (1.1378)  loss_n_100: 0.8196 (1.1682)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0274)  triple_60: 0.0000 (0.0209)  triple_40: 0.0000 (0.0120)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 750/1724]  eta: 1:03:43  lr: 0.000140  loss: 2.9934 (4.5159)  loss_n_40: 0.6968 (1.0838)  loss_n_60: 0.7186 (1.0730)  loss_n_80: 0.7225 (1.1325)  loss_n_100: 0.7920 (1.1634)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0270)  triple_60: 0.0000 (0.0206)  triple_40: 0.0000 (0.0118)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 760/1724]  eta: 1:03:03  lr: 0.000140  loss: 2.7888 (4.4937)  loss_n_40: 0.6450 (1.0784)  loss_n_60: 0.6748 (1.0676)  loss_n_80: 0.7091 (1.1269)  loss_n_100: 0.8033 (1.1584)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0267)  triple_60: 0.0000 (0.0203)  triple_40: 0.0000 (0.0117)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 770/1724]  eta: 1:02:24  lr: 0.000140  loss: 3.0984 (4.4956)  loss_n_40: 0.6751 (1.0768)  loss_n_60: 0.6981 (1.0667)  loss_n_80: 0.7221 (1.1271)  loss_n_100: 0.8410 (1.1605)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0263)  triple_60: 0.0000 (0.0201)  triple_40: 0.0000 (0.0115)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 780/1724]  eta: 1:01:45  lr: 0.000140  loss: 4.7603 (4.5031)  loss_n_40: 1.1083 (1.0781)  loss_n_60: 1.0791 (1.0678)  loss_n_80: 1.2297 (1.1290)  loss_n_100: 1.3349 (1.1630)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0260)  triple_60: 0.0000 (0.0209)  triple_40: 0.0000 (0.0120)  time: 3.9238  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [ 790/1724]  eta: 1:01:06  lr: 0.000140  loss: 4.1982 (4.4969)  loss_n_40: 1.0241 (1.0766)  loss_n_60: 0.9853 (1.0661)  loss_n_80: 1.0626 (1.1272)  loss_n_100: 1.1478 (1.1625)  triple_100: 0.0000 (0.0063)  triple_80: 0.0000 (0.0257)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0118)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 800/1724]  eta: 1:00:26  lr: 0.000140  loss: 3.8192 (4.4861)  loss_n_40: 0.8677 (1.0741)  loss_n_60: 0.9064 (1.0633)  loss_n_80: 0.9647 (1.1245)  loss_n_100: 1.0587 (1.1605)  triple_100: 0.0000 (0.0062)  triple_80: 0.0000 (0.0254)  triple_60: 0.0000 (0.0204)  triple_40: 0.0000 (0.0117)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 810/1724]  eta: 0:59:47  lr: 0.000140  loss: 3.3089 (4.4712)  loss_n_40: 0.8270 (1.0714)  loss_n_60: 0.7668 (1.0596)  loss_n_80: 0.8239 (1.1203)  loss_n_100: 0.9449 (1.1571)  triple_100: 0.0000 (0.0062)  triple_80: 0.0000 (0.0250)  triple_60: 0.0000 (0.0202)  triple_40: 0.0000 (0.0115)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 820/1724]  eta: 0:59:08  lr: 0.000140  loss: 3.1424 (4.4538)  loss_n_40: 0.8254 (1.0677)  loss_n_60: 0.7241 (1.0553)  loss_n_80: 0.7407 (1.1154)  loss_n_100: 0.8277 (1.1532)  triple_100: 0.0000 (0.0061)  triple_80: 0.0000 (0.0247)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0114)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 830/1724]  eta: 0:58:29  lr: 0.000140  loss: 2.7840 (4.4315)  loss_n_40: 0.6291 (1.0621)  loss_n_60: 0.6343 (1.0497)  loss_n_80: 0.6874 (1.1097)  loss_n_100: 0.7921 (1.1486)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0244)  triple_60: 0.0000 (0.0197)  triple_40: 0.0000 (0.0113)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 840/1724]  eta: 0:57:49  lr: 0.000140  loss: 2.6452 (4.4138)  loss_n_40: 0.6107 (1.0584)  loss_n_60: 0.5907 (1.0453)  loss_n_80: 0.6416 (1.1052)  loss_n_100: 0.7687 (1.1442)  triple_100: 0.0000 (0.0059)  triple_80: 0.0000 (0.0241)  triple_60: 0.0000 (0.0194)  triple_40: 0.0000 (0.0111)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 850/1724]  eta: 0:57:10  lr: 0.000140  loss: 2.6955 (4.3941)  loss_n_40: 0.6107 (1.0534)  loss_n_60: 0.6153 (1.0405)  loss_n_80: 0.6960 (1.1003)  loss_n_100: 0.7604 (1.1400)  triple_100: 0.0000 (0.0059)  triple_80: 0.0000 (0.0239)  triple_60: 0.0000 (0.0192)  triple_40: 0.0000 (0.0110)  time: 3.9250  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [ 860/1724]  eta: 0:56:31  lr: 0.000140  loss: 2.5258 (4.3710)  loss_n_40: 0.5549 (1.0478)  loss_n_60: 0.5891 (1.0349)  loss_n_80: 0.6251 (1.0942)  loss_n_100: 0.7243 (1.1345)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0239)  triple_60: 0.0000 (0.0190)  triple_40: 0.0000 (0.0109)  time: 3.9255  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [ 870/1724]  eta: 0:55:52  lr: 0.000140  loss: 2.3735 (4.3510)  loss_n_40: 0.5524 (1.0431)  loss_n_60: 0.5444 (1.0300)  loss_n_80: 0.5806 (1.0890)  loss_n_100: 0.6947 (1.1300)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0236)  triple_60: 0.0000 (0.0188)  triple_40: 0.0000 (0.0108)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 880/1724]  eta: 0:55:12  lr: 0.000140  loss: 2.4072 (4.3306)  loss_n_40: 0.5610 (1.0379)  loss_n_60: 0.5431 (1.0250)  loss_n_80: 0.6018 (1.0838)  loss_n_100: 0.7189 (1.1256)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0233)  triple_60: 0.0000 (0.0186)  triple_40: 0.0000 (0.0108)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 890/1724]  eta: 0:54:33  lr: 0.000140  loss: 2.8021 (4.3181)  loss_n_40: 0.6197 (1.0342)  loss_n_60: 0.6675 (1.0218)  loss_n_80: 0.6826 (1.0802)  loss_n_100: 0.7947 (1.1228)  triple_100: 0.0000 (0.0066)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0183)  triple_40: 0.0000 (0.0107)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 900/1724]  eta: 0:53:54  lr: 0.000140  loss: 3.1296 (4.3047)  loss_n_40: 0.6640 (1.0305)  loss_n_60: 0.7412 (1.0186)  loss_n_80: 0.7668 (1.0770)  loss_n_100: 0.8617 (1.1200)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0232)  triple_60: 0.0000 (0.0181)  triple_40: 0.0000 (0.0106)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 910/1724]  eta: 0:53:15  lr: 0.000140  loss: 2.8907 (4.2879)  loss_n_40: 0.6196 (1.0261)  loss_n_60: 0.7087 (1.0149)  loss_n_80: 0.7414 (1.0729)  loss_n_100: 0.7912 (1.1162)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0179)  triple_40: 0.0000 (0.0105)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 920/1724]  eta: 0:52:35  lr: 0.000140  loss: 2.7021 (4.2693)  loss_n_40: 0.5936 (1.0216)  loss_n_60: 0.6557 (1.0105)  loss_n_80: 0.6638 (1.0680)  loss_n_100: 0.7122 (1.1115)  triple_100: 0.0000 (0.0063)  triple_80: 0.0000 (0.0232)  triple_60: 0.0000 (0.0177)  triple_40: 0.0000 (0.0103)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 930/1724]  eta: 0:51:56  lr: 0.000140  loss: 2.5041 (4.2536)  loss_n_40: 0.5707 (1.0172)  loss_n_60: 0.5882 (1.0067)  loss_n_80: 0.6054 (1.0636)  loss_n_100: 0.6990 (1.1076)  triple_100: 0.0000 (0.0063)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0189)  triple_40: 0.0000 (0.0102)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 940/1724]  eta: 0:51:17  lr: 0.000140  loss: 2.4883 (4.2352)  loss_n_40: 0.5503 (1.0122)  loss_n_60: 0.5939 (1.0024)  loss_n_80: 0.6232 (1.0591)  loss_n_100: 0.7472 (1.1034)  triple_100: 0.0000 (0.0062)  triple_80: 0.0000 (0.0229)  triple_60: 0.0000 (0.0187)  triple_40: 0.0000 (0.0101)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 950/1724]  eta: 0:50:38  lr: 0.000140  loss: 2.4817 (4.2167)  loss_n_40: 0.5254 (1.0078)  loss_n_60: 0.5697 (0.9980)  loss_n_80: 0.6232 (1.0545)  loss_n_100: 0.6966 (1.0989)  triple_100: 0.0000 (0.0061)  triple_80: 0.0000 (0.0227)  triple_60: 0.0000 (0.0185)  triple_40: 0.0000 (0.0100)  time: 3.9297  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 960/1724]  eta: 0:49:58  lr: 0.000140  loss: 2.3725 (4.1992)  loss_n_40: 0.5580 (1.0036)  loss_n_60: 0.6027 (0.9940)  loss_n_80: 0.5954 (1.0500)  loss_n_100: 0.6773 (1.0948)  triple_100: 0.0000 (0.0061)  triple_80: 0.0000 (0.0224)  triple_60: 0.0000 (0.0184)  triple_40: 0.0000 (0.0099)  time: 3.9302  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 970/1724]  eta: 0:49:19  lr: 0.000140  loss: 2.3558 (4.1801)  loss_n_40: 0.5223 (0.9988)  loss_n_60: 0.5717 (0.9895)  loss_n_80: 0.5911 (1.0451)  loss_n_100: 0.6773 (1.0903)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0222)  triple_60: 0.0000 (0.0182)  triple_40: 0.0000 (0.0100)  time: 3.9303  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 980/1724]  eta: 0:48:40  lr: 0.000140  loss: 2.3029 (4.1634)  loss_n_40: 0.4915 (0.9945)  loss_n_60: 0.5387 (0.9853)  loss_n_80: 0.5776 (1.0406)  loss_n_100: 0.6681 (1.0861)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0228)  triple_60: 0.0000 (0.0180)  triple_40: 0.0000 (0.0099)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [ 990/1724]  eta: 0:48:01  lr: 0.000140  loss: 2.4624 (4.1474)  loss_n_40: 0.5267 (0.9908)  loss_n_60: 0.5867 (0.9815)  loss_n_80: 0.6119 (1.0367)  loss_n_100: 0.6821 (1.0823)  triple_100: 0.0000 (0.0059)  triple_80: 0.0000 (0.0226)  triple_60: 0.0000 (0.0178)  triple_40: 0.0000 (0.0098)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1000/1724]  eta: 0:47:22  lr: 0.000140  loss: 2.4624 (4.1306)  loss_n_40: 0.5573 (0.9871)  loss_n_60: 0.5941 (0.9776)  loss_n_80: 0.6219 (1.0323)  loss_n_100: 0.6742 (1.0781)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0224)  triple_60: 0.0000 (0.0176)  triple_40: 0.0000 (0.0097)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1010/1724]  eta: 0:46:42  lr: 0.000140  loss: 2.2966 (4.1119)  loss_n_40: 0.5387 (0.9826)  loss_n_60: 0.5422 (0.9730)  loss_n_80: 0.5551 (1.0274)  loss_n_100: 0.6273 (1.0735)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0222)  triple_60: 0.0000 (0.0178)  triple_40: 0.0000 (0.0096)  time: 3.9269  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [1020/1724]  eta: 0:46:03  lr: 0.000140  loss: 2.1853 (4.0938)  loss_n_40: 0.5052 (0.9788)  loss_n_60: 0.5078 (0.9689)  loss_n_80: 0.5296 (1.0227)  loss_n_100: 0.5764 (1.0686)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0219)  triple_60: 0.0000 (0.0176)  triple_40: 0.0000 (0.0095)  time: 3.9278  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [1030/1724]  eta: 0:45:24  lr: 0.000140  loss: 2.0032 (4.0739)  loss_n_40: 0.4640 (0.9741)  loss_n_60: 0.4820 (0.9642)  loss_n_80: 0.4994 (1.0176)  loss_n_100: 0.5666 (1.0638)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0217)  triple_60: 0.0000 (0.0174)  triple_40: 0.0000 (0.0094)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1040/1724]  eta: 0:44:45  lr: 0.000140  loss: 1.9236 (4.0541)  loss_n_40: 0.4522 (0.9693)  loss_n_60: 0.4500 (0.9594)  loss_n_80: 0.4909 (1.0126)  loss_n_100: 0.5518 (1.0589)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0215)  triple_60: 0.0000 (0.0173)  triple_40: 0.0000 (0.0094)  time: 3.9277  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1050/1724]  eta: 0:44:05  lr: 0.000140  loss: 1.9236 (4.0345)  loss_n_40: 0.4605 (0.9647)  loss_n_60: 0.4587 (0.9547)  loss_n_80: 0.4803 (1.0077)  loss_n_100: 0.5480 (1.0541)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0213)  triple_60: 0.0000 (0.0171)  triple_40: 0.0000 (0.0093)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1060/1724]  eta: 0:43:26  lr: 0.000140  loss: 2.2145 (4.0355)  loss_n_40: 0.5096 (0.9629)  loss_n_60: 0.5153 (0.9540)  loss_n_80: 0.5493 (1.0073)  loss_n_100: 0.6017 (1.0541)  triple_100: 0.0000 (0.0069)  triple_80: 0.0000 (0.0233)  triple_60: 0.0000 (0.0178)  triple_40: 0.0000 (0.0092)  time: 3.9284  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1070/1724]  eta: 0:42:47  lr: 0.000140  loss: 3.8541 (4.0345)  loss_n_40: 0.8589 (0.9628)  loss_n_60: 0.9206 (0.9540)  loss_n_80: 0.9388 (1.0071)  loss_n_100: 0.9845 (1.0540)  triple_100: 0.0000 (0.0069)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0176)  triple_40: 0.0000 (0.0091)  time: 3.9273  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [1080/1724]  eta: 0:42:08  lr: 0.000140  loss: 3.4307 (4.0276)  loss_n_40: 0.8741 (0.9615)  loss_n_60: 0.8512 (0.9522)  loss_n_80: 0.8501 (1.0052)  loss_n_100: 0.9601 (1.0526)  triple_100: 0.0000 (0.0068)  triple_80: 0.0000 (0.0228)  triple_60: 0.0000 (0.0174)  triple_40: 0.0000 (0.0090)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1090/1724]  eta: 0:41:28  lr: 0.000140  loss: 3.0528 (4.0174)  loss_n_40: 0.7347 (0.9598)  loss_n_60: 0.6931 (0.9499)  loss_n_80: 0.7023 (1.0021)  loss_n_100: 0.7878 (1.0499)  triple_100: 0.0000 (0.0068)  triple_80: 0.0000 (0.0226)  triple_60: 0.0000 (0.0173)  triple_40: 0.0000 (0.0089)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1100/1724]  eta: 0:40:49  lr: 0.000140  loss: 2.6077 (4.0032)  loss_n_40: 0.5990 (0.9563)  loss_n_60: 0.6288 (0.9465)  loss_n_80: 0.6181 (0.9985)  loss_n_100: 0.7163 (1.0468)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0224)  triple_60: 0.0000 (0.0171)  triple_40: 0.0000 (0.0088)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1110/1724]  eta: 0:40:10  lr: 0.000140  loss: 2.3646 (3.9889)  loss_n_40: 0.5679 (0.9529)  loss_n_60: 0.5663 (0.9431)  loss_n_80: 0.5922 (0.9949)  loss_n_100: 0.6610 (1.0434)  triple_100: 0.0000 (0.0066)  triple_80: 0.0000 (0.0222)  triple_60: 0.0000 (0.0170)  triple_40: 0.0000 (0.0088)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1120/1724]  eta: 0:39:31  lr: 0.000140  loss: 2.3584 (3.9731)  loss_n_40: 0.5429 (0.9491)  loss_n_60: 0.5481 (0.9394)  loss_n_80: 0.5781 (0.9909)  loss_n_100: 0.6402 (1.0396)  triple_100: 0.0000 (0.0066)  triple_80: 0.0000 (0.0220)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0087)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1130/1724]  eta: 0:38:51  lr: 0.000140  loss: 2.3373 (3.9595)  loss_n_40: 0.5389 (0.9463)  loss_n_60: 0.5481 (0.9362)  loss_n_80: 0.5599 (0.9873)  loss_n_100: 0.6233 (1.0360)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0219)  triple_60: 0.0000 (0.0167)  triple_40: 0.0000 (0.0086)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1140/1724]  eta: 0:38:12  lr: 0.000140  loss: 2.2351 (3.9435)  loss_n_40: 0.5163 (0.9426)  loss_n_60: 0.5326 (0.9325)  loss_n_80: 0.5327 (0.9831)  loss_n_100: 0.6013 (1.0321)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0217)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0085)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1150/1724]  eta: 0:37:33  lr: 0.000140  loss: 2.1997 (3.9300)  loss_n_40: 0.4983 (0.9396)  loss_n_60: 0.5075 (0.9293)  loss_n_80: 0.5239 (0.9795)  loss_n_100: 0.5965 (1.0286)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0215)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0085)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1160/1724]  eta: 0:36:54  lr: 0.000140  loss: 2.5682 (3.9348)  loss_n_40: 0.5877 (0.9373)  loss_n_60: 0.5622 (0.9271)  loss_n_80: 0.6076 (0.9773)  loss_n_100: 0.6483 (1.0266)  triple_100: 0.0000 (0.0115)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0182)  triple_40: 0.0000 (0.0133)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1170/1724]  eta: 0:36:14  lr: 0.000140  loss: 5.7471 (3.9600)  loss_n_40: 0.9170 (0.9420)  loss_n_60: 0.9762 (0.9309)  loss_n_80: 1.0190 (0.9823)  loss_n_100: 1.0424 (1.0318)  triple_100: 0.0000 (0.0151)  triple_80: 0.0000 (0.0266)  triple_60: 0.0000 (0.0181)  triple_40: 0.0000 (0.0132)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1180/1724]  eta: 0:35:35  lr: 0.000140  loss: 5.8681 (3.9748)  loss_n_40: 1.3826 (0.9444)  loss_n_60: 1.3580 (0.9346)  loss_n_80: 1.4329 (0.9868)  loss_n_100: 1.5522 (1.0368)  triple_100: 0.0000 (0.0149)  triple_80: 0.0000 (0.0264)  triple_60: 0.0000 (0.0179)  triple_40: 0.0000 (0.0131)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1190/1724]  eta: 0:34:56  lr: 0.000140  loss: 5.1103 (3.9830)  loss_n_40: 1.1035 (0.9457)  loss_n_60: 1.2210 (0.9367)  loss_n_80: 1.3302 (0.9891)  loss_n_100: 1.4579 (1.0398)  triple_100: 0.0000 (0.0148)  triple_80: 0.0000 (0.0261)  triple_60: 0.0000 (0.0178)  triple_40: 0.0000 (0.0130)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1200/1724]  eta: 0:34:17  lr: 0.000140  loss: 4.6415 (3.9869)  loss_n_40: 1.0227 (0.9461)  loss_n_60: 1.0985 (0.9375)  loss_n_80: 1.1571 (0.9901)  loss_n_100: 1.3169 (1.0420)  triple_100: 0.0000 (0.0147)  triple_80: 0.0000 (0.0259)  triple_60: 0.0000 (0.0176)  triple_40: 0.0000 (0.0129)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1210/1724]  eta: 0:33:37  lr: 0.000140  loss: 3.9998 (3.9892)  loss_n_40: 0.8967 (0.9458)  loss_n_60: 0.9199 (0.9377)  loss_n_80: 1.0200 (0.9906)  loss_n_100: 1.2231 (1.0437)  triple_100: 0.0000 (0.0153)  triple_80: 0.0000 (0.0257)  triple_60: 0.0000 (0.0175)  triple_40: 0.0000 (0.0128)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1220/1724]  eta: 0:32:58  lr: 0.000140  loss: 3.9187 (3.9892)  loss_n_40: 0.8356 (0.9446)  loss_n_60: 0.9007 (0.9377)  loss_n_80: 0.9587 (0.9908)  loss_n_100: 1.2005 (1.0453)  triple_100: 0.0000 (0.0152)  triple_80: 0.0000 (0.0255)  triple_60: 0.0000 (0.0174)  triple_40: 0.0000 (0.0127)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1230/1724]  eta: 0:32:19  lr: 0.000140  loss: 3.5942 (3.9839)  loss_n_40: 0.7803 (0.9431)  loss_n_60: 0.8324 (0.9366)  loss_n_80: 0.8748 (0.9894)  loss_n_100: 1.1009 (1.0446)  triple_100: 0.0000 (0.0151)  triple_80: 0.0000 (0.0253)  triple_60: 0.0000 (0.0172)  triple_40: 0.0000 (0.0127)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1240/1724]  eta: 0:31:40  lr: 0.000140  loss: 3.2360 (3.9787)  loss_n_40: 0.7456 (0.9421)  loss_n_60: 0.7853 (0.9356)  loss_n_80: 0.7597 (0.9879)  loss_n_100: 0.8642 (1.0435)  triple_100: 0.0000 (0.0149)  triple_80: 0.0000 (0.0251)  triple_60: 0.0000 (0.0171)  triple_40: 0.0000 (0.0126)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1250/1724]  eta: 0:31:00  lr: 0.000140  loss: 3.0668 (3.9720)  loss_n_40: 0.6475 (0.9404)  loss_n_60: 0.7539 (0.9342)  loss_n_80: 0.7358 (0.9860)  loss_n_100: 0.8787 (1.0423)  triple_100: 0.0000 (0.0148)  triple_80: 0.0000 (0.0249)  triple_60: 0.0000 (0.0169)  triple_40: 0.0000 (0.0125)  time: 3.9264  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [1260/1724]  eta: 0:30:21  lr: 0.000140  loss: 2.9421 (3.9618)  loss_n_40: 0.6137 (0.9378)  loss_n_60: 0.7221 (0.9319)  loss_n_80: 0.7227 (0.9833)  loss_n_100: 0.8337 (1.0402)  triple_100: 0.0000 (0.0147)  triple_80: 0.0000 (0.0247)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0124)  time: 3.9264  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [1270/1724]  eta: 0:29:42  lr: 0.000140  loss: 2.5801 (3.9510)  loss_n_40: 0.5842 (0.9351)  loss_n_60: 0.6320 (0.9295)  loss_n_80: 0.6170 (0.9805)  loss_n_100: 0.7492 (1.0378)  triple_100: 0.0000 (0.0146)  triple_80: 0.0000 (0.0245)  triple_60: 0.0000 (0.0167)  triple_40: 0.0000 (0.0123)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1280/1724]  eta: 0:29:03  lr: 0.000140  loss: 2.5165 (3.9399)  loss_n_40: 0.5719 (0.9327)  loss_n_60: 0.5923 (0.9269)  loss_n_80: 0.6079 (0.9776)  loss_n_100: 0.7237 (1.0352)  triple_100: 0.0000 (0.0145)  triple_80: 0.0000 (0.0243)  triple_60: 0.0000 (0.0166)  triple_40: 0.0000 (0.0122)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1290/1724]  eta: 0:28:23  lr: 0.000140  loss: 2.4556 (3.9290)  loss_n_40: 0.5333 (0.9299)  loss_n_60: 0.5923 (0.9244)  loss_n_80: 0.5816 (0.9748)  loss_n_100: 0.6904 (1.0328)  triple_100: 0.0000 (0.0144)  triple_80: 0.0000 (0.0241)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0121)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1300/1724]  eta: 0:27:44  lr: 0.000140  loss: 2.3529 (3.9171)  loss_n_40: 0.5095 (0.9267)  loss_n_60: 0.5800 (0.9215)  loss_n_80: 0.5759 (0.9718)  loss_n_100: 0.6821 (1.0302)  triple_100: 0.0000 (0.0143)  triple_80: 0.0000 (0.0239)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0125)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1310/1724]  eta: 0:27:05  lr: 0.000140  loss: 2.2603 (3.9046)  loss_n_40: 0.5068 (0.9238)  loss_n_60: 0.5344 (0.9187)  loss_n_80: 0.5420 (0.9685)  loss_n_100: 0.6527 (1.0272)  triple_100: 0.0000 (0.0142)  triple_80: 0.0000 (0.0238)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0124)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1320/1724]  eta: 0:26:25  lr: 0.000140  loss: 2.2603 (3.8928)  loss_n_40: 0.4827 (0.9206)  loss_n_60: 0.5567 (0.9158)  loss_n_80: 0.5425 (0.9654)  loss_n_100: 0.6328 (1.0243)  triple_100: 0.0000 (0.0140)  triple_80: 0.0000 (0.0236)  triple_60: 0.0000 (0.0160)  triple_40: 0.0000 (0.0131)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1330/1724]  eta: 0:25:46  lr: 0.000140  loss: 2.2250 (3.8797)  loss_n_40: 0.4698 (0.9172)  loss_n_60: 0.5146 (0.9127)  loss_n_80: 0.5628 (0.9622)  loss_n_100: 0.6328 (1.0213)  triple_100: 0.0000 (0.0139)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0159)  triple_40: 0.0000 (0.0130)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1340/1724]  eta: 0:25:07  lr: 0.000140  loss: 2.2946 (3.9003)  loss_n_40: 0.5206 (0.9188)  loss_n_60: 0.5419 (0.9146)  loss_n_80: 0.5959 (0.9648)  loss_n_100: 0.6839 (1.0244)  triple_100: 0.0000 (0.0182)  triple_80: 0.0000 (0.0281)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0149)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1350/1724]  eta: 0:24:28  lr: 0.000140  loss: 7.3714 (3.9289)  loss_n_40: 1.2933 (0.9225)  loss_n_60: 1.6413 (0.9210)  loss_n_80: 2.0066 (0.9742)  loss_n_100: 2.0249 (1.0326)  triple_100: 0.0000 (0.0195)  triple_80: 0.0000 (0.0279)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0148)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1360/1724]  eta: 0:23:48  lr: 0.000140  loss: 6.8746 (3.9457)  loss_n_40: 1.3743 (0.9249)  loss_n_60: 1.6413 (0.9248)  loss_n_80: 2.0566 (0.9805)  loss_n_100: 1.9022 (1.0376)  triple_100: 0.0000 (0.0193)  triple_80: 0.0000 (0.0277)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0147)  time: 3.9271  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [1370/1724]  eta: 0:23:09  lr: 0.000140  loss: 5.1656 (3.9527)  loss_n_40: 0.9664 (0.9250)  loss_n_60: 1.2214 (0.9265)  loss_n_80: 1.5168 (0.9836)  loss_n_100: 1.4831 (1.0402)  triple_100: 0.0000 (0.0192)  triple_80: 0.0000 (0.0275)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0146)  time: 3.9279  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [1380/1724]  eta: 0:22:30  lr: 0.000140  loss: 4.5930 (3.9616)  loss_n_40: 0.9348 (0.9256)  loss_n_60: 1.0638 (0.9272)  loss_n_80: 1.2703 (0.9852)  loss_n_100: 1.2871 (1.0413)  triple_100: 0.0000 (0.0204)  triple_80: 0.0000 (0.0296)  triple_60: 0.0000 (0.0178)  triple_40: 0.0000 (0.0145)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1390/1724]  eta: 0:21:51  lr: 0.000140  loss: 4.6889 (3.9681)  loss_n_40: 1.1565 (0.9276)  loss_n_60: 1.0671 (0.9286)  loss_n_80: 1.2328 (0.9871)  loss_n_100: 1.2205 (1.0431)  triple_100: 0.0000 (0.0202)  triple_80: 0.0000 (0.0294)  triple_60: 0.0000 (0.0177)  triple_40: 0.0000 (0.0144)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1400/1724]  eta: 0:21:11  lr: 0.000140  loss: 4.2891 (3.9679)  loss_n_40: 1.0937 (0.9274)  loss_n_60: 0.9889 (0.9284)  loss_n_80: 1.0561 (0.9871)  loss_n_100: 1.2145 (1.0440)  triple_100: 0.0000 (0.0201)  triple_80: 0.0000 (0.0292)  triple_60: 0.0000 (0.0175)  triple_40: 0.0000 (0.0143)  time: 3.9287  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1410/1724]  eta: 0:20:32  lr: 0.000140  loss: 3.5410 (3.9630)  loss_n_40: 0.7909 (0.9262)  loss_n_60: 0.7894 (0.9273)  loss_n_80: 0.8849 (0.9858)  loss_n_100: 1.0282 (1.0431)  triple_100: 0.0000 (0.0199)  triple_80: 0.0000 (0.0290)  triple_60: 0.0000 (0.0174)  triple_40: 0.0000 (0.0142)  time: 3.9289  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1420/1724]  eta: 0:19:53  lr: 0.000140  loss: 3.1101 (3.9572)  loss_n_40: 0.6786 (0.9245)  loss_n_60: 0.7422 (0.9260)  loss_n_80: 0.7868 (0.9845)  loss_n_100: 0.9294 (1.0423)  triple_100: 0.0000 (0.0198)  triple_80: 0.0000 (0.0288)  triple_60: 0.0000 (0.0173)  triple_40: 0.0000 (0.0141)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1430/1724]  eta: 0:19:14  lr: 0.000140  loss: 2.9986 (3.9502)  loss_n_40: 0.6726 (0.9229)  loss_n_60: 0.7059 (0.9245)  loss_n_80: 0.7727 (0.9828)  loss_n_100: 0.8504 (1.0407)  triple_100: 0.0000 (0.0196)  triple_80: 0.0000 (0.0286)  triple_60: 0.0000 (0.0172)  triple_40: 0.0000 (0.0140)  time: 3.9292  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1440/1724]  eta: 0:18:34  lr: 0.000140  loss: 2.6121 (3.9403)  loss_n_40: 0.5836 (0.9206)  loss_n_60: 0.6256 (0.9222)  loss_n_80: 0.6565 (0.9803)  loss_n_100: 0.7195 (1.0384)  triple_100: 0.0000 (0.0195)  triple_80: 0.0000 (0.0284)  triple_60: 0.0000 (0.0171)  triple_40: 0.0000 (0.0139)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1450/1724]  eta: 0:17:55  lr: 0.000140  loss: 2.4005 (3.9303)  loss_n_40: 0.5442 (0.9183)  loss_n_60: 0.5780 (0.9199)  loss_n_80: 0.6038 (0.9779)  loss_n_100: 0.6598 (1.0358)  triple_100: 0.0000 (0.0194)  triple_80: 0.0000 (0.0282)  triple_60: 0.0000 (0.0169)  triple_40: 0.0000 (0.0138)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1460/1724]  eta: 0:17:16  lr: 0.000140  loss: 2.3164 (3.9194)  loss_n_40: 0.4933 (0.9158)  loss_n_60: 0.5626 (0.9175)  loss_n_80: 0.5795 (0.9752)  loss_n_100: 0.6352 (1.0331)  triple_100: 0.0000 (0.0192)  triple_80: 0.0000 (0.0280)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0137)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1470/1724]  eta: 0:16:37  lr: 0.000140  loss: 2.4889 (3.9135)  loss_n_40: 0.5896 (0.9144)  loss_n_60: 0.5693 (0.9157)  loss_n_80: 0.6193 (0.9735)  loss_n_100: 0.6771 (1.0315)  triple_100: 0.0000 (0.0194)  triple_80: 0.0000 (0.0278)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0144)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1480/1724]  eta: 0:15:57  lr: 0.000140  loss: 3.3314 (3.9117)  loss_n_40: 0.7760 (0.9140)  loss_n_60: 0.7682 (0.9150)  loss_n_80: 0.7566 (0.9726)  loss_n_100: 0.8324 (1.0305)  triple_100: 0.0000 (0.0192)  triple_80: 0.0000 (0.0286)  triple_60: 0.0000 (0.0167)  triple_40: 0.0000 (0.0152)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1490/1724]  eta: 0:15:18  lr: 0.000140  loss: 3.2928 (3.9067)  loss_n_40: 0.7760 (0.9127)  loss_n_60: 0.8003 (0.9141)  loss_n_80: 0.7568 (0.9711)  loss_n_100: 0.8362 (1.0294)  triple_100: 0.0000 (0.0191)  triple_80: 0.0000 (0.0284)  triple_60: 0.0000 (0.0166)  triple_40: 0.0000 (0.0154)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1500/1724]  eta: 0:14:39  lr: 0.000140  loss: 2.7282 (3.8981)  loss_n_40: 0.6200 (0.9106)  loss_n_60: 0.6904 (0.9123)  loss_n_80: 0.6840 (0.9691)  loss_n_100: 0.7596 (1.0272)  triple_100: 0.0000 (0.0190)  triple_80: 0.0000 (0.0282)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0153)  time: 3.9286  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [1510/1724]  eta: 0:14:00  lr: 0.000140  loss: 2.4172 (3.8886)  loss_n_40: 0.5393 (0.9084)  loss_n_60: 0.5987 (0.9102)  loss_n_80: 0.6321 (0.9668)  loss_n_100: 0.6570 (1.0249)  triple_100: 0.0000 (0.0188)  triple_80: 0.0000 (0.0280)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0152)  time: 3.9291  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1520/1724]  eta: 0:13:20  lr: 0.000140  loss: 2.3528 (3.8791)  loss_n_40: 0.5477 (0.9061)  loss_n_60: 0.5960 (0.9081)  loss_n_80: 0.6037 (0.9646)  loss_n_100: 0.6490 (1.0225)  triple_100: 0.0000 (0.0187)  triple_80: 0.0000 (0.0278)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0151)  time: 3.9307  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1530/1724]  eta: 0:12:41  lr: 0.000140  loss: 2.3551 (3.8714)  loss_n_40: 0.5285 (0.9044)  loss_n_60: 0.5872 (0.9061)  loss_n_80: 0.6129 (0.9623)  loss_n_100: 0.6658 (1.0203)  triple_100: 0.0000 (0.0190)  triple_80: 0.0000 (0.0278)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0150)  time: 3.9309  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1540/1724]  eta: 0:12:02  lr: 0.000140  loss: 3.0260 (3.8675)  loss_n_40: 0.6902 (0.9031)  loss_n_60: 0.6592 (0.9048)  loss_n_80: 0.6650 (0.9613)  loss_n_100: 0.7474 (1.0195)  triple_100: 0.0000 (0.0196)  triple_80: 0.0000 (0.0279)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0149)  time: 3.9303  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1550/1724]  eta: 0:11:23  lr: 0.000140  loss: 3.1160 (3.8715)  loss_n_40: 0.6963 (0.9021)  loss_n_60: 0.6891 (0.9038)  loss_n_80: 0.7797 (0.9603)  loss_n_100: 0.8757 (1.0187)  triple_100: 0.0000 (0.0254)  triple_80: 0.0000 (0.0297)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0148)  time: 3.9309  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1560/1724]  eta: 0:10:43  lr: 0.000140  loss: 4.6513 (3.8949)  loss_n_40: 0.9470 (0.9042)  loss_n_60: 0.9810 (0.9081)  loss_n_80: 1.0746 (0.9666)  loss_n_100: 1.1130 (1.0263)  triple_100: 0.0000 (0.0265)  triple_80: 0.0000 (0.0318)  triple_60: 0.0000 (0.0167)  triple_40: 0.0000 (0.0147)  time: 3.9307  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1570/1724]  eta: 0:10:04  lr: 0.000140  loss: 7.5846 (3.9173)  loss_n_40: 1.4608 (0.9080)  loss_n_60: 1.6841 (0.9133)  loss_n_80: 2.1156 (0.9731)  loss_n_100: 2.2901 (1.0337)  triple_100: 0.0000 (0.0264)  triple_80: 0.0000 (0.0316)  triple_60: 0.0000 (0.0166)  triple_40: 0.0000 (0.0146)  time: 3.9300  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1580/1724]  eta: 0:09:25  lr: 0.000140  loss: 6.4111 (3.9301)  loss_n_40: 1.3834 (0.9106)  loss_n_60: 1.5085 (0.9164)  loss_n_80: 1.6760 (0.9767)  loss_n_100: 1.8845 (1.0379)  triple_100: 0.0000 (0.0262)  triple_80: 0.0000 (0.0314)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0145)  time: 3.9314  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1590/1724]  eta: 0:08:46  lr: 0.000140  loss: 5.3520 (3.9364)  loss_n_40: 1.2335 (0.9122)  loss_n_60: 1.2851 (0.9183)  loss_n_80: 1.3112 (0.9782)  loss_n_100: 1.4504 (1.0398)  triple_100: 0.0000 (0.0260)  triple_80: 0.0000 (0.0312)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0144)  time: 3.9310  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1600/1724]  eta: 0:08:06  lr: 0.000140  loss: 4.5225 (3.9382)  loss_n_40: 1.1142 (0.9129)  loss_n_60: 1.0879 (0.9188)  loss_n_80: 1.0867 (0.9784)  loss_n_100: 1.2675 (1.0406)  triple_100: 0.0000 (0.0259)  triple_80: 0.0000 (0.0310)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0143)  time: 3.9295  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [1610/1724]  eta: 0:07:27  lr: 0.000140  loss: 3.9244 (3.9373)  loss_n_40: 0.9889 (0.9131)  loss_n_60: 0.9373 (0.9187)  loss_n_80: 0.9490 (0.9781)  loss_n_100: 1.0600 (1.0405)  triple_100: 0.0000 (0.0257)  triple_80: 0.0000 (0.0308)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0142)  time: 3.9294  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [1620/1724]  eta: 0:06:48  lr: 0.000140  loss: 3.7632 (3.9369)  loss_n_40: 0.9392 (0.9131)  loss_n_60: 0.8822 (0.9188)  loss_n_80: 0.8883 (0.9779)  loss_n_100: 1.0526 (1.0407)  triple_100: 0.0000 (0.0255)  triple_80: 0.0000 (0.0306)  triple_60: 0.0000 (0.0161)  triple_40: 0.0000 (0.0141)  time: 3.9285  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1630/1724]  eta: 0:06:09  lr: 0.000140  loss: 3.5090 (3.9331)  loss_n_40: 0.8280 (0.9123)  loss_n_60: 0.8822 (0.9181)  loss_n_80: 0.8170 (0.9767)  loss_n_100: 1.0064 (1.0402)  triple_100: 0.0000 (0.0254)  triple_80: 0.0000 (0.0304)  triple_60: 0.0000 (0.0160)  triple_40: 0.0000 (0.0140)  time: 3.9280  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1640/1724]  eta: 0:05:29  lr: 0.000140  loss: 3.3714 (3.9303)  loss_n_40: 0.7978 (0.9119)  loss_n_60: 0.8090 (0.9176)  loss_n_80: 0.7982 (0.9758)  loss_n_100: 0.9759 (1.0397)  triple_100: 0.0000 (0.0252)  triple_80: 0.0000 (0.0303)  triple_60: 0.0000 (0.0159)  triple_40: 0.0000 (0.0140)  time: 3.9278  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1650/1724]  eta: 0:04:50  lr: 0.000140  loss: 3.3337 (3.9254)  loss_n_40: 0.7861 (0.9108)  loss_n_60: 0.7815 (0.9166)  loss_n_80: 0.7801 (0.9744)  loss_n_100: 0.8993 (1.0387)  triple_100: 0.0000 (0.0251)  triple_80: 0.0000 (0.0301)  triple_60: 0.0000 (0.0158)  triple_40: 0.0000 (0.0139)  time: 3.9274  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1660/1724]  eta: 0:04:11  lr: 0.000140  loss: 2.8945 (3.9187)  loss_n_40: 0.6700 (0.9095)  loss_n_60: 0.6940 (0.9150)  loss_n_80: 0.6941 (0.9725)  loss_n_100: 0.8223 (1.0373)  triple_100: 0.0000 (0.0249)  triple_80: 0.0000 (0.0299)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0138)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1670/1724]  eta: 0:03:32  lr: 0.000140  loss: 2.7571 (3.9117)  loss_n_40: 0.6435 (0.9079)  loss_n_60: 0.6334 (0.9135)  loss_n_80: 0.6597 (0.9706)  loss_n_100: 0.7985 (1.0359)  triple_100: 0.0000 (0.0248)  triple_80: 0.0000 (0.0297)  triple_60: 0.0000 (0.0156)  triple_40: 0.0000 (0.0137)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1680/1724]  eta: 0:02:52  lr: 0.000140  loss: 2.6603 (3.9041)  loss_n_40: 0.6289 (0.9063)  loss_n_60: 0.6467 (0.9119)  loss_n_80: 0.6354 (0.9686)  loss_n_100: 0.7504 (1.0341)  triple_100: 0.0000 (0.0246)  triple_80: 0.0000 (0.0295)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0136)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1690/1724]  eta: 0:02:13  lr: 0.000140  loss: 2.6655 (3.8983)  loss_n_40: 0.6500 (0.9051)  loss_n_60: 0.6467 (0.9105)  loss_n_80: 0.6354 (0.9670)  loss_n_100: 0.7553 (1.0328)  triple_100: 0.0000 (0.0245)  triple_80: 0.0000 (0.0294)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0135)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1700/1724]  eta: 0:01:34  lr: 0.000140  loss: 2.5156 (3.8900)  loss_n_40: 0.5814 (0.9029)  loss_n_60: 0.5933 (0.9086)  loss_n_80: 0.6244 (0.9648)  loss_n_100: 0.7542 (1.0309)  triple_100: 0.0000 (0.0243)  triple_80: 0.0000 (0.0292)  triple_60: 0.0000 (0.0156)  triple_40: 0.0000 (0.0137)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:8]  [1710/1724]  eta: 0:00:54  lr: 0.000140  loss: 2.3434 (3.8813)  loss_n_40: 0.5171 (0.9007)  loss_n_60: 0.5555 (0.9065)  loss_n_80: 0.5589 (0.9626)  loss_n_100: 0.7044 (1.0291)  triple_100: 0.0000 (0.0242)  triple_80: 0.0000 (0.0290)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0136)  time: 3.9264  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [1720/1724]  eta: 0:00:15  lr: 0.000140  loss: 2.4561 (3.8737)  loss_n_40: 0.5459 (0.8991)  loss_n_60: 0.5603 (0.9048)  loss_n_80: 0.5855 (0.9606)  loss_n_100: 0.7124 (1.0274)  triple_100: 0.0000 (0.0241)  triple_80: 0.0000 (0.0289)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0136)  time: 3.9255  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8]  [1723/1724]  eta: 0:00:03  lr: 0.000140  loss: 2.3475 (3.8718)  loss_n_40: 0.5809 (0.8988)  loss_n_60: 0.5495 (0.9043)  loss_n_80: 0.5855 (0.9600)  loss_n_100: 0.7044 (1.0268)  triple_100: 0.0000 (0.0240)  triple_80: 0.0000 (0.0289)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0135)  time: 3.9254  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:8] Total time: 1:52:49 (3.9265 s / it)\n",
      "Averaged stats: lr: 0.000140  loss: 2.3475 (3.8718)  loss_n_40: 0.5809 (0.8988)  loss_n_60: 0.5495 (0.9043)  loss_n_80: 0.5855 (0.9600)  loss_n_100: 0.7044 (1.0268)  triple_100: 0.0000 (0.0240)  triple_80: 0.0000 (0.0289)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0135)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:8]  [  0/845]  eta: 0:10:14  loss: 2.0041 (2.0041)  loss_n_40: 0.4220 (0.4220)  loss_n_60: 0.4603 (0.4603)  loss_n_80: 0.5028 (0.5028)  loss_n_100: 0.6190 (0.6190)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7268  data: 0.3886  max mem: 46473\n",
      "Valid: [epoch:8]  [ 10/845]  eta: 0:05:08  loss: 2.3646 (2.4965)  loss_n_40: 0.5257 (0.6447)  loss_n_60: 0.4876 (0.5543)  loss_n_80: 0.5229 (0.5899)  loss_n_100: 0.6932 (0.7075)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3698  data: 0.0354  max mem: 46473\n",
      "Valid: [epoch:8]  [ 20/845]  eta: 0:04:50  loss: 2.3260 (2.5080)  loss_n_40: 0.5012 (0.6122)  loss_n_60: 0.4876 (0.5679)  loss_n_80: 0.5229 (0.6110)  loss_n_100: 0.6932 (0.7170)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [ 30/845]  eta: 0:04:42  loss: 2.1103 (2.4523)  loss_n_40: 0.4646 (0.5784)  loss_n_60: 0.4697 (0.5616)  loss_n_80: 0.5148 (0.6019)  loss_n_100: 0.6332 (0.7105)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [ 40/845]  eta: 0:04:36  loss: 1.8773 (2.3516)  loss_n_40: 0.3961 (0.5379)  loss_n_60: 0.4534 (0.5421)  loss_n_80: 0.4700 (0.5768)  loss_n_100: 0.5897 (0.6948)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [ 50/845]  eta: 0:04:31  loss: 1.8773 (2.3466)  loss_n_40: 0.3961 (0.5533)  loss_n_60: 0.4476 (0.5402)  loss_n_80: 0.4700 (0.5692)  loss_n_100: 0.5791 (0.6840)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [ 60/845]  eta: 0:04:27  loss: 2.1871 (2.4015)  loss_n_40: 0.4176 (0.5740)  loss_n_60: 0.4970 (0.5521)  loss_n_80: 0.5385 (0.5802)  loss_n_100: 0.7011 (0.6952)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [ 70/845]  eta: 0:04:23  loss: 2.3153 (2.3787)  loss_n_40: 0.4371 (0.5613)  loss_n_60: 0.5186 (0.5502)  loss_n_80: 0.5421 (0.5734)  loss_n_100: 0.7125 (0.6938)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [ 80/845]  eta: 0:04:19  loss: 2.1817 (2.3754)  loss_n_40: 0.4384 (0.5635)  loss_n_60: 0.4957 (0.5454)  loss_n_80: 0.5230 (0.5692)  loss_n_100: 0.6709 (0.6972)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [ 90/845]  eta: 0:04:15  loss: 2.2097 (2.3754)  loss_n_40: 0.4735 (0.5588)  loss_n_60: 0.4885 (0.5470)  loss_n_80: 0.5459 (0.5699)  loss_n_100: 0.6956 (0.6997)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [100/845]  eta: 0:04:12  loss: 2.2097 (2.3469)  loss_n_40: 0.4593 (0.5469)  loss_n_60: 0.4885 (0.5409)  loss_n_80: 0.5268 (0.5643)  loss_n_100: 0.6824 (0.6948)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [110/845]  eta: 0:04:08  loss: 2.2126 (2.3486)  loss_n_40: 0.4398 (0.5500)  loss_n_60: 0.4919 (0.5389)  loss_n_80: 0.5346 (0.5637)  loss_n_100: 0.6669 (0.6960)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [120/845]  eta: 0:04:04  loss: 2.2402 (2.3533)  loss_n_40: 0.4686 (0.5493)  loss_n_60: 0.5018 (0.5421)  loss_n_80: 0.5502 (0.5638)  loss_n_100: 0.6824 (0.6981)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [130/845]  eta: 0:04:01  loss: 2.1251 (2.3489)  loss_n_40: 0.4282 (0.5456)  loss_n_60: 0.5033 (0.5458)  loss_n_80: 0.5073 (0.5634)  loss_n_100: 0.6990 (0.6941)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [140/845]  eta: 0:03:57  loss: 1.9228 (2.3451)  loss_n_40: 0.4273 (0.5436)  loss_n_60: 0.5136 (0.5461)  loss_n_80: 0.4690 (0.5618)  loss_n_100: 0.5960 (0.6937)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [150/845]  eta: 0:03:54  loss: 2.2215 (2.3425)  loss_n_40: 0.4519 (0.5441)  loss_n_60: 0.5259 (0.5448)  loss_n_80: 0.5300 (0.5610)  loss_n_100: 0.6186 (0.6927)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [160/845]  eta: 0:03:50  loss: 2.2326 (2.3519)  loss_n_40: 0.4800 (0.5450)  loss_n_60: 0.5140 (0.5472)  loss_n_80: 0.5517 (0.5653)  loss_n_100: 0.6864 (0.6944)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [170/845]  eta: 0:03:47  loss: 2.2280 (2.3469)  loss_n_40: 0.4576 (0.5457)  loss_n_60: 0.4808 (0.5451)  loss_n_80: 0.4844 (0.5629)  loss_n_100: 0.6777 (0.6932)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [180/845]  eta: 0:03:43  loss: 2.2280 (2.3404)  loss_n_40: 0.4506 (0.5427)  loss_n_60: 0.4808 (0.5432)  loss_n_80: 0.5286 (0.5614)  loss_n_100: 0.6837 (0.6930)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [190/845]  eta: 0:03:40  loss: 2.2547 (2.3516)  loss_n_40: 0.4425 (0.5423)  loss_n_60: 0.4881 (0.5469)  loss_n_80: 0.5302 (0.5650)  loss_n_100: 0.7365 (0.6974)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [200/845]  eta: 0:03:37  loss: 2.2178 (2.3592)  loss_n_40: 0.4589 (0.5467)  loss_n_60: 0.5201 (0.5502)  loss_n_80: 0.5554 (0.5664)  loss_n_100: 0.7021 (0.6959)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [210/845]  eta: 0:03:33  loss: 2.2178 (2.3693)  loss_n_40: 0.4595 (0.5505)  loss_n_60: 0.5201 (0.5529)  loss_n_80: 0.5614 (0.5683)  loss_n_100: 0.7091 (0.6977)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [220/845]  eta: 0:03:30  loss: 2.2918 (2.3837)  loss_n_40: 0.4962 (0.5604)  loss_n_60: 0.5384 (0.5547)  loss_n_80: 0.5695 (0.5704)  loss_n_100: 0.7174 (0.6982)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [230/845]  eta: 0:03:26  loss: 2.1859 (2.3756)  loss_n_40: 0.4596 (0.5561)  loss_n_60: 0.4677 (0.5529)  loss_n_80: 0.5140 (0.5687)  loss_n_100: 0.6832 (0.6979)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3351  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:8]  [240/845]  eta: 0:03:23  loss: 2.0624 (2.3887)  loss_n_40: 0.4383 (0.5618)  loss_n_60: 0.4754 (0.5569)  loss_n_80: 0.4984 (0.5722)  loss_n_100: 0.6774 (0.6978)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3351  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:8]  [250/845]  eta: 0:03:20  loss: 2.2009 (2.3889)  loss_n_40: 0.4581 (0.5606)  loss_n_60: 0.5126 (0.5577)  loss_n_80: 0.5314 (0.5726)  loss_n_100: 0.6829 (0.6980)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3353  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [260/845]  eta: 0:03:16  loss: 2.1805 (2.3855)  loss_n_40: 0.4581 (0.5581)  loss_n_60: 0.5064 (0.5568)  loss_n_80: 0.5242 (0.5723)  loss_n_100: 0.6491 (0.6983)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3359  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:8]  [270/845]  eta: 0:03:13  loss: 2.2603 (2.3912)  loss_n_40: 0.4576 (0.5599)  loss_n_60: 0.5075 (0.5580)  loss_n_80: 0.5052 (0.5738)  loss_n_100: 0.7081 (0.6995)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3357  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:8]  [280/845]  eta: 0:03:09  loss: 2.2603 (2.3879)  loss_n_40: 0.4683 (0.5573)  loss_n_60: 0.4872 (0.5566)  loss_n_80: 0.5242 (0.5731)  loss_n_100: 0.7081 (0.7009)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3352  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:8]  [290/845]  eta: 0:03:06  loss: 1.9263 (2.3767)  loss_n_40: 0.4421 (0.5536)  loss_n_60: 0.4517 (0.5544)  loss_n_80: 0.4520 (0.5701)  loss_n_100: 0.6062 (0.6985)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3352  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:8]  [300/845]  eta: 0:03:03  loss: 2.2348 (2.3864)  loss_n_40: 0.4610 (0.5578)  loss_n_60: 0.4813 (0.5570)  loss_n_80: 0.5353 (0.5715)  loss_n_100: 0.6855 (0.7001)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [310/845]  eta: 0:02:59  loss: 2.2519 (2.3830)  loss_n_40: 0.4620 (0.5566)  loss_n_60: 0.5307 (0.5558)  loss_n_80: 0.5581 (0.5709)  loss_n_100: 0.6948 (0.6996)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [320/845]  eta: 0:02:56  loss: 2.2109 (2.3872)  loss_n_40: 0.4639 (0.5606)  loss_n_60: 0.5230 (0.5565)  loss_n_80: 0.5447 (0.5706)  loss_n_100: 0.6638 (0.6994)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [330/845]  eta: 0:02:53  loss: 2.1549 (2.3818)  loss_n_40: 0.4572 (0.5601)  loss_n_60: 0.4708 (0.5549)  loss_n_80: 0.4882 (0.5692)  loss_n_100: 0.6444 (0.6976)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [340/845]  eta: 0:02:49  loss: 2.0771 (2.3757)  loss_n_40: 0.4315 (0.5590)  loss_n_60: 0.4708 (0.5538)  loss_n_80: 0.4698 (0.5675)  loss_n_100: 0.5666 (0.6954)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [350/845]  eta: 0:02:46  loss: 2.0771 (2.4101)  loss_n_40: 0.4545 (0.5616)  loss_n_60: 0.4792 (0.5554)  loss_n_80: 0.4865 (0.5695)  loss_n_100: 0.5869 (0.6953)  triple_100: 0.0000 (0.0098)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0000)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [360/845]  eta: 0:02:42  loss: 1.9989 (2.4033)  loss_n_40: 0.4210 (0.5600)  loss_n_60: 0.4724 (0.5541)  loss_n_80: 0.4687 (0.5680)  loss_n_100: 0.5869 (0.6935)  triple_100: 0.0000 (0.0095)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [370/845]  eta: 0:02:39  loss: 2.0117 (2.3994)  loss_n_40: 0.4194 (0.5578)  loss_n_60: 0.4877 (0.5534)  loss_n_80: 0.4687 (0.5673)  loss_n_100: 0.6016 (0.6940)  triple_100: 0.0000 (0.0093)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0000)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [380/845]  eta: 0:02:36  loss: 2.3285 (2.3999)  loss_n_40: 0.4667 (0.5574)  loss_n_60: 0.5347 (0.5538)  loss_n_80: 0.5468 (0.5674)  loss_n_100: 0.6835 (0.6952)  triple_100: 0.0000 (0.0090)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0039)  triple_40: 0.0000 (0.0000)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [390/845]  eta: 0:02:32  loss: 2.2663 (2.3925)  loss_n_40: 0.4638 (0.5547)  loss_n_60: 0.5229 (0.5524)  loss_n_80: 0.5388 (0.5659)  loss_n_100: 0.6835 (0.6941)  triple_100: 0.0000 (0.0088)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0038)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [400/845]  eta: 0:02:29  loss: 2.2464 (2.3933)  loss_n_40: 0.4612 (0.5550)  loss_n_60: 0.5037 (0.5528)  loss_n_80: 0.5499 (0.5662)  loss_n_100: 0.7232 (0.6945)  triple_100: 0.0000 (0.0086)  triple_80: 0.0000 (0.0125)  triple_60: 0.0000 (0.0037)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [410/845]  eta: 0:02:26  loss: 2.2379 (2.3987)  loss_n_40: 0.4335 (0.5573)  loss_n_60: 0.5099 (0.5539)  loss_n_80: 0.5645 (0.5678)  loss_n_100: 0.6757 (0.6954)  triple_100: 0.0000 (0.0084)  triple_80: 0.0000 (0.0122)  triple_60: 0.0000 (0.0036)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [420/845]  eta: 0:02:22  loss: 2.1621 (2.3957)  loss_n_40: 0.4160 (0.5551)  loss_n_60: 0.5099 (0.5536)  loss_n_80: 0.5176 (0.5674)  loss_n_100: 0.6480 (0.6960)  triple_100: 0.0000 (0.0082)  triple_80: 0.0000 (0.0119)  triple_60: 0.0000 (0.0035)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [430/845]  eta: 0:02:19  loss: 2.0012 (2.3966)  loss_n_40: 0.4237 (0.5567)  loss_n_60: 0.4700 (0.5542)  loss_n_80: 0.4905 (0.5668)  loss_n_100: 0.6459 (0.6950)  triple_100: 0.0000 (0.0080)  triple_80: 0.0000 (0.0124)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [440/845]  eta: 0:02:15  loss: 2.1273 (2.3952)  loss_n_40: 0.4399 (0.5563)  loss_n_60: 0.4740 (0.5539)  loss_n_80: 0.4905 (0.5666)  loss_n_100: 0.6415 (0.6951)  triple_100: 0.0000 (0.0078)  triple_80: 0.0000 (0.0122)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [450/845]  eta: 0:02:12  loss: 2.0504 (2.3872)  loss_n_40: 0.4399 (0.5538)  loss_n_60: 0.4882 (0.5519)  loss_n_80: 0.4935 (0.5648)  loss_n_100: 0.6271 (0.6939)  triple_100: 0.0000 (0.0076)  triple_80: 0.0000 (0.0119)  triple_60: 0.0000 (0.0033)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [460/845]  eta: 0:02:09  loss: 1.9855 (2.3812)  loss_n_40: 0.4005 (0.5514)  loss_n_60: 0.4493 (0.5505)  loss_n_80: 0.4740 (0.5636)  loss_n_100: 0.6083 (0.6933)  triple_100: 0.0000 (0.0075)  triple_80: 0.0000 (0.0116)  triple_60: 0.0000 (0.0032)  triple_40: 0.0000 (0.0000)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [470/845]  eta: 0:02:05  loss: 2.0567 (2.3844)  loss_n_40: 0.4524 (0.5531)  loss_n_60: 0.5000 (0.5517)  loss_n_80: 0.4885 (0.5644)  loss_n_100: 0.6275 (0.6933)  triple_100: 0.0000 (0.0073)  triple_80: 0.0000 (0.0114)  triple_60: 0.0000 (0.0032)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [480/845]  eta: 0:02:02  loss: 1.9956 (2.3789)  loss_n_40: 0.4177 (0.5528)  loss_n_60: 0.4476 (0.5503)  loss_n_80: 0.4602 (0.5631)  loss_n_100: 0.5780 (0.6913)  triple_100: 0.0000 (0.0072)  triple_80: 0.0000 (0.0111)  triple_60: 0.0000 (0.0031)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [490/845]  eta: 0:01:59  loss: 2.2115 (2.3859)  loss_n_40: 0.4667 (0.5546)  loss_n_60: 0.4721 (0.5521)  loss_n_80: 0.5363 (0.5650)  loss_n_100: 0.6916 (0.6933)  triple_100: 0.0000 (0.0070)  triple_80: 0.0000 (0.0109)  triple_60: 0.0000 (0.0030)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:8]  [500/845]  eta: 0:01:55  loss: 2.2115 (2.3823)  loss_n_40: 0.4667 (0.5541)  loss_n_60: 0.5002 (0.5513)  loss_n_80: 0.5363 (0.5641)  loss_n_100: 0.6997 (0.6922)  triple_100: 0.0000 (0.0069)  triple_80: 0.0000 (0.0107)  triple_60: 0.0000 (0.0030)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [510/845]  eta: 0:01:52  loss: 2.0247 (2.3796)  loss_n_40: 0.4338 (0.5528)  loss_n_60: 0.4786 (0.5506)  loss_n_80: 0.4816 (0.5637)  loss_n_100: 0.6251 (0.6924)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0029)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [520/845]  eta: 0:01:49  loss: 2.2435 (2.3774)  loss_n_40: 0.4588 (0.5518)  loss_n_60: 0.5062 (0.5502)  loss_n_80: 0.5348 (0.5631)  loss_n_100: 0.7051 (0.6926)  triple_100: 0.0000 (0.0066)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0028)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:8]  [530/845]  eta: 0:01:45  loss: 2.2602 (2.3815)  loss_n_40: 0.4784 (0.5534)  loss_n_60: 0.5062 (0.5512)  loss_n_80: 0.5254 (0.5639)  loss_n_100: 0.6729 (0.6936)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0101)  triple_60: 0.0000 (0.0028)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:8]  [540/845]  eta: 0:01:42  loss: 2.0662 (2.3775)  loss_n_40: 0.4624 (0.5532)  loss_n_60: 0.4755 (0.5500)  loss_n_80: 0.5051 (0.5628)  loss_n_100: 0.6481 (0.6925)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0099)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:8]  [550/845]  eta: 0:01:38  loss: 2.0662 (2.3745)  loss_n_40: 0.4392 (0.5521)  loss_n_60: 0.4755 (0.5499)  loss_n_80: 0.4953 (0.5623)  loss_n_100: 0.5948 (0.6916)  triple_100: 0.0000 (0.0063)  triple_80: 0.0000 (0.0097)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:8]  [560/845]  eta: 0:01:35  loss: 2.2742 (2.3815)  loss_n_40: 0.4626 (0.5536)  loss_n_60: 0.5415 (0.5516)  loss_n_80: 0.5679 (0.5644)  loss_n_100: 0.7393 (0.6936)  triple_100: 0.0000 (0.0061)  triple_80: 0.0000 (0.0096)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [570/845]  eta: 0:01:32  loss: 2.2516 (2.3750)  loss_n_40: 0.4575 (0.5512)  loss_n_60: 0.5086 (0.5502)  loss_n_80: 0.5662 (0.5630)  loss_n_100: 0.7391 (0.6926)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0094)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [580/845]  eta: 0:01:28  loss: 1.8829 (2.3709)  loss_n_40: 0.4078 (0.5501)  loss_n_60: 0.4549 (0.5496)  loss_n_80: 0.4610 (0.5621)  loss_n_100: 0.5658 (0.6915)  triple_100: 0.0000 (0.0059)  triple_80: 0.0000 (0.0092)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [590/845]  eta: 0:01:25  loss: 1.8787 (2.3673)  loss_n_40: 0.4085 (0.5492)  loss_n_60: 0.4549 (0.5490)  loss_n_80: 0.4569 (0.5615)  loss_n_100: 0.5614 (0.6902)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0091)  triple_60: 0.0000 (0.0025)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [600/845]  eta: 0:01:22  loss: 1.8904 (2.3707)  loss_n_40: 0.4214 (0.5501)  loss_n_60: 0.4787 (0.5500)  loss_n_80: 0.4716 (0.5626)  loss_n_100: 0.5823 (0.6908)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0089)  triple_60: 0.0000 (0.0025)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [610/845]  eta: 0:01:18  loss: 2.2189 (2.3689)  loss_n_40: 0.4413 (0.5491)  loss_n_60: 0.4906 (0.5494)  loss_n_80: 0.5623 (0.5626)  loss_n_100: 0.6778 (0.6910)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0088)  triple_60: 0.0000 (0.0024)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:8]  [620/845]  eta: 0:01:15  loss: 2.2764 (2.3719)  loss_n_40: 0.4413 (0.5500)  loss_n_60: 0.5043 (0.5506)  loss_n_80: 0.5310 (0.5632)  loss_n_100: 0.6778 (0.6915)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0086)  triple_60: 0.0000 (0.0024)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [630/845]  eta: 0:01:12  loss: 2.0730 (2.3655)  loss_n_40: 0.4059 (0.5474)  loss_n_60: 0.4839 (0.5492)  loss_n_80: 0.4990 (0.5619)  loss_n_100: 0.6436 (0.6907)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0085)  triple_60: 0.0000 (0.0024)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [640/845]  eta: 0:01:08  loss: 1.9175 (2.3642)  loss_n_40: 0.3867 (0.5484)  loss_n_60: 0.4596 (0.5489)  loss_n_80: 0.4680 (0.5616)  loss_n_100: 0.5914 (0.6893)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0084)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [650/845]  eta: 0:01:05  loss: 1.9175 (2.3637)  loss_n_40: 0.4080 (0.5488)  loss_n_60: 0.4881 (0.5491)  loss_n_80: 0.4713 (0.5615)  loss_n_100: 0.5557 (0.6885)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0082)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [660/845]  eta: 0:01:02  loss: 2.2684 (2.3732)  loss_n_40: 0.4834 (0.5501)  loss_n_60: 0.4881 (0.5503)  loss_n_80: 0.5488 (0.5630)  loss_n_100: 0.6745 (0.6897)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0122)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [670/845]  eta: 0:00:58  loss: 2.2186 (2.3718)  loss_n_40: 0.4831 (0.5494)  loss_n_60: 0.4837 (0.5501)  loss_n_80: 0.5397 (0.5626)  loss_n_100: 0.7068 (0.6899)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0120)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [680/845]  eta: 0:00:55  loss: 2.0925 (2.3737)  loss_n_40: 0.4523 (0.5513)  loss_n_60: 0.4994 (0.5504)  loss_n_80: 0.4781 (0.5630)  loss_n_100: 0.6794 (0.6894)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0118)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [690/845]  eta: 0:00:51  loss: 2.3020 (2.3767)  loss_n_40: 0.4671 (0.5518)  loss_n_60: 0.5314 (0.5511)  loss_n_80: 0.5613 (0.5637)  loss_n_100: 0.7204 (0.6908)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0117)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [700/845]  eta: 0:00:48  loss: 2.3659 (2.3773)  loss_n_40: 0.4671 (0.5518)  loss_n_60: 0.4882 (0.5514)  loss_n_80: 0.5534 (0.5642)  loss_n_100: 0.7202 (0.6909)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0115)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [710/845]  eta: 0:00:45  loss: 2.2209 (2.3771)  loss_n_40: 0.4384 (0.5519)  loss_n_60: 0.4730 (0.5513)  loss_n_80: 0.5351 (0.5645)  loss_n_100: 0.6796 (0.6907)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0113)  triple_60: 0.0000 (0.0025)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [720/845]  eta: 0:00:41  loss: 2.2362 (2.3792)  loss_n_40: 0.4486 (0.5518)  loss_n_60: 0.4760 (0.5511)  loss_n_80: 0.5119 (0.5643)  loss_n_100: 0.6796 (0.6908)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0025)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [730/845]  eta: 0:00:38  loss: 2.2820 (2.3803)  loss_n_40: 0.4357 (0.5519)  loss_n_60: 0.5326 (0.5517)  loss_n_80: 0.5170 (0.5646)  loss_n_100: 0.6927 (0.6913)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0025)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [740/845]  eta: 0:00:35  loss: 2.1475 (2.3780)  loss_n_40: 0.4263 (0.5514)  loss_n_60: 0.5213 (0.5511)  loss_n_80: 0.5163 (0.5641)  loss_n_100: 0.6480 (0.6908)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0024)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:8]  [750/845]  eta: 0:00:31  loss: 2.2536 (2.3789)  loss_n_40: 0.4512 (0.5513)  loss_n_60: 0.4932 (0.5514)  loss_n_80: 0.4958 (0.5645)  loss_n_100: 0.6719 (0.6914)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0024)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [760/845]  eta: 0:00:28  loss: 2.2798 (2.3787)  loss_n_40: 0.4977 (0.5513)  loss_n_60: 0.4988 (0.5514)  loss_n_80: 0.5445 (0.5646)  loss_n_100: 0.6719 (0.6913)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0024)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [770/845]  eta: 0:00:25  loss: 2.2681 (2.3804)  loss_n_40: 0.4777 (0.5524)  loss_n_60: 0.4988 (0.5518)  loss_n_80: 0.5395 (0.5649)  loss_n_100: 0.6719 (0.6915)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [780/845]  eta: 0:00:21  loss: 2.1951 (2.3831)  loss_n_40: 0.4663 (0.5535)  loss_n_60: 0.5075 (0.5524)  loss_n_80: 0.5395 (0.5658)  loss_n_100: 0.6836 (0.6919)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:8]  [790/845]  eta: 0:00:18  loss: 2.0195 (2.3797)  loss_n_40: 0.4409 (0.5524)  loss_n_60: 0.4926 (0.5518)  loss_n_80: 0.4883 (0.5650)  loss_n_100: 0.6505 (0.6913)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0126)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [800/845]  eta: 0:00:15  loss: 1.9680 (2.3789)  loss_n_40: 0.4100 (0.5523)  loss_n_60: 0.4532 (0.5518)  loss_n_80: 0.4837 (0.5646)  loss_n_100: 0.6061 (0.6912)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0125)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [810/845]  eta: 0:00:11  loss: 2.2190 (2.3809)  loss_n_40: 0.4505 (0.5536)  loss_n_60: 0.5244 (0.5525)  loss_n_80: 0.5276 (0.5651)  loss_n_100: 0.6864 (0.6910)  triple_100: 0.0000 (0.0042)  triple_80: 0.0000 (0.0123)  triple_60: 0.0000 (0.0022)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [820/845]  eta: 0:00:08  loss: 2.1980 (2.3798)  loss_n_40: 0.4819 (0.5531)  loss_n_60: 0.5197 (0.5524)  loss_n_80: 0.5083 (0.5648)  loss_n_100: 0.6426 (0.6909)  triple_100: 0.0000 (0.0042)  triple_80: 0.0000 (0.0122)  triple_60: 0.0000 (0.0022)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [830/845]  eta: 0:00:05  loss: 2.2676 (2.3829)  loss_n_40: 0.4991 (0.5542)  loss_n_60: 0.5108 (0.5531)  loss_n_80: 0.5386 (0.5655)  loss_n_100: 0.6550 (0.6917)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0120)  triple_60: 0.0000 (0.0022)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [840/845]  eta: 0:00:01  loss: 2.2853 (2.3809)  loss_n_40: 0.4998 (0.5534)  loss_n_60: 0.5203 (0.5528)  loss_n_80: 0.5386 (0.5650)  loss_n_100: 0.6550 (0.6916)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0119)  triple_60: 0.0000 (0.0022)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8]  [844/845]  eta: 0:00:00  loss: 2.2469 (2.3805)  loss_n_40: 0.5065 (0.5533)  loss_n_60: 0.5108 (0.5527)  loss_n_80: 0.5242 (0.5650)  loss_n_100: 0.6401 (0.6915)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0118)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:8] Total time: 0:04:43 (0.3355 s / it)\n",
      "Averaged stats: loss: 2.2469 (2.3805)  loss_n_40: 0.5065 (0.5533)  loss_n_60: 0.5108 (0.5527)  loss_n_80: 0.5242 (0.5650)  loss_n_100: 0.6401 (0.6915)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0118)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_8_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.691%\n",
      "Min loss_n_100: 0.691\n",
      "Best Epoch: 8.000\n",
      "Train: [epoch:9]  [   0/1724]  eta: 2:02:09  lr: 0.000160  loss: 2.5458 (2.5458)  loss_n_40: 0.5980 (0.5980)  loss_n_60: 0.6174 (0.6174)  loss_n_80: 0.6282 (0.6282)  loss_n_100: 0.7021 (0.7021)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.2515  data: 0.4981  max mem: 46473\n",
      "Train: [epoch:9]  [  10/1724]  eta: 1:53:01  lr: 0.000160  loss: 2.9863 (2.8528)  loss_n_40: 0.7236 (0.7033)  loss_n_60: 0.6595 (0.6474)  loss_n_80: 0.6837 (0.6685)  loss_n_100: 0.7416 (0.7642)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0694)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9564  data: 0.0454  max mem: 46473\n",
      "Train: [epoch:9]  [  20/1724]  eta: 1:51:57  lr: 0.000160  loss: 2.6272 (2.7358)  loss_n_40: 0.6815 (0.6879)  loss_n_60: 0.5999 (0.6253)  loss_n_80: 0.6147 (0.6403)  loss_n_100: 0.7246 (0.7460)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0363)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [  30/1724]  eta: 1:51:09  lr: 0.000160  loss: 2.4584 (2.6129)  loss_n_40: 0.6442 (0.6758)  loss_n_60: 0.5606 (0.5926)  loss_n_80: 0.5574 (0.6098)  loss_n_100: 0.6802 (0.7100)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0246)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [  40/1724]  eta: 1:50:25  lr: 0.000160  loss: 2.3698 (2.5648)  loss_n_40: 0.6075 (0.6570)  loss_n_60: 0.5145 (0.5851)  loss_n_80: 0.5427 (0.6009)  loss_n_100: 0.6311 (0.7033)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0186)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [  50/1724]  eta: 1:49:43  lr: 0.000160  loss: 2.3095 (2.5071)  loss_n_40: 0.5694 (0.6401)  loss_n_60: 0.5088 (0.5680)  loss_n_80: 0.5751 (0.5931)  loss_n_100: 0.6417 (0.6910)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [  60/1724]  eta: 1:49:01  lr: 0.000160  loss: 2.1929 (2.4536)  loss_n_40: 0.5494 (0.6284)  loss_n_60: 0.4872 (0.5568)  loss_n_80: 0.5140 (0.5811)  loss_n_100: 0.5936 (0.6749)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0125)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [  70/1724]  eta: 1:48:20  lr: 0.000160  loss: 2.0272 (2.4011)  loss_n_40: 0.5067 (0.6111)  loss_n_60: 0.4525 (0.5455)  loss_n_80: 0.4851 (0.5702)  loss_n_100: 0.5686 (0.6635)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0108)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [  80/1724]  eta: 1:47:40  lr: 0.000160  loss: 2.5284 (2.5186)  loss_n_40: 0.5175 (0.6249)  loss_n_60: 0.5110 (0.5739)  loss_n_80: 0.5910 (0.5923)  loss_n_100: 0.6866 (0.6953)  triple_100: 0.0000 (0.0218)  triple_80: 0.0000 (0.0094)  triple_60: 0.0000 (0.0009)  triple_40: 0.0000 (0.0000)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [  90/1724]  eta: 1:47:00  lr: 0.000160  loss: 3.9503 (2.7191)  loss_n_40: 0.9473 (0.6701)  loss_n_60: 0.9931 (0.6325)  loss_n_80: 0.8927 (0.6414)  loss_n_100: 1.0511 (0.7461)  triple_100: 0.0000 (0.0194)  triple_80: 0.0000 (0.0084)  triple_60: 0.0000 (0.0008)  triple_40: 0.0000 (0.0003)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 100/1724]  eta: 1:46:20  lr: 0.000160  loss: 4.1695 (2.8475)  loss_n_40: 1.0128 (0.7030)  loss_n_60: 1.0177 (0.6619)  loss_n_80: 0.9911 (0.6732)  loss_n_100: 1.1131 (0.7761)  triple_100: 0.0000 (0.0175)  triple_80: 0.0000 (0.0076)  triple_60: 0.0000 (0.0007)  triple_40: 0.0000 (0.0075)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 110/1724]  eta: 1:45:41  lr: 0.000160  loss: 3.1168 (2.8585)  loss_n_40: 0.8050 (0.7060)  loss_n_60: 0.7358 (0.6652)  loss_n_80: 0.7731 (0.6779)  loss_n_100: 0.8403 (0.7790)  triple_100: 0.0000 (0.0159)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0007)  triple_40: 0.0000 (0.0068)  time: 3.9255  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:9]  [ 120/1724]  eta: 1:45:01  lr: 0.000160  loss: 2.9689 (2.8579)  loss_n_40: 0.7079 (0.7045)  loss_n_60: 0.6895 (0.6650)  loss_n_80: 0.7352 (0.6801)  loss_n_100: 0.8089 (0.7805)  triple_100: 0.0000 (0.0146)  triple_80: 0.0000 (0.0063)  triple_60: 0.0000 (0.0006)  triple_40: 0.0000 (0.0063)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 130/1724]  eta: 1:44:21  lr: 0.000160  loss: 2.3335 (2.8167)  loss_n_40: 0.5727 (0.6955)  loss_n_60: 0.5259 (0.6552)  loss_n_80: 0.5791 (0.6710)  loss_n_100: 0.6663 (0.7694)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0006)  triple_40: 0.0000 (0.0058)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 140/1724]  eta: 1:43:41  lr: 0.000160  loss: 2.2026 (2.7714)  loss_n_40: 0.5357 (0.6831)  loss_n_60: 0.4966 (0.6436)  loss_n_80: 0.5482 (0.6619)  loss_n_100: 0.6180 (0.7590)  triple_100: 0.0000 (0.0125)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0005)  triple_40: 0.0000 (0.0054)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 150/1724]  eta: 1:43:02  lr: 0.000160  loss: 2.1594 (2.7221)  loss_n_40: 0.5244 (0.6723)  loss_n_60: 0.4928 (0.6323)  loss_n_80: 0.5116 (0.6501)  loss_n_100: 0.5603 (0.7443)  triple_100: 0.0000 (0.0117)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0005)  triple_40: 0.0000 (0.0055)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 160/1724]  eta: 1:42:22  lr: 0.000160  loss: 2.1571 (2.6971)  loss_n_40: 0.5138 (0.6655)  loss_n_60: 0.5019 (0.6270)  loss_n_80: 0.5167 (0.6451)  loss_n_100: 0.5570 (0.7378)  triple_100: 0.0000 (0.0110)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0005)  triple_40: 0.0000 (0.0051)  time: 3.9243  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 170/1724]  eta: 1:41:43  lr: 0.000160  loss: 2.2445 (2.7000)  loss_n_40: 0.5552 (0.6628)  loss_n_60: 0.5197 (0.6230)  loss_n_80: 0.5504 (0.6408)  loss_n_100: 0.6078 (0.7303)  triple_100: 0.0000 (0.0187)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0064)  triple_40: 0.0000 (0.0048)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 180/1724]  eta: 1:41:03  lr: 0.000160  loss: 2.7122 (2.7793)  loss_n_40: 0.6463 (0.6748)  loss_n_60: 0.6582 (0.6363)  loss_n_80: 0.7161 (0.6605)  loss_n_100: 0.7222 (0.7503)  triple_100: 0.0000 (0.0319)  triple_80: 0.0000 (0.0126)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0048)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 190/1724]  eta: 1:40:23  lr: 0.000160  loss: 4.0275 (2.8406)  loss_n_40: 0.8733 (0.6870)  loss_n_60: 0.8936 (0.6495)  loss_n_80: 1.0188 (0.6797)  loss_n_100: 1.1233 (0.7699)  triple_100: 0.0000 (0.0302)  triple_80: 0.0000 (0.0119)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0046)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 200/1724]  eta: 1:39:44  lr: 0.000160  loss: 3.5027 (2.8637)  loss_n_40: 0.8286 (0.6934)  loss_n_60: 0.8271 (0.6554)  loss_n_80: 0.9308 (0.6877)  loss_n_100: 1.0072 (0.7754)  triple_100: 0.0000 (0.0287)  triple_80: 0.0000 (0.0113)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0043)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 210/1724]  eta: 1:39:05  lr: 0.000160  loss: 3.0190 (2.8559)  loss_n_40: 0.6996 (0.6916)  loss_n_60: 0.6903 (0.6540)  loss_n_80: 0.7615 (0.6881)  loss_n_100: 0.8185 (0.7729)  triple_100: 0.0000 (0.0274)  triple_80: 0.0000 (0.0108)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0041)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 220/1724]  eta: 1:38:25  lr: 0.000160  loss: 2.5729 (2.8415)  loss_n_40: 0.6286 (0.6875)  loss_n_60: 0.6011 (0.6518)  loss_n_80: 0.6663 (0.6867)  loss_n_100: 0.6863 (0.7685)  triple_100: 0.0000 (0.0261)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0039)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 230/1724]  eta: 1:37:46  lr: 0.000160  loss: 2.3498 (2.8160)  loss_n_40: 0.5470 (0.6820)  loss_n_60: 0.5763 (0.6458)  loss_n_80: 0.6061 (0.6816)  loss_n_100: 0.6393 (0.7616)  triple_100: 0.0000 (0.0250)  triple_80: 0.0000 (0.0098)  triple_60: 0.0000 (0.0064)  triple_40: 0.0000 (0.0038)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 240/1724]  eta: 1:37:06  lr: 0.000160  loss: 1.9081 (2.7839)  loss_n_40: 0.4695 (0.6760)  loss_n_60: 0.4476 (0.6389)  loss_n_80: 0.4978 (0.6742)  loss_n_100: 0.5305 (0.7516)  triple_100: 0.0000 (0.0239)  triple_80: 0.0000 (0.0094)  triple_60: 0.0000 (0.0061)  triple_40: 0.0000 (0.0036)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 250/1724]  eta: 1:36:27  lr: 0.000160  loss: 1.9661 (2.7532)  loss_n_40: 0.4695 (0.6682)  loss_n_60: 0.4598 (0.6322)  loss_n_80: 0.5072 (0.6680)  loss_n_100: 0.5357 (0.7435)  triple_100: 0.0000 (0.0230)  triple_80: 0.0000 (0.0091)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0035)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 260/1724]  eta: 1:35:47  lr: 0.000160  loss: 1.9661 (2.7202)  loss_n_40: 0.4233 (0.6589)  loss_n_60: 0.4588 (0.6249)  loss_n_80: 0.5064 (0.6611)  loss_n_100: 0.5405 (0.7354)  triple_100: 0.0000 (0.0221)  triple_80: 0.0000 (0.0087)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0033)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 270/1724]  eta: 1:35:08  lr: 0.000160  loss: 1.8871 (2.6909)  loss_n_40: 0.4224 (0.6519)  loss_n_60: 0.4364 (0.6185)  loss_n_80: 0.4843 (0.6552)  loss_n_100: 0.5002 (0.7269)  triple_100: 0.0000 (0.0213)  triple_80: 0.0000 (0.0084)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0032)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 280/1724]  eta: 1:34:29  lr: 0.000160  loss: 1.8142 (2.6674)  loss_n_40: 0.4266 (0.6463)  loss_n_60: 0.4271 (0.6134)  loss_n_80: 0.4721 (0.6505)  loss_n_100: 0.4986 (0.7202)  triple_100: 0.0000 (0.0205)  triple_80: 0.0000 (0.0081)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0031)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 290/1724]  eta: 1:33:50  lr: 0.000160  loss: 1.8534 (2.6424)  loss_n_40: 0.4235 (0.6402)  loss_n_60: 0.4398 (0.6082)  loss_n_80: 0.4721 (0.6451)  loss_n_100: 0.5008 (0.7132)  triple_100: 0.0000 (0.0198)  triple_80: 0.0000 (0.0078)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0030)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 300/1724]  eta: 1:33:10  lr: 0.000160  loss: 1.9302 (2.6236)  loss_n_40: 0.4399 (0.6355)  loss_n_60: 0.4666 (0.6040)  loss_n_80: 0.4984 (0.6412)  loss_n_100: 0.5202 (0.7084)  triple_100: 0.0000 (0.0192)  triple_80: 0.0000 (0.0076)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0029)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 310/1724]  eta: 1:32:31  lr: 0.000160  loss: 1.9153 (2.6036)  loss_n_40: 0.4720 (0.6315)  loss_n_60: 0.4666 (0.5998)  loss_n_80: 0.4969 (0.6365)  loss_n_100: 0.5334 (0.7025)  triple_100: 0.0000 (0.0186)  triple_80: 0.0000 (0.0073)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0028)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 320/1724]  eta: 1:31:52  lr: 0.000160  loss: 2.0260 (2.5918)  loss_n_40: 0.4663 (0.6274)  loss_n_60: 0.4674 (0.5963)  loss_n_80: 0.5290 (0.6339)  loss_n_100: 0.5365 (0.6996)  triple_100: 0.0000 (0.0180)  triple_80: 0.0000 (0.0088)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0032)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 330/1724]  eta: 1:31:13  lr: 0.000160  loss: 2.1976 (2.5834)  loss_n_40: 0.5196 (0.6254)  loss_n_60: 0.5001 (0.5940)  loss_n_80: 0.5634 (0.6325)  loss_n_100: 0.6173 (0.6980)  triple_100: 0.0000 (0.0174)  triple_80: 0.0000 (0.0085)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0031)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 340/1724]  eta: 1:30:33  lr: 0.000160  loss: 2.1248 (2.5673)  loss_n_40: 0.5196 (0.6223)  loss_n_60: 0.4942 (0.5903)  loss_n_80: 0.5323 (0.6288)  loss_n_100: 0.5788 (0.6932)  triple_100: 0.0000 (0.0169)  triple_80: 0.0000 (0.0083)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0030)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 350/1724]  eta: 1:29:54  lr: 0.000160  loss: 2.0167 (2.5522)  loss_n_40: 0.5111 (0.6191)  loss_n_60: 0.4529 (0.5870)  loss_n_80: 0.4801 (0.6256)  loss_n_100: 0.5264 (0.6889)  triple_100: 0.0000 (0.0164)  triple_80: 0.0000 (0.0081)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0030)  time: 3.9249  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:9]  [ 360/1724]  eta: 1:29:15  lr: 0.000160  loss: 1.8003 (2.5323)  loss_n_40: 0.4607 (0.6143)  loss_n_60: 0.4279 (0.5828)  loss_n_80: 0.4570 (0.6210)  loss_n_100: 0.4834 (0.6834)  triple_100: 0.0000 (0.0160)  triple_80: 0.0000 (0.0078)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0029)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 370/1724]  eta: 1:28:35  lr: 0.000160  loss: 1.7590 (2.5164)  loss_n_40: 0.4303 (0.6106)  loss_n_60: 0.4158 (0.5792)  loss_n_80: 0.4409 (0.6175)  loss_n_100: 0.4845 (0.6792)  triple_100: 0.0000 (0.0156)  triple_80: 0.0000 (0.0076)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0028)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 380/1724]  eta: 1:27:56  lr: 0.000160  loss: 1.7452 (2.4959)  loss_n_40: 0.4224 (0.6053)  loss_n_60: 0.4026 (0.5747)  loss_n_80: 0.4437 (0.6129)  loss_n_100: 0.4917 (0.6738)  triple_100: 0.0000 (0.0151)  triple_80: 0.0000 (0.0074)  triple_60: 0.0000 (0.0039)  triple_40: 0.0000 (0.0027)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 390/1724]  eta: 1:27:17  lr: 0.000160  loss: 1.7452 (2.4825)  loss_n_40: 0.4110 (0.6032)  loss_n_60: 0.4026 (0.5718)  loss_n_80: 0.4479 (0.6096)  loss_n_100: 0.4766 (0.6694)  triple_100: 0.0000 (0.0148)  triple_80: 0.0000 (0.0072)  triple_60: 0.0000 (0.0038)  triple_40: 0.0000 (0.0027)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 400/1724]  eta: 1:26:38  lr: 0.000160  loss: 1.6549 (2.4610)  loss_n_40: 0.3699 (0.5979)  loss_n_60: 0.3937 (0.5669)  loss_n_80: 0.4261 (0.6047)  loss_n_100: 0.4492 (0.6638)  triple_100: 0.0000 (0.0144)  triple_80: 0.0000 (0.0070)  triple_60: 0.0000 (0.0037)  triple_40: 0.0000 (0.0026)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 410/1724]  eta: 1:25:58  lr: 0.000160  loss: 1.6089 (2.4415)  loss_n_40: 0.3754 (0.5931)  loss_n_60: 0.3673 (0.5625)  loss_n_80: 0.4118 (0.6001)  loss_n_100: 0.4359 (0.6585)  triple_100: 0.0000 (0.0140)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0036)  triple_40: 0.0000 (0.0028)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 420/1724]  eta: 1:25:19  lr: 0.000160  loss: 1.6566 (2.4254)  loss_n_40: 0.3892 (0.5896)  loss_n_60: 0.3856 (0.5587)  loss_n_80: 0.4202 (0.5960)  loss_n_100: 0.4359 (0.6536)  triple_100: 0.0000 (0.0146)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0035)  triple_40: 0.0000 (0.0028)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 430/1724]  eta: 1:24:40  lr: 0.000160  loss: 1.7252 (2.4094)  loss_n_40: 0.4193 (0.5852)  loss_n_60: 0.4080 (0.5551)  loss_n_80: 0.4397 (0.5926)  loss_n_100: 0.4682 (0.6495)  triple_100: 0.0000 (0.0142)  triple_80: 0.0000 (0.0066)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0027)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 440/1724]  eta: 1:24:00  lr: 0.000160  loss: 1.7545 (2.3947)  loss_n_40: 0.4171 (0.5818)  loss_n_60: 0.4140 (0.5520)  loss_n_80: 0.4397 (0.5892)  loss_n_100: 0.4499 (0.6453)  triple_100: 0.0000 (0.0139)  triple_80: 0.0000 (0.0064)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0027)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 450/1724]  eta: 1:23:21  lr: 0.000160  loss: 1.6593 (2.3825)  loss_n_40: 0.4015 (0.5789)  loss_n_60: 0.3953 (0.5491)  loss_n_80: 0.4266 (0.5865)  loss_n_100: 0.4477 (0.6417)  triple_100: 0.0000 (0.0136)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0033)  triple_40: 0.0000 (0.0026)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 460/1724]  eta: 1:22:42  lr: 0.000160  loss: 1.9196 (2.3990)  loss_n_40: 0.4492 (0.5777)  loss_n_60: 0.4663 (0.5488)  loss_n_80: 0.4943 (0.5867)  loss_n_100: 0.5167 (0.6411)  triple_100: 0.0000 (0.0133)  triple_80: 0.0000 (0.0213)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0030)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 470/1724]  eta: 1:22:02  lr: 0.000160  loss: 2.3020 (2.4058)  loss_n_40: 0.4926 (0.5777)  loss_n_60: 0.5429 (0.5499)  loss_n_80: 0.5959 (0.5884)  loss_n_100: 0.6690 (0.6424)  triple_100: 0.0000 (0.0168)  triple_80: 0.0000 (0.0209)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0029)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 480/1724]  eta: 1:21:23  lr: 0.000160  loss: 2.3870 (2.4111)  loss_n_40: 0.5493 (0.5794)  loss_n_60: 0.5936 (0.5522)  loss_n_80: 0.6506 (0.5903)  loss_n_100: 0.6829 (0.6429)  triple_100: 0.0000 (0.0164)  triple_80: 0.0000 (0.0205)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0029)  time: 3.9231  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 490/1724]  eta: 1:20:44  lr: 0.000160  loss: 2.1583 (2.4031)  loss_n_40: 0.5154 (0.5770)  loss_n_60: 0.5230 (0.5505)  loss_n_80: 0.5687 (0.5890)  loss_n_100: 0.6095 (0.6412)  triple_100: 0.0000 (0.0161)  triple_80: 0.0000 (0.0200)  triple_60: 0.0000 (0.0065)  triple_40: 0.0000 (0.0028)  time: 3.9231  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 500/1724]  eta: 1:20:04  lr: 0.000160  loss: 1.9665 (2.3962)  loss_n_40: 0.4423 (0.5755)  loss_n_60: 0.4469 (0.5492)  loss_n_80: 0.5087 (0.5876)  loss_n_100: 0.5300 (0.6393)  triple_100: 0.0000 (0.0158)  triple_80: 0.0000 (0.0196)  triple_60: 0.0000 (0.0064)  triple_40: 0.0000 (0.0028)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 510/1724]  eta: 1:19:25  lr: 0.000160  loss: 1.7851 (2.3856)  loss_n_40: 0.4027 (0.5726)  loss_n_60: 0.4162 (0.5471)  loss_n_80: 0.4636 (0.5855)  loss_n_100: 0.5064 (0.6368)  triple_100: 0.0000 (0.0155)  triple_80: 0.0000 (0.0193)  triple_60: 0.0000 (0.0063)  triple_40: 0.0000 (0.0027)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 520/1724]  eta: 1:18:46  lr: 0.000160  loss: 1.6013 (2.3713)  loss_n_40: 0.3878 (0.5692)  loss_n_60: 0.3784 (0.5439)  loss_n_80: 0.4072 (0.5823)  loss_n_100: 0.4514 (0.6331)  triple_100: 0.0000 (0.0152)  triple_80: 0.0000 (0.0189)  triple_60: 0.0000 (0.0061)  triple_40: 0.0000 (0.0026)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 530/1724]  eta: 1:18:06  lr: 0.000160  loss: 1.4924 (2.3568)  loss_n_40: 0.3528 (0.5659)  loss_n_60: 0.3570 (0.5407)  loss_n_80: 0.3996 (0.5790)  loss_n_100: 0.4118 (0.6291)  triple_100: 0.0000 (0.0149)  triple_80: 0.0000 (0.0185)  triple_60: 0.0000 (0.0060)  triple_40: 0.0000 (0.0026)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 540/1724]  eta: 1:17:27  lr: 0.000160  loss: 1.6331 (2.3490)  loss_n_40: 0.3869 (0.5648)  loss_n_60: 0.3802 (0.5393)  loss_n_80: 0.4199 (0.5772)  loss_n_100: 0.4350 (0.6264)  triple_100: 0.0000 (0.0146)  triple_80: 0.0000 (0.0182)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0025)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 550/1724]  eta: 1:16:48  lr: 0.000160  loss: 2.0477 (2.3543)  loss_n_40: 0.4586 (0.5635)  loss_n_60: 0.4785 (0.5390)  loss_n_80: 0.4883 (0.5774)  loss_n_100: 0.4802 (0.6259)  triple_100: 0.0000 (0.0179)  triple_80: 0.0000 (0.0186)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0040)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 560/1724]  eta: 1:16:08  lr: 0.000160  loss: 4.0501 (2.3950)  loss_n_40: 0.7334 (0.5704)  loss_n_60: 0.8365 (0.5485)  loss_n_80: 0.9566 (0.5889)  loss_n_100: 0.9825 (0.6388)  triple_100: 0.0000 (0.0176)  triple_80: 0.0000 (0.0189)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0039)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 570/1724]  eta: 1:15:29  lr: 0.000160  loss: 4.0501 (2.4205)  loss_n_40: 0.8617 (0.5747)  loss_n_60: 0.9814 (0.5555)  loss_n_80: 1.0743 (0.5961)  loss_n_100: 1.1742 (0.6466)  triple_100: 0.0000 (0.0173)  triple_80: 0.0000 (0.0186)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0038)  time: 3.9231  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 580/1724]  eta: 1:14:50  lr: 0.000160  loss: 3.5208 (2.4332)  loss_n_40: 0.7236 (0.5767)  loss_n_60: 0.8548 (0.5594)  loss_n_80: 0.8547 (0.5992)  loss_n_100: 0.9682 (0.6512)  triple_100: 0.0000 (0.0170)  triple_80: 0.0000 (0.0182)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0038)  time: 3.9231  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 590/1724]  eta: 1:14:11  lr: 0.000160  loss: 2.8294 (2.4373)  loss_n_40: 0.5990 (0.5771)  loss_n_60: 0.6573 (0.5604)  loss_n_80: 0.7094 (0.6004)  loss_n_100: 0.8412 (0.6535)  triple_100: 0.0000 (0.0167)  triple_80: 0.0000 (0.0179)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0037)  time: 3.9230  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:9]  [ 600/1724]  eta: 1:13:31  lr: 0.000160  loss: 2.5496 (2.4360)  loss_n_40: 0.5599 (0.5760)  loss_n_60: 0.5746 (0.5600)  loss_n_80: 0.6691 (0.6004)  loss_n_100: 0.7614 (0.6544)  triple_100: 0.0000 (0.0164)  triple_80: 0.0000 (0.0176)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0037)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 610/1724]  eta: 1:12:52  lr: 0.000160  loss: 2.1554 (2.4334)  loss_n_40: 0.4924 (0.5752)  loss_n_60: 0.4880 (0.5594)  loss_n_80: 0.5560 (0.5999)  loss_n_100: 0.6370 (0.6545)  triple_100: 0.0000 (0.0161)  triple_80: 0.0000 (0.0174)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0036)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 620/1724]  eta: 1:12:13  lr: 0.000160  loss: 2.0731 (2.4288)  loss_n_40: 0.4712 (0.5742)  loss_n_60: 0.4723 (0.5583)  loss_n_80: 0.5125 (0.5987)  loss_n_100: 0.6155 (0.6539)  triple_100: 0.0000 (0.0159)  triple_80: 0.0000 (0.0171)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0035)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 630/1724]  eta: 1:11:34  lr: 0.000160  loss: 1.8604 (2.4186)  loss_n_40: 0.4145 (0.5717)  loss_n_60: 0.4154 (0.5561)  loss_n_80: 0.4586 (0.5966)  loss_n_100: 0.5293 (0.6512)  triple_100: 0.0000 (0.0156)  triple_80: 0.0000 (0.0168)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0035)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 640/1724]  eta: 1:10:54  lr: 0.000160  loss: 1.7617 (2.4109)  loss_n_40: 0.4163 (0.5699)  loss_n_60: 0.4005 (0.5545)  loss_n_80: 0.4606 (0.5951)  loss_n_100: 0.4826 (0.6490)  triple_100: 0.0000 (0.0154)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0034)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 650/1724]  eta: 1:10:15  lr: 0.000160  loss: 1.9311 (2.4039)  loss_n_40: 0.4339 (0.5683)  loss_n_60: 0.4535 (0.5531)  loss_n_80: 0.4882 (0.5934)  loss_n_100: 0.5176 (0.6473)  triple_100: 0.0000 (0.0152)  triple_80: 0.0000 (0.0163)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0034)  time: 3.9264  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 660/1724]  eta: 1:09:36  lr: 0.000160  loss: 1.8770 (2.3958)  loss_n_40: 0.4172 (0.5663)  loss_n_60: 0.4404 (0.5515)  loss_n_80: 0.4882 (0.5916)  loss_n_100: 0.5176 (0.6453)  triple_100: 0.0000 (0.0149)  triple_80: 0.0000 (0.0160)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0033)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 670/1724]  eta: 1:08:57  lr: 0.000160  loss: 1.7655 (2.3871)  loss_n_40: 0.4069 (0.5640)  loss_n_60: 0.4184 (0.5498)  loss_n_80: 0.4527 (0.5896)  loss_n_100: 0.5075 (0.6432)  triple_100: 0.0000 (0.0147)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0033)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 680/1724]  eta: 1:08:17  lr: 0.000160  loss: 1.7130 (2.3789)  loss_n_40: 0.3935 (0.5623)  loss_n_60: 0.4182 (0.5479)  loss_n_80: 0.4365 (0.5874)  loss_n_100: 0.4693 (0.6404)  triple_100: 0.0000 (0.0145)  triple_80: 0.0000 (0.0160)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0032)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 690/1724]  eta: 1:07:38  lr: 0.000160  loss: 1.7011 (2.3712)  loss_n_40: 0.3897 (0.5608)  loss_n_60: 0.3942 (0.5463)  loss_n_80: 0.4241 (0.5854)  loss_n_100: 0.4508 (0.6383)  triple_100: 0.0000 (0.0143)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0032)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 700/1724]  eta: 1:06:59  lr: 0.000160  loss: 1.6492 (2.3603)  loss_n_40: 0.3807 (0.5582)  loss_n_60: 0.3795 (0.5440)  loss_n_80: 0.4178 (0.5829)  loss_n_100: 0.4508 (0.6355)  triple_100: 0.0000 (0.0141)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0031)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 710/1724]  eta: 1:06:20  lr: 0.000160  loss: 1.6492 (2.3516)  loss_n_40: 0.3807 (0.5562)  loss_n_60: 0.3795 (0.5421)  loss_n_80: 0.4178 (0.5809)  loss_n_100: 0.4335 (0.6332)  triple_100: 0.0000 (0.0139)  triple_80: 0.0000 (0.0154)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0031)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 720/1724]  eta: 1:05:40  lr: 0.000160  loss: 1.7103 (2.3423)  loss_n_40: 0.3757 (0.5538)  loss_n_60: 0.4072 (0.5401)  loss_n_80: 0.4334 (0.5788)  loss_n_100: 0.4811 (0.6309)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0030)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 730/1724]  eta: 1:05:01  lr: 0.000160  loss: 1.6462 (2.3324)  loss_n_40: 0.3669 (0.5516)  loss_n_60: 0.3891 (0.5379)  loss_n_80: 0.4299 (0.5765)  loss_n_100: 0.4450 (0.6283)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0030)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 740/1724]  eta: 1:04:22  lr: 0.000160  loss: 1.6244 (2.3254)  loss_n_40: 0.3839 (0.5497)  loss_n_60: 0.3891 (0.5362)  loss_n_80: 0.3980 (0.5744)  loss_n_100: 0.4389 (0.6260)  triple_100: 0.0000 (0.0143)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0030)  time: 3.9260  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [ 750/1724]  eta: 1:03:43  lr: 0.000160  loss: 2.8012 (2.3407)  loss_n_40: 0.5260 (0.5519)  loss_n_60: 0.4841 (0.5399)  loss_n_80: 0.5725 (0.5792)  loss_n_100: 0.6457 (0.6309)  triple_100: 0.0000 (0.0141)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0029)  time: 3.9242  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [ 760/1724]  eta: 1:03:03  lr: 0.000160  loss: 3.0978 (2.3469)  loss_n_40: 0.6578 (0.5525)  loss_n_60: 0.7504 (0.5417)  loss_n_80: 0.8426 (0.5814)  loss_n_100: 0.8841 (0.6329)  triple_100: 0.0000 (0.0139)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0066)  triple_40: 0.0000 (0.0029)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 770/1724]  eta: 1:02:24  lr: 0.000160  loss: 2.5867 (2.3478)  loss_n_40: 0.5642 (0.5525)  loss_n_60: 0.6221 (0.5426)  loss_n_80: 0.6594 (0.5819)  loss_n_100: 0.6830 (0.6329)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0066)  triple_40: 0.0000 (0.0029)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 780/1724]  eta: 1:01:45  lr: 0.000160  loss: 2.2079 (2.3451)  loss_n_40: 0.4946 (0.5516)  loss_n_60: 0.5500 (0.5423)  loss_n_80: 0.5727 (0.5815)  loss_n_100: 0.6122 (0.6325)  triple_100: 0.0000 (0.0136)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0065)  triple_40: 0.0000 (0.0028)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 790/1724]  eta: 1:01:05  lr: 0.000160  loss: 2.0427 (2.3414)  loss_n_40: 0.4638 (0.5507)  loss_n_60: 0.4932 (0.5417)  loss_n_80: 0.5094 (0.5807)  loss_n_100: 0.5668 (0.6315)  triple_100: 0.0000 (0.0134)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0064)  triple_40: 0.0000 (0.0028)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 800/1724]  eta: 1:00:26  lr: 0.000160  loss: 1.9687 (2.3372)  loss_n_40: 0.4359 (0.5497)  loss_n_60: 0.4780 (0.5410)  loss_n_80: 0.4937 (0.5798)  loss_n_100: 0.5482 (0.6303)  triple_100: 0.0000 (0.0132)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0063)  triple_40: 0.0000 (0.0027)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 810/1724]  eta: 0:59:47  lr: 0.000160  loss: 1.7467 (2.3302)  loss_n_40: 0.3833 (0.5481)  loss_n_60: 0.4239 (0.5396)  loss_n_80: 0.4521 (0.5782)  loss_n_100: 0.5123 (0.6284)  triple_100: 0.0000 (0.0131)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0062)  triple_40: 0.0000 (0.0027)  time: 3.9231  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 820/1724]  eta: 0:59:08  lr: 0.000160  loss: 1.6219 (2.3209)  loss_n_40: 0.3642 (0.5457)  loss_n_60: 0.3842 (0.5376)  loss_n_80: 0.4060 (0.5760)  loss_n_100: 0.4538 (0.6260)  triple_100: 0.0000 (0.0129)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0062)  triple_40: 0.0000 (0.0027)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 830/1724]  eta: 0:58:28  lr: 0.000160  loss: 1.6219 (2.3139)  loss_n_40: 0.3774 (0.5441)  loss_n_60: 0.3842 (0.5360)  loss_n_80: 0.4151 (0.5745)  loss_n_100: 0.4541 (0.6242)  triple_100: 0.0000 (0.0128)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0061)  triple_40: 0.0000 (0.0026)  time: 3.9230  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:9]  [ 840/1724]  eta: 0:57:49  lr: 0.000160  loss: 1.6961 (2.3070)  loss_n_40: 0.3810 (0.5426)  loss_n_60: 0.4151 (0.5346)  loss_n_80: 0.4271 (0.5729)  loss_n_100: 0.4564 (0.6223)  triple_100: 0.0000 (0.0126)  triple_80: 0.0000 (0.0134)  triple_60: 0.0000 (0.0060)  triple_40: 0.0000 (0.0026)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 850/1724]  eta: 0:57:10  lr: 0.000160  loss: 1.6532 (2.2981)  loss_n_40: 0.3788 (0.5405)  loss_n_60: 0.4029 (0.5326)  loss_n_80: 0.4119 (0.5707)  loss_n_100: 0.4503 (0.6201)  triple_100: 0.0000 (0.0125)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0026)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 860/1724]  eta: 0:56:31  lr: 0.000160  loss: 1.5390 (2.2903)  loss_n_40: 0.3457 (0.5389)  loss_n_60: 0.3653 (0.5309)  loss_n_80: 0.3870 (0.5687)  loss_n_100: 0.4336 (0.6180)  triple_100: 0.0000 (0.0123)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0026)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 870/1724]  eta: 0:55:51  lr: 0.000160  loss: 1.5664 (2.2851)  loss_n_40: 0.3510 (0.5379)  loss_n_60: 0.3627 (0.5298)  loss_n_80: 0.4025 (0.5674)  loss_n_100: 0.4371 (0.6164)  triple_100: 0.0000 (0.0122)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0025)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 880/1724]  eta: 0:55:12  lr: 0.000160  loss: 1.5876 (2.2793)  loss_n_40: 0.3492 (0.5362)  loss_n_60: 0.3627 (0.5285)  loss_n_80: 0.4123 (0.5662)  loss_n_100: 0.4654 (0.6151)  triple_100: 0.0000 (0.0120)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0025)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 890/1724]  eta: 0:54:33  lr: 0.000160  loss: 1.5535 (2.2714)  loss_n_40: 0.3440 (0.5342)  loss_n_60: 0.3532 (0.5268)  loss_n_80: 0.4060 (0.5644)  loss_n_100: 0.4367 (0.6131)  triple_100: 0.0000 (0.0119)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0025)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 900/1724]  eta: 0:53:54  lr: 0.000160  loss: 2.0637 (2.3018)  loss_n_40: 0.4128 (0.5362)  loss_n_60: 0.4746 (0.5299)  loss_n_80: 0.4947 (0.5689)  loss_n_100: 0.5302 (0.6183)  triple_100: 0.0000 (0.0180)  triple_80: 0.0000 (0.0168)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0043)  time: 3.9258  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [ 910/1724]  eta: 0:53:14  lr: 0.000160  loss: 4.0598 (2.3191)  loss_n_40: 0.7936 (0.5388)  loss_n_60: 0.9075 (0.5341)  loss_n_80: 1.0644 (0.5740)  loss_n_100: 1.1313 (0.6243)  triple_100: 0.0000 (0.0178)  triple_80: 0.0000 (0.0167)  triple_60: 0.0000 (0.0092)  triple_40: 0.0000 (0.0043)  time: 3.9259  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [ 920/1724]  eta: 0:52:35  lr: 0.000160  loss: 3.6348 (2.3274)  loss_n_40: 0.7194 (0.5403)  loss_n_60: 0.7919 (0.5360)  loss_n_80: 0.9385 (0.5767)  loss_n_100: 1.0211 (0.6271)  triple_100: 0.0000 (0.0176)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0042)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 930/1724]  eta: 0:51:56  lr: 0.000160  loss: 2.8402 (2.3322)  loss_n_40: 0.6420 (0.5416)  loss_n_60: 0.6427 (0.5370)  loss_n_80: 0.7640 (0.5785)  loss_n_100: 0.7730 (0.6282)  triple_100: 0.0000 (0.0174)  triple_80: 0.0000 (0.0163)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0042)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 940/1724]  eta: 0:51:17  lr: 0.000160  loss: 2.5967 (2.3326)  loss_n_40: 0.5656 (0.5414)  loss_n_60: 0.5852 (0.5372)  loss_n_80: 0.6684 (0.5790)  loss_n_100: 0.6890 (0.6286)  triple_100: 0.0000 (0.0173)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0041)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 950/1724]  eta: 0:50:37  lr: 0.000160  loss: 2.2862 (2.3314)  loss_n_40: 0.5228 (0.5413)  loss_n_60: 0.5380 (0.5372)  loss_n_80: 0.6008 (0.5790)  loss_n_100: 0.6106 (0.6280)  triple_100: 0.0000 (0.0171)  triple_80: 0.0000 (0.0160)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0041)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 960/1724]  eta: 0:49:58  lr: 0.000160  loss: 2.2384 (2.3312)  loss_n_40: 0.5485 (0.5415)  loss_n_60: 0.5380 (0.5375)  loss_n_80: 0.5992 (0.5792)  loss_n_100: 0.5613 (0.6276)  triple_100: 0.0000 (0.0169)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0040)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 970/1724]  eta: 0:49:19  lr: 0.000160  loss: 2.1793 (2.3297)  loss_n_40: 0.4728 (0.5410)  loss_n_60: 0.4908 (0.5373)  loss_n_80: 0.5754 (0.5792)  loss_n_100: 0.5447 (0.6273)  triple_100: 0.0000 (0.0167)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0040)  time: 3.9236  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [ 980/1724]  eta: 0:48:40  lr: 0.000160  loss: 1.9508 (2.3259)  loss_n_40: 0.4448 (0.5399)  loss_n_60: 0.4785 (0.5367)  loss_n_80: 0.5083 (0.5785)  loss_n_100: 0.5306 (0.6263)  triple_100: 0.0000 (0.0166)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0040)  time: 3.9247  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [ 990/1724]  eta: 0:48:00  lr: 0.000160  loss: 1.8223 (2.3206)  loss_n_40: 0.4155 (0.5387)  loss_n_60: 0.4274 (0.5356)  loss_n_80: 0.4832 (0.5774)  loss_n_100: 0.4874 (0.6249)  triple_100: 0.0000 (0.0164)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0039)  time: 3.9245  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1000/1724]  eta: 0:47:21  lr: 0.000160  loss: 1.7455 (2.3161)  loss_n_40: 0.4125 (0.5379)  loss_n_60: 0.4242 (0.5347)  loss_n_80: 0.4583 (0.5764)  loss_n_100: 0.4781 (0.6235)  triple_100: 0.0000 (0.0162)  triple_80: 0.0000 (0.0152)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0039)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1010/1724]  eta: 0:46:42  lr: 0.000160  loss: 1.8303 (2.3116)  loss_n_40: 0.4210 (0.5369)  loss_n_60: 0.4298 (0.5337)  loss_n_80: 0.4766 (0.5755)  loss_n_100: 0.4987 (0.6224)  triple_100: 0.0000 (0.0161)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0038)  time: 3.9252  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1020/1724]  eta: 0:46:03  lr: 0.000160  loss: 1.7489 (2.3054)  loss_n_40: 0.3824 (0.5354)  loss_n_60: 0.4050 (0.5324)  loss_n_80: 0.4436 (0.5742)  loss_n_100: 0.4681 (0.6207)  triple_100: 0.0000 (0.0159)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0038)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1030/1724]  eta: 0:45:23  lr: 0.000160  loss: 1.6360 (2.3008)  loss_n_40: 0.3693 (0.5344)  loss_n_60: 0.3935 (0.5315)  loss_n_80: 0.4417 (0.5733)  loss_n_100: 0.4455 (0.6194)  triple_100: 0.0000 (0.0158)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0038)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1040/1724]  eta: 0:44:44  lr: 0.000160  loss: 1.7658 (2.2955)  loss_n_40: 0.4136 (0.5332)  loss_n_60: 0.4133 (0.5304)  loss_n_80: 0.4469 (0.5720)  loss_n_100: 0.4521 (0.6177)  triple_100: 0.0000 (0.0156)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0037)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1050/1724]  eta: 0:44:05  lr: 0.000160  loss: 1.7658 (2.2916)  loss_n_40: 0.4283 (0.5327)  loss_n_60: 0.4326 (0.5296)  loss_n_80: 0.4378 (0.5711)  loss_n_100: 0.4343 (0.6164)  triple_100: 0.0000 (0.0155)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0037)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1060/1724]  eta: 0:43:26  lr: 0.000160  loss: 1.7352 (2.2872)  loss_n_40: 0.4392 (0.5323)  loss_n_60: 0.4326 (0.5289)  loss_n_80: 0.4323 (0.5699)  loss_n_100: 0.4139 (0.6147)  triple_100: 0.0000 (0.0153)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0037)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1070/1724]  eta: 0:42:46  lr: 0.000160  loss: 1.5850 (2.2819)  loss_n_40: 0.3598 (0.5310)  loss_n_60: 0.3775 (0.5277)  loss_n_80: 0.4243 (0.5689)  loss_n_100: 0.4335 (0.6133)  triple_100: 0.0000 (0.0152)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0036)  time: 3.9247  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:9]  [1080/1724]  eta: 0:42:07  lr: 0.000160  loss: 1.6023 (2.2762)  loss_n_40: 0.3763 (0.5298)  loss_n_60: 0.3821 (0.5264)  loss_n_80: 0.4246 (0.5676)  loss_n_100: 0.4651 (0.6117)  triple_100: 0.0000 (0.0150)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0036)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1090/1724]  eta: 0:41:28  lr: 0.000160  loss: 1.7056 (2.2718)  loss_n_40: 0.3916 (0.5292)  loss_n_60: 0.3906 (0.5255)  loss_n_80: 0.4354 (0.5667)  loss_n_100: 0.4468 (0.6102)  triple_100: 0.0000 (0.0149)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0036)  time: 3.9260  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1100/1724]  eta: 0:40:49  lr: 0.000160  loss: 1.6734 (2.2658)  loss_n_40: 0.3903 (0.5281)  loss_n_60: 0.3906 (0.5242)  loss_n_80: 0.4303 (0.5653)  loss_n_100: 0.4242 (0.6084)  triple_100: 0.0000 (0.0147)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0035)  time: 3.9258  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1110/1724]  eta: 0:40:09  lr: 0.000160  loss: 1.5960 (2.2596)  loss_n_40: 0.3780 (0.5267)  loss_n_60: 0.3782 (0.5228)  loss_n_80: 0.3986 (0.5638)  loss_n_100: 0.4105 (0.6067)  triple_100: 0.0000 (0.0146)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0035)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1120/1724]  eta: 0:39:30  lr: 0.000160  loss: 1.4308 (2.2523)  loss_n_40: 0.3620 (0.5252)  loss_n_60: 0.3341 (0.5211)  loss_n_80: 0.3664 (0.5620)  loss_n_100: 0.3950 (0.6048)  triple_100: 0.0000 (0.0145)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0035)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1130/1724]  eta: 0:38:51  lr: 0.000160  loss: 1.5149 (2.2475)  loss_n_40: 0.3614 (0.5242)  loss_n_60: 0.3643 (0.5201)  loss_n_80: 0.3764 (0.5609)  loss_n_100: 0.4121 (0.6035)  triple_100: 0.0000 (0.0144)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0034)  time: 3.9257  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1140/1724]  eta: 0:38:12  lr: 0.000160  loss: 1.6906 (2.2434)  loss_n_40: 0.3908 (0.5232)  loss_n_60: 0.3948 (0.5192)  loss_n_80: 0.4236 (0.5600)  loss_n_100: 0.4572 (0.6025)  triple_100: 0.0000 (0.0142)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0034)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1150/1724]  eta: 0:37:32  lr: 0.000160  loss: 1.7389 (2.2391)  loss_n_40: 0.4072 (0.5223)  loss_n_60: 0.4116 (0.5183)  loss_n_80: 0.4506 (0.5590)  loss_n_100: 0.4547 (0.6014)  triple_100: 0.0000 (0.0141)  triple_80: 0.0000 (0.0134)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0034)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1160/1724]  eta: 0:36:53  lr: 0.000160  loss: 1.6354 (2.2346)  loss_n_40: 0.4108 (0.5217)  loss_n_60: 0.3858 (0.5173)  loss_n_80: 0.4127 (0.5577)  loss_n_100: 0.4390 (0.6001)  triple_100: 0.0000 (0.0140)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0033)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1170/1724]  eta: 0:36:14  lr: 0.000160  loss: 1.6297 (2.2302)  loss_n_40: 0.4053 (0.5209)  loss_n_60: 0.3790 (0.5162)  loss_n_80: 0.3916 (0.5565)  loss_n_100: 0.4281 (0.5989)  triple_100: 0.0000 (0.0139)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0033)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1180/1724]  eta: 0:35:35  lr: 0.000160  loss: 1.6531 (2.2265)  loss_n_40: 0.3934 (0.5203)  loss_n_60: 0.3790 (0.5153)  loss_n_80: 0.3980 (0.5556)  loss_n_100: 0.4386 (0.5979)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0033)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1190/1724]  eta: 0:34:55  lr: 0.000160  loss: 1.6983 (2.2235)  loss_n_40: 0.4224 (0.5199)  loss_n_60: 0.3999 (0.5144)  loss_n_80: 0.4308 (0.5547)  loss_n_100: 0.4577 (0.5968)  triple_100: 0.0000 (0.0136)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0033)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1200/1724]  eta: 0:34:16  lr: 0.000160  loss: 1.6366 (2.2183)  loss_n_40: 0.3999 (0.5190)  loss_n_60: 0.3785 (0.5132)  loss_n_80: 0.3938 (0.5533)  loss_n_100: 0.4242 (0.5952)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0134)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0032)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1210/1724]  eta: 0:33:37  lr: 0.000160  loss: 1.5819 (2.2138)  loss_n_40: 0.3956 (0.5183)  loss_n_60: 0.3744 (0.5122)  loss_n_80: 0.3914 (0.5522)  loss_n_100: 0.4185 (0.5940)  triple_100: 0.0000 (0.0134)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0032)  time: 3.9249  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1220/1724]  eta: 0:32:58  lr: 0.000160  loss: 1.5994 (2.2101)  loss_n_40: 0.3912 (0.5176)  loss_n_60: 0.3838 (0.5114)  loss_n_80: 0.3977 (0.5511)  loss_n_100: 0.4349 (0.5927)  triple_100: 0.0000 (0.0133)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0036)  time: 3.9249  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1230/1724]  eta: 0:32:18  lr: 0.000160  loss: 1.9470 (2.2086)  loss_n_40: 0.4840 (0.5178)  loss_n_60: 0.4469 (0.5110)  loss_n_80: 0.4809 (0.5507)  loss_n_100: 0.4787 (0.5922)  triple_100: 0.0000 (0.0132)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0035)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1240/1724]  eta: 0:31:39  lr: 0.000160  loss: 1.9333 (2.2065)  loss_n_40: 0.4687 (0.5176)  loss_n_60: 0.4548 (0.5106)  loss_n_80: 0.4844 (0.5501)  loss_n_100: 0.5080 (0.5915)  triple_100: 0.0000 (0.0131)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0035)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1250/1724]  eta: 0:31:00  lr: 0.000160  loss: 1.8251 (2.2030)  loss_n_40: 0.4511 (0.5170)  loss_n_60: 0.4393 (0.5099)  loss_n_80: 0.4541 (0.5493)  loss_n_100: 0.4747 (0.5904)  triple_100: 0.0000 (0.0130)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0035)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1260/1724]  eta: 0:30:21  lr: 0.000160  loss: 1.7417 (2.2000)  loss_n_40: 0.4261 (0.5164)  loss_n_60: 0.4126 (0.5093)  loss_n_80: 0.4419 (0.5483)  loss_n_100: 0.4747 (0.5894)  triple_100: 0.0000 (0.0129)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0036)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1270/1724]  eta: 0:29:41  lr: 0.000160  loss: 1.9102 (2.1996)  loss_n_40: 0.4697 (0.5163)  loss_n_60: 0.4256 (0.5091)  loss_n_80: 0.4701 (0.5481)  loss_n_100: 0.5050 (0.5891)  triple_100: 0.0000 (0.0128)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0036)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1280/1724]  eta: 0:29:02  lr: 0.000160  loss: 2.1266 (2.1994)  loss_n_40: 0.5088 (0.5166)  loss_n_60: 0.4983 (0.5092)  loss_n_80: 0.5088 (0.5480)  loss_n_100: 0.5596 (0.5890)  triple_100: 0.0000 (0.0127)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0036)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1290/1724]  eta: 0:28:23  lr: 0.000160  loss: 2.1497 (2.1991)  loss_n_40: 0.4950 (0.5166)  loss_n_60: 0.5093 (0.5094)  loss_n_80: 0.5296 (0.5479)  loss_n_100: 0.5763 (0.5889)  triple_100: 0.0000 (0.0126)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0036)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1300/1724]  eta: 0:27:44  lr: 0.000160  loss: 1.8821 (2.1955)  loss_n_40: 0.4406 (0.5159)  loss_n_60: 0.4606 (0.5087)  loss_n_80: 0.4533 (0.5469)  loss_n_100: 0.4932 (0.5879)  triple_100: 0.0000 (0.0125)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0035)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1310/1724]  eta: 0:27:04  lr: 0.000160  loss: 1.7887 (2.1939)  loss_n_40: 0.4280 (0.5154)  loss_n_60: 0.4274 (0.5084)  loss_n_80: 0.4420 (0.5465)  loss_n_100: 0.4850 (0.5877)  triple_100: 0.0000 (0.0125)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0035)  time: 3.9267  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:9]  [1320/1724]  eta: 0:26:25  lr: 0.000160  loss: 1.8007 (2.1907)  loss_n_40: 0.4326 (0.5147)  loss_n_60: 0.4299 (0.5077)  loss_n_80: 0.4506 (0.5457)  loss_n_100: 0.5015 (0.5870)  triple_100: 0.0000 (0.0124)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0035)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1330/1724]  eta: 0:25:46  lr: 0.000160  loss: 1.7014 (2.1879)  loss_n_40: 0.4047 (0.5140)  loss_n_60: 0.3852 (0.5069)  loss_n_80: 0.4253 (0.5448)  loss_n_100: 0.4621 (0.5861)  triple_100: 0.0000 (0.0123)  triple_80: 0.0000 (0.0127)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0038)  time: 3.9288  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1340/1724]  eta: 0:25:07  lr: 0.000160  loss: 1.6754 (2.1834)  loss_n_40: 0.3751 (0.5129)  loss_n_60: 0.3637 (0.5059)  loss_n_80: 0.4179 (0.5437)  loss_n_100: 0.4381 (0.5851)  triple_100: 0.0000 (0.0123)  triple_80: 0.0000 (0.0126)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0038)  time: 3.9289  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1350/1724]  eta: 0:24:27  lr: 0.000160  loss: 1.6876 (2.1803)  loss_n_40: 0.3944 (0.5123)  loss_n_60: 0.3828 (0.5053)  loss_n_80: 0.4179 (0.5428)  loss_n_100: 0.4850 (0.5844)  triple_100: 0.0000 (0.0122)  triple_80: 0.0000 (0.0125)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0038)  time: 3.9286  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1360/1724]  eta: 0:23:48  lr: 0.000160  loss: 1.5445 (2.1755)  loss_n_40: 0.3856 (0.5112)  loss_n_60: 0.3545 (0.5042)  loss_n_80: 0.3900 (0.5416)  loss_n_100: 0.4325 (0.5832)  triple_100: 0.0000 (0.0121)  triple_80: 0.0000 (0.0125)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0037)  time: 3.9300  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1370/1724]  eta: 0:23:09  lr: 0.000160  loss: 1.4659 (2.1705)  loss_n_40: 0.3398 (0.5101)  loss_n_60: 0.3405 (0.5031)  loss_n_80: 0.3708 (0.5404)  loss_n_100: 0.4066 (0.5819)  triple_100: 0.0000 (0.0120)  triple_80: 0.0000 (0.0124)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0037)  time: 3.9295  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1380/1724]  eta: 0:22:30  lr: 0.000160  loss: 1.4327 (2.1660)  loss_n_40: 0.3367 (0.5090)  loss_n_60: 0.3393 (0.5021)  loss_n_80: 0.3553 (0.5392)  loss_n_100: 0.3913 (0.5806)  triple_100: 0.0000 (0.0122)  triple_80: 0.0000 (0.0123)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0037)  time: 3.9269  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1390/1724]  eta: 0:21:51  lr: 0.000160  loss: 1.7205 (2.1656)  loss_n_40: 0.3979 (0.5085)  loss_n_60: 0.4054 (0.5017)  loss_n_80: 0.4265 (0.5389)  loss_n_100: 0.4836 (0.5803)  triple_100: 0.0000 (0.0122)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0037)  time: 3.9260  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1400/1724]  eta: 0:21:11  lr: 0.000160  loss: 2.2281 (2.1705)  loss_n_40: 0.4888 (0.5099)  loss_n_60: 0.5074 (0.5026)  loss_n_80: 0.5448 (0.5399)  loss_n_100: 0.6419 (0.5815)  triple_100: 0.0000 (0.0128)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0036)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1410/1724]  eta: 0:20:32  lr: 0.000160  loss: 2.9556 (2.1768)  loss_n_40: 0.6646 (0.5114)  loss_n_60: 0.6735 (0.5040)  loss_n_80: 0.7234 (0.5412)  loss_n_100: 0.7762 (0.5831)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0036)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1420/1724]  eta: 0:19:53  lr: 0.000160  loss: 2.9223 (2.1808)  loss_n_40: 0.6620 (0.5123)  loss_n_60: 0.6735 (0.5050)  loss_n_80: 0.7268 (0.5422)  loss_n_100: 0.7957 (0.5843)  triple_100: 0.0000 (0.0134)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0036)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1430/1724]  eta: 0:19:14  lr: 0.000160  loss: 2.8386 (2.1947)  loss_n_40: 0.6587 (0.5143)  loss_n_60: 0.6359 (0.5065)  loss_n_80: 0.7370 (0.5439)  loss_n_100: 0.7983 (0.5863)  triple_100: 0.0000 (0.0172)  triple_80: 0.0000 (0.0152)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0036)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1440/1724]  eta: 0:18:34  lr: 0.000160  loss: 3.4499 (2.2052)  loss_n_40: 0.7531 (0.5166)  loss_n_60: 0.7569 (0.5085)  loss_n_80: 0.8691 (0.5467)  loss_n_100: 0.9778 (0.5898)  triple_100: 0.0000 (0.0171)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0035)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1450/1724]  eta: 0:17:55  lr: 0.000160  loss: 4.3429 (2.2577)  loss_n_40: 0.9875 (0.5232)  loss_n_60: 0.8862 (0.5158)  loss_n_80: 1.0919 (0.5557)  loss_n_100: 1.2660 (0.5986)  triple_100: 0.0000 (0.0269)  triple_80: 0.0000 (0.0240)  triple_60: 0.0000 (0.0099)  triple_40: 0.0000 (0.0035)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1460/1724]  eta: 0:17:16  lr: 0.000160  loss: 9.6565 (2.3144)  loss_n_40: 2.1083 (0.5365)  loss_n_60: 2.2982 (0.5308)  loss_n_80: 2.5006 (0.5709)  loss_n_100: 2.3620 (0.6120)  triple_100: 0.0000 (0.0267)  triple_80: 0.0000 (0.0242)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0035)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1470/1724]  eta: 0:16:37  lr: 0.000160  loss: 11.1040 (2.3895)  loss_n_40: 2.5014 (0.5501)  loss_n_60: 2.7755 (0.5469)  loss_n_80: 2.8619 (0.5870)  loss_n_100: 2.5421 (0.6268)  triple_100: 0.0000 (0.0286)  triple_80: 0.0000 (0.0250)  triple_60: 0.0000 (0.0187)  triple_40: 0.0000 (0.0063)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1480/1724]  eta: 0:15:57  lr: 0.000160  loss: 13.3615 (2.4750)  loss_n_40: 2.7758 (0.5684)  loss_n_60: 3.1684 (0.5674)  loss_n_80: 3.0262 (0.6060)  loss_n_100: 3.0180 (0.6451)  triple_100: 0.0000 (0.0284)  triple_80: 0.0000 (0.0250)  triple_60: 0.0000 (0.0201)  triple_40: 0.0000 (0.0147)  time: 3.9273  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1490/1724]  eta: 0:15:18  lr: 0.000160  loss: 13.8645 (2.5487)  loss_n_40: 3.1270 (0.5847)  loss_n_60: 3.5172 (0.5875)  loss_n_80: 3.4001 (0.6251)  loss_n_100: 3.2889 (0.6631)  triple_100: 0.0000 (0.0282)  triple_80: 0.0000 (0.0248)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0146)  time: 3.9294  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1500/1724]  eta: 0:14:39  lr: 0.000160  loss: 12.2519 (2.6106)  loss_n_40: 2.8056 (0.5989)  loss_n_60: 3.3056 (0.6048)  loss_n_80: 3.1865 (0.6408)  loss_n_100: 3.0887 (0.6782)  triple_100: 0.0000 (0.0280)  triple_80: 0.0000 (0.0246)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0145)  time: 3.9276  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1510/1724]  eta: 0:14:00  lr: 0.000160  loss: 11.3037 (2.6688)  loss_n_40: 2.5642 (0.6111)  loss_n_60: 3.0925 (0.6204)  loss_n_80: 2.9253 (0.6561)  loss_n_100: 2.8336 (0.6930)  triple_100: 0.0000 (0.0278)  triple_80: 0.0000 (0.0245)  triple_60: 0.0000 (0.0205)  triple_40: 0.0000 (0.0153)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1520/1724]  eta: 0:13:20  lr: 0.000160  loss: 10.8594 (2.7217)  loss_n_40: 2.1794 (0.6211)  loss_n_60: 2.8777 (0.6354)  loss_n_80: 2.9161 (0.6707)  loss_n_100: 2.7686 (0.7064)  triple_100: 0.0000 (0.0277)  triple_80: 0.0000 (0.0243)  triple_60: 0.0000 (0.0204)  triple_40: 0.0000 (0.0157)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1530/1724]  eta: 0:12:41  lr: 0.000160  loss: 10.5735 (2.7713)  loss_n_40: 2.0441 (0.6296)  loss_n_60: 2.8612 (0.6497)  loss_n_80: 2.8873 (0.6850)  loss_n_100: 2.7498 (0.7195)  triple_100: 0.0000 (0.0275)  triple_80: 0.0000 (0.0242)  triple_60: 0.0000 (0.0203)  triple_40: 0.0000 (0.0156)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1540/1724]  eta: 0:12:02  lr: 0.000160  loss: 9.7350 (2.8125)  loss_n_40: 1.8326 (0.6368)  loss_n_60: 2.5699 (0.6611)  loss_n_80: 2.7640 (0.6971)  loss_n_100: 2.5598 (0.7305)  triple_100: 0.0000 (0.0273)  triple_80: 0.0000 (0.0240)  triple_60: 0.0000 (0.0201)  triple_40: 0.0000 (0.0155)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1550/1724]  eta: 0:11:22  lr: 0.000160  loss: 8.6069 (2.8484)  loss_n_40: 1.6531 (0.6430)  loss_n_60: 2.2492 (0.6709)  loss_n_80: 2.4204 (0.7075)  loss_n_100: 2.2970 (0.7405)  triple_100: 0.0000 (0.0271)  triple_80: 0.0000 (0.0239)  triple_60: 0.0000 (0.0200)  triple_40: 0.0000 (0.0154)  time: 3.9246  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:9]  [1560/1724]  eta: 0:10:43  lr: 0.000160  loss: 8.2784 (2.8826)  loss_n_40: 1.5564 (0.6491)  loss_n_60: 2.1275 (0.6798)  loss_n_80: 2.3162 (0.7174)  loss_n_100: 2.2940 (0.7504)  triple_100: 0.0000 (0.0270)  triple_80: 0.0000 (0.0237)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0153)  time: 3.9248  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1570/1724]  eta: 0:10:04  lr: 0.000160  loss: 8.3118 (2.9177)  loss_n_40: 1.5304 (0.6546)  loss_n_60: 2.0484 (0.6885)  loss_n_80: 2.3192 (0.7278)  loss_n_100: 2.3594 (0.7615)  triple_100: 0.0000 (0.0268)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0198)  triple_40: 0.0000 (0.0152)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1580/1724]  eta: 0:09:25  lr: 0.000160  loss: 8.0971 (2.9469)  loss_n_40: 1.4742 (0.6596)  loss_n_60: 1.9055 (0.6953)  loss_n_80: 2.2153 (0.7359)  loss_n_100: 2.2865 (0.7707)  triple_100: 0.0000 (0.0266)  triple_80: 0.0000 (0.0234)  triple_60: 0.0000 (0.0203)  triple_40: 0.0000 (0.0151)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1590/1724]  eta: 0:08:45  lr: 0.000160  loss: 7.5852 (2.9775)  loss_n_40: 1.4584 (0.6647)  loss_n_60: 1.8192 (0.7025)  loss_n_80: 2.0257 (0.7448)  loss_n_100: 2.2614 (0.7806)  triple_100: 0.0000 (0.0264)  triple_80: 0.0000 (0.0233)  triple_60: 0.0000 (0.0202)  triple_40: 0.0000 (0.0150)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1600/1724]  eta: 0:08:06  lr: 0.000160  loss: 7.3691 (3.0045)  loss_n_40: 1.4344 (0.6694)  loss_n_60: 1.7676 (0.7091)  loss_n_80: 1.9873 (0.7526)  loss_n_100: 2.1706 (0.7891)  triple_100: 0.0000 (0.0263)  triple_80: 0.0000 (0.0231)  triple_60: 0.0000 (0.0200)  triple_40: 0.0000 (0.0149)  time: 3.9239  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1610/1724]  eta: 0:07:27  lr: 0.000160  loss: 7.0345 (3.0289)  loss_n_40: 1.3774 (0.6737)  loss_n_60: 1.6496 (0.7149)  loss_n_80: 1.9281 (0.7597)  loss_n_100: 2.0427 (0.7967)  triple_100: 0.0000 (0.0261)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0148)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1620/1724]  eta: 0:06:48  lr: 0.000160  loss: 6.8178 (3.0503)  loss_n_40: 1.3327 (0.6777)  loss_n_60: 1.5850 (0.7199)  loss_n_80: 1.8528 (0.7657)  loss_n_100: 1.9943 (0.8035)  triple_100: 0.0000 (0.0260)  triple_80: 0.0000 (0.0229)  triple_60: 0.0000 (0.0198)  triple_40: 0.0000 (0.0148)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1630/1724]  eta: 0:06:08  lr: 0.000160  loss: 6.2281 (3.0695)  loss_n_40: 1.2947 (0.6816)  loss_n_60: 1.5073 (0.7246)  loss_n_80: 1.6659 (0.7710)  loss_n_100: 1.7625 (0.8091)  triple_100: 0.0000 (0.0258)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0197)  triple_40: 0.0000 (0.0147)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1640/1724]  eta: 0:05:29  lr: 0.000160  loss: 6.0008 (3.0866)  loss_n_40: 1.2774 (0.6850)  loss_n_60: 1.4574 (0.7287)  loss_n_80: 1.6149 (0.7757)  loss_n_100: 1.7548 (0.8146)  triple_100: 0.0000 (0.0256)  triple_80: 0.0000 (0.0229)  triple_60: 0.0000 (0.0196)  triple_40: 0.0000 (0.0146)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1650/1724]  eta: 0:04:50  lr: 0.000160  loss: 5.6149 (3.1010)  loss_n_40: 1.2004 (0.6881)  loss_n_60: 1.3284 (0.7323)  loss_n_80: 1.4589 (0.7796)  loss_n_100: 1.5825 (0.8189)  triple_100: 0.0000 (0.0255)  triple_80: 0.0000 (0.0227)  triple_60: 0.0000 (0.0194)  triple_40: 0.0000 (0.0145)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1660/1724]  eta: 0:04:11  lr: 0.000160  loss: 5.1248 (3.1121)  loss_n_40: 1.1488 (0.6903)  loss_n_60: 1.2483 (0.7351)  loss_n_80: 1.2709 (0.7827)  loss_n_100: 1.4079 (0.8223)  triple_100: 0.0000 (0.0253)  triple_80: 0.0000 (0.0226)  triple_60: 0.0000 (0.0193)  triple_40: 0.0000 (0.0144)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1670/1724]  eta: 0:03:31  lr: 0.000160  loss: 4.7466 (3.1230)  loss_n_40: 1.0304 (0.6927)  loss_n_60: 1.1382 (0.7378)  loss_n_80: 1.2517 (0.7858)  loss_n_100: 1.3362 (0.8256)  triple_100: 0.0000 (0.0252)  triple_80: 0.0000 (0.0224)  triple_60: 0.0000 (0.0192)  triple_40: 0.0000 (0.0143)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1680/1724]  eta: 0:02:52  lr: 0.000160  loss: 4.6758 (3.1331)  loss_n_40: 1.0197 (0.6947)  loss_n_60: 1.1382 (0.7403)  loss_n_80: 1.2119 (0.7886)  loss_n_100: 1.3362 (0.8288)  triple_100: 0.0000 (0.0250)  triple_80: 0.0000 (0.0223)  triple_60: 0.0000 (0.0191)  triple_40: 0.0000 (0.0142)  time: 3.9262  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1690/1724]  eta: 0:02:13  lr: 0.000160  loss: 4.6081 (3.1426)  loss_n_40: 0.9737 (0.6966)  loss_n_60: 1.1051 (0.7425)  loss_n_80: 1.2116 (0.7912)  loss_n_100: 1.3200 (0.8318)  triple_100: 0.0000 (0.0249)  triple_80: 0.0000 (0.0222)  triple_60: 0.0000 (0.0192)  triple_40: 0.0000 (0.0142)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1700/1724]  eta: 0:01:34  lr: 0.000160  loss: 5.3859 (3.1591)  loss_n_40: 1.2379 (0.7007)  loss_n_60: 1.2660 (0.7462)  loss_n_80: 1.4038 (0.7955)  loss_n_100: 1.5395 (0.8367)  triple_100: 0.0000 (0.0247)  triple_80: 0.0000 (0.0220)  triple_60: 0.0000 (0.0191)  triple_40: 0.0000 (0.0141)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:9]  [1710/1724]  eta: 0:00:54  lr: 0.000160  loss: 5.7067 (3.1740)  loss_n_40: 1.2976 (0.7043)  loss_n_60: 1.3033 (0.7492)  loss_n_80: 1.4038 (0.7988)  loss_n_100: 1.5982 (0.8408)  triple_100: 0.0000 (0.0254)  triple_80: 0.0000 (0.0219)  triple_60: 0.0000 (0.0190)  triple_40: 0.0000 (0.0146)  time: 3.9250  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1720/1724]  eta: 0:00:15  lr: 0.000160  loss: 5.6060 (3.1882)  loss_n_40: 1.2808 (0.7081)  loss_n_60: 1.2415 (0.7524)  loss_n_80: 1.3592 (0.8022)  loss_n_100: 1.4840 (0.8451)  triple_100: 0.0000 (0.0253)  triple_80: 0.0000 (0.0218)  triple_60: 0.0000 (0.0189)  triple_40: 0.0000 (0.0145)  time: 3.9247  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9]  [1723/1724]  eta: 0:00:03  lr: 0.000160  loss: 5.5584 (3.1924)  loss_n_40: 1.2840 (0.7092)  loss_n_60: 1.2415 (0.7533)  loss_n_80: 1.3614 (0.8033)  loss_n_100: 1.5273 (0.8464)  triple_100: 0.0000 (0.0252)  triple_80: 0.0000 (0.0218)  triple_60: 0.0000 (0.0188)  triple_40: 0.0000 (0.0145)  time: 3.9245  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:9] Total time: 1:52:47 (3.9254 s / it)\n",
      "Averaged stats: lr: 0.000160  loss: 5.5584 (3.1924)  loss_n_40: 1.2840 (0.7092)  loss_n_60: 1.2415 (0.7533)  loss_n_80: 1.3614 (0.8033)  loss_n_100: 1.5273 (0.8464)  triple_100: 0.0000 (0.0252)  triple_80: 0.0000 (0.0218)  triple_60: 0.0000 (0.0188)  triple_40: 0.0000 (0.0145)\n",
      "Valid: [epoch:9]  [  0/845]  eta: 0:10:43  loss: 5.1483 (5.1483)  loss_n_40: 1.2601 (1.2601)  loss_n_60: 1.0683 (1.0683)  loss_n_80: 1.2122 (1.2122)  loss_n_100: 1.6077 (1.6077)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7617  data: 0.4253  max mem: 46473\n",
      "Valid: [epoch:9]  [ 10/845]  eta: 0:05:11  loss: 4.6536 (4.7342)  loss_n_40: 1.2329 (1.2166)  loss_n_60: 1.0683 (1.0663)  loss_n_80: 1.1951 (1.1366)  loss_n_100: 1.3661 (1.3147)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3736  data: 0.0388  max mem: 46473\n",
      "Valid: [epoch:9]  [ 20/845]  eta: 0:04:52  loss: 4.7359 (4.9639)  loss_n_40: 1.1974 (1.2679)  loss_n_60: 1.0691 (1.1491)  loss_n_80: 1.1951 (1.1932)  loss_n_100: 1.3661 (1.3536)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:9]  [ 30/845]  eta: 0:04:43  loss: 5.1882 (5.0291)  loss_n_40: 1.1974 (1.2566)  loss_n_60: 1.2255 (1.1696)  loss_n_80: 1.2401 (1.2173)  loss_n_100: 1.4310 (1.3856)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:9]  [ 40/845]  eta: 0:04:37  loss: 5.3083 (5.0949)  loss_n_40: 1.1625 (1.2497)  loss_n_60: 1.2255 (1.1945)  loss_n_80: 1.2832 (1.2382)  loss_n_100: 1.4379 (1.4125)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:9]  [ 50/845]  eta: 0:04:32  loss: 5.1377 (5.1072)  loss_n_40: 1.2233 (1.2435)  loss_n_60: 1.2676 (1.2000)  loss_n_80: 1.3017 (1.2414)  loss_n_100: 1.4133 (1.4223)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [ 60/845]  eta: 0:04:28  loss: 4.5871 (4.9995)  loss_n_40: 1.1742 (1.2218)  loss_n_60: 0.9859 (1.1619)  loss_n_80: 1.0594 (1.2087)  loss_n_100: 1.3927 (1.4071)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [ 70/845]  eta: 0:04:24  loss: 4.5180 (5.0539)  loss_n_40: 1.1742 (1.2275)  loss_n_60: 0.9859 (1.1725)  loss_n_80: 1.0875 (1.2301)  loss_n_100: 1.3412 (1.4238)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [ 80/845]  eta: 0:04:20  loss: 5.1887 (5.0245)  loss_n_40: 1.2359 (1.2259)  loss_n_60: 1.1499 (1.1581)  loss_n_80: 1.2605 (1.2231)  loss_n_100: 1.3788 (1.4174)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [ 90/845]  eta: 0:04:16  loss: 5.1887 (5.0640)  loss_n_40: 1.1892 (1.2278)  loss_n_60: 1.1236 (1.1661)  loss_n_80: 1.2605 (1.2347)  loss_n_100: 1.4736 (1.4354)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [100/845]  eta: 0:04:12  loss: 5.2825 (5.1011)  loss_n_40: 1.1892 (1.2234)  loss_n_60: 1.1506 (1.1783)  loss_n_80: 1.3564 (1.2443)  loss_n_100: 1.5902 (1.4552)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3352  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [110/845]  eta: 0:04:08  loss: 5.0215 (5.0906)  loss_n_40: 1.1903 (1.2216)  loss_n_60: 1.1241 (1.1764)  loss_n_80: 1.2147 (1.2407)  loss_n_100: 1.5796 (1.4518)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [120/845]  eta: 0:04:05  loss: 5.0407 (5.0986)  loss_n_40: 1.2410 (1.2257)  loss_n_60: 1.1101 (1.1764)  loss_n_80: 1.2504 (1.2450)  loss_n_100: 1.4069 (1.4514)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [130/845]  eta: 0:04:01  loss: 5.1492 (5.1093)  loss_n_40: 1.2635 (1.2277)  loss_n_60: 1.1101 (1.1730)  loss_n_80: 1.3162 (1.2439)  loss_n_100: 1.4096 (1.4480)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0166)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [140/845]  eta: 0:03:58  loss: 5.1620 (5.1258)  loss_n_40: 1.2455 (1.2347)  loss_n_60: 1.1501 (1.1747)  loss_n_80: 1.2550 (1.2469)  loss_n_100: 1.3997 (1.4542)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0154)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [150/845]  eta: 0:03:54  loss: 5.2289 (5.1421)  loss_n_40: 1.2614 (1.2411)  loss_n_60: 1.1814 (1.1757)  loss_n_80: 1.3095 (1.2531)  loss_n_100: 1.4499 (1.4579)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0144)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [160/845]  eta: 0:03:51  loss: 5.2052 (5.1374)  loss_n_40: 1.2376 (1.2432)  loss_n_60: 1.0992 (1.1720)  loss_n_80: 1.2504 (1.2503)  loss_n_100: 1.4543 (1.4583)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0135)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [170/845]  eta: 0:03:47  loss: 5.0634 (5.1240)  loss_n_40: 1.1957 (1.2351)  loss_n_60: 1.0700 (1.1710)  loss_n_80: 1.1718 (1.2468)  loss_n_100: 1.4543 (1.4584)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0127)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [180/845]  eta: 0:03:44  loss: 5.0634 (5.1330)  loss_n_40: 1.2054 (1.2385)  loss_n_60: 1.1827 (1.1757)  loss_n_80: 1.2351 (1.2477)  loss_n_100: 1.4027 (1.4590)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0120)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [190/845]  eta: 0:03:40  loss: 5.0969 (5.1349)  loss_n_40: 1.3067 (1.2390)  loss_n_60: 1.1704 (1.1778)  loss_n_80: 1.2465 (1.2482)  loss_n_100: 1.4155 (1.4585)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0114)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [200/845]  eta: 0:03:37  loss: 4.9126 (5.1319)  loss_n_40: 1.1681 (1.2361)  loss_n_60: 1.1040 (1.1754)  loss_n_80: 1.2465 (1.2472)  loss_n_100: 1.4392 (1.4624)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0108)  time: 0.3348  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:9]  [210/845]  eta: 0:03:33  loss: 5.2078 (5.1420)  loss_n_40: 1.1995 (1.2380)  loss_n_60: 1.1742 (1.1804)  loss_n_80: 1.2799 (1.2510)  loss_n_100: 1.4608 (1.4623)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0103)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [220/845]  eta: 0:03:30  loss: 5.3358 (5.1579)  loss_n_40: 1.2659 (1.2473)  loss_n_60: 1.2161 (1.1865)  loss_n_80: 1.3349 (1.2539)  loss_n_100: 1.3676 (1.4604)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0098)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [230/845]  eta: 0:03:27  loss: 5.0910 (5.1479)  loss_n_40: 1.1996 (1.2454)  loss_n_60: 1.1754 (1.1823)  loss_n_80: 1.2323 (1.2513)  loss_n_100: 1.3786 (1.4595)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0094)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [240/845]  eta: 0:03:23  loss: 4.8659 (5.1373)  loss_n_40: 1.1615 (1.2456)  loss_n_60: 1.0244 (1.1788)  loss_n_80: 1.1959 (1.2491)  loss_n_100: 1.3824 (1.4547)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0090)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [250/845]  eta: 0:03:20  loss: 4.7900 (5.1274)  loss_n_40: 1.0919 (1.2416)  loss_n_60: 1.0194 (1.1758)  loss_n_80: 1.1959 (1.2485)  loss_n_100: 1.4379 (1.4528)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0086)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [260/845]  eta: 0:03:16  loss: 5.0865 (5.1363)  loss_n_40: 1.2081 (1.2460)  loss_n_60: 1.0923 (1.1789)  loss_n_80: 1.2614 (1.2512)  loss_n_100: 1.4379 (1.4518)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0083)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [270/845]  eta: 0:03:13  loss: 5.2450 (5.1313)  loss_n_40: 1.2497 (1.2446)  loss_n_60: 1.1989 (1.1774)  loss_n_80: 1.2475 (1.2502)  loss_n_100: 1.3774 (1.4511)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0080)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [280/845]  eta: 0:03:10  loss: 5.2608 (5.1378)  loss_n_40: 1.1629 (1.2436)  loss_n_60: 1.1989 (1.1812)  loss_n_80: 1.2477 (1.2515)  loss_n_100: 1.4094 (1.4537)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0077)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [290/845]  eta: 0:03:06  loss: 5.2608 (5.1458)  loss_n_40: 1.2306 (1.2463)  loss_n_60: 1.2163 (1.1835)  loss_n_80: 1.2978 (1.2521)  loss_n_100: 1.5388 (1.4565)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0075)  time: 0.3349  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:9]  [300/845]  eta: 0:03:03  loss: 5.3425 (5.1587)  loss_n_40: 1.2306 (1.2474)  loss_n_60: 1.3050 (1.1893)  loss_n_80: 1.3265 (1.2556)  loss_n_100: 1.5411 (1.4593)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0072)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [310/845]  eta: 0:02:59  loss: 5.3647 (5.1558)  loss_n_40: 1.2592 (1.2473)  loss_n_60: 1.2591 (1.1883)  loss_n_80: 1.3187 (1.2544)  loss_n_100: 1.5234 (1.4588)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0070)  time: 0.3350  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:9]  [320/845]  eta: 0:02:56  loss: 5.2768 (5.1556)  loss_n_40: 1.2592 (1.2490)  loss_n_60: 1.1105 (1.1872)  loss_n_80: 1.2357 (1.2533)  loss_n_100: 1.4193 (1.4594)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0068)  time: 0.3351  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:9]  [330/845]  eta: 0:02:53  loss: 5.2840 (5.1581)  loss_n_40: 1.2242 (1.2470)  loss_n_60: 1.1105 (1.1875)  loss_n_80: 1.2701 (1.2543)  loss_n_100: 1.5500 (1.4626)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0066)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [340/845]  eta: 0:02:49  loss: 5.1105 (5.1605)  loss_n_40: 1.2009 (1.2457)  loss_n_60: 1.1240 (1.1881)  loss_n_80: 1.2445 (1.2551)  loss_n_100: 1.5951 (1.4652)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0064)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [350/845]  eta: 0:02:46  loss: 5.1105 (5.1677)  loss_n_40: 1.2274 (1.2478)  loss_n_60: 1.1465 (1.1904)  loss_n_80: 1.2380 (1.2567)  loss_n_100: 1.4448 (1.4666)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0062)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [360/845]  eta: 0:02:42  loss: 4.9842 (5.1659)  loss_n_40: 1.1654 (1.2492)  loss_n_60: 1.1458 (1.1909)  loss_n_80: 1.2278 (1.2558)  loss_n_100: 1.3909 (1.4640)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0060)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [370/845]  eta: 0:02:39  loss: 4.9394 (5.1695)  loss_n_40: 1.2141 (1.2504)  loss_n_60: 1.1195 (1.1916)  loss_n_80: 1.2610 (1.2567)  loss_n_100: 1.3597 (1.4649)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0058)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [380/845]  eta: 0:02:36  loss: 4.8980 (5.1615)  loss_n_40: 1.2141 (1.2478)  loss_n_60: 1.2203 (1.1905)  loss_n_80: 1.1982 (1.2547)  loss_n_100: 1.4024 (1.4628)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0057)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [390/845]  eta: 0:02:32  loss: 4.9535 (5.1636)  loss_n_40: 1.1642 (1.2509)  loss_n_60: 1.1696 (1.1917)  loss_n_80: 1.2057 (1.2546)  loss_n_100: 1.4206 (1.4608)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0055)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [400/845]  eta: 0:02:29  loss: 5.0918 (5.1760)  loss_n_40: 1.3160 (1.2537)  loss_n_60: 1.1985 (1.1940)  loss_n_80: 1.2719 (1.2564)  loss_n_100: 1.4206 (1.4595)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0024)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0100)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [410/845]  eta: 0:02:26  loss: 5.0646 (5.1760)  loss_n_40: 1.2934 (1.2566)  loss_n_60: 1.1985 (1.1927)  loss_n_80: 1.2343 (1.2561)  loss_n_100: 1.3610 (1.4586)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0023)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0097)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [420/845]  eta: 0:02:22  loss: 5.3612 (5.1857)  loss_n_40: 1.2745 (1.2596)  loss_n_60: 1.1228 (1.1949)  loss_n_80: 1.3054 (1.2588)  loss_n_100: 1.4754 (1.4606)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0023)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0095)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [430/845]  eta: 0:02:19  loss: 5.0830 (5.1756)  loss_n_40: 1.1879 (1.2571)  loss_n_60: 1.1228 (1.1917)  loss_n_80: 1.3054 (1.2575)  loss_n_100: 1.4754 (1.4579)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0022)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0093)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [440/845]  eta: 0:02:15  loss: 4.8251 (5.1819)  loss_n_40: 1.1796 (1.2588)  loss_n_60: 1.1184 (1.1940)  loss_n_80: 1.2332 (1.2596)  loss_n_100: 1.3962 (1.4583)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0022)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0091)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [450/845]  eta: 0:02:12  loss: 5.2872 (5.1862)  loss_n_40: 1.2467 (1.2599)  loss_n_60: 1.1989 (1.1948)  loss_n_80: 1.3129 (1.2606)  loss_n_100: 1.3780 (1.4599)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0021)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0089)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [460/845]  eta: 0:02:09  loss: 4.8358 (5.1793)  loss_n_40: 1.1677 (1.2570)  loss_n_60: 1.0863 (1.1929)  loss_n_80: 1.1860 (1.2589)  loss_n_100: 1.4007 (1.4599)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0021)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0087)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [470/845]  eta: 0:02:05  loss: 5.0524 (5.1848)  loss_n_40: 1.1677 (1.2595)  loss_n_60: 1.0863 (1.1952)  loss_n_80: 1.1860 (1.2601)  loss_n_100: 1.4007 (1.4594)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0020)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0085)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [480/845]  eta: 0:02:02  loss: 5.4243 (5.1925)  loss_n_40: 1.3348 (1.2653)  loss_n_60: 1.1509 (1.1972)  loss_n_80: 1.3538 (1.2612)  loss_n_100: 1.4308 (1.4585)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0020)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0083)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [490/845]  eta: 0:01:59  loss: 5.2571 (5.1974)  loss_n_40: 1.2999 (1.2649)  loss_n_60: 1.1417 (1.1983)  loss_n_80: 1.2759 (1.2603)  loss_n_100: 1.4308 (1.4583)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0073)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0081)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [500/845]  eta: 0:01:55  loss: 5.0072 (5.1954)  loss_n_40: 1.1818 (1.2640)  loss_n_60: 1.1865 (1.1989)  loss_n_80: 1.2252 (1.2605)  loss_n_100: 1.4339 (1.4569)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0072)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0080)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [510/845]  eta: 0:01:52  loss: 5.0560 (5.1957)  loss_n_40: 1.1893 (1.2634)  loss_n_60: 1.1986 (1.1991)  loss_n_80: 1.2428 (1.2610)  loss_n_100: 1.4381 (1.4574)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0070)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0078)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [520/845]  eta: 0:01:49  loss: 5.0435 (5.1908)  loss_n_40: 1.1893 (1.2620)  loss_n_60: 1.0931 (1.1976)  loss_n_80: 1.2252 (1.2597)  loss_n_100: 1.4211 (1.4569)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0077)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [530/845]  eta: 0:01:45  loss: 4.7526 (5.1865)  loss_n_40: 1.1469 (1.2617)  loss_n_60: 1.0079 (1.1961)  loss_n_80: 1.1992 (1.2579)  loss_n_100: 1.4197 (1.4565)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0075)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [540/845]  eta: 0:01:42  loss: 4.7526 (5.1830)  loss_n_40: 1.1502 (1.2607)  loss_n_60: 0.9909 (1.1941)  loss_n_80: 1.1386 (1.2572)  loss_n_100: 1.4277 (1.4570)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0066)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0074)  time: 0.3349  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:9]  [550/845]  eta: 0:01:38  loss: 4.8616 (5.1831)  loss_n_40: 1.1494 (1.2593)  loss_n_60: 1.0573 (1.1950)  loss_n_80: 1.1756 (1.2573)  loss_n_100: 1.5470 (1.4578)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0065)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0073)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [560/845]  eta: 0:01:35  loss: 5.2339 (5.1838)  loss_n_40: 1.2126 (1.2597)  loss_n_60: 1.1885 (1.1953)  loss_n_80: 1.3152 (1.2578)  loss_n_100: 1.5626 (1.4574)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0064)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0071)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [570/845]  eta: 0:01:32  loss: 5.2339 (5.1832)  loss_n_40: 1.2388 (1.2594)  loss_n_60: 1.1019 (1.1945)  loss_n_80: 1.2431 (1.2576)  loss_n_100: 1.4944 (1.4584)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0063)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0070)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [580/845]  eta: 0:01:28  loss: 5.2787 (5.1856)  loss_n_40: 1.1943 (1.2590)  loss_n_60: 1.1741 (1.1951)  loss_n_80: 1.2298 (1.2584)  loss_n_100: 1.5151 (1.4599)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0069)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [590/845]  eta: 0:01:25  loss: 5.3972 (5.1881)  loss_n_40: 1.2095 (1.2589)  loss_n_60: 1.2442 (1.1961)  loss_n_80: 1.3893 (1.2597)  loss_n_100: 1.5746 (1.4606)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0068)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [600/845]  eta: 0:01:22  loss: 5.3131 (5.1852)  loss_n_40: 1.1885 (1.2581)  loss_n_60: 1.1966 (1.1952)  loss_n_80: 1.3432 (1.2591)  loss_n_100: 1.4033 (1.4601)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0067)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [610/845]  eta: 0:01:18  loss: 5.1249 (5.1847)  loss_n_40: 1.1634 (1.2572)  loss_n_60: 1.0845 (1.1951)  loss_n_80: 1.3098 (1.2590)  loss_n_100: 1.4187 (1.4610)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0065)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [620/845]  eta: 0:01:15  loss: 4.6645 (5.1816)  loss_n_40: 1.1681 (1.2563)  loss_n_60: 1.0280 (1.1939)  loss_n_80: 1.2198 (1.2580)  loss_n_100: 1.4780 (1.4612)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0064)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [630/845]  eta: 0:01:12  loss: 4.6427 (5.1860)  loss_n_40: 1.1943 (1.2591)  loss_n_60: 0.9753 (1.1941)  loss_n_80: 1.1250 (1.2584)  loss_n_100: 1.4896 (1.4624)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0063)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [640/845]  eta: 0:01:08  loss: 5.1351 (5.1891)  loss_n_40: 1.2142 (1.2594)  loss_n_60: 1.1755 (1.1951)  loss_n_80: 1.2564 (1.2590)  loss_n_100: 1.5462 (1.4637)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0056)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0062)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [650/845]  eta: 0:01:05  loss: 5.0925 (5.1868)  loss_n_40: 1.2142 (1.2588)  loss_n_60: 1.1888 (1.1946)  loss_n_80: 1.2692 (1.2594)  loss_n_100: 1.4420 (1.4623)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0055)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0061)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [660/845]  eta: 0:01:02  loss: 5.0197 (5.1880)  loss_n_40: 1.2166 (1.2593)  loss_n_60: 1.1701 (1.1953)  loss_n_80: 1.2826 (1.2597)  loss_n_100: 1.3348 (1.4622)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0060)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [670/845]  eta: 0:00:58  loss: 5.5189 (5.1889)  loss_n_40: 1.1943 (1.2580)  loss_n_60: 1.2708 (1.1960)  loss_n_80: 1.2592 (1.2603)  loss_n_100: 1.5467 (1.4633)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0060)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [680/845]  eta: 0:00:55  loss: 5.1219 (5.1916)  loss_n_40: 1.2009 (1.2606)  loss_n_60: 1.2327 (1.1964)  loss_n_80: 1.2433 (1.2607)  loss_n_100: 1.4771 (1.4628)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0059)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [690/845]  eta: 0:00:51  loss: 5.2270 (5.1971)  loss_n_40: 1.2458 (1.2619)  loss_n_60: 1.1348 (1.1980)  loss_n_80: 1.3119 (1.2616)  loss_n_100: 1.5067 (1.4647)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0052)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0058)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [700/845]  eta: 0:00:48  loss: 5.2270 (5.1928)  loss_n_40: 1.1926 (1.2611)  loss_n_60: 1.1348 (1.1965)  loss_n_80: 1.2455 (1.2608)  loss_n_100: 1.5105 (1.4635)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0057)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [710/845]  eta: 0:00:45  loss: 4.9812 (5.1924)  loss_n_40: 1.1926 (1.2616)  loss_n_60: 1.0921 (1.1965)  loss_n_80: 1.2189 (1.2609)  loss_n_100: 1.4212 (1.4628)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0056)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [720/845]  eta: 0:00:41  loss: 4.8645 (5.1865)  loss_n_40: 1.1762 (1.2610)  loss_n_60: 1.0921 (1.1951)  loss_n_80: 1.2584 (1.2605)  loss_n_100: 1.2824 (1.4593)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0055)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [730/845]  eta: 0:00:38  loss: 4.8181 (5.1860)  loss_n_40: 1.2538 (1.2638)  loss_n_60: 1.1176 (1.1949)  loss_n_80: 1.2073 (1.2600)  loss_n_100: 1.1623 (1.4569)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0055)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [740/845]  eta: 0:00:35  loss: 4.7057 (5.1796)  loss_n_40: 1.2210 (1.2632)  loss_n_60: 1.0612 (1.1926)  loss_n_80: 1.1426 (1.2584)  loss_n_100: 1.2756 (1.4551)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0048)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0054)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [750/845]  eta: 0:00:31  loss: 5.0010 (5.1807)  loss_n_40: 1.1778 (1.2625)  loss_n_60: 1.1064 (1.1930)  loss_n_80: 1.2194 (1.2588)  loss_n_100: 1.3998 (1.4564)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0048)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0053)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [760/845]  eta: 0:00:28  loss: 5.0546 (5.1757)  loss_n_40: 1.1778 (1.2616)  loss_n_60: 1.1229 (1.1917)  loss_n_80: 1.2426 (1.2584)  loss_n_100: 1.4403 (1.4540)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0047)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0053)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [770/845]  eta: 0:00:25  loss: 4.4465 (5.1714)  loss_n_40: 1.1374 (1.2619)  loss_n_60: 0.9292 (1.1901)  loss_n_80: 1.1365 (1.2568)  loss_n_100: 1.3653 (1.4528)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0047)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0052)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [780/845]  eta: 0:00:21  loss: 4.9614 (5.1736)  loss_n_40: 1.2155 (1.2625)  loss_n_60: 1.1045 (1.1905)  loss_n_80: 1.1469 (1.2576)  loss_n_100: 1.3984 (1.4533)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0046)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0051)  time: 0.3353  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:9]  [790/845]  eta: 0:00:18  loss: 5.1798 (5.1734)  loss_n_40: 1.1915 (1.2617)  loss_n_60: 1.2327 (1.1911)  loss_n_80: 1.2912 (1.2578)  loss_n_100: 1.4108 (1.4532)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0045)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0051)  time: 0.3353  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:9]  [800/845]  eta: 0:00:15  loss: 4.9180 (5.1720)  loss_n_40: 1.1370 (1.2619)  loss_n_60: 1.1217 (1.1909)  loss_n_80: 1.2444 (1.2569)  loss_n_100: 1.4187 (1.4529)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0045)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0050)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [810/845]  eta: 0:00:11  loss: 4.8524 (5.1704)  loss_n_40: 1.1437 (1.2614)  loss_n_60: 1.0618 (1.1907)  loss_n_80: 1.2359 (1.2568)  loss_n_100: 1.4379 (1.4522)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0044)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0049)  time: 0.3348  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:9]  [820/845]  eta: 0:00:08  loss: 4.8556 (5.1680)  loss_n_40: 1.2033 (1.2609)  loss_n_60: 1.0402 (1.1896)  loss_n_80: 1.2187 (1.2562)  loss_n_100: 1.4421 (1.4521)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0044)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0049)  time: 0.3350  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:9]  [830/845]  eta: 0:00:05  loss: 4.8713 (5.1643)  loss_n_40: 1.1491 (1.2604)  loss_n_60: 1.0586 (1.1891)  loss_n_80: 1.1938 (1.2551)  loss_n_100: 1.3937 (1.4506)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0043)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0048)  time: 0.3351  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:9]  [840/845]  eta: 0:00:01  loss: 5.1510 (5.1808)  loss_n_40: 1.3161 (1.2630)  loss_n_60: 1.2121 (1.1909)  loss_n_80: 1.2582 (1.2560)  loss_n_100: 1.3787 (1.4517)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0072)  triple_60: 0.0000 (0.0018)  triple_40: 0.0000 (0.0048)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9]  [844/845]  eta: 0:00:00  loss: 5.1433 (5.1790)  loss_n_40: 1.3161 (1.2628)  loss_n_60: 1.1390 (1.1903)  loss_n_80: 1.1935 (1.2556)  loss_n_100: 1.3787 (1.4513)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0072)  triple_60: 0.0000 (0.0018)  triple_40: 0.0000 (0.0047)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:9] Total time: 0:04:43 (0.3355 s / it)\n",
      "Averaged stats: loss: 5.1433 (5.1790)  loss_n_40: 1.3161 (1.2628)  loss_n_60: 1.1390 (1.1903)  loss_n_80: 1.1935 (1.2556)  loss_n_100: 1.3787 (1.4513)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0072)  triple_60: 0.0000 (0.0018)  triple_40: 0.0000 (0.0047)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_9_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 1.451%\n",
      "Min loss_n_100: 0.691\n",
      "Best Epoch: 8.000\n",
      "Train: [epoch:10]  [   0/1724]  eta: 2:00:32  lr: 0.000180  loss: 5.3948 (5.3948)  loss_n_40: 1.2625 (1.2625)  loss_n_60: 1.2403 (1.2403)  loss_n_80: 1.3305 (1.3305)  loss_n_100: 1.5614 (1.5614)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1952  data: 0.4361  max mem: 46473\n",
      "Train: [epoch:10]  [  10/1724]  eta: 1:52:48  lr: 0.000180  loss: 4.7808 (4.7526)  loss_n_40: 1.1741 (1.1945)  loss_n_60: 1.0501 (1.0904)  loss_n_80: 1.1762 (1.1659)  loss_n_100: 1.2884 (1.3018)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9490  data: 0.0398  max mem: 46473\n",
      "Train: [epoch:10]  [  20/1724]  eta: 1:51:48  lr: 0.000180  loss: 4.5072 (4.5160)  loss_n_40: 1.1155 (1.1557)  loss_n_60: 1.0350 (1.0349)  loss_n_80: 1.0964 (1.0993)  loss_n_100: 1.1972 (1.2156)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [  30/1724]  eta: 1:51:02  lr: 0.000180  loss: 4.0213 (4.3253)  loss_n_40: 1.0574 (1.1191)  loss_n_60: 0.9531 (0.9958)  loss_n_80: 0.9507 (1.0474)  loss_n_100: 1.0663 (1.1559)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0071)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [  40/1724]  eta: 1:50:20  lr: 0.000180  loss: 3.7494 (4.1591)  loss_n_40: 0.9579 (1.0679)  loss_n_60: 0.8705 (0.9582)  loss_n_80: 0.9039 (1.0116)  loss_n_100: 0.9922 (1.1160)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [  50/1724]  eta: 1:49:38  lr: 0.000180  loss: 3.4148 (4.0650)  loss_n_40: 0.8910 (1.0385)  loss_n_60: 0.8040 (0.9326)  loss_n_80: 0.8598 (0.9891)  loss_n_100: 0.9710 (1.0891)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0104)  triple_60: 0.0000 (0.0017)  triple_40: 0.0000 (0.0035)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [  60/1724]  eta: 1:48:57  lr: 0.000180  loss: 3.5350 (3.9955)  loss_n_40: 0.9050 (1.0141)  loss_n_60: 0.8040 (0.9126)  loss_n_80: 0.8588 (0.9787)  loss_n_100: 0.9828 (1.0771)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0087)  triple_60: 0.0000 (0.0014)  triple_40: 0.0000 (0.0029)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [  70/1724]  eta: 1:48:17  lr: 0.000180  loss: 3.5350 (3.9263)  loss_n_40: 0.8951 (1.0003)  loss_n_60: 0.7774 (0.8950)  loss_n_80: 0.8727 (0.9645)  loss_n_100: 0.9626 (1.0553)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0075)  triple_60: 0.0000 (0.0012)  triple_40: 0.0000 (0.0025)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [  80/1724]  eta: 1:47:37  lr: 0.000180  loss: 3.3278 (3.8554)  loss_n_40: 0.8261 (0.9807)  loss_n_60: 0.7303 (0.8813)  loss_n_80: 0.8314 (0.9484)  loss_n_100: 0.8869 (1.0352)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0066)  triple_60: 0.0000 (0.0010)  triple_40: 0.0000 (0.0022)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [  90/1724]  eta: 1:46:57  lr: 0.000180  loss: 3.0535 (3.7550)  loss_n_40: 0.7198 (0.9511)  loss_n_60: 0.6968 (0.8585)  loss_n_80: 0.7617 (0.9250)  loss_n_100: 0.8338 (1.0111)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0065)  triple_60: 0.0000 (0.0009)  triple_40: 0.0000 (0.0020)  time: 3.9254  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [ 100/1724]  eta: 1:46:18  lr: 0.000180  loss: 2.9332 (3.6830)  loss_n_40: 0.7111 (0.9342)  loss_n_60: 0.6535 (0.8418)  loss_n_80: 0.7176 (0.9066)  loss_n_100: 0.7955 (0.9919)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0008)  triple_40: 0.0000 (0.0018)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 110/1724]  eta: 1:45:39  lr: 0.000180  loss: 2.7419 (3.6099)  loss_n_40: 0.6545 (0.9132)  loss_n_60: 0.6312 (0.8258)  loss_n_80: 0.7012 (0.8892)  loss_n_100: 0.7713 (0.9740)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0008)  triple_40: 0.0000 (0.0017)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 120/1724]  eta: 1:44:59  lr: 0.000180  loss: 2.7000 (3.5274)  loss_n_40: 0.6097 (0.8884)  loss_n_60: 0.6200 (0.8082)  loss_n_80: 0.6718 (0.8691)  loss_n_100: 0.7425 (0.9546)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0007)  triple_40: 0.0000 (0.0015)  time: 3.9273  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [ 130/1724]  eta: 1:44:20  lr: 0.000180  loss: 2.5942 (3.4689)  loss_n_40: 0.6229 (0.8700)  loss_n_60: 0.6183 (0.7952)  loss_n_80: 0.6290 (0.8553)  loss_n_100: 0.7316 (0.9404)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0045)  triple_60: 0.0000 (0.0006)  triple_40: 0.0000 (0.0027)  time: 3.9243  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 140/1724]  eta: 1:43:40  lr: 0.000180  loss: 2.8062 (3.4276)  loss_n_40: 0.6611 (0.8546)  loss_n_60: 0.6262 (0.7860)  loss_n_80: 0.6961 (0.8452)  loss_n_100: 0.7203 (0.9288)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0036)  triple_40: 0.0000 (0.0043)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 150/1724]  eta: 1:43:01  lr: 0.000180  loss: 3.5906 (3.5725)  loss_n_40: 0.7746 (0.8746)  loss_n_60: 0.7680 (0.8094)  loss_n_80: 0.9284 (0.8829)  loss_n_100: 1.0343 (0.9705)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0095)  triple_60: 0.0000 (0.0125)  triple_40: 0.0000 (0.0065)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 160/1724]  eta: 1:42:21  lr: 0.000180  loss: 5.7939 (3.7294)  loss_n_40: 1.1904 (0.8957)  loss_n_60: 1.2761 (0.8432)  loss_n_80: 1.5368 (0.9289)  loss_n_100: 1.6481 (1.0239)  triple_100: 0.0000 (0.0063)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0117)  triple_40: 0.0000 (0.0061)  time: 3.9261  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:10]  [ 170/1724]  eta: 1:41:42  lr: 0.000180  loss: 5.5294 (3.8132)  loss_n_40: 1.1273 (0.9065)  loss_n_60: 1.2750 (0.8610)  loss_n_80: 1.4434 (0.9527)  loss_n_100: 1.6106 (1.0517)  triple_100: 0.0000 (0.0118)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0057)  time: 3.9281  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 180/1724]  eta: 1:41:03  lr: 0.000180  loss: 4.8561 (3.8749)  loss_n_40: 1.0567 (0.9127)  loss_n_60: 1.1154 (0.8736)  loss_n_80: 1.2936 (0.9728)  loss_n_100: 1.4923 (1.0762)  triple_100: 0.0000 (0.0111)  triple_80: 0.0000 (0.0121)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0058)  time: 3.9298  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 190/1724]  eta: 1:40:24  lr: 0.000180  loss: 4.7176 (3.9137)  loss_n_40: 0.9788 (0.9157)  loss_n_60: 1.0487 (0.8817)  loss_n_80: 1.2827 (0.9863)  loss_n_100: 1.3989 (1.0912)  triple_100: 0.0000 (0.0105)  triple_80: 0.0000 (0.0115)  triple_60: 0.0000 (0.0099)  triple_40: 0.0000 (0.0069)  time: 3.9284  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 200/1724]  eta: 1:39:44  lr: 0.000180  loss: 4.5707 (3.9617)  loss_n_40: 0.9486 (0.9160)  loss_n_60: 1.0303 (0.8870)  loss_n_80: 1.2064 (0.9946)  loss_n_100: 1.3162 (1.0993)  triple_100: 0.0000 (0.0160)  triple_80: 0.0000 (0.0291)  triple_60: 0.0000 (0.0115)  triple_40: 0.0000 (0.0081)  time: 3.9261  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 210/1724]  eta: 1:39:05  lr: 0.000180  loss: 4.5707 (4.0083)  loss_n_40: 0.9386 (0.9179)  loss_n_60: 0.9984 (0.8947)  loss_n_80: 1.2099 (1.0062)  loss_n_100: 1.2921 (1.1122)  triple_100: 0.0000 (0.0153)  triple_80: 0.0000 (0.0435)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0077)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 220/1724]  eta: 1:38:25  lr: 0.000180  loss: 4.3296 (4.0162)  loss_n_40: 0.9386 (0.9159)  loss_n_60: 0.9984 (0.8980)  loss_n_80: 1.1430 (1.0109)  loss_n_100: 1.2494 (1.1175)  triple_100: 0.0000 (0.0146)  triple_80: 0.0000 (0.0415)  triple_60: 0.0000 (0.0105)  triple_40: 0.0000 (0.0073)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 230/1724]  eta: 1:37:46  lr: 0.000180  loss: 4.0967 (4.0241)  loss_n_40: 0.8286 (0.9119)  loss_n_60: 0.9409 (0.9025)  loss_n_80: 1.1122 (1.0171)  loss_n_100: 1.2065 (1.1220)  triple_100: 0.0000 (0.0140)  triple_80: 0.0000 (0.0397)  triple_60: 0.0000 (0.0100)  triple_40: 0.0000 (0.0070)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 240/1724]  eta: 1:37:07  lr: 0.000180  loss: 3.7986 (4.0178)  loss_n_40: 0.7684 (0.9043)  loss_n_60: 0.8592 (0.8991)  loss_n_80: 1.0328 (1.0159)  loss_n_100: 1.1126 (1.1190)  triple_100: 0.0000 (0.0134)  triple_80: 0.0000 (0.0443)  triple_60: 0.0000 (0.0139)  triple_40: 0.0000 (0.0078)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 250/1724]  eta: 1:36:27  lr: 0.000180  loss: 4.6462 (4.0648)  loss_n_40: 0.8094 (0.9062)  loss_n_60: 0.9299 (0.9099)  loss_n_80: 1.0638 (1.0329)  loss_n_100: 1.1502 (1.1392)  triple_100: 0.0000 (0.0132)  triple_80: 0.0000 (0.0425)  triple_60: 0.0000 (0.0134)  triple_40: 0.0000 (0.0075)  time: 3.9262  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [ 260/1724]  eta: 1:35:48  lr: 0.000180  loss: 4.9748 (4.1126)  loss_n_40: 0.8802 (0.9091)  loss_n_60: 1.1317 (0.9211)  loss_n_80: 1.3743 (1.0478)  loss_n_100: 1.6202 (1.1591)  triple_100: 0.0000 (0.0127)  triple_80: 0.0000 (0.0409)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0072)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 270/1724]  eta: 1:35:09  lr: 0.000180  loss: 4.5259 (4.1161)  loss_n_40: 0.7688 (0.9026)  loss_n_60: 1.0513 (0.9230)  loss_n_80: 1.2095 (1.0505)  loss_n_100: 1.4437 (1.1660)  triple_100: 0.0000 (0.0122)  triple_80: 0.0000 (0.0394)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0069)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 280/1724]  eta: 1:34:29  lr: 0.000180  loss: 3.8844 (4.0949)  loss_n_40: 0.7047 (0.8947)  loss_n_60: 0.8807 (0.9184)  loss_n_80: 1.0483 (1.0457)  loss_n_100: 1.2237 (1.1642)  triple_100: 0.0000 (0.0118)  triple_80: 0.0000 (0.0380)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0067)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 290/1724]  eta: 1:33:50  lr: 0.000180  loss: 3.3667 (4.1191)  loss_n_40: 0.6713 (0.8887)  loss_n_60: 0.7862 (0.9158)  loss_n_80: 0.8730 (1.0419)  loss_n_100: 1.0350 (1.1615)  triple_100: 0.0000 (0.0337)  triple_80: 0.0000 (0.0479)  triple_60: 0.0000 (0.0231)  triple_40: 0.0000 (0.0064)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 300/1724]  eta: 1:33:10  lr: 0.000180  loss: 7.2139 (4.3156)  loss_n_40: 0.9015 (0.9115)  loss_n_60: 1.0864 (0.9538)  loss_n_80: 1.1853 (1.0826)  loss_n_100: 1.4524 (1.2003)  triple_100: 0.0000 (0.0778)  triple_80: 0.0000 (0.0610)  triple_60: 0.0000 (0.0223)  triple_40: 0.0000 (0.0062)  time: 3.9236  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 310/1724]  eta: 1:32:31  lr: 0.000180  loss: 10.9919 (4.6212)  loss_n_40: 2.0435 (0.9559)  loss_n_60: 2.4803 (1.0115)  loss_n_80: 2.5635 (1.1426)  loss_n_100: 2.6568 (1.2613)  triple_100: 0.0000 (0.0976)  triple_80: 0.0000 (0.0817)  triple_60: 0.0000 (0.0225)  triple_40: 0.0000 (0.0480)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 320/1724]  eta: 1:31:52  lr: 0.000180  loss: 11.5822 (4.8572)  loss_n_40: 2.3430 (0.9973)  loss_n_60: 2.7194 (1.0621)  loss_n_80: 2.9942 (1.2012)  loss_n_100: 3.0664 (1.3194)  triple_100: 0.0000 (0.1022)  triple_80: 0.0000 (0.1066)  triple_60: 0.0000 (0.0218)  triple_40: 0.0000 (0.0465)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 330/1724]  eta: 1:31:13  lr: 0.000180  loss: 10.7798 (5.0200)  loss_n_40: 2.2275 (1.0329)  loss_n_60: 2.6044 (1.1069)  loss_n_80: 2.9362 (1.2494)  loss_n_100: 2.9148 (1.3621)  triple_100: 0.0000 (0.0992)  triple_80: 0.0000 (0.1034)  triple_60: 0.0000 (0.0212)  triple_40: 0.0000 (0.0451)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 340/1724]  eta: 1:30:33  lr: 0.000180  loss: 10.2568 (5.1616)  loss_n_40: 2.1158 (1.0620)  loss_n_60: 2.5398 (1.1465)  loss_n_80: 2.8096 (1.2912)  loss_n_100: 2.7696 (1.4011)  triple_100: 0.0000 (0.0962)  triple_80: 0.0000 (0.1003)  triple_60: 0.0000 (0.0205)  triple_40: 0.0000 (0.0438)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 350/1724]  eta: 1:29:54  lr: 0.000180  loss: 9.6277 (5.2779)  loss_n_40: 1.9111 (1.0839)  loss_n_60: 2.3759 (1.1807)  loss_n_80: 2.5818 (1.3246)  loss_n_100: 2.6769 (1.4352)  triple_100: 0.0000 (0.0935)  triple_80: 0.0000 (0.0975)  triple_60: 0.0000 (0.0200)  triple_40: 0.0000 (0.0425)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 360/1724]  eta: 1:29:14  lr: 0.000180  loss: 9.3408 (5.3962)  loss_n_40: 1.8596 (1.1064)  loss_n_60: 2.4161 (1.2159)  loss_n_80: 2.4668 (1.3567)  loss_n_100: 2.7172 (1.4708)  triple_100: 0.0000 (0.0909)  triple_80: 0.0000 (0.0948)  triple_60: 0.0000 (0.0194)  triple_40: 0.0000 (0.0413)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 370/1724]  eta: 1:28:35  lr: 0.000180  loss: 9.1254 (5.4770)  loss_n_40: 1.7712 (1.1227)  loss_n_60: 2.3471 (1.2390)  loss_n_80: 2.4010 (1.3772)  loss_n_100: 2.6322 (1.4938)  triple_100: 0.0000 (0.0903)  triple_80: 0.0000 (0.0922)  triple_60: 0.0000 (0.0189)  triple_40: 0.0000 (0.0430)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 380/1724]  eta: 1:27:55  lr: 0.000180  loss: 8.4117 (5.5621)  loss_n_40: 1.7116 (1.1383)  loss_n_60: 2.1264 (1.2641)  loss_n_80: 2.2083 (1.4007)  loss_n_100: 2.4741 (1.5209)  triple_100: 0.0000 (0.0879)  triple_80: 0.0000 (0.0898)  triple_60: 0.0000 (0.0184)  triple_40: 0.0000 (0.0419)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 390/1724]  eta: 1:27:16  lr: 0.000180  loss: 8.4689 (5.6451)  loss_n_40: 1.6835 (1.1525)  loss_n_60: 2.1540 (1.2891)  loss_n_80: 2.2251 (1.4242)  loss_n_100: 2.4776 (1.5475)  triple_100: 0.0000 (0.0857)  triple_80: 0.0000 (0.0875)  triple_60: 0.0000 (0.0179)  triple_40: 0.0000 (0.0408)  time: 3.9229  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [ 400/1724]  eta: 1:26:37  lr: 0.000180  loss: 8.2862 (5.7051)  loss_n_40: 1.5843 (1.1625)  loss_n_60: 2.1163 (1.3073)  loss_n_80: 2.1486 (1.4409)  loss_n_100: 2.4303 (1.5672)  triple_100: 0.0000 (0.0835)  triple_80: 0.0000 (0.0865)  triple_60: 0.0000 (0.0175)  triple_40: 0.0000 (0.0398)  time: 3.9240  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:10]  [ 410/1724]  eta: 1:25:58  lr: 0.000180  loss: 7.8055 (5.7564)  loss_n_40: 1.4980 (1.1710)  loss_n_60: 1.9618 (1.3226)  loss_n_80: 2.1007 (1.4553)  loss_n_100: 2.3710 (1.5842)  triple_100: 0.0000 (0.0817)  triple_80: 0.0000 (0.0857)  triple_60: 0.0000 (0.0170)  triple_40: 0.0000 (0.0388)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 420/1724]  eta: 1:25:18  lr: 0.000180  loss: 7.6225 (5.7942)  loss_n_40: 1.3804 (1.1746)  loss_n_60: 1.8760 (1.3334)  loss_n_80: 2.0242 (1.4681)  loss_n_100: 2.3419 (1.6002)  triple_100: 0.0000 (0.0798)  triple_80: 0.0000 (0.0837)  triple_60: 0.0000 (0.0166)  triple_40: 0.0000 (0.0379)  time: 3.9243  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 430/1724]  eta: 1:24:39  lr: 0.000180  loss: 7.5075 (5.8366)  loss_n_40: 1.3602 (1.1782)  loss_n_60: 1.7903 (1.3434)  loss_n_80: 2.0242 (1.4810)  loss_n_100: 2.2820 (1.6164)  triple_100: 0.0000 (0.0789)  triple_80: 0.0000 (0.0852)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0370)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 440/1724]  eta: 1:24:00  lr: 0.000180  loss: 7.1127 (5.8587)  loss_n_40: 1.3192 (1.1804)  loss_n_60: 1.6511 (1.3488)  loss_n_80: 1.9602 (1.4898)  loss_n_100: 2.2226 (1.6272)  triple_100: 0.0000 (0.0772)  triple_80: 0.0000 (0.0833)  triple_60: 0.0000 (0.0159)  triple_40: 0.0000 (0.0362)  time: 3.9238  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [ 450/1724]  eta: 1:23:20  lr: 0.000180  loss: 6.4923 (5.8661)  loss_n_40: 1.2487 (1.1819)  loss_n_60: 1.5196 (1.3496)  loss_n_80: 1.7194 (1.4935)  loss_n_100: 1.9923 (1.6332)  triple_100: 0.0000 (0.0754)  triple_80: 0.0000 (0.0814)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0354)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 460/1724]  eta: 1:22:41  lr: 0.000180  loss: 6.3971 (5.8867)  loss_n_40: 1.2076 (1.1845)  loss_n_60: 1.4012 (1.3510)  loss_n_80: 1.6188 (1.4958)  loss_n_100: 1.8960 (1.6378)  triple_100: 0.0000 (0.0835)  triple_80: 0.0000 (0.0833)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0356)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 470/1724]  eta: 1:22:02  lr: 0.000180  loss: 6.1618 (5.8904)  loss_n_40: 1.1912 (1.1844)  loss_n_60: 1.3746 (1.3521)  loss_n_80: 1.6148 (1.4983)  loss_n_100: 1.8935 (1.6427)  triple_100: 0.0000 (0.0818)  triple_80: 0.0000 (0.0815)  triple_60: 0.0000 (0.0149)  triple_40: 0.0000 (0.0348)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 480/1724]  eta: 1:21:23  lr: 0.000180  loss: 5.8270 (5.8877)  loss_n_40: 1.1703 (1.1838)  loss_n_60: 1.3355 (1.3508)  loss_n_80: 1.5245 (1.4972)  loss_n_100: 1.8072 (1.6437)  triple_100: 0.0000 (0.0837)  triple_80: 0.0000 (0.0798)  triple_60: 0.0000 (0.0146)  triple_40: 0.0000 (0.0341)  time: 3.9253  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 490/1724]  eta: 1:20:43  lr: 0.000180  loss: 5.1167 (5.8657)  loss_n_40: 1.0310 (1.1796)  loss_n_60: 1.1802 (1.3463)  loss_n_80: 1.3098 (1.4919)  loss_n_100: 1.5125 (1.6401)  triple_100: 0.0000 (0.0820)  triple_80: 0.0000 (0.0782)  triple_60: 0.0000 (0.0143)  triple_40: 0.0000 (0.0334)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 500/1724]  eta: 1:20:04  lr: 0.000180  loss: 4.5756 (5.8398)  loss_n_40: 0.9774 (1.1747)  loss_n_60: 1.0857 (1.3408)  loss_n_80: 1.2023 (1.4854)  loss_n_100: 1.4167 (1.6346)  triple_100: 0.0000 (0.0809)  triple_80: 0.0000 (0.0766)  triple_60: 0.0000 (0.0140)  triple_40: 0.0000 (0.0327)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 510/1724]  eta: 1:19:25  lr: 0.000180  loss: 4.2275 (5.8056)  loss_n_40: 0.8463 (1.1691)  loss_n_60: 0.9984 (1.3340)  loss_n_80: 1.0853 (1.4765)  loss_n_100: 1.2560 (1.6258)  triple_100: 0.0000 (0.0794)  triple_80: 0.0000 (0.0751)  triple_60: 0.0000 (0.0138)  triple_40: 0.0000 (0.0321)  time: 3.9279  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 520/1724]  eta: 1:18:46  lr: 0.000180  loss: 4.1371 (5.7794)  loss_n_40: 0.8166 (1.1630)  loss_n_60: 0.9446 (1.3264)  loss_n_80: 1.0308 (1.4680)  loss_n_100: 1.1729 (1.6175)  triple_100: 0.0000 (0.0784)  triple_80: 0.0000 (0.0745)  triple_60: 0.0000 (0.0179)  triple_40: 0.0000 (0.0336)  time: 3.9294  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 530/1724]  eta: 1:18:07  lr: 0.000180  loss: 4.4341 (5.7621)  loss_n_40: 0.8565 (1.1588)  loss_n_60: 1.0212 (1.3222)  loss_n_80: 1.1580 (1.4636)  loss_n_100: 1.3223 (1.6137)  triple_100: 0.0000 (0.0770)  triple_80: 0.0000 (0.0732)  triple_60: 0.0000 (0.0175)  triple_40: 0.0000 (0.0362)  time: 3.9301  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 540/1724]  eta: 1:17:28  lr: 0.000180  loss: 4.3251 (5.7308)  loss_n_40: 0.8546 (1.1530)  loss_n_60: 1.0130 (1.3151)  loss_n_80: 1.1149 (1.4558)  loss_n_100: 1.3211 (1.6068)  triple_100: 0.0000 (0.0755)  triple_80: 0.0000 (0.0719)  triple_60: 0.0000 (0.0172)  triple_40: 0.0000 (0.0355)  time: 3.9297  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 550/1724]  eta: 1:16:48  lr: 0.000180  loss: 3.8665 (5.6951)  loss_n_40: 0.7960 (1.1474)  loss_n_60: 0.8891 (1.3066)  loss_n_80: 0.9866 (1.4456)  loss_n_100: 1.1725 (1.5969)  triple_100: 0.0000 (0.0763)  triple_80: 0.0000 (0.0706)  triple_60: 0.0000 (0.0169)  triple_40: 0.0000 (0.0349)  time: 3.9289  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 560/1724]  eta: 1:16:09  lr: 0.000180  loss: 3.6251 (5.6588)  loss_n_40: 0.7971 (1.1435)  loss_n_60: 0.8397 (1.2987)  loss_n_80: 0.8996 (1.4359)  loss_n_100: 1.0158 (1.5856)  triple_100: 0.0000 (0.0750)  triple_80: 0.0000 (0.0693)  triple_60: 0.0000 (0.0166)  triple_40: 0.0000 (0.0342)  time: 3.9288  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [ 570/1724]  eta: 1:15:30  lr: 0.000180  loss: 3.4671 (5.6204)  loss_n_40: 0.8037 (1.1377)  loss_n_60: 0.8310 (1.2902)  loss_n_80: 0.8473 (1.4263)  loss_n_100: 0.9589 (1.5745)  triple_100: 0.0000 (0.0737)  triple_80: 0.0000 (0.0681)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0336)  time: 3.9289  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 580/1724]  eta: 1:14:51  lr: 0.000180  loss: 3.2969 (5.5769)  loss_n_40: 0.7574 (1.1307)  loss_n_60: 0.7698 (1.2804)  loss_n_80: 0.8415 (1.4153)  loss_n_100: 0.8921 (1.5621)  triple_100: 0.0000 (0.0724)  triple_80: 0.0000 (0.0670)  triple_60: 0.0000 (0.0160)  triple_40: 0.0000 (0.0331)  time: 3.9295  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 590/1724]  eta: 1:14:12  lr: 0.000180  loss: 3.0142 (5.5326)  loss_n_40: 0.6926 (1.1234)  loss_n_60: 0.6875 (1.2705)  loss_n_80: 0.7391 (1.4040)  loss_n_100: 0.8180 (1.5494)  triple_100: 0.0000 (0.0712)  triple_80: 0.0000 (0.0658)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0325)  time: 3.9313  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 600/1724]  eta: 1:13:32  lr: 0.000180  loss: 2.7927 (5.4869)  loss_n_40: 0.6280 (1.1153)  loss_n_60: 0.6486 (1.2602)  loss_n_80: 0.6981 (1.3926)  loss_n_100: 0.8069 (1.5367)  triple_100: 0.0000 (0.0700)  triple_80: 0.0000 (0.0647)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0320)  time: 3.9328  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [ 610/1724]  eta: 1:12:53  lr: 0.000180  loss: 2.9545 (5.4551)  loss_n_40: 0.6854 (1.1098)  loss_n_60: 0.7101 (1.2527)  loss_n_80: 0.7489 (1.3840)  loss_n_100: 0.8069 (1.5278)  triple_100: 0.0000 (0.0688)  triple_80: 0.0000 (0.0642)  triple_60: 0.0000 (0.0153)  triple_40: 0.0000 (0.0325)  time: 3.9322  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [ 620/1724]  eta: 1:12:14  lr: 0.000180  loss: 3.7770 (5.4311)  loss_n_40: 0.7597 (1.1061)  loss_n_60: 0.8827 (1.2476)  loss_n_80: 0.9598 (1.3779)  loss_n_100: 1.0873 (1.5213)  triple_100: 0.0000 (0.0677)  triple_80: 0.0000 (0.0631)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0320)  time: 3.9318  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 630/1724]  eta: 1:11:35  lr: 0.000180  loss: 3.5184 (5.3991)  loss_n_40: 0.7597 (1.1005)  loss_n_60: 0.8365 (1.2405)  loss_n_80: 0.9349 (1.3697)  loss_n_100: 1.0234 (1.5122)  triple_100: 0.0000 (0.0667)  triple_80: 0.0000 (0.0627)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0314)  time: 3.9341  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 640/1724]  eta: 1:10:56  lr: 0.000180  loss: 3.1188 (5.3610)  loss_n_40: 0.6806 (1.0937)  loss_n_60: 0.7405 (1.2321)  loss_n_80: 0.7881 (1.3599)  loss_n_100: 0.8698 (1.5018)  triple_100: 0.0000 (0.0656)  triple_80: 0.0000 (0.0617)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0310)  time: 3.9357  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:10]  [ 650/1724]  eta: 1:10:17  lr: 0.000180  loss: 2.8354 (5.3214)  loss_n_40: 0.6228 (1.0867)  loss_n_60: 0.6455 (1.2232)  loss_n_80: 0.7030 (1.3497)  loss_n_100: 0.8168 (1.4910)  triple_100: 0.0000 (0.0646)  triple_80: 0.0000 (0.0609)  triple_60: 0.0000 (0.0149)  triple_40: 0.0000 (0.0305)  time: 3.9344  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 660/1724]  eta: 1:09:38  lr: 0.000180  loss: 2.6593 (5.2862)  loss_n_40: 0.5993 (1.0812)  loss_n_60: 0.6195 (1.2152)  loss_n_80: 0.6762 (1.3400)  loss_n_100: 0.7587 (1.4801)  triple_100: 0.0000 (0.0636)  triple_80: 0.0000 (0.0613)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0300)  time: 3.9341  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [ 670/1724]  eta: 1:08:58  lr: 0.000180  loss: 2.8008 (5.2507)  loss_n_40: 0.5883 (1.0739)  loss_n_60: 0.6520 (1.2069)  loss_n_80: 0.6904 (1.3307)  loss_n_100: 0.7781 (1.4705)  triple_100: 0.0000 (0.0627)  triple_80: 0.0000 (0.0617)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0296)  time: 3.9335  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 680/1724]  eta: 1:08:19  lr: 0.000180  loss: 2.8008 (5.2187)  loss_n_40: 0.5556 (1.0673)  loss_n_60: 0.6520 (1.1991)  loss_n_80: 0.7237 (1.3220)  loss_n_100: 0.8566 (1.4613)  triple_100: 0.0000 (0.0618)  triple_80: 0.0000 (0.0622)  triple_60: 0.0000 (0.0159)  triple_40: 0.0000 (0.0291)  time: 3.9326  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 690/1724]  eta: 1:07:40  lr: 0.000180  loss: 2.9765 (5.1872)  loss_n_40: 0.6463 (1.0614)  loss_n_60: 0.6929 (1.1922)  loss_n_80: 0.7610 (1.3143)  loss_n_100: 0.8602 (1.4527)  triple_100: 0.0000 (0.0609)  triple_80: 0.0000 (0.0613)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0287)  time: 3.9324  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 700/1724]  eta: 1:07:01  lr: 0.000180  loss: 2.8497 (5.1519)  loss_n_40: 0.6353 (1.0548)  loss_n_60: 0.6877 (1.1842)  loss_n_80: 0.7366 (1.3053)  loss_n_100: 0.8344 (1.4433)  triple_100: 0.0000 (0.0601)  triple_80: 0.0000 (0.0605)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0283)  time: 3.9322  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 710/1724]  eta: 1:06:22  lr: 0.000180  loss: 2.5397 (5.1135)  loss_n_40: 0.5603 (1.0483)  loss_n_60: 0.5930 (1.1756)  loss_n_80: 0.6111 (1.2951)  loss_n_100: 0.7119 (1.4325)  triple_100: 0.0000 (0.0592)  triple_80: 0.0000 (0.0596)  triple_60: 0.0000 (0.0153)  triple_40: 0.0000 (0.0279)  time: 3.9314  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 720/1724]  eta: 1:05:42  lr: 0.000180  loss: 2.3109 (5.0748)  loss_n_40: 0.5435 (1.0416)  loss_n_60: 0.5424 (1.1670)  loss_n_80: 0.5699 (1.2849)  loss_n_100: 0.6490 (1.4216)  triple_100: 0.0000 (0.0584)  triple_80: 0.0000 (0.0588)  triple_60: 0.0000 (0.0150)  triple_40: 0.0000 (0.0275)  time: 3.9282  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 730/1724]  eta: 1:05:03  lr: 0.000180  loss: 2.1993 (5.0476)  loss_n_40: 0.5435 (1.0349)  loss_n_60: 0.5381 (1.1594)  loss_n_80: 0.5400 (1.2763)  loss_n_100: 0.6345 (1.4130)  triple_100: 0.0000 (0.0583)  triple_80: 0.0000 (0.0594)  triple_60: 0.0000 (0.0161)  triple_40: 0.0000 (0.0302)  time: 3.9255  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 740/1724]  eta: 1:04:24  lr: 0.000180  loss: 4.2186 (5.0426)  loss_n_40: 0.7088 (1.0321)  loss_n_60: 0.9660 (1.1585)  loss_n_80: 1.1476 (1.2763)  loss_n_100: 1.2230 (1.4129)  triple_100: 0.0000 (0.0575)  triple_80: 0.0000 (0.0596)  triple_60: 0.0000 (0.0159)  triple_40: 0.0000 (0.0298)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 750/1724]  eta: 1:03:45  lr: 0.000180  loss: 4.4124 (5.0304)  loss_n_40: 0.8223 (1.0301)  loss_n_60: 1.0322 (1.1562)  loss_n_80: 1.1856 (1.2738)  loss_n_100: 1.2429 (1.4097)  triple_100: 0.0000 (0.0568)  triple_80: 0.0000 (0.0588)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0294)  time: 3.9249  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 760/1724]  eta: 1:03:05  lr: 0.000180  loss: 3.8009 (5.0127)  loss_n_40: 0.7979 (1.0270)  loss_n_60: 0.9166 (1.1526)  loss_n_80: 0.9923 (1.2694)  loss_n_100: 1.1022 (1.4051)  triple_100: 0.0000 (0.0560)  triple_80: 0.0000 (0.0581)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0290)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 770/1724]  eta: 1:02:26  lr: 0.000180  loss: 3.3847 (4.9886)  loss_n_40: 0.6993 (1.0223)  loss_n_60: 0.7831 (1.1473)  loss_n_80: 0.8525 (1.2633)  loss_n_100: 0.9738 (1.3993)  triple_100: 0.0000 (0.0553)  triple_80: 0.0000 (0.0573)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0287)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 780/1724]  eta: 1:01:47  lr: 0.000180  loss: 3.3847 (4.9764)  loss_n_40: 0.7074 (1.0200)  loss_n_60: 0.7725 (1.1435)  loss_n_80: 0.8617 (1.2590)  loss_n_100: 0.9738 (1.3948)  triple_100: 0.0000 (0.0546)  triple_80: 0.0000 (0.0605)  triple_60: 0.0000 (0.0156)  triple_40: 0.0000 (0.0283)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 790/1724]  eta: 1:01:07  lr: 0.000180  loss: 3.9303 (4.9636)  loss_n_40: 0.8817 (1.0181)  loss_n_60: 0.8955 (1.1407)  loss_n_80: 0.9533 (1.2556)  loss_n_100: 1.1201 (1.3922)  triple_100: 0.0000 (0.0539)  triple_80: 0.0000 (0.0598)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0279)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 800/1724]  eta: 1:00:28  lr: 0.000180  loss: 3.5878 (4.9413)  loss_n_40: 0.7633 (1.0140)  loss_n_60: 0.8402 (1.1358)  loss_n_80: 0.8759 (1.2497)  loss_n_100: 1.0429 (1.3866)  triple_100: 0.0000 (0.0532)  triple_80: 0.0000 (0.0590)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0277)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 810/1724]  eta: 0:59:49  lr: 0.000180  loss: 3.3051 (4.9426)  loss_n_40: 0.7134 (1.0127)  loss_n_60: 0.7523 (1.1340)  loss_n_80: 0.8261 (1.2480)  loss_n_100: 1.0135 (1.3867)  triple_100: 0.0000 (0.0561)  triple_80: 0.0000 (0.0608)  triple_60: 0.0000 (0.0166)  triple_40: 0.0000 (0.0276)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 820/1724]  eta: 0:59:09  lr: 0.000180  loss: 5.0135 (4.9476)  loss_n_40: 0.9754 (1.0133)  loss_n_60: 1.1617 (1.1354)  loss_n_80: 1.2714 (1.2503)  loss_n_100: 1.4083 (1.3886)  triple_100: 0.0000 (0.0555)  triple_80: 0.0000 (0.0601)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0276)  time: 3.9254  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [ 830/1724]  eta: 0:58:30  lr: 0.000180  loss: 5.1340 (4.9500)  loss_n_40: 1.0938 (1.0157)  loss_n_60: 1.2383 (1.1366)  loss_n_80: 1.3023 (1.2514)  loss_n_100: 1.3699 (1.3884)  triple_100: 0.0000 (0.0548)  triple_80: 0.0000 (0.0594)  triple_60: 0.0000 (0.0166)  triple_40: 0.0000 (0.0273)  time: 3.9250  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [ 840/1724]  eta: 0:57:51  lr: 0.000180  loss: 4.6529 (4.9448)  loss_n_40: 1.1288 (1.0165)  loss_n_60: 1.1650 (1.1366)  loss_n_80: 1.1912 (1.2504)  loss_n_100: 1.1868 (1.3852)  triple_100: 0.0000 (0.0541)  triple_80: 0.0000 (0.0586)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0269)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 850/1724]  eta: 0:57:12  lr: 0.000180  loss: 4.0653 (4.9288)  loss_n_40: 0.8789 (1.0143)  loss_n_60: 1.0275 (1.1334)  loss_n_80: 1.0815 (1.2469)  loss_n_100: 1.0577 (1.3798)  triple_100: 0.0000 (0.0535)  triple_80: 0.0000 (0.0580)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0266)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 860/1724]  eta: 0:56:32  lr: 0.000180  loss: 3.4766 (4.9107)  loss_n_40: 0.7958 (1.0123)  loss_n_60: 0.8237 (1.1296)  loss_n_80: 0.9193 (1.2428)  loss_n_100: 0.8390 (1.3735)  triple_100: 0.0000 (0.0529)  triple_80: 0.0000 (0.0573)  triple_60: 0.0000 (0.0160)  triple_40: 0.0000 (0.0263)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 870/1724]  eta: 0:55:53  lr: 0.000180  loss: 3.1241 (4.8885)  loss_n_40: 0.7223 (1.0087)  loss_n_60: 0.7528 (1.1247)  loss_n_80: 0.8706 (1.2377)  loss_n_100: 0.7874 (1.3667)  triple_100: 0.0000 (0.0523)  triple_80: 0.0000 (0.0566)  triple_60: 0.0000 (0.0159)  triple_40: 0.0000 (0.0260)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 880/1724]  eta: 0:55:14  lr: 0.000180  loss: 3.0229 (4.8682)  loss_n_40: 0.7102 (1.0066)  loss_n_60: 0.6950 (1.1207)  loss_n_80: 0.7529 (1.2325)  loss_n_100: 0.7142 (1.3593)  triple_100: 0.0000 (0.0517)  triple_80: 0.0000 (0.0560)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0257)  time: 3.9236  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:10]  [ 890/1724]  eta: 0:54:34  lr: 0.000180  loss: 2.7994 (4.8441)  loss_n_40: 0.6749 (1.0030)  loss_n_60: 0.6950 (1.1154)  loss_n_80: 0.7311 (1.2266)  loss_n_100: 0.6941 (1.3517)  triple_100: 0.0000 (0.0511)  triple_80: 0.0000 (0.0554)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0254)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 900/1724]  eta: 0:53:55  lr: 0.000180  loss: 2.4810 (4.8179)  loss_n_40: 0.5691 (0.9984)  loss_n_60: 0.5637 (1.1093)  loss_n_80: 0.6639 (1.2201)  loss_n_100: 0.6759 (1.3442)  triple_100: 0.0000 (0.0505)  triple_80: 0.0000 (0.0547)  triple_60: 0.0000 (0.0153)  triple_40: 0.0000 (0.0253)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 910/1724]  eta: 0:53:16  lr: 0.000180  loss: 2.5635 (4.7966)  loss_n_40: 0.5498 (0.9942)  loss_n_60: 0.5703 (1.1043)  loss_n_80: 0.6662 (1.2150)  loss_n_100: 0.7159 (1.3382)  triple_100: 0.0000 (0.0504)  triple_80: 0.0000 (0.0543)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0250)  time: 3.9271  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 920/1724]  eta: 0:52:37  lr: 0.000180  loss: 2.8338 (4.7765)  loss_n_40: 0.6548 (0.9910)  loss_n_60: 0.6771 (1.0999)  loss_n_80: 0.7514 (1.2101)  loss_n_100: 0.7969 (1.3322)  triple_100: 0.0000 (0.0499)  triple_80: 0.0000 (0.0537)  triple_60: 0.0000 (0.0150)  triple_40: 0.0000 (0.0247)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 930/1724]  eta: 0:51:57  lr: 0.000180  loss: 2.8099 (4.7537)  loss_n_40: 0.6613 (0.9874)  loss_n_60: 0.6706 (1.0951)  loss_n_80: 0.6939 (1.2043)  loss_n_100: 0.7043 (1.3251)  triple_100: 0.0000 (0.0494)  triple_80: 0.0000 (0.0531)  triple_60: 0.0000 (0.0149)  triple_40: 0.0000 (0.0245)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 940/1724]  eta: 0:51:18  lr: 0.000180  loss: 2.5901 (4.7374)  loss_n_40: 0.5709 (0.9834)  loss_n_60: 0.5961 (1.0900)  loss_n_80: 0.6733 (1.1991)  loss_n_100: 0.6726 (1.3188)  triple_100: 0.0000 (0.0504)  triple_80: 0.0000 (0.0542)  triple_60: 0.0000 (0.0173)  triple_40: 0.0000 (0.0242)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 950/1724]  eta: 0:50:39  lr: 0.000180  loss: 3.0395 (4.7216)  loss_n_40: 0.6166 (0.9802)  loss_n_60: 0.6669 (1.0865)  loss_n_80: 0.7582 (1.1956)  loss_n_100: 0.8232 (1.3148)  triple_100: 0.0000 (0.0499)  triple_80: 0.0000 (0.0536)  triple_60: 0.0000 (0.0171)  triple_40: 0.0000 (0.0240)  time: 3.9250  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 960/1724]  eta: 0:49:59  lr: 0.000180  loss: 3.1148 (4.7036)  loss_n_40: 0.6448 (0.9772)  loss_n_60: 0.7273 (1.0825)  loss_n_80: 0.8455 (1.1914)  loss_n_100: 0.8863 (1.3095)  triple_100: 0.0000 (0.0494)  triple_80: 0.0000 (0.0530)  triple_60: 0.0000 (0.0169)  triple_40: 0.0000 (0.0237)  time: 3.9254  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 970/1724]  eta: 0:49:20  lr: 0.000180  loss: 3.0243 (4.6863)  loss_n_40: 0.6727 (0.9748)  loss_n_60: 0.7036 (1.0787)  loss_n_80: 0.7939 (1.1872)  loss_n_100: 0.7747 (1.3039)  triple_100: 0.0000 (0.0489)  triple_80: 0.0000 (0.0525)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0236)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 980/1724]  eta: 0:48:41  lr: 0.000180  loss: 2.6330 (4.6634)  loss_n_40: 0.6008 (0.9702)  loss_n_60: 0.6085 (1.0733)  loss_n_80: 0.6844 (1.1813)  loss_n_100: 0.7304 (1.2976)  triple_100: 0.0000 (0.0487)  triple_80: 0.0000 (0.0522)  triple_60: 0.0000 (0.0166)  triple_40: 0.0000 (0.0234)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [ 990/1724]  eta: 0:48:02  lr: 0.000180  loss: 2.6027 (4.6465)  loss_n_40: 0.5779 (0.9676)  loss_n_60: 0.5813 (1.0694)  loss_n_80: 0.6783 (1.1772)  loss_n_100: 0.7434 (1.2927)  triple_100: 0.0000 (0.0482)  triple_80: 0.0000 (0.0517)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0231)  time: 3.9263  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1000/1724]  eta: 0:47:22  lr: 0.000180  loss: 2.6422 (4.6251)  loss_n_40: 0.6004 (0.9639)  loss_n_60: 0.5900 (1.0644)  loss_n_80: 0.7151 (1.1721)  loss_n_100: 0.7460 (1.2866)  triple_100: 0.0000 (0.0477)  triple_80: 0.0000 (0.0512)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0229)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1010/1724]  eta: 0:46:43  lr: 0.000180  loss: 2.3485 (4.6024)  loss_n_40: 0.5602 (0.9601)  loss_n_60: 0.5292 (1.0592)  loss_n_80: 0.6156 (1.1665)  loss_n_100: 0.6374 (1.2798)  triple_100: 0.0000 (0.0473)  triple_80: 0.0000 (0.0507)  triple_60: 0.0000 (0.0161)  triple_40: 0.0000 (0.0227)  time: 3.9268  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1020/1724]  eta: 0:46:04  lr: 0.000180  loss: 2.2876 (4.5793)  loss_n_40: 0.5198 (0.9556)  loss_n_60: 0.5084 (1.0538)  loss_n_80: 0.6085 (1.1609)  loss_n_100: 0.5951 (1.2734)  triple_100: 0.0000 (0.0468)  triple_80: 0.0000 (0.0503)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0225)  time: 3.9266  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1030/1724]  eta: 0:45:25  lr: 0.000180  loss: 2.2424 (4.5567)  loss_n_40: 0.5150 (0.9513)  loss_n_60: 0.5169 (1.0486)  loss_n_80: 0.5955 (1.1553)  loss_n_100: 0.6172 (1.2670)  triple_100: 0.0000 (0.0464)  triple_80: 0.0000 (0.0498)  triple_60: 0.0000 (0.0160)  triple_40: 0.0000 (0.0222)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1040/1724]  eta: 0:44:45  lr: 0.000180  loss: 2.1797 (4.5339)  loss_n_40: 0.4936 (0.9469)  loss_n_60: 0.5102 (1.0433)  loss_n_80: 0.5600 (1.1496)  loss_n_100: 0.5958 (1.2606)  triple_100: 0.0000 (0.0460)  triple_80: 0.0000 (0.0493)  triple_60: 0.0000 (0.0158)  triple_40: 0.0000 (0.0223)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1050/1724]  eta: 0:44:06  lr: 0.000180  loss: 2.2275 (4.5126)  loss_n_40: 0.5026 (0.9437)  loss_n_60: 0.5305 (1.0386)  loss_n_80: 0.5556 (1.1441)  loss_n_100: 0.5735 (1.2541)  triple_100: 0.0000 (0.0456)  triple_80: 0.0000 (0.0488)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0221)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1060/1724]  eta: 0:43:27  lr: 0.000180  loss: 2.1649 (4.4886)  loss_n_40: 0.4719 (0.9391)  loss_n_60: 0.5220 (1.0330)  loss_n_80: 0.5132 (1.1380)  loss_n_100: 0.5488 (1.2474)  triple_100: 0.0000 (0.0453)  triple_80: 0.0000 (0.0484)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0219)  time: 3.9251  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1070/1724]  eta: 0:42:47  lr: 0.000180  loss: 1.9079 (4.4666)  loss_n_40: 0.4070 (0.9353)  loss_n_60: 0.4381 (1.0280)  loss_n_80: 0.4956 (1.1324)  loss_n_100: 0.5527 (1.2411)  triple_100: 0.0000 (0.0448)  triple_80: 0.0000 (0.0479)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0217)  time: 3.9251  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1080/1724]  eta: 0:42:08  lr: 0.000180  loss: 1.9279 (4.4435)  loss_n_40: 0.4470 (0.9312)  loss_n_60: 0.4437 (1.0228)  loss_n_80: 0.4971 (1.1265)  loss_n_100: 0.5474 (1.2343)  triple_100: 0.0000 (0.0444)  triple_80: 0.0000 (0.0475)  triple_60: 0.0000 (0.0153)  triple_40: 0.0000 (0.0215)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1090/1724]  eta: 0:41:29  lr: 0.000180  loss: 1.7460 (4.4204)  loss_n_40: 0.3884 (0.9265)  loss_n_60: 0.3899 (1.0174)  loss_n_80: 0.4585 (1.1208)  loss_n_100: 0.4999 (1.2279)  triple_100: 0.0000 (0.0440)  triple_80: 0.0000 (0.0474)  triple_60: 0.0000 (0.0151)  triple_40: 0.0000 (0.0213)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1100/1724]  eta: 0:40:50  lr: 0.000180  loss: 1.8881 (4.4078)  loss_n_40: 0.4484 (0.9235)  loss_n_60: 0.4218 (1.0136)  loss_n_80: 0.5117 (1.1167)  loss_n_100: 0.5276 (1.2233)  triple_100: 0.0000 (0.0451)  triple_80: 0.0000 (0.0479)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0219)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1110/1724]  eta: 0:40:10  lr: 0.000180  loss: 3.5971 (4.4198)  loss_n_40: 0.7765 (0.9242)  loss_n_60: 0.8307 (1.0142)  loss_n_80: 0.9386 (1.1168)  loss_n_100: 0.9297 (1.2227)  triple_100: 0.0000 (0.0451)  triple_80: 0.0000 (0.0548)  triple_60: 0.0000 (0.0202)  triple_40: 0.0000 (0.0218)  time: 3.9257  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1120/1724]  eta: 0:39:31  lr: 0.000180  loss: 4.1784 (4.4194)  loss_n_40: 0.9712 (0.9244)  loss_n_60: 1.0089 (1.0138)  loss_n_80: 1.1069 (1.1166)  loss_n_100: 1.1287 (1.2218)  triple_100: 0.0000 (0.0451)  triple_80: 0.0000 (0.0561)  triple_60: 0.0000 (0.0200)  triple_40: 0.0000 (0.0216)  time: 3.9247  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:10]  [1130/1724]  eta: 0:38:52  lr: 0.000180  loss: 3.6948 (4.4110)  loss_n_40: 0.8774 (0.9234)  loss_n_60: 0.8580 (1.0121)  loss_n_80: 1.0012 (1.1147)  loss_n_100: 1.0201 (1.2193)  triple_100: 0.0000 (0.0447)  triple_80: 0.0000 (0.0556)  triple_60: 0.0000 (0.0198)  triple_40: 0.0000 (0.0214)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1140/1724]  eta: 0:38:13  lr: 0.000180  loss: 3.4653 (4.4018)  loss_n_40: 0.7623 (0.9222)  loss_n_60: 0.7966 (1.0101)  loss_n_80: 0.9140 (1.1125)  loss_n_100: 0.9051 (1.2167)  triple_100: 0.0000 (0.0443)  triple_80: 0.0000 (0.0551)  triple_60: 0.0000 (0.0196)  triple_40: 0.0000 (0.0212)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1150/1724]  eta: 0:37:33  lr: 0.000180  loss: 2.9194 (4.3885)  loss_n_40: 0.6324 (0.9196)  loss_n_60: 0.6829 (1.0070)  loss_n_80: 0.7813 (1.1093)  loss_n_100: 0.8440 (1.2134)  triple_100: 0.0000 (0.0439)  triple_80: 0.0000 (0.0546)  triple_60: 0.0000 (0.0195)  triple_40: 0.0000 (0.0210)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1160/1724]  eta: 0:36:54  lr: 0.000180  loss: 2.7812 (4.3731)  loss_n_40: 0.6101 (0.9168)  loss_n_60: 0.6514 (1.0036)  loss_n_80: 0.7169 (1.1056)  loss_n_100: 0.8017 (1.2092)  triple_100: 0.0000 (0.0436)  triple_80: 0.0000 (0.0542)  triple_60: 0.0000 (0.0193)  triple_40: 0.0000 (0.0208)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1170/1724]  eta: 0:36:15  lr: 0.000180  loss: 2.6038 (4.3579)  loss_n_40: 0.5805 (0.9145)  loss_n_60: 0.6209 (1.0003)  loss_n_80: 0.6704 (1.1018)  loss_n_100: 0.7034 (1.2047)  triple_100: 0.0000 (0.0432)  triple_80: 0.0000 (0.0537)  triple_60: 0.0000 (0.0192)  triple_40: 0.0000 (0.0207)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1180/1724]  eta: 0:35:35  lr: 0.000180  loss: 2.3211 (4.3398)  loss_n_40: 0.5227 (0.9108)  loss_n_60: 0.5144 (0.9960)  loss_n_80: 0.6029 (1.0972)  loss_n_100: 0.6239 (1.1997)  triple_100: 0.0000 (0.0428)  triple_80: 0.0000 (0.0537)  triple_60: 0.0000 (0.0190)  triple_40: 0.0000 (0.0205)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1190/1724]  eta: 0:34:56  lr: 0.000180  loss: 2.3119 (4.3289)  loss_n_40: 0.5125 (0.9085)  loss_n_60: 0.5367 (0.9932)  loss_n_80: 0.5943 (1.0939)  loss_n_100: 0.6499 (1.1961)  triple_100: 0.0000 (0.0432)  triple_80: 0.0000 (0.0542)  triple_60: 0.0000 (0.0193)  triple_40: 0.0000 (0.0206)  time: 3.9259  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1200/1724]  eta: 0:34:17  lr: 0.000180  loss: 2.8731 (4.3176)  loss_n_40: 0.6006 (0.9064)  loss_n_60: 0.6509 (0.9904)  loss_n_80: 0.7057 (1.0913)  loss_n_100: 0.7743 (1.1934)  triple_100: 0.0000 (0.0428)  triple_80: 0.0000 (0.0537)  triple_60: 0.0000 (0.0191)  triple_40: 0.0000 (0.0204)  time: 3.9265  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1210/1724]  eta: 0:33:38  lr: 0.000180  loss: 2.9330 (4.3074)  loss_n_40: 0.6006 (0.9038)  loss_n_60: 0.6512 (0.9876)  loss_n_80: 0.7483 (1.0886)  loss_n_100: 0.8430 (1.1907)  triple_100: 0.0000 (0.0442)  triple_80: 0.0000 (0.0533)  triple_60: 0.0000 (0.0190)  triple_40: 0.0000 (0.0202)  time: 3.9258  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1220/1724]  eta: 0:32:58  lr: 0.000180  loss: 3.0831 (4.3038)  loss_n_40: 0.6852 (0.9043)  loss_n_60: 0.6723 (0.9863)  loss_n_80: 0.8068 (1.0877)  loss_n_100: 0.8862 (1.1891)  triple_100: 0.0000 (0.0438)  triple_80: 0.0000 (0.0533)  triple_60: 0.0000 (0.0189)  triple_40: 0.0000 (0.0203)  time: 3.9256  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1230/1724]  eta: 0:32:19  lr: 0.000180  loss: 4.3034 (4.3092)  loss_n_40: 0.9295 (0.9050)  loss_n_60: 0.9328 (0.9870)  loss_n_80: 1.0862 (1.0893)  loss_n_100: 1.1537 (1.1905)  triple_100: 0.0000 (0.0440)  triple_80: 0.0000 (0.0529)  triple_60: 0.0000 (0.0194)  triple_40: 0.0000 (0.0212)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1240/1724]  eta: 0:31:40  lr: 0.000180  loss: 4.3736 (4.3079)  loss_n_40: 0.9295 (0.9046)  loss_n_60: 0.9875 (0.9868)  loss_n_80: 1.1745 (1.0892)  loss_n_100: 1.2158 (1.1908)  triple_100: 0.0000 (0.0438)  triple_80: 0.0000 (0.0525)  triple_60: 0.0000 (0.0193)  triple_40: 0.0000 (0.0210)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1250/1724]  eta: 0:31:01  lr: 0.000180  loss: 4.0074 (4.3073)  loss_n_40: 0.7950 (0.9038)  loss_n_60: 0.9282 (0.9864)  loss_n_80: 1.0384 (1.0892)  loss_n_100: 1.1481 (1.1910)  triple_100: 0.0000 (0.0444)  triple_80: 0.0000 (0.0521)  triple_60: 0.0000 (0.0192)  triple_40: 0.0000 (0.0213)  time: 3.9260  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1260/1724]  eta: 0:30:21  lr: 0.000180  loss: 3.7468 (4.3020)  loss_n_40: 0.7950 (0.9032)  loss_n_60: 0.9422 (0.9856)  loss_n_80: 0.9508 (1.0878)  loss_n_100: 1.0143 (1.1895)  triple_100: 0.0000 (0.0440)  triple_80: 0.0000 (0.0516)  triple_60: 0.0000 (0.0191)  triple_40: 0.0000 (0.0211)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1270/1724]  eta: 0:29:42  lr: 0.000180  loss: 3.5109 (4.2951)  loss_n_40: 0.7818 (0.9022)  loss_n_60: 0.8354 (0.9842)  loss_n_80: 0.9072 (1.0862)  loss_n_100: 0.9501 (1.1878)  triple_100: 0.0000 (0.0437)  triple_80: 0.0000 (0.0512)  triple_60: 0.0000 (0.0189)  triple_40: 0.0000 (0.0209)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1280/1724]  eta: 0:29:03  lr: 0.000180  loss: 3.1878 (4.2850)  loss_n_40: 0.6612 (0.9006)  loss_n_60: 0.7604 (0.9821)  loss_n_80: 0.7944 (1.0836)  loss_n_100: 0.9205 (1.1849)  triple_100: 0.0000 (0.0433)  triple_80: 0.0000 (0.0508)  triple_60: 0.0000 (0.0188)  triple_40: 0.0000 (0.0208)  time: 3.9246  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1290/1724]  eta: 0:28:24  lr: 0.000180  loss: 3.0593 (4.2748)  loss_n_40: 0.6442 (0.8988)  loss_n_60: 0.6811 (0.9798)  loss_n_80: 0.7247 (1.0809)  loss_n_100: 0.7716 (1.1820)  triple_100: 0.0000 (0.0435)  triple_80: 0.0000 (0.0506)  triple_60: 0.0000 (0.0186)  triple_40: 0.0000 (0.0206)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1300/1724]  eta: 0:27:44  lr: 0.000180  loss: 3.1238 (4.2663)  loss_n_40: 0.6645 (0.8972)  loss_n_60: 0.7299 (0.9782)  loss_n_80: 0.7874 (1.0789)  loss_n_100: 0.8055 (1.1797)  triple_100: 0.0000 (0.0431)  triple_80: 0.0000 (0.0502)  triple_60: 0.0000 (0.0185)  triple_40: 0.0000 (0.0204)  time: 3.9251  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1310/1724]  eta: 0:27:05  lr: 0.000180  loss: 2.9924 (4.2564)  loss_n_40: 0.6564 (0.8955)  loss_n_60: 0.7086 (0.9762)  loss_n_80: 0.7772 (1.0765)  loss_n_100: 0.8248 (1.1770)  triple_100: 0.0000 (0.0428)  triple_80: 0.0000 (0.0499)  triple_60: 0.0000 (0.0183)  triple_40: 0.0000 (0.0203)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1320/1724]  eta: 0:26:26  lr: 0.000180  loss: 2.7136 (4.2436)  loss_n_40: 0.5814 (0.8932)  loss_n_60: 0.6577 (0.9735)  loss_n_80: 0.7144 (1.0733)  loss_n_100: 0.7508 (1.1734)  triple_100: 0.0000 (0.0425)  triple_80: 0.0000 (0.0495)  triple_60: 0.0000 (0.0182)  triple_40: 0.0000 (0.0201)  time: 3.9275  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1330/1724]  eta: 0:25:47  lr: 0.000180  loss: 2.4239 (4.2291)  loss_n_40: 0.5294 (0.8904)  loss_n_60: 0.5879 (0.9703)  loss_n_80: 0.6193 (1.0696)  loss_n_100: 0.6963 (1.1694)  triple_100: 0.0000 (0.0422)  triple_80: 0.0000 (0.0491)  triple_60: 0.0000 (0.0181)  triple_40: 0.0000 (0.0200)  time: 3.9272  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1340/1724]  eta: 0:25:07  lr: 0.000180  loss: 2.3248 (4.2161)  loss_n_40: 0.4768 (0.8876)  loss_n_60: 0.5713 (0.9674)  loss_n_80: 0.6044 (1.0663)  loss_n_100: 0.6608 (1.1658)  triple_100: 0.0000 (0.0423)  triple_80: 0.0000 (0.0488)  triple_60: 0.0000 (0.0181)  triple_40: 0.0000 (0.0198)  time: 3.9270  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1350/1724]  eta: 0:24:28  lr: 0.000180  loss: 2.3248 (4.2025)  loss_n_40: 0.5134 (0.8853)  loss_n_60: 0.5713 (0.9645)  loss_n_80: 0.5860 (1.0627)  loss_n_100: 0.6385 (1.1617)  triple_100: 0.0000 (0.0420)  triple_80: 0.0000 (0.0485)  triple_60: 0.0000 (0.0180)  triple_40: 0.0000 (0.0197)  time: 3.9267  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1360/1724]  eta: 0:23:49  lr: 0.000180  loss: 2.2424 (4.1889)  loss_n_40: 0.5256 (0.8829)  loss_n_60: 0.5456 (0.9615)  loss_n_80: 0.5734 (1.0592)  loss_n_100: 0.6074 (1.1578)  triple_100: 0.0000 (0.0419)  triple_80: 0.0000 (0.0481)  triple_60: 0.0000 (0.0179)  triple_40: 0.0000 (0.0195)  time: 3.9261  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:10]  [1370/1724]  eta: 0:23:09  lr: 0.000180  loss: 2.2308 (4.1751)  loss_n_40: 0.5138 (0.8806)  loss_n_60: 0.5428 (0.9585)  loss_n_80: 0.5789 (1.0557)  loss_n_100: 0.6146 (1.1538)  triple_100: 0.0000 (0.0416)  triple_80: 0.0000 (0.0478)  triple_60: 0.0000 (0.0177)  triple_40: 0.0000 (0.0194)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1380/1724]  eta: 0:22:30  lr: 0.000180  loss: 2.1738 (4.1627)  loss_n_40: 0.4920 (0.8783)  loss_n_60: 0.5170 (0.9556)  loss_n_80: 0.5585 (1.0521)  loss_n_100: 0.5929 (1.1498)  triple_100: 0.0000 (0.0419)  triple_80: 0.0000 (0.0479)  triple_60: 0.0000 (0.0179)  triple_40: 0.0000 (0.0193)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1390/1724]  eta: 0:21:51  lr: 0.000180  loss: 2.9355 (4.1614)  loss_n_40: 0.5326 (0.8769)  loss_n_60: 0.6581 (0.9550)  loss_n_80: 0.6446 (1.0518)  loss_n_100: 0.7013 (1.1495)  triple_100: 0.0000 (0.0428)  triple_80: 0.0000 (0.0485)  triple_60: 0.0000 (0.0178)  triple_40: 0.0000 (0.0191)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1400/1724]  eta: 0:21:12  lr: 0.000180  loss: 4.6831 (4.2289)  loss_n_40: 0.9030 (0.8781)  loss_n_60: 1.0890 (0.9573)  loss_n_80: 1.2714 (1.0551)  loss_n_100: 1.3819 (1.1538)  triple_100: 0.0000 (0.0756)  triple_80: 0.0000 (0.0676)  triple_60: 0.0000 (0.0224)  triple_40: 0.0000 (0.0190)  time: 3.9231  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1410/1724]  eta: 0:20:32  lr: 0.000180  loss: 7.1239 (4.2692)  loss_n_40: 1.2857 (0.8822)  loss_n_60: 1.5537 (0.9624)  loss_n_80: 1.8556 (1.0615)  loss_n_100: 2.0516 (1.1612)  triple_100: 0.0000 (0.0842)  triple_80: 0.0000 (0.0756)  triple_60: 0.0000 (0.0234)  triple_40: 0.0000 (0.0188)  time: 3.9231  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1420/1724]  eta: 0:19:53  lr: 0.000180  loss: 7.9364 (4.3030)  loss_n_40: 1.5357 (0.8875)  loss_n_60: 1.6936 (0.9701)  loss_n_80: 2.0433 (1.0703)  loss_n_100: 2.2542 (1.1710)  triple_100: 0.0000 (0.0871)  triple_80: 0.0000 (0.0750)  triple_60: 0.0000 (0.0232)  triple_40: 0.0000 (0.0187)  time: 3.9238  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1430/1724]  eta: 0:19:14  lr: 0.000180  loss: 8.8168 (4.3340)  loss_n_40: 1.6437 (0.8930)  loss_n_60: 2.1675 (0.9787)  loss_n_80: 2.3989 (1.0796)  loss_n_100: 2.4719 (1.1800)  triple_100: 0.0000 (0.0865)  triple_80: 0.0000 (0.0745)  triple_60: 0.0000 (0.0230)  triple_40: 0.0000 (0.0186)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1440/1724]  eta: 0:18:35  lr: 0.000180  loss: 8.3901 (4.3596)  loss_n_40: 1.5634 (0.8975)  loss_n_60: 2.0772 (0.9856)  loss_n_80: 2.3290 (1.0873)  loss_n_100: 2.3977 (1.1881)  triple_100: 0.0000 (0.0859)  triple_80: 0.0000 (0.0740)  triple_60: 0.0000 (0.0229)  triple_40: 0.0000 (0.0185)  time: 3.9221  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1450/1724]  eta: 0:17:55  lr: 0.000180  loss: 7.7009 (4.3807)  loss_n_40: 1.4029 (0.9007)  loss_n_60: 1.8862 (0.9908)  loss_n_80: 2.1554 (1.0940)  loss_n_100: 2.3857 (1.1954)  triple_100: 0.0000 (0.0853)  triple_80: 0.0000 (0.0735)  triple_60: 0.0000 (0.0227)  triple_40: 0.0000 (0.0183)  time: 3.9229  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1460/1724]  eta: 0:17:16  lr: 0.000180  loss: 6.9081 (4.3966)  loss_n_40: 1.2368 (0.9023)  loss_n_60: 1.6145 (0.9944)  loss_n_80: 1.9624 (1.0998)  loss_n_100: 2.1746 (1.2016)  triple_100: 0.0000 (0.0847)  triple_80: 0.0000 (0.0730)  triple_60: 0.0000 (0.0226)  triple_40: 0.0000 (0.0182)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1470/1724]  eta: 0:16:37  lr: 0.000180  loss: 5.9661 (4.4048)  loss_n_40: 1.0690 (0.9034)  loss_n_60: 1.3309 (0.9962)  loss_n_80: 1.6457 (1.1030)  loss_n_100: 1.8581 (1.2052)  triple_100: 0.0000 (0.0841)  triple_80: 0.0000 (0.0725)  triple_60: 0.0000 (0.0224)  triple_40: 0.0000 (0.0181)  time: 3.9219  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1480/1724]  eta: 0:15:57  lr: 0.000180  loss: 4.9376 (4.4083)  loss_n_40: 1.0086 (0.9041)  loss_n_60: 1.1521 (0.9971)  loss_n_80: 1.3819 (1.1047)  loss_n_100: 1.4768 (1.2067)  triple_100: 0.0000 (0.0836)  triple_80: 0.0000 (0.0720)  triple_60: 0.0000 (0.0222)  triple_40: 0.0000 (0.0180)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1490/1724]  eta: 0:15:18  lr: 0.000180  loss: 4.6965 (4.4100)  loss_n_40: 0.9799 (0.9050)  loss_n_60: 1.1083 (0.9977)  loss_n_80: 1.2406 (1.1055)  loss_n_100: 1.3176 (1.2072)  triple_100: 0.0000 (0.0830)  triple_80: 0.0000 (0.0715)  triple_60: 0.0000 (0.0221)  triple_40: 0.0000 (0.0180)  time: 3.9219  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1500/1724]  eta: 0:14:39  lr: 0.000180  loss: 4.4231 (4.4076)  loss_n_40: 0.9696 (0.9051)  loss_n_60: 1.0307 (0.9975)  loss_n_80: 1.1747 (1.1052)  loss_n_100: 1.1679 (1.2065)  triple_100: 0.0000 (0.0824)  triple_80: 0.0000 (0.0710)  triple_60: 0.0000 (0.0220)  triple_40: 0.0000 (0.0178)  time: 3.9223  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1510/1724]  eta: 0:14:00  lr: 0.000180  loss: 3.9591 (4.4049)  loss_n_40: 0.8999 (0.9053)  loss_n_60: 0.9569 (0.9973)  loss_n_80: 1.0347 (1.1046)  loss_n_100: 1.0578 (1.2057)  triple_100: 0.0000 (0.0819)  triple_80: 0.0000 (0.0706)  triple_60: 0.0000 (0.0218)  triple_40: 0.0000 (0.0177)  time: 3.9221  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1520/1724]  eta: 0:13:20  lr: 0.000180  loss: 3.6144 (4.3991)  loss_n_40: 0.7866 (0.9045)  loss_n_60: 0.8931 (0.9964)  loss_n_80: 0.9406 (1.1034)  loss_n_100: 1.0218 (1.2041)  triple_100: 0.0000 (0.0814)  triple_80: 0.0000 (0.0701)  triple_60: 0.0000 (0.0217)  triple_40: 0.0000 (0.0176)  time: 3.9226  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1530/1724]  eta: 0:12:41  lr: 0.000180  loss: 3.4660 (4.3931)  loss_n_40: 0.7770 (0.9038)  loss_n_60: 0.8345 (0.9954)  loss_n_80: 0.9050 (1.1021)  loss_n_100: 0.9250 (1.2024)  triple_100: 0.0000 (0.0808)  triple_80: 0.0000 (0.0696)  triple_60: 0.0000 (0.0215)  triple_40: 0.0000 (0.0175)  time: 3.9234  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1540/1724]  eta: 0:12:02  lr: 0.000180  loss: 3.2423 (4.3858)  loss_n_40: 0.6995 (0.9030)  loss_n_60: 0.7995 (0.9941)  loss_n_80: 0.8353 (1.1004)  loss_n_100: 0.8619 (1.2002)  triple_100: 0.0000 (0.0803)  triple_80: 0.0000 (0.0692)  triple_60: 0.0000 (0.0214)  triple_40: 0.0000 (0.0174)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1550/1724]  eta: 0:11:23  lr: 0.000180  loss: 3.0249 (4.3772)  loss_n_40: 0.6905 (0.9019)  loss_n_60: 0.7275 (0.9924)  loss_n_80: 0.7817 (1.0982)  loss_n_100: 0.8196 (1.1977)  triple_100: 0.0000 (0.0798)  triple_80: 0.0000 (0.0687)  triple_60: 0.0000 (0.0212)  triple_40: 0.0000 (0.0173)  time: 3.9227  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1560/1724]  eta: 0:10:43  lr: 0.000180  loss: 2.9013 (4.3675)  loss_n_40: 0.6571 (0.9006)  loss_n_60: 0.6889 (0.9904)  loss_n_80: 0.7540 (1.0959)  loss_n_100: 0.7810 (1.1949)  triple_100: 0.0000 (0.0793)  triple_80: 0.0000 (0.0683)  triple_60: 0.0000 (0.0211)  triple_40: 0.0000 (0.0172)  time: 3.9217  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1570/1724]  eta: 0:10:04  lr: 0.000180  loss: 2.8714 (4.3573)  loss_n_40: 0.6598 (0.8993)  loss_n_60: 0.6385 (0.9882)  loss_n_80: 0.6997 (1.0933)  loss_n_100: 0.7384 (1.1918)  triple_100: 0.0000 (0.0788)  triple_80: 0.0000 (0.0679)  triple_60: 0.0000 (0.0210)  triple_40: 0.0000 (0.0171)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1580/1724]  eta: 0:09:25  lr: 0.000180  loss: 2.6831 (4.3464)  loss_n_40: 0.6264 (0.8975)  loss_n_60: 0.6357 (0.9860)  loss_n_80: 0.6933 (1.0907)  loss_n_100: 0.6920 (1.1887)  triple_100: 0.0000 (0.0783)  triple_80: 0.0000 (0.0674)  triple_60: 0.0000 (0.0208)  triple_40: 0.0000 (0.0169)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1590/1724]  eta: 0:08:46  lr: 0.000180  loss: 2.5537 (4.3355)  loss_n_40: 0.5879 (0.8960)  loss_n_60: 0.6308 (0.9838)  loss_n_80: 0.6619 (1.0878)  loss_n_100: 0.6671 (1.1854)  triple_100: 0.0000 (0.0778)  triple_80: 0.0000 (0.0670)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0169)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1600/1724]  eta: 0:08:06  lr: 0.000180  loss: 2.5728 (4.3242)  loss_n_40: 0.5654 (0.8942)  loss_n_60: 0.6278 (0.9815)  loss_n_80: 0.6369 (1.0851)  loss_n_100: 0.6645 (1.1822)  triple_100: 0.0000 (0.0773)  triple_80: 0.0000 (0.0666)  triple_60: 0.0000 (0.0206)  triple_40: 0.0000 (0.0168)  time: 3.9220  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:10]  [1610/1724]  eta: 0:07:27  lr: 0.000180  loss: 2.3649 (4.3122)  loss_n_40: 0.5454 (0.8923)  loss_n_60: 0.5659 (0.9789)  loss_n_80: 0.6033 (1.0821)  loss_n_100: 0.6509 (1.1788)  triple_100: 0.0000 (0.0768)  triple_80: 0.0000 (0.0662)  triple_60: 0.0000 (0.0205)  triple_40: 0.0000 (0.0167)  time: 3.9222  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1620/1724]  eta: 0:06:48  lr: 0.000180  loss: 2.3040 (4.2995)  loss_n_40: 0.5109 (0.8904)  loss_n_60: 0.5455 (0.9762)  loss_n_80: 0.5702 (1.0789)  loss_n_100: 0.5866 (1.1750)  triple_100: 0.0000 (0.0763)  triple_80: 0.0000 (0.0658)  triple_60: 0.0000 (0.0203)  triple_40: 0.0000 (0.0166)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1630/1724]  eta: 0:06:09  lr: 0.000180  loss: 2.2154 (4.2890)  loss_n_40: 0.5298 (0.8886)  loss_n_60: 0.5376 (0.9739)  loss_n_80: 0.5602 (1.0761)  loss_n_100: 0.5805 (1.1718)  triple_100: 0.0000 (0.0759)  triple_80: 0.0000 (0.0654)  triple_60: 0.0000 (0.0202)  triple_40: 0.0000 (0.0171)  time: 3.9226  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1640/1724]  eta: 0:05:29  lr: 0.000180  loss: 3.0015 (4.2949)  loss_n_40: 0.6418 (0.8876)  loss_n_60: 0.6777 (0.9737)  loss_n_80: 0.7322 (1.0769)  loss_n_100: 0.7585 (1.1733)  triple_100: 0.0000 (0.0757)  triple_80: 0.0000 (0.0668)  triple_60: 0.0000 (0.0227)  triple_40: 0.0000 (0.0182)  time: 3.9228  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1650/1724]  eta: 0:04:50  lr: 0.000180  loss: 5.3499 (4.3008)  loss_n_40: 0.8490 (0.8880)  loss_n_60: 1.1015 (0.9751)  loss_n_80: 1.3988 (1.0792)  loss_n_100: 1.6306 (1.1763)  triple_100: 0.0000 (0.0752)  triple_80: 0.0000 (0.0664)  triple_60: 0.0000 (0.0226)  triple_40: 0.0000 (0.0181)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1660/1724]  eta: 0:04:11  lr: 0.000180  loss: 4.9435 (4.3012)  loss_n_40: 0.8490 (0.8874)  loss_n_60: 1.0593 (0.9751)  loss_n_80: 1.3138 (1.0798)  loss_n_100: 1.5346 (1.1777)  triple_100: 0.0000 (0.0748)  triple_80: 0.0000 (0.0660)  triple_60: 0.0000 (0.0224)  triple_40: 0.0000 (0.0180)  time: 3.9219  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1670/1724]  eta: 0:03:31  lr: 0.000180  loss: 4.0510 (4.2993)  loss_n_40: 0.7079 (0.8864)  loss_n_60: 0.8994 (0.9746)  loss_n_80: 1.0989 (1.0799)  loss_n_100: 1.3331 (1.1784)  triple_100: 0.0000 (0.0743)  triple_80: 0.0000 (0.0656)  triple_60: 0.0000 (0.0223)  triple_40: 0.0000 (0.0179)  time: 3.9221  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1680/1724]  eta: 0:02:52  lr: 0.000180  loss: 3.7000 (4.2939)  loss_n_40: 0.6512 (0.8851)  loss_n_60: 0.8182 (0.9734)  loss_n_80: 1.0170 (1.0789)  loss_n_100: 1.2101 (1.1775)  triple_100: 0.0000 (0.0739)  triple_80: 0.0000 (0.0652)  triple_60: 0.0000 (0.0222)  triple_40: 0.0000 (0.0178)  time: 3.9229  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1690/1724]  eta: 0:02:13  lr: 0.000180  loss: 3.2006 (4.2860)  loss_n_40: 0.6272 (0.8839)  loss_n_60: 0.7266 (0.9718)  loss_n_80: 0.8123 (1.0769)  loss_n_100: 0.8896 (1.1754)  triple_100: 0.0000 (0.0735)  triple_80: 0.0000 (0.0648)  triple_60: 0.0000 (0.0220)  triple_40: 0.0000 (0.0177)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1700/1724]  eta: 0:01:34  lr: 0.000180  loss: 2.7960 (4.2770)  loss_n_40: 0.5572 (0.8825)  loss_n_60: 0.6694 (0.9701)  loss_n_80: 0.7037 (1.0746)  loss_n_100: 0.7621 (1.1728)  triple_100: 0.0000 (0.0730)  triple_80: 0.0000 (0.0644)  triple_60: 0.0000 (0.0219)  triple_40: 0.0000 (0.0176)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:10]  [1710/1724]  eta: 0:00:54  lr: 0.000180  loss: 2.5899 (4.2678)  loss_n_40: 0.5951 (0.8817)  loss_n_60: 0.6338 (0.9682)  loss_n_80: 0.6363 (1.0722)  loss_n_100: 0.6832 (1.1698)  triple_100: 0.0000 (0.0726)  triple_80: 0.0000 (0.0641)  triple_60: 0.0000 (0.0218)  triple_40: 0.0000 (0.0175)  time: 3.9233  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1720/1724]  eta: 0:00:15  lr: 0.000180  loss: 2.4846 (4.2564)  loss_n_40: 0.5643 (0.8797)  loss_n_60: 0.5927 (0.9658)  loss_n_80: 0.6325 (1.0694)  loss_n_100: 0.6527 (1.1666)  triple_100: 0.0000 (0.0722)  triple_80: 0.0000 (0.0637)  triple_60: 0.0000 (0.0216)  triple_40: 0.0000 (0.0174)  time: 3.9230  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10]  [1723/1724]  eta: 0:00:03  lr: 0.000180  loss: 2.3662 (4.2525)  loss_n_40: 0.5197 (0.8789)  loss_n_60: 0.5453 (0.9649)  loss_n_80: 0.6182 (1.0684)  loss_n_100: 0.6164 (1.1656)  triple_100: 0.0000 (0.0721)  triple_80: 0.0000 (0.0636)  triple_60: 0.0000 (0.0216)  triple_40: 0.0000 (0.0174)  time: 3.9232  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:10] Total time: 1:52:48 (3.9258 s / it)\n",
      "Averaged stats: lr: 0.000180  loss: 2.3662 (4.2525)  loss_n_40: 0.5197 (0.8789)  loss_n_60: 0.5453 (0.9649)  loss_n_80: 0.6182 (1.0684)  loss_n_100: 0.6164 (1.1656)  triple_100: 0.0000 (0.0721)  triple_80: 0.0000 (0.0636)  triple_60: 0.0000 (0.0216)  triple_40: 0.0000 (0.0174)\n",
      "Valid: [epoch:10]  [  0/845]  eta: 0:09:43  loss: 2.2333 (2.2333)  loss_n_40: 0.3968 (0.3968)  loss_n_60: 0.5548 (0.5548)  loss_n_80: 0.6448 (0.6448)  loss_n_100: 0.6369 (0.6369)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.6906  data: 0.3528  max mem: 46473\n",
      "Valid: [epoch:10]  [ 10/845]  eta: 0:05:05  loss: 2.2301 (2.1442)  loss_n_40: 0.4685 (0.4768)  loss_n_60: 0.5033 (0.5038)  loss_n_80: 0.5928 (0.5557)  loss_n_100: 0.6205 (0.6079)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3664  data: 0.0322  max mem: 46473\n",
      "Valid: [epoch:10]  [ 20/845]  eta: 0:04:49  loss: 2.1627 (2.1396)  loss_n_40: 0.4381 (0.4820)  loss_n_60: 0.4693 (0.4988)  loss_n_80: 0.5770 (0.5525)  loss_n_100: 0.5705 (0.6063)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [ 30/845]  eta: 0:04:41  loss: 2.0405 (2.1562)  loss_n_40: 0.4381 (0.5121)  loss_n_60: 0.4500 (0.5011)  loss_n_80: 0.5231 (0.5502)  loss_n_100: 0.5524 (0.5875)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0053)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [ 40/845]  eta: 0:04:36  loss: 2.1563 (2.2006)  loss_n_40: 0.4923 (0.5301)  loss_n_60: 0.4795 (0.5116)  loss_n_80: 0.5670 (0.5598)  loss_n_100: 0.5576 (0.5900)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0091)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [ 50/845]  eta: 0:04:31  loss: 2.1000 (2.1514)  loss_n_40: 0.4791 (0.5131)  loss_n_60: 0.4598 (0.4993)  loss_n_80: 0.5670 (0.5514)  loss_n_100: 0.5386 (0.5802)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0073)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [ 60/845]  eta: 0:04:27  loss: 1.8726 (2.1282)  loss_n_40: 0.4139 (0.5020)  loss_n_60: 0.4373 (0.4984)  loss_n_80: 0.4823 (0.5452)  loss_n_100: 0.5386 (0.5764)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0061)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [ 70/845]  eta: 0:04:22  loss: 2.2718 (2.1785)  loss_n_40: 0.4752 (0.5199)  loss_n_60: 0.5375 (0.5093)  loss_n_80: 0.5300 (0.5553)  loss_n_100: 0.6404 (0.5888)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0053)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [ 80/845]  eta: 0:04:19  loss: 2.2718 (2.1719)  loss_n_40: 0.4752 (0.5263)  loss_n_60: 0.5295 (0.5107)  loss_n_80: 0.5300 (0.5496)  loss_n_100: 0.6256 (0.5807)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0046)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [ 90/845]  eta: 0:04:15  loss: 1.8704 (2.1753)  loss_n_40: 0.4088 (0.5350)  loss_n_60: 0.4249 (0.5132)  loss_n_80: 0.4668 (0.5462)  loss_n_100: 0.4902 (0.5768)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0041)  time: 0.3347  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:10]  [100/845]  eta: 0:04:11  loss: 1.9529 (2.1683)  loss_n_40: 0.4516 (0.5334)  loss_n_60: 0.4178 (0.5096)  loss_n_80: 0.5247 (0.5448)  loss_n_100: 0.5064 (0.5722)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0084)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [110/845]  eta: 0:04:08  loss: 2.0992 (2.1748)  loss_n_40: 0.4500 (0.5420)  loss_n_60: 0.4542 (0.5115)  loss_n_80: 0.5491 (0.5434)  loss_n_100: 0.5104 (0.5702)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0076)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [120/845]  eta: 0:04:04  loss: 1.9771 (2.1529)  loss_n_40: 0.4187 (0.5317)  loss_n_60: 0.4264 (0.5072)  loss_n_80: 0.5134 (0.5389)  loss_n_100: 0.5418 (0.5682)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0070)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [130/845]  eta: 0:04:01  loss: 1.5483 (2.1138)  loss_n_40: 0.3559 (0.5184)  loss_n_60: 0.3811 (0.4983)  loss_n_80: 0.4007 (0.5296)  loss_n_100: 0.4566 (0.5610)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0064)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [140/845]  eta: 0:03:57  loss: 1.6710 (2.1138)  loss_n_40: 0.3632 (0.5253)  loss_n_60: 0.3908 (0.4980)  loss_n_80: 0.4281 (0.5276)  loss_n_100: 0.4750 (0.5570)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0060)  time: 0.3349  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [150/845]  eta: 0:03:54  loss: 1.9131 (2.1084)  loss_n_40: 0.4481 (0.5207)  loss_n_60: 0.4235 (0.4976)  loss_n_80: 0.5181 (0.5266)  loss_n_100: 0.5075 (0.5580)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0056)  time: 0.3352  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [160/845]  eta: 0:03:50  loss: 1.8416 (2.0967)  loss_n_40: 0.4342 (0.5142)  loss_n_60: 0.4207 (0.4958)  loss_n_80: 0.4948 (0.5243)  loss_n_100: 0.5299 (0.5572)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0052)  time: 0.3349  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [170/845]  eta: 0:03:47  loss: 2.0292 (2.1562)  loss_n_40: 0.4344 (0.5252)  loss_n_60: 0.4416 (0.4989)  loss_n_80: 0.5523 (0.5289)  loss_n_100: 0.5320 (0.5583)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0450)  time: 0.3348  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [180/845]  eta: 0:03:43  loss: 2.1235 (2.1508)  loss_n_40: 0.4888 (0.5218)  loss_n_60: 0.4562 (0.4978)  loss_n_80: 0.5562 (0.5297)  loss_n_100: 0.5374 (0.5591)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0425)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [190/845]  eta: 0:03:40  loss: 1.9521 (2.1517)  loss_n_40: 0.4680 (0.5202)  loss_n_60: 0.4198 (0.4986)  loss_n_80: 0.5398 (0.5311)  loss_n_100: 0.5285 (0.5615)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0403)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [200/845]  eta: 0:03:36  loss: 2.1470 (2.1571)  loss_n_40: 0.4612 (0.5226)  loss_n_60: 0.4578 (0.4993)  loss_n_80: 0.5722 (0.5330)  loss_n_100: 0.5241 (0.5640)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0383)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [210/845]  eta: 0:03:33  loss: 2.1848 (2.1532)  loss_n_40: 0.4612 (0.5198)  loss_n_60: 0.5022 (0.4996)  loss_n_80: 0.5599 (0.5332)  loss_n_100: 0.5804 (0.5641)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0364)  time: 0.3347  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [220/845]  eta: 0:03:30  loss: 2.1143 (2.1742)  loss_n_40: 0.4433 (0.5323)  loss_n_60: 0.4710 (0.5049)  loss_n_80: 0.5597 (0.5370)  loss_n_100: 0.5646 (0.5651)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0348)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [230/845]  eta: 0:03:26  loss: 2.1143 (2.1928)  loss_n_40: 0.4892 (0.5381)  loss_n_60: 0.4559 (0.5065)  loss_n_80: 0.5597 (0.5378)  loss_n_100: 0.5538 (0.5648)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0456)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [240/845]  eta: 0:03:23  loss: 2.1694 (2.1880)  loss_n_40: 0.4624 (0.5352)  loss_n_60: 0.4973 (0.5055)  loss_n_80: 0.5617 (0.5382)  loss_n_100: 0.5911 (0.5653)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0437)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [250/845]  eta: 0:03:19  loss: 1.9305 (2.1984)  loss_n_40: 0.4322 (0.5338)  loss_n_60: 0.4528 (0.5048)  loss_n_80: 0.5559 (0.5392)  loss_n_100: 0.5714 (0.5669)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0537)  time: 0.3349  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [260/845]  eta: 0:03:16  loss: 1.9305 (2.2041)  loss_n_40: 0.4509 (0.5364)  loss_n_60: 0.4541 (0.5065)  loss_n_80: 0.5559 (0.5410)  loss_n_100: 0.5476 (0.5685)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0516)  time: 0.3350  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [270/845]  eta: 0:03:13  loss: 2.0265 (2.2225)  loss_n_40: 0.4509 (0.5368)  loss_n_60: 0.4541 (0.5067)  loss_n_80: 0.5227 (0.5410)  loss_n_100: 0.5394 (0.5685)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0620)  time: 0.3349  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [280/845]  eta: 0:03:09  loss: 1.8788 (2.2197)  loss_n_40: 0.4459 (0.5408)  loss_n_60: 0.4095 (0.5064)  loss_n_80: 0.4977 (0.5399)  loss_n_100: 0.4584 (0.5655)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0598)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [290/845]  eta: 0:03:06  loss: 2.2932 (2.2434)  loss_n_40: 0.5342 (0.5469)  loss_n_60: 0.5029 (0.5098)  loss_n_80: 0.5705 (0.5415)  loss_n_100: 0.5790 (0.5671)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0709)  time: 0.3348  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [300/845]  eta: 0:03:03  loss: 2.3749 (2.2395)  loss_n_40: 0.4933 (0.5442)  loss_n_60: 0.5091 (0.5097)  loss_n_80: 0.5705 (0.5418)  loss_n_100: 0.6178 (0.5683)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0686)  time: 0.3349  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [310/845]  eta: 0:02:59  loss: 2.0959 (2.2423)  loss_n_40: 0.4521 (0.5431)  loss_n_60: 0.4740 (0.5102)  loss_n_80: 0.5667 (0.5428)  loss_n_100: 0.6103 (0.5701)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0695)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [320/845]  eta: 0:02:56  loss: 2.3861 (2.2478)  loss_n_40: 0.4543 (0.5424)  loss_n_60: 0.5625 (0.5110)  loss_n_80: 0.5919 (0.5442)  loss_n_100: 0.6366 (0.5716)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0722)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [330/845]  eta: 0:02:52  loss: 2.3036 (2.2432)  loss_n_40: 0.4766 (0.5429)  loss_n_60: 0.5328 (0.5107)  loss_n_80: 0.5727 (0.5432)  loss_n_100: 0.5168 (0.5702)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0700)  time: 0.3346  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [340/845]  eta: 0:02:49  loss: 2.0483 (2.2546)  loss_n_40: 0.4582 (0.5442)  loss_n_60: 0.4505 (0.5116)  loss_n_80: 0.5241 (0.5432)  loss_n_100: 0.4982 (0.5708)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0788)  time: 0.3345  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:10]  [350/845]  eta: 0:02:46  loss: 2.0810 (2.2557)  loss_n_40: 0.4117 (0.5416)  loss_n_60: 0.4903 (0.5114)  loss_n_80: 0.5241 (0.5423)  loss_n_100: 0.5806 (0.5708)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0821)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [360/845]  eta: 0:02:42  loss: 1.9113 (2.2576)  loss_n_40: 0.4037 (0.5450)  loss_n_60: 0.4381 (0.5130)  loss_n_80: 0.5278 (0.5429)  loss_n_100: 0.5123 (0.5696)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0799)  time: 0.3351  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [370/845]  eta: 0:02:39  loss: 1.9113 (2.2480)  loss_n_40: 0.4106 (0.5412)  loss_n_60: 0.4029 (0.5110)  loss_n_80: 0.5315 (0.5421)  loss_n_100: 0.5123 (0.5689)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0777)  time: 0.3350  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [380/845]  eta: 0:02:36  loss: 1.9726 (2.2542)  loss_n_40: 0.4148 (0.5405)  loss_n_60: 0.4465 (0.5124)  loss_n_80: 0.5839 (0.5440)  loss_n_100: 0.5777 (0.5716)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0788)  time: 0.3350  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [390/845]  eta: 0:02:32  loss: 2.3262 (2.2516)  loss_n_40: 0.4790 (0.5388)  loss_n_60: 0.5342 (0.5123)  loss_n_80: 0.6189 (0.5446)  loss_n_100: 0.6492 (0.5724)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0768)  time: 0.3350  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [400/845]  eta: 0:02:29  loss: 2.2592 (2.2521)  loss_n_40: 0.4840 (0.5381)  loss_n_60: 0.4981 (0.5129)  loss_n_80: 0.6162 (0.5457)  loss_n_100: 0.6492 (0.5739)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0749)  time: 0.3352  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [410/845]  eta: 0:02:25  loss: 2.2493 (2.2464)  loss_n_40: 0.4952 (0.5365)  loss_n_60: 0.5108 (0.5119)  loss_n_80: 0.5994 (0.5451)  loss_n_100: 0.6418 (0.5735)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0731)  time: 0.3354  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [420/845]  eta: 0:02:22  loss: 2.1450 (2.2559)  loss_n_40: 0.4968 (0.5400)  loss_n_60: 0.5108 (0.5135)  loss_n_80: 0.5923 (0.5479)  loss_n_100: 0.6018 (0.5749)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0733)  time: 0.3350  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [430/845]  eta: 0:02:19  loss: 2.3100 (2.2602)  loss_n_40: 0.5170 (0.5408)  loss_n_60: 0.5494 (0.5156)  loss_n_80: 0.6479 (0.5497)  loss_n_100: 0.6181 (0.5764)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0716)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [440/845]  eta: 0:02:15  loss: 2.0485 (2.2527)  loss_n_40: 0.4808 (0.5387)  loss_n_60: 0.4753 (0.5140)  loss_n_80: 0.5618 (0.5487)  loss_n_100: 0.5703 (0.5753)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0700)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [450/845]  eta: 0:02:12  loss: 1.9106 (2.2577)  loss_n_40: 0.4808 (0.5422)  loss_n_60: 0.4467 (0.5153)  loss_n_80: 0.4920 (0.5492)  loss_n_100: 0.4952 (0.5753)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0698)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [460/845]  eta: 0:02:09  loss: 2.1140 (2.2502)  loss_n_40: 0.4391 (0.5399)  loss_n_60: 0.4556 (0.5140)  loss_n_80: 0.5678 (0.5479)  loss_n_100: 0.5300 (0.5744)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0683)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [470/845]  eta: 0:02:05  loss: 1.9661 (2.2485)  loss_n_40: 0.4300 (0.5397)  loss_n_60: 0.4366 (0.5144)  loss_n_80: 0.5611 (0.5477)  loss_n_100: 0.5300 (0.5743)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0669)  time: 0.3348  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [480/845]  eta: 0:02:02  loss: 1.8915 (2.2415)  loss_n_40: 0.4059 (0.5374)  loss_n_60: 0.4366 (0.5129)  loss_n_80: 0.5271 (0.5470)  loss_n_100: 0.5027 (0.5734)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0655)  time: 0.3352  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [490/845]  eta: 0:01:59  loss: 1.8915 (2.2397)  loss_n_40: 0.4435 (0.5361)  loss_n_60: 0.4488 (0.5132)  loss_n_80: 0.5271 (0.5470)  loss_n_100: 0.5351 (0.5739)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0641)  time: 0.3351  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [500/845]  eta: 0:01:55  loss: 1.8424 (2.2374)  loss_n_40: 0.4435 (0.5348)  loss_n_60: 0.4383 (0.5131)  loss_n_80: 0.4761 (0.5467)  loss_n_100: 0.5197 (0.5743)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0631)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [510/845]  eta: 0:01:52  loss: 1.8872 (2.2358)  loss_n_40: 0.4491 (0.5360)  loss_n_60: 0.4134 (0.5129)  loss_n_80: 0.4972 (0.5459)  loss_n_100: 0.5138 (0.5739)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0618)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [520/845]  eta: 0:01:49  loss: 1.6979 (2.2261)  loss_n_40: 0.3735 (0.5332)  loss_n_60: 0.4127 (0.5112)  loss_n_80: 0.4167 (0.5438)  loss_n_100: 0.4838 (0.5720)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0607)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [530/845]  eta: 0:01:45  loss: 1.6979 (2.2245)  loss_n_40: 0.3735 (0.5331)  loss_n_60: 0.4216 (0.5113)  loss_n_80: 0.4167 (0.5437)  loss_n_100: 0.4883 (0.5717)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0595)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [540/845]  eta: 0:01:42  loss: 1.7919 (2.2191)  loss_n_40: 0.4081 (0.5311)  loss_n_60: 0.4188 (0.5100)  loss_n_80: 0.4677 (0.5431)  loss_n_100: 0.5087 (0.5713)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0584)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [550/845]  eta: 0:01:38  loss: 1.9123 (2.2195)  loss_n_40: 0.4294 (0.5316)  loss_n_60: 0.4233 (0.5105)  loss_n_80: 0.5045 (0.5435)  loss_n_100: 0.5087 (0.5716)  triple_100: 0.0000 (0.0010)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0039)  triple_40: 0.0000 (0.0574)  time: 0.3350  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [560/845]  eta: 0:01:35  loss: 1.9096 (2.2166)  loss_n_40: 0.4294 (0.5317)  loss_n_60: 0.4481 (0.5098)  loss_n_80: 0.5092 (0.5428)  loss_n_100: 0.5392 (0.5711)  triple_100: 0.0000 (0.0010)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0038)  triple_40: 0.0000 (0.0563)  time: 0.3347  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [570/845]  eta: 0:01:32  loss: 1.9005 (2.2139)  loss_n_40: 0.3997 (0.5303)  loss_n_60: 0.4169 (0.5095)  loss_n_80: 0.5031 (0.5425)  loss_n_100: 0.5339 (0.5715)  triple_100: 0.0000 (0.0010)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0038)  triple_40: 0.0000 (0.0553)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [580/845]  eta: 0:01:28  loss: 2.1206 (2.2125)  loss_n_40: 0.4494 (0.5299)  loss_n_60: 0.5027 (0.5094)  loss_n_80: 0.5434 (0.5425)  loss_n_100: 0.5825 (0.5716)  triple_100: 0.0000 (0.0010)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0037)  triple_40: 0.0000 (0.0544)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [590/845]  eta: 0:01:25  loss: 2.1857 (2.2215)  loss_n_40: 0.5148 (0.5320)  loss_n_60: 0.5052 (0.5100)  loss_n_80: 0.5620 (0.5428)  loss_n_100: 0.5446 (0.5713)  triple_100: 0.0000 (0.0010)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0037)  triple_40: 0.0000 (0.0607)  time: 0.3346  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:10]  [600/845]  eta: 0:01:22  loss: 2.1118 (2.2192)  loss_n_40: 0.4412 (0.5307)  loss_n_60: 0.4728 (0.5094)  loss_n_80: 0.5611 (0.5429)  loss_n_100: 0.5851 (0.5719)  triple_100: 0.0000 (0.0010)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0036)  triple_40: 0.0000 (0.0597)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [610/845]  eta: 0:01:18  loss: 2.1107 (2.2209)  loss_n_40: 0.4412 (0.5308)  loss_n_60: 0.4883 (0.5100)  loss_n_80: 0.5679 (0.5438)  loss_n_100: 0.6145 (0.5732)  triple_100: 0.0000 (0.0009)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0035)  triple_40: 0.0000 (0.0587)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [620/845]  eta: 0:01:15  loss: 2.2183 (2.2206)  loss_n_40: 0.4998 (0.5302)  loss_n_60: 0.5257 (0.5105)  loss_n_80: 0.5737 (0.5438)  loss_n_100: 0.6229 (0.5736)  triple_100: 0.0000 (0.0009)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0035)  triple_40: 0.0000 (0.0580)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [630/845]  eta: 0:01:12  loss: 2.3104 (2.2207)  loss_n_40: 0.4631 (0.5298)  loss_n_60: 0.5722 (0.5111)  loss_n_80: 0.5473 (0.5439)  loss_n_100: 0.6372 (0.5744)  triple_100: 0.0000 (0.0009)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0571)  time: 0.3351  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [640/845]  eta: 0:01:08  loss: 2.2244 (2.2192)  loss_n_40: 0.4631 (0.5290)  loss_n_60: 0.5441 (0.5114)  loss_n_80: 0.5473 (0.5438)  loss_n_100: 0.6170 (0.5744)  triple_100: 0.0000 (0.0009)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0562)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [650/845]  eta: 0:01:05  loss: 2.2029 (2.2247)  loss_n_40: 0.4530 (0.5313)  loss_n_60: 0.4929 (0.5128)  loss_n_80: 0.5545 (0.5446)  loss_n_100: 0.5703 (0.5751)  triple_100: 0.0000 (0.0009)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0033)  triple_40: 0.0000 (0.0567)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [660/845]  eta: 0:01:02  loss: 2.2418 (2.2273)  loss_n_40: 0.4562 (0.5321)  loss_n_60: 0.5647 (0.5135)  loss_n_80: 0.5911 (0.5446)  loss_n_100: 0.5704 (0.5750)  triple_100: 0.0000 (0.0009)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0033)  triple_40: 0.0000 (0.0579)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [670/845]  eta: 0:00:58  loss: 2.4294 (2.2311)  loss_n_40: 0.4919 (0.5336)  loss_n_60: 0.5490 (0.5146)  loss_n_80: 0.5911 (0.5458)  loss_n_100: 0.5826 (0.5760)  triple_100: 0.0000 (0.0009)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0032)  triple_40: 0.0000 (0.0570)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [680/845]  eta: 0:00:55  loss: 2.0860 (2.2295)  loss_n_40: 0.4894 (0.5339)  loss_n_60: 0.4509 (0.5142)  loss_n_80: 0.5690 (0.5457)  loss_n_100: 0.5539 (0.5754)  triple_100: 0.0000 (0.0008)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0032)  triple_40: 0.0000 (0.0562)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [690/845]  eta: 0:00:51  loss: 2.1120 (2.2279)  loss_n_40: 0.4603 (0.5328)  loss_n_60: 0.4650 (0.5142)  loss_n_80: 0.5667 (0.5458)  loss_n_100: 0.5539 (0.5758)  triple_100: 0.0000 (0.0008)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0031)  triple_40: 0.0000 (0.0554)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [700/845]  eta: 0:00:48  loss: 2.1773 (2.2256)  loss_n_40: 0.4470 (0.5317)  loss_n_60: 0.5029 (0.5137)  loss_n_80: 0.5391 (0.5457)  loss_n_100: 0.5860 (0.5759)  triple_100: 0.0000 (0.0008)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0031)  triple_40: 0.0000 (0.0546)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [710/845]  eta: 0:00:45  loss: 2.1164 (2.2378)  loss_n_40: 0.5052 (0.5333)  loss_n_60: 0.4744 (0.5142)  loss_n_80: 0.5517 (0.5461)  loss_n_100: 0.5470 (0.5757)  triple_100: 0.0000 (0.0008)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0030)  triple_40: 0.0000 (0.0647)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [720/845]  eta: 0:00:41  loss: 2.1164 (2.2363)  loss_n_40: 0.4679 (0.5334)  loss_n_60: 0.4744 (0.5142)  loss_n_80: 0.5517 (0.5457)  loss_n_100: 0.5360 (0.5755)  triple_100: 0.0000 (0.0008)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0030)  triple_40: 0.0000 (0.0638)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [730/845]  eta: 0:00:38  loss: 2.0345 (2.2359)  loss_n_40: 0.4649 (0.5329)  loss_n_60: 0.4839 (0.5143)  loss_n_80: 0.5408 (0.5462)  loss_n_100: 0.5613 (0.5759)  triple_100: 0.0000 (0.0008)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0030)  triple_40: 0.0000 (0.0629)  time: 0.3351  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [740/845]  eta: 0:00:35  loss: 2.3017 (2.2396)  loss_n_40: 0.4662 (0.5326)  loss_n_60: 0.4989 (0.5142)  loss_n_80: 0.5681 (0.5458)  loss_n_100: 0.5930 (0.5757)  triple_100: 0.0000 (0.0008)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0029)  triple_40: 0.0000 (0.0675)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [750/845]  eta: 0:00:31  loss: 1.9649 (2.2375)  loss_n_40: 0.3980 (0.5318)  loss_n_60: 0.4407 (0.5140)  loss_n_80: 0.5044 (0.5457)  loss_n_100: 0.5772 (0.5755)  triple_100: 0.0000 (0.0008)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0029)  triple_40: 0.0000 (0.0668)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [760/845]  eta: 0:00:28  loss: 2.0491 (2.2375)  loss_n_40: 0.4349 (0.5319)  loss_n_60: 0.4478 (0.5143)  loss_n_80: 0.5554 (0.5461)  loss_n_100: 0.5993 (0.5758)  triple_100: 0.0000 (0.0008)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0028)  triple_40: 0.0000 (0.0659)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [770/845]  eta: 0:00:25  loss: 2.2602 (2.2416)  loss_n_40: 0.4733 (0.5327)  loss_n_60: 0.5230 (0.5151)  loss_n_80: 0.6038 (0.5469)  loss_n_100: 0.6253 (0.5766)  triple_100: 0.0000 (0.0007)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0028)  triple_40: 0.0000 (0.0668)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [780/845]  eta: 0:00:21  loss: 2.3128 (2.2448)  loss_n_40: 0.4966 (0.5336)  loss_n_60: 0.5099 (0.5154)  loss_n_80: 0.5901 (0.5470)  loss_n_100: 0.5912 (0.5765)  triple_100: 0.0000 (0.0007)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0028)  triple_40: 0.0000 (0.0688)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [790/845]  eta: 0:00:18  loss: 2.1651 (2.2446)  loss_n_40: 0.4966 (0.5351)  loss_n_60: 0.4874 (0.5159)  loss_n_80: 0.5310 (0.5465)  loss_n_100: 0.5088 (0.5756)  triple_100: 0.0000 (0.0007)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0680)  time: 0.3350  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [800/845]  eta: 0:00:15  loss: 2.2818 (2.2448)  loss_n_40: 0.4471 (0.5361)  loss_n_60: 0.5313 (0.5162)  loss_n_80: 0.4963 (0.5463)  loss_n_100: 0.4864 (0.5752)  triple_100: 0.0000 (0.0007)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0675)  time: 0.3349  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [810/845]  eta: 0:00:11  loss: 1.8755 (2.2438)  loss_n_40: 0.4471 (0.5364)  loss_n_60: 0.4477 (0.5165)  loss_n_80: 0.4963 (0.5459)  loss_n_100: 0.5074 (0.5749)  triple_100: 0.0000 (0.0007)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0666)  time: 0.3349  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:10]  [820/845]  eta: 0:00:08  loss: 1.8192 (2.2397)  loss_n_40: 0.3839 (0.5353)  loss_n_60: 0.3982 (0.5157)  loss_n_80: 0.4612 (0.5451)  loss_n_100: 0.5058 (0.5745)  triple_100: 0.0000 (0.0007)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0658)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [830/845]  eta: 0:00:05  loss: 1.8574 (2.2386)  loss_n_40: 0.3839 (0.5353)  loss_n_60: 0.3974 (0.5153)  loss_n_80: 0.4897 (0.5450)  loss_n_100: 0.4830 (0.5740)  triple_100: 0.0000 (0.0007)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0656)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10]  [840/845]  eta: 0:00:01  loss: 1.9532 (2.2394)  loss_n_40: 0.4208 (0.5352)  loss_n_60: 0.4267 (0.5155)  loss_n_80: 0.5811 (0.5454)  loss_n_100: 0.5488 (0.5746)  triple_100: 0.0000 (0.0007)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0655)  time: 0.3349  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:10]  [844/845]  eta: 0:00:00  loss: 2.3563 (2.2398)  loss_n_40: 0.4736 (0.5350)  loss_n_60: 0.5482 (0.5155)  loss_n_80: 0.6202 (0.5459)  loss_n_100: 0.5663 (0.5751)  triple_100: 0.0000 (0.0007)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0652)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:10] Total time: 0:04:43 (0.3354 s / it)\n",
      "Averaged stats: loss: 2.3563 (2.2398)  loss_n_40: 0.4736 (0.5350)  loss_n_60: 0.5482 (0.5155)  loss_n_80: 0.6202 (0.5459)  loss_n_100: 0.5663 (0.5751)  triple_100: 0.0000 (0.0007)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0652)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_10_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.575%\n",
      "Min loss_n_100: 0.575\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:11]  [   0/1724]  eta: 2:00:20  lr: 0.000200  loss: 2.2509 (2.2509)  loss_n_40: 0.5138 (0.5138)  loss_n_60: 0.5726 (0.5726)  loss_n_80: 0.5736 (0.5736)  loss_n_100: 0.5908 (0.5908)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1884  data: 0.4145  max mem: 46473\n",
      "Train: [epoch:11]  [  10/1724]  eta: 1:52:46  lr: 0.000200  loss: 2.1681 (2.1364)  loss_n_40: 0.5105 (0.5456)  loss_n_60: 0.5233 (0.5201)  loss_n_80: 0.5417 (0.5198)  loss_n_100: 0.5556 (0.5509)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9478  data: 0.0379  max mem: 46473\n",
      "Train: [epoch:11]  [  20/1724]  eta: 1:51:48  lr: 0.000200  loss: 2.3285 (2.4837)  loss_n_40: 0.5081 (0.5446)  loss_n_60: 0.5491 (0.5717)  loss_n_80: 0.5785 (0.6214)  loss_n_100: 0.6370 (0.6962)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0128)  triple_40: 0.0000 (0.0370)  time: 3.9242  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [  30/1724]  eta: 1:51:01  lr: 0.000200  loss: 2.7107 (2.5642)  loss_n_40: 0.5242 (0.5516)  loss_n_60: 0.6188 (0.5956)  loss_n_80: 0.7218 (0.6541)  loss_n_100: 0.8167 (0.7292)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0251)  time: 3.9244  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [  40/1724]  eta: 1:50:19  lr: 0.000200  loss: 2.5016 (2.4922)  loss_n_40: 0.5088 (0.5359)  loss_n_60: 0.5628 (0.5799)  loss_n_80: 0.6278 (0.6397)  loss_n_100: 0.7073 (0.7112)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0065)  triple_40: 0.0000 (0.0189)  time: 3.9240  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [  50/1724]  eta: 1:49:37  lr: 0.000200  loss: 2.2998 (2.4609)  loss_n_40: 0.5088 (0.5441)  loss_n_60: 0.5440 (0.5726)  loss_n_80: 0.5705 (0.6262)  loss_n_100: 0.6034 (0.6865)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0263)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [  60/1724]  eta: 1:48:56  lr: 0.000200  loss: 2.0684 (2.4150)  loss_n_40: 0.4753 (0.5315)  loss_n_60: 0.5123 (0.5595)  loss_n_80: 0.5271 (0.6097)  loss_n_100: 0.5473 (0.6671)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0406)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [  70/1724]  eta: 1:48:15  lr: 0.000200  loss: 1.9774 (2.3638)  loss_n_40: 0.4383 (0.5209)  loss_n_60: 0.4688 (0.5492)  loss_n_80: 0.5151 (0.5988)  loss_n_100: 0.5436 (0.6543)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0349)  time: 3.9231  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [  80/1724]  eta: 1:47:35  lr: 0.000200  loss: 2.0694 (2.3337)  loss_n_40: 0.4333 (0.5180)  loss_n_60: 0.4841 (0.5443)  loss_n_80: 0.5282 (0.5914)  loss_n_100: 0.5543 (0.6444)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0306)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [  90/1724]  eta: 1:46:54  lr: 0.000200  loss: 2.0411 (2.2914)  loss_n_40: 0.4450 (0.5108)  loss_n_60: 0.4797 (0.5347)  loss_n_80: 0.5194 (0.5814)  loss_n_100: 0.5424 (0.6328)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0272)  time: 3.9202  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 100/1724]  eta: 1:46:14  lr: 0.000200  loss: 1.9173 (2.2586)  loss_n_40: 0.4371 (0.5040)  loss_n_60: 0.4494 (0.5250)  loss_n_80: 0.4902 (0.5705)  loss_n_100: 0.5303 (0.6194)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0319)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 110/1724]  eta: 1:45:34  lr: 0.000200  loss: 1.9173 (2.2454)  loss_n_40: 0.4344 (0.4991)  loss_n_60: 0.4546 (0.5227)  loss_n_80: 0.4907 (0.5682)  loss_n_100: 0.5419 (0.6193)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0037)  triple_40: 0.0000 (0.0290)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 120/1724]  eta: 1:44:54  lr: 0.000200  loss: 2.1432 (2.2338)  loss_n_40: 0.4413 (0.4999)  loss_n_60: 0.5069 (0.5214)  loss_n_80: 0.5602 (0.5654)  loss_n_100: 0.5941 (0.6140)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0266)  time: 3.9207  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 130/1724]  eta: 1:44:15  lr: 0.000200  loss: 2.1432 (2.2180)  loss_n_40: 0.5061 (0.5004)  loss_n_60: 0.5088 (0.5187)  loss_n_80: 0.5149 (0.5609)  loss_n_100: 0.5322 (0.6074)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0031)  triple_40: 0.0000 (0.0246)  time: 3.9201  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 140/1724]  eta: 1:43:35  lr: 0.000200  loss: 1.9229 (2.2156)  loss_n_40: 0.4306 (0.4980)  loss_n_60: 0.4486 (0.5133)  loss_n_80: 0.4815 (0.5548)  loss_n_100: 0.5166 (0.5991)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0071)  triple_60: 0.0000 (0.0108)  triple_40: 0.0000 (0.0299)  time: 3.9199  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 150/1724]  eta: 1:42:55  lr: 0.000200  loss: 2.7705 (2.3086)  loss_n_40: 0.5117 (0.5056)  loss_n_60: 0.5910 (0.5261)  loss_n_80: 0.6210 (0.5770)  loss_n_100: 0.6216 (0.6245)  triple_100: 0.0000 (0.0144)  triple_80: 0.0000 (0.0112)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0340)  time: 3.9198  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 160/1724]  eta: 1:42:16  lr: 0.000200  loss: 3.4478 (2.3767)  loss_n_40: 0.6773 (0.5208)  loss_n_60: 0.7594 (0.5413)  loss_n_80: 0.8918 (0.5980)  loss_n_100: 0.9721 (0.6459)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0319)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 170/1724]  eta: 1:41:36  lr: 0.000200  loss: 3.3681 (2.4402)  loss_n_40: 0.7557 (0.5359)  loss_n_60: 0.7594 (0.5564)  loss_n_80: 0.8792 (0.6155)  loss_n_100: 0.9371 (0.6624)  triple_100: 0.0000 (0.0154)  triple_80: 0.0000 (0.0099)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0300)  time: 3.9212  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 180/1724]  eta: 1:40:57  lr: 0.000200  loss: 3.0681 (2.4701)  loss_n_40: 0.6153 (0.5416)  loss_n_60: 0.6867 (0.5627)  loss_n_80: 0.8204 (0.6259)  loss_n_100: 0.8566 (0.6738)  triple_100: 0.0000 (0.0145)  triple_80: 0.0000 (0.0094)  triple_60: 0.0000 (0.0139)  triple_40: 0.0000 (0.0284)  time: 3.9213  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 190/1724]  eta: 1:40:17  lr: 0.000200  loss: 2.6905 (2.4654)  loss_n_40: 0.5531 (0.5418)  loss_n_60: 0.6082 (0.5624)  loss_n_80: 0.7115 (0.6259)  loss_n_100: 0.7380 (0.6726)  triple_100: 0.0000 (0.0138)  triple_80: 0.0000 (0.0089)  triple_60: 0.0000 (0.0132)  triple_40: 0.0000 (0.0269)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 200/1724]  eta: 1:39:38  lr: 0.000200  loss: 2.2900 (2.4547)  loss_n_40: 0.5135 (0.5409)  loss_n_60: 0.5413 (0.5606)  loss_n_80: 0.6181 (0.6245)  loss_n_100: 0.6081 (0.6691)  triple_100: 0.0000 (0.0131)  triple_80: 0.0000 (0.0084)  triple_60: 0.0000 (0.0125)  triple_40: 0.0000 (0.0256)  time: 3.9198  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [ 210/1724]  eta: 1:38:58  lr: 0.000200  loss: 2.2994 (2.4494)  loss_n_40: 0.5249 (0.5407)  loss_n_60: 0.5573 (0.5602)  loss_n_80: 0.5998 (0.6238)  loss_n_100: 0.6100 (0.6677)  triple_100: 0.0000 (0.0125)  triple_80: 0.0000 (0.0080)  triple_60: 0.0000 (0.0119)  triple_40: 0.0000 (0.0246)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 220/1724]  eta: 1:38:19  lr: 0.000200  loss: 2.2389 (2.4305)  loss_n_40: 0.4745 (0.5373)  loss_n_60: 0.5186 (0.5568)  loss_n_80: 0.5791 (0.6195)  loss_n_100: 0.5903 (0.6625)  triple_100: 0.0000 (0.0119)  triple_80: 0.0000 (0.0077)  triple_60: 0.0000 (0.0114)  triple_40: 0.0000 (0.0235)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 230/1724]  eta: 1:37:40  lr: 0.000200  loss: 1.9902 (2.4174)  loss_n_40: 0.4446 (0.5354)  loss_n_60: 0.4797 (0.5544)  loss_n_80: 0.5251 (0.6168)  loss_n_100: 0.5636 (0.6587)  triple_100: 0.0000 (0.0114)  triple_80: 0.0000 (0.0073)  triple_60: 0.0000 (0.0109)  triple_40: 0.0000 (0.0225)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 240/1724]  eta: 1:37:00  lr: 0.000200  loss: 1.9634 (2.3953)  loss_n_40: 0.4270 (0.5316)  loss_n_60: 0.4578 (0.5496)  loss_n_80: 0.5229 (0.6115)  loss_n_100: 0.5439 (0.6527)  triple_100: 0.0000 (0.0109)  triple_80: 0.0000 (0.0070)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0215)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 250/1724]  eta: 1:36:21  lr: 0.000200  loss: 1.8360 (2.3775)  loss_n_40: 0.4048 (0.5281)  loss_n_60: 0.4437 (0.5466)  loss_n_80: 0.5001 (0.6072)  loss_n_100: 0.5062 (0.6476)  triple_100: 0.0000 (0.0105)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0100)  triple_40: 0.0000 (0.0207)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 260/1724]  eta: 1:35:42  lr: 0.000200  loss: 1.8293 (2.3586)  loss_n_40: 0.4413 (0.5260)  loss_n_60: 0.4363 (0.5422)  loss_n_80: 0.4750 (0.6023)  loss_n_100: 0.5037 (0.6421)  triple_100: 0.0000 (0.0101)  triple_80: 0.0000 (0.0065)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0199)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 270/1724]  eta: 1:35:02  lr: 0.000200  loss: 1.6733 (2.3266)  loss_n_40: 0.3783 (0.5191)  loss_n_60: 0.3882 (0.5351)  loss_n_80: 0.4112 (0.5943)  loss_n_100: 0.4491 (0.6337)  triple_100: 0.0000 (0.0097)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0191)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 280/1724]  eta: 1:34:23  lr: 0.000200  loss: 1.5360 (2.3130)  loss_n_40: 0.3472 (0.5147)  loss_n_60: 0.3593 (0.5315)  loss_n_80: 0.4026 (0.5904)  loss_n_100: 0.4519 (0.6301)  triple_100: 0.0000 (0.0115)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0198)  time: 3.9205  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 290/1724]  eta: 1:33:44  lr: 0.000200  loss: 2.1841 (2.3142)  loss_n_40: 0.4221 (0.5141)  loss_n_60: 0.5074 (0.5320)  loss_n_80: 0.5747 (0.5920)  loss_n_100: 0.5970 (0.6315)  triple_100: 0.0000 (0.0111)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0191)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 300/1724]  eta: 1:33:04  lr: 0.000200  loss: 2.3904 (2.3321)  loss_n_40: 0.4935 (0.5149)  loss_n_60: 0.5390 (0.5332)  loss_n_80: 0.6419 (0.5934)  loss_n_100: 0.6658 (0.6337)  triple_100: 0.0000 (0.0164)  triple_80: 0.0000 (0.0102)  triple_60: 0.0000 (0.0118)  triple_40: 0.0000 (0.0186)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 310/1724]  eta: 1:32:25  lr: 0.000200  loss: 2.4082 (2.3362)  loss_n_40: 0.5062 (0.5154)  loss_n_60: 0.5532 (0.5347)  loss_n_80: 0.6187 (0.5952)  loss_n_100: 0.6693 (0.6358)  triple_100: 0.0000 (0.0159)  triple_80: 0.0000 (0.0098)  triple_60: 0.0000 (0.0114)  triple_40: 0.0000 (0.0180)  time: 3.9215  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 320/1724]  eta: 1:31:46  lr: 0.000200  loss: 2.1924 (2.3312)  loss_n_40: 0.4898 (0.5150)  loss_n_60: 0.5260 (0.5336)  loss_n_80: 0.5867 (0.5934)  loss_n_100: 0.6137 (0.6334)  triple_100: 0.0000 (0.0154)  triple_80: 0.0000 (0.0095)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0198)  time: 3.9210  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 330/1724]  eta: 1:31:07  lr: 0.000200  loss: 2.1154 (2.3238)  loss_n_40: 0.4858 (0.5149)  loss_n_60: 0.4996 (0.5325)  loss_n_80: 0.5339 (0.5914)  loss_n_100: 0.5564 (0.6308)  triple_100: 0.0000 (0.0149)  triple_80: 0.0000 (0.0093)  triple_60: 0.0000 (0.0107)  triple_40: 0.0000 (0.0192)  time: 3.9217  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 340/1724]  eta: 1:30:28  lr: 0.000200  loss: 2.0627 (2.3137)  loss_n_40: 0.4978 (0.5143)  loss_n_60: 0.4729 (0.5310)  loss_n_80: 0.5019 (0.5887)  loss_n_100: 0.5244 (0.6272)  triple_100: 0.0000 (0.0145)  triple_80: 0.0000 (0.0090)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0186)  time: 3.9225  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 350/1724]  eta: 1:29:48  lr: 0.000200  loss: 1.9263 (2.3020)  loss_n_40: 0.4978 (0.5138)  loss_n_60: 0.4606 (0.5287)  loss_n_80: 0.4769 (0.5854)  loss_n_100: 0.4939 (0.6232)  triple_100: 0.0000 (0.0141)  triple_80: 0.0000 (0.0087)  triple_60: 0.0000 (0.0101)  triple_40: 0.0000 (0.0181)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 360/1724]  eta: 1:29:09  lr: 0.000200  loss: 1.8743 (2.2906)  loss_n_40: 0.4487 (0.5124)  loss_n_60: 0.4642 (0.5263)  loss_n_80: 0.4792 (0.5823)  loss_n_100: 0.4800 (0.6197)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0085)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0180)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 370/1724]  eta: 1:28:30  lr: 0.000200  loss: 1.8743 (2.2804)  loss_n_40: 0.4513 (0.5121)  loss_n_60: 0.4486 (0.5242)  loss_n_80: 0.4792 (0.5793)  loss_n_100: 0.4800 (0.6161)  triple_100: 0.0000 (0.0133)  triple_80: 0.0000 (0.0083)  triple_60: 0.0000 (0.0095)  triple_40: 0.0000 (0.0175)  time: 3.9220  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 380/1724]  eta: 1:27:51  lr: 0.000200  loss: 1.7867 (2.2676)  loss_n_40: 0.4320 (0.5106)  loss_n_60: 0.4232 (0.5215)  loss_n_80: 0.4388 (0.5754)  loss_n_100: 0.4611 (0.6119)  triple_100: 0.0000 (0.0130)  triple_80: 0.0000 (0.0080)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0179)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 390/1724]  eta: 1:27:12  lr: 0.000200  loss: 1.7050 (2.2528)  loss_n_40: 0.4014 (0.5079)  loss_n_60: 0.3940 (0.5182)  loss_n_80: 0.4196 (0.5716)  loss_n_100: 0.4611 (0.6081)  triple_100: 0.0000 (0.0127)  triple_80: 0.0000 (0.0078)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0175)  time: 3.9232  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 400/1724]  eta: 1:26:32  lr: 0.000200  loss: 1.7367 (2.2447)  loss_n_40: 0.4052 (0.5077)  loss_n_60: 0.4019 (0.5168)  loss_n_80: 0.4461 (0.5693)  loss_n_100: 0.4718 (0.6051)  triple_100: 0.0000 (0.0123)  triple_80: 0.0000 (0.0076)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0170)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 410/1724]  eta: 1:25:53  lr: 0.000200  loss: 1.7732 (2.2329)  loss_n_40: 0.4184 (0.5064)  loss_n_60: 0.4168 (0.5144)  loss_n_80: 0.4541 (0.5660)  loss_n_100: 0.4634 (0.6015)  triple_100: 0.0000 (0.0120)  triple_80: 0.0000 (0.0075)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0166)  time: 3.9229  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 420/1724]  eta: 1:25:14  lr: 0.000200  loss: 1.7295 (2.2263)  loss_n_40: 0.4204 (0.5073)  loss_n_60: 0.3997 (0.5128)  loss_n_80: 0.4407 (0.5634)  loss_n_100: 0.4518 (0.5982)  triple_100: 0.0000 (0.0118)  triple_80: 0.0000 (0.0073)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0171)  time: 3.9234  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 430/1724]  eta: 1:24:35  lr: 0.000200  loss: 1.8496 (2.2218)  loss_n_40: 0.4775 (0.5063)  loss_n_60: 0.4193 (0.5114)  loss_n_80: 0.4822 (0.5621)  loss_n_100: 0.4793 (0.5968)  triple_100: 0.0000 (0.0115)  triple_80: 0.0000 (0.0071)  triple_60: 0.0000 (0.0100)  triple_40: 0.0000 (0.0167)  time: 3.9236  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 440/1724]  eta: 1:23:56  lr: 0.000200  loss: 1.8262 (2.2118)  loss_n_40: 0.4136 (0.5040)  loss_n_60: 0.4278 (0.5092)  loss_n_80: 0.4611 (0.5598)  loss_n_100: 0.4986 (0.5946)  triple_100: 0.0000 (0.0112)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0097)  triple_40: 0.0000 (0.0163)  time: 3.9228  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [ 450/1724]  eta: 1:23:16  lr: 0.000200  loss: 1.6565 (2.2009)  loss_n_40: 0.3996 (0.5026)  loss_n_60: 0.3867 (0.5068)  loss_n_80: 0.4320 (0.5570)  loss_n_100: 0.4564 (0.5913)  triple_100: 0.0000 (0.0110)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0095)  triple_40: 0.0000 (0.0160)  time: 3.9218  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 460/1724]  eta: 1:22:37  lr: 0.000200  loss: 1.8015 (2.2044)  loss_n_40: 0.3962 (0.5008)  loss_n_60: 0.4130 (0.5059)  loss_n_80: 0.4703 (0.5568)  loss_n_100: 0.4935 (0.5917)  triple_100: 0.0000 (0.0165)  triple_80: 0.0000 (0.0074)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0160)  time: 3.9223  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 470/1724]  eta: 1:21:58  lr: 0.000200  loss: 2.0340 (2.2020)  loss_n_40: 0.4319 (0.5005)  loss_n_60: 0.4709 (0.5056)  loss_n_80: 0.5202 (0.5564)  loss_n_100: 0.5859 (0.5915)  triple_100: 0.0000 (0.0162)  triple_80: 0.0000 (0.0072)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0157)  time: 3.9222  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 480/1724]  eta: 1:21:19  lr: 0.000200  loss: 2.0236 (2.1975)  loss_n_40: 0.4329 (0.4993)  loss_n_60: 0.4709 (0.5047)  loss_n_80: 0.5238 (0.5557)  loss_n_100: 0.5687 (0.5907)  triple_100: 0.0000 (0.0158)  triple_80: 0.0000 (0.0071)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0153)  time: 3.9231  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 490/1724]  eta: 1:20:40  lr: 0.000200  loss: 1.9576 (2.1931)  loss_n_40: 0.4259 (0.4989)  loss_n_60: 0.4588 (0.5037)  loss_n_80: 0.5149 (0.5545)  loss_n_100: 0.5558 (0.5897)  triple_100: 0.0000 (0.0155)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0150)  time: 3.9241  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 500/1724]  eta: 1:20:00  lr: 0.000200  loss: 1.9187 (2.1896)  loss_n_40: 0.4577 (0.4992)  loss_n_60: 0.4427 (0.5031)  loss_n_80: 0.4860 (0.5530)  loss_n_100: 0.5182 (0.5880)  triple_100: 0.0000 (0.0152)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0157)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 510/1724]  eta: 1:19:21  lr: 0.000200  loss: 1.8961 (2.1826)  loss_n_40: 0.4446 (0.4976)  loss_n_60: 0.4331 (0.5014)  loss_n_80: 0.4788 (0.5516)  loss_n_100: 0.5097 (0.5867)  triple_100: 0.0000 (0.0149)  triple_80: 0.0000 (0.0066)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0154)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 520/1724]  eta: 1:18:42  lr: 0.000200  loss: 1.8174 (2.1753)  loss_n_40: 0.4108 (0.4966)  loss_n_60: 0.4093 (0.4996)  loss_n_80: 0.4769 (0.5500)  loss_n_100: 0.5073 (0.5847)  triple_100: 0.0000 (0.0146)  triple_80: 0.0000 (0.0065)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0151)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 530/1724]  eta: 1:18:03  lr: 0.000200  loss: 1.7376 (2.1652)  loss_n_40: 0.3843 (0.4943)  loss_n_60: 0.3801 (0.4971)  loss_n_80: 0.4537 (0.5479)  loss_n_100: 0.4644 (0.5822)  triple_100: 0.0000 (0.0144)  triple_80: 0.0000 (0.0064)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0148)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 540/1724]  eta: 1:17:23  lr: 0.000200  loss: 1.6096 (2.1566)  loss_n_40: 0.3860 (0.4936)  loss_n_60: 0.3636 (0.4951)  loss_n_80: 0.4160 (0.5455)  loss_n_100: 0.4378 (0.5796)  triple_100: 0.0000 (0.0141)  triple_80: 0.0000 (0.0063)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0146)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 550/1724]  eta: 1:16:44  lr: 0.000200  loss: 1.5810 (2.1473)  loss_n_40: 0.3870 (0.4921)  loss_n_60: 0.3607 (0.4927)  loss_n_80: 0.4052 (0.5428)  loss_n_100: 0.4278 (0.5767)  triple_100: 0.0000 (0.0138)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0151)  time: 3.9225  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 560/1724]  eta: 1:16:05  lr: 0.000200  loss: 2.1238 (2.1577)  loss_n_40: 0.4445 (0.4928)  loss_n_60: 0.4810 (0.4947)  loss_n_80: 0.4848 (0.5449)  loss_n_100: 0.4879 (0.5794)  triple_100: 0.0000 (0.0161)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0149)  time: 3.9221  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 570/1724]  eta: 1:15:26  lr: 0.000200  loss: 2.4801 (2.1628)  loss_n_40: 0.5005 (0.4938)  loss_n_60: 0.5961 (0.4959)  loss_n_80: 0.6164 (0.5466)  loss_n_100: 0.6803 (0.5814)  triple_100: 0.0000 (0.0158)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0146)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 580/1724]  eta: 1:14:47  lr: 0.000200  loss: 2.1762 (2.1617)  loss_n_40: 0.5087 (0.4938)  loss_n_60: 0.4955 (0.4956)  loss_n_80: 0.5631 (0.5462)  loss_n_100: 0.6025 (0.5813)  triple_100: 0.0000 (0.0156)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0147)  time: 3.9235  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 590/1724]  eta: 1:14:07  lr: 0.000200  loss: 1.9995 (2.1592)  loss_n_40: 0.4587 (0.4939)  loss_n_60: 0.4477 (0.4951)  loss_n_80: 0.5072 (0.5456)  loss_n_100: 0.5514 (0.5805)  triple_100: 0.0000 (0.0153)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0145)  time: 3.9234  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 600/1724]  eta: 1:13:28  lr: 0.000200  loss: 1.8684 (2.1546)  loss_n_40: 0.4525 (0.4935)  loss_n_60: 0.4274 (0.4943)  loss_n_80: 0.4919 (0.5445)  loss_n_100: 0.5132 (0.5792)  triple_100: 0.0000 (0.0150)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0142)  time: 3.9225  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 610/1724]  eta: 1:12:49  lr: 0.000200  loss: 1.8180 (2.1493)  loss_n_40: 0.4197 (0.4927)  loss_n_60: 0.4189 (0.4931)  loss_n_80: 0.4643 (0.5432)  loss_n_100: 0.4837 (0.5778)  triple_100: 0.0000 (0.0148)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0140)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 620/1724]  eta: 1:12:10  lr: 0.000200  loss: 1.7052 (2.1425)  loss_n_40: 0.4007 (0.4915)  loss_n_60: 0.4006 (0.4916)  loss_n_80: 0.4572 (0.5414)  loss_n_100: 0.4813 (0.5760)  triple_100: 0.0000 (0.0146)  triple_80: 0.0000 (0.0056)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0138)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 630/1724]  eta: 1:11:31  lr: 0.000200  loss: 1.6951 (2.1374)  loss_n_40: 0.4379 (0.4907)  loss_n_60: 0.3944 (0.4905)  loss_n_80: 0.4283 (0.5401)  loss_n_100: 0.4662 (0.5748)  triple_100: 0.0000 (0.0144)  triple_80: 0.0000 (0.0055)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0136)  time: 3.9211  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 640/1724]  eta: 1:10:51  lr: 0.000200  loss: 1.6729 (2.1309)  loss_n_40: 0.4065 (0.4892)  loss_n_60: 0.3944 (0.4890)  loss_n_80: 0.4218 (0.5385)  loss_n_100: 0.4571 (0.5734)  triple_100: 0.0000 (0.0141)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0134)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 650/1724]  eta: 1:10:12  lr: 0.000200  loss: 1.5937 (2.1224)  loss_n_40: 0.3745 (0.4875)  loss_n_60: 0.3695 (0.4872)  loss_n_80: 0.4109 (0.5365)  loss_n_100: 0.4507 (0.5712)  triple_100: 0.0000 (0.0139)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0131)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 660/1724]  eta: 1:09:33  lr: 0.000200  loss: 1.6107 (2.1191)  loss_n_40: 0.3827 (0.4862)  loss_n_60: 0.3695 (0.4858)  loss_n_80: 0.4016 (0.5351)  loss_n_100: 0.4405 (0.5700)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0146)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 670/1724]  eta: 1:08:54  lr: 0.000200  loss: 2.5339 (2.1585)  loss_n_40: 0.5558 (0.4924)  loss_n_60: 0.5262 (0.4914)  loss_n_80: 0.5854 (0.5418)  loss_n_100: 0.6514 (0.5769)  triple_100: 0.0000 (0.0246)  triple_80: 0.0000 (0.0080)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0151)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 680/1724]  eta: 1:08:14  lr: 0.000200  loss: 4.1538 (2.1898)  loss_n_40: 0.8939 (0.4998)  loss_n_60: 0.8989 (0.4977)  loss_n_80: 1.0988 (0.5508)  loss_n_100: 1.1854 (0.5863)  triple_100: 0.0000 (0.0242)  triple_80: 0.0000 (0.0079)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0149)  time: 3.9212  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [ 690/1724]  eta: 1:07:35  lr: 0.000200  loss: 3.7846 (2.2061)  loss_n_40: 0.8268 (0.5033)  loss_n_60: 0.8216 (0.5012)  loss_n_80: 1.0105 (0.5556)  loss_n_100: 1.0782 (0.5915)  triple_100: 0.0000 (0.0239)  triple_80: 0.0000 (0.0078)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0146)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 700/1724]  eta: 1:06:56  lr: 0.000200  loss: 2.9988 (2.2100)  loss_n_40: 0.6111 (0.5041)  loss_n_60: 0.6448 (0.5023)  loss_n_80: 0.8255 (0.5571)  loss_n_100: 0.7293 (0.5929)  triple_100: 0.0000 (0.0235)  triple_80: 0.0000 (0.0076)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0144)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 710/1724]  eta: 1:06:17  lr: 0.000200  loss: 2.3688 (2.2115)  loss_n_40: 0.4903 (0.5045)  loss_n_60: 0.5200 (0.5027)  loss_n_80: 0.6394 (0.5576)  loss_n_100: 0.6597 (0.5934)  triple_100: 0.0000 (0.0232)  triple_80: 0.0000 (0.0075)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0146)  time: 3.9232  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 720/1724]  eta: 1:05:38  lr: 0.000200  loss: 2.2200 (2.2102)  loss_n_40: 0.4398 (0.5038)  loss_n_60: 0.4746 (0.5024)  loss_n_80: 0.5588 (0.5577)  loss_n_100: 0.6155 (0.5937)  triple_100: 0.0000 (0.0229)  triple_80: 0.0000 (0.0074)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0144)  time: 3.9243  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 730/1724]  eta: 1:04:58  lr: 0.000200  loss: 2.2200 (2.2157)  loss_n_40: 0.4606 (0.5043)  loss_n_60: 0.5090 (0.5028)  loss_n_80: 0.5588 (0.5584)  loss_n_100: 0.6122 (0.5946)  triple_100: 0.0000 (0.0233)  triple_80: 0.0000 (0.0091)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0143)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 740/1724]  eta: 1:04:19  lr: 0.000200  loss: 2.8411 (2.2308)  loss_n_40: 0.5418 (0.5048)  loss_n_60: 0.6337 (0.5057)  loss_n_80: 0.7530 (0.5634)  loss_n_100: 0.8033 (0.6019)  triple_100: 0.0000 (0.0230)  triple_80: 0.0000 (0.0090)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0141)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 750/1724]  eta: 1:03:40  lr: 0.000200  loss: 3.1003 (2.2403)  loss_n_40: 0.5533 (0.5057)  loss_n_60: 0.6640 (0.5076)  loss_n_80: 0.8585 (0.5664)  loss_n_100: 1.0695 (0.6065)  triple_100: 0.0000 (0.0226)  triple_80: 0.0000 (0.0089)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0139)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 760/1724]  eta: 1:03:01  lr: 0.000200  loss: 2.6108 (2.2435)  loss_n_40: 0.5185 (0.5066)  loss_n_60: 0.5665 (0.5082)  loss_n_80: 0.6905 (0.5673)  loss_n_100: 0.8261 (0.6080)  triple_100: 0.0000 (0.0224)  triple_80: 0.0000 (0.0088)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0138)  time: 3.9245  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 770/1724]  eta: 1:02:22  lr: 0.000200  loss: 2.0547 (2.2397)  loss_n_40: 0.4679 (0.5062)  loss_n_60: 0.4690 (0.5072)  loss_n_80: 0.5187 (0.5661)  loss_n_100: 0.6012 (0.6074)  triple_100: 0.0000 (0.0221)  triple_80: 0.0000 (0.0086)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0136)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 780/1724]  eta: 1:01:42  lr: 0.000200  loss: 1.8215 (2.2350)  loss_n_40: 0.4100 (0.5054)  loss_n_60: 0.4044 (0.5061)  loss_n_80: 0.4574 (0.5648)  loss_n_100: 0.5346 (0.6065)  triple_100: 0.0000 (0.0218)  triple_80: 0.0000 (0.0085)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0134)  time: 3.9244  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 790/1724]  eta: 1:01:03  lr: 0.000200  loss: 1.6766 (2.2288)  loss_n_40: 0.3870 (0.5046)  loss_n_60: 0.3680 (0.5048)  loss_n_80: 0.4203 (0.5631)  loss_n_100: 0.4990 (0.6049)  triple_100: 0.0000 (0.0215)  triple_80: 0.0000 (0.0084)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0132)  time: 3.9243  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 800/1724]  eta: 1:00:24  lr: 0.000200  loss: 1.5954 (2.2234)  loss_n_40: 0.3805 (0.5034)  loss_n_60: 0.3680 (0.5034)  loss_n_80: 0.3966 (0.5617)  loss_n_100: 0.4774 (0.6039)  triple_100: 0.0000 (0.0212)  triple_80: 0.0000 (0.0083)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0132)  time: 3.9237  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 810/1724]  eta: 0:59:45  lr: 0.000200  loss: 1.6179 (2.2164)  loss_n_40: 0.3548 (0.5021)  loss_n_60: 0.3629 (0.5017)  loss_n_80: 0.4150 (0.5597)  loss_n_100: 0.4839 (0.6025)  triple_100: 0.0000 (0.0210)  triple_80: 0.0000 (0.0082)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0130)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 820/1724]  eta: 0:59:05  lr: 0.000200  loss: 1.6179 (2.2103)  loss_n_40: 0.3548 (0.5014)  loss_n_60: 0.3629 (0.5003)  loss_n_80: 0.4162 (0.5579)  loss_n_100: 0.4839 (0.6010)  triple_100: 0.0000 (0.0207)  triple_80: 0.0000 (0.0081)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0128)  time: 3.9239  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 830/1724]  eta: 0:58:26  lr: 0.000200  loss: 1.5954 (2.2020)  loss_n_40: 0.3639 (0.4999)  loss_n_60: 0.3588 (0.4984)  loss_n_80: 0.4097 (0.5557)  loss_n_100: 0.4446 (0.5989)  triple_100: 0.0000 (0.0205)  triple_80: 0.0000 (0.0080)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0127)  time: 3.9238  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 840/1724]  eta: 0:57:47  lr: 0.000200  loss: 1.5237 (2.1957)  loss_n_40: 0.3502 (0.4989)  loss_n_60: 0.3408 (0.4970)  loss_n_80: 0.3628 (0.5540)  loss_n_100: 0.4408 (0.5973)  triple_100: 0.0000 (0.0202)  triple_80: 0.0000 (0.0079)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0125)  time: 3.9234  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 850/1724]  eta: 0:57:08  lr: 0.000200  loss: 1.7672 (2.1907)  loss_n_40: 0.3903 (0.4982)  loss_n_60: 0.3830 (0.4958)  loss_n_80: 0.4166 (0.5524)  loss_n_100: 0.4563 (0.5959)  triple_100: 0.0000 (0.0200)  triple_80: 0.0000 (0.0082)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0124)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 860/1724]  eta: 0:56:29  lr: 0.000200  loss: 1.7279 (2.1837)  loss_n_40: 0.3875 (0.4967)  loss_n_60: 0.3827 (0.4942)  loss_n_80: 0.4166 (0.5506)  loss_n_100: 0.4650 (0.5942)  triple_100: 0.0000 (0.0198)  triple_80: 0.0000 (0.0081)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0122)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 870/1724]  eta: 0:55:49  lr: 0.000200  loss: 1.5527 (2.1766)  loss_n_40: 0.3686 (0.4953)  loss_n_60: 0.3551 (0.4925)  loss_n_80: 0.3882 (0.5488)  loss_n_100: 0.4405 (0.5926)  triple_100: 0.0000 (0.0195)  triple_80: 0.0000 (0.0080)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0121)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 880/1724]  eta: 0:55:10  lr: 0.000200  loss: 1.5237 (2.1681)  loss_n_40: 0.3471 (0.4937)  loss_n_60: 0.3385 (0.4906)  loss_n_80: 0.3759 (0.5466)  loss_n_100: 0.4321 (0.5905)  triple_100: 0.0000 (0.0193)  triple_80: 0.0000 (0.0079)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0120)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 890/1724]  eta: 0:54:31  lr: 0.000200  loss: 1.4873 (2.1614)  loss_n_40: 0.3218 (0.4928)  loss_n_60: 0.3372 (0.4891)  loss_n_80: 0.3684 (0.5447)  loss_n_100: 0.3971 (0.5885)  triple_100: 0.0000 (0.0191)  triple_80: 0.0000 (0.0078)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0118)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 900/1724]  eta: 0:53:52  lr: 0.000200  loss: 1.6806 (2.1588)  loss_n_40: 0.3957 (0.4921)  loss_n_60: 0.3907 (0.4882)  loss_n_80: 0.3915 (0.5438)  loss_n_100: 0.4422 (0.5877)  triple_100: 0.0000 (0.0189)  triple_80: 0.0000 (0.0081)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0121)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 910/1724]  eta: 0:53:12  lr: 0.000200  loss: 2.0004 (2.1578)  loss_n_40: 0.4830 (0.4922)  loss_n_60: 0.4475 (0.4880)  loss_n_80: 0.4958 (0.5435)  loss_n_100: 0.5608 (0.5875)  triple_100: 0.0000 (0.0187)  triple_80: 0.0000 (0.0080)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0119)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 920/1724]  eta: 0:52:33  lr: 0.000200  loss: 1.8876 (2.1543)  loss_n_40: 0.4788 (0.4919)  loss_n_60: 0.4361 (0.4874)  loss_n_80: 0.4781 (0.5425)  loss_n_100: 0.5122 (0.5865)  triple_100: 0.0000 (0.0185)  triple_80: 0.0000 (0.0079)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0118)  time: 3.9236  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [ 930/1724]  eta: 0:51:54  lr: 0.000200  loss: 1.6466 (2.1486)  loss_n_40: 0.3970 (0.4909)  loss_n_60: 0.3740 (0.4861)  loss_n_80: 0.4229 (0.5410)  loss_n_100: 0.4713 (0.5851)  triple_100: 0.0000 (0.0183)  triple_80: 0.0000 (0.0078)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0117)  time: 3.9232  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 940/1724]  eta: 0:51:15  lr: 0.000200  loss: 1.5197 (2.1423)  loss_n_40: 0.3358 (0.4897)  loss_n_60: 0.3480 (0.4847)  loss_n_80: 0.3677 (0.5393)  loss_n_100: 0.4232 (0.5835)  triple_100: 0.0000 (0.0181)  triple_80: 0.0000 (0.0077)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0115)  time: 3.9224  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [ 950/1724]  eta: 0:50:36  lr: 0.000200  loss: 1.4533 (2.1347)  loss_n_40: 0.3259 (0.4881)  loss_n_60: 0.3254 (0.4829)  loss_n_80: 0.3677 (0.5373)  loss_n_100: 0.4232 (0.5816)  triple_100: 0.0000 (0.0180)  triple_80: 0.0000 (0.0077)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0114)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 960/1724]  eta: 0:49:56  lr: 0.000200  loss: 1.4533 (2.1317)  loss_n_40: 0.3215 (0.4866)  loss_n_60: 0.3209 (0.4813)  loss_n_80: 0.3551 (0.5354)  loss_n_100: 0.4042 (0.5799)  triple_100: 0.0000 (0.0191)  triple_80: 0.0000 (0.0089)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0121)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 970/1724]  eta: 0:49:17  lr: 0.000200  loss: 1.7195 (2.1374)  loss_n_40: 0.3883 (0.4870)  loss_n_60: 0.3783 (0.4821)  loss_n_80: 0.4362 (0.5366)  loss_n_100: 0.4523 (0.5813)  triple_100: 0.0000 (0.0203)  triple_80: 0.0000 (0.0095)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0120)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 980/1724]  eta: 0:48:38  lr: 0.000200  loss: 2.6503 (2.1421)  loss_n_40: 0.5492 (0.4876)  loss_n_60: 0.5876 (0.4832)  loss_n_80: 0.6909 (0.5382)  loss_n_100: 0.7782 (0.5833)  triple_100: 0.0000 (0.0201)  triple_80: 0.0000 (0.0094)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0119)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [ 990/1724]  eta: 0:47:59  lr: 0.000200  loss: 2.5333 (2.1457)  loss_n_40: 0.5349 (0.4887)  loss_n_60: 0.5583 (0.4839)  loss_n_80: 0.6611 (0.5392)  loss_n_100: 0.7441 (0.5845)  triple_100: 0.0000 (0.0199)  triple_80: 0.0000 (0.0093)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0118)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1000/1724]  eta: 0:47:19  lr: 0.000200  loss: 2.2456 (2.1463)  loss_n_40: 0.5266 (0.4890)  loss_n_60: 0.5151 (0.4841)  loss_n_80: 0.5653 (0.5394)  loss_n_100: 0.6632 (0.5851)  triple_100: 0.0000 (0.0197)  triple_80: 0.0000 (0.0093)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0116)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1010/1724]  eta: 0:46:40  lr: 0.000200  loss: 2.0411 (2.1448)  loss_n_40: 0.4557 (0.4893)  loss_n_60: 0.4544 (0.4838)  loss_n_80: 0.5094 (0.5388)  loss_n_100: 0.5726 (0.5845)  triple_100: 0.0000 (0.0195)  triple_80: 0.0000 (0.0092)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0115)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1020/1724]  eta: 0:46:01  lr: 0.000200  loss: 1.7716 (2.1415)  loss_n_40: 0.4033 (0.4887)  loss_n_60: 0.3972 (0.4831)  loss_n_80: 0.4544 (0.5379)  loss_n_100: 0.5126 (0.5839)  triple_100: 0.0000 (0.0193)  triple_80: 0.0000 (0.0091)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0114)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1030/1724]  eta: 0:45:22  lr: 0.000200  loss: 1.7315 (2.1376)  loss_n_40: 0.3954 (0.4881)  loss_n_60: 0.3888 (0.4823)  loss_n_80: 0.4294 (0.5368)  loss_n_100: 0.5014 (0.5829)  triple_100: 0.0000 (0.0191)  triple_80: 0.0000 (0.0090)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0113)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1040/1724]  eta: 0:44:42  lr: 0.000200  loss: 1.6562 (2.1329)  loss_n_40: 0.3807 (0.4873)  loss_n_60: 0.3700 (0.4813)  loss_n_80: 0.4088 (0.5356)  loss_n_100: 0.4585 (0.5817)  triple_100: 0.0000 (0.0189)  triple_80: 0.0000 (0.0089)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0112)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1050/1724]  eta: 0:44:03  lr: 0.000200  loss: 1.5114 (2.1285)  loss_n_40: 0.3629 (0.4870)  loss_n_60: 0.3510 (0.4803)  loss_n_80: 0.3808 (0.5342)  loss_n_100: 0.4442 (0.5804)  triple_100: 0.0000 (0.0188)  triple_80: 0.0000 (0.0088)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0113)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1060/1724]  eta: 0:43:24  lr: 0.000200  loss: 1.6347 (2.1247)  loss_n_40: 0.3716 (0.4864)  loss_n_60: 0.3713 (0.4795)  loss_n_80: 0.4000 (0.5332)  loss_n_100: 0.4503 (0.5794)  triple_100: 0.0000 (0.0186)  triple_80: 0.0000 (0.0087)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0112)  time: 3.9190  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1070/1724]  eta: 0:42:45  lr: 0.000200  loss: 1.6491 (2.1212)  loss_n_40: 0.4009 (0.4860)  loss_n_60: 0.3774 (0.4787)  loss_n_80: 0.4230 (0.5322)  loss_n_100: 0.4756 (0.5785)  triple_100: 0.0000 (0.0184)  triple_80: 0.0000 (0.0087)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0111)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1080/1724]  eta: 0:42:05  lr: 0.000200  loss: 1.7096 (2.1202)  loss_n_40: 0.4362 (0.4859)  loss_n_60: 0.3978 (0.4784)  loss_n_80: 0.4263 (0.5317)  loss_n_100: 0.4937 (0.5780)  triple_100: 0.0000 (0.0182)  triple_80: 0.0000 (0.0086)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0118)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1090/1724]  eta: 0:41:26  lr: 0.000200  loss: 1.7750 (2.1166)  loss_n_40: 0.4066 (0.4855)  loss_n_60: 0.3959 (0.4776)  loss_n_80: 0.4287 (0.5307)  loss_n_100: 0.4804 (0.5771)  triple_100: 0.0000 (0.0181)  triple_80: 0.0000 (0.0085)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0117)  time: 3.9204  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1100/1724]  eta: 0:40:47  lr: 0.000200  loss: 1.6919 (2.1129)  loss_n_40: 0.3932 (0.4849)  loss_n_60: 0.3888 (0.4768)  loss_n_80: 0.4228 (0.5297)  loss_n_100: 0.4740 (0.5762)  triple_100: 0.0000 (0.0179)  triple_80: 0.0000 (0.0084)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0116)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1110/1724]  eta: 0:40:08  lr: 0.000200  loss: 1.6556 (2.1085)  loss_n_40: 0.3823 (0.4842)  loss_n_60: 0.3695 (0.4758)  loss_n_80: 0.4169 (0.5285)  loss_n_100: 0.4657 (0.5750)  triple_100: 0.0000 (0.0178)  triple_80: 0.0000 (0.0083)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0115)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1120/1724]  eta: 0:39:28  lr: 0.000200  loss: 1.5704 (2.1091)  loss_n_40: 0.3660 (0.4835)  loss_n_60: 0.3563 (0.4747)  loss_n_80: 0.3844 (0.5271)  loss_n_100: 0.4161 (0.5736)  triple_100: 0.0000 (0.0176)  triple_80: 0.0000 (0.0095)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0141)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1130/1724]  eta: 0:38:49  lr: 0.000200  loss: 1.9161 (2.1301)  loss_n_40: 0.5622 (0.4851)  loss_n_60: 0.4092 (0.4775)  loss_n_80: 0.4601 (0.5310)  loss_n_100: 0.5190 (0.5787)  triple_100: 0.0000 (0.0254)  triple_80: 0.0000 (0.0094)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0140)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1140/1724]  eta: 0:38:10  lr: 0.000200  loss: 3.8060 (2.1418)  loss_n_40: 0.6537 (0.4868)  loss_n_60: 0.7977 (0.4802)  loss_n_80: 0.9918 (0.5344)  loss_n_100: 1.1613 (0.5833)  triple_100: 0.0000 (0.0252)  triple_80: 0.0000 (0.0093)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0139)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1150/1724]  eta: 0:37:31  lr: 0.000200  loss: 2.9540 (2.1476)  loss_n_40: 0.5818 (0.4876)  loss_n_60: 0.6958 (0.4816)  loss_n_80: 0.7814 (0.5361)  loss_n_100: 0.9152 (0.5856)  triple_100: 0.0000 (0.0249)  triple_80: 0.0000 (0.0092)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0138)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1160/1724]  eta: 0:36:52  lr: 0.000200  loss: 2.7248 (2.1517)  loss_n_40: 0.5495 (0.4885)  loss_n_60: 0.6268 (0.4828)  loss_n_80: 0.6899 (0.5372)  loss_n_100: 0.7877 (0.5870)  triple_100: 0.0000 (0.0247)  triple_80: 0.0000 (0.0092)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0137)  time: 3.9212  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [1170/1724]  eta: 0:36:12  lr: 0.000200  loss: 2.5732 (2.1544)  loss_n_40: 0.5495 (0.4893)  loss_n_60: 0.5965 (0.4833)  loss_n_80: 0.6523 (0.5377)  loss_n_100: 0.7033 (0.5876)  triple_100: 0.0000 (0.0245)  triple_80: 0.0000 (0.0091)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0135)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1180/1724]  eta: 0:35:33  lr: 0.000200  loss: 2.8206 (2.1728)  loss_n_40: 0.6490 (0.4913)  loss_n_60: 0.6325 (0.4870)  loss_n_80: 0.7051 (0.5423)  loss_n_100: 0.7937 (0.5931)  triple_100: 0.0000 (0.0254)  triple_80: 0.0000 (0.0099)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0140)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1190/1724]  eta: 0:34:54  lr: 0.000200  loss: 4.8423 (2.1993)  loss_n_40: 0.9440 (0.4964)  loss_n_60: 1.1398 (0.4934)  loss_n_80: 1.2313 (0.5492)  loss_n_100: 1.4232 (0.6008)  triple_100: 0.0000 (0.0252)  triple_80: 0.0000 (0.0098)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0148)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1200/1724]  eta: 0:34:15  lr: 0.000200  loss: 4.9304 (2.2225)  loss_n_40: 1.0507 (0.5016)  loss_n_60: 1.1675 (0.4990)  loss_n_80: 1.2749 (0.5550)  loss_n_100: 1.4424 (0.6074)  triple_100: 0.0000 (0.0250)  triple_80: 0.0000 (0.0097)  triple_60: 0.0000 (0.0101)  triple_40: 0.0000 (0.0147)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1210/1724]  eta: 0:33:35  lr: 0.000200  loss: 4.5849 (2.2406)  loss_n_40: 0.9853 (0.5056)  loss_n_60: 1.0546 (0.5033)  loss_n_80: 1.1514 (0.5597)  loss_n_100: 1.2941 (0.6130)  triple_100: 0.0000 (0.0248)  triple_80: 0.0000 (0.0096)  triple_60: 0.0000 (0.0100)  triple_40: 0.0000 (0.0145)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1220/1724]  eta: 0:32:56  lr: 0.000200  loss: 3.9993 (2.2537)  loss_n_40: 0.9160 (0.5088)  loss_n_60: 0.9466 (0.5065)  loss_n_80: 1.0190 (0.5631)  loss_n_100: 1.1569 (0.6168)  triple_100: 0.0000 (0.0246)  triple_80: 0.0000 (0.0096)  triple_60: 0.0000 (0.0100)  triple_40: 0.0000 (0.0144)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1230/1724]  eta: 0:32:17  lr: 0.000200  loss: 3.7163 (2.2658)  loss_n_40: 0.8757 (0.5121)  loss_n_60: 0.8737 (0.5096)  loss_n_80: 0.9378 (0.5660)  loss_n_100: 1.0401 (0.6198)  triple_100: 0.0000 (0.0244)  triple_80: 0.0000 (0.0095)  triple_60: 0.0000 (0.0099)  triple_40: 0.0000 (0.0146)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1240/1724]  eta: 0:31:38  lr: 0.000200  loss: 3.4590 (2.2760)  loss_n_40: 0.8381 (0.5149)  loss_n_60: 0.8195 (0.5124)  loss_n_80: 0.8845 (0.5685)  loss_n_100: 0.9518 (0.6223)  triple_100: 0.0000 (0.0242)  triple_80: 0.0000 (0.0094)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0145)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1250/1724]  eta: 0:30:58  lr: 0.000200  loss: 3.1506 (2.2810)  loss_n_40: 0.7284 (0.5163)  loss_n_60: 0.7261 (0.5137)  loss_n_80: 0.7754 (0.5697)  loss_n_100: 0.8459 (0.6237)  triple_100: 0.0000 (0.0241)  triple_80: 0.0000 (0.0093)  triple_60: 0.0000 (0.0097)  triple_40: 0.0000 (0.0144)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1260/1724]  eta: 0:30:19  lr: 0.000200  loss: 2.7211 (2.2973)  loss_n_40: 0.6491 (0.5177)  loss_n_60: 0.6474 (0.5152)  loss_n_80: 0.6997 (0.5713)  loss_n_100: 0.7743 (0.6255)  triple_100: 0.0000 (0.0283)  triple_80: 0.0000 (0.0127)  triple_60: 0.0000 (0.0103)  triple_40: 0.0000 (0.0163)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1270/1724]  eta: 0:29:40  lr: 0.000200  loss: 5.5742 (2.3366)  loss_n_40: 0.9175 (0.5226)  loss_n_60: 1.1227 (0.5218)  loss_n_80: 1.2309 (0.5797)  loss_n_100: 1.2809 (0.6357)  triple_100: 0.0000 (0.0374)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0102)  triple_40: 0.0000 (0.0161)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1280/1724]  eta: 0:29:01  lr: 0.000200  loss: 6.2735 (2.3669)  loss_n_40: 1.1813 (0.5284)  loss_n_60: 1.3703 (0.5286)  loss_n_80: 1.6200 (0.5881)  loss_n_100: 1.8638 (0.6457)  triple_100: 0.0000 (0.0372)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0101)  triple_40: 0.0000 (0.0160)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1290/1724]  eta: 0:28:22  lr: 0.000200  loss: 5.7592 (2.3957)  loss_n_40: 1.1827 (0.5336)  loss_n_60: 1.2929 (0.5341)  loss_n_80: 1.5593 (0.5948)  loss_n_100: 1.7814 (0.6538)  triple_100: 0.0000 (0.0369)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0124)  triple_40: 0.0000 (0.0166)  time: 3.9206  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1300/1724]  eta: 0:27:42  lr: 0.000200  loss: 6.7292 (2.4421)  loss_n_40: 1.2291 (0.5409)  loss_n_60: 1.5100 (0.5447)  loss_n_80: 1.6025 (0.6080)  loss_n_100: 1.8198 (0.6681)  triple_100: 0.0000 (0.0382)  triple_80: 0.0000 (0.0134)  triple_60: 0.0000 (0.0123)  triple_40: 0.0000 (0.0165)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1310/1724]  eta: 0:27:03  lr: 0.000200  loss: 7.5843 (2.4823)  loss_n_40: 1.2845 (0.5457)  loss_n_60: 1.6634 (0.5536)  loss_n_80: 2.0758 (0.6197)  loss_n_100: 2.4226 (0.6818)  triple_100: 0.0000 (0.0396)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0122)  triple_40: 0.0000 (0.0164)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1320/1724]  eta: 0:26:24  lr: 0.000200  loss: 7.0059 (2.5153)  loss_n_40: 1.1681 (0.5507)  loss_n_60: 1.5388 (0.5613)  loss_n_80: 1.9270 (0.6294)  loss_n_100: 2.2595 (0.6931)  triple_100: 0.0000 (0.0393)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0121)  triple_40: 0.0000 (0.0162)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1330/1724]  eta: 0:25:45  lr: 0.000200  loss: 6.5536 (2.5456)  loss_n_40: 1.1620 (0.5550)  loss_n_60: 1.4840 (0.5685)  loss_n_80: 1.8113 (0.6382)  loss_n_100: 2.1134 (0.7034)  triple_100: 0.0000 (0.0392)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0120)  triple_40: 0.0000 (0.0161)  time: 3.9213  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1340/1724]  eta: 0:25:05  lr: 0.000200  loss: 5.9554 (2.5705)  loss_n_40: 1.0175 (0.5585)  loss_n_60: 1.3961 (0.5737)  loss_n_80: 1.5994 (0.6441)  loss_n_100: 1.7457 (0.7103)  triple_100: 0.0000 (0.0423)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0119)  triple_40: 0.0000 (0.0160)  time: 3.9226  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1350/1724]  eta: 0:24:26  lr: 0.000200  loss: 6.1545 (2.6094)  loss_n_40: 1.0376 (0.5623)  loss_n_60: 1.3765 (0.5810)  loss_n_80: 1.5994 (0.6541)  loss_n_100: 1.7894 (0.7216)  triple_100: 0.0000 (0.0476)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0119)  triple_40: 0.0000 (0.0163)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1360/1724]  eta: 0:23:47  lr: 0.000200  loss: 7.6324 (2.6488)  loss_n_40: 1.0695 (0.5664)  loss_n_60: 1.7748 (0.5907)  loss_n_80: 2.2271 (0.6671)  loss_n_100: 2.4904 (0.7349)  triple_100: 0.0000 (0.0472)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0118)  triple_40: 0.0000 (0.0161)  time: 3.9207  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1370/1724]  eta: 0:23:08  lr: 0.000200  loss: 7.9525 (2.6889)  loss_n_40: 1.1917 (0.5713)  loss_n_60: 1.8642 (0.5999)  loss_n_80: 2.3886 (0.6803)  loss_n_100: 2.5423 (0.7483)  triple_100: 0.0000 (0.0469)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0117)  triple_40: 0.0000 (0.0160)  time: 3.9204  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1380/1724]  eta: 0:22:29  lr: 0.000200  loss: 7.2413 (2.7164)  loss_n_40: 1.1863 (0.5742)  loss_n_60: 1.5561 (0.6056)  loss_n_80: 2.3325 (0.6900)  loss_n_100: 2.3615 (0.7583)  triple_100: 0.0000 (0.0466)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0117)  triple_40: 0.0000 (0.0159)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1390/1724]  eta: 0:21:49  lr: 0.000200  loss: 6.4230 (2.7535)  loss_n_40: 0.9911 (0.5778)  loss_n_60: 1.4157 (0.6122)  loss_n_80: 1.8810 (0.6994)  loss_n_100: 1.9991 (0.7681)  triple_100: 0.0000 (0.0462)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0146)  triple_40: 0.0000 (0.0189)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1400/1724]  eta: 0:21:10  lr: 0.000200  loss: 8.3751 (2.8027)  loss_n_40: 1.1826 (0.5827)  loss_n_60: 1.9343 (0.6239)  loss_n_80: 2.3703 (0.7136)  loss_n_100: 2.5769 (0.7826)  triple_100: 0.0000 (0.0485)  triple_80: 0.0000 (0.0181)  triple_60: 0.0000 (0.0145)  triple_40: 0.0000 (0.0188)  time: 3.9176  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [1410/1724]  eta: 0:20:31  lr: 0.000200  loss: 8.3732 (2.8387)  loss_n_40: 1.1878 (0.5868)  loss_n_60: 1.8762 (0.6323)  loss_n_80: 2.5062 (0.7257)  loss_n_100: 2.5769 (0.7946)  triple_100: 0.0000 (0.0482)  triple_80: 0.0000 (0.0180)  triple_60: 0.0000 (0.0144)  triple_40: 0.0000 (0.0186)  time: 3.9183  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1420/1724]  eta: 0:19:52  lr: 0.000200  loss: 7.8960 (2.8750)  loss_n_40: 1.1258 (0.5911)  loss_n_60: 1.8014 (0.6400)  loss_n_80: 2.4689 (0.7378)  loss_n_100: 2.5016 (0.8076)  triple_100: 0.0000 (0.0478)  triple_80: 0.0000 (0.0179)  triple_60: 0.0000 (0.0143)  triple_40: 0.0000 (0.0185)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1430/1724]  eta: 0:19:12  lr: 0.000200  loss: 7.0898 (2.8997)  loss_n_40: 1.0975 (0.5941)  loss_n_60: 1.5222 (0.6454)  loss_n_80: 2.1621 (0.7460)  loss_n_100: 2.2488 (0.8163)  triple_100: 0.0000 (0.0475)  triple_80: 0.0000 (0.0178)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0184)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1440/1724]  eta: 0:18:33  lr: 0.000200  loss: 5.9242 (2.9187)  loss_n_40: 1.0438 (0.5976)  loss_n_60: 1.3408 (0.6498)  loss_n_80: 1.6579 (0.7518)  loss_n_100: 1.8220 (0.8223)  triple_100: 0.0000 (0.0472)  triple_80: 0.0000 (0.0176)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0183)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1450/1724]  eta: 0:17:54  lr: 0.000200  loss: 5.4458 (2.9359)  loss_n_40: 0.9589 (0.6008)  loss_n_60: 1.2057 (0.6537)  loss_n_80: 1.5462 (0.7571)  loss_n_100: 1.6749 (0.8278)  triple_100: 0.0000 (0.0469)  triple_80: 0.0000 (0.0175)  triple_60: 0.0000 (0.0140)  triple_40: 0.0000 (0.0181)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1460/1724]  eta: 0:17:15  lr: 0.000200  loss: 4.8430 (2.9462)  loss_n_40: 0.9092 (0.6024)  loss_n_60: 1.0565 (0.6558)  loss_n_80: 1.3971 (0.7606)  loss_n_100: 1.4736 (0.8316)  triple_100: 0.0000 (0.0465)  triple_80: 0.0000 (0.0174)  triple_60: 0.0000 (0.0139)  triple_40: 0.0000 (0.0180)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1470/1724]  eta: 0:16:36  lr: 0.000200  loss: 4.2960 (2.9554)  loss_n_40: 0.8227 (0.6042)  loss_n_60: 0.9311 (0.6579)  loss_n_80: 1.1672 (0.7633)  loss_n_100: 1.3359 (0.8348)  triple_100: 0.0000 (0.0462)  triple_80: 0.0000 (0.0173)  triple_60: 0.0000 (0.0138)  triple_40: 0.0000 (0.0179)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1480/1724]  eta: 0:15:56  lr: 0.000200  loss: 4.0821 (2.9626)  loss_n_40: 0.7627 (0.6054)  loss_n_60: 0.9085 (0.6595)  loss_n_80: 1.0908 (0.7655)  loss_n_100: 1.2529 (0.8377)  triple_100: 0.0000 (0.0459)  triple_80: 0.0000 (0.0172)  triple_60: 0.0000 (0.0137)  triple_40: 0.0000 (0.0178)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1490/1724]  eta: 0:15:17  lr: 0.000200  loss: 3.8930 (2.9679)  loss_n_40: 0.7538 (0.6066)  loss_n_60: 0.8828 (0.6607)  loss_n_80: 1.0293 (0.7669)  loss_n_100: 1.1694 (0.8395)  triple_100: 0.0000 (0.0458)  triple_80: 0.0000 (0.0170)  triple_60: 0.0000 (0.0136)  triple_40: 0.0000 (0.0176)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1500/1724]  eta: 0:14:38  lr: 0.000200  loss: 3.5070 (2.9715)  loss_n_40: 0.6420 (0.6069)  loss_n_60: 0.7785 (0.6615)  loss_n_80: 0.9271 (0.7680)  loss_n_100: 1.1286 (0.8416)  triple_100: 0.0000 (0.0455)  triple_80: 0.0000 (0.0169)  triple_60: 0.0000 (0.0135)  triple_40: 0.0000 (0.0175)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1510/1724]  eta: 0:13:59  lr: 0.000200  loss: 3.4329 (2.9759)  loss_n_40: 0.6106 (0.6080)  loss_n_60: 0.7475 (0.6626)  loss_n_80: 0.9180 (0.7691)  loss_n_100: 1.1286 (0.8434)  triple_100: 0.0000 (0.0452)  triple_80: 0.0000 (0.0168)  triple_60: 0.0000 (0.0134)  triple_40: 0.0000 (0.0174)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1520/1724]  eta: 0:13:20  lr: 0.000200  loss: 3.3160 (2.9777)  loss_n_40: 0.6549 (0.6083)  loss_n_60: 0.7324 (0.6630)  loss_n_80: 0.8703 (0.7696)  loss_n_100: 1.0262 (0.8445)  triple_100: 0.0000 (0.0449)  triple_80: 0.0000 (0.0167)  triple_60: 0.0000 (0.0134)  triple_40: 0.0000 (0.0173)  time: 3.9208  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1530/1724]  eta: 0:12:40  lr: 0.000200  loss: 3.0132 (2.9768)  loss_n_40: 0.5889 (0.6081)  loss_n_60: 0.6653 (0.6628)  loss_n_80: 0.8035 (0.7694)  loss_n_100: 0.9737 (0.8449)  triple_100: 0.0000 (0.0446)  triple_80: 0.0000 (0.0166)  triple_60: 0.0000 (0.0133)  triple_40: 0.0000 (0.0172)  time: 3.9212  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1540/1724]  eta: 0:12:01  lr: 0.000200  loss: 2.6855 (2.9752)  loss_n_40: 0.5633 (0.6081)  loss_n_60: 0.6017 (0.6626)  loss_n_80: 0.6952 (0.7689)  loss_n_100: 0.8125 (0.8446)  triple_100: 0.0000 (0.0443)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0132)  triple_40: 0.0000 (0.0171)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1550/1724]  eta: 0:11:22  lr: 0.000200  loss: 2.7552 (2.9749)  loss_n_40: 0.5721 (0.6082)  loss_n_60: 0.5993 (0.6624)  loss_n_80: 0.6952 (0.7686)  loss_n_100: 0.8125 (0.8445)  triple_100: 0.0000 (0.0445)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0132)  triple_40: 0.0000 (0.0170)  time: 3.9233  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1560/1724]  eta: 0:10:43  lr: 0.000200  loss: 2.6105 (2.9720)  loss_n_40: 0.5341 (0.6078)  loss_n_60: 0.5891 (0.6620)  loss_n_80: 0.6659 (0.7678)  loss_n_100: 0.7655 (0.8438)  triple_100: 0.0000 (0.0442)  triple_80: 0.0000 (0.0164)  triple_60: 0.0000 (0.0131)  triple_40: 0.0000 (0.0169)  time: 3.9225  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1570/1724]  eta: 0:10:03  lr: 0.000200  loss: 2.4734 (2.9687)  loss_n_40: 0.4882 (0.6073)  loss_n_60: 0.5693 (0.6615)  loss_n_80: 0.6476 (0.7670)  loss_n_100: 0.7263 (0.8429)  triple_100: 0.0000 (0.0439)  triple_80: 0.0000 (0.0163)  triple_60: 0.0000 (0.0131)  triple_40: 0.0000 (0.0168)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1580/1724]  eta: 0:09:24  lr: 0.000200  loss: 2.5677 (2.9917)  loss_n_40: 0.5322 (0.6090)  loss_n_60: 0.6373 (0.6642)  loss_n_80: 0.6999 (0.7700)  loss_n_100: 0.7580 (0.8475)  triple_100: 0.0000 (0.0494)  triple_80: 0.0000 (0.0207)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0168)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1590/1724]  eta: 0:08:45  lr: 0.000200  loss: 5.1945 (3.0056)  loss_n_40: 1.0062 (0.6118)  loss_n_60: 1.1660 (0.6676)  loss_n_80: 1.3227 (0.7737)  loss_n_100: 1.6134 (0.8521)  triple_100: 0.0000 (0.0491)  triple_80: 0.0000 (0.0206)  triple_60: 0.0000 (0.0140)  triple_40: 0.0000 (0.0167)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1600/1724]  eta: 0:08:06  lr: 0.000200  loss: 4.8427 (3.0168)  loss_n_40: 1.0168 (0.6146)  loss_n_60: 1.1217 (0.6699)  loss_n_80: 1.2244 (0.7762)  loss_n_100: 1.3602 (0.8548)  triple_100: 0.0000 (0.0488)  triple_80: 0.0000 (0.0207)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0166)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1610/1724]  eta: 0:07:27  lr: 0.000200  loss: 4.1946 (3.0215)  loss_n_40: 0.8491 (0.6159)  loss_n_60: 0.9393 (0.6710)  loss_n_80: 1.0810 (0.7774)  loss_n_100: 1.2052 (0.8564)  triple_100: 0.0000 (0.0485)  triple_80: 0.0000 (0.0206)  triple_60: 0.0000 (0.0151)  triple_40: 0.0000 (0.0165)  time: 3.9215  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1620/1724]  eta: 0:06:47  lr: 0.000200  loss: 3.6644 (3.0249)  loss_n_40: 0.7486 (0.6167)  loss_n_60: 0.7984 (0.6717)  loss_n_80: 0.9433 (0.7782)  loss_n_100: 1.0371 (0.8573)  triple_100: 0.0000 (0.0482)  triple_80: 0.0000 (0.0205)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0172)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1630/1724]  eta: 0:06:08  lr: 0.000200  loss: 3.2337 (3.0271)  loss_n_40: 0.6768 (0.6171)  loss_n_60: 0.7330 (0.6720)  loss_n_80: 0.8393 (0.7785)  loss_n_100: 0.9251 (0.8576)  triple_100: 0.0000 (0.0482)  triple_80: 0.0000 (0.0203)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0171)  time: 3.9213  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1640/1724]  eta: 0:05:29  lr: 0.000200  loss: 3.0563 (3.0270)  loss_n_40: 0.6360 (0.6174)  loss_n_60: 0.6916 (0.6720)  loss_n_80: 0.8034 (0.7785)  loss_n_100: 0.8922 (0.8577)  triple_100: 0.0000 (0.0479)  triple_80: 0.0000 (0.0203)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0169)  time: 3.9210  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:11]  [1650/1724]  eta: 0:04:50  lr: 0.000200  loss: 2.8707 (3.0257)  loss_n_40: 0.5490 (0.6171)  loss_n_60: 0.6529 (0.6720)  loss_n_80: 0.7571 (0.7783)  loss_n_100: 0.8454 (0.8576)  triple_100: 0.0000 (0.0476)  triple_80: 0.0000 (0.0201)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0168)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1660/1724]  eta: 0:04:10  lr: 0.000200  loss: 2.6040 (3.0230)  loss_n_40: 0.5149 (0.6165)  loss_n_60: 0.6259 (0.6715)  loss_n_80: 0.6903 (0.7774)  loss_n_100: 0.7770 (0.8568)  triple_100: 0.0000 (0.0474)  triple_80: 0.0000 (0.0201)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0167)  time: 3.9218  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1670/1724]  eta: 0:03:31  lr: 0.000200  loss: 2.3679 (3.0201)  loss_n_40: 0.4926 (0.6163)  loss_n_60: 0.5561 (0.6711)  loss_n_80: 0.6080 (0.7765)  loss_n_100: 0.6970 (0.8559)  triple_100: 0.0000 (0.0471)  triple_80: 0.0000 (0.0200)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0166)  time: 3.9225  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1680/1724]  eta: 0:02:52  lr: 0.000200  loss: 2.3414 (3.0171)  loss_n_40: 0.4872 (0.6161)  loss_n_60: 0.5522 (0.6708)  loss_n_80: 0.5985 (0.7756)  loss_n_100: 0.6754 (0.8549)  triple_100: 0.0000 (0.0469)  triple_80: 0.0000 (0.0200)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0165)  time: 3.9225  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1690/1724]  eta: 0:02:13  lr: 0.000200  loss: 2.2811 (3.0131)  loss_n_40: 0.4583 (0.6152)  loss_n_60: 0.5422 (0.6700)  loss_n_80: 0.5847 (0.7743)  loss_n_100: 0.6533 (0.8537)  triple_100: 0.0000 (0.0468)  triple_80: 0.0000 (0.0201)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0164)  time: 3.9235  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1700/1724]  eta: 0:01:34  lr: 0.000200  loss: 2.1834 (3.0101)  loss_n_40: 0.4500 (0.6150)  loss_n_60: 0.5302 (0.6696)  loss_n_80: 0.5237 (0.7732)  loss_n_100: 0.6183 (0.8526)  triple_100: 0.0000 (0.0469)  triple_80: 0.0000 (0.0199)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0164)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:11]  [1710/1724]  eta: 0:00:54  lr: 0.000200  loss: 2.3674 (3.0070)  loss_n_40: 0.4923 (0.6145)  loss_n_60: 0.6027 (0.6693)  loss_n_80: 0.6076 (0.7725)  loss_n_100: 0.6730 (0.8517)  triple_100: 0.0000 (0.0466)  triple_80: 0.0000 (0.0198)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0163)  time: 3.9224  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1720/1724]  eta: 0:00:15  lr: 0.000200  loss: 2.3674 (3.0044)  loss_n_40: 0.4853 (0.6139)  loss_n_60: 0.6036 (0.6689)  loss_n_80: 0.6134 (0.7716)  loss_n_100: 0.7096 (0.8509)  triple_100: 0.0000 (0.0466)  triple_80: 0.0000 (0.0199)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0162)  time: 3.9225  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11]  [1723/1724]  eta: 0:00:03  lr: 0.000200  loss: 2.3674 (3.0031)  loss_n_40: 0.4625 (0.6136)  loss_n_60: 0.5965 (0.6687)  loss_n_80: 0.6134 (0.7713)  loss_n_100: 0.7096 (0.8507)  triple_100: 0.0000 (0.0465)  triple_80: 0.0000 (0.0199)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0161)  time: 3.9218  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:11] Total time: 1:52:41 (3.9218 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 2.3674 (3.0031)  loss_n_40: 0.4625 (0.6136)  loss_n_60: 0.5965 (0.6687)  loss_n_80: 0.6134 (0.7713)  loss_n_100: 0.7096 (0.8507)  triple_100: 0.0000 (0.0465)  triple_80: 0.0000 (0.0199)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0161)\n",
      "Valid: [epoch:11]  [  0/845]  eta: 0:10:54  loss: 2.8619 (2.8619)  loss_n_40: 0.5343 (0.5343)  loss_n_60: 0.7113 (0.7113)  loss_n_80: 0.7726 (0.7726)  loss_n_100: 0.8436 (0.8436)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7749  data: 0.4395  max mem: 46473\n",
      "Valid: [epoch:11]  [ 10/845]  eta: 0:05:12  loss: 2.6889 (2.6334)  loss_n_40: 0.4717 (0.4956)  loss_n_60: 0.6981 (0.6723)  loss_n_80: 0.7239 (0.7112)  loss_n_100: 0.7477 (0.7543)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3740  data: 0.0401  max mem: 46473\n",
      "Valid: [epoch:11]  [ 20/845]  eta: 0:04:52  loss: 2.4751 (2.6060)  loss_n_40: 0.4431 (0.5098)  loss_n_60: 0.6776 (0.6470)  loss_n_80: 0.6680 (0.6997)  loss_n_100: 0.7176 (0.7495)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [ 30/845]  eta: 0:04:44  loss: 2.3688 (2.5968)  loss_n_40: 0.4431 (0.5257)  loss_n_60: 0.5684 (0.6318)  loss_n_80: 0.6281 (0.6946)  loss_n_100: 0.7003 (0.7447)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [ 40/845]  eta: 0:04:37  loss: 2.2986 (2.5352)  loss_n_40: 0.4182 (0.5039)  loss_n_60: 0.5717 (0.6192)  loss_n_80: 0.6201 (0.6799)  loss_n_100: 0.6918 (0.7322)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:11]  [ 50/845]  eta: 0:04:32  loss: 2.3009 (2.5737)  loss_n_40: 0.4569 (0.5238)  loss_n_60: 0.5904 (0.6243)  loss_n_80: 0.6359 (0.6873)  loss_n_100: 0.6751 (0.7382)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:11]  [ 60/845]  eta: 0:04:28  loss: 2.4865 (2.5675)  loss_n_40: 0.4639 (0.5123)  loss_n_60: 0.6190 (0.6266)  loss_n_80: 0.6856 (0.6886)  loss_n_100: 0.7362 (0.7401)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [ 70/845]  eta: 0:04:23  loss: 2.4865 (2.5464)  loss_n_40: 0.4374 (0.5050)  loss_n_60: 0.6151 (0.6261)  loss_n_80: 0.6746 (0.6827)  loss_n_100: 0.7362 (0.7325)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [ 80/845]  eta: 0:04:20  loss: 2.3008 (2.5249)  loss_n_40: 0.4386 (0.5012)  loss_n_60: 0.5942 (0.6229)  loss_n_80: 0.6505 (0.6760)  loss_n_100: 0.6558 (0.7248)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [ 90/845]  eta: 0:04:16  loss: 2.2188 (2.4930)  loss_n_40: 0.3986 (0.4950)  loss_n_60: 0.5649 (0.6184)  loss_n_80: 0.6106 (0.6643)  loss_n_100: 0.6372 (0.7153)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:11]  [100/845]  eta: 0:04:12  loss: 2.2697 (2.5051)  loss_n_40: 0.4331 (0.5119)  loss_n_60: 0.5657 (0.6161)  loss_n_80: 0.5938 (0.6619)  loss_n_100: 0.6531 (0.7152)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [110/845]  eta: 0:04:08  loss: 2.2965 (2.5034)  loss_n_40: 0.4609 (0.5191)  loss_n_60: 0.5690 (0.6155)  loss_n_80: 0.5996 (0.6578)  loss_n_100: 0.6552 (0.7110)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [120/845]  eta: 0:04:05  loss: 2.2789 (2.4966)  loss_n_40: 0.4247 (0.5156)  loss_n_60: 0.5809 (0.6138)  loss_n_80: 0.6050 (0.6567)  loss_n_100: 0.6657 (0.7104)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [130/845]  eta: 0:04:01  loss: 2.3742 (2.5156)  loss_n_40: 0.4375 (0.5138)  loss_n_60: 0.6034 (0.6154)  loss_n_80: 0.6270 (0.6650)  loss_n_100: 0.6912 (0.7214)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:11]  [140/845]  eta: 0:03:58  loss: 2.5198 (2.5209)  loss_n_40: 0.4535 (0.5158)  loss_n_60: 0.6044 (0.6150)  loss_n_80: 0.6886 (0.6657)  loss_n_100: 0.7779 (0.7243)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:11]  [150/845]  eta: 0:03:54  loss: 2.2300 (2.4985)  loss_n_40: 0.4280 (0.5109)  loss_n_60: 0.5809 (0.6118)  loss_n_80: 0.6304 (0.6592)  loss_n_100: 0.6897 (0.7166)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [160/845]  eta: 0:03:51  loss: 2.3533 (2.5169)  loss_n_40: 0.4220 (0.5143)  loss_n_60: 0.6016 (0.6167)  loss_n_80: 0.6332 (0.6633)  loss_n_100: 0.7201 (0.7227)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [170/845]  eta: 0:03:47  loss: 2.6084 (2.5211)  loss_n_40: 0.4614 (0.5136)  loss_n_60: 0.6328 (0.6173)  loss_n_80: 0.6885 (0.6654)  loss_n_100: 0.7613 (0.7248)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [180/845]  eta: 0:03:44  loss: 2.3565 (2.5093)  loss_n_40: 0.4142 (0.5091)  loss_n_60: 0.6328 (0.6167)  loss_n_80: 0.6504 (0.6623)  loss_n_100: 0.6491 (0.7212)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [190/845]  eta: 0:03:40  loss: 2.2851 (2.5134)  loss_n_40: 0.4142 (0.5084)  loss_n_60: 0.5862 (0.6166)  loss_n_80: 0.6435 (0.6653)  loss_n_100: 0.6796 (0.7231)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [200/845]  eta: 0:03:37  loss: 2.3107 (2.5119)  loss_n_40: 0.4727 (0.5071)  loss_n_60: 0.6175 (0.6174)  loss_n_80: 0.6347 (0.6646)  loss_n_100: 0.6796 (0.7228)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [210/845]  eta: 0:03:33  loss: 2.3544 (2.5140)  loss_n_40: 0.4727 (0.5061)  loss_n_60: 0.6275 (0.6171)  loss_n_80: 0.6347 (0.6654)  loss_n_100: 0.7088 (0.7254)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [220/845]  eta: 0:03:30  loss: 2.5540 (2.5250)  loss_n_40: 0.4598 (0.5106)  loss_n_60: 0.6391 (0.6185)  loss_n_80: 0.6963 (0.6679)  loss_n_100: 0.7152 (0.7279)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [230/845]  eta: 0:03:26  loss: 2.5162 (2.5250)  loss_n_40: 0.4489 (0.5110)  loss_n_60: 0.6143 (0.6184)  loss_n_80: 0.6612 (0.6676)  loss_n_100: 0.7397 (0.7281)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [240/845]  eta: 0:03:23  loss: 2.3303 (2.5160)  loss_n_40: 0.3939 (0.5065)  loss_n_60: 0.5905 (0.6167)  loss_n_80: 0.6256 (0.6651)  loss_n_100: 0.6833 (0.7276)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [250/845]  eta: 0:03:20  loss: 2.3045 (2.5153)  loss_n_40: 0.4076 (0.5056)  loss_n_60: 0.5889 (0.6176)  loss_n_80: 0.5866 (0.6646)  loss_n_100: 0.6676 (0.7276)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [260/845]  eta: 0:03:16  loss: 2.4714 (2.5173)  loss_n_40: 0.4498 (0.5045)  loss_n_60: 0.6298 (0.6179)  loss_n_80: 0.6695 (0.6659)  loss_n_100: 0.7350 (0.7290)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [270/845]  eta: 0:03:13  loss: 2.4255 (2.5162)  loss_n_40: 0.4591 (0.5025)  loss_n_60: 0.6210 (0.6181)  loss_n_80: 0.6496 (0.6659)  loss_n_100: 0.7265 (0.7298)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [280/845]  eta: 0:03:09  loss: 2.2876 (2.5058)  loss_n_40: 0.4224 (0.4993)  loss_n_60: 0.5723 (0.6164)  loss_n_80: 0.6094 (0.6633)  loss_n_100: 0.6650 (0.7268)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [290/845]  eta: 0:03:06  loss: 2.2876 (2.5095)  loss_n_40: 0.4224 (0.5024)  loss_n_60: 0.5614 (0.6161)  loss_n_80: 0.5890 (0.6631)  loss_n_100: 0.6590 (0.7279)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [300/845]  eta: 0:03:03  loss: 2.4384 (2.5185)  loss_n_40: 0.4749 (0.5095)  loss_n_60: 0.5762 (0.6161)  loss_n_80: 0.6707 (0.6640)  loss_n_100: 0.6919 (0.7289)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [310/845]  eta: 0:02:59  loss: 2.3368 (2.5139)  loss_n_40: 0.4361 (0.5073)  loss_n_60: 0.5752 (0.6155)  loss_n_80: 0.6149 (0.6634)  loss_n_100: 0.6731 (0.7277)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [320/845]  eta: 0:02:56  loss: 2.3015 (2.5117)  loss_n_40: 0.4225 (0.5061)  loss_n_60: 0.5869 (0.6147)  loss_n_80: 0.6330 (0.6636)  loss_n_100: 0.6660 (0.7273)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [330/845]  eta: 0:02:52  loss: 2.3015 (2.5117)  loss_n_40: 0.4363 (0.5049)  loss_n_60: 0.5697 (0.6147)  loss_n_80: 0.6330 (0.6638)  loss_n_100: 0.6693 (0.7282)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [340/845]  eta: 0:02:49  loss: 2.4022 (2.5117)  loss_n_40: 0.4638 (0.5044)  loss_n_60: 0.6126 (0.6151)  loss_n_80: 0.6462 (0.6640)  loss_n_100: 0.7322 (0.7282)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [350/845]  eta: 0:02:46  loss: 2.4140 (2.5153)  loss_n_40: 0.4638 (0.5066)  loss_n_60: 0.6212 (0.6153)  loss_n_80: 0.6462 (0.6641)  loss_n_100: 0.7371 (0.7293)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [360/845]  eta: 0:02:42  loss: 2.4649 (2.5142)  loss_n_40: 0.4588 (0.5052)  loss_n_60: 0.5889 (0.6144)  loss_n_80: 0.6452 (0.6642)  loss_n_100: 0.7442 (0.7304)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [370/845]  eta: 0:02:39  loss: 2.3825 (2.5128)  loss_n_40: 0.4397 (0.5059)  loss_n_60: 0.5828 (0.6139)  loss_n_80: 0.6313 (0.6633)  loss_n_100: 0.7018 (0.7296)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [380/845]  eta: 0:02:36  loss: 2.3627 (2.5165)  loss_n_40: 0.4526 (0.5086)  loss_n_60: 0.6036 (0.6147)  loss_n_80: 0.6118 (0.6638)  loss_n_100: 0.6546 (0.7293)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:11]  [390/845]  eta: 0:02:32  loss: 2.3098 (2.5159)  loss_n_40: 0.4588 (0.5107)  loss_n_60: 0.6036 (0.6140)  loss_n_80: 0.6217 (0.6628)  loss_n_100: 0.6622 (0.7283)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [400/845]  eta: 0:02:29  loss: 2.4489 (2.5135)  loss_n_40: 0.4753 (0.5099)  loss_n_60: 0.5977 (0.6132)  loss_n_80: 0.6439 (0.6622)  loss_n_100: 0.6979 (0.7282)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [410/845]  eta: 0:02:25  loss: 2.3938 (2.5106)  loss_n_40: 0.4621 (0.5090)  loss_n_60: 0.5977 (0.6126)  loss_n_80: 0.6439 (0.6616)  loss_n_100: 0.6979 (0.7273)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [420/845]  eta: 0:02:22  loss: 2.2531 (2.5080)  loss_n_40: 0.4439 (0.5080)  loss_n_60: 0.5891 (0.6126)  loss_n_80: 0.5855 (0.6611)  loss_n_100: 0.6397 (0.7262)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [430/845]  eta: 0:02:19  loss: 2.2387 (2.5082)  loss_n_40: 0.4136 (0.5065)  loss_n_60: 0.5699 (0.6126)  loss_n_80: 0.6235 (0.6619)  loss_n_100: 0.6610 (0.7270)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [440/845]  eta: 0:02:15  loss: 2.4916 (2.5096)  loss_n_40: 0.4406 (0.5061)  loss_n_60: 0.5997 (0.6132)  loss_n_80: 0.6751 (0.6624)  loss_n_100: 0.7723 (0.7278)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [450/845]  eta: 0:02:12  loss: 2.4660 (2.5043)  loss_n_40: 0.4406 (0.5060)  loss_n_60: 0.6034 (0.6120)  loss_n_80: 0.6407 (0.6605)  loss_n_100: 0.7256 (0.7256)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [460/845]  eta: 0:02:09  loss: 2.2673 (2.5014)  loss_n_40: 0.4121 (0.5050)  loss_n_60: 0.5718 (0.6116)  loss_n_80: 0.6159 (0.6598)  loss_n_100: 0.6409 (0.7248)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [470/845]  eta: 0:02:05  loss: 2.3904 (2.5050)  loss_n_40: 0.4312 (0.5047)  loss_n_60: 0.5926 (0.6119)  loss_n_80: 0.6584 (0.6617)  loss_n_100: 0.7487 (0.7266)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [480/845]  eta: 0:02:02  loss: 2.5628 (2.5044)  loss_n_40: 0.4265 (0.5033)  loss_n_60: 0.6167 (0.6122)  loss_n_80: 0.6646 (0.6619)  loss_n_100: 0.7535 (0.7269)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [490/845]  eta: 0:01:59  loss: 2.5104 (2.5069)  loss_n_40: 0.4271 (0.5039)  loss_n_60: 0.6123 (0.6127)  loss_n_80: 0.6831 (0.6624)  loss_n_100: 0.7826 (0.7278)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [500/845]  eta: 0:01:55  loss: 2.4993 (2.5103)  loss_n_40: 0.4273 (0.5040)  loss_n_60: 0.5977 (0.6132)  loss_n_80: 0.6434 (0.6638)  loss_n_100: 0.7390 (0.7291)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [510/845]  eta: 0:01:52  loss: 2.3347 (2.5102)  loss_n_40: 0.4289 (0.5038)  loss_n_60: 0.6009 (0.6135)  loss_n_80: 0.6434 (0.6640)  loss_n_100: 0.7040 (0.7288)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [520/845]  eta: 0:01:49  loss: 2.4109 (2.5128)  loss_n_40: 0.4511 (0.5036)  loss_n_60: 0.6068 (0.6138)  loss_n_80: 0.6556 (0.6653)  loss_n_100: 0.7121 (0.7300)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [530/845]  eta: 0:01:45  loss: 2.3921 (2.5095)  loss_n_40: 0.4609 (0.5024)  loss_n_60: 0.6068 (0.6135)  loss_n_80: 0.6459 (0.6644)  loss_n_100: 0.7121 (0.7291)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [540/845]  eta: 0:01:42  loss: 2.3919 (2.5090)  loss_n_40: 0.4567 (0.5028)  loss_n_60: 0.6171 (0.6133)  loss_n_80: 0.6300 (0.6643)  loss_n_100: 0.6971 (0.7286)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [550/845]  eta: 0:01:38  loss: 2.4615 (2.5130)  loss_n_40: 0.4567 (0.5058)  loss_n_60: 0.6085 (0.6134)  loss_n_80: 0.6583 (0.6648)  loss_n_100: 0.7073 (0.7288)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [560/845]  eta: 0:01:35  loss: 2.3273 (2.5094)  loss_n_40: 0.4535 (0.5052)  loss_n_60: 0.5936 (0.6128)  loss_n_80: 0.6217 (0.6636)  loss_n_100: 0.6745 (0.7276)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [570/845]  eta: 0:01:32  loss: 2.3002 (2.5067)  loss_n_40: 0.4420 (0.5046)  loss_n_60: 0.5936 (0.6128)  loss_n_80: 0.6060 (0.6626)  loss_n_100: 0.6807 (0.7266)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3351  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:11]  [580/845]  eta: 0:01:28  loss: 2.4483 (2.5058)  loss_n_40: 0.4490 (0.5036)  loss_n_60: 0.5917 (0.6124)  loss_n_80: 0.6010 (0.6627)  loss_n_100: 0.6862 (0.7270)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:11]  [590/845]  eta: 0:01:25  loss: 2.2284 (2.5016)  loss_n_40: 0.4023 (0.5020)  loss_n_60: 0.5621 (0.6115)  loss_n_80: 0.5978 (0.6618)  loss_n_100: 0.6862 (0.7263)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:11]  [600/845]  eta: 0:01:22  loss: 2.2797 (2.5011)  loss_n_40: 0.4007 (0.5030)  loss_n_60: 0.5563 (0.6110)  loss_n_80: 0.6100 (0.6611)  loss_n_100: 0.6897 (0.7259)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:11]  [610/845]  eta: 0:01:18  loss: 2.2799 (2.4998)  loss_n_40: 0.4265 (0.5037)  loss_n_60: 0.5683 (0.6108)  loss_n_80: 0.6100 (0.6604)  loss_n_100: 0.6817 (0.7247)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [620/845]  eta: 0:01:15  loss: 2.2799 (2.4997)  loss_n_40: 0.4230 (0.5051)  loss_n_60: 0.5890 (0.6109)  loss_n_80: 0.5801 (0.6598)  loss_n_100: 0.6220 (0.7239)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [630/845]  eta: 0:01:12  loss: 2.4630 (2.5000)  loss_n_40: 0.4623 (0.5048)  loss_n_60: 0.6013 (0.6112)  loss_n_80: 0.6481 (0.6598)  loss_n_100: 0.6641 (0.7241)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:11]  [640/845]  eta: 0:01:08  loss: 2.3314 (2.4989)  loss_n_40: 0.4228 (0.5038)  loss_n_60: 0.5936 (0.6114)  loss_n_80: 0.6420 (0.6598)  loss_n_100: 0.7156 (0.7238)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [650/845]  eta: 0:01:05  loss: 2.2979 (2.4960)  loss_n_40: 0.4122 (0.5026)  loss_n_60: 0.5919 (0.6110)  loss_n_80: 0.6306 (0.6590)  loss_n_100: 0.6879 (0.7234)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [660/845]  eta: 0:01:02  loss: 2.5899 (2.4993)  loss_n_40: 0.4769 (0.5039)  loss_n_60: 0.6082 (0.6110)  loss_n_80: 0.6833 (0.6599)  loss_n_100: 0.7244 (0.7244)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [670/845]  eta: 0:00:58  loss: 2.6616 (2.5035)  loss_n_40: 0.4947 (0.5049)  loss_n_60: 0.6163 (0.6114)  loss_n_80: 0.7079 (0.6611)  loss_n_100: 0.7831 (0.7261)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [680/845]  eta: 0:00:55  loss: 2.5160 (2.5072)  loss_n_40: 0.5011 (0.5063)  loss_n_60: 0.6319 (0.6126)  loss_n_80: 0.6545 (0.6615)  loss_n_100: 0.7345 (0.7267)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3350  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [690/845]  eta: 0:00:51  loss: 2.3560 (2.5074)  loss_n_40: 0.4576 (0.5067)  loss_n_60: 0.6319 (0.6125)  loss_n_80: 0.6419 (0.6616)  loss_n_100: 0.7125 (0.7265)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [700/845]  eta: 0:00:48  loss: 2.4815 (2.5073)  loss_n_40: 0.4275 (0.5059)  loss_n_60: 0.5784 (0.6127)  loss_n_80: 0.6688 (0.6619)  loss_n_100: 0.7228 (0.7268)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [710/845]  eta: 0:00:45  loss: 2.3592 (2.5078)  loss_n_40: 0.4302 (0.5064)  loss_n_60: 0.5808 (0.6125)  loss_n_80: 0.6506 (0.6620)  loss_n_100: 0.6929 (0.7269)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0001)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [720/845]  eta: 0:00:41  loss: 2.2407 (2.5111)  loss_n_40: 0.4336 (0.5074)  loss_n_60: 0.5750 (0.6132)  loss_n_80: 0.5975 (0.6629)  loss_n_100: 0.6839 (0.7276)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [730/845]  eta: 0:00:38  loss: 2.3266 (2.5125)  loss_n_40: 0.4323 (0.5085)  loss_n_60: 0.6060 (0.6137)  loss_n_80: 0.6160 (0.6630)  loss_n_100: 0.6624 (0.7271)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [740/845]  eta: 0:00:35  loss: 2.3834 (2.5108)  loss_n_40: 0.4242 (0.5077)  loss_n_60: 0.5852 (0.6132)  loss_n_80: 0.6422 (0.6628)  loss_n_100: 0.6624 (0.7270)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [750/845]  eta: 0:00:31  loss: 2.3056 (2.5066)  loss_n_40: 0.4105 (0.5063)  loss_n_60: 0.5321 (0.6124)  loss_n_80: 0.6285 (0.6619)  loss_n_100: 0.6770 (0.7259)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [760/845]  eta: 0:00:28  loss: 2.3582 (2.5076)  loss_n_40: 0.4289 (0.5064)  loss_n_60: 0.5783 (0.6125)  loss_n_80: 0.6285 (0.6624)  loss_n_100: 0.7024 (0.7263)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [770/845]  eta: 0:00:25  loss: 2.5955 (2.5082)  loss_n_40: 0.4660 (0.5059)  loss_n_60: 0.6196 (0.6127)  loss_n_80: 0.6937 (0.6628)  loss_n_100: 0.7364 (0.7267)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [780/845]  eta: 0:00:21  loss: 2.8396 (2.5146)  loss_n_40: 0.5502 (0.5072)  loss_n_60: 0.6715 (0.6137)  loss_n_80: 0.7469 (0.6649)  loss_n_100: 0.8254 (0.7287)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [790/845]  eta: 0:00:18  loss: 2.8635 (2.5156)  loss_n_40: 0.5480 (0.5069)  loss_n_60: 0.6854 (0.6141)  loss_n_80: 0.7469 (0.6654)  loss_n_100: 0.7927 (0.7292)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [800/845]  eta: 0:00:15  loss: 2.6400 (2.5198)  loss_n_40: 0.4594 (0.5102)  loss_n_60: 0.6415 (0.6144)  loss_n_80: 0.6867 (0.6657)  loss_n_100: 0.7514 (0.7294)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [810/845]  eta: 0:00:11  loss: 2.5136 (2.5193)  loss_n_40: 0.4195 (0.5106)  loss_n_60: 0.6182 (0.6140)  loss_n_80: 0.6332 (0.6655)  loss_n_100: 0.7071 (0.7292)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [820/845]  eta: 0:00:08  loss: 2.3566 (2.5211)  loss_n_40: 0.4252 (0.5118)  loss_n_60: 0.5839 (0.6147)  loss_n_80: 0.6297 (0.6654)  loss_n_100: 0.6758 (0.7291)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [830/845]  eta: 0:00:05  loss: 2.3410 (2.5203)  loss_n_40: 0.4128 (0.5122)  loss_n_60: 0.5969 (0.6147)  loss_n_80: 0.6208 (0.6650)  loss_n_100: 0.6430 (0.7284)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [840/845]  eta: 0:00:01  loss: 2.3076 (2.5189)  loss_n_40: 0.4128 (0.5114)  loss_n_60: 0.6021 (0.6145)  loss_n_80: 0.6208 (0.6648)  loss_n_100: 0.6430 (0.7282)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11]  [844/845]  eta: 0:00:00  loss: 2.3076 (2.5181)  loss_n_40: 0.4128 (0.5111)  loss_n_60: 0.6021 (0.6144)  loss_n_80: 0.6214 (0.6646)  loss_n_100: 0.6700 (0.7280)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:11] Total time: 0:04:43 (0.3353 s / it)\n",
      "Averaged stats: loss: 2.3076 (2.5181)  loss_n_40: 0.4128 (0.5111)  loss_n_60: 0.6021 (0.6144)  loss_n_80: 0.6214 (0.6646)  loss_n_100: 0.6700 (0.7280)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_11_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.728%\n",
      "Min loss_n_100: 0.575\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:12]  [   0/1724]  eta: 1:59:32  lr: 0.000200  loss: 2.7220 (2.7220)  loss_n_40: 0.4621 (0.4621)  loss_n_60: 0.6906 (0.6906)  loss_n_80: 0.7587 (0.7587)  loss_n_100: 0.8105 (0.8105)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1607  data: 0.4016  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [  10/1724]  eta: 1:52:36  lr: 0.000200  loss: 2.1824 (2.3529)  loss_n_40: 0.4639 (0.4906)  loss_n_60: 0.5608 (0.5821)  loss_n_80: 0.5631 (0.6134)  loss_n_100: 0.6187 (0.6668)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9421  data: 0.0367  max mem: 46473\n",
      "Train: [epoch:12]  [  20/1724]  eta: 1:51:41  lr: 0.000200  loss: 2.1536 (2.2850)  loss_n_40: 0.4639 (0.4921)  loss_n_60: 0.5418 (0.5668)  loss_n_80: 0.5591 (0.5820)  loss_n_100: 0.5973 (0.6442)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9213  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [  30/1724]  eta: 1:50:55  lr: 0.000200  loss: 2.0554 (2.2016)  loss_n_40: 0.4391 (0.4856)  loss_n_60: 0.5112 (0.5429)  loss_n_80: 0.5017 (0.5560)  loss_n_100: 0.5765 (0.6171)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [  40/1724]  eta: 1:50:13  lr: 0.000200  loss: 1.9698 (2.1532)  loss_n_40: 0.4202 (0.4742)  loss_n_60: 0.4780 (0.5292)  loss_n_80: 0.5017 (0.5455)  loss_n_100: 0.5689 (0.6043)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9215  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [  50/1724]  eta: 1:49:32  lr: 0.000200  loss: 2.2003 (2.3595)  loss_n_40: 0.4553 (0.4878)  loss_n_60: 0.5496 (0.5547)  loss_n_80: 0.5660 (0.5729)  loss_n_100: 0.6145 (0.6343)  triple_100: 0.0000 (0.0115)  triple_80: 0.0000 (0.0367)  triple_60: 0.0000 (0.0466)  triple_40: 0.0000 (0.0149)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [  60/1724]  eta: 1:48:51  lr: 0.000200  loss: 2.9525 (2.5298)  loss_n_40: 0.6142 (0.5260)  loss_n_60: 0.7034 (0.5840)  loss_n_80: 0.7322 (0.6108)  loss_n_100: 0.8131 (0.6770)  triple_100: 0.0000 (0.0096)  triple_80: 0.0000 (0.0434)  triple_60: 0.0000 (0.0665)  triple_40: 0.0000 (0.0124)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [  70/1724]  eta: 1:48:11  lr: 0.000200  loss: 2.8621 (2.5683)  loss_n_40: 0.6142 (0.5396)  loss_n_60: 0.6533 (0.5930)  loss_n_80: 0.7322 (0.6262)  loss_n_100: 0.8405 (0.6961)  triple_100: 0.0000 (0.0083)  triple_80: 0.0000 (0.0373)  triple_60: 0.0000 (0.0572)  triple_40: 0.0000 (0.0107)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [  80/1724]  eta: 1:47:31  lr: 0.000200  loss: 2.7148 (2.5913)  loss_n_40: 0.5750 (0.5528)  loss_n_60: 0.6143 (0.6012)  loss_n_80: 0.6855 (0.6342)  loss_n_100: 0.7727 (0.7037)  triple_100: 0.0000 (0.0073)  triple_80: 0.0000 (0.0327)  triple_60: 0.0000 (0.0501)  triple_40: 0.0000 (0.0094)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [  90/1724]  eta: 1:46:51  lr: 0.000200  loss: 2.4315 (2.5596)  loss_n_40: 0.5105 (0.5473)  loss_n_60: 0.5665 (0.5980)  loss_n_80: 0.6423 (0.6291)  loss_n_100: 0.6983 (0.6968)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0291)  triple_60: 0.0000 (0.0446)  triple_40: 0.0000 (0.0083)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 100/1724]  eta: 1:46:11  lr: 0.000200  loss: 2.3034 (2.5369)  loss_n_40: 0.4907 (0.5458)  loss_n_60: 0.5646 (0.5952)  loss_n_80: 0.5758 (0.6239)  loss_n_100: 0.6548 (0.6924)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0262)  triple_60: 0.0000 (0.0402)  triple_40: 0.0000 (0.0075)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 110/1724]  eta: 1:45:32  lr: 0.000200  loss: 2.3034 (2.5095)  loss_n_40: 0.4993 (0.5480)  loss_n_60: 0.5517 (0.5919)  loss_n_80: 0.5480 (0.6156)  loss_n_100: 0.6101 (0.6814)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0238)  triple_60: 0.0000 (0.0366)  triple_40: 0.0000 (0.0068)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 120/1724]  eta: 1:44:52  lr: 0.000200  loss: 1.9691 (2.4603)  loss_n_40: 0.4476 (0.5381)  loss_n_60: 0.4871 (0.5822)  loss_n_80: 0.4967 (0.6041)  loss_n_100: 0.5443 (0.6693)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0219)  triple_60: 0.0000 (0.0335)  triple_40: 0.0000 (0.0063)  time: 3.9221  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 130/1724]  eta: 1:44:13  lr: 0.000200  loss: 1.8356 (2.4134)  loss_n_40: 0.3963 (0.5289)  loss_n_60: 0.4641 (0.5729)  loss_n_80: 0.4662 (0.5929)  loss_n_100: 0.5083 (0.6572)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0202)  triple_60: 0.0000 (0.0310)  triple_40: 0.0000 (0.0058)  time: 3.9226  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 140/1724]  eta: 1:43:34  lr: 0.000200  loss: 1.7959 (2.3968)  loss_n_40: 0.3892 (0.5227)  loss_n_60: 0.4492 (0.5653)  loss_n_80: 0.4399 (0.5843)  loss_n_100: 0.5034 (0.6480)  triple_100: 0.0000 (0.0093)  triple_80: 0.0000 (0.0224)  triple_60: 0.0000 (0.0305)  triple_40: 0.0000 (0.0143)  time: 3.9227  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 150/1724]  eta: 1:42:54  lr: 0.000200  loss: 1.9885 (2.3915)  loss_n_40: 0.4209 (0.5203)  loss_n_60: 0.4867 (0.5655)  loss_n_80: 0.5001 (0.5844)  loss_n_100: 0.5395 (0.6474)  triple_100: 0.0000 (0.0087)  triple_80: 0.0000 (0.0210)  triple_60: 0.0000 (0.0310)  triple_40: 0.0000 (0.0134)  time: 3.9222  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 160/1724]  eta: 1:42:15  lr: 0.000200  loss: 2.4341 (2.4070)  loss_n_40: 0.5259 (0.5229)  loss_n_60: 0.6027 (0.5704)  loss_n_80: 0.5980 (0.5897)  loss_n_100: 0.6594 (0.6542)  triple_100: 0.0000 (0.0085)  triple_80: 0.0000 (0.0197)  triple_60: 0.0000 (0.0291)  triple_40: 0.0000 (0.0125)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 170/1724]  eta: 1:41:36  lr: 0.000200  loss: 2.4901 (2.4157)  loss_n_40: 0.5451 (0.5222)  loss_n_60: 0.6114 (0.5712)  loss_n_80: 0.6386 (0.5919)  loss_n_100: 0.7043 (0.6557)  triple_100: 0.0000 (0.0105)  triple_80: 0.0000 (0.0241)  triple_60: 0.0000 (0.0282)  triple_40: 0.0000 (0.0118)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 180/1724]  eta: 1:40:56  lr: 0.000200  loss: 2.3585 (2.4092)  loss_n_40: 0.5102 (0.5226)  loss_n_60: 0.5539 (0.5703)  loss_n_80: 0.6147 (0.5917)  loss_n_100: 0.6398 (0.6542)  triple_100: 0.0000 (0.0099)  triple_80: 0.0000 (0.0228)  triple_60: 0.0000 (0.0266)  triple_40: 0.0000 (0.0111)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 190/1724]  eta: 1:40:17  lr: 0.000200  loss: 2.1389 (2.3900)  loss_n_40: 0.4427 (0.5187)  loss_n_60: 0.5215 (0.5667)  loss_n_80: 0.5406 (0.5878)  loss_n_100: 0.5909 (0.6500)  triple_100: 0.0000 (0.0094)  triple_80: 0.0000 (0.0216)  triple_60: 0.0000 (0.0252)  triple_40: 0.0000 (0.0106)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 200/1724]  eta: 1:39:38  lr: 0.000200  loss: 2.0555 (2.3876)  loss_n_40: 0.4522 (0.5189)  loss_n_60: 0.5097 (0.5667)  loss_n_80: 0.5170 (0.5862)  loss_n_100: 0.5833 (0.6479)  triple_100: 0.0000 (0.0105)  triple_80: 0.0000 (0.0218)  triple_60: 0.0000 (0.0255)  triple_40: 0.0000 (0.0100)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 210/1724]  eta: 1:38:59  lr: 0.000200  loss: 1.9190 (2.3672)  loss_n_40: 0.4428 (0.5162)  loss_n_60: 0.4970 (0.5632)  loss_n_80: 0.4753 (0.5809)  loss_n_100: 0.5379 (0.6423)  triple_100: 0.0000 (0.0100)  triple_80: 0.0000 (0.0208)  triple_60: 0.0000 (0.0243)  triple_40: 0.0000 (0.0096)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 220/1724]  eta: 1:38:19  lr: 0.000200  loss: 1.9395 (2.3548)  loss_n_40: 0.4272 (0.5145)  loss_n_60: 0.4970 (0.5607)  loss_n_80: 0.4753 (0.5767)  loss_n_100: 0.5330 (0.6376)  triple_100: 0.0000 (0.0096)  triple_80: 0.0000 (0.0198)  triple_60: 0.0000 (0.0239)  triple_40: 0.0000 (0.0119)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 230/1724]  eta: 1:37:40  lr: 0.000200  loss: 2.0107 (2.3387)  loss_n_40: 0.4257 (0.5104)  loss_n_60: 0.5034 (0.5571)  loss_n_80: 0.4862 (0.5727)  loss_n_100: 0.5483 (0.6337)  triple_100: 0.0000 (0.0092)  triple_80: 0.0000 (0.0196)  triple_60: 0.0000 (0.0230)  triple_40: 0.0000 (0.0131)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 240/1724]  eta: 1:37:01  lr: 0.000200  loss: 1.9493 (2.3256)  loss_n_40: 0.4041 (0.5070)  loss_n_60: 0.4701 (0.5540)  loss_n_80: 0.5019 (0.5708)  loss_n_100: 0.5535 (0.6316)  triple_100: 0.0000 (0.0088)  triple_80: 0.0000 (0.0188)  triple_60: 0.0000 (0.0220)  triple_40: 0.0000 (0.0125)  time: 3.9221  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [ 250/1724]  eta: 1:36:22  lr: 0.000200  loss: 1.8719 (2.3051)  loss_n_40: 0.3815 (0.5023)  loss_n_60: 0.4473 (0.5492)  loss_n_80: 0.4918 (0.5668)  loss_n_100: 0.5412 (0.6271)  triple_100: 0.0000 (0.0084)  triple_80: 0.0000 (0.0180)  triple_60: 0.0000 (0.0211)  triple_40: 0.0000 (0.0120)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 260/1724]  eta: 1:35:42  lr: 0.000200  loss: 1.8722 (2.2998)  loss_n_40: 0.3588 (0.5000)  loss_n_60: 0.4437 (0.5474)  loss_n_80: 0.4955 (0.5653)  loss_n_100: 0.5423 (0.6253)  triple_100: 0.0000 (0.0093)  triple_80: 0.0000 (0.0187)  triple_60: 0.0000 (0.0222)  triple_40: 0.0000 (0.0116)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 270/1724]  eta: 1:35:03  lr: 0.000200  loss: 1.8976 (2.2824)  loss_n_40: 0.3921 (0.4975)  loss_n_60: 0.4646 (0.5439)  loss_n_80: 0.4955 (0.5609)  loss_n_100: 0.5359 (0.6205)  triple_100: 0.0000 (0.0090)  triple_80: 0.0000 (0.0181)  triple_60: 0.0000 (0.0214)  triple_40: 0.0000 (0.0111)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 280/1724]  eta: 1:34:24  lr: 0.000200  loss: 1.7518 (2.2645)  loss_n_40: 0.3921 (0.4939)  loss_n_60: 0.4322 (0.5401)  loss_n_80: 0.4250 (0.5568)  loss_n_100: 0.4838 (0.6162)  triple_100: 0.0000 (0.0087)  triple_80: 0.0000 (0.0174)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0107)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 290/1724]  eta: 1:33:45  lr: 0.000200  loss: 1.7837 (2.2895)  loss_n_40: 0.4070 (0.4943)  loss_n_60: 0.4409 (0.5390)  loss_n_80: 0.4416 (0.5556)  loss_n_100: 0.5073 (0.6151)  triple_100: 0.0000 (0.0149)  triple_80: 0.0000 (0.0218)  triple_60: 0.0000 (0.0260)  triple_40: 0.0000 (0.0228)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 300/1724]  eta: 1:33:05  lr: 0.000200  loss: 2.4951 (2.3010)  loss_n_40: 0.5849 (0.4990)  loss_n_60: 0.5626 (0.5412)  loss_n_80: 0.5971 (0.5586)  loss_n_100: 0.6821 (0.6195)  triple_100: 0.0000 (0.0144)  triple_80: 0.0000 (0.0211)  triple_60: 0.0000 (0.0252)  triple_40: 0.0000 (0.0220)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 310/1724]  eta: 1:32:26  lr: 0.000200  loss: 2.5015 (2.3054)  loss_n_40: 0.5849 (0.5013)  loss_n_60: 0.5742 (0.5420)  loss_n_80: 0.6183 (0.5599)  loss_n_100: 0.7086 (0.6217)  triple_100: 0.0000 (0.0139)  triple_80: 0.0000 (0.0204)  triple_60: 0.0000 (0.0243)  triple_40: 0.0000 (0.0218)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 320/1724]  eta: 1:31:47  lr: 0.000200  loss: 2.3377 (2.3076)  loss_n_40: 0.5357 (0.5037)  loss_n_60: 0.5439 (0.5421)  loss_n_80: 0.5849 (0.5605)  loss_n_100: 0.6673 (0.6233)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0198)  triple_60: 0.0000 (0.0236)  triple_40: 0.0000 (0.0211)  time: 3.9211  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 330/1724]  eta: 1:31:08  lr: 0.000200  loss: 2.2498 (2.3020)  loss_n_40: 0.5268 (0.5040)  loss_n_60: 0.5181 (0.5406)  loss_n_80: 0.5513 (0.5591)  loss_n_100: 0.6546 (0.6227)  triple_100: 0.0000 (0.0131)  triple_80: 0.0000 (0.0192)  triple_60: 0.0000 (0.0229)  triple_40: 0.0000 (0.0205)  time: 3.9227  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 340/1724]  eta: 1:30:28  lr: 0.000200  loss: 2.1061 (2.2938)  loss_n_40: 0.5046 (0.5043)  loss_n_60: 0.4738 (0.5386)  loss_n_80: 0.4951 (0.5568)  loss_n_100: 0.5780 (0.6207)  triple_100: 0.0000 (0.0127)  triple_80: 0.0000 (0.0186)  triple_60: 0.0000 (0.0222)  triple_40: 0.0000 (0.0199)  time: 3.9237  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 350/1724]  eta: 1:29:49  lr: 0.000200  loss: 1.9729 (2.2871)  loss_n_40: 0.4328 (0.5039)  loss_n_60: 0.4560 (0.5372)  loss_n_80: 0.4891 (0.5552)  loss_n_100: 0.5737 (0.6195)  triple_100: 0.0000 (0.0123)  triple_80: 0.0000 (0.0181)  triple_60: 0.0000 (0.0216)  triple_40: 0.0000 (0.0193)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 360/1724]  eta: 1:29:10  lr: 0.000200  loss: 1.9245 (2.2767)  loss_n_40: 0.4328 (0.5023)  loss_n_60: 0.4611 (0.5351)  loss_n_80: 0.4884 (0.5528)  loss_n_100: 0.5453 (0.6171)  triple_100: 0.0000 (0.0120)  triple_80: 0.0000 (0.0176)  triple_60: 0.0000 (0.0210)  triple_40: 0.0000 (0.0188)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 370/1724]  eta: 1:28:31  lr: 0.000200  loss: 1.8968 (2.2659)  loss_n_40: 0.4296 (0.5007)  loss_n_60: 0.4611 (0.5328)  loss_n_80: 0.4547 (0.5504)  loss_n_100: 0.5210 (0.6145)  triple_100: 0.0000 (0.0117)  triple_80: 0.0000 (0.0171)  triple_60: 0.0000 (0.0204)  triple_40: 0.0000 (0.0183)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 380/1724]  eta: 1:27:52  lr: 0.000200  loss: 1.7714 (2.2532)  loss_n_40: 0.3877 (0.4993)  loss_n_60: 0.4120 (0.5300)  loss_n_80: 0.4319 (0.5472)  loss_n_100: 0.4772 (0.6110)  triple_100: 0.0000 (0.0114)  triple_80: 0.0000 (0.0167)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0178)  time: 3.9240  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 390/1724]  eta: 1:27:12  lr: 0.000200  loss: 1.7223 (2.2396)  loss_n_40: 0.3828 (0.4967)  loss_n_60: 0.4055 (0.5271)  loss_n_80: 0.4276 (0.5441)  loss_n_100: 0.4723 (0.6077)  triple_100: 0.0000 (0.0111)  triple_80: 0.0000 (0.0162)  triple_60: 0.0000 (0.0194)  triple_40: 0.0000 (0.0173)  time: 3.9233  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 400/1724]  eta: 1:26:33  lr: 0.000200  loss: 1.7377 (2.2265)  loss_n_40: 0.3927 (0.4946)  loss_n_60: 0.4119 (0.5242)  loss_n_80: 0.4276 (0.5411)  loss_n_100: 0.4804 (0.6043)  triple_100: 0.0000 (0.0108)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0189)  triple_40: 0.0000 (0.0169)  time: 3.9225  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 410/1724]  eta: 1:25:54  lr: 0.000200  loss: 1.7543 (2.2158)  loss_n_40: 0.3927 (0.4928)  loss_n_60: 0.4119 (0.5216)  loss_n_80: 0.4125 (0.5383)  loss_n_100: 0.4755 (0.6011)  triple_100: 0.0000 (0.0105)  triple_80: 0.0000 (0.0154)  triple_60: 0.0000 (0.0184)  triple_40: 0.0000 (0.0176)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 420/1724]  eta: 1:25:15  lr: 0.000200  loss: 1.6507 (2.2039)  loss_n_40: 0.3852 (0.4908)  loss_n_60: 0.4044 (0.5189)  loss_n_80: 0.4074 (0.5356)  loss_n_100: 0.4611 (0.5981)  triple_100: 0.0000 (0.0103)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0180)  triple_40: 0.0000 (0.0172)  time: 3.9232  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 430/1724]  eta: 1:24:36  lr: 0.000200  loss: 1.6507 (2.1935)  loss_n_40: 0.4019 (0.4892)  loss_n_60: 0.4044 (0.5166)  loss_n_80: 0.4044 (0.5332)  loss_n_100: 0.4506 (0.5954)  triple_100: 0.0000 (0.0101)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0176)  triple_40: 0.0000 (0.0168)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 440/1724]  eta: 1:23:56  lr: 0.000200  loss: 1.6143 (2.1806)  loss_n_40: 0.3855 (0.4871)  loss_n_60: 0.3768 (0.5137)  loss_n_80: 0.3978 (0.5302)  loss_n_100: 0.4225 (0.5919)  triple_100: 0.0000 (0.0098)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0172)  triple_40: 0.0000 (0.0164)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 450/1724]  eta: 1:23:17  lr: 0.000200  loss: 1.5152 (2.1723)  loss_n_40: 0.3346 (0.4848)  loss_n_60: 0.3634 (0.5109)  loss_n_80: 0.3821 (0.5276)  loss_n_100: 0.4214 (0.5888)  triple_100: 0.0000 (0.0096)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0198)  time: 3.9246  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 460/1724]  eta: 1:22:38  lr: 0.000200  loss: 1.5257 (2.1640)  loss_n_40: 0.3564 (0.4833)  loss_n_60: 0.3634 (0.5092)  loss_n_80: 0.3851 (0.5259)  loss_n_100: 0.4366 (0.5867)  triple_100: 0.0000 (0.0094)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0194)  time: 3.9252  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 470/1724]  eta: 1:21:59  lr: 0.000200  loss: 1.6795 (2.1523)  loss_n_40: 0.3604 (0.4812)  loss_n_60: 0.4148 (0.5065)  loss_n_80: 0.4251 (0.5232)  loss_n_100: 0.4583 (0.5835)  triple_100: 0.0000 (0.0092)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0161)  triple_40: 0.0000 (0.0191)  time: 3.9258  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 480/1724]  eta: 1:21:20  lr: 0.000200  loss: 1.6433 (2.1413)  loss_n_40: 0.3571 (0.4789)  loss_n_60: 0.3901 (0.5041)  loss_n_80: 0.4032 (0.5208)  loss_n_100: 0.4565 (0.5809)  triple_100: 0.0000 (0.0090)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0187)  time: 3.9251  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [ 490/1724]  eta: 1:20:40  lr: 0.000200  loss: 1.5980 (2.1383)  loss_n_40: 0.3648 (0.4773)  loss_n_60: 0.3809 (0.5018)  loss_n_80: 0.4013 (0.5187)  loss_n_100: 0.4560 (0.5787)  triple_100: 0.0000 (0.0112)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0189)  triple_40: 0.0000 (0.0187)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 500/1724]  eta: 1:20:01  lr: 0.000200  loss: 1.8891 (2.1502)  loss_n_40: 0.4035 (0.4763)  loss_n_60: 0.4320 (0.5018)  loss_n_80: 0.4725 (0.5200)  loss_n_100: 0.5601 (0.5811)  triple_100: 0.0000 (0.0160)  triple_80: 0.0000 (0.0181)  triple_60: 0.0000 (0.0185)  triple_40: 0.0000 (0.0184)  time: 3.9247  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 510/1724]  eta: 1:19:22  lr: 0.000200  loss: 2.8246 (2.1801)  loss_n_40: 0.4877 (0.4792)  loss_n_60: 0.5579 (0.5068)  loss_n_80: 0.7032 (0.5272)  loss_n_100: 0.8204 (0.5890)  triple_100: 0.0000 (0.0172)  triple_80: 0.0000 (0.0200)  triple_60: 0.0000 (0.0216)  triple_40: 0.0000 (0.0191)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 520/1724]  eta: 1:18:43  lr: 0.000200  loss: 3.1220 (2.1951)  loss_n_40: 0.5911 (0.4816)  loss_n_60: 0.7372 (0.5111)  loss_n_80: 0.7695 (0.5316)  loss_n_100: 0.8694 (0.5941)  triple_100: 0.0000 (0.0169)  triple_80: 0.0000 (0.0197)  triple_60: 0.0000 (0.0214)  triple_40: 0.0000 (0.0187)  time: 3.9218  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 530/1724]  eta: 1:18:04  lr: 0.000200  loss: 2.8006 (2.2047)  loss_n_40: 0.5828 (0.4847)  loss_n_60: 0.6650 (0.5131)  loss_n_80: 0.7039 (0.5335)  loss_n_100: 0.7909 (0.5963)  triple_100: 0.0000 (0.0166)  triple_80: 0.0000 (0.0213)  triple_60: 0.0000 (0.0210)  triple_40: 0.0000 (0.0184)  time: 3.9219  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 540/1724]  eta: 1:17:24  lr: 0.000200  loss: 2.3909 (2.2058)  loss_n_40: 0.5092 (0.4854)  loss_n_60: 0.5716 (0.5134)  loss_n_80: 0.5696 (0.5339)  loss_n_100: 0.6777 (0.5970)  triple_100: 0.0000 (0.0163)  triple_80: 0.0000 (0.0213)  triple_60: 0.0000 (0.0206)  triple_40: 0.0000 (0.0180)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 550/1724]  eta: 1:16:45  lr: 0.000200  loss: 1.9996 (2.2009)  loss_n_40: 0.4431 (0.4844)  loss_n_60: 0.4792 (0.5125)  loss_n_80: 0.5065 (0.5329)  loss_n_100: 0.5708 (0.5959)  triple_100: 0.0000 (0.0163)  triple_80: 0.0000 (0.0210)  triple_60: 0.0000 (0.0202)  triple_40: 0.0000 (0.0177)  time: 3.9217  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 560/1724]  eta: 1:16:06  lr: 0.000200  loss: 1.9081 (2.1957)  loss_n_40: 0.4302 (0.4835)  loss_n_60: 0.4639 (0.5115)  loss_n_80: 0.4801 (0.5319)  loss_n_100: 0.5368 (0.5948)  triple_100: 0.0000 (0.0160)  triple_80: 0.0000 (0.0207)  triple_60: 0.0000 (0.0200)  triple_40: 0.0000 (0.0174)  time: 3.9218  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 570/1724]  eta: 1:15:27  lr: 0.000200  loss: 1.8110 (2.1893)  loss_n_40: 0.3937 (0.4825)  loss_n_60: 0.4291 (0.5102)  loss_n_80: 0.4544 (0.5305)  loss_n_100: 0.5286 (0.5934)  triple_100: 0.0000 (0.0157)  triple_80: 0.0000 (0.0203)  triple_60: 0.0000 (0.0196)  triple_40: 0.0000 (0.0171)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 580/1724]  eta: 1:14:47  lr: 0.000200  loss: 1.7962 (2.1855)  loss_n_40: 0.4081 (0.4827)  loss_n_60: 0.4291 (0.5093)  loss_n_80: 0.4467 (0.5296)  loss_n_100: 0.5095 (0.5924)  triple_100: 0.0000 (0.0154)  triple_80: 0.0000 (0.0200)  triple_60: 0.0000 (0.0193)  triple_40: 0.0000 (0.0168)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 590/1724]  eta: 1:14:08  lr: 0.000200  loss: 1.7962 (2.1803)  loss_n_40: 0.4092 (0.4815)  loss_n_60: 0.4399 (0.5083)  loss_n_80: 0.4529 (0.5284)  loss_n_100: 0.5015 (0.5911)  triple_100: 0.0000 (0.0159)  triple_80: 0.0000 (0.0198)  triple_60: 0.0000 (0.0190)  triple_40: 0.0000 (0.0165)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 600/1724]  eta: 1:13:29  lr: 0.000200  loss: 1.9359 (2.1797)  loss_n_40: 0.4092 (0.4808)  loss_n_60: 0.4499 (0.5076)  loss_n_80: 0.4753 (0.5277)  loss_n_100: 0.5256 (0.5903)  triple_100: 0.0000 (0.0156)  triple_80: 0.0000 (0.0194)  triple_60: 0.0000 (0.0221)  triple_40: 0.0000 (0.0162)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 610/1724]  eta: 1:12:50  lr: 0.000200  loss: 2.0158 (2.1774)  loss_n_40: 0.4076 (0.4798)  loss_n_60: 0.4838 (0.5073)  loss_n_80: 0.5130 (0.5277)  loss_n_100: 0.5764 (0.5904)  triple_100: 0.0000 (0.0153)  triple_80: 0.0000 (0.0191)  triple_60: 0.0000 (0.0217)  triple_40: 0.0000 (0.0159)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 620/1724]  eta: 1:12:10  lr: 0.000200  loss: 2.0122 (2.1722)  loss_n_40: 0.4054 (0.4789)  loss_n_60: 0.4505 (0.5062)  loss_n_80: 0.5130 (0.5268)  loss_n_100: 0.5732 (0.5894)  triple_100: 0.0000 (0.0151)  triple_80: 0.0000 (0.0188)  triple_60: 0.0000 (0.0214)  triple_40: 0.0000 (0.0157)  time: 3.9235  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 630/1724]  eta: 1:11:31  lr: 0.000200  loss: 1.7144 (2.1664)  loss_n_40: 0.3789 (0.4772)  loss_n_60: 0.4155 (0.5048)  loss_n_80: 0.4297 (0.5255)  loss_n_100: 0.4943 (0.5880)  triple_100: 0.0000 (0.0148)  triple_80: 0.0000 (0.0185)  triple_60: 0.0000 (0.0217)  triple_40: 0.0000 (0.0158)  time: 3.9231  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 640/1724]  eta: 1:10:52  lr: 0.000200  loss: 1.7381 (2.1613)  loss_n_40: 0.3859 (0.4764)  loss_n_60: 0.4214 (0.5037)  loss_n_80: 0.4297 (0.5245)  loss_n_100: 0.4976 (0.5869)  triple_100: 0.0000 (0.0146)  triple_80: 0.0000 (0.0182)  triple_60: 0.0000 (0.0213)  triple_40: 0.0000 (0.0156)  time: 3.9221  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 650/1724]  eta: 1:10:13  lr: 0.000200  loss: 1.7323 (2.1521)  loss_n_40: 0.3859 (0.4745)  loss_n_60: 0.3964 (0.5017)  loss_n_80: 0.4420 (0.5226)  loss_n_100: 0.4976 (0.5846)  triple_100: 0.0000 (0.0144)  triple_80: 0.0000 (0.0180)  triple_60: 0.0000 (0.0210)  triple_40: 0.0000 (0.0154)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 660/1724]  eta: 1:09:33  lr: 0.000200  loss: 1.8449 (2.1612)  loss_n_40: 0.3959 (0.4752)  loss_n_60: 0.4108 (0.5020)  loss_n_80: 0.4541 (0.5229)  loss_n_100: 0.5261 (0.5852)  triple_100: 0.0000 (0.0151)  triple_80: 0.0000 (0.0194)  triple_60: 0.0000 (0.0222)  triple_40: 0.0000 (0.0191)  time: 3.9221  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 670/1724]  eta: 1:08:54  lr: 0.000200  loss: 2.3662 (2.1688)  loss_n_40: 0.5170 (0.4772)  loss_n_60: 0.5427 (0.5034)  loss_n_80: 0.5847 (0.5249)  loss_n_100: 0.6449 (0.5879)  triple_100: 0.0000 (0.0149)  triple_80: 0.0000 (0.0191)  triple_60: 0.0000 (0.0219)  triple_40: 0.0000 (0.0195)  time: 3.9227  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 680/1724]  eta: 1:08:15  lr: 0.000200  loss: 2.6685 (2.1766)  loss_n_40: 0.5406 (0.4785)  loss_n_60: 0.5698 (0.5044)  loss_n_80: 0.6568 (0.5270)  loss_n_100: 0.8124 (0.5921)  triple_100: 0.0000 (0.0149)  triple_80: 0.0000 (0.0188)  triple_60: 0.0000 (0.0216)  triple_40: 0.0000 (0.0193)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 690/1724]  eta: 1:07:36  lr: 0.000200  loss: 2.4909 (2.1785)  loss_n_40: 0.5209 (0.4788)  loss_n_60: 0.5250 (0.5044)  loss_n_80: 0.6315 (0.5277)  loss_n_100: 0.7983 (0.5942)  triple_100: 0.0000 (0.0147)  triple_80: 0.0000 (0.0185)  triple_60: 0.0000 (0.0213)  triple_40: 0.0000 (0.0190)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 700/1724]  eta: 1:06:56  lr: 0.000200  loss: 2.2784 (2.1793)  loss_n_40: 0.4862 (0.4788)  loss_n_60: 0.4864 (0.5042)  loss_n_80: 0.5934 (0.5284)  loss_n_100: 0.7067 (0.5955)  triple_100: 0.0000 (0.0145)  triple_80: 0.0000 (0.0183)  triple_60: 0.0000 (0.0210)  triple_40: 0.0000 (0.0187)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 710/1724]  eta: 1:06:17  lr: 0.000200  loss: 2.1880 (2.1777)  loss_n_40: 0.4713 (0.4786)  loss_n_60: 0.4768 (0.5037)  loss_n_80: 0.5698 (0.5284)  loss_n_100: 0.6560 (0.5956)  triple_100: 0.0000 (0.0143)  triple_80: 0.0000 (0.0180)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0184)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 720/1724]  eta: 1:05:38  lr: 0.000200  loss: 1.8971 (2.1730)  loss_n_40: 0.4385 (0.4779)  loss_n_60: 0.4349 (0.5027)  loss_n_80: 0.4729 (0.5275)  loss_n_100: 0.5325 (0.5944)  triple_100: 0.0000 (0.0141)  triple_80: 0.0000 (0.0177)  triple_60: 0.0000 (0.0204)  triple_40: 0.0000 (0.0182)  time: 3.9224  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [ 730/1724]  eta: 1:04:59  lr: 0.000200  loss: 1.8682 (2.1704)  loss_n_40: 0.4041 (0.4773)  loss_n_60: 0.4297 (0.5019)  loss_n_80: 0.4633 (0.5268)  loss_n_100: 0.5184 (0.5937)  triple_100: 0.0000 (0.0142)  triple_80: 0.0000 (0.0177)  triple_60: 0.0000 (0.0209)  triple_40: 0.0000 (0.0179)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 740/1724]  eta: 1:04:20  lr: 0.000200  loss: 1.9113 (2.1671)  loss_n_40: 0.4226 (0.4771)  loss_n_60: 0.4260 (0.5009)  loss_n_80: 0.4637 (0.5261)  loss_n_100: 0.5408 (0.5932)  triple_100: 0.0000 (0.0140)  triple_80: 0.0000 (0.0175)  triple_60: 0.0000 (0.0206)  triple_40: 0.0000 (0.0177)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 750/1724]  eta: 1:03:40  lr: 0.000200  loss: 1.8704 (2.1626)  loss_n_40: 0.4241 (0.4766)  loss_n_60: 0.4179 (0.4998)  loss_n_80: 0.4567 (0.5251)  loss_n_100: 0.5381 (0.5922)  triple_100: 0.0000 (0.0138)  triple_80: 0.0000 (0.0172)  triple_60: 0.0000 (0.0203)  triple_40: 0.0000 (0.0175)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 760/1724]  eta: 1:03:01  lr: 0.000200  loss: 1.7704 (2.1571)  loss_n_40: 0.4000 (0.4756)  loss_n_60: 0.4054 (0.4985)  loss_n_80: 0.4414 (0.5240)  loss_n_100: 0.5125 (0.5910)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0170)  triple_60: 0.0000 (0.0200)  triple_40: 0.0000 (0.0172)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 770/1724]  eta: 1:02:22  lr: 0.000200  loss: 1.7647 (2.1512)  loss_n_40: 0.3913 (0.4745)  loss_n_60: 0.4005 (0.4973)  loss_n_80: 0.4414 (0.5227)  loss_n_100: 0.5073 (0.5896)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0168)  triple_60: 0.0000 (0.0198)  triple_40: 0.0000 (0.0170)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 780/1724]  eta: 1:01:43  lr: 0.000200  loss: 1.7579 (2.1464)  loss_n_40: 0.3831 (0.4743)  loss_n_60: 0.4005 (0.4962)  loss_n_80: 0.4445 (0.5215)  loss_n_100: 0.4903 (0.5882)  triple_100: 0.0000 (0.0133)  triple_80: 0.0000 (0.0166)  triple_60: 0.0000 (0.0196)  triple_40: 0.0000 (0.0168)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 790/1724]  eta: 1:01:03  lr: 0.000200  loss: 1.6552 (2.1410)  loss_n_40: 0.3950 (0.4736)  loss_n_60: 0.3883 (0.4949)  loss_n_80: 0.4100 (0.5202)  loss_n_100: 0.4612 (0.5868)  triple_100: 0.0000 (0.0131)  triple_80: 0.0000 (0.0164)  triple_60: 0.0000 (0.0193)  triple_40: 0.0000 (0.0166)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 800/1724]  eta: 1:00:24  lr: 0.000200  loss: 1.6512 (2.1397)  loss_n_40: 0.3849 (0.4731)  loss_n_60: 0.3824 (0.4936)  loss_n_80: 0.3961 (0.5187)  loss_n_100: 0.4612 (0.5854)  triple_100: 0.0000 (0.0130)  triple_80: 0.0000 (0.0168)  triple_60: 0.0000 (0.0200)  triple_40: 0.0000 (0.0191)  time: 3.9209  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 810/1724]  eta: 0:59:45  lr: 0.000200  loss: 2.1741 (2.1619)  loss_n_40: 0.5395 (0.4747)  loss_n_60: 0.4757 (0.4960)  loss_n_80: 0.5123 (0.5220)  loss_n_100: 0.5285 (0.5894)  triple_100: 0.0000 (0.0208)  triple_80: 0.0000 (0.0186)  triple_60: 0.0000 (0.0216)  triple_40: 0.0000 (0.0189)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 820/1724]  eta: 0:59:06  lr: 0.000200  loss: 3.5937 (2.1821)  loss_n_40: 0.6808 (0.4786)  loss_n_60: 0.7802 (0.5009)  loss_n_80: 0.8935 (0.5279)  loss_n_100: 1.0353 (0.5957)  triple_100: 0.0000 (0.0206)  triple_80: 0.0000 (0.0184)  triple_60: 0.0000 (0.0213)  triple_40: 0.0000 (0.0187)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 830/1724]  eta: 0:58:26  lr: 0.000200  loss: 3.5113 (2.1959)  loss_n_40: 0.7527 (0.4816)  loss_n_60: 0.7986 (0.5041)  loss_n_80: 0.9043 (0.5319)  loss_n_100: 1.0231 (0.6004)  triple_100: 0.0000 (0.0203)  triple_80: 0.0000 (0.0181)  triple_60: 0.0000 (0.0211)  triple_40: 0.0000 (0.0184)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 840/1724]  eta: 0:57:47  lr: 0.000200  loss: 3.0049 (2.2033)  loss_n_40: 0.5607 (0.4823)  loss_n_60: 0.6612 (0.5054)  loss_n_80: 0.7732 (0.5341)  loss_n_100: 0.9308 (0.6036)  triple_100: 0.0000 (0.0201)  triple_80: 0.0000 (0.0180)  triple_60: 0.0000 (0.0210)  triple_40: 0.0000 (0.0188)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 850/1724]  eta: 0:57:08  lr: 0.000200  loss: 2.8919 (2.2253)  loss_n_40: 0.5008 (0.4846)  loss_n_60: 0.6501 (0.5084)  loss_n_80: 0.7680 (0.5385)  loss_n_100: 0.9045 (0.6088)  triple_100: 0.0000 (0.0233)  triple_80: 0.0000 (0.0210)  triple_60: 0.0000 (0.0208)  triple_40: 0.0000 (0.0198)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 860/1724]  eta: 0:56:29  lr: 0.000200  loss: 4.3073 (2.2536)  loss_n_40: 0.8170 (0.4896)  loss_n_60: 0.9472 (0.5146)  loss_n_80: 1.1354 (0.5466)  loss_n_100: 1.2452 (0.6185)  triple_100: 0.0000 (0.0234)  triple_80: 0.0000 (0.0208)  triple_60: 0.0000 (0.0206)  triple_40: 0.0000 (0.0196)  time: 3.9223  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [ 870/1724]  eta: 0:55:49  lr: 0.000200  loss: 4.2639 (2.2732)  loss_n_40: 0.8271 (0.4928)  loss_n_60: 0.9927 (0.5192)  loss_n_80: 1.1405 (0.5526)  loss_n_100: 1.2862 (0.6253)  triple_100: 0.0000 (0.0231)  triple_80: 0.0000 (0.0205)  triple_60: 0.0000 (0.0204)  triple_40: 0.0000 (0.0194)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 880/1724]  eta: 0:55:10  lr: 0.000200  loss: 3.4643 (2.2847)  loss_n_40: 0.7264 (0.4954)  loss_n_60: 0.7864 (0.5218)  loss_n_80: 0.9304 (0.5560)  loss_n_100: 1.0407 (0.6291)  triple_100: 0.0000 (0.0228)  triple_80: 0.0000 (0.0203)  triple_60: 0.0000 (0.0201)  triple_40: 0.0000 (0.0192)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 890/1724]  eta: 0:54:31  lr: 0.000200  loss: 2.9593 (2.2887)  loss_n_40: 0.5925 (0.4961)  loss_n_60: 0.6608 (0.5227)  loss_n_80: 0.7869 (0.5575)  loss_n_100: 0.8972 (0.6309)  triple_100: 0.0000 (0.0226)  triple_80: 0.0000 (0.0201)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0189)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 900/1724]  eta: 0:53:52  lr: 0.000200  loss: 2.4603 (2.2906)  loss_n_40: 0.4645 (0.4961)  loss_n_60: 0.5594 (0.5230)  loss_n_80: 0.6300 (0.5582)  loss_n_100: 0.6968 (0.6316)  triple_100: 0.0000 (0.0224)  triple_80: 0.0000 (0.0203)  triple_60: 0.0000 (0.0203)  triple_40: 0.0000 (0.0187)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 910/1724]  eta: 0:53:13  lr: 0.000200  loss: 2.3209 (2.2917)  loss_n_40: 0.4753 (0.4961)  loss_n_60: 0.5352 (0.5234)  loss_n_80: 0.5958 (0.5588)  loss_n_100: 0.6865 (0.6324)  triple_100: 0.0000 (0.0222)  triple_80: 0.0000 (0.0201)  triple_60: 0.0000 (0.0201)  triple_40: 0.0000 (0.0185)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 920/1724]  eta: 0:52:33  lr: 0.000200  loss: 2.3078 (2.2914)  loss_n_40: 0.4780 (0.4967)  loss_n_60: 0.5352 (0.5236)  loss_n_80: 0.5895 (0.5589)  loss_n_100: 0.6318 (0.6320)  triple_100: 0.0000 (0.0220)  triple_80: 0.0000 (0.0199)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0183)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 930/1724]  eta: 0:51:54  lr: 0.000200  loss: 2.1228 (2.3025)  loss_n_40: 0.4668 (0.4968)  loss_n_60: 0.5012 (0.5239)  loss_n_80: 0.5348 (0.5594)  loss_n_100: 0.5854 (0.6325)  triple_100: 0.0000 (0.0259)  triple_80: 0.0000 (0.0228)  triple_60: 0.0000 (0.0230)  triple_40: 0.0000 (0.0183)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 940/1724]  eta: 0:51:15  lr: 0.000200  loss: 4.2280 (2.3803)  loss_n_40: 0.7720 (0.5038)  loss_n_60: 0.8948 (0.5339)  loss_n_80: 0.9761 (0.5726)  loss_n_100: 1.1877 (0.6470)  triple_100: 0.0000 (0.0492)  triple_80: 0.0000 (0.0309)  triple_60: 0.0000 (0.0239)  triple_40: 0.0000 (0.0190)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 950/1724]  eta: 0:50:36  lr: 0.000200  loss: 7.6055 (2.4319)  loss_n_40: 1.2953 (0.5136)  loss_n_60: 1.6070 (0.5462)  loss_n_80: 1.9195 (0.5873)  loss_n_100: 1.9992 (0.6623)  triple_100: 0.0000 (0.0487)  triple_80: 0.0000 (0.0306)  triple_60: 0.0000 (0.0243)  triple_40: 0.0000 (0.0188)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 960/1724]  eta: 0:49:56  lr: 0.000200  loss: 6.4730 (2.4696)  loss_n_40: 1.3004 (0.5216)  loss_n_60: 1.4389 (0.5551)  loss_n_80: 1.7137 (0.5982)  loss_n_100: 1.8523 (0.6736)  triple_100: 0.0000 (0.0482)  triple_80: 0.0000 (0.0303)  triple_60: 0.0000 (0.0241)  triple_40: 0.0000 (0.0186)  time: 3.9211  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [ 970/1724]  eta: 0:49:17  lr: 0.000200  loss: 5.2878 (2.4934)  loss_n_40: 1.0908 (0.5263)  loss_n_60: 1.1603 (0.5602)  loss_n_80: 1.4772 (0.6054)  loss_n_100: 1.6267 (0.6815)  triple_100: 0.0000 (0.0477)  triple_80: 0.0000 (0.0300)  triple_60: 0.0000 (0.0238)  triple_40: 0.0000 (0.0184)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 980/1724]  eta: 0:48:38  lr: 0.000200  loss: 4.5908 (2.5121)  loss_n_40: 0.9187 (0.5302)  loss_n_60: 1.0121 (0.5641)  loss_n_80: 1.2127 (0.6111)  loss_n_100: 1.3658 (0.6880)  triple_100: 0.0000 (0.0472)  triple_80: 0.0000 (0.0297)  triple_60: 0.0000 (0.0236)  triple_40: 0.0000 (0.0182)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [ 990/1724]  eta: 0:47:59  lr: 0.000200  loss: 3.9713 (2.5227)  loss_n_40: 0.7774 (0.5325)  loss_n_60: 0.8373 (0.5660)  loss_n_80: 1.0490 (0.6147)  loss_n_100: 1.1628 (0.6920)  triple_100: 0.0000 (0.0467)  triple_80: 0.0000 (0.0294)  triple_60: 0.0000 (0.0233)  triple_40: 0.0000 (0.0180)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1000/1724]  eta: 0:47:19  lr: 0.000200  loss: 3.4000 (2.5318)  loss_n_40: 0.6705 (0.5342)  loss_n_60: 0.6937 (0.5678)  loss_n_80: 0.9506 (0.6181)  loss_n_100: 1.0841 (0.6954)  triple_100: 0.0000 (0.0463)  triple_80: 0.0000 (0.0291)  triple_60: 0.0000 (0.0231)  triple_40: 0.0000 (0.0179)  time: 3.9155  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1010/1724]  eta: 0:46:40  lr: 0.000200  loss: 3.1553 (2.5360)  loss_n_40: 0.6078 (0.5349)  loss_n_60: 0.6706 (0.5685)  loss_n_80: 0.8287 (0.6200)  loss_n_100: 0.9085 (0.6974)  triple_100: 0.0000 (0.0458)  triple_80: 0.0000 (0.0288)  triple_60: 0.0000 (0.0229)  triple_40: 0.0000 (0.0177)  time: 3.9153  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1020/1724]  eta: 0:46:01  lr: 0.000200  loss: 2.8501 (2.5392)  loss_n_40: 0.5508 (0.5354)  loss_n_60: 0.6119 (0.5690)  loss_n_80: 0.7943 (0.6217)  loss_n_100: 0.8779 (0.6990)  triple_100: 0.0000 (0.0454)  triple_80: 0.0000 (0.0285)  triple_60: 0.0000 (0.0226)  triple_40: 0.0000 (0.0175)  time: 3.9147  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [1030/1724]  eta: 0:45:21  lr: 0.000200  loss: 2.7565 (2.5422)  loss_n_40: 0.5955 (0.5364)  loss_n_60: 0.6044 (0.5695)  loss_n_80: 0.7522 (0.6231)  loss_n_100: 0.8227 (0.7003)  triple_100: 0.0000 (0.0449)  triple_80: 0.0000 (0.0282)  triple_60: 0.0000 (0.0224)  triple_40: 0.0000 (0.0173)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1040/1724]  eta: 0:44:42  lr: 0.000200  loss: 2.5235 (2.5397)  loss_n_40: 0.5227 (0.5358)  loss_n_60: 0.5452 (0.5691)  loss_n_80: 0.6872 (0.6230)  loss_n_100: 0.7478 (0.6999)  triple_100: 0.0000 (0.0445)  triple_80: 0.0000 (0.0280)  triple_60: 0.0000 (0.0222)  triple_40: 0.0000 (0.0172)  time: 3.9154  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1050/1724]  eta: 0:44:03  lr: 0.000200  loss: 2.2319 (2.5369)  loss_n_40: 0.4494 (0.5355)  loss_n_60: 0.5327 (0.5687)  loss_n_80: 0.6097 (0.6226)  loss_n_100: 0.6514 (0.6993)  triple_100: 0.0000 (0.0441)  triple_80: 0.0000 (0.0277)  triple_60: 0.0000 (0.0220)  triple_40: 0.0000 (0.0170)  time: 3.9150  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1060/1724]  eta: 0:43:24  lr: 0.000200  loss: 2.0777 (2.5319)  loss_n_40: 0.4362 (0.5345)  loss_n_60: 0.5060 (0.5679)  loss_n_80: 0.5485 (0.6218)  loss_n_100: 0.6002 (0.6981)  triple_100: 0.0000 (0.0436)  triple_80: 0.0000 (0.0274)  triple_60: 0.0000 (0.0218)  triple_40: 0.0000 (0.0169)  time: 3.9150  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1070/1724]  eta: 0:42:44  lr: 0.000200  loss: 2.0201 (2.5292)  loss_n_40: 0.4190 (0.5338)  loss_n_60: 0.4752 (0.5673)  loss_n_80: 0.5275 (0.6212)  loss_n_100: 0.5688 (0.6971)  triple_100: 0.0000 (0.0432)  triple_80: 0.0000 (0.0274)  triple_60: 0.0000 (0.0221)  triple_40: 0.0000 (0.0171)  time: 3.9145  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1080/1724]  eta: 0:42:05  lr: 0.000200  loss: 2.2593 (2.5286)  loss_n_40: 0.4624 (0.5337)  loss_n_60: 0.5229 (0.5674)  loss_n_80: 0.6110 (0.6215)  loss_n_100: 0.6596 (0.6971)  triple_100: 0.0000 (0.0428)  triple_80: 0.0000 (0.0272)  triple_60: 0.0000 (0.0219)  triple_40: 0.0000 (0.0169)  time: 3.9142  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1090/1724]  eta: 0:41:26  lr: 0.000200  loss: 2.2114 (2.5236)  loss_n_40: 0.4203 (0.5327)  loss_n_60: 0.4902 (0.5665)  loss_n_80: 0.5716 (0.6207)  loss_n_100: 0.6337 (0.6960)  triple_100: 0.0000 (0.0424)  triple_80: 0.0000 (0.0269)  triple_60: 0.0000 (0.0217)  triple_40: 0.0000 (0.0168)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1100/1724]  eta: 0:40:47  lr: 0.000200  loss: 2.0409 (2.5204)  loss_n_40: 0.4444 (0.5322)  loss_n_60: 0.4861 (0.5659)  loss_n_80: 0.5355 (0.6202)  loss_n_100: 0.5727 (0.6952)  triple_100: 0.0000 (0.0421)  triple_80: 0.0000 (0.0267)  triple_60: 0.0000 (0.0215)  triple_40: 0.0000 (0.0166)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1110/1724]  eta: 0:40:07  lr: 0.000200  loss: 1.9827 (2.5149)  loss_n_40: 0.4327 (0.5315)  loss_n_60: 0.4694 (0.5649)  loss_n_80: 0.5265 (0.6190)  loss_n_100: 0.5603 (0.6936)  triple_100: 0.0000 (0.0417)  triple_80: 0.0000 (0.0264)  triple_60: 0.0000 (0.0213)  triple_40: 0.0000 (0.0165)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1120/1724]  eta: 0:39:28  lr: 0.000200  loss: 1.9084 (2.5089)  loss_n_40: 0.3868 (0.5301)  loss_n_60: 0.4460 (0.5637)  loss_n_80: 0.4931 (0.6179)  loss_n_100: 0.5303 (0.6922)  triple_100: 0.0000 (0.0413)  triple_80: 0.0000 (0.0262)  triple_60: 0.0000 (0.0211)  triple_40: 0.0000 (0.0163)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1130/1724]  eta: 0:38:49  lr: 0.000200  loss: 1.9114 (2.5037)  loss_n_40: 0.3852 (0.5293)  loss_n_60: 0.4499 (0.5628)  loss_n_80: 0.4947 (0.6169)  loss_n_100: 0.5393 (0.6908)  triple_100: 0.0000 (0.0409)  triple_80: 0.0000 (0.0260)  triple_60: 0.0000 (0.0209)  triple_40: 0.0000 (0.0162)  time: 3.9154  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1140/1724]  eta: 0:38:10  lr: 0.000200  loss: 1.8701 (2.4979)  loss_n_40: 0.3930 (0.5284)  loss_n_60: 0.4539 (0.5618)  loss_n_80: 0.4707 (0.6156)  loss_n_100: 0.4865 (0.6891)  triple_100: 0.0000 (0.0406)  triple_80: 0.0000 (0.0257)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0160)  time: 3.9148  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1150/1724]  eta: 0:37:30  lr: 0.000200  loss: 1.7555 (2.4930)  loss_n_40: 0.3667 (0.5271)  loss_n_60: 0.4047 (0.5605)  loss_n_80: 0.4510 (0.6142)  loss_n_100: 0.4847 (0.6874)  triple_100: 0.0000 (0.0407)  triple_80: 0.0000 (0.0259)  triple_60: 0.0000 (0.0210)  triple_40: 0.0000 (0.0163)  time: 3.9157  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1160/1724]  eta: 0:36:51  lr: 0.000200  loss: 2.3306 (2.4929)  loss_n_40: 0.4149 (0.5269)  loss_n_60: 0.4758 (0.5605)  loss_n_80: 0.5225 (0.6146)  loss_n_100: 0.5759 (0.6877)  triple_100: 0.0000 (0.0403)  triple_80: 0.0000 (0.0257)  triple_60: 0.0000 (0.0211)  triple_40: 0.0000 (0.0161)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1170/1724]  eta: 0:36:12  lr: 0.000200  loss: 2.4801 (2.4949)  loss_n_40: 0.5206 (0.5274)  loss_n_60: 0.5582 (0.5610)  loss_n_80: 0.6825 (0.6156)  loss_n_100: 0.7137 (0.6885)  triple_100: 0.0000 (0.0400)  triple_80: 0.0000 (0.0255)  triple_60: 0.0000 (0.0209)  triple_40: 0.0000 (0.0160)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1180/1724]  eta: 0:35:33  lr: 0.000200  loss: 2.4055 (2.4936)  loss_n_40: 0.5155 (0.5272)  loss_n_60: 0.5534 (0.5609)  loss_n_80: 0.6320 (0.6156)  loss_n_100: 0.6911 (0.6884)  triple_100: 0.0000 (0.0397)  triple_80: 0.0000 (0.0253)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0159)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1190/1724]  eta: 0:34:53  lr: 0.000200  loss: 2.2090 (2.4903)  loss_n_40: 0.4557 (0.5264)  loss_n_60: 0.5115 (0.5603)  loss_n_80: 0.5917 (0.6152)  loss_n_100: 0.6605 (0.6877)  triple_100: 0.0000 (0.0393)  triple_80: 0.0000 (0.0251)  triple_60: 0.0000 (0.0206)  triple_40: 0.0000 (0.0157)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1200/1724]  eta: 0:34:14  lr: 0.000200  loss: 1.9168 (2.4848)  loss_n_40: 0.3837 (0.5255)  loss_n_60: 0.4495 (0.5592)  loss_n_80: 0.5167 (0.6140)  loss_n_100: 0.5522 (0.6862)  triple_100: 0.0000 (0.0390)  triple_80: 0.0000 (0.0249)  triple_60: 0.0000 (0.0204)  triple_40: 0.0000 (0.0156)  time: 3.9178  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [1210/1724]  eta: 0:33:35  lr: 0.000200  loss: 1.8181 (2.4796)  loss_n_40: 0.3699 (0.5247)  loss_n_60: 0.4111 (0.5582)  loss_n_80: 0.4797 (0.6129)  loss_n_100: 0.5194 (0.6848)  triple_100: 0.0000 (0.0387)  triple_80: 0.0000 (0.0247)  triple_60: 0.0000 (0.0202)  triple_40: 0.0000 (0.0155)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1220/1724]  eta: 0:32:56  lr: 0.000200  loss: 1.7115 (2.4726)  loss_n_40: 0.3594 (0.5233)  loss_n_60: 0.4091 (0.5568)  loss_n_80: 0.4384 (0.6114)  loss_n_100: 0.4650 (0.6829)  triple_100: 0.0000 (0.0384)  triple_80: 0.0000 (0.0245)  triple_60: 0.0000 (0.0201)  triple_40: 0.0000 (0.0153)  time: 3.9180  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [1230/1724]  eta: 0:32:17  lr: 0.000200  loss: 1.5854 (2.4658)  loss_n_40: 0.3455 (0.5220)  loss_n_60: 0.3821 (0.5555)  loss_n_80: 0.4198 (0.6099)  loss_n_100: 0.4435 (0.6811)  triple_100: 0.0000 (0.0380)  triple_80: 0.0000 (0.0243)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0152)  time: 3.9184  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [1240/1724]  eta: 0:31:37  lr: 0.000200  loss: 1.6395 (2.4604)  loss_n_40: 0.3683 (0.5208)  loss_n_60: 0.3976 (0.5544)  loss_n_80: 0.4257 (0.6087)  loss_n_100: 0.4582 (0.6797)  triple_100: 0.0000 (0.0377)  triple_80: 0.0000 (0.0241)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0151)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1250/1724]  eta: 0:30:58  lr: 0.000200  loss: 1.7316 (2.4558)  loss_n_40: 0.3862 (0.5204)  loss_n_60: 0.4223 (0.5535)  loss_n_80: 0.4514 (0.6076)  loss_n_100: 0.4827 (0.6783)  triple_100: 0.0000 (0.0374)  triple_80: 0.0000 (0.0239)  triple_60: 0.0000 (0.0197)  triple_40: 0.0000 (0.0150)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1260/1724]  eta: 0:30:19  lr: 0.000200  loss: 1.7176 (2.4500)  loss_n_40: 0.4046 (0.5193)  loss_n_60: 0.3932 (0.5524)  loss_n_80: 0.4642 (0.6064)  loss_n_100: 0.4685 (0.6768)  triple_100: 0.0000 (0.0371)  triple_80: 0.0000 (0.0237)  triple_60: 0.0000 (0.0195)  triple_40: 0.0000 (0.0149)  time: 3.9173  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [1270/1724]  eta: 0:29:40  lr: 0.000200  loss: 1.6281 (2.4427)  loss_n_40: 0.3516 (0.5178)  loss_n_60: 0.3774 (0.5508)  loss_n_80: 0.4259 (0.6048)  loss_n_100: 0.4664 (0.6749)  triple_100: 0.0000 (0.0368)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0194)  triple_40: 0.0000 (0.0147)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1280/1724]  eta: 0:29:00  lr: 0.000200  loss: 1.5955 (2.4360)  loss_n_40: 0.3516 (0.5166)  loss_n_60: 0.3576 (0.5494)  loss_n_80: 0.3992 (0.6032)  loss_n_100: 0.4263 (0.6730)  triple_100: 0.0000 (0.0366)  triple_80: 0.0000 (0.0233)  triple_60: 0.0000 (0.0192)  triple_40: 0.0000 (0.0146)  time: 3.9164  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [1290/1724]  eta: 0:28:21  lr: 0.000200  loss: 1.4822 (2.4313)  loss_n_40: 0.3419 (0.5154)  loss_n_60: 0.3458 (0.5480)  loss_n_80: 0.3859 (0.6016)  loss_n_100: 0.4195 (0.6711)  triple_100: 0.0000 (0.0363)  triple_80: 0.0000 (0.0236)  triple_60: 0.0000 (0.0200)  triple_40: 0.0000 (0.0152)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1300/1724]  eta: 0:27:42  lr: 0.000200  loss: 1.8430 (2.4283)  loss_n_40: 0.3739 (0.5146)  loss_n_60: 0.4070 (0.5474)  loss_n_80: 0.4438 (0.6010)  loss_n_100: 0.5109 (0.6705)  triple_100: 0.0000 (0.0361)  triple_80: 0.0000 (0.0236)  triple_60: 0.0000 (0.0200)  triple_40: 0.0000 (0.0151)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1310/1724]  eta: 0:27:03  lr: 0.000200  loss: 2.0525 (2.4257)  loss_n_40: 0.4091 (0.5141)  loss_n_60: 0.4624 (0.5470)  loss_n_80: 0.5353 (0.6006)  loss_n_100: 0.5892 (0.6700)  triple_100: 0.0000 (0.0359)  triple_80: 0.0000 (0.0234)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0150)  time: 3.9158  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1320/1724]  eta: 0:26:24  lr: 0.000200  loss: 1.8947 (2.4218)  loss_n_40: 0.4074 (0.5132)  loss_n_60: 0.4446 (0.5462)  loss_n_80: 0.4945 (0.5997)  loss_n_100: 0.5759 (0.6692)  triple_100: 0.0000 (0.0356)  triple_80: 0.0000 (0.0232)  triple_60: 0.0000 (0.0197)  triple_40: 0.0000 (0.0149)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1330/1724]  eta: 0:25:44  lr: 0.000200  loss: 1.8677 (2.4182)  loss_n_40: 0.3891 (0.5125)  loss_n_60: 0.4394 (0.5455)  loss_n_80: 0.4871 (0.5990)  loss_n_100: 0.5617 (0.6684)  triple_100: 0.0000 (0.0353)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0196)  triple_40: 0.0000 (0.0148)  time: 3.9156  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1340/1724]  eta: 0:25:05  lr: 0.000200  loss: 1.8012 (2.4134)  loss_n_40: 0.3891 (0.5117)  loss_n_60: 0.4331 (0.5446)  loss_n_80: 0.4862 (0.5979)  loss_n_100: 0.5123 (0.6671)  triple_100: 0.0000 (0.0351)  triple_80: 0.0000 (0.0229)  triple_60: 0.0000 (0.0194)  triple_40: 0.0000 (0.0146)  time: 3.9153  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [1350/1724]  eta: 0:24:26  lr: 0.000200  loss: 1.7477 (2.4086)  loss_n_40: 0.3755 (0.5108)  loss_n_60: 0.4193 (0.5437)  loss_n_80: 0.4389 (0.5968)  loss_n_100: 0.4885 (0.6659)  triple_100: 0.0000 (0.0348)  triple_80: 0.0000 (0.0227)  triple_60: 0.0000 (0.0193)  triple_40: 0.0000 (0.0145)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1360/1724]  eta: 0:23:47  lr: 0.000200  loss: 1.6791 (2.4031)  loss_n_40: 0.3484 (0.5100)  loss_n_60: 0.3902 (0.5426)  loss_n_80: 0.4280 (0.5954)  loss_n_100: 0.4784 (0.6643)  triple_100: 0.0000 (0.0345)  triple_80: 0.0000 (0.0225)  triple_60: 0.0000 (0.0191)  triple_40: 0.0000 (0.0144)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1370/1724]  eta: 0:23:07  lr: 0.000200  loss: 1.7287 (2.3982)  loss_n_40: 0.3586 (0.5091)  loss_n_60: 0.3985 (0.5417)  loss_n_80: 0.4533 (0.5944)  loss_n_100: 0.4784 (0.6630)  triple_100: 0.0000 (0.0343)  triple_80: 0.0000 (0.0224)  triple_60: 0.0000 (0.0190)  triple_40: 0.0000 (0.0143)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1380/1724]  eta: 0:22:28  lr: 0.000200  loss: 1.5173 (2.3911)  loss_n_40: 0.3296 (0.5077)  loss_n_60: 0.3622 (0.5403)  loss_n_80: 0.3920 (0.5927)  loss_n_100: 0.4325 (0.6611)  triple_100: 0.0000 (0.0340)  triple_80: 0.0000 (0.0222)  triple_60: 0.0000 (0.0189)  triple_40: 0.0000 (0.0142)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1390/1724]  eta: 0:21:49  lr: 0.000200  loss: 1.5173 (2.4043)  loss_n_40: 0.3252 (0.5073)  loss_n_60: 0.3558 (0.5402)  loss_n_80: 0.3975 (0.5929)  loss_n_100: 0.4346 (0.6612)  triple_100: 0.0000 (0.0380)  triple_80: 0.0000 (0.0257)  triple_60: 0.0000 (0.0223)  triple_40: 0.0000 (0.0166)  time: 3.9180  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [1400/1724]  eta: 0:21:10  lr: 0.000200  loss: 2.7373 (2.4075)  loss_n_40: 0.4942 (0.5077)  loss_n_60: 0.5778 (0.5412)  loss_n_80: 0.6504 (0.5941)  loss_n_100: 0.7249 (0.6627)  triple_100: 0.0000 (0.0377)  triple_80: 0.0000 (0.0255)  triple_60: 0.0000 (0.0222)  triple_40: 0.0000 (0.0165)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1410/1724]  eta: 0:20:31  lr: 0.000200  loss: 2.4986 (2.4093)  loss_n_40: 0.5076 (0.5080)  loss_n_60: 0.5778 (0.5417)  loss_n_80: 0.6504 (0.5948)  loss_n_100: 0.7827 (0.6637)  triple_100: 0.0000 (0.0375)  triple_80: 0.0000 (0.0253)  triple_60: 0.0000 (0.0220)  triple_40: 0.0000 (0.0164)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1420/1724]  eta: 0:19:51  lr: 0.000200  loss: 2.4227 (2.4081)  loss_n_40: 0.4846 (0.5078)  loss_n_60: 0.5507 (0.5414)  loss_n_80: 0.6213 (0.5947)  loss_n_100: 0.7186 (0.6638)  triple_100: 0.0000 (0.0372)  triple_80: 0.0000 (0.0251)  triple_60: 0.0000 (0.0218)  triple_40: 0.0000 (0.0163)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1430/1724]  eta: 0:19:12  lr: 0.000200  loss: 2.1275 (2.4063)  loss_n_40: 0.4565 (0.5077)  loss_n_60: 0.4857 (0.5411)  loss_n_80: 0.5420 (0.5943)  loss_n_100: 0.6423 (0.6634)  triple_100: 0.0000 (0.0370)  triple_80: 0.0000 (0.0250)  triple_60: 0.0000 (0.0217)  triple_40: 0.0000 (0.0162)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1440/1724]  eta: 0:18:33  lr: 0.000200  loss: 2.0213 (2.4034)  loss_n_40: 0.4378 (0.5072)  loss_n_60: 0.4857 (0.5407)  loss_n_80: 0.5251 (0.5937)  loss_n_100: 0.5791 (0.6627)  triple_100: 0.0000 (0.0367)  triple_80: 0.0000 (0.0248)  triple_60: 0.0000 (0.0215)  triple_40: 0.0000 (0.0160)  time: 3.9154  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [1450/1724]  eta: 0:17:54  lr: 0.000200  loss: 1.9565 (2.4002)  loss_n_40: 0.4156 (0.5067)  loss_n_60: 0.4532 (0.5401)  loss_n_80: 0.4982 (0.5930)  loss_n_100: 0.5572 (0.6620)  triple_100: 0.0000 (0.0364)  triple_80: 0.0000 (0.0246)  triple_60: 0.0000 (0.0214)  triple_40: 0.0000 (0.0159)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1460/1724]  eta: 0:17:15  lr: 0.000200  loss: 1.8636 (2.3964)  loss_n_40: 0.4156 (0.5060)  loss_n_60: 0.4348 (0.5393)  loss_n_80: 0.4791 (0.5923)  loss_n_100: 0.5366 (0.6611)  triple_100: 0.0000 (0.0362)  triple_80: 0.0000 (0.0245)  triple_60: 0.0000 (0.0212)  triple_40: 0.0000 (0.0158)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1470/1724]  eta: 0:16:35  lr: 0.000200  loss: 1.7686 (2.3920)  loss_n_40: 0.3935 (0.5052)  loss_n_60: 0.4177 (0.5385)  loss_n_80: 0.4539 (0.5913)  loss_n_100: 0.4970 (0.6600)  triple_100: 0.0000 (0.0359)  triple_80: 0.0000 (0.0243)  triple_60: 0.0000 (0.0211)  triple_40: 0.0000 (0.0157)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1480/1724]  eta: 0:15:56  lr: 0.000200  loss: 1.6913 (2.3882)  loss_n_40: 0.3935 (0.5048)  loss_n_60: 0.4091 (0.5378)  loss_n_80: 0.4332 (0.5904)  loss_n_100: 0.4658 (0.6588)  triple_100: 0.0000 (0.0357)  triple_80: 0.0000 (0.0241)  triple_60: 0.0000 (0.0210)  triple_40: 0.0000 (0.0156)  time: 3.9156  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1490/1724]  eta: 0:15:17  lr: 0.000200  loss: 1.6396 (2.3829)  loss_n_40: 0.3684 (0.5039)  loss_n_60: 0.3755 (0.5368)  loss_n_80: 0.4223 (0.5891)  loss_n_100: 0.4473 (0.6574)  triple_100: 0.0000 (0.0355)  triple_80: 0.0000 (0.0240)  triple_60: 0.0000 (0.0208)  triple_40: 0.0000 (0.0155)  time: 3.9151  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1500/1724]  eta: 0:14:38  lr: 0.000200  loss: 1.5263 (2.3783)  loss_n_40: 0.3421 (0.5031)  loss_n_60: 0.3755 (0.5359)  loss_n_80: 0.3886 (0.5880)  loss_n_100: 0.4458 (0.6562)  triple_100: 0.0000 (0.0352)  triple_80: 0.0000 (0.0238)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0154)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1510/1724]  eta: 0:13:58  lr: 0.000200  loss: 1.6609 (2.3741)  loss_n_40: 0.3795 (0.5025)  loss_n_60: 0.3940 (0.5351)  loss_n_80: 0.4160 (0.5870)  loss_n_100: 0.4640 (0.6550)  triple_100: 0.0000 (0.0350)  triple_80: 0.0000 (0.0236)  triple_60: 0.0000 (0.0205)  triple_40: 0.0000 (0.0153)  time: 3.9158  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1520/1724]  eta: 0:13:19  lr: 0.000200  loss: 1.6007 (2.3691)  loss_n_40: 0.3720 (0.5015)  loss_n_60: 0.3915 (0.5342)  loss_n_80: 0.4056 (0.5858)  loss_n_100: 0.4458 (0.6537)  triple_100: 0.0000 (0.0348)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0204)  triple_40: 0.0000 (0.0152)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1530/1724]  eta: 0:12:40  lr: 0.000200  loss: 1.5616 (2.3642)  loss_n_40: 0.3539 (0.5008)  loss_n_60: 0.3744 (0.5332)  loss_n_80: 0.3899 (0.5846)  loss_n_100: 0.4202 (0.6523)  triple_100: 0.0000 (0.0345)  triple_80: 0.0000 (0.0233)  triple_60: 0.0000 (0.0203)  triple_40: 0.0000 (0.0151)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1540/1724]  eta: 0:12:01  lr: 0.000200  loss: 1.5616 (2.3592)  loss_n_40: 0.3539 (0.4999)  loss_n_60: 0.3744 (0.5322)  loss_n_80: 0.3930 (0.5835)  loss_n_100: 0.4225 (0.6509)  triple_100: 0.0000 (0.0343)  triple_80: 0.0000 (0.0232)  triple_60: 0.0000 (0.0201)  triple_40: 0.0000 (0.0150)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1550/1724]  eta: 0:11:22  lr: 0.000200  loss: 1.5674 (2.3540)  loss_n_40: 0.3253 (0.4990)  loss_n_60: 0.3681 (0.5312)  loss_n_80: 0.4101 (0.5822)  loss_n_100: 0.4407 (0.6496)  triple_100: 0.0000 (0.0341)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0200)  triple_40: 0.0000 (0.0149)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1560/1724]  eta: 0:10:42  lr: 0.000200  loss: 1.5472 (2.3486)  loss_n_40: 0.3205 (0.4980)  loss_n_60: 0.3652 (0.5301)  loss_n_80: 0.3894 (0.5809)  loss_n_100: 0.4251 (0.6481)  triple_100: 0.0000 (0.0339)  triple_80: 0.0000 (0.0229)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0148)  time: 3.9153  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1570/1724]  eta: 0:10:03  lr: 0.000200  loss: 1.4916 (2.3432)  loss_n_40: 0.3155 (0.4970)  loss_n_60: 0.3619 (0.5290)  loss_n_80: 0.3667 (0.5796)  loss_n_100: 0.3913 (0.6466)  triple_100: 0.0000 (0.0337)  triple_80: 0.0000 (0.0227)  triple_60: 0.0000 (0.0198)  triple_40: 0.0000 (0.0147)  time: 3.9141  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1580/1724]  eta: 0:09:24  lr: 0.000200  loss: 1.3950 (2.3374)  loss_n_40: 0.3075 (0.4959)  loss_n_60: 0.3370 (0.5279)  loss_n_80: 0.3611 (0.5783)  loss_n_100: 0.3906 (0.6451)  triple_100: 0.0000 (0.0334)  triple_80: 0.0000 (0.0226)  triple_60: 0.0000 (0.0196)  triple_40: 0.0000 (0.0146)  time: 3.9136  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1590/1724]  eta: 0:08:45  lr: 0.000200  loss: 1.5587 (2.3406)  loss_n_40: 0.3591 (0.4957)  loss_n_60: 0.3679 (0.5279)  loss_n_80: 0.3891 (0.5784)  loss_n_100: 0.4430 (0.6453)  triple_100: 0.0000 (0.0355)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0197)  triple_40: 0.0000 (0.0147)  time: 3.9145  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1600/1724]  eta: 0:08:06  lr: 0.000200  loss: 3.2424 (2.3503)  loss_n_40: 0.6160 (0.4973)  loss_n_60: 0.7105 (0.5299)  loss_n_80: 0.8389 (0.5813)  loss_n_100: 0.9343 (0.6481)  triple_100: 0.0000 (0.0356)  triple_80: 0.0000 (0.0239)  triple_60: 0.0000 (0.0196)  triple_40: 0.0000 (0.0146)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1610/1724]  eta: 0:07:26  lr: 0.000200  loss: 3.4623 (2.3565)  loss_n_40: 0.7182 (0.4987)  loss_n_60: 0.7884 (0.5316)  loss_n_80: 0.9821 (0.5833)  loss_n_100: 1.0161 (0.6500)  triple_100: 0.0000 (0.0353)  triple_80: 0.0000 (0.0237)  triple_60: 0.0000 (0.0194)  triple_40: 0.0000 (0.0145)  time: 3.9149  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1620/1724]  eta: 0:06:47  lr: 0.000200  loss: 3.1444 (2.3609)  loss_n_40: 0.6788 (0.4996)  loss_n_60: 0.7404 (0.5326)  loss_n_80: 0.8573 (0.5849)  loss_n_100: 0.8988 (0.6515)  triple_100: 0.0000 (0.0351)  triple_80: 0.0000 (0.0236)  triple_60: 0.0000 (0.0193)  triple_40: 0.0000 (0.0144)  time: 3.9138  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1630/1724]  eta: 0:06:08  lr: 0.000200  loss: 2.7501 (2.3617)  loss_n_40: 0.5608 (0.4998)  loss_n_60: 0.6005 (0.5327)  loss_n_80: 0.7472 (0.5854)  loss_n_100: 0.7992 (0.6519)  triple_100: 0.0000 (0.0349)  triple_80: 0.0000 (0.0234)  triple_60: 0.0000 (0.0192)  triple_40: 0.0000 (0.0143)  time: 3.9144  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [1640/1724]  eta: 0:05:29  lr: 0.000200  loss: 2.3532 (2.3606)  loss_n_40: 0.5295 (0.4998)  loss_n_60: 0.5309 (0.5326)  loss_n_80: 0.6075 (0.5853)  loss_n_100: 0.6680 (0.6516)  triple_100: 0.0000 (0.0347)  triple_80: 0.0000 (0.0233)  triple_60: 0.0000 (0.0191)  triple_40: 0.0000 (0.0142)  time: 3.9147  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [1650/1724]  eta: 0:04:50  lr: 0.000200  loss: 2.0475 (2.3586)  loss_n_40: 0.4513 (0.4995)  loss_n_60: 0.4815 (0.5322)  loss_n_80: 0.5485 (0.5851)  loss_n_100: 0.5801 (0.6511)  triple_100: 0.0000 (0.0345)  triple_80: 0.0000 (0.0232)  triple_60: 0.0000 (0.0190)  triple_40: 0.0000 (0.0141)  time: 3.9141  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1660/1724]  eta: 0:04:10  lr: 0.000200  loss: 1.9569 (2.3561)  loss_n_40: 0.4244 (0.4990)  loss_n_60: 0.4453 (0.5318)  loss_n_80: 0.5141 (0.5846)  loss_n_100: 0.5481 (0.6505)  triple_100: 0.0000 (0.0343)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0188)  triple_40: 0.0000 (0.0141)  time: 3.9134  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1670/1724]  eta: 0:03:31  lr: 0.000200  loss: 1.9569 (2.3541)  loss_n_40: 0.4317 (0.4988)  loss_n_60: 0.4664 (0.5314)  loss_n_80: 0.5141 (0.5843)  loss_n_100: 0.5633 (0.6500)  triple_100: 0.0000 (0.0341)  triple_80: 0.0000 (0.0229)  triple_60: 0.0000 (0.0187)  triple_40: 0.0000 (0.0140)  time: 3.9138  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1680/1724]  eta: 0:02:52  lr: 0.000200  loss: 1.7853 (2.3504)  loss_n_40: 0.4013 (0.4980)  loss_n_60: 0.4195 (0.5307)  loss_n_80: 0.4687 (0.5835)  loss_n_100: 0.5051 (0.6490)  triple_100: 0.0000 (0.0339)  triple_80: 0.0000 (0.0227)  triple_60: 0.0000 (0.0186)  triple_40: 0.0000 (0.0139)  time: 3.9158  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:12]  [1690/1724]  eta: 0:02:13  lr: 0.000200  loss: 1.6741 (2.3467)  loss_n_40: 0.3678 (0.4975)  loss_n_60: 0.3933 (0.5300)  loss_n_80: 0.4463 (0.5827)  loss_n_100: 0.4718 (0.6480)  triple_100: 0.0000 (0.0337)  triple_80: 0.0000 (0.0226)  triple_60: 0.0000 (0.0185)  triple_40: 0.0000 (0.0138)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1700/1724]  eta: 0:01:34  lr: 0.000200  loss: 1.5071 (2.3420)  loss_n_40: 0.3286 (0.4965)  loss_n_60: 0.3571 (0.5290)  loss_n_80: 0.4053 (0.5817)  loss_n_100: 0.4287 (0.6467)  triple_100: 0.0000 (0.0335)  triple_80: 0.0000 (0.0225)  triple_60: 0.0000 (0.0184)  triple_40: 0.0000 (0.0137)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12]  [1710/1724]  eta: 0:00:54  lr: 0.000200  loss: 1.5385 (2.3378)  loss_n_40: 0.3375 (0.4959)  loss_n_60: 0.3571 (0.5281)  loss_n_80: 0.4053 (0.5807)  loss_n_100: 0.4331 (0.6455)  triple_100: 0.0000 (0.0333)  triple_80: 0.0000 (0.0223)  triple_60: 0.0000 (0.0183)  triple_40: 0.0000 (0.0136)  time: 3.9162  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [1720/1724]  eta: 0:00:15  lr: 0.000200  loss: 1.6184 (2.3364)  loss_n_40: 0.3536 (0.4951)  loss_n_60: 0.3761 (0.5276)  loss_n_80: 0.4328 (0.5803)  loss_n_100: 0.4720 (0.6451)  triple_100: 0.0000 (0.0334)  triple_80: 0.0000 (0.0225)  triple_60: 0.0000 (0.0186)  triple_40: 0.0000 (0.0139)  time: 3.9153  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:12]  [1723/1724]  eta: 0:00:03  lr: 0.000200  loss: 1.8291 (2.3369)  loss_n_40: 0.4214 (0.4952)  loss_n_60: 0.4272 (0.5277)  loss_n_80: 0.4539 (0.5805)  loss_n_100: 0.4941 (0.6453)  triple_100: 0.0000 (0.0334)  triple_80: 0.0000 (0.0225)  triple_60: 0.0000 (0.0185)  triple_40: 0.0000 (0.0139)  time: 3.9153  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:12] Total time: 1:52:37 (3.9199 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.8291 (2.3369)  loss_n_40: 0.4214 (0.4952)  loss_n_60: 0.4272 (0.5277)  loss_n_80: 0.4539 (0.5805)  loss_n_100: 0.4941 (0.6453)  triple_100: 0.0000 (0.0334)  triple_80: 0.0000 (0.0225)  triple_60: 0.0000 (0.0185)  triple_40: 0.0000 (0.0139)\n",
      "Valid: [epoch:12]  [  0/845]  eta: 0:10:39  loss: 2.5339 (2.5339)  loss_n_40: 0.9057 (0.9057)  loss_n_60: 0.6084 (0.6084)  loss_n_80: 0.5000 (0.5000)  loss_n_100: 0.5197 (0.5197)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7564  data: 0.4189  max mem: 46473\n",
      "Valid: [epoch:12]  [ 10/845]  eta: 0:05:10  loss: 2.1677 (2.2221)  loss_n_40: 0.4499 (0.5071)  loss_n_60: 0.5075 (0.5126)  loss_n_80: 0.5258 (0.5730)  loss_n_100: 0.5960 (0.6226)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3720  data: 0.0382  max mem: 46473\n",
      "Valid: [epoch:12]  [ 20/845]  eta: 0:04:51  loss: 2.0482 (2.1525)  loss_n_40: 0.4517 (0.4872)  loss_n_60: 0.4797 (0.4987)  loss_n_80: 0.5764 (0.5632)  loss_n_100: 0.5720 (0.5999)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0036)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [ 30/845]  eta: 0:04:43  loss: 2.1819 (2.1970)  loss_n_40: 0.4566 (0.4895)  loss_n_60: 0.4849 (0.5101)  loss_n_80: 0.5764 (0.5742)  loss_n_100: 0.5838 (0.6208)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0024)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [ 40/845]  eta: 0:04:36  loss: 2.1526 (2.1604)  loss_n_40: 0.4441 (0.4845)  loss_n_60: 0.4760 (0.5024)  loss_n_80: 0.5045 (0.5602)  loss_n_100: 0.5838 (0.6114)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0018)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3336  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [ 50/845]  eta: 0:04:31  loss: 1.8995 (2.1289)  loss_n_40: 0.4074 (0.4744)  loss_n_60: 0.4319 (0.4945)  loss_n_80: 0.4716 (0.5538)  loss_n_100: 0.5339 (0.6047)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0015)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3334  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [ 60/845]  eta: 0:04:27  loss: 1.8995 (2.1057)  loss_n_40: 0.4127 (0.4685)  loss_n_60: 0.4317 (0.4900)  loss_n_80: 0.4997 (0.5489)  loss_n_100: 0.5249 (0.5971)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0012)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3333  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [ 70/845]  eta: 0:04:23  loss: 1.9634 (2.1115)  loss_n_40: 0.4127 (0.4741)  loss_n_60: 0.4335 (0.4914)  loss_n_80: 0.5227 (0.5499)  loss_n_100: 0.5668 (0.5951)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0011)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3334  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [ 80/845]  eta: 0:04:19  loss: 1.9789 (2.1251)  loss_n_40: 0.4304 (0.4729)  loss_n_60: 0.4415 (0.4897)  loss_n_80: 0.5546 (0.5529)  loss_n_100: 0.5882 (0.5961)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [ 90/845]  eta: 0:04:15  loss: 2.0526 (2.1429)  loss_n_40: 0.4518 (0.4782)  loss_n_60: 0.4698 (0.4967)  loss_n_80: 0.5585 (0.5552)  loss_n_100: 0.5882 (0.6009)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0120)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [100/845]  eta: 0:04:11  loss: 2.0649 (2.1538)  loss_n_40: 0.5071 (0.4795)  loss_n_60: 0.4788 (0.4983)  loss_n_80: 0.5585 (0.5598)  loss_n_100: 0.6212 (0.6054)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0108)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [110/845]  eta: 0:04:08  loss: 1.9364 (2.1414)  loss_n_40: 0.4380 (0.4754)  loss_n_60: 0.4599 (0.4954)  loss_n_80: 0.5139 (0.5575)  loss_n_100: 0.5635 (0.6032)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0098)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [120/845]  eta: 0:04:04  loss: 1.9151 (2.1340)  loss_n_40: 0.4278 (0.4742)  loss_n_60: 0.4481 (0.4941)  loss_n_80: 0.5317 (0.5582)  loss_n_100: 0.5321 (0.5985)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0090)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3335  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [130/845]  eta: 0:04:00  loss: 2.0753 (2.1404)  loss_n_40: 0.4594 (0.4769)  loss_n_60: 0.4607 (0.4956)  loss_n_80: 0.5474 (0.5591)  loss_n_100: 0.5678 (0.6005)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0083)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [140/845]  eta: 0:03:57  loss: 2.1158 (2.1514)  loss_n_40: 0.4780 (0.4795)  loss_n_60: 0.4982 (0.4982)  loss_n_80: 0.5880 (0.5632)  loss_n_100: 0.6269 (0.6027)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0077)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [150/845]  eta: 0:03:53  loss: 2.0467 (2.1431)  loss_n_40: 0.4455 (0.4778)  loss_n_60: 0.4961 (0.4968)  loss_n_80: 0.5421 (0.5597)  loss_n_100: 0.5536 (0.6016)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0072)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [160/845]  eta: 0:03:50  loss: 1.9904 (2.1570)  loss_n_40: 0.4160 (0.4764)  loss_n_60: 0.4487 (0.4968)  loss_n_80: 0.5035 (0.5597)  loss_n_100: 0.5627 (0.6036)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0187)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0019)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [170/845]  eta: 0:03:46  loss: 2.1626 (2.1670)  loss_n_40: 0.4763 (0.4796)  loss_n_60: 0.5023 (0.4988)  loss_n_80: 0.5506 (0.5620)  loss_n_100: 0.6346 (0.6072)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0176)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0017)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [180/845]  eta: 0:03:43  loss: 2.1626 (2.1679)  loss_n_40: 0.5136 (0.4805)  loss_n_60: 0.4815 (0.4986)  loss_n_80: 0.5506 (0.5624)  loss_n_100: 0.6313 (0.6080)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0167)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0017)  time: 0.3340  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:12]  [190/845]  eta: 0:03:40  loss: 1.9030 (2.1604)  loss_n_40: 0.4232 (0.4774)  loss_n_60: 0.4412 (0.4972)  loss_n_80: 0.5281 (0.5609)  loss_n_100: 0.5828 (0.6076)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0016)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [200/845]  eta: 0:03:36  loss: 2.1505 (2.1681)  loss_n_40: 0.4450 (0.4791)  loss_n_60: 0.4922 (0.5002)  loss_n_80: 0.5281 (0.5623)  loss_n_100: 0.6011 (0.6094)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0015)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [210/845]  eta: 0:03:33  loss: 2.2056 (2.1679)  loss_n_40: 0.4543 (0.4794)  loss_n_60: 0.5335 (0.5000)  loss_n_80: 0.5644 (0.5623)  loss_n_100: 0.6099 (0.6100)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0014)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [220/845]  eta: 0:03:29  loss: 2.2056 (2.1886)  loss_n_40: 0.4543 (0.4799)  loss_n_60: 0.4718 (0.5010)  loss_n_80: 0.5339 (0.5631)  loss_n_100: 0.5855 (0.6113)  triple_100: 0.0000 (0.0006)  triple_80: 0.0000 (0.0313)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0014)  time: 0.3336  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [230/845]  eta: 0:03:26  loss: 1.8344 (2.1793)  loss_n_40: 0.4131 (0.4778)  loss_n_60: 0.4253 (0.4993)  loss_n_80: 0.4972 (0.5616)  loss_n_100: 0.5291 (0.6088)  triple_100: 0.0000 (0.0006)  triple_80: 0.0000 (0.0299)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0013)  time: 0.3336  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [240/845]  eta: 0:03:22  loss: 1.8629 (2.1720)  loss_n_40: 0.3814 (0.4759)  loss_n_60: 0.4226 (0.4977)  loss_n_80: 0.5068 (0.5604)  loss_n_100: 0.5146 (0.6075)  triple_100: 0.0000 (0.0005)  triple_80: 0.0000 (0.0287)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0012)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [250/845]  eta: 0:03:19  loss: 1.9988 (2.1824)  loss_n_40: 0.4160 (0.4764)  loss_n_60: 0.4787 (0.4984)  loss_n_80: 0.5307 (0.5609)  loss_n_100: 0.5791 (0.6090)  triple_100: 0.0000 (0.0005)  triple_80: 0.0000 (0.0308)  triple_60: 0.0000 (0.0029)  triple_40: 0.0000 (0.0035)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [260/845]  eta: 0:03:16  loss: 2.1765 (2.1768)  loss_n_40: 0.4606 (0.4752)  loss_n_60: 0.4906 (0.4977)  loss_n_80: 0.5642 (0.5601)  loss_n_100: 0.6058 (0.6076)  triple_100: 0.0000 (0.0005)  triple_80: 0.0000 (0.0296)  triple_60: 0.0000 (0.0028)  triple_40: 0.0000 (0.0034)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [270/845]  eta: 0:03:12  loss: 2.1404 (2.1837)  loss_n_40: 0.4203 (0.4749)  loss_n_60: 0.4784 (0.4981)  loss_n_80: 0.5292 (0.5599)  loss_n_100: 0.5851 (0.6079)  triple_100: 0.0000 (0.0005)  triple_80: 0.0000 (0.0315)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0056)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [280/845]  eta: 0:03:09  loss: 2.0995 (2.1840)  loss_n_40: 0.4406 (0.4748)  loss_n_60: 0.4763 (0.4986)  loss_n_80: 0.5670 (0.5614)  loss_n_100: 0.5834 (0.6078)  triple_100: 0.0000 (0.0005)  triple_80: 0.0000 (0.0303)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0054)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [290/845]  eta: 0:03:06  loss: 2.0742 (2.1799)  loss_n_40: 0.4420 (0.4742)  loss_n_60: 0.4628 (0.4974)  loss_n_80: 0.5670 (0.5611)  loss_n_100: 0.5717 (0.6073)  triple_100: 0.0000 (0.0004)  triple_80: 0.0000 (0.0293)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0052)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [300/845]  eta: 0:03:02  loss: 2.0208 (2.1799)  loss_n_40: 0.4585 (0.4757)  loss_n_60: 0.4727 (0.4976)  loss_n_80: 0.5341 (0.5615)  loss_n_100: 0.5686 (0.6065)  triple_100: 0.0000 (0.0004)  triple_80: 0.0000 (0.0283)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0050)  time: 0.3343  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:12]  [310/845]  eta: 0:02:59  loss: 1.9657 (2.1762)  loss_n_40: 0.4944 (0.4764)  loss_n_60: 0.4591 (0.4975)  loss_n_80: 0.4908 (0.5599)  loss_n_100: 0.5302 (0.6050)  triple_100: 0.0000 (0.0004)  triple_80: 0.0000 (0.0274)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0049)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [320/845]  eta: 0:02:55  loss: 1.8635 (2.1733)  loss_n_40: 0.4487 (0.4757)  loss_n_60: 0.4283 (0.4974)  loss_n_80: 0.5057 (0.5591)  loss_n_100: 0.5457 (0.6049)  triple_100: 0.0000 (0.0004)  triple_80: 0.0000 (0.0266)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0047)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [330/845]  eta: 0:02:52  loss: 2.2367 (2.1767)  loss_n_40: 0.4621 (0.4776)  loss_n_60: 0.5019 (0.4998)  loss_n_80: 0.5669 (0.5594)  loss_n_100: 0.5566 (0.6047)  triple_100: 0.0000 (0.0004)  triple_80: 0.0000 (0.0258)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0046)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [340/845]  eta: 0:02:49  loss: 2.0492 (2.1704)  loss_n_40: 0.4621 (0.4764)  loss_n_60: 0.4681 (0.4983)  loss_n_80: 0.5144 (0.5585)  loss_n_100: 0.5373 (0.6030)  triple_100: 0.0000 (0.0004)  triple_80: 0.0000 (0.0250)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0045)  time: 0.3341  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:12]  [350/845]  eta: 0:02:45  loss: 2.0492 (2.1737)  loss_n_40: 0.4670 (0.4773)  loss_n_60: 0.4838 (0.4988)  loss_n_80: 0.5144 (0.5591)  loss_n_100: 0.5594 (0.6039)  triple_100: 0.0000 (0.0004)  triple_80: 0.0000 (0.0257)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0043)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [360/845]  eta: 0:02:42  loss: 2.2176 (2.1743)  loss_n_40: 0.4840 (0.4781)  loss_n_60: 0.5005 (0.4988)  loss_n_80: 0.5804 (0.5594)  loss_n_100: 0.5828 (0.6045)  triple_100: 0.0000 (0.0004)  triple_80: 0.0000 (0.0250)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0042)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [370/845]  eta: 0:02:39  loss: 2.1754 (2.1812)  loss_n_40: 0.4669 (0.4781)  loss_n_60: 0.4965 (0.4988)  loss_n_80: 0.5796 (0.5589)  loss_n_100: 0.5268 (0.6042)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0321)  triple_60: 0.0000 (0.0039)  triple_40: 0.0000 (0.0041)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [380/845]  eta: 0:02:35  loss: 1.9083 (2.1785)  loss_n_40: 0.4331 (0.4777)  loss_n_60: 0.4245 (0.4985)  loss_n_80: 0.5174 (0.5586)  loss_n_100: 0.5268 (0.6036)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0313)  triple_60: 0.0000 (0.0038)  triple_40: 0.0000 (0.0040)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [390/845]  eta: 0:02:32  loss: 2.1881 (2.1805)  loss_n_40: 0.4546 (0.4781)  loss_n_60: 0.4952 (0.4989)  loss_n_80: 0.5414 (0.5591)  loss_n_100: 0.5674 (0.6044)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0305)  triple_60: 0.0000 (0.0037)  triple_40: 0.0000 (0.0039)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [400/845]  eta: 0:02:29  loss: 2.2150 (2.1812)  loss_n_40: 0.4484 (0.4776)  loss_n_60: 0.4952 (0.4985)  loss_n_80: 0.5414 (0.5589)  loss_n_100: 0.5840 (0.6045)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0309)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0044)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [410/845]  eta: 0:02:25  loss: 2.0566 (2.1778)  loss_n_40: 0.4351 (0.4771)  loss_n_60: 0.4628 (0.4976)  loss_n_80: 0.5214 (0.5583)  loss_n_100: 0.5826 (0.6041)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0302)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0043)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [420/845]  eta: 0:02:22  loss: 1.8527 (2.1737)  loss_n_40: 0.4350 (0.4760)  loss_n_60: 0.4192 (0.4962)  loss_n_80: 0.4973 (0.5571)  loss_n_100: 0.5426 (0.6028)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0303)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0047)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [430/845]  eta: 0:02:18  loss: 2.1152 (2.1753)  loss_n_40: 0.4357 (0.4764)  loss_n_60: 0.4637 (0.4960)  loss_n_80: 0.5244 (0.5570)  loss_n_100: 0.5843 (0.6034)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0306)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0053)  time: 0.3338  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:12]  [440/845]  eta: 0:02:15  loss: 2.1152 (2.1724)  loss_n_40: 0.4333 (0.4760)  loss_n_60: 0.4870 (0.4953)  loss_n_80: 0.5280 (0.5564)  loss_n_100: 0.6044 (0.6030)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0299)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0051)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [450/845]  eta: 0:02:12  loss: 2.0300 (2.1687)  loss_n_40: 0.4279 (0.4751)  loss_n_60: 0.4498 (0.4944)  loss_n_80: 0.4947 (0.5559)  loss_n_100: 0.5471 (0.6026)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0292)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0050)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [460/845]  eta: 0:02:08  loss: 2.0367 (2.1663)  loss_n_40: 0.4559 (0.4753)  loss_n_60: 0.4438 (0.4939)  loss_n_80: 0.4988 (0.5557)  loss_n_100: 0.5471 (0.6016)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0286)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0049)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [470/845]  eta: 0:02:05  loss: 1.9778 (2.1619)  loss_n_40: 0.4437 (0.4741)  loss_n_60: 0.4438 (0.4929)  loss_n_80: 0.4988 (0.5548)  loss_n_100: 0.5574 (0.6011)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0280)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0048)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [480/845]  eta: 0:02:02  loss: 2.1184 (2.1639)  loss_n_40: 0.4375 (0.4748)  loss_n_60: 0.4870 (0.4932)  loss_n_80: 0.5500 (0.5557)  loss_n_100: 0.5763 (0.6020)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0274)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0047)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [490/845]  eta: 0:01:58  loss: 2.1869 (2.1611)  loss_n_40: 0.4533 (0.4743)  loss_n_60: 0.4888 (0.4927)  loss_n_80: 0.5589 (0.5553)  loss_n_100: 0.5590 (0.6013)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0268)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0046)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [500/845]  eta: 0:01:55  loss: 2.0942 (2.1599)  loss_n_40: 0.4254 (0.4741)  loss_n_60: 0.4736 (0.4930)  loss_n_80: 0.5212 (0.5550)  loss_n_100: 0.5590 (0.6012)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0263)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0045)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [510/845]  eta: 0:01:52  loss: 2.0118 (2.1595)  loss_n_40: 0.4457 (0.4740)  loss_n_60: 0.4607 (0.4928)  loss_n_80: 0.5288 (0.5553)  loss_n_100: 0.5595 (0.6015)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0258)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0044)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [520/845]  eta: 0:01:48  loss: 2.1368 (2.1602)  loss_n_40: 0.4568 (0.4740)  loss_n_60: 0.4821 (0.4930)  loss_n_80: 0.5740 (0.5556)  loss_n_100: 0.6250 (0.6023)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0253)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0043)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [530/845]  eta: 0:01:45  loss: 1.9462 (2.1580)  loss_n_40: 0.4459 (0.4738)  loss_n_60: 0.4490 (0.4927)  loss_n_80: 0.5700 (0.5555)  loss_n_100: 0.5459 (0.6014)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0248)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0043)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [540/845]  eta: 0:01:42  loss: 1.9076 (2.1567)  loss_n_40: 0.4327 (0.4742)  loss_n_60: 0.4258 (0.4924)  loss_n_80: 0.5034 (0.5552)  loss_n_100: 0.5184 (0.6009)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0244)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0042)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [550/845]  eta: 0:01:38  loss: 2.0680 (2.1554)  loss_n_40: 0.4619 (0.4743)  loss_n_60: 0.4531 (0.4928)  loss_n_80: 0.5352 (0.5548)  loss_n_100: 0.5184 (0.6001)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0239)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0041)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [560/845]  eta: 0:01:35  loss: 2.0670 (2.1547)  loss_n_40: 0.4525 (0.4746)  loss_n_60: 0.4488 (0.4926)  loss_n_80: 0.5497 (0.5550)  loss_n_100: 0.5435 (0.5998)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0039)  triple_40: 0.0000 (0.0040)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [570/845]  eta: 0:01:32  loss: 1.9870 (2.1561)  loss_n_40: 0.4364 (0.4761)  loss_n_60: 0.4488 (0.4933)  loss_n_80: 0.5516 (0.5551)  loss_n_100: 0.5438 (0.5995)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0231)  triple_60: 0.0000 (0.0038)  triple_40: 0.0000 (0.0040)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [580/845]  eta: 0:01:28  loss: 2.1139 (2.1554)  loss_n_40: 0.4443 (0.4758)  loss_n_60: 0.4675 (0.4930)  loss_n_80: 0.5506 (0.5555)  loss_n_100: 0.5608 (0.5995)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0227)  triple_60: 0.0000 (0.0038)  triple_40: 0.0000 (0.0039)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [590/845]  eta: 0:01:25  loss: 1.9517 (2.1518)  loss_n_40: 0.4432 (0.4751)  loss_n_60: 0.4537 (0.4922)  loss_n_80: 0.5286 (0.5546)  loss_n_100: 0.5581 (0.5989)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0223)  triple_60: 0.0000 (0.0037)  triple_40: 0.0000 (0.0038)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [600/845]  eta: 0:01:21  loss: 2.0513 (2.1549)  loss_n_40: 0.4456 (0.4756)  loss_n_60: 0.4941 (0.4932)  loss_n_80: 0.5347 (0.5547)  loss_n_100: 0.5393 (0.5987)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0219)  triple_60: 0.0000 (0.0036)  triple_40: 0.0000 (0.0038)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [610/845]  eta: 0:01:18  loss: 2.1870 (2.1548)  loss_n_40: 0.4832 (0.4754)  loss_n_60: 0.5214 (0.4933)  loss_n_80: 0.5800 (0.5550)  loss_n_100: 0.5845 (0.5989)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0216)  triple_60: 0.0000 (0.0036)  triple_40: 0.0000 (0.0037)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [620/845]  eta: 0:01:15  loss: 2.0658 (2.1543)  loss_n_40: 0.4790 (0.4761)  loss_n_60: 0.4528 (0.4930)  loss_n_80: 0.5802 (0.5552)  loss_n_100: 0.5318 (0.5984)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0212)  triple_60: 0.0000 (0.0035)  triple_40: 0.0000 (0.0036)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [630/845]  eta: 0:01:11  loss: 2.0221 (2.1524)  loss_n_40: 0.4309 (0.4754)  loss_n_60: 0.4585 (0.4927)  loss_n_80: 0.5563 (0.5549)  loss_n_100: 0.5606 (0.5983)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0209)  triple_60: 0.0000 (0.0035)  triple_40: 0.0000 (0.0036)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [640/845]  eta: 0:01:08  loss: 2.0681 (2.1519)  loss_n_40: 0.4309 (0.4759)  loss_n_60: 0.4673 (0.4928)  loss_n_80: 0.5180 (0.5546)  loss_n_100: 0.5690 (0.5979)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0206)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0035)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [650/845]  eta: 0:01:05  loss: 2.0825 (2.1524)  loss_n_40: 0.4491 (0.4759)  loss_n_60: 0.4673 (0.4928)  loss_n_80: 0.5111 (0.5552)  loss_n_100: 0.5646 (0.5983)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0202)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0035)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [660/845]  eta: 0:01:01  loss: 2.2121 (2.1543)  loss_n_40: 0.4545 (0.4760)  loss_n_60: 0.5061 (0.4932)  loss_n_80: 0.5837 (0.5560)  loss_n_100: 0.6517 (0.5993)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0199)  triple_60: 0.0000 (0.0033)  triple_40: 0.0000 (0.0034)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [670/845]  eta: 0:00:58  loss: 2.1942 (2.1535)  loss_n_40: 0.4757 (0.4760)  loss_n_60: 0.5061 (0.4931)  loss_n_80: 0.5889 (0.5562)  loss_n_100: 0.6209 (0.5989)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0196)  triple_60: 0.0000 (0.0033)  triple_40: 0.0000 (0.0034)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [680/845]  eta: 0:00:55  loss: 2.1569 (2.1585)  loss_n_40: 0.4670 (0.4759)  loss_n_60: 0.4869 (0.4930)  loss_n_80: 0.5833 (0.5569)  loss_n_100: 0.5890 (0.5998)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0209)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0046)  time: 0.3340  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:12]  [690/845]  eta: 0:00:51  loss: 2.2084 (2.1584)  loss_n_40: 0.4509 (0.4761)  loss_n_60: 0.4754 (0.4930)  loss_n_80: 0.5690 (0.5570)  loss_n_100: 0.6083 (0.5998)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0206)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0046)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [700/845]  eta: 0:00:48  loss: 2.2041 (2.1688)  loss_n_40: 0.4708 (0.4763)  loss_n_60: 0.4877 (0.4935)  loss_n_80: 0.5951 (0.5575)  loss_n_100: 0.6084 (0.6003)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0233)  triple_60: 0.0000 (0.0066)  triple_40: 0.0000 (0.0084)  time: 0.3335  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [710/845]  eta: 0:00:45  loss: 2.1843 (2.1670)  loss_n_40: 0.4371 (0.4757)  loss_n_60: 0.4961 (0.4931)  loss_n_80: 0.5554 (0.5573)  loss_n_100: 0.6084 (0.6003)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0065)  triple_40: 0.0000 (0.0083)  time: 0.3336  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [720/845]  eta: 0:00:41  loss: 1.9828 (2.1684)  loss_n_40: 0.4308 (0.4761)  loss_n_60: 0.4678 (0.4933)  loss_n_80: 0.5302 (0.5578)  loss_n_100: 0.5799 (0.6005)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0234)  triple_60: 0.0000 (0.0064)  triple_40: 0.0000 (0.0082)  time: 0.3336  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [730/845]  eta: 0:00:38  loss: 1.9490 (2.1671)  loss_n_40: 0.4353 (0.4755)  loss_n_60: 0.4548 (0.4928)  loss_n_80: 0.5148 (0.5578)  loss_n_100: 0.5655 (0.6008)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0063)  triple_40: 0.0000 (0.0081)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [740/845]  eta: 0:00:35  loss: 1.8746 (2.1678)  loss_n_40: 0.4385 (0.4765)  loss_n_60: 0.4171 (0.4935)  loss_n_80: 0.4989 (0.5576)  loss_n_100: 0.5238 (0.6004)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0227)  triple_60: 0.0000 (0.0062)  triple_40: 0.0000 (0.0080)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [750/845]  eta: 0:00:31  loss: 1.8950 (2.1646)  loss_n_40: 0.4416 (0.4765)  loss_n_60: 0.4281 (0.4929)  loss_n_80: 0.4989 (0.5568)  loss_n_100: 0.5173 (0.5993)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0224)  triple_60: 0.0000 (0.0062)  triple_40: 0.0000 (0.0079)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [760/845]  eta: 0:00:28  loss: 2.0619 (2.1641)  loss_n_40: 0.4192 (0.4766)  loss_n_60: 0.4398 (0.4926)  loss_n_80: 0.5152 (0.5570)  loss_n_100: 0.5048 (0.5993)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0221)  triple_60: 0.0000 (0.0061)  triple_40: 0.0000 (0.0078)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [770/845]  eta: 0:00:25  loss: 2.1174 (2.1652)  loss_n_40: 0.4528 (0.4769)  loss_n_60: 0.4699 (0.4926)  loss_n_80: 0.5783 (0.5576)  loss_n_100: 0.6146 (0.6000)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0218)  triple_60: 0.0000 (0.0060)  triple_40: 0.0000 (0.0077)  time: 0.3336  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [780/845]  eta: 0:00:21  loss: 2.1809 (2.1665)  loss_n_40: 0.4873 (0.4771)  loss_n_60: 0.4802 (0.4933)  loss_n_80: 0.5890 (0.5581)  loss_n_100: 0.6165 (0.6003)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0216)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0076)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [790/845]  eta: 0:00:18  loss: 2.1205 (2.1655)  loss_n_40: 0.4665 (0.4774)  loss_n_60: 0.4909 (0.4932)  loss_n_80: 0.5473 (0.5579)  loss_n_100: 0.5961 (0.5999)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0213)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0075)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [800/845]  eta: 0:00:15  loss: 2.0972 (2.1656)  loss_n_40: 0.4629 (0.4776)  loss_n_60: 0.4850 (0.4932)  loss_n_80: 0.5198 (0.5578)  loss_n_100: 0.5529 (0.6003)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0210)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0074)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [810/845]  eta: 0:00:11  loss: 1.9843 (2.1647)  loss_n_40: 0.4736 (0.4776)  loss_n_60: 0.4636 (0.4929)  loss_n_80: 0.5102 (0.5578)  loss_n_100: 0.5529 (0.6002)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0208)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0073)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [820/845]  eta: 0:00:08  loss: 1.9557 (2.1620)  loss_n_40: 0.4235 (0.4766)  loss_n_60: 0.4313 (0.4927)  loss_n_80: 0.5102 (0.5571)  loss_n_100: 0.5344 (0.5997)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0205)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0072)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [830/845]  eta: 0:00:05  loss: 1.9714 (2.1610)  loss_n_40: 0.4171 (0.4766)  loss_n_60: 0.4236 (0.4924)  loss_n_80: 0.5049 (0.5570)  loss_n_100: 0.5621 (0.5996)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0203)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0071)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [840/845]  eta: 0:00:01  loss: 2.0193 (2.1610)  loss_n_40: 0.4418 (0.4770)  loss_n_60: 0.4344 (0.4924)  loss_n_80: 0.5541 (0.5571)  loss_n_100: 0.5635 (0.5996)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0200)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0070)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12]  [844/845]  eta: 0:00:00  loss: 2.0620 (2.1665)  loss_n_40: 0.4796 (0.4772)  loss_n_60: 0.4806 (0.4927)  loss_n_80: 0.5655 (0.5577)  loss_n_100: 0.5883 (0.6000)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0075)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:12] Total time: 0:04:42 (0.3345 s / it)\n",
      "Averaged stats: loss: 2.0620 (2.1665)  loss_n_40: 0.4796 (0.4772)  loss_n_60: 0.4806 (0.4927)  loss_n_80: 0.5655 (0.5577)  loss_n_100: 0.5883 (0.6000)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0075)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_12_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.600%\n",
      "Min loss_n_100: 0.575\n",
      "Best Epoch: 10.000\n",
      "Train: [epoch:13]  [   0/1724]  eta: 1:59:55  lr: 0.000200  loss: 2.4878 (2.4878)  loss_n_40: 0.4897 (0.4897)  loss_n_60: 0.5155 (0.5155)  loss_n_80: 0.6258 (0.6258)  loss_n_100: 0.6565 (0.6565)  triple_100: 0.0000 (0.0000)  triple_80: 0.2003 (0.2003)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1737  data: 0.4183  max mem: 46473\n",
      "Train: [epoch:13]  [  10/1724]  eta: 1:52:33  lr: 0.000200  loss: 2.1326 (2.1711)  loss_n_40: 0.4652 (0.4771)  loss_n_60: 0.4788 (0.4881)  loss_n_80: 0.5690 (0.5700)  loss_n_100: 0.5924 (0.6014)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0341)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0005)  time: 3.9400  data: 0.0382  max mem: 46473\n",
      "Train: [epoch:13]  [  20/1724]  eta: 1:51:35  lr: 0.000200  loss: 2.0283 (2.0633)  loss_n_40: 0.4402 (0.4521)  loss_n_60: 0.4681 (0.4642)  loss_n_80: 0.5334 (0.5408)  loss_n_100: 0.5706 (0.5881)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0179)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0002)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [  30/1724]  eta: 1:50:50  lr: 0.000200  loss: 1.8980 (2.0005)  loss_n_40: 0.4299 (0.4401)  loss_n_60: 0.4419 (0.4546)  loss_n_80: 0.4883 (0.5222)  loss_n_100: 0.5428 (0.5713)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0121)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0002)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [  40/1724]  eta: 1:50:08  lr: 0.000200  loss: 1.8020 (1.9322)  loss_n_40: 0.3728 (0.4306)  loss_n_60: 0.4098 (0.4391)  loss_n_80: 0.4483 (0.5005)  loss_n_100: 0.5014 (0.5528)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0092)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0001)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [  50/1724]  eta: 1:49:27  lr: 0.000200  loss: 1.6711 (1.8848)  loss_n_40: 0.3576 (0.4171)  loss_n_60: 0.3840 (0.4288)  loss_n_80: 0.4403 (0.4906)  loss_n_100: 0.4862 (0.5408)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0074)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0001)  time: 3.9186  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [  60/1724]  eta: 1:48:47  lr: 0.000200  loss: 1.7013 (1.8685)  loss_n_40: 0.3631 (0.4127)  loss_n_60: 0.3940 (0.4286)  loss_n_80: 0.4626 (0.4861)  loss_n_100: 0.5013 (0.5348)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0001)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [  70/1724]  eta: 1:48:06  lr: 0.000200  loss: 1.7301 (1.8475)  loss_n_40: 0.3777 (0.4104)  loss_n_60: 0.4104 (0.4263)  loss_n_80: 0.4377 (0.4797)  loss_n_100: 0.4794 (0.5258)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0001)  time: 3.9173  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [  80/1724]  eta: 1:47:25  lr: 0.000200  loss: 1.7053 (1.8302)  loss_n_40: 0.3876 (0.4109)  loss_n_60: 0.4121 (0.4236)  loss_n_80: 0.4324 (0.4722)  loss_n_100: 0.4753 (0.5188)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0046)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0001)  time: 3.9158  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [  90/1724]  eta: 1:46:45  lr: 0.000200  loss: 1.5654 (1.7902)  loss_n_40: 0.3617 (0.4026)  loss_n_60: 0.3563 (0.4146)  loss_n_80: 0.3973 (0.4617)  loss_n_100: 0.4276 (0.5071)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0041)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0001)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 100/1724]  eta: 1:46:05  lr: 0.000200  loss: 1.5095 (1.7702)  loss_n_40: 0.3350 (0.3983)  loss_n_60: 0.3563 (0.4107)  loss_n_80: 0.3929 (0.4574)  loss_n_100: 0.4135 (0.5000)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0037)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0001)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 110/1724]  eta: 1:45:26  lr: 0.000200  loss: 1.5265 (1.7678)  loss_n_40: 0.3350 (0.3946)  loss_n_60: 0.3641 (0.4073)  loss_n_80: 0.4122 (0.4532)  loss_n_100: 0.4233 (0.4943)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0034)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0151)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 120/1724]  eta: 1:44:46  lr: 0.000200  loss: 1.4996 (1.7501)  loss_n_40: 0.3150 (0.3897)  loss_n_60: 0.3593 (0.4040)  loss_n_80: 0.3815 (0.4494)  loss_n_100: 0.4233 (0.4900)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0031)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0139)  time: 3.9171  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 130/1724]  eta: 1:44:07  lr: 0.000200  loss: 1.6558 (1.7570)  loss_n_40: 0.3486 (0.3897)  loss_n_60: 0.3955 (0.4064)  loss_n_80: 0.4530 (0.4522)  loss_n_100: 0.4587 (0.4926)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0029)  triple_60: 0.0000 (0.0004)  triple_40: 0.0000 (0.0128)  time: 3.9160  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 140/1724]  eta: 1:43:27  lr: 0.000200  loss: 1.7006 (1.7476)  loss_n_40: 0.3581 (0.3866)  loss_n_60: 0.4049 (0.4052)  loss_n_80: 0.4453 (0.4505)  loss_n_100: 0.4831 (0.4904)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0027)  triple_60: 0.0000 (0.0004)  triple_40: 0.0000 (0.0119)  time: 3.9149  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 150/1724]  eta: 1:42:47  lr: 0.000200  loss: 1.6163 (1.7397)  loss_n_40: 0.3359 (0.3847)  loss_n_60: 0.3818 (0.4047)  loss_n_80: 0.4186 (0.4483)  loss_n_100: 0.4609 (0.4881)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0025)  triple_60: 0.0000 (0.0004)  triple_40: 0.0000 (0.0111)  time: 3.9148  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 160/1724]  eta: 1:42:08  lr: 0.000200  loss: 1.4647 (1.7260)  loss_n_40: 0.3411 (0.3823)  loss_n_60: 0.3511 (0.4018)  loss_n_80: 0.3780 (0.4447)  loss_n_100: 0.4063 (0.4840)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0023)  triple_60: 0.0000 (0.0003)  triple_40: 0.0000 (0.0104)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 170/1724]  eta: 1:41:28  lr: 0.000200  loss: 1.4647 (1.7210)  loss_n_40: 0.3520 (0.3821)  loss_n_60: 0.3482 (0.4013)  loss_n_80: 0.3796 (0.4433)  loss_n_100: 0.4063 (0.4819)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0022)  triple_60: 0.0000 (0.0003)  triple_40: 0.0000 (0.0098)  time: 3.9159  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 180/1724]  eta: 1:40:49  lr: 0.000200  loss: 1.4579 (1.7102)  loss_n_40: 0.3158 (0.3802)  loss_n_60: 0.3513 (0.3993)  loss_n_80: 0.3704 (0.4403)  loss_n_100: 0.4126 (0.4787)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0021)  triple_60: 0.0000 (0.0003)  triple_40: 0.0000 (0.0093)  time: 3.9147  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 190/1724]  eta: 1:40:10  lr: 0.000200  loss: 1.5411 (1.7249)  loss_n_40: 0.3438 (0.3811)  loss_n_60: 0.3568 (0.4003)  loss_n_80: 0.3876 (0.4415)  loss_n_100: 0.4339 (0.4813)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0037)  triple_40: 0.0000 (0.0105)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 200/1724]  eta: 1:39:31  lr: 0.000200  loss: 1.9398 (1.7403)  loss_n_40: 0.4284 (0.3832)  loss_n_60: 0.4350 (0.4028)  loss_n_80: 0.5019 (0.4451)  loss_n_100: 0.5514 (0.4860)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0108)  time: 3.9175  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 210/1724]  eta: 1:38:51  lr: 0.000200  loss: 1.7913 (1.7380)  loss_n_40: 0.4105 (0.3828)  loss_n_60: 0.4054 (0.4021)  loss_n_80: 0.4766 (0.4447)  loss_n_100: 0.5386 (0.4864)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0064)  triple_60: 0.0000 (0.0039)  triple_40: 0.0000 (0.0103)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 220/1724]  eta: 1:38:12  lr: 0.000200  loss: 1.6458 (1.7368)  loss_n_40: 0.3497 (0.3830)  loss_n_60: 0.3788 (0.4018)  loss_n_80: 0.4202 (0.4443)  loss_n_100: 0.4858 (0.4866)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0037)  triple_40: 0.0000 (0.0099)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 230/1724]  eta: 1:37:33  lr: 0.000200  loss: 1.5825 (1.7296)  loss_n_40: 0.3256 (0.3818)  loss_n_60: 0.3777 (0.4000)  loss_n_80: 0.4097 (0.4426)  loss_n_100: 0.4612 (0.4850)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0036)  triple_40: 0.0000 (0.0094)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 240/1724]  eta: 1:36:54  lr: 0.000200  loss: 1.4727 (1.7180)  loss_n_40: 0.3171 (0.3793)  loss_n_60: 0.3357 (0.3975)  loss_n_80: 0.3741 (0.4395)  loss_n_100: 0.4275 (0.4824)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0056)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0090)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 250/1724]  eta: 1:36:15  lr: 0.000200  loss: 1.4483 (1.7092)  loss_n_40: 0.3124 (0.3772)  loss_n_60: 0.3390 (0.3958)  loss_n_80: 0.3635 (0.4373)  loss_n_100: 0.4219 (0.4803)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0033)  triple_40: 0.0000 (0.0087)  time: 3.9195  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 260/1724]  eta: 1:35:36  lr: 0.000200  loss: 1.4900 (1.7052)  loss_n_40: 0.3231 (0.3770)  loss_n_60: 0.3501 (0.3955)  loss_n_80: 0.3944 (0.4361)  loss_n_100: 0.4264 (0.4788)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0032)  triple_40: 0.0000 (0.0083)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 270/1724]  eta: 1:34:57  lr: 0.000200  loss: 1.4736 (1.6957)  loss_n_40: 0.3193 (0.3751)  loss_n_60: 0.3498 (0.3936)  loss_n_80: 0.3718 (0.4335)  loss_n_100: 0.4155 (0.4763)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0030)  triple_40: 0.0000 (0.0080)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 280/1724]  eta: 1:34:17  lr: 0.000200  loss: 1.4217 (1.7152)  loss_n_40: 0.3078 (0.3737)  loss_n_60: 0.3303 (0.3920)  loss_n_80: 0.3682 (0.4318)  loss_n_100: 0.4111 (0.4745)  triple_100: 0.0000 (0.0092)  triple_80: 0.0000 (0.0122)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0145)  time: 3.9169  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 290/1724]  eta: 1:33:38  lr: 0.000200  loss: 1.7166 (1.7567)  loss_n_40: 0.3781 (0.3770)  loss_n_60: 0.4016 (0.3967)  loss_n_80: 0.4560 (0.4395)  loss_n_100: 0.5339 (0.4841)  triple_100: 0.0000 (0.0108)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0126)  triple_40: 0.0000 (0.0204)  time: 3.9159  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [ 300/1724]  eta: 1:32:59  lr: 0.000200  loss: 3.1334 (1.8176)  loss_n_40: 0.5812 (0.3866)  loss_n_60: 0.6523 (0.4089)  loss_n_80: 0.8595 (0.4572)  loss_n_100: 0.9722 (0.5065)  triple_100: 0.0000 (0.0107)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0121)  triple_40: 0.0000 (0.0197)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 310/1724]  eta: 1:32:19  lr: 0.000200  loss: 3.2192 (1.8600)  loss_n_40: 0.6242 (0.3960)  loss_n_60: 0.6877 (0.4179)  loss_n_80: 0.8695 (0.4688)  loss_n_100: 1.0300 (0.5208)  triple_100: 0.0000 (0.0104)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0117)  triple_40: 0.0000 (0.0191)  time: 3.9149  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 320/1724]  eta: 1:31:40  lr: 0.000200  loss: 2.8394 (1.8872)  loss_n_40: 0.6242 (0.4031)  loss_n_60: 0.6179 (0.4232)  loss_n_80: 0.7395 (0.4762)  loss_n_100: 0.8840 (0.5299)  triple_100: 0.0000 (0.0100)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0114)  triple_40: 0.0000 (0.0185)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 330/1724]  eta: 1:31:01  lr: 0.000200  loss: 2.5725 (1.8970)  loss_n_40: 0.5495 (0.4057)  loss_n_60: 0.5316 (0.4259)  loss_n_80: 0.6316 (0.4792)  loss_n_100: 0.7111 (0.5332)  triple_100: 0.0000 (0.0097)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0179)  time: 3.9171  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 340/1724]  eta: 1:30:22  lr: 0.000200  loss: 2.0573 (1.9020)  loss_n_40: 0.4791 (0.4073)  loss_n_60: 0.4802 (0.4274)  loss_n_80: 0.5404 (0.4807)  loss_n_100: 0.6242 (0.5351)  triple_100: 0.0000 (0.0095)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0107)  triple_40: 0.0000 (0.0174)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 350/1724]  eta: 1:29:43  lr: 0.000200  loss: 1.9867 (1.9042)  loss_n_40: 0.4405 (0.4084)  loss_n_60: 0.4533 (0.4285)  loss_n_80: 0.4924 (0.4812)  loss_n_100: 0.5694 (0.5360)  triple_100: 0.0000 (0.0092)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0169)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 360/1724]  eta: 1:29:03  lr: 0.000200  loss: 1.9176 (1.9049)  loss_n_40: 0.4315 (0.4097)  loss_n_60: 0.4410 (0.4293)  loss_n_80: 0.4713 (0.4810)  loss_n_100: 0.5389 (0.5363)  triple_100: 0.0000 (0.0089)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0101)  triple_40: 0.0000 (0.0165)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 370/1724]  eta: 1:28:24  lr: 0.000200  loss: 1.7723 (1.9016)  loss_n_40: 0.4138 (0.4097)  loss_n_60: 0.4111 (0.4291)  loss_n_80: 0.4438 (0.4802)  loss_n_100: 0.5092 (0.5352)  triple_100: 0.0000 (0.0087)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0160)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 380/1724]  eta: 1:27:45  lr: 0.000200  loss: 1.7396 (1.8978)  loss_n_40: 0.3797 (0.4088)  loss_n_60: 0.3968 (0.4286)  loss_n_80: 0.4362 (0.4796)  loss_n_100: 0.4992 (0.5346)  triple_100: 0.0000 (0.0085)  triple_80: 0.0000 (0.0125)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0156)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 390/1724]  eta: 1:27:06  lr: 0.000200  loss: 1.6155 (1.8910)  loss_n_40: 0.3618 (0.4076)  loss_n_60: 0.3787 (0.4274)  loss_n_80: 0.4190 (0.4780)  loss_n_100: 0.4829 (0.5329)  triple_100: 0.0000 (0.0082)  triple_80: 0.0000 (0.0122)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0152)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 400/1724]  eta: 1:26:27  lr: 0.000200  loss: 1.5951 (1.8851)  loss_n_40: 0.3588 (0.4073)  loss_n_60: 0.3787 (0.4266)  loss_n_80: 0.3998 (0.4764)  loss_n_100: 0.4625 (0.5310)  triple_100: 0.0000 (0.0080)  triple_80: 0.0000 (0.0119)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0148)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 410/1724]  eta: 1:25:47  lr: 0.000200  loss: 1.6497 (1.8780)  loss_n_40: 0.3679 (0.4068)  loss_n_60: 0.3843 (0.4252)  loss_n_80: 0.3976 (0.4746)  loss_n_100: 0.4567 (0.5286)  triple_100: 0.0000 (0.0078)  triple_80: 0.0000 (0.0116)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0145)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 420/1724]  eta: 1:25:08  lr: 0.000200  loss: 1.6294 (1.8722)  loss_n_40: 0.3610 (0.4059)  loss_n_60: 0.3806 (0.4243)  loss_n_80: 0.4148 (0.4733)  loss_n_100: 0.4567 (0.5269)  triple_100: 0.0000 (0.0077)  triple_80: 0.0000 (0.0113)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0141)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 430/1724]  eta: 1:24:29  lr: 0.000200  loss: 1.5921 (1.8658)  loss_n_40: 0.3517 (0.4051)  loss_n_60: 0.3682 (0.4234)  loss_n_80: 0.3976 (0.4715)  loss_n_100: 0.4506 (0.5250)  triple_100: 0.0000 (0.0075)  triple_80: 0.0000 (0.0110)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0138)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 440/1724]  eta: 1:23:50  lr: 0.000200  loss: 1.5418 (1.8594)  loss_n_40: 0.3509 (0.4045)  loss_n_60: 0.3682 (0.4224)  loss_n_80: 0.3886 (0.4698)  loss_n_100: 0.4276 (0.5228)  triple_100: 0.0000 (0.0073)  triple_80: 0.0000 (0.0108)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0135)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 450/1724]  eta: 1:23:11  lr: 0.000200  loss: 1.4731 (1.8527)  loss_n_40: 0.3196 (0.4043)  loss_n_60: 0.3548 (0.4213)  loss_n_80: 0.3806 (0.4678)  loss_n_100: 0.4037 (0.5204)  triple_100: 0.0000 (0.0071)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0132)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 460/1724]  eta: 1:22:32  lr: 0.000200  loss: 1.5018 (1.8456)  loss_n_40: 0.3196 (0.4032)  loss_n_60: 0.3589 (0.4199)  loss_n_80: 0.3807 (0.4662)  loss_n_100: 0.4037 (0.5182)  triple_100: 0.0000 (0.0070)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0129)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 470/1724]  eta: 1:21:52  lr: 0.000200  loss: 1.4704 (1.8393)  loss_n_40: 0.3259 (0.4020)  loss_n_60: 0.3512 (0.4190)  loss_n_80: 0.3841 (0.4647)  loss_n_100: 0.4092 (0.5162)  triple_100: 0.0000 (0.0068)  triple_80: 0.0000 (0.0101)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0126)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 480/1724]  eta: 1:21:13  lr: 0.000200  loss: 1.4044 (1.8304)  loss_n_40: 0.3206 (0.4004)  loss_n_60: 0.3311 (0.4173)  loss_n_80: 0.3627 (0.4625)  loss_n_100: 0.4009 (0.5136)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0099)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0124)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 490/1724]  eta: 1:20:34  lr: 0.000200  loss: 1.4281 (1.8231)  loss_n_40: 0.3183 (0.3992)  loss_n_60: 0.3363 (0.4160)  loss_n_80: 0.3640 (0.4607)  loss_n_100: 0.3936 (0.5114)  triple_100: 0.0000 (0.0066)  triple_80: 0.0000 (0.0097)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0121)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 500/1724]  eta: 1:19:55  lr: 0.000200  loss: 1.3740 (1.8146)  loss_n_40: 0.3118 (0.3979)  loss_n_60: 0.3286 (0.4143)  loss_n_80: 0.3572 (0.4586)  loss_n_100: 0.3887 (0.5087)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0095)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0119)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 510/1724]  eta: 1:19:16  lr: 0.000200  loss: 1.3587 (1.8078)  loss_n_40: 0.2889 (0.3964)  loss_n_60: 0.3177 (0.4128)  loss_n_80: 0.3568 (0.4569)  loss_n_100: 0.3833 (0.5065)  triple_100: 0.0000 (0.0063)  triple_80: 0.0000 (0.0094)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0119)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 520/1724]  eta: 1:18:36  lr: 0.000200  loss: 1.5572 (1.8051)  loss_n_40: 0.3276 (0.3961)  loss_n_60: 0.3552 (0.4123)  loss_n_80: 0.3989 (0.4563)  loss_n_100: 0.4271 (0.5059)  triple_100: 0.0000 (0.0062)  triple_80: 0.0000 (0.0092)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0117)  time: 3.9166  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 530/1724]  eta: 1:17:57  lr: 0.000200  loss: 1.6765 (1.8020)  loss_n_40: 0.3569 (0.3957)  loss_n_60: 0.3912 (0.4119)  loss_n_80: 0.4288 (0.4555)  loss_n_100: 0.4829 (0.5051)  triple_100: 0.0000 (0.0061)  triple_80: 0.0000 (0.0090)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0114)  time: 3.9163  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [ 540/1724]  eta: 1:17:18  lr: 0.000200  loss: 1.5871 (1.7976)  loss_n_40: 0.3413 (0.3948)  loss_n_60: 0.3714 (0.4111)  loss_n_80: 0.4065 (0.4546)  loss_n_100: 0.4496 (0.5039)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0089)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0112)  time: 3.9157  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 550/1724]  eta: 1:16:39  lr: 0.000200  loss: 1.5546 (1.7933)  loss_n_40: 0.3311 (0.3941)  loss_n_60: 0.3598 (0.4105)  loss_n_80: 0.4111 (0.4537)  loss_n_100: 0.4130 (0.5024)  triple_100: 0.0000 (0.0059)  triple_80: 0.0000 (0.0087)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0110)  time: 3.9150  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 560/1724]  eta: 1:16:00  lr: 0.000200  loss: 1.4655 (1.7858)  loss_n_40: 0.3127 (0.3927)  loss_n_60: 0.3413 (0.4090)  loss_n_80: 0.3808 (0.4518)  loss_n_100: 0.3978 (0.5002)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0086)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0108)  time: 3.9151  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 570/1724]  eta: 1:15:20  lr: 0.000200  loss: 1.3847 (1.7805)  loss_n_40: 0.3127 (0.3916)  loss_n_60: 0.3314 (0.4081)  loss_n_80: 0.3528 (0.4506)  loss_n_100: 0.3782 (0.4988)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0084)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0106)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 580/1724]  eta: 1:14:41  lr: 0.000200  loss: 1.5144 (1.7962)  loss_n_40: 0.3566 (0.3914)  loss_n_60: 0.3673 (0.4082)  loss_n_80: 0.3917 (0.4507)  loss_n_100: 0.4224 (0.4991)  triple_100: 0.0000 (0.0102)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0103)  triple_40: 0.0000 (0.0133)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 590/1724]  eta: 1:14:02  lr: 0.000200  loss: 2.3657 (1.8114)  loss_n_40: 0.4180 (0.3929)  loss_n_60: 0.5145 (0.4107)  loss_n_80: 0.5612 (0.4543)  loss_n_100: 0.6310 (0.5034)  triple_100: 0.0000 (0.0104)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0101)  triple_40: 0.0000 (0.0167)  time: 3.9160  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 600/1724]  eta: 1:13:23  lr: 0.000200  loss: 2.2940 (1.8166)  loss_n_40: 0.4313 (0.3939)  loss_n_60: 0.5145 (0.4118)  loss_n_80: 0.6095 (0.4562)  loss_n_100: 0.6807 (0.5055)  triple_100: 0.0000 (0.0102)  triple_80: 0.0000 (0.0126)  triple_60: 0.0000 (0.0099)  triple_40: 0.0000 (0.0165)  time: 3.9145  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 610/1724]  eta: 1:12:44  lr: 0.000200  loss: 2.0928 (1.8205)  loss_n_40: 0.4272 (0.3945)  loss_n_60: 0.4645 (0.4127)  loss_n_80: 0.5529 (0.4575)  loss_n_100: 0.6223 (0.5074)  triple_100: 0.0000 (0.0101)  triple_80: 0.0000 (0.0124)  triple_60: 0.0000 (0.0097)  triple_40: 0.0000 (0.0162)  time: 3.9156  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 620/1724]  eta: 1:12:04  lr: 0.000200  loss: 1.8820 (1.8214)  loss_n_40: 0.3975 (0.3947)  loss_n_60: 0.4356 (0.4131)  loss_n_80: 0.5068 (0.4581)  loss_n_100: 0.5800 (0.5079)  triple_100: 0.0000 (0.0099)  triple_80: 0.0000 (0.0122)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0159)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 630/1724]  eta: 1:11:25  lr: 0.000200  loss: 1.7961 (1.8213)  loss_n_40: 0.3816 (0.3948)  loss_n_60: 0.4075 (0.4133)  loss_n_80: 0.4635 (0.4582)  loss_n_100: 0.5238 (0.5082)  triple_100: 0.0000 (0.0098)  triple_80: 0.0000 (0.0120)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0157)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 640/1724]  eta: 1:10:46  lr: 0.000200  loss: 1.7764 (1.8209)  loss_n_40: 0.3678 (0.3952)  loss_n_60: 0.4075 (0.4134)  loss_n_80: 0.4311 (0.4580)  loss_n_100: 0.5160 (0.5082)  triple_100: 0.0000 (0.0096)  triple_80: 0.0000 (0.0118)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0154)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 650/1724]  eta: 1:10:07  lr: 0.000200  loss: 1.6778 (1.8190)  loss_n_40: 0.3634 (0.3950)  loss_n_60: 0.3915 (0.4132)  loss_n_80: 0.4268 (0.4576)  loss_n_100: 0.4896 (0.5078)  triple_100: 0.0000 (0.0095)  triple_80: 0.0000 (0.0116)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0152)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 660/1724]  eta: 1:09:28  lr: 0.000200  loss: 1.6778 (1.8157)  loss_n_40: 0.3605 (0.3946)  loss_n_60: 0.3915 (0.4127)  loss_n_80: 0.4172 (0.4569)  loss_n_100: 0.4647 (0.5068)  triple_100: 0.0000 (0.0093)  triple_80: 0.0000 (0.0114)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0150)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 670/1724]  eta: 1:08:48  lr: 0.000200  loss: 1.5251 (1.8120)  loss_n_40: 0.3295 (0.3940)  loss_n_60: 0.3659 (0.4120)  loss_n_80: 0.3859 (0.4560)  loss_n_100: 0.4431 (0.5060)  triple_100: 0.0000 (0.0092)  triple_80: 0.0000 (0.0112)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0147)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 680/1724]  eta: 1:08:09  lr: 0.000200  loss: 1.6435 (1.8101)  loss_n_40: 0.3673 (0.3943)  loss_n_60: 0.3723 (0.4118)  loss_n_80: 0.4023 (0.4555)  loss_n_100: 0.4431 (0.5052)  triple_100: 0.0000 (0.0090)  triple_80: 0.0000 (0.0111)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0145)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 690/1724]  eta: 1:07:30  lr: 0.000200  loss: 1.5791 (1.8068)  loss_n_40: 0.3673 (0.3938)  loss_n_60: 0.3790 (0.4113)  loss_n_80: 0.3944 (0.4547)  loss_n_100: 0.4290 (0.5042)  triple_100: 0.0000 (0.0089)  triple_80: 0.0000 (0.0109)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0143)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 700/1724]  eta: 1:06:51  lr: 0.000200  loss: 1.5379 (1.8037)  loss_n_40: 0.3418 (0.3934)  loss_n_60: 0.3633 (0.4108)  loss_n_80: 0.3944 (0.4540)  loss_n_100: 0.4290 (0.5033)  triple_100: 0.0000 (0.0088)  triple_80: 0.0000 (0.0108)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0141)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 710/1724]  eta: 1:06:12  lr: 0.000200  loss: 1.5366 (1.7995)  loss_n_40: 0.3418 (0.3928)  loss_n_60: 0.3633 (0.4101)  loss_n_80: 0.4043 (0.4530)  loss_n_100: 0.4250 (0.5021)  triple_100: 0.0000 (0.0087)  triple_80: 0.0000 (0.0106)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0139)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 720/1724]  eta: 1:05:33  lr: 0.000200  loss: 1.4538 (1.7955)  loss_n_40: 0.3323 (0.3921)  loss_n_60: 0.3391 (0.4094)  loss_n_80: 0.3464 (0.4521)  loss_n_100: 0.3815 (0.5010)  triple_100: 0.0000 (0.0085)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0137)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 730/1724]  eta: 1:04:53  lr: 0.000200  loss: 1.4571 (1.7907)  loss_n_40: 0.3251 (0.3914)  loss_n_60: 0.3466 (0.4087)  loss_n_80: 0.3497 (0.4508)  loss_n_100: 0.3871 (0.4994)  triple_100: 0.0000 (0.0084)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0135)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 740/1724]  eta: 1:04:14  lr: 0.000200  loss: 1.4571 (1.7857)  loss_n_40: 0.3355 (0.3906)  loss_n_60: 0.3466 (0.4079)  loss_n_80: 0.3497 (0.4495)  loss_n_100: 0.3723 (0.4978)  triple_100: 0.0000 (0.0083)  triple_80: 0.0000 (0.0102)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0133)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 750/1724]  eta: 1:03:35  lr: 0.000200  loss: 1.3033 (1.7796)  loss_n_40: 0.2964 (0.3895)  loss_n_60: 0.3121 (0.4067)  loss_n_80: 0.3237 (0.4480)  loss_n_100: 0.3539 (0.4960)  triple_100: 0.0000 (0.0082)  triple_80: 0.0000 (0.0100)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0132)  time: 3.9190  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 760/1724]  eta: 1:02:56  lr: 0.000200  loss: 1.3033 (1.7739)  loss_n_40: 0.2875 (0.3885)  loss_n_60: 0.3121 (0.4056)  loss_n_80: 0.3233 (0.4466)  loss_n_100: 0.3505 (0.4944)  triple_100: 0.0000 (0.0081)  triple_80: 0.0000 (0.0099)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0130)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 770/1724]  eta: 1:02:17  lr: 0.000200  loss: 1.3430 (1.7688)  loss_n_40: 0.2930 (0.3878)  loss_n_60: 0.3166 (0.4046)  loss_n_80: 0.3461 (0.4453)  loss_n_100: 0.3712 (0.4928)  triple_100: 0.0000 (0.0080)  triple_80: 0.0000 (0.0098)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0128)  time: 3.9197  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [ 780/1724]  eta: 1:01:38  lr: 0.000200  loss: 1.4068 (1.7652)  loss_n_40: 0.3063 (0.3873)  loss_n_60: 0.3312 (0.4039)  loss_n_80: 0.3530 (0.4443)  loss_n_100: 0.3790 (0.4918)  triple_100: 0.0000 (0.0079)  triple_80: 0.0000 (0.0097)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0127)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 790/1724]  eta: 1:00:59  lr: 0.000200  loss: 1.3516 (1.7594)  loss_n_40: 0.2975 (0.3861)  loss_n_60: 0.3277 (0.4029)  loss_n_80: 0.3524 (0.4429)  loss_n_100: 0.3789 (0.4902)  triple_100: 0.0000 (0.0078)  triple_80: 0.0000 (0.0095)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0125)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 800/1724]  eta: 1:00:19  lr: 0.000200  loss: 1.3516 (1.7589)  loss_n_40: 0.3061 (0.3859)  loss_n_60: 0.3277 (0.4024)  loss_n_80: 0.3543 (0.4420)  loss_n_100: 0.3789 (0.4891)  triple_100: 0.0000 (0.0082)  triple_80: 0.0000 (0.0102)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0129)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 810/1724]  eta: 0:59:40  lr: 0.000200  loss: 1.6729 (1.7779)  loss_n_40: 0.3439 (0.3856)  loss_n_60: 0.3986 (0.4026)  loss_n_80: 0.4259 (0.4424)  loss_n_100: 0.4558 (0.4897)  triple_100: 0.0000 (0.0131)  triple_80: 0.0000 (0.0163)  triple_60: 0.0000 (0.0113)  triple_40: 0.0000 (0.0169)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 820/1724]  eta: 0:59:01  lr: 0.000200  loss: 1.9345 (1.7816)  loss_n_40: 0.3581 (0.3858)  loss_n_60: 0.4521 (0.4034)  loss_n_80: 0.5178 (0.4439)  loss_n_100: 0.5894 (0.4916)  triple_100: 0.0000 (0.0129)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0112)  triple_40: 0.0000 (0.0167)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 830/1724]  eta: 0:58:22  lr: 0.000200  loss: 2.1177 (1.7856)  loss_n_40: 0.4157 (0.3865)  loss_n_60: 0.4606 (0.4041)  loss_n_80: 0.5792 (0.4455)  loss_n_100: 0.6567 (0.4934)  triple_100: 0.0000 (0.0128)  triple_80: 0.0000 (0.0159)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0165)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 840/1724]  eta: 0:57:43  lr: 0.000200  loss: 1.9800 (1.7868)  loss_n_40: 0.4205 (0.3871)  loss_n_60: 0.4357 (0.4043)  loss_n_80: 0.5299 (0.4460)  loss_n_100: 0.5785 (0.4939)  triple_100: 0.0000 (0.0126)  triple_80: 0.0000 (0.0157)  triple_60: 0.0000 (0.0109)  triple_40: 0.0000 (0.0163)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 850/1724]  eta: 0:57:03  lr: 0.000200  loss: 1.7480 (1.7858)  loss_n_40: 0.3988 (0.3870)  loss_n_60: 0.4028 (0.4042)  loss_n_80: 0.4492 (0.4459)  loss_n_100: 0.5042 (0.4938)  triple_100: 0.0000 (0.0125)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0108)  triple_40: 0.0000 (0.0161)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 860/1724]  eta: 0:56:24  lr: 0.000200  loss: 1.6731 (1.7842)  loss_n_40: 0.3717 (0.3869)  loss_n_60: 0.3930 (0.4040)  loss_n_80: 0.4275 (0.4455)  loss_n_100: 0.4735 (0.4935)  triple_100: 0.0000 (0.0123)  triple_80: 0.0000 (0.0154)  triple_60: 0.0000 (0.0107)  triple_40: 0.0000 (0.0159)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 870/1724]  eta: 0:55:45  lr: 0.000200  loss: 1.6479 (1.7828)  loss_n_40: 0.3648 (0.3870)  loss_n_60: 0.3840 (0.4039)  loss_n_80: 0.4074 (0.4452)  loss_n_100: 0.4565 (0.4931)  triple_100: 0.0000 (0.0122)  triple_80: 0.0000 (0.0152)  triple_60: 0.0000 (0.0105)  triple_40: 0.0000 (0.0157)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 880/1724]  eta: 0:55:06  lr: 0.000200  loss: 1.4849 (1.7786)  loss_n_40: 0.3404 (0.3863)  loss_n_60: 0.3512 (0.4032)  loss_n_80: 0.3652 (0.4442)  loss_n_100: 0.4131 (0.4920)  triple_100: 0.0000 (0.0120)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0155)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 890/1724]  eta: 0:54:27  lr: 0.000200  loss: 1.4329 (1.7761)  loss_n_40: 0.3260 (0.3862)  loss_n_60: 0.3349 (0.4027)  loss_n_80: 0.3615 (0.4435)  loss_n_100: 0.3880 (0.4912)  triple_100: 0.0000 (0.0119)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0103)  triple_40: 0.0000 (0.0153)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 900/1724]  eta: 0:53:48  lr: 0.000200  loss: 1.4471 (1.7725)  loss_n_40: 0.3200 (0.3857)  loss_n_60: 0.3374 (0.4021)  loss_n_80: 0.3740 (0.4426)  loss_n_100: 0.3880 (0.4902)  triple_100: 0.0000 (0.0118)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0102)  triple_40: 0.0000 (0.0152)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 910/1724]  eta: 0:53:08  lr: 0.000200  loss: 1.4598 (1.7695)  loss_n_40: 0.3226 (0.3854)  loss_n_60: 0.3473 (0.4017)  loss_n_80: 0.3639 (0.4418)  loss_n_100: 0.4066 (0.4893)  triple_100: 0.0000 (0.0116)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0101)  triple_40: 0.0000 (0.0150)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 920/1724]  eta: 0:52:29  lr: 0.000200  loss: 1.4531 (1.7669)  loss_n_40: 0.3485 (0.3851)  loss_n_60: 0.3473 (0.4013)  loss_n_80: 0.3639 (0.4412)  loss_n_100: 0.3958 (0.4887)  triple_100: 0.0000 (0.0115)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0100)  triple_40: 0.0000 (0.0148)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 930/1724]  eta: 0:51:50  lr: 0.000200  loss: 1.4697 (1.7640)  loss_n_40: 0.3653 (0.3847)  loss_n_60: 0.3560 (0.4010)  loss_n_80: 0.3573 (0.4403)  loss_n_100: 0.3958 (0.4878)  triple_100: 0.0000 (0.0114)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0099)  triple_40: 0.0000 (0.0147)  time: 3.9213  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 940/1724]  eta: 0:51:11  lr: 0.000200  loss: 1.4697 (1.7603)  loss_n_40: 0.3387 (0.3842)  loss_n_60: 0.3487 (0.4004)  loss_n_80: 0.3573 (0.4394)  loss_n_100: 0.3868 (0.4867)  triple_100: 0.0000 (0.0113)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0145)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 950/1724]  eta: 0:50:32  lr: 0.000200  loss: 1.4329 (1.7586)  loss_n_40: 0.3137 (0.3841)  loss_n_60: 0.3362 (0.4002)  loss_n_80: 0.3621 (0.4390)  loss_n_100: 0.3898 (0.4863)  triple_100: 0.0000 (0.0111)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0097)  triple_40: 0.0000 (0.0144)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 960/1724]  eta: 0:49:53  lr: 0.000200  loss: 1.3766 (1.7546)  loss_n_40: 0.3022 (0.3831)  loss_n_60: 0.3251 (0.3994)  loss_n_80: 0.3589 (0.4382)  loss_n_100: 0.3989 (0.4854)  triple_100: 0.0000 (0.0110)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0142)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 970/1724]  eta: 0:49:14  lr: 0.000200  loss: 1.2967 (1.7504)  loss_n_40: 0.2841 (0.3824)  loss_n_60: 0.3117 (0.3985)  loss_n_80: 0.3274 (0.4372)  loss_n_100: 0.3728 (0.4842)  triple_100: 0.0000 (0.0109)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0095)  triple_40: 0.0000 (0.0141)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [ 980/1724]  eta: 0:48:34  lr: 0.000200  loss: 1.2947 (1.7459)  loss_n_40: 0.2978 (0.3815)  loss_n_60: 0.3066 (0.3977)  loss_n_80: 0.3274 (0.4361)  loss_n_100: 0.3608 (0.4830)  triple_100: 0.0000 (0.0108)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0139)  time: 3.9195  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [ 990/1724]  eta: 0:47:55  lr: 0.000200  loss: 1.3452 (1.7422)  loss_n_40: 0.3060 (0.3810)  loss_n_60: 0.3210 (0.3970)  loss_n_80: 0.3305 (0.4352)  loss_n_100: 0.3608 (0.4819)  triple_100: 0.0000 (0.0107)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0138)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1000/1724]  eta: 0:47:16  lr: 0.000200  loss: 1.3924 (1.7388)  loss_n_40: 0.3348 (0.3805)  loss_n_60: 0.3335 (0.3963)  loss_n_80: 0.3585 (0.4344)  loss_n_100: 0.3747 (0.4809)  triple_100: 0.0000 (0.0106)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0092)  triple_40: 0.0000 (0.0137)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1010/1724]  eta: 0:46:37  lr: 0.000200  loss: 1.3520 (1.7351)  loss_n_40: 0.3255 (0.3798)  loss_n_60: 0.3384 (0.3957)  loss_n_80: 0.3408 (0.4335)  loss_n_100: 0.3716 (0.4799)  triple_100: 0.0000 (0.0105)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0135)  time: 3.9189  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [1020/1724]  eta: 0:45:58  lr: 0.000200  loss: 1.3515 (1.7313)  loss_n_40: 0.2966 (0.3791)  loss_n_60: 0.3262 (0.3950)  loss_n_80: 0.3318 (0.4326)  loss_n_100: 0.3569 (0.4789)  triple_100: 0.0000 (0.0104)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0134)  time: 3.9199  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1030/1724]  eta: 0:45:19  lr: 0.000200  loss: 1.5257 (1.7383)  loss_n_40: 0.3403 (0.3792)  loss_n_60: 0.3732 (0.3956)  loss_n_80: 0.3780 (0.4334)  loss_n_100: 0.4126 (0.4801)  triple_100: 0.0000 (0.0124)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0135)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1040/1724]  eta: 0:44:39  lr: 0.000200  loss: 2.7125 (1.7507)  loss_n_40: 0.4521 (0.3811)  loss_n_60: 0.5683 (0.3982)  loss_n_80: 0.7095 (0.4370)  loss_n_100: 0.8238 (0.4844)  triple_100: 0.0000 (0.0128)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0097)  triple_40: 0.0000 (0.0133)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1050/1724]  eta: 0:44:00  lr: 0.000200  loss: 2.7125 (1.7578)  loss_n_40: 0.5013 (0.3821)  loss_n_60: 0.5780 (0.3993)  loss_n_80: 0.7362 (0.4387)  loss_n_100: 0.8238 (0.4866)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0132)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1060/1724]  eta: 0:43:21  lr: 0.000200  loss: 2.1822 (1.7635)  loss_n_40: 0.4746 (0.3834)  loss_n_60: 0.4877 (0.4005)  loss_n_80: 0.5734 (0.4404)  loss_n_100: 0.7026 (0.4886)  triple_100: 0.0000 (0.0134)  triple_80: 0.0000 (0.0146)  triple_60: 0.0000 (0.0095)  triple_40: 0.0000 (0.0131)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1070/1724]  eta: 0:42:42  lr: 0.000200  loss: 2.0655 (1.7658)  loss_n_40: 0.4570 (0.3838)  loss_n_60: 0.4599 (0.4010)  loss_n_80: 0.5266 (0.4412)  loss_n_100: 0.6077 (0.4895)  triple_100: 0.0000 (0.0133)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0130)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1080/1724]  eta: 0:42:03  lr: 0.000200  loss: 1.7607 (1.7659)  loss_n_40: 0.3884 (0.3839)  loss_n_60: 0.4112 (0.4011)  loss_n_80: 0.4762 (0.4414)  loss_n_100: 0.5213 (0.4898)  triple_100: 0.0000 (0.0131)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0128)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1090/1724]  eta: 0:41:23  lr: 0.000200  loss: 1.6337 (1.7644)  loss_n_40: 0.3619 (0.3837)  loss_n_60: 0.3661 (0.4009)  loss_n_80: 0.4123 (0.4412)  loss_n_100: 0.4760 (0.4895)  triple_100: 0.0000 (0.0130)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0127)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1100/1724]  eta: 0:40:44  lr: 0.000200  loss: 1.5881 (1.7635)  loss_n_40: 0.3622 (0.3837)  loss_n_60: 0.3593 (0.4008)  loss_n_80: 0.3984 (0.4410)  loss_n_100: 0.4621 (0.4893)  triple_100: 0.0000 (0.0129)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0092)  triple_40: 0.0000 (0.0126)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1110/1724]  eta: 0:40:05  lr: 0.000200  loss: 1.5794 (1.7618)  loss_n_40: 0.3579 (0.3835)  loss_n_60: 0.3589 (0.4005)  loss_n_80: 0.3963 (0.4406)  loss_n_100: 0.4419 (0.4889)  triple_100: 0.0000 (0.0128)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0125)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1120/1724]  eta: 0:39:26  lr: 0.000200  loss: 1.5229 (1.7604)  loss_n_40: 0.3403 (0.3833)  loss_n_60: 0.3594 (0.4003)  loss_n_80: 0.3986 (0.4403)  loss_n_100: 0.4338 (0.4886)  triple_100: 0.0000 (0.0127)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0124)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1130/1724]  eta: 0:38:47  lr: 0.000200  loss: 1.5585 (1.7586)  loss_n_40: 0.3403 (0.3831)  loss_n_60: 0.3608 (0.4001)  loss_n_80: 0.4060 (0.4399)  loss_n_100: 0.4489 (0.4881)  triple_100: 0.0000 (0.0126)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0123)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1140/1724]  eta: 0:38:08  lr: 0.000200  loss: 1.5084 (1.7565)  loss_n_40: 0.3300 (0.3828)  loss_n_60: 0.3557 (0.3997)  loss_n_80: 0.3971 (0.4394)  loss_n_100: 0.4237 (0.4876)  triple_100: 0.0000 (0.0125)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0122)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1150/1724]  eta: 0:37:28  lr: 0.000200  loss: 1.4787 (1.7544)  loss_n_40: 0.3324 (0.3826)  loss_n_60: 0.3469 (0.3994)  loss_n_80: 0.3623 (0.4388)  loss_n_100: 0.4013 (0.4869)  triple_100: 0.0000 (0.0123)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0121)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1160/1724]  eta: 0:36:49  lr: 0.000200  loss: 1.4251 (1.7523)  loss_n_40: 0.3324 (0.3825)  loss_n_60: 0.3367 (0.3991)  loss_n_80: 0.3546 (0.4383)  loss_n_100: 0.3916 (0.4862)  triple_100: 0.0000 (0.0122)  triple_80: 0.0000 (0.0134)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0119)  time: 3.9158  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1170/1724]  eta: 0:36:10  lr: 0.000200  loss: 1.3802 (1.7500)  loss_n_40: 0.3009 (0.3821)  loss_n_60: 0.3207 (0.3986)  loss_n_80: 0.3566 (0.4378)  loss_n_100: 0.3938 (0.4856)  triple_100: 0.0000 (0.0121)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0118)  time: 3.9160  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1180/1724]  eta: 0:35:31  lr: 0.000200  loss: 1.5160 (1.7483)  loss_n_40: 0.3530 (0.3820)  loss_n_60: 0.3563 (0.3984)  loss_n_80: 0.3700 (0.4373)  loss_n_100: 0.4317 (0.4852)  triple_100: 0.0000 (0.0120)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0117)  time: 3.9159  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1190/1724]  eta: 0:34:52  lr: 0.000200  loss: 1.8766 (1.7869)  loss_n_40: 0.4612 (0.3860)  loss_n_60: 0.4465 (0.4034)  loss_n_80: 0.4559 (0.4438)  loss_n_100: 0.5129 (0.4926)  triple_100: 0.0000 (0.0216)  triple_80: 0.0000 (0.0168)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0140)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1200/1724]  eta: 0:34:12  lr: 0.000200  loss: 5.1694 (1.8158)  loss_n_40: 1.0146 (0.3919)  loss_n_60: 1.0737 (0.4098)  loss_n_80: 1.3054 (0.4517)  loss_n_100: 1.5288 (0.5018)  triple_100: 0.0000 (0.0214)  triple_80: 0.0000 (0.0166)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0139)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1210/1724]  eta: 0:33:33  lr: 0.000200  loss: 4.7098 (1.8380)  loss_n_40: 0.9888 (0.3961)  loss_n_60: 1.0278 (0.4146)  loss_n_80: 1.2350 (0.4579)  loss_n_100: 1.4574 (0.5094)  triple_100: 0.0000 (0.0212)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0138)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1220/1724]  eta: 0:32:54  lr: 0.000200  loss: 4.1553 (1.8542)  loss_n_40: 0.8582 (0.3996)  loss_n_60: 0.9138 (0.4184)  loss_n_80: 1.0571 (0.4622)  loss_n_100: 1.2378 (0.5144)  triple_100: 0.0000 (0.0210)  triple_80: 0.0000 (0.0163)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0137)  time: 3.9156  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1230/1724]  eta: 0:32:15  lr: 0.000200  loss: 3.3376 (1.8633)  loss_n_40: 0.7214 (0.4020)  loss_n_60: 0.7448 (0.4205)  loss_n_80: 0.8663 (0.4645)  loss_n_100: 0.9736 (0.5172)  triple_100: 0.0000 (0.0209)  triple_80: 0.0000 (0.0162)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0136)  time: 3.9162  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1240/1724]  eta: 0:31:36  lr: 0.000200  loss: 2.7842 (1.8711)  loss_n_40: 0.6337 (0.4038)  loss_n_60: 0.6245 (0.4221)  loss_n_80: 0.6993 (0.4666)  loss_n_100: 0.8389 (0.5200)  triple_100: 0.0000 (0.0207)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0135)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1250/1724]  eta: 0:30:56  lr: 0.000200  loss: 2.6550 (1.8765)  loss_n_40: 0.6026 (0.4056)  loss_n_60: 0.5788 (0.4234)  loss_n_80: 0.6816 (0.4680)  loss_n_100: 0.7812 (0.5215)  triple_100: 0.0000 (0.0205)  triple_80: 0.0000 (0.0159)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0134)  time: 3.9180  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [1260/1724]  eta: 0:30:17  lr: 0.000200  loss: 2.4257 (1.8799)  loss_n_40: 0.5600 (0.4067)  loss_n_60: 0.5500 (0.4241)  loss_n_80: 0.5856 (0.4689)  loss_n_100: 0.6656 (0.5225)  triple_100: 0.0000 (0.0204)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0133)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1270/1724]  eta: 0:29:38  lr: 0.000200  loss: 2.1961 (1.8818)  loss_n_40: 0.5339 (0.4078)  loss_n_60: 0.4717 (0.4243)  loss_n_80: 0.5691 (0.4695)  loss_n_100: 0.6160 (0.5231)  triple_100: 0.0000 (0.0202)  triple_80: 0.0000 (0.0157)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0132)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1280/1724]  eta: 0:28:59  lr: 0.000200  loss: 2.0963 (1.8833)  loss_n_40: 0.5219 (0.4087)  loss_n_60: 0.4493 (0.4245)  loss_n_80: 0.5198 (0.4699)  loss_n_100: 0.5705 (0.5235)  triple_100: 0.0000 (0.0200)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0131)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1290/1724]  eta: 0:28:20  lr: 0.000200  loss: 1.9412 (1.8836)  loss_n_40: 0.4734 (0.4092)  loss_n_60: 0.4224 (0.4245)  loss_n_80: 0.4850 (0.4699)  loss_n_100: 0.5451 (0.5236)  triple_100: 0.0000 (0.0199)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0130)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1300/1724]  eta: 0:27:41  lr: 0.000200  loss: 1.8617 (1.8837)  loss_n_40: 0.4457 (0.4097)  loss_n_60: 0.4093 (0.4245)  loss_n_80: 0.4635 (0.4700)  loss_n_100: 0.5238 (0.5236)  triple_100: 0.0000 (0.0197)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0129)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1310/1724]  eta: 0:27:01  lr: 0.000200  loss: 1.8127 (1.8830)  loss_n_40: 0.4405 (0.4100)  loss_n_60: 0.3959 (0.4243)  loss_n_80: 0.4617 (0.4698)  loss_n_100: 0.5168 (0.5234)  triple_100: 0.0000 (0.0196)  triple_80: 0.0000 (0.0152)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0128)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1320/1724]  eta: 0:26:22  lr: 0.000200  loss: 1.7319 (1.8824)  loss_n_40: 0.4260 (0.4104)  loss_n_60: 0.3856 (0.4241)  loss_n_80: 0.4349 (0.4696)  loss_n_100: 0.4909 (0.5231)  triple_100: 0.0000 (0.0194)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0127)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1330/1724]  eta: 0:25:43  lr: 0.000200  loss: 1.6918 (1.8807)  loss_n_40: 0.4269 (0.4106)  loss_n_60: 0.3718 (0.4237)  loss_n_80: 0.4128 (0.4692)  loss_n_100: 0.4611 (0.5226)  triple_100: 0.0000 (0.0193)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0126)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1340/1724]  eta: 0:25:04  lr: 0.000200  loss: 1.6394 (1.8790)  loss_n_40: 0.4001 (0.4108)  loss_n_60: 0.3608 (0.4233)  loss_n_80: 0.4014 (0.4687)  loss_n_100: 0.4414 (0.5221)  triple_100: 0.0000 (0.0192)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0125)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1350/1724]  eta: 0:24:25  lr: 0.000200  loss: 1.6417 (1.8779)  loss_n_40: 0.4293 (0.4109)  loss_n_60: 0.3608 (0.4230)  loss_n_80: 0.4144 (0.4684)  loss_n_100: 0.4526 (0.5216)  triple_100: 0.0000 (0.0190)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0124)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1360/1724]  eta: 0:23:46  lr: 0.000200  loss: 1.8499 (1.8782)  loss_n_40: 0.4401 (0.4113)  loss_n_60: 0.4137 (0.4230)  loss_n_80: 0.4581 (0.4685)  loss_n_100: 0.5018 (0.5218)  triple_100: 0.0000 (0.0189)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0123)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1370/1724]  eta: 0:23:06  lr: 0.000200  loss: 1.9213 (1.8792)  loss_n_40: 0.4415 (0.4116)  loss_n_60: 0.4164 (0.4230)  loss_n_80: 0.4748 (0.4686)  loss_n_100: 0.5448 (0.5219)  triple_100: 0.0000 (0.0191)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0122)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1380/1724]  eta: 0:22:27  lr: 0.000200  loss: 1.8027 (1.8787)  loss_n_40: 0.4380 (0.4116)  loss_n_60: 0.4124 (0.4229)  loss_n_80: 0.4567 (0.4685)  loss_n_100: 0.5437 (0.5221)  triple_100: 0.0000 (0.0190)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0121)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1390/1724]  eta: 0:21:48  lr: 0.000200  loss: 1.6825 (1.8772)  loss_n_40: 0.3868 (0.4116)  loss_n_60: 0.3749 (0.4226)  loss_n_80: 0.4106 (0.4680)  loss_n_100: 0.4907 (0.5217)  triple_100: 0.0000 (0.0188)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0120)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1400/1724]  eta: 0:21:09  lr: 0.000200  loss: 1.6239 (1.8758)  loss_n_40: 0.4077 (0.4119)  loss_n_60: 0.3698 (0.4223)  loss_n_80: 0.3964 (0.4675)  loss_n_100: 0.4433 (0.5212)  triple_100: 0.0000 (0.0187)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0119)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1410/1724]  eta: 0:20:30  lr: 0.000200  loss: 1.5858 (1.8736)  loss_n_40: 0.4163 (0.4118)  loss_n_60: 0.3568 (0.4219)  loss_n_80: 0.3760 (0.4669)  loss_n_100: 0.4261 (0.5205)  triple_100: 0.0000 (0.0186)  triple_80: 0.0000 (0.0146)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0119)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1420/1724]  eta: 0:19:50  lr: 0.000200  loss: 1.5937 (1.8719)  loss_n_40: 0.4018 (0.4118)  loss_n_60: 0.3615 (0.4216)  loss_n_80: 0.3870 (0.4664)  loss_n_100: 0.4179 (0.5199)  triple_100: 0.0000 (0.0184)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0118)  time: 3.9185  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1430/1724]  eta: 0:19:11  lr: 0.000200  loss: 1.5855 (1.8698)  loss_n_40: 0.4052 (0.4116)  loss_n_60: 0.3578 (0.4211)  loss_n_80: 0.3923 (0.4659)  loss_n_100: 0.4166 (0.5193)  triple_100: 0.0000 (0.0183)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0117)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1440/1724]  eta: 0:18:32  lr: 0.000200  loss: 1.4765 (1.8673)  loss_n_40: 0.3659 (0.4114)  loss_n_60: 0.3372 (0.4206)  loss_n_80: 0.3664 (0.4653)  loss_n_100: 0.4138 (0.5186)  triple_100: 0.0000 (0.0182)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0116)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1450/1724]  eta: 0:17:53  lr: 0.000200  loss: 1.4957 (1.8652)  loss_n_40: 0.3593 (0.4112)  loss_n_60: 0.3389 (0.4201)  loss_n_80: 0.3697 (0.4647)  loss_n_100: 0.4096 (0.5179)  triple_100: 0.0000 (0.0181)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0115)  time: 3.9199  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1460/1724]  eta: 0:17:14  lr: 0.000200  loss: 1.5595 (1.8633)  loss_n_40: 0.3670 (0.4112)  loss_n_60: 0.3443 (0.4197)  loss_n_80: 0.3849 (0.4642)  loss_n_100: 0.4052 (0.5172)  triple_100: 0.0000 (0.0180)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0115)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1470/1724]  eta: 0:16:35  lr: 0.000200  loss: 1.5670 (1.8619)  loss_n_40: 0.3948 (0.4113)  loss_n_60: 0.3621 (0.4194)  loss_n_80: 0.3899 (0.4638)  loss_n_100: 0.4208 (0.5167)  triple_100: 0.0000 (0.0178)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0114)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1480/1724]  eta: 0:15:55  lr: 0.000200  loss: 1.7422 (1.8670)  loss_n_40: 0.4383 (0.4117)  loss_n_60: 0.3908 (0.4199)  loss_n_80: 0.4204 (0.4644)  loss_n_100: 0.4593 (0.5174)  triple_100: 0.0000 (0.0188)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0114)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1490/1724]  eta: 0:15:16  lr: 0.000200  loss: 2.4169 (1.8709)  loss_n_40: 0.5094 (0.4125)  loss_n_60: 0.5291 (0.4208)  loss_n_80: 0.6256 (0.4656)  loss_n_100: 0.6580 (0.5188)  triple_100: 0.0000 (0.0187)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0114)  time: 3.9178  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13]  [1500/1724]  eta: 0:14:37  lr: 0.000200  loss: 2.1600 (1.8721)  loss_n_40: 0.4897 (0.4129)  loss_n_60: 0.4801 (0.4211)  loss_n_80: 0.5480 (0.4660)  loss_n_100: 0.6085 (0.5193)  triple_100: 0.0000 (0.0186)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0113)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1510/1724]  eta: 0:13:58  lr: 0.000200  loss: 1.9644 (1.8723)  loss_n_40: 0.4578 (0.4133)  loss_n_60: 0.4382 (0.4211)  loss_n_80: 0.4915 (0.4660)  loss_n_100: 0.5440 (0.5193)  triple_100: 0.0000 (0.0185)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0112)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1520/1724]  eta: 0:13:19  lr: 0.000200  loss: 1.7829 (1.8708)  loss_n_40: 0.4273 (0.4132)  loss_n_60: 0.4052 (0.4209)  loss_n_80: 0.4334 (0.4656)  loss_n_100: 0.4771 (0.5189)  triple_100: 0.0000 (0.0183)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0111)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1530/1724]  eta: 0:12:40  lr: 0.000200  loss: 1.6389 (1.8697)  loss_n_40: 0.3928 (0.4133)  loss_n_60: 0.3802 (0.4207)  loss_n_80: 0.4085 (0.4653)  loss_n_100: 0.4591 (0.5185)  triple_100: 0.0000 (0.0182)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0111)  time: 3.9199  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1540/1724]  eta: 0:12:00  lr: 0.000200  loss: 1.6179 (1.8688)  loss_n_40: 0.4246 (0.4135)  loss_n_60: 0.3760 (0.4206)  loss_n_80: 0.3996 (0.4651)  loss_n_100: 0.4424 (0.5182)  triple_100: 0.0000 (0.0181)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0110)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1550/1724]  eta: 0:11:21  lr: 0.000200  loss: 1.5344 (1.8666)  loss_n_40: 0.3820 (0.4133)  loss_n_60: 0.3538 (0.4201)  loss_n_80: 0.3795 (0.4645)  loss_n_100: 0.4239 (0.5175)  triple_100: 0.0000 (0.0180)  triple_80: 0.0000 (0.0146)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0109)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1560/1724]  eta: 0:10:42  lr: 0.000200  loss: 1.4967 (1.8653)  loss_n_40: 0.3695 (0.4135)  loss_n_60: 0.3386 (0.4199)  loss_n_80: 0.3503 (0.4641)  loss_n_100: 0.3929 (0.5169)  triple_100: 0.0000 (0.0179)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0109)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1570/1724]  eta: 0:10:03  lr: 0.000200  loss: 1.4819 (1.8631)  loss_n_40: 0.3695 (0.4134)  loss_n_60: 0.3362 (0.4194)  loss_n_80: 0.3587 (0.4636)  loss_n_100: 0.4033 (0.5163)  triple_100: 0.0000 (0.0177)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0108)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1580/1724]  eta: 0:09:24  lr: 0.000200  loss: 1.4416 (1.8608)  loss_n_40: 0.3682 (0.4132)  loss_n_60: 0.3253 (0.4189)  loss_n_80: 0.3587 (0.4630)  loss_n_100: 0.4002 (0.5155)  triple_100: 0.0000 (0.0176)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0107)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1590/1724]  eta: 0:08:44  lr: 0.000200  loss: 1.4272 (1.8580)  loss_n_40: 0.3427 (0.4127)  loss_n_60: 0.3080 (0.4183)  loss_n_80: 0.3515 (0.4623)  loss_n_100: 0.3891 (0.5147)  triple_100: 0.0000 (0.0175)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0108)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1600/1724]  eta: 0:08:05  lr: 0.000200  loss: 1.5356 (1.8567)  loss_n_40: 0.3556 (0.4127)  loss_n_60: 0.3390 (0.4179)  loss_n_80: 0.3730 (0.4619)  loss_n_100: 0.4120 (0.5143)  triple_100: 0.0000 (0.0175)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0107)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1610/1724]  eta: 0:07:26  lr: 0.000200  loss: 1.5382 (1.8551)  loss_n_40: 0.3850 (0.4128)  loss_n_60: 0.3507 (0.4176)  loss_n_80: 0.3852 (0.4614)  loss_n_100: 0.4218 (0.5137)  triple_100: 0.0000 (0.0174)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0106)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1620/1724]  eta: 0:06:47  lr: 0.000200  loss: 1.5699 (1.8547)  loss_n_40: 0.4044 (0.4128)  loss_n_60: 0.3559 (0.4173)  loss_n_80: 0.3852 (0.4611)  loss_n_100: 0.4282 (0.5133)  triple_100: 0.0000 (0.0175)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0106)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1630/1724]  eta: 0:06:08  lr: 0.000200  loss: 1.6730 (1.8535)  loss_n_40: 0.4044 (0.4128)  loss_n_60: 0.3659 (0.4169)  loss_n_80: 0.4166 (0.4608)  loss_n_100: 0.4678 (0.5130)  triple_100: 0.0000 (0.0174)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0105)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1640/1724]  eta: 0:05:29  lr: 0.000200  loss: 1.6018 (1.8515)  loss_n_40: 0.3961 (0.4127)  loss_n_60: 0.3547 (0.4165)  loss_n_80: 0.3884 (0.4603)  loss_n_100: 0.4504 (0.5125)  triple_100: 0.0000 (0.0173)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0104)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1650/1724]  eta: 0:04:49  lr: 0.000200  loss: 1.5425 (1.8497)  loss_n_40: 0.3928 (0.4127)  loss_n_60: 0.3398 (0.4161)  loss_n_80: 0.3755 (0.4598)  loss_n_100: 0.4088 (0.5118)  triple_100: 0.0000 (0.0172)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0104)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1660/1724]  eta: 0:04:10  lr: 0.000200  loss: 1.4407 (1.8475)  loss_n_40: 0.3897 (0.4127)  loss_n_60: 0.3181 (0.4155)  loss_n_80: 0.3626 (0.4592)  loss_n_100: 0.3894 (0.5111)  triple_100: 0.0000 (0.0170)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0103)  time: 3.9190  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1670/1724]  eta: 0:03:31  lr: 0.000200  loss: 1.4299 (1.8454)  loss_n_40: 0.3687 (0.4126)  loss_n_60: 0.3181 (0.4150)  loss_n_80: 0.3544 (0.4586)  loss_n_100: 0.3816 (0.5104)  triple_100: 0.0000 (0.0169)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0103)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1680/1724]  eta: 0:02:52  lr: 0.000200  loss: 1.3818 (1.8426)  loss_n_40: 0.3477 (0.4122)  loss_n_60: 0.3095 (0.4144)  loss_n_80: 0.3417 (0.4579)  loss_n_100: 0.3807 (0.5096)  triple_100: 0.0000 (0.0168)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0102)  time: 3.9196  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1690/1724]  eta: 0:02:13  lr: 0.000200  loss: 1.4401 (1.8440)  loss_n_40: 0.3482 (0.4121)  loss_n_60: 0.3312 (0.4147)  loss_n_80: 0.3573 (0.4582)  loss_n_100: 0.4024 (0.5100)  triple_100: 0.0000 (0.0170)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0102)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1700/1724]  eta: 0:01:34  lr: 0.000200  loss: 2.2983 (1.8472)  loss_n_40: 0.4643 (0.4130)  loss_n_60: 0.5220 (0.4154)  loss_n_80: 0.5790 (0.4589)  loss_n_100: 0.6248 (0.5108)  triple_100: 0.0000 (0.0170)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0102)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:13]  [1710/1724]  eta: 0:00:54  lr: 0.000200  loss: 2.1929 (1.8487)  loss_n_40: 0.5221 (0.4135)  loss_n_60: 0.5178 (0.4158)  loss_n_80: 0.5481 (0.4594)  loss_n_100: 0.6144 (0.5113)  triple_100: 0.0000 (0.0169)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0102)  time: 3.9180  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1720/1724]  eta: 0:00:15  lr: 0.000200  loss: 1.9787 (1.8485)  loss_n_40: 0.4781 (0.4137)  loss_n_60: 0.4378 (0.4158)  loss_n_80: 0.4873 (0.4593)  loss_n_100: 0.5296 (0.5111)  triple_100: 0.0000 (0.0168)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0101)  time: 3.9174  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:13]  [1723/1724]  eta: 0:00:03  lr: 0.000200  loss: 1.8762 (1.8482)  loss_n_40: 0.4507 (0.4137)  loss_n_60: 0.4343 (0.4158)  loss_n_80: 0.4718 (0.4593)  loss_n_100: 0.5008 (0.5110)  triple_100: 0.0000 (0.0167)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0101)  time: 3.9172  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:13] Total time: 1:52:34 (3.9180 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.8762 (1.8482)  loss_n_40: 0.4507 (0.4137)  loss_n_60: 0.4343 (0.4158)  loss_n_80: 0.4718 (0.4593)  loss_n_100: 0.5008 (0.5110)  triple_100: 0.0000 (0.0167)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0101)\n",
      "Valid: [epoch:13]  [  0/845]  eta: 0:10:14  loss: 1.4327 (1.4327)  loss_n_40: 0.3299 (0.3299)  loss_n_60: 0.3540 (0.3540)  loss_n_80: 0.3654 (0.3654)  loss_n_100: 0.3834 (0.3834)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7275  data: 0.3950  max mem: 46473\n",
      "Valid: [epoch:13]  [ 10/845]  eta: 0:05:08  loss: 1.6109 (1.7898)  loss_n_40: 0.3648 (0.4754)  loss_n_60: 0.3713 (0.4145)  loss_n_80: 0.4083 (0.4416)  loss_n_100: 0.4395 (0.4582)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3695  data: 0.0360  max mem: 46473\n",
      "Valid: [epoch:13]  [ 20/845]  eta: 0:04:50  loss: 1.5754 (1.6755)  loss_n_40: 0.3638 (0.4291)  loss_n_60: 0.3683 (0.3899)  loss_n_80: 0.3829 (0.4158)  loss_n_100: 0.4287 (0.4407)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [ 30/845]  eta: 0:04:42  loss: 1.5595 (1.6584)  loss_n_40: 0.3617 (0.4161)  loss_n_60: 0.3693 (0.3901)  loss_n_80: 0.3828 (0.4115)  loss_n_100: 0.4045 (0.4406)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [ 40/845]  eta: 0:04:36  loss: 1.5546 (1.6267)  loss_n_40: 0.3700 (0.4082)  loss_n_60: 0.3738 (0.3843)  loss_n_80: 0.3869 (0.4015)  loss_n_100: 0.4060 (0.4326)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [ 50/845]  eta: 0:04:31  loss: 1.5960 (1.6557)  loss_n_40: 0.3737 (0.4100)  loss_n_60: 0.3833 (0.3948)  loss_n_80: 0.3897 (0.4101)  loss_n_100: 0.4248 (0.4407)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [ 60/845]  eta: 0:04:27  loss: 1.5963 (1.6397)  loss_n_40: 0.3699 (0.4082)  loss_n_60: 0.3650 (0.3912)  loss_n_80: 0.4029 (0.4062)  loss_n_100: 0.4184 (0.4341)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [ 70/845]  eta: 0:04:23  loss: 1.5267 (1.6451)  loss_n_40: 0.3699 (0.4079)  loss_n_60: 0.3620 (0.3925)  loss_n_80: 0.3786 (0.4070)  loss_n_100: 0.4014 (0.4377)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [ 80/845]  eta: 0:04:19  loss: 1.6023 (1.6435)  loss_n_40: 0.3823 (0.4059)  loss_n_60: 0.3839 (0.3908)  loss_n_80: 0.3958 (0.4069)  loss_n_100: 0.4339 (0.4399)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [ 90/845]  eta: 0:04:15  loss: 1.6492 (1.6479)  loss_n_40: 0.3820 (0.4073)  loss_n_60: 0.3811 (0.3910)  loss_n_80: 0.3909 (0.4071)  loss_n_100: 0.4361 (0.4426)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [100/845]  eta: 0:04:11  loss: 1.6740 (1.6575)  loss_n_40: 0.3829 (0.4070)  loss_n_60: 0.4000 (0.3924)  loss_n_80: 0.4211 (0.4118)  loss_n_100: 0.4576 (0.4463)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [110/845]  eta: 0:04:08  loss: 1.6524 (1.6578)  loss_n_40: 0.3790 (0.4076)  loss_n_60: 0.3945 (0.3918)  loss_n_80: 0.4211 (0.4129)  loss_n_100: 0.4474 (0.4455)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [120/845]  eta: 0:04:04  loss: 1.5703 (1.6592)  loss_n_40: 0.3693 (0.4114)  loss_n_60: 0.3678 (0.3916)  loss_n_80: 0.3952 (0.4126)  loss_n_100: 0.3970 (0.4437)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [130/845]  eta: 0:04:00  loss: 1.4547 (1.6543)  loss_n_40: 0.3461 (0.4097)  loss_n_60: 0.3550 (0.3903)  loss_n_80: 0.3592 (0.4116)  loss_n_100: 0.4052 (0.4428)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [140/845]  eta: 0:03:57  loss: 1.4455 (1.6397)  loss_n_40: 0.3436 (0.4055)  loss_n_60: 0.3433 (0.3870)  loss_n_80: 0.3633 (0.4079)  loss_n_100: 0.4052 (0.4393)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [150/845]  eta: 0:03:53  loss: 1.4468 (1.6349)  loss_n_40: 0.3485 (0.4031)  loss_n_60: 0.3433 (0.3859)  loss_n_80: 0.3647 (0.4068)  loss_n_100: 0.3991 (0.4390)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [160/845]  eta: 0:03:50  loss: 1.6193 (1.6401)  loss_n_40: 0.3717 (0.4053)  loss_n_60: 0.3705 (0.3867)  loss_n_80: 0.4071 (0.4078)  loss_n_100: 0.4307 (0.4403)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [170/845]  eta: 0:03:47  loss: 1.7256 (1.6482)  loss_n_40: 0.3846 (0.4078)  loss_n_60: 0.3943 (0.3878)  loss_n_80: 0.4244 (0.4099)  loss_n_100: 0.4664 (0.4427)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [180/845]  eta: 0:03:43  loss: 1.6383 (1.6458)  loss_n_40: 0.3736 (0.4076)  loss_n_60: 0.3875 (0.3879)  loss_n_80: 0.4023 (0.4090)  loss_n_100: 0.4365 (0.4413)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [190/845]  eta: 0:03:40  loss: 1.5494 (1.6465)  loss_n_40: 0.3736 (0.4103)  loss_n_60: 0.3791 (0.3877)  loss_n_80: 0.3803 (0.4081)  loss_n_100: 0.3989 (0.4403)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [200/845]  eta: 0:03:36  loss: 1.7179 (1.6574)  loss_n_40: 0.4096 (0.4142)  loss_n_60: 0.4069 (0.3895)  loss_n_80: 0.4054 (0.4111)  loss_n_100: 0.4429 (0.4426)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [210/845]  eta: 0:03:33  loss: 1.7267 (1.6692)  loss_n_40: 0.3952 (0.4190)  loss_n_60: 0.4069 (0.3916)  loss_n_80: 0.4172 (0.4138)  loss_n_100: 0.4751 (0.4448)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [220/845]  eta: 0:03:29  loss: 1.5715 (1.6635)  loss_n_40: 0.3677 (0.4167)  loss_n_60: 0.3737 (0.3906)  loss_n_80: 0.3895 (0.4123)  loss_n_100: 0.4478 (0.4439)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [230/845]  eta: 0:03:26  loss: 1.5546 (1.6666)  loss_n_40: 0.3686 (0.4174)  loss_n_60: 0.3665 (0.3912)  loss_n_80: 0.3842 (0.4128)  loss_n_100: 0.4243 (0.4453)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:13]  [240/845]  eta: 0:03:23  loss: 1.5546 (1.6622)  loss_n_40: 0.3686 (0.4166)  loss_n_60: 0.3618 (0.3908)  loss_n_80: 0.3629 (0.4112)  loss_n_100: 0.4070 (0.4437)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [250/845]  eta: 0:03:19  loss: 1.4686 (1.6595)  loss_n_40: 0.3521 (0.4156)  loss_n_60: 0.3587 (0.3903)  loss_n_80: 0.3620 (0.4105)  loss_n_100: 0.3920 (0.4431)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [260/845]  eta: 0:03:16  loss: 1.5421 (1.6588)  loss_n_40: 0.3747 (0.4154)  loss_n_60: 0.3594 (0.3898)  loss_n_80: 0.3740 (0.4105)  loss_n_100: 0.4025 (0.4431)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [270/845]  eta: 0:03:12  loss: 1.6416 (1.6613)  loss_n_40: 0.3782 (0.4152)  loss_n_60: 0.3687 (0.3901)  loss_n_80: 0.3958 (0.4112)  loss_n_100: 0.4276 (0.4448)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [280/845]  eta: 0:03:09  loss: 1.5938 (1.6571)  loss_n_40: 0.3677 (0.4133)  loss_n_60: 0.3756 (0.3896)  loss_n_80: 0.4047 (0.4103)  loss_n_100: 0.4276 (0.4439)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [290/845]  eta: 0:03:06  loss: 1.5938 (1.6644)  loss_n_40: 0.3795 (0.4144)  loss_n_60: 0.3853 (0.3915)  loss_n_80: 0.4142 (0.4122)  loss_n_100: 0.4400 (0.4463)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [300/845]  eta: 0:03:02  loss: 1.7008 (1.6631)  loss_n_40: 0.3879 (0.4140)  loss_n_60: 0.3925 (0.3910)  loss_n_80: 0.4342 (0.4124)  loss_n_100: 0.4542 (0.4456)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [310/845]  eta: 0:02:59  loss: 1.6893 (1.6704)  loss_n_40: 0.3939 (0.4154)  loss_n_60: 0.3979 (0.3926)  loss_n_80: 0.4049 (0.4149)  loss_n_100: 0.4461 (0.4475)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [320/845]  eta: 0:02:56  loss: 1.7247 (1.6715)  loss_n_40: 0.4186 (0.4158)  loss_n_60: 0.4178 (0.3930)  loss_n_80: 0.4191 (0.4150)  loss_n_100: 0.4559 (0.4477)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [330/845]  eta: 0:02:52  loss: 1.6817 (1.6705)  loss_n_40: 0.3854 (0.4160)  loss_n_60: 0.3709 (0.3925)  loss_n_80: 0.3990 (0.4151)  loss_n_100: 0.4132 (0.4469)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [340/845]  eta: 0:02:49  loss: 1.4965 (1.6682)  loss_n_40: 0.3742 (0.4154)  loss_n_60: 0.3594 (0.3919)  loss_n_80: 0.3936 (0.4146)  loss_n_100: 0.4132 (0.4464)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [350/845]  eta: 0:02:45  loss: 1.6446 (1.6712)  loss_n_40: 0.3806 (0.4167)  loss_n_60: 0.3874 (0.3924)  loss_n_80: 0.3947 (0.4151)  loss_n_100: 0.4495 (0.4470)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [360/845]  eta: 0:02:42  loss: 1.6627 (1.6700)  loss_n_40: 0.3863 (0.4161)  loss_n_60: 0.3874 (0.3922)  loss_n_80: 0.4084 (0.4149)  loss_n_100: 0.4718 (0.4468)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [370/845]  eta: 0:02:39  loss: 1.6627 (1.6741)  loss_n_40: 0.3868 (0.4179)  loss_n_60: 0.3855 (0.3933)  loss_n_80: 0.4084 (0.4158)  loss_n_100: 0.4320 (0.4473)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [380/845]  eta: 0:02:35  loss: 1.6659 (1.6766)  loss_n_40: 0.4065 (0.4191)  loss_n_60: 0.3922 (0.3938)  loss_n_80: 0.4382 (0.4161)  loss_n_100: 0.4342 (0.4476)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [390/845]  eta: 0:02:32  loss: 1.6087 (1.6770)  loss_n_40: 0.3803 (0.4197)  loss_n_60: 0.3952 (0.3939)  loss_n_80: 0.4051 (0.4161)  loss_n_100: 0.4342 (0.4473)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [400/845]  eta: 0:02:29  loss: 1.5145 (1.6744)  loss_n_40: 0.3589 (0.4185)  loss_n_60: 0.3712 (0.3934)  loss_n_80: 0.3734 (0.4157)  loss_n_100: 0.4201 (0.4468)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [410/845]  eta: 0:02:25  loss: 1.4797 (1.6675)  loss_n_40: 0.3433 (0.4163)  loss_n_60: 0.3509 (0.3919)  loss_n_80: 0.3628 (0.4140)  loss_n_100: 0.3945 (0.4453)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [420/845]  eta: 0:02:22  loss: 1.4885 (1.6687)  loss_n_40: 0.3495 (0.4171)  loss_n_60: 0.3557 (0.3924)  loss_n_80: 0.3628 (0.4143)  loss_n_100: 0.3834 (0.4450)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [430/845]  eta: 0:02:19  loss: 1.6808 (1.6684)  loss_n_40: 0.3794 (0.4167)  loss_n_60: 0.3926 (0.3924)  loss_n_80: 0.3966 (0.4139)  loss_n_100: 0.4422 (0.4455)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [440/845]  eta: 0:02:15  loss: 1.7272 (1.6694)  loss_n_40: 0.3783 (0.4164)  loss_n_60: 0.3926 (0.3922)  loss_n_80: 0.3939 (0.4144)  loss_n_100: 0.4919 (0.4463)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [450/845]  eta: 0:02:12  loss: 1.7664 (1.6748)  loss_n_40: 0.4047 (0.4182)  loss_n_60: 0.3896 (0.3931)  loss_n_80: 0.4544 (0.4161)  loss_n_100: 0.5010 (0.4475)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [460/845]  eta: 0:02:09  loss: 1.7961 (1.6791)  loss_n_40: 0.4697 (0.4201)  loss_n_60: 0.4187 (0.3940)  loss_n_80: 0.4559 (0.4170)  loss_n_100: 0.4838 (0.4479)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [470/845]  eta: 0:02:05  loss: 1.5406 (1.6799)  loss_n_40: 0.3657 (0.4211)  loss_n_60: 0.3784 (0.3942)  loss_n_80: 0.3843 (0.4171)  loss_n_100: 0.4156 (0.4475)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [480/845]  eta: 0:02:02  loss: 1.6449 (1.6814)  loss_n_40: 0.3789 (0.4209)  loss_n_60: 0.3870 (0.3944)  loss_n_80: 0.4335 (0.4177)  loss_n_100: 0.4421 (0.4484)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:13]  [490/845]  eta: 0:01:58  loss: 1.7298 (1.6803)  loss_n_40: 0.3813 (0.4202)  loss_n_60: 0.3897 (0.3942)  loss_n_80: 0.4363 (0.4175)  loss_n_100: 0.4653 (0.4483)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [500/845]  eta: 0:01:55  loss: 1.5642 (1.6810)  loss_n_40: 0.3890 (0.4208)  loss_n_60: 0.3690 (0.3944)  loss_n_80: 0.3970 (0.4175)  loss_n_100: 0.4271 (0.4484)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [510/845]  eta: 0:01:52  loss: 1.6450 (1.6816)  loss_n_40: 0.3890 (0.4210)  loss_n_60: 0.3741 (0.3944)  loss_n_80: 0.3970 (0.4178)  loss_n_100: 0.4268 (0.4485)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [520/845]  eta: 0:01:48  loss: 1.7182 (1.6830)  loss_n_40: 0.3928 (0.4216)  loss_n_60: 0.3882 (0.3943)  loss_n_80: 0.4229 (0.4180)  loss_n_100: 0.4708 (0.4491)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [530/845]  eta: 0:01:45  loss: 1.7372 (1.6864)  loss_n_40: 0.4124 (0.4225)  loss_n_60: 0.3849 (0.3948)  loss_n_80: 0.4434 (0.4193)  loss_n_100: 0.4951 (0.4498)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [540/845]  eta: 0:01:42  loss: 1.8129 (1.6882)  loss_n_40: 0.4086 (0.4236)  loss_n_60: 0.3901 (0.3952)  loss_n_80: 0.4389 (0.4195)  loss_n_100: 0.4859 (0.4500)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [550/845]  eta: 0:01:38  loss: 1.6384 (1.6858)  loss_n_40: 0.3648 (0.4224)  loss_n_60: 0.3861 (0.3947)  loss_n_80: 0.4154 (0.4190)  loss_n_100: 0.4468 (0.4497)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [560/845]  eta: 0:01:35  loss: 1.6230 (1.6859)  loss_n_40: 0.3703 (0.4226)  loss_n_60: 0.3760 (0.3947)  loss_n_80: 0.4016 (0.4191)  loss_n_100: 0.4453 (0.4495)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [570/845]  eta: 0:01:32  loss: 1.6193 (1.6853)  loss_n_40: 0.3769 (0.4224)  loss_n_60: 0.3739 (0.3946)  loss_n_80: 0.3859 (0.4188)  loss_n_100: 0.4453 (0.4494)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [580/845]  eta: 0:01:28  loss: 1.4718 (1.6831)  loss_n_40: 0.3508 (0.4214)  loss_n_60: 0.3563 (0.3941)  loss_n_80: 0.3713 (0.4183)  loss_n_100: 0.4070 (0.4492)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [590/845]  eta: 0:01:25  loss: 1.4993 (1.6822)  loss_n_40: 0.3694 (0.4211)  loss_n_60: 0.3608 (0.3939)  loss_n_80: 0.3724 (0.4180)  loss_n_100: 0.4113 (0.4492)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [600/845]  eta: 0:01:22  loss: 1.5853 (1.6835)  loss_n_40: 0.3724 (0.4216)  loss_n_60: 0.3822 (0.3941)  loss_n_80: 0.3961 (0.4183)  loss_n_100: 0.4113 (0.4495)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [610/845]  eta: 0:01:18  loss: 1.7155 (1.6844)  loss_n_40: 0.3966 (0.4221)  loss_n_60: 0.4049 (0.3944)  loss_n_80: 0.4383 (0.4184)  loss_n_100: 0.4422 (0.4496)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [620/845]  eta: 0:01:15  loss: 1.6347 (1.6837)  loss_n_40: 0.3856 (0.4216)  loss_n_60: 0.3969 (0.3942)  loss_n_80: 0.3988 (0.4183)  loss_n_100: 0.4381 (0.4497)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [630/845]  eta: 0:01:12  loss: 1.6280 (1.6829)  loss_n_40: 0.3845 (0.4217)  loss_n_60: 0.3806 (0.3941)  loss_n_80: 0.4004 (0.4177)  loss_n_100: 0.4381 (0.4495)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [640/845]  eta: 0:01:08  loss: 1.6364 (1.6830)  loss_n_40: 0.3845 (0.4216)  loss_n_60: 0.3821 (0.3941)  loss_n_80: 0.4074 (0.4179)  loss_n_100: 0.4609 (0.4495)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [650/845]  eta: 0:01:05  loss: 1.5846 (1.6814)  loss_n_40: 0.3787 (0.4214)  loss_n_60: 0.3602 (0.3935)  loss_n_80: 0.3882 (0.4175)  loss_n_100: 0.4233 (0.4490)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [660/845]  eta: 0:01:01  loss: 1.5846 (1.6822)  loss_n_40: 0.3787 (0.4213)  loss_n_60: 0.3701 (0.3935)  loss_n_80: 0.3882 (0.4181)  loss_n_100: 0.4233 (0.4493)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [670/845]  eta: 0:00:58  loss: 1.6822 (1.6826)  loss_n_40: 0.3866 (0.4216)  loss_n_60: 0.3779 (0.3935)  loss_n_80: 0.4080 (0.4180)  loss_n_100: 0.4420 (0.4495)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:13]  [680/845]  eta: 0:00:55  loss: 1.7406 (1.6840)  loss_n_40: 0.4475 (0.4220)  loss_n_60: 0.3937 (0.3937)  loss_n_80: 0.4257 (0.4185)  loss_n_100: 0.4608 (0.4497)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:13]  [690/845]  eta: 0:00:51  loss: 1.7171 (1.6835)  loss_n_40: 0.4185 (0.4216)  loss_n_60: 0.3828 (0.3936)  loss_n_80: 0.4257 (0.4184)  loss_n_100: 0.4608 (0.4498)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [700/845]  eta: 0:00:48  loss: 1.6464 (1.6839)  loss_n_40: 0.3813 (0.4221)  loss_n_60: 0.3682 (0.3936)  loss_n_80: 0.4131 (0.4186)  loss_n_100: 0.4392 (0.4497)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [710/845]  eta: 0:00:45  loss: 1.6650 (1.6842)  loss_n_40: 0.3973 (0.4222)  loss_n_60: 0.3912 (0.3937)  loss_n_80: 0.4163 (0.4184)  loss_n_100: 0.4532 (0.4499)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [720/845]  eta: 0:00:41  loss: 1.6803 (1.6842)  loss_n_40: 0.4067 (0.4223)  loss_n_60: 0.3856 (0.3937)  loss_n_80: 0.4135 (0.4185)  loss_n_100: 0.4456 (0.4497)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [730/845]  eta: 0:00:38  loss: 1.6622 (1.6839)  loss_n_40: 0.3807 (0.4220)  loss_n_60: 0.3753 (0.3936)  loss_n_80: 0.3976 (0.4185)  loss_n_100: 0.4426 (0.4498)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:13]  [740/845]  eta: 0:00:35  loss: 1.5388 (1.6818)  loss_n_40: 0.3641 (0.4213)  loss_n_60: 0.3648 (0.3932)  loss_n_80: 0.3726 (0.4179)  loss_n_100: 0.4202 (0.4494)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [750/845]  eta: 0:00:31  loss: 1.4835 (1.6810)  loss_n_40: 0.3641 (0.4209)  loss_n_60: 0.3606 (0.3931)  loss_n_80: 0.3726 (0.4176)  loss_n_100: 0.4104 (0.4493)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [760/845]  eta: 0:00:28  loss: 1.6036 (1.6803)  loss_n_40: 0.3664 (0.4207)  loss_n_60: 0.3714 (0.3928)  loss_n_80: 0.3806 (0.4176)  loss_n_100: 0.4410 (0.4491)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [770/845]  eta: 0:00:25  loss: 1.6036 (1.6809)  loss_n_40: 0.3691 (0.4204)  loss_n_60: 0.3743 (0.3929)  loss_n_80: 0.4149 (0.4179)  loss_n_100: 0.4410 (0.4496)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [780/845]  eta: 0:00:21  loss: 1.6013 (1.6811)  loss_n_40: 0.3757 (0.4204)  loss_n_60: 0.3775 (0.3930)  loss_n_80: 0.4141 (0.4179)  loss_n_100: 0.4402 (0.4497)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [790/845]  eta: 0:00:18  loss: 1.5953 (1.6804)  loss_n_40: 0.3785 (0.4200)  loss_n_60: 0.3709 (0.3929)  loss_n_80: 0.4062 (0.4177)  loss_n_100: 0.4413 (0.4497)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [800/845]  eta: 0:00:15  loss: 1.5953 (1.6804)  loss_n_40: 0.3675 (0.4197)  loss_n_60: 0.3733 (0.3929)  loss_n_80: 0.4039 (0.4178)  loss_n_100: 0.4413 (0.4499)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [810/845]  eta: 0:00:11  loss: 1.6416 (1.6799)  loss_n_40: 0.3638 (0.4193)  loss_n_60: 0.3945 (0.3929)  loss_n_80: 0.4055 (0.4178)  loss_n_100: 0.4410 (0.4500)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [820/845]  eta: 0:00:08  loss: 1.7164 (1.6821)  loss_n_40: 0.3809 (0.4200)  loss_n_60: 0.3972 (0.3934)  loss_n_80: 0.4312 (0.4183)  loss_n_100: 0.4736 (0.4504)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [830/845]  eta: 0:00:05  loss: 1.6712 (1.6814)  loss_n_40: 0.3936 (0.4197)  loss_n_60: 0.3841 (0.3933)  loss_n_80: 0.4312 (0.4182)  loss_n_100: 0.4492 (0.4502)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [840/845]  eta: 0:00:01  loss: 1.5860 (1.6800)  loss_n_40: 0.3780 (0.4192)  loss_n_60: 0.3713 (0.3929)  loss_n_80: 0.4048 (0.4179)  loss_n_100: 0.4291 (0.4500)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13]  [844/845]  eta: 0:00:00  loss: 1.5860 (1.6802)  loss_n_40: 0.3780 (0.4191)  loss_n_60: 0.3698 (0.3929)  loss_n_80: 0.4048 (0.4180)  loss_n_100: 0.4283 (0.4501)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:13] Total time: 0:04:43 (0.3350 s / it)\n",
      "Averaged stats: loss: 1.5860 (1.6802)  loss_n_40: 0.3780 (0.4191)  loss_n_60: 0.3698 (0.3929)  loss_n_80: 0.4048 (0.4180)  loss_n_100: 0.4283 (0.4501)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_13_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.450%\n",
      "Min loss_n_100: 0.450\n",
      "Best Epoch: 13.000\n",
      "Train: [epoch:14]  [   0/1724]  eta: 1:58:57  lr: 0.000200  loss: 1.8791 (1.8791)  loss_n_40: 0.4626 (0.4626)  loss_n_60: 0.4644 (0.4644)  loss_n_80: 0.4616 (0.4616)  loss_n_100: 0.4906 (0.4906)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1402  data: 0.3847  max mem: 46473\n",
      "Train: [epoch:14]  [  10/1724]  eta: 1:52:37  lr: 0.000200  loss: 1.5590 (1.6346)  loss_n_40: 0.4060 (0.4203)  loss_n_60: 0.3669 (0.3823)  loss_n_80: 0.3798 (0.4038)  loss_n_100: 0.4029 (0.4281)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9428  data: 0.0352  max mem: 46473\n",
      "Train: [epoch:14]  [  20/1724]  eta: 1:51:42  lr: 0.000200  loss: 1.6128 (1.6462)  loss_n_40: 0.3913 (0.4132)  loss_n_60: 0.3674 (0.3847)  loss_n_80: 0.4065 (0.4115)  loss_n_100: 0.4228 (0.4368)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9233  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [  30/1724]  eta: 1:50:56  lr: 0.000200  loss: 1.5995 (1.6151)  loss_n_40: 0.3871 (0.4067)  loss_n_60: 0.3776 (0.3785)  loss_n_80: 0.3979 (0.4035)  loss_n_100: 0.4282 (0.4265)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [  40/1724]  eta: 1:50:13  lr: 0.000200  loss: 1.4351 (1.5659)  loss_n_40: 0.3644 (0.3970)  loss_n_60: 0.3384 (0.3677)  loss_n_80: 0.3467 (0.3878)  loss_n_100: 0.3836 (0.4133)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9201  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [  50/1724]  eta: 1:49:32  lr: 0.000200  loss: 1.4197 (1.5511)  loss_n_40: 0.3530 (0.3935)  loss_n_60: 0.3294 (0.3639)  loss_n_80: 0.3423 (0.3837)  loss_n_100: 0.3835 (0.4101)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9208  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [  60/1724]  eta: 1:48:51  lr: 0.000200  loss: 1.4312 (1.5399)  loss_n_40: 0.3687 (0.3923)  loss_n_60: 0.3334 (0.3599)  loss_n_80: 0.3545 (0.3814)  loss_n_100: 0.3843 (0.4062)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [  70/1724]  eta: 1:48:11  lr: 0.000200  loss: 1.4627 (1.5324)  loss_n_40: 0.3809 (0.3951)  loss_n_60: 0.3369 (0.3583)  loss_n_80: 0.3549 (0.3782)  loss_n_100: 0.3765 (0.4009)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [  80/1724]  eta: 1:47:31  lr: 0.000200  loss: 1.4071 (1.5280)  loss_n_40: 0.3645 (0.3939)  loss_n_60: 0.3340 (0.3570)  loss_n_80: 0.3476 (0.3776)  loss_n_100: 0.3584 (0.3995)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [  90/1724]  eta: 1:46:52  lr: 0.000200  loss: 1.3573 (1.5154)  loss_n_40: 0.3652 (0.3912)  loss_n_60: 0.3117 (0.3539)  loss_n_80: 0.3277 (0.3739)  loss_n_100: 0.3584 (0.3964)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9226  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 100/1724]  eta: 1:46:12  lr: 0.000200  loss: 1.3314 (1.5065)  loss_n_40: 0.3479 (0.3878)  loss_n_60: 0.3065 (0.3519)  loss_n_80: 0.3231 (0.3716)  loss_n_100: 0.3628 (0.3952)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9222  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [ 110/1724]  eta: 1:45:33  lr: 0.000200  loss: 1.2843 (1.4955)  loss_n_40: 0.3271 (0.3849)  loss_n_60: 0.2975 (0.3491)  loss_n_80: 0.3231 (0.3687)  loss_n_100: 0.3628 (0.3928)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9223  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 120/1724]  eta: 1:44:53  lr: 0.000200  loss: 1.2572 (1.4732)  loss_n_40: 0.3003 (0.3776)  loss_n_60: 0.2904 (0.3435)  loss_n_80: 0.3155 (0.3635)  loss_n_100: 0.3492 (0.3886)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9229  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 130/1724]  eta: 1:44:14  lr: 0.000200  loss: 1.3267 (1.4827)  loss_n_40: 0.3129 (0.3791)  loss_n_60: 0.3116 (0.3449)  loss_n_80: 0.3230 (0.3652)  loss_n_100: 0.3624 (0.3898)  triple_100: 0.0000 (0.0003)  triple_80: 0.0000 (0.0004)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0031)  time: 3.9228  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 140/1724]  eta: 1:43:34  lr: 0.000200  loss: 1.5237 (1.4813)  loss_n_40: 0.3708 (0.3775)  loss_n_60: 0.3474 (0.3441)  loss_n_80: 0.3904 (0.3654)  loss_n_100: 0.4131 (0.3908)  triple_100: 0.0000 (0.0003)  triple_80: 0.0000 (0.0003)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0029)  time: 3.9219  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 150/1724]  eta: 1:42:55  lr: 0.000200  loss: 1.3600 (1.4757)  loss_n_40: 0.3416 (0.3757)  loss_n_60: 0.3082 (0.3429)  loss_n_80: 0.3401 (0.3643)  loss_n_100: 0.3744 (0.3896)  triple_100: 0.0000 (0.0002)  triple_80: 0.0000 (0.0003)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0027)  time: 3.9215  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 160/1724]  eta: 1:42:15  lr: 0.000200  loss: 1.3339 (1.4815)  loss_n_40: 0.3416 (0.3773)  loss_n_60: 0.3108 (0.3436)  loss_n_80: 0.3373 (0.3651)  loss_n_100: 0.3712 (0.3903)  triple_100: 0.0000 (0.0002)  triple_80: 0.0000 (0.0011)  triple_60: 0.0000 (0.0010)  triple_40: 0.0000 (0.0028)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 170/1724]  eta: 1:41:36  lr: 0.000200  loss: 1.4098 (1.4792)  loss_n_40: 0.3501 (0.3757)  loss_n_60: 0.3323 (0.3430)  loss_n_80: 0.3518 (0.3649)  loss_n_100: 0.3860 (0.3907)  triple_100: 0.0000 (0.0002)  triple_80: 0.0000 (0.0010)  triple_60: 0.0000 (0.0009)  triple_40: 0.0000 (0.0027)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 180/1724]  eta: 1:40:56  lr: 0.000200  loss: 1.4615 (1.4825)  loss_n_40: 0.3717 (0.3770)  loss_n_60: 0.3324 (0.3435)  loss_n_80: 0.3586 (0.3656)  loss_n_100: 0.3920 (0.3919)  triple_100: 0.0000 (0.0002)  triple_80: 0.0000 (0.0010)  triple_60: 0.0000 (0.0009)  triple_40: 0.0000 (0.0025)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 190/1724]  eta: 1:40:17  lr: 0.000200  loss: 1.5544 (1.5023)  loss_n_40: 0.3931 (0.3779)  loss_n_60: 0.3461 (0.3453)  loss_n_80: 0.3739 (0.3688)  loss_n_100: 0.4099 (0.3964)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0031)  triple_60: 0.0000 (0.0036)  triple_40: 0.0000 (0.0024)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 200/1724]  eta: 1:39:38  lr: 0.000200  loss: 1.5845 (1.5084)  loss_n_40: 0.3869 (0.3775)  loss_n_60: 0.3566 (0.3464)  loss_n_80: 0.4088 (0.3713)  loss_n_100: 0.4535 (0.4000)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0029)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0023)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 210/1724]  eta: 1:38:58  lr: 0.000200  loss: 1.5651 (1.5270)  loss_n_40: 0.3748 (0.3797)  loss_n_60: 0.3455 (0.3474)  loss_n_80: 0.3999 (0.3728)  loss_n_100: 0.4369 (0.4014)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0028)  triple_60: 0.0000 (0.0032)  triple_40: 0.0000 (0.0154)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 220/1724]  eta: 1:38:19  lr: 0.000200  loss: 1.6445 (1.5455)  loss_n_40: 0.4154 (0.3832)  loss_n_60: 0.3607 (0.3516)  loss_n_80: 0.4191 (0.3784)  loss_n_100: 0.4304 (0.4077)  triple_100: 0.0000 (0.0042)  triple_80: 0.0000 (0.0027)  triple_60: 0.0000 (0.0031)  triple_40: 0.0000 (0.0147)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 230/1724]  eta: 1:37:39  lr: 0.000200  loss: 1.7853 (1.5627)  loss_n_40: 0.4221 (0.3862)  loss_n_60: 0.4110 (0.3558)  loss_n_80: 0.4543 (0.3834)  loss_n_100: 0.4888 (0.4137)  triple_100: 0.0000 (0.0040)  triple_80: 0.0000 (0.0026)  triple_60: 0.0000 (0.0029)  triple_40: 0.0000 (0.0141)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 240/1724]  eta: 1:37:00  lr: 0.000200  loss: 1.8356 (1.5752)  loss_n_40: 0.4395 (0.3888)  loss_n_60: 0.4317 (0.3587)  loss_n_80: 0.4759 (0.3871)  loss_n_100: 0.5077 (0.4181)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0025)  triple_60: 0.0000 (0.0028)  triple_40: 0.0000 (0.0135)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 250/1724]  eta: 1:36:20  lr: 0.000200  loss: 1.7307 (1.5717)  loss_n_40: 0.4395 (0.3882)  loss_n_60: 0.3750 (0.3577)  loss_n_80: 0.4248 (0.3863)  loss_n_100: 0.4466 (0.4178)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0024)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0129)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 260/1724]  eta: 1:35:41  lr: 0.000200  loss: 1.3713 (1.5712)  loss_n_40: 0.3507 (0.3893)  loss_n_60: 0.3119 (0.3576)  loss_n_80: 0.3532 (0.3860)  loss_n_100: 0.3935 (0.4175)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0023)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0124)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 270/1724]  eta: 1:35:01  lr: 0.000200  loss: 1.3635 (1.5637)  loss_n_40: 0.3482 (0.3883)  loss_n_60: 0.3162 (0.3559)  loss_n_80: 0.3358 (0.3842)  loss_n_100: 0.3638 (0.4152)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0022)  triple_60: 0.0000 (0.0025)  triple_40: 0.0000 (0.0120)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 280/1724]  eta: 1:34:22  lr: 0.000200  loss: 1.2940 (1.5554)  loss_n_40: 0.3369 (0.3865)  loss_n_60: 0.2936 (0.3539)  loss_n_80: 0.3321 (0.3822)  loss_n_100: 0.3542 (0.4134)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0021)  triple_60: 0.0000 (0.0024)  triple_40: 0.0000 (0.0116)  time: 3.9178  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 290/1724]  eta: 1:33:43  lr: 0.000200  loss: 1.2944 (1.5481)  loss_n_40: 0.3213 (0.3850)  loss_n_60: 0.3014 (0.3523)  loss_n_80: 0.3321 (0.3804)  loss_n_100: 0.3581 (0.4116)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0020)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0112)  time: 3.9174  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 300/1724]  eta: 1:33:03  lr: 0.000200  loss: 1.3118 (1.5401)  loss_n_40: 0.3307 (0.3836)  loss_n_60: 0.3048 (0.3507)  loss_n_80: 0.3098 (0.3783)  loss_n_100: 0.3506 (0.4095)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0020)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0108)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 310/1724]  eta: 1:32:24  lr: 0.000200  loss: 1.3149 (1.5349)  loss_n_40: 0.3466 (0.3829)  loss_n_60: 0.3059 (0.3496)  loss_n_80: 0.3098 (0.3769)  loss_n_100: 0.3478 (0.4080)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0019)  triple_60: 0.0000 (0.0022)  triple_40: 0.0000 (0.0104)  time: 3.9189  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 320/1724]  eta: 1:31:45  lr: 0.000200  loss: 1.4137 (1.5319)  loss_n_40: 0.3602 (0.3829)  loss_n_60: 0.3294 (0.3490)  loss_n_80: 0.3378 (0.3761)  loss_n_100: 0.3775 (0.4070)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0018)  triple_60: 0.0000 (0.0022)  triple_40: 0.0000 (0.0101)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 330/1724]  eta: 1:31:05  lr: 0.000200  loss: 1.4137 (1.5299)  loss_n_40: 0.3602 (0.3825)  loss_n_60: 0.3147 (0.3484)  loss_n_80: 0.3465 (0.3756)  loss_n_100: 0.3775 (0.4063)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0020)  triple_60: 0.0000 (0.0025)  triple_40: 0.0000 (0.0098)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 340/1724]  eta: 1:30:26  lr: 0.000200  loss: 1.3699 (1.5418)  loss_n_40: 0.3492 (0.3815)  loss_n_60: 0.3129 (0.3476)  loss_n_80: 0.3352 (0.3744)  loss_n_100: 0.3772 (0.4055)  triple_100: 0.0000 (0.0069)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0137)  time: 3.9167  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [ 350/1724]  eta: 1:29:46  lr: 0.000200  loss: 1.6940 (1.5612)  loss_n_40: 0.3637 (0.3846)  loss_n_60: 0.3667 (0.3516)  loss_n_80: 0.4585 (0.3806)  loss_n_100: 0.4731 (0.4126)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0065)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0134)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 360/1724]  eta: 1:29:07  lr: 0.000200  loss: 2.1201 (1.5817)  loss_n_40: 0.4555 (0.3867)  loss_n_60: 0.4657 (0.3548)  loss_n_80: 0.5661 (0.3856)  loss_n_100: 0.6216 (0.4189)  triple_100: 0.0000 (0.0074)  triple_80: 0.0000 (0.0073)  triple_60: 0.0000 (0.0064)  triple_40: 0.0000 (0.0145)  time: 3.9171  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 370/1724]  eta: 1:28:28  lr: 0.000200  loss: 2.0997 (1.5970)  loss_n_40: 0.4773 (0.3901)  loss_n_60: 0.4589 (0.3580)  loss_n_80: 0.5560 (0.3900)  loss_n_100: 0.6187 (0.4242)  triple_100: 0.0000 (0.0072)  triple_80: 0.0000 (0.0071)  triple_60: 0.0000 (0.0062)  triple_40: 0.0000 (0.0141)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 380/1724]  eta: 1:27:49  lr: 0.000200  loss: 2.0859 (1.6060)  loss_n_40: 0.4932 (0.3922)  loss_n_60: 0.4507 (0.3600)  loss_n_80: 0.5266 (0.3926)  loss_n_100: 0.5535 (0.4273)  triple_100: 0.0000 (0.0070)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0061)  triple_40: 0.0000 (0.0138)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 390/1724]  eta: 1:27:09  lr: 0.000200  loss: 1.8177 (1.6110)  loss_n_40: 0.4517 (0.3936)  loss_n_60: 0.4135 (0.3615)  loss_n_80: 0.4396 (0.3937)  loss_n_100: 0.5087 (0.4292)  triple_100: 0.0000 (0.0068)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0134)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 400/1724]  eta: 1:26:30  lr: 0.000200  loss: 1.7862 (1.6144)  loss_n_40: 0.4357 (0.3945)  loss_n_60: 0.4055 (0.3624)  loss_n_80: 0.4359 (0.3947)  loss_n_100: 0.4927 (0.4307)  triple_100: 0.0000 (0.0066)  triple_80: 0.0000 (0.0066)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0131)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 410/1724]  eta: 1:25:51  lr: 0.000200  loss: 1.6758 (1.6157)  loss_n_40: 0.4178 (0.3952)  loss_n_60: 0.3804 (0.3630)  loss_n_80: 0.4233 (0.3950)  loss_n_100: 0.4577 (0.4312)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0064)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0128)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 420/1724]  eta: 1:25:11  lr: 0.000200  loss: 1.5947 (1.6162)  loss_n_40: 0.4133 (0.3958)  loss_n_60: 0.3623 (0.3633)  loss_n_80: 0.3984 (0.3951)  loss_n_100: 0.4349 (0.4315)  triple_100: 0.0000 (0.0063)  triple_80: 0.0000 (0.0063)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0125)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 430/1724]  eta: 1:24:32  lr: 0.000200  loss: 1.5416 (1.6153)  loss_n_40: 0.4210 (0.3963)  loss_n_60: 0.3418 (0.3631)  loss_n_80: 0.3676 (0.3948)  loss_n_100: 0.4271 (0.4312)  triple_100: 0.0000 (0.0062)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0122)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 440/1724]  eta: 1:23:53  lr: 0.000200  loss: 1.4975 (1.6124)  loss_n_40: 0.3924 (0.3959)  loss_n_60: 0.3411 (0.3626)  loss_n_80: 0.3676 (0.3942)  loss_n_100: 0.4201 (0.4306)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0119)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 450/1724]  eta: 1:23:14  lr: 0.000200  loss: 1.4095 (1.6085)  loss_n_40: 0.3621 (0.3958)  loss_n_60: 0.3164 (0.3615)  loss_n_80: 0.3380 (0.3930)  loss_n_100: 0.3848 (0.4297)  triple_100: 0.0000 (0.0059)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0116)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 460/1724]  eta: 1:22:34  lr: 0.000200  loss: 1.4095 (1.6056)  loss_n_40: 0.3621 (0.3960)  loss_n_60: 0.3099 (0.3609)  loss_n_80: 0.3339 (0.3922)  loss_n_100: 0.3766 (0.4287)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0114)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 470/1724]  eta: 1:21:55  lr: 0.000200  loss: 1.3425 (1.5979)  loss_n_40: 0.3378 (0.3947)  loss_n_60: 0.2995 (0.3592)  loss_n_80: 0.3280 (0.3902)  loss_n_100: 0.3607 (0.4266)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0056)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0111)  time: 3.9194  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 480/1724]  eta: 1:21:16  lr: 0.000200  loss: 1.2844 (1.5934)  loss_n_40: 0.3363 (0.3946)  loss_n_60: 0.2814 (0.3580)  loss_n_80: 0.2997 (0.3888)  loss_n_100: 0.3460 (0.4252)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0055)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0109)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 490/1724]  eta: 1:20:37  lr: 0.000200  loss: 1.3347 (1.5893)  loss_n_40: 0.3485 (0.3937)  loss_n_60: 0.3070 (0.3573)  loss_n_80: 0.3221 (0.3878)  loss_n_100: 0.3568 (0.4243)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0107)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 500/1724]  eta: 1:19:58  lr: 0.000200  loss: 1.3256 (1.5845)  loss_n_40: 0.3318 (0.3926)  loss_n_60: 0.3145 (0.3567)  loss_n_80: 0.3251 (0.3866)  loss_n_100: 0.3609 (0.4230)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0105)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 510/1724]  eta: 1:19:18  lr: 0.000200  loss: 1.3401 (1.5821)  loss_n_40: 0.3626 (0.3927)  loss_n_60: 0.3189 (0.3563)  loss_n_80: 0.3189 (0.3858)  loss_n_100: 0.3597 (0.4221)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0052)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0103)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 520/1724]  eta: 1:18:39  lr: 0.000200  loss: 1.3295 (1.5779)  loss_n_40: 0.3467 (0.3918)  loss_n_60: 0.3026 (0.3556)  loss_n_80: 0.3270 (0.3848)  loss_n_100: 0.3533 (0.4210)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0101)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 530/1724]  eta: 1:18:00  lr: 0.000200  loss: 1.3262 (1.5741)  loss_n_40: 0.3256 (0.3909)  loss_n_60: 0.3095 (0.3549)  loss_n_80: 0.3298 (0.3840)  loss_n_100: 0.3562 (0.4201)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0099)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 540/1724]  eta: 1:17:21  lr: 0.000200  loss: 1.3262 (1.5685)  loss_n_40: 0.3190 (0.3894)  loss_n_60: 0.3047 (0.3538)  loss_n_80: 0.3312 (0.3827)  loss_n_100: 0.3602 (0.4189)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0097)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 550/1724]  eta: 1:16:42  lr: 0.000200  loss: 1.1875 (1.5616)  loss_n_40: 0.2958 (0.3878)  loss_n_60: 0.2763 (0.3524)  loss_n_80: 0.2874 (0.3810)  loss_n_100: 0.3307 (0.4171)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0048)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0095)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 560/1724]  eta: 1:16:02  lr: 0.000200  loss: 1.2250 (1.5628)  loss_n_40: 0.3306 (0.3875)  loss_n_60: 0.2824 (0.3518)  loss_n_80: 0.2852 (0.3802)  loss_n_100: 0.3195 (0.4161)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0103)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 570/1724]  eta: 1:15:23  lr: 0.000200  loss: 1.9014 (1.5951)  loss_n_40: 0.4416 (0.3901)  loss_n_60: 0.4082 (0.3547)  loss_n_80: 0.4604 (0.3838)  loss_n_100: 0.4670 (0.4204)  triple_100: 0.0000 (0.0069)  triple_80: 0.0000 (0.0125)  triple_60: 0.0000 (0.0112)  triple_40: 0.0000 (0.0156)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 580/1724]  eta: 1:14:44  lr: 0.000200  loss: 2.4995 (1.6119)  loss_n_40: 0.5190 (0.3930)  loss_n_60: 0.5333 (0.3584)  loss_n_80: 0.6307 (0.3888)  loss_n_100: 0.7257 (0.4264)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0123)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0153)  time: 3.9166  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [ 590/1724]  eta: 1:14:04  lr: 0.000200  loss: 2.3227 (1.6188)  loss_n_40: 0.5187 (0.3948)  loss_n_60: 0.5230 (0.3600)  loss_n_80: 0.6064 (0.3908)  loss_n_100: 0.6556 (0.4287)  triple_100: 0.0000 (0.0066)  triple_80: 0.0000 (0.0121)  triple_60: 0.0000 (0.0108)  triple_40: 0.0000 (0.0151)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 600/1724]  eta: 1:13:25  lr: 0.000200  loss: 1.9038 (1.6228)  loss_n_40: 0.4748 (0.3958)  loss_n_60: 0.4458 (0.3610)  loss_n_80: 0.4889 (0.3922)  loss_n_100: 0.5232 (0.4301)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0119)  triple_60: 0.0000 (0.0106)  triple_40: 0.0000 (0.0148)  time: 3.9157  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 610/1724]  eta: 1:12:46  lr: 0.000200  loss: 1.7230 (1.6239)  loss_n_40: 0.4291 (0.3962)  loss_n_60: 0.3905 (0.3615)  loss_n_80: 0.4272 (0.3926)  loss_n_100: 0.4974 (0.4305)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0117)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0146)  time: 3.9156  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 620/1724]  eta: 1:12:07  lr: 0.000200  loss: 1.6530 (1.6253)  loss_n_40: 0.4224 (0.3970)  loss_n_60: 0.3877 (0.3619)  loss_n_80: 0.3944 (0.3931)  loss_n_100: 0.4451 (0.4308)  triple_100: 0.0000 (0.0063)  triple_80: 0.0000 (0.0115)  triple_60: 0.0000 (0.0103)  triple_40: 0.0000 (0.0143)  time: 3.9147  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 630/1724]  eta: 1:11:27  lr: 0.000200  loss: 1.5500 (1.6240)  loss_n_40: 0.4088 (0.3973)  loss_n_60: 0.3554 (0.3618)  loss_n_80: 0.3730 (0.3929)  loss_n_100: 0.4157 (0.4304)  triple_100: 0.0000 (0.0062)  triple_80: 0.0000 (0.0113)  triple_60: 0.0000 (0.0101)  triple_40: 0.0000 (0.0141)  time: 3.9153  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 640/1724]  eta: 1:10:48  lr: 0.000200  loss: 1.4841 (1.6222)  loss_n_40: 0.3796 (0.3970)  loss_n_60: 0.3360 (0.3615)  loss_n_80: 0.3644 (0.3925)  loss_n_100: 0.4101 (0.4301)  triple_100: 0.0000 (0.0061)  triple_80: 0.0000 (0.0111)  triple_60: 0.0000 (0.0100)  triple_40: 0.0000 (0.0139)  time: 3.9169  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 650/1724]  eta: 1:10:09  lr: 0.000200  loss: 1.4805 (1.6204)  loss_n_40: 0.3746 (0.3970)  loss_n_60: 0.3360 (0.3613)  loss_n_80: 0.3624 (0.3920)  loss_n_100: 0.4043 (0.4295)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0110)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0137)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 660/1724]  eta: 1:09:30  lr: 0.000200  loss: 1.5182 (1.6194)  loss_n_40: 0.3907 (0.3972)  loss_n_60: 0.3386 (0.3612)  loss_n_80: 0.3591 (0.3919)  loss_n_100: 0.4003 (0.4293)  triple_100: 0.0000 (0.0059)  triple_80: 0.0000 (0.0108)  triple_60: 0.0000 (0.0097)  triple_40: 0.0000 (0.0135)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 670/1724]  eta: 1:08:50  lr: 0.000200  loss: 1.4263 (1.6174)  loss_n_40: 0.3907 (0.3973)  loss_n_60: 0.3322 (0.3608)  loss_n_80: 0.3418 (0.3913)  loss_n_100: 0.3843 (0.4287)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0106)  triple_60: 0.0000 (0.0095)  triple_40: 0.0000 (0.0133)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 680/1724]  eta: 1:08:11  lr: 0.000200  loss: 1.3413 (1.6133)  loss_n_40: 0.3486 (0.3967)  loss_n_60: 0.3059 (0.3600)  loss_n_80: 0.3277 (0.3903)  loss_n_100: 0.3603 (0.4276)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0131)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 690/1724]  eta: 1:07:32  lr: 0.000200  loss: 1.3413 (1.6103)  loss_n_40: 0.3481 (0.3963)  loss_n_60: 0.3059 (0.3594)  loss_n_80: 0.3277 (0.3896)  loss_n_100: 0.3597 (0.4269)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0092)  triple_40: 0.0000 (0.0129)  time: 3.9157  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 700/1724]  eta: 1:06:53  lr: 0.000200  loss: 1.4179 (1.6072)  loss_n_40: 0.3578 (0.3958)  loss_n_60: 0.3071 (0.3587)  loss_n_80: 0.3437 (0.3890)  loss_n_100: 0.3705 (0.4261)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0102)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0127)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 710/1724]  eta: 1:06:14  lr: 0.000200  loss: 1.4122 (1.6049)  loss_n_40: 0.3578 (0.3957)  loss_n_60: 0.3082 (0.3583)  loss_n_80: 0.3365 (0.3884)  loss_n_100: 0.3705 (0.4255)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0100)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0125)  time: 3.9196  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 720/1724]  eta: 1:05:34  lr: 0.000200  loss: 1.3483 (1.6021)  loss_n_40: 0.3522 (0.3954)  loss_n_60: 0.3082 (0.3578)  loss_n_80: 0.3198 (0.3877)  loss_n_100: 0.3533 (0.4247)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0099)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0123)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 730/1724]  eta: 1:04:55  lr: 0.000200  loss: 1.3301 (1.5986)  loss_n_40: 0.3391 (0.3947)  loss_n_60: 0.2996 (0.3571)  loss_n_80: 0.3154 (0.3869)  loss_n_100: 0.3352 (0.4239)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0098)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0122)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 740/1724]  eta: 1:04:16  lr: 0.000200  loss: 1.2503 (1.5937)  loss_n_40: 0.3245 (0.3937)  loss_n_60: 0.2803 (0.3561)  loss_n_80: 0.2975 (0.3857)  loss_n_100: 0.3309 (0.4226)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0096)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0120)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 750/1724]  eta: 1:03:37  lr: 0.000200  loss: 1.2503 (1.5896)  loss_n_40: 0.3245 (0.3929)  loss_n_60: 0.2824 (0.3553)  loss_n_80: 0.2989 (0.3848)  loss_n_100: 0.3329 (0.4216)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0095)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0119)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 760/1724]  eta: 1:02:58  lr: 0.000200  loss: 1.2791 (1.5859)  loss_n_40: 0.3238 (0.3922)  loss_n_60: 0.2876 (0.3545)  loss_n_80: 0.3144 (0.3839)  loss_n_100: 0.3404 (0.4207)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0094)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0117)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 770/1724]  eta: 1:02:18  lr: 0.000200  loss: 1.2725 (1.5831)  loss_n_40: 0.3338 (0.3920)  loss_n_60: 0.2876 (0.3540)  loss_n_80: 0.3127 (0.3832)  loss_n_100: 0.3354 (0.4198)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0093)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0115)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 780/1724]  eta: 1:01:39  lr: 0.000200  loss: 1.2725 (1.5796)  loss_n_40: 0.3346 (0.3913)  loss_n_60: 0.2898 (0.3533)  loss_n_80: 0.3127 (0.3824)  loss_n_100: 0.3331 (0.4189)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0091)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0114)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 790/1724]  eta: 1:01:00  lr: 0.000200  loss: 1.2486 (1.5751)  loss_n_40: 0.3125 (0.3905)  loss_n_60: 0.2802 (0.3524)  loss_n_80: 0.2956 (0.3813)  loss_n_100: 0.3345 (0.4177)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0090)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0113)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 800/1724]  eta: 1:00:21  lr: 0.000200  loss: 1.2005 (1.5710)  loss_n_40: 0.3106 (0.3896)  loss_n_60: 0.2802 (0.3516)  loss_n_80: 0.3042 (0.3804)  loss_n_100: 0.3289 (0.4166)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0089)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0111)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 810/1724]  eta: 0:59:42  lr: 0.000200  loss: 1.4008 (1.5768)  loss_n_40: 0.3371 (0.3895)  loss_n_60: 0.3188 (0.3524)  loss_n_80: 0.3360 (0.3819)  loss_n_100: 0.3647 (0.4186)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0095)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0110)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 820/1724]  eta: 0:59:02  lr: 0.000200  loss: 1.8591 (1.5839)  loss_n_40: 0.3957 (0.3902)  loss_n_60: 0.4194 (0.3539)  loss_n_80: 0.5055 (0.3843)  loss_n_100: 0.5732 (0.4215)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0094)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0108)  time: 3.9189  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [ 830/1724]  eta: 0:58:23  lr: 0.000200  loss: 1.8777 (1.5884)  loss_n_40: 0.4213 (0.3908)  loss_n_60: 0.4363 (0.3550)  loss_n_80: 0.5015 (0.3857)  loss_n_100: 0.5779 (0.4233)  triple_100: 0.0000 (0.0059)  triple_80: 0.0000 (0.0092)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0107)  time: 3.9198  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 840/1724]  eta: 0:57:44  lr: 0.000200  loss: 1.8310 (1.5904)  loss_n_40: 0.4275 (0.3912)  loss_n_60: 0.4076 (0.3555)  loss_n_80: 0.4693 (0.3864)  loss_n_100: 0.5251 (0.4242)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0091)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0106)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 850/1724]  eta: 0:57:05  lr: 0.000200  loss: 1.6509 (1.5910)  loss_n_40: 0.4234 (0.3915)  loss_n_60: 0.3853 (0.3556)  loss_n_80: 0.4142 (0.3867)  loss_n_100: 0.4639 (0.4245)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0090)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0105)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 860/1724]  eta: 0:56:26  lr: 0.000200  loss: 1.5522 (1.5900)  loss_n_40: 0.4086 (0.3915)  loss_n_60: 0.3531 (0.3555)  loss_n_80: 0.3922 (0.3865)  loss_n_100: 0.3997 (0.4241)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0089)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0103)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 870/1724]  eta: 0:55:46  lr: 0.000200  loss: 1.5088 (1.5892)  loss_n_40: 0.3843 (0.3914)  loss_n_60: 0.3354 (0.3554)  loss_n_80: 0.3711 (0.3864)  loss_n_100: 0.3978 (0.4240)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0088)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0102)  time: 3.9191  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 880/1724]  eta: 0:55:07  lr: 0.000200  loss: 1.4138 (1.5860)  loss_n_40: 0.3619 (0.3908)  loss_n_60: 0.3244 (0.3548)  loss_n_80: 0.3520 (0.3857)  loss_n_100: 0.3829 (0.4231)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0087)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0101)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 890/1724]  eta: 0:54:28  lr: 0.000200  loss: 1.2943 (1.5830)  loss_n_40: 0.3350 (0.3903)  loss_n_60: 0.2938 (0.3542)  loss_n_80: 0.3019 (0.3849)  loss_n_100: 0.3315 (0.4223)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0086)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0100)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 900/1724]  eta: 0:53:49  lr: 0.000200  loss: 1.2882 (1.5802)  loss_n_40: 0.3242 (0.3900)  loss_n_60: 0.2881 (0.3536)  loss_n_80: 0.3116 (0.3842)  loss_n_100: 0.3411 (0.4215)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0085)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0099)  time: 3.9189  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 910/1724]  eta: 0:53:10  lr: 0.000200  loss: 1.2571 (1.5765)  loss_n_40: 0.3194 (0.3893)  loss_n_60: 0.2861 (0.3528)  loss_n_80: 0.3102 (0.3833)  loss_n_100: 0.3338 (0.4205)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0084)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0098)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 920/1724]  eta: 0:52:30  lr: 0.000200  loss: 1.2170 (1.5731)  loss_n_40: 0.3077 (0.3887)  loss_n_60: 0.2741 (0.3522)  loss_n_80: 0.2980 (0.3824)  loss_n_100: 0.3307 (0.4196)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0083)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0097)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 930/1724]  eta: 0:51:51  lr: 0.000200  loss: 1.2200 (1.5704)  loss_n_40: 0.3115 (0.3883)  loss_n_60: 0.2741 (0.3516)  loss_n_80: 0.3085 (0.3818)  loss_n_100: 0.3346 (0.4189)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0082)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0096)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 940/1724]  eta: 0:51:12  lr: 0.000200  loss: 1.2806 (1.5674)  loss_n_40: 0.3292 (0.3877)  loss_n_60: 0.2897 (0.3510)  loss_n_80: 0.3095 (0.3811)  loss_n_100: 0.3366 (0.4180)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0082)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0095)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 950/1724]  eta: 0:50:33  lr: 0.000200  loss: 1.2578 (1.5649)  loss_n_40: 0.3239 (0.3871)  loss_n_60: 0.2900 (0.3506)  loss_n_80: 0.3067 (0.3805)  loss_n_100: 0.3366 (0.4173)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0081)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0094)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 960/1724]  eta: 0:49:54  lr: 0.000200  loss: 1.2273 (1.5612)  loss_n_40: 0.2924 (0.3861)  loss_n_60: 0.2735 (0.3498)  loss_n_80: 0.2918 (0.3795)  loss_n_100: 0.3238 (0.4163)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0082)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0093)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 970/1724]  eta: 0:49:15  lr: 0.000200  loss: 1.2825 (1.5742)  loss_n_40: 0.3160 (0.3861)  loss_n_60: 0.2880 (0.3503)  loss_n_80: 0.3101 (0.3804)  loss_n_100: 0.3456 (0.4173)  triple_100: 0.0000 (0.0075)  triple_80: 0.0000 (0.0109)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0123)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [ 980/1724]  eta: 0:48:35  lr: 0.000200  loss: 2.2203 (1.6055)  loss_n_40: 0.4456 (0.3878)  loss_n_60: 0.4942 (0.3528)  loss_n_80: 0.5736 (0.3837)  loss_n_100: 0.6322 (0.4211)  triple_100: 0.0000 (0.0164)  triple_80: 0.0000 (0.0190)  triple_60: 0.0000 (0.0107)  triple_40: 0.0000 (0.0139)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [ 990/1724]  eta: 0:47:56  lr: 0.000200  loss: 3.5373 (1.6358)  loss_n_40: 0.7301 (0.3931)  loss_n_60: 0.7709 (0.3586)  loss_n_80: 0.8785 (0.3907)  loss_n_100: 0.9762 (0.4286)  triple_100: 0.0000 (0.0187)  triple_80: 0.0000 (0.0214)  triple_60: 0.0000 (0.0106)  triple_40: 0.0000 (0.0141)  time: 3.9167  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1000/1724]  eta: 0:47:17  lr: 0.000200  loss: 4.2037 (1.6599)  loss_n_40: 0.9102 (0.3994)  loss_n_60: 0.9197 (0.3642)  loss_n_80: 1.0515 (0.3970)  loss_n_100: 1.0827 (0.4351)  triple_100: 0.0000 (0.0185)  triple_80: 0.0000 (0.0212)  triple_60: 0.0000 (0.0105)  triple_40: 0.0000 (0.0140)  time: 3.9151  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1010/1724]  eta: 0:46:38  lr: 0.000200  loss: 3.4707 (1.6767)  loss_n_40: 0.9101 (0.4038)  loss_n_60: 0.8453 (0.3683)  loss_n_80: 0.8672 (0.4013)  loss_n_100: 0.9744 (0.4397)  triple_100: 0.0000 (0.0184)  triple_80: 0.0000 (0.0210)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0138)  time: 3.9156  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1020/1724]  eta: 0:45:58  lr: 0.000200  loss: 4.1662 (1.8001)  loss_n_40: 0.9231 (0.4153)  loss_n_60: 0.9160 (0.3827)  loss_n_80: 0.9436 (0.4179)  loss_n_100: 1.0143 (0.4566)  triple_100: 0.0000 (0.0381)  triple_80: 0.0000 (0.0394)  triple_60: 0.0000 (0.0264)  triple_40: 0.0000 (0.0239)  time: 3.9160  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1030/1724]  eta: 0:45:19  lr: 0.000200  loss: 10.6516 (1.8873)  loss_n_40: 2.3685 (0.4370)  loss_n_60: 2.4193 (0.4038)  loss_n_80: 2.5786 (0.4417)  loss_n_100: 2.5669 (0.4779)  triple_100: 0.0000 (0.0377)  triple_80: 0.0000 (0.0390)  triple_60: 0.0000 (0.0261)  triple_40: 0.0000 (0.0240)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1040/1724]  eta: 0:44:40  lr: 0.000200  loss: 11.2198 (1.9755)  loss_n_40: 2.6288 (0.4579)  loss_n_60: 2.6463 (0.4260)  loss_n_80: 2.9984 (0.4655)  loss_n_100: 2.8485 (0.5005)  triple_100: 0.0000 (0.0373)  triple_80: 0.0000 (0.0386)  triple_60: 0.0000 (0.0259)  triple_40: 0.0000 (0.0238)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1050/1724]  eta: 0:44:01  lr: 0.000200  loss: 10.5588 (2.0532)  loss_n_40: 2.4145 (0.4771)  loss_n_60: 2.5861 (0.4458)  loss_n_80: 2.7316 (0.4856)  loss_n_100: 2.7663 (0.5203)  triple_100: 0.0000 (0.0370)  triple_80: 0.0000 (0.0382)  triple_60: 0.0000 (0.0256)  triple_40: 0.0000 (0.0236)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1060/1724]  eta: 0:43:22  lr: 0.000200  loss: 10.0971 (2.1311)  loss_n_40: 2.3612 (0.4955)  loss_n_60: 2.5155 (0.4658)  loss_n_80: 2.5447 (0.5056)  loss_n_100: 2.4693 (0.5395)  triple_100: 0.0000 (0.0366)  triple_80: 0.0000 (0.0379)  triple_60: 0.0000 (0.0254)  triple_40: 0.0000 (0.0248)  time: 3.9161  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [1070/1724]  eta: 0:42:42  lr: 0.000200  loss: 10.7443 (2.2130)  loss_n_40: 2.4397 (0.5133)  loss_n_60: 2.6798 (0.4861)  loss_n_80: 2.7188 (0.5263)  loss_n_100: 2.6825 (0.5592)  triple_100: 0.0000 (0.0374)  triple_80: 0.0000 (0.0375)  triple_60: 0.0000 (0.0274)  triple_40: 0.0000 (0.0257)  time: 3.9155  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1080/1724]  eta: 0:42:03  lr: 0.000200  loss: 9.7794 (2.2760)  loss_n_40: 2.2619 (0.5274)  loss_n_60: 2.4396 (0.5028)  loss_n_80: 2.4689 (0.5429)  loss_n_100: 2.4798 (0.5760)  triple_100: 0.0000 (0.0371)  triple_80: 0.0000 (0.0373)  triple_60: 0.0000 (0.0272)  triple_40: 0.0000 (0.0254)  time: 3.9155  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1090/1724]  eta: 0:41:24  lr: 0.000200  loss: 9.2972 (2.3457)  loss_n_40: 2.1409 (0.5426)  loss_n_60: 2.3678 (0.5208)  loss_n_80: 2.4092 (0.5615)  loss_n_100: 2.4647 (0.5950)  triple_100: 0.0000 (0.0367)  triple_80: 0.0000 (0.0370)  triple_60: 0.0000 (0.0269)  triple_40: 0.0000 (0.0252)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1100/1724]  eta: 0:40:45  lr: 0.000200  loss: 9.9499 (2.4091)  loss_n_40: 2.0601 (0.5554)  loss_n_60: 2.4486 (0.5369)  loss_n_80: 2.6538 (0.5787)  loss_n_100: 2.6514 (0.6125)  triple_100: 0.0000 (0.0364)  triple_80: 0.0000 (0.0366)  triple_60: 0.0000 (0.0267)  triple_40: 0.0000 (0.0259)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1110/1724]  eta: 0:40:06  lr: 0.000200  loss: 8.9587 (2.4695)  loss_n_40: 1.8815 (0.5674)  loss_n_60: 2.2506 (0.5526)  loss_n_80: 2.4000 (0.5956)  loss_n_100: 2.4399 (0.6295)  triple_100: 0.0000 (0.0361)  triple_80: 0.0000 (0.0363)  triple_60: 0.0000 (0.0264)  triple_40: 0.0000 (0.0257)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1120/1724]  eta: 0:39:26  lr: 0.000200  loss: 9.0046 (2.5291)  loss_n_40: 1.8029 (0.5790)  loss_n_60: 2.2069 (0.5676)  loss_n_80: 2.4255 (0.6125)  loss_n_100: 2.5240 (0.6467)  triple_100: 0.0000 (0.0358)  triple_80: 0.0000 (0.0360)  triple_60: 0.0000 (0.0262)  triple_40: 0.0000 (0.0254)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1130/1724]  eta: 0:38:47  lr: 0.000200  loss: 9.0227 (2.5838)  loss_n_40: 1.8191 (0.5897)  loss_n_60: 2.1965 (0.5816)  loss_n_80: 2.4563 (0.6280)  loss_n_100: 2.5240 (0.6622)  triple_100: 0.0000 (0.0354)  triple_80: 0.0000 (0.0356)  triple_60: 0.0000 (0.0260)  triple_40: 0.0000 (0.0252)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1140/1724]  eta: 0:38:08  lr: 0.000200  loss: 7.5438 (2.6318)  loss_n_40: 1.6030 (0.5988)  loss_n_60: 1.8715 (0.5936)  loss_n_80: 2.0686 (0.6416)  loss_n_100: 2.1541 (0.6761)  triple_100: 0.0000 (0.0353)  triple_80: 0.0000 (0.0356)  triple_60: 0.0000 (0.0257)  triple_40: 0.0000 (0.0250)  time: 3.9171  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1150/1724]  eta: 0:37:29  lr: 0.000200  loss: 7.5175 (2.6811)  loss_n_40: 1.5255 (0.6077)  loss_n_60: 1.7974 (0.6057)  loss_n_80: 2.0673 (0.6557)  loss_n_100: 2.2623 (0.6910)  triple_100: 0.0000 (0.0350)  triple_80: 0.0000 (0.0358)  triple_60: 0.0000 (0.0255)  triple_40: 0.0000 (0.0248)  time: 3.9175  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1160/1724]  eta: 0:36:50  lr: 0.000200  loss: 8.3379 (2.7326)  loss_n_40: 1.6205 (0.6171)  loss_n_60: 1.9908 (0.6182)  loss_n_80: 2.2715 (0.6705)  loss_n_100: 2.4016 (0.7067)  triple_100: 0.0000 (0.0347)  triple_80: 0.0000 (0.0355)  triple_60: 0.0000 (0.0253)  triple_40: 0.0000 (0.0246)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1170/1724]  eta: 0:36:10  lr: 0.000200  loss: 8.3379 (2.7777)  loss_n_40: 1.6358 (0.6255)  loss_n_60: 1.9097 (0.6291)  loss_n_80: 2.2751 (0.6836)  loss_n_100: 2.4211 (0.7205)  triple_100: 0.0000 (0.0344)  triple_80: 0.0000 (0.0352)  triple_60: 0.0000 (0.0251)  triple_40: 0.0000 (0.0244)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1180/1724]  eta: 0:35:31  lr: 0.000200  loss: 7.6621 (2.8171)  loss_n_40: 1.5994 (0.6332)  loss_n_60: 1.8007 (0.6384)  loss_n_80: 2.0635 (0.6947)  loss_n_100: 2.2795 (0.7327)  triple_100: 0.0000 (0.0341)  triple_80: 0.0000 (0.0349)  triple_60: 0.0000 (0.0249)  triple_40: 0.0000 (0.0242)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1190/1724]  eta: 0:34:52  lr: 0.000200  loss: 7.6613 (2.8700)  loss_n_40: 1.5502 (0.6415)  loss_n_60: 1.7732 (0.6487)  loss_n_80: 2.0446 (0.7064)  loss_n_100: 2.1673 (0.7458)  triple_100: 0.0000 (0.0355)  triple_80: 0.0000 (0.0377)  triple_60: 0.0000 (0.0287)  triple_40: 0.0000 (0.0256)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1200/1724]  eta: 0:34:13  lr: 0.000200  loss: 8.0866 (2.9141)  loss_n_40: 1.6731 (0.6505)  loss_n_60: 1.9272 (0.6599)  loss_n_80: 2.1003 (0.7182)  loss_n_100: 2.3436 (0.7591)  triple_100: 0.0000 (0.0352)  triple_80: 0.0000 (0.0374)  triple_60: 0.0000 (0.0285)  triple_40: 0.0000 (0.0254)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1210/1724]  eta: 0:33:34  lr: 0.000200  loss: 7.7923 (2.9492)  loss_n_40: 1.6558 (0.6578)  loss_n_60: 1.8601 (0.6687)  loss_n_80: 1.9740 (0.7273)  loss_n_100: 2.2168 (0.7699)  triple_100: 0.0000 (0.0349)  triple_80: 0.0000 (0.0371)  triple_60: 0.0000 (0.0283)  triple_40: 0.0000 (0.0252)  time: 3.9198  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1220/1724]  eta: 0:32:55  lr: 0.000200  loss: 7.3263 (2.9815)  loss_n_40: 1.6082 (0.6653)  loss_n_60: 1.7308 (0.6770)  loss_n_80: 1.8643 (0.7356)  loss_n_100: 2.0683 (0.7793)  triple_100: 0.0000 (0.0346)  triple_80: 0.0000 (0.0368)  triple_60: 0.0000 (0.0280)  triple_40: 0.0000 (0.0250)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1230/1724]  eta: 0:32:15  lr: 0.000200  loss: 7.0712 (3.0130)  loss_n_40: 1.5783 (0.6722)  loss_n_60: 1.7200 (0.6850)  loss_n_80: 1.7642 (0.7437)  loss_n_100: 1.9545 (0.7888)  triple_100: 0.0000 (0.0343)  triple_80: 0.0000 (0.0365)  triple_60: 0.0000 (0.0278)  triple_40: 0.0000 (0.0248)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1240/1724]  eta: 0:31:36  lr: 0.000200  loss: 6.9905 (3.0440)  loss_n_40: 1.5612 (0.6790)  loss_n_60: 1.6748 (0.6928)  loss_n_80: 1.7642 (0.7519)  loss_n_100: 1.9663 (0.7979)  triple_100: 0.0000 (0.0340)  triple_80: 0.0000 (0.0362)  triple_60: 0.0000 (0.0276)  triple_40: 0.0000 (0.0246)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1250/1724]  eta: 0:30:57  lr: 0.000200  loss: 6.9138 (3.0722)  loss_n_40: 1.5612 (0.6856)  loss_n_60: 1.6424 (0.6998)  loss_n_80: 1.7201 (0.7590)  loss_n_100: 1.9149 (0.8063)  triple_100: 0.0000 (0.0338)  triple_80: 0.0000 (0.0359)  triple_60: 0.0000 (0.0274)  triple_40: 0.0000 (0.0244)  time: 3.9214  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1260/1724]  eta: 0:30:18  lr: 0.000200  loss: 6.5288 (3.0982)  loss_n_40: 1.5110 (0.6917)  loss_n_60: 1.5627 (0.7063)  loss_n_80: 1.6161 (0.7657)  loss_n_100: 1.8389 (0.8140)  triple_100: 0.0000 (0.0335)  triple_80: 0.0000 (0.0356)  triple_60: 0.0000 (0.0271)  triple_40: 0.0000 (0.0242)  time: 3.9222  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1270/1724]  eta: 0:29:39  lr: 0.000200  loss: 6.0437 (3.1202)  loss_n_40: 1.3696 (0.6969)  loss_n_60: 1.4332 (0.7119)  loss_n_80: 1.5025 (0.7714)  loss_n_100: 1.7131 (0.8204)  triple_100: 0.0000 (0.0332)  triple_80: 0.0000 (0.0354)  triple_60: 0.0000 (0.0269)  triple_40: 0.0000 (0.0240)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1280/1724]  eta: 0:28:59  lr: 0.000200  loss: 5.4978 (3.1381)  loss_n_40: 1.2841 (0.7012)  loss_n_60: 1.2928 (0.7164)  loss_n_80: 1.3415 (0.7759)  loss_n_100: 1.5829 (0.8261)  triple_100: 0.0000 (0.0330)  triple_80: 0.0000 (0.0351)  triple_60: 0.0000 (0.0267)  triple_40: 0.0000 (0.0238)  time: 3.9218  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1290/1724]  eta: 0:28:20  lr: 0.000200  loss: 5.2089 (3.1529)  loss_n_40: 1.2069 (0.7048)  loss_n_60: 1.2130 (0.7201)  loss_n_80: 1.2714 (0.7796)  loss_n_100: 1.4753 (0.8307)  triple_100: 0.0000 (0.0327)  triple_80: 0.0000 (0.0348)  triple_60: 0.0000 (0.0265)  triple_40: 0.0000 (0.0236)  time: 3.9211  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1300/1724]  eta: 0:27:41  lr: 0.000200  loss: 4.7554 (3.1658)  loss_n_40: 1.1131 (0.7078)  loss_n_60: 1.1121 (0.7234)  loss_n_80: 1.1776 (0.7829)  loss_n_100: 1.3526 (0.8349)  triple_100: 0.0000 (0.0325)  triple_80: 0.0000 (0.0346)  triple_60: 0.0000 (0.0263)  triple_40: 0.0000 (0.0234)  time: 3.9194  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [1310/1724]  eta: 0:27:02  lr: 0.000200  loss: 4.9976 (3.1804)  loss_n_40: 1.1297 (0.7113)  loss_n_60: 1.1927 (0.7272)  loss_n_80: 1.2482 (0.7867)  loss_n_100: 1.4068 (0.8394)  triple_100: 0.0000 (0.0322)  triple_80: 0.0000 (0.0343)  triple_60: 0.0000 (0.0261)  triple_40: 0.0000 (0.0232)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1320/1724]  eta: 0:26:23  lr: 0.000200  loss: 4.9976 (3.1930)  loss_n_40: 1.1297 (0.7143)  loss_n_60: 1.1927 (0.7304)  loss_n_80: 1.2321 (0.7899)  loss_n_100: 1.3745 (0.8434)  triple_100: 0.0000 (0.0320)  triple_80: 0.0000 (0.0340)  triple_60: 0.0000 (0.0259)  triple_40: 0.0000 (0.0231)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1330/1724]  eta: 0:25:44  lr: 0.000200  loss: 4.7040 (3.2053)  loss_n_40: 1.0202 (0.7165)  loss_n_60: 1.1220 (0.7331)  loss_n_80: 1.2104 (0.7931)  loss_n_100: 1.3650 (0.8480)  triple_100: 0.0000 (0.0317)  triple_80: 0.0000 (0.0338)  triple_60: 0.0000 (0.0257)  triple_40: 0.0000 (0.0233)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1340/1724]  eta: 0:25:04  lr: 0.000200  loss: 4.8072 (3.2205)  loss_n_40: 0.9854 (0.7192)  loss_n_60: 1.1149 (0.7363)  loss_n_80: 1.2294 (0.7968)  loss_n_100: 1.4802 (0.8527)  triple_100: 0.0000 (0.0315)  triple_80: 0.0000 (0.0341)  triple_60: 0.0000 (0.0268)  triple_40: 0.0000 (0.0231)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1350/1724]  eta: 0:24:25  lr: 0.000200  loss: 4.8072 (3.2320)  loss_n_40: 1.0489 (0.7217)  loss_n_60: 1.1149 (0.7391)  loss_n_80: 1.2430 (0.8000)  loss_n_100: 1.3830 (0.8565)  triple_100: 0.0000 (0.0313)  triple_80: 0.0000 (0.0338)  triple_60: 0.0000 (0.0266)  triple_40: 0.0000 (0.0230)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1360/1724]  eta: 0:23:46  lr: 0.000200  loss: 4.5334 (3.2437)  loss_n_40: 1.0164 (0.7241)  loss_n_60: 1.0604 (0.7417)  loss_n_80: 1.1533 (0.8028)  loss_n_100: 1.2834 (0.8599)  triple_100: 0.0000 (0.0310)  triple_80: 0.0000 (0.0336)  triple_60: 0.0000 (0.0264)  triple_40: 0.0000 (0.0243)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1370/1724]  eta: 0:23:07  lr: 0.000200  loss: 4.2092 (3.2496)  loss_n_40: 0.9621 (0.7255)  loss_n_60: 1.0100 (0.7433)  loss_n_80: 1.0906 (0.8045)  loss_n_100: 1.2093 (0.8618)  triple_100: 0.0000 (0.0308)  triple_80: 0.0000 (0.0333)  triple_60: 0.0000 (0.0262)  triple_40: 0.0000 (0.0241)  time: 3.9206  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1380/1724]  eta: 0:22:28  lr: 0.000200  loss: 3.8411 (3.2541)  loss_n_40: 0.8614 (0.7265)  loss_n_60: 0.8999 (0.7446)  loss_n_80: 1.0129 (0.8060)  loss_n_100: 1.0859 (0.8632)  triple_100: 0.0000 (0.0306)  triple_80: 0.0000 (0.0333)  triple_60: 0.0000 (0.0260)  triple_40: 0.0000 (0.0239)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1390/1724]  eta: 0:21:48  lr: 0.000200  loss: 3.6529 (3.2567)  loss_n_40: 0.8349 (0.7272)  loss_n_60: 0.8729 (0.7454)  loss_n_80: 0.9630 (0.8069)  loss_n_100: 1.0068 (0.8641)  triple_100: 0.0000 (0.0304)  triple_80: 0.0000 (0.0331)  triple_60: 0.0000 (0.0259)  triple_40: 0.0000 (0.0237)  time: 3.9213  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1400/1724]  eta: 0:21:09  lr: 0.000200  loss: 3.3928 (3.2588)  loss_n_40: 0.7814 (0.7277)  loss_n_60: 0.8206 (0.7459)  loss_n_80: 0.8976 (0.8076)  loss_n_100: 0.9267 (0.8647)  triple_100: 0.0000 (0.0302)  triple_80: 0.0000 (0.0335)  triple_60: 0.0000 (0.0257)  triple_40: 0.0000 (0.0236)  time: 3.9205  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1410/1724]  eta: 0:20:30  lr: 0.000200  loss: 3.3786 (3.2602)  loss_n_40: 0.7685 (0.7282)  loss_n_60: 0.8068 (0.7464)  loss_n_80: 0.8677 (0.8081)  loss_n_100: 0.9238 (0.8653)  triple_100: 0.0000 (0.0299)  triple_80: 0.0000 (0.0333)  triple_60: 0.0000 (0.0255)  triple_40: 0.0000 (0.0234)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1420/1724]  eta: 0:19:51  lr: 0.000200  loss: 3.5055 (3.2659)  loss_n_40: 0.7732 (0.7290)  loss_n_60: 0.8187 (0.7473)  loss_n_80: 0.8981 (0.8095)  loss_n_100: 0.9686 (0.8669)  triple_100: 0.0000 (0.0315)  triple_80: 0.0000 (0.0332)  triple_60: 0.0000 (0.0253)  triple_40: 0.0000 (0.0232)  time: 3.9198  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1430/1724]  eta: 0:19:12  lr: 0.000200  loss: 4.1954 (3.2742)  loss_n_40: 0.8303 (0.7305)  loss_n_60: 0.8860 (0.7493)  loss_n_80: 1.0609 (0.8121)  loss_n_100: 1.1445 (0.8698)  triple_100: 0.0000 (0.0313)  triple_80: 0.0000 (0.0330)  triple_60: 0.0000 (0.0251)  triple_40: 0.0000 (0.0231)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1440/1724]  eta: 0:18:32  lr: 0.000200  loss: 4.2863 (3.2808)  loss_n_40: 0.9671 (0.7322)  loss_n_60: 1.0126 (0.7510)  loss_n_80: 1.1112 (0.8140)  loss_n_100: 1.1667 (0.8718)  triple_100: 0.0000 (0.0311)  triple_80: 0.0000 (0.0328)  triple_60: 0.0000 (0.0250)  triple_40: 0.0000 (0.0229)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1450/1724]  eta: 0:17:53  lr: 0.000200  loss: 3.7142 (3.2817)  loss_n_40: 0.8334 (0.7327)  loss_n_60: 0.8494 (0.7513)  loss_n_80: 0.9722 (0.8145)  loss_n_100: 1.0121 (0.8721)  triple_100: 0.0000 (0.0309)  triple_80: 0.0000 (0.0325)  triple_60: 0.0000 (0.0248)  triple_40: 0.0000 (0.0228)  time: 3.9211  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1460/1724]  eta: 0:17:14  lr: 0.000200  loss: 3.3223 (3.2842)  loss_n_40: 0.8068 (0.7330)  loss_n_60: 0.7941 (0.7519)  loss_n_80: 0.8676 (0.8155)  loss_n_100: 0.8706 (0.8731)  triple_100: 0.0000 (0.0311)  triple_80: 0.0000 (0.0323)  triple_60: 0.0000 (0.0246)  triple_40: 0.0000 (0.0226)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1470/1724]  eta: 0:16:35  lr: 0.000200  loss: 3.3122 (3.2845)  loss_n_40: 0.7126 (0.7329)  loss_n_60: 0.7616 (0.7520)  loss_n_80: 0.8593 (0.8160)  loss_n_100: 0.9473 (0.8737)  triple_100: 0.0000 (0.0309)  triple_80: 0.0000 (0.0321)  triple_60: 0.0000 (0.0244)  triple_40: 0.0000 (0.0224)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1480/1724]  eta: 0:15:56  lr: 0.000200  loss: 3.2282 (3.2849)  loss_n_40: 0.7147 (0.7332)  loss_n_60: 0.7496 (0.7522)  loss_n_80: 0.8494 (0.8163)  loss_n_100: 0.9280 (0.8739)  triple_100: 0.0000 (0.0307)  triple_80: 0.0000 (0.0319)  triple_60: 0.0000 (0.0243)  triple_40: 0.0000 (0.0223)  time: 3.9206  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1490/1724]  eta: 0:15:17  lr: 0.000200  loss: 3.0576 (3.2830)  loss_n_40: 0.7282 (0.7331)  loss_n_60: 0.7264 (0.7520)  loss_n_80: 0.7904 (0.8160)  loss_n_100: 0.8378 (0.8735)  triple_100: 0.0000 (0.0305)  triple_80: 0.0000 (0.0317)  triple_60: 0.0000 (0.0241)  triple_40: 0.0000 (0.0221)  time: 3.9205  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1500/1724]  eta: 0:14:37  lr: 0.000200  loss: 2.8378 (3.2812)  loss_n_40: 0.7119 (0.7331)  loss_n_60: 0.6912 (0.7517)  loss_n_80: 0.7190 (0.8157)  loss_n_100: 0.7654 (0.8729)  triple_100: 0.0000 (0.0303)  triple_80: 0.0000 (0.0315)  triple_60: 0.0000 (0.0240)  triple_40: 0.0000 (0.0220)  time: 3.9196  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1510/1724]  eta: 0:13:58  lr: 0.000200  loss: 2.6201 (3.2763)  loss_n_40: 0.6171 (0.7323)  loss_n_60: 0.6249 (0.7508)  loss_n_80: 0.6760 (0.8146)  loss_n_100: 0.6990 (0.8716)  triple_100: 0.0000 (0.0301)  triple_80: 0.0000 (0.0312)  triple_60: 0.0000 (0.0238)  triple_40: 0.0000 (0.0218)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1520/1724]  eta: 0:13:19  lr: 0.000200  loss: 2.5012 (3.2711)  loss_n_40: 0.5868 (0.7315)  loss_n_60: 0.6040 (0.7498)  loss_n_80: 0.6385 (0.8134)  loss_n_100: 0.6630 (0.8702)  triple_100: 0.0000 (0.0299)  triple_80: 0.0000 (0.0310)  triple_60: 0.0000 (0.0236)  triple_40: 0.0000 (0.0217)  time: 3.9205  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1530/1724]  eta: 0:12:40  lr: 0.000200  loss: 2.1897 (3.2638)  loss_n_40: 0.5296 (0.7300)  loss_n_60: 0.5208 (0.7482)  loss_n_80: 0.5548 (0.8117)  loss_n_100: 0.5943 (0.8683)  triple_100: 0.0000 (0.0297)  triple_80: 0.0000 (0.0308)  triple_60: 0.0000 (0.0235)  triple_40: 0.0000 (0.0216)  time: 3.9195  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1540/1724]  eta: 0:12:01  lr: 0.000200  loss: 2.3468 (3.2777)  loss_n_40: 0.5515 (0.7299)  loss_n_60: 0.5678 (0.7484)  loss_n_80: 0.6076 (0.8125)  loss_n_100: 0.6455 (0.8695)  triple_100: 0.0000 (0.0389)  triple_80: 0.0000 (0.0333)  triple_60: 0.0000 (0.0237)  triple_40: 0.0000 (0.0214)  time: 3.9159  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:14]  [1550/1724]  eta: 0:11:21  lr: 0.000200  loss: 4.6124 (3.2888)  loss_n_40: 0.9171 (0.7318)  loss_n_60: 0.9812 (0.7508)  loss_n_80: 1.2549 (0.8162)  loss_n_100: 1.3280 (0.8735)  triple_100: 0.0000 (0.0386)  triple_80: 0.0000 (0.0330)  triple_60: 0.0000 (0.0235)  triple_40: 0.0000 (0.0213)  time: 3.9155  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1560/1724]  eta: 0:10:42  lr: 0.000200  loss: 4.4622 (3.2947)  loss_n_40: 0.9752 (0.7333)  loss_n_60: 0.9838 (0.7520)  loss_n_80: 1.1722 (0.8180)  loss_n_100: 1.3224 (0.8756)  triple_100: 0.0000 (0.0384)  triple_80: 0.0000 (0.0328)  triple_60: 0.0000 (0.0234)  triple_40: 0.0000 (0.0211)  time: 3.9177  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1570/1724]  eta: 0:10:03  lr: 0.000200  loss: 3.8285 (3.2970)  loss_n_40: 0.8786 (0.7339)  loss_n_60: 0.8613 (0.7524)  loss_n_80: 1.0360 (0.8190)  loss_n_100: 1.1244 (0.8767)  triple_100: 0.0000 (0.0381)  triple_80: 0.0000 (0.0326)  triple_60: 0.0000 (0.0232)  triple_40: 0.0000 (0.0210)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1580/1724]  eta: 0:09:24  lr: 0.000200  loss: 3.4673 (3.2972)  loss_n_40: 0.7986 (0.7341)  loss_n_60: 0.7909 (0.7525)  loss_n_80: 0.9225 (0.8194)  loss_n_100: 0.9682 (0.8769)  triple_100: 0.0000 (0.0379)  triple_80: 0.0000 (0.0324)  triple_60: 0.0000 (0.0231)  triple_40: 0.0000 (0.0209)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1590/1724]  eta: 0:08:45  lr: 0.000200  loss: 3.1742 (3.2949)  loss_n_40: 0.7212 (0.7335)  loss_n_60: 0.7131 (0.7520)  loss_n_80: 0.8194 (0.8191)  loss_n_100: 0.8705 (0.8767)  triple_100: 0.0000 (0.0377)  triple_80: 0.0000 (0.0322)  triple_60: 0.0000 (0.0230)  triple_40: 0.0000 (0.0208)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1600/1724]  eta: 0:08:05  lr: 0.000200  loss: 2.8349 (3.2923)  loss_n_40: 0.6642 (0.7332)  loss_n_60: 0.6598 (0.7515)  loss_n_80: 0.7475 (0.8187)  loss_n_100: 0.7958 (0.8760)  triple_100: 0.0000 (0.0374)  triple_80: 0.0000 (0.0320)  triple_60: 0.0000 (0.0228)  triple_40: 0.0000 (0.0206)  time: 3.9181  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1610/1724]  eta: 0:07:26  lr: 0.000200  loss: 2.7787 (3.2889)  loss_n_40: 0.6053 (0.7326)  loss_n_60: 0.6565 (0.7508)  loss_n_80: 0.7199 (0.8181)  loss_n_100: 0.7445 (0.8752)  triple_100: 0.0000 (0.0372)  triple_80: 0.0000 (0.0318)  triple_60: 0.0000 (0.0227)  triple_40: 0.0000 (0.0205)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1620/1724]  eta: 0:06:47  lr: 0.000200  loss: 2.6137 (3.2844)  loss_n_40: 0.5903 (0.7318)  loss_n_60: 0.5950 (0.7499)  loss_n_80: 0.6880 (0.8172)  loss_n_100: 0.6912 (0.8740)  triple_100: 0.0000 (0.0370)  triple_80: 0.0000 (0.0316)  triple_60: 0.0000 (0.0225)  triple_40: 0.0000 (0.0204)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1630/1724]  eta: 0:06:08  lr: 0.000200  loss: 2.4225 (3.2793)  loss_n_40: 0.5744 (0.7310)  loss_n_60: 0.5751 (0.7489)  loss_n_80: 0.6335 (0.8160)  loss_n_100: 0.6467 (0.8725)  triple_100: 0.0000 (0.0367)  triple_80: 0.0000 (0.0314)  triple_60: 0.0000 (0.0224)  triple_40: 0.0000 (0.0202)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1640/1724]  eta: 0:05:29  lr: 0.000200  loss: 2.2097 (3.2729)  loss_n_40: 0.5039 (0.7297)  loss_n_60: 0.5360 (0.7477)  loss_n_80: 0.5744 (0.8146)  loss_n_100: 0.5770 (0.8709)  triple_100: 0.0000 (0.0365)  triple_80: 0.0000 (0.0312)  triple_60: 0.0000 (0.0223)  triple_40: 0.0000 (0.0201)  time: 3.9198  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1650/1724]  eta: 0:04:50  lr: 0.000200  loss: 2.2257 (3.2664)  loss_n_40: 0.4967 (0.7285)  loss_n_60: 0.5236 (0.7463)  loss_n_80: 0.5833 (0.8131)  loss_n_100: 0.5871 (0.8691)  triple_100: 0.0000 (0.0363)  triple_80: 0.0000 (0.0310)  triple_60: 0.0000 (0.0221)  triple_40: 0.0000 (0.0200)  time: 3.9201  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1660/1724]  eta: 0:04:10  lr: 0.000200  loss: 2.0208 (3.2589)  loss_n_40: 0.4689 (0.7270)  loss_n_60: 0.4768 (0.7447)  loss_n_80: 0.5290 (0.8113)  loss_n_100: 0.5428 (0.8670)  triple_100: 0.0000 (0.0361)  triple_80: 0.0000 (0.0309)  triple_60: 0.0000 (0.0220)  triple_40: 0.0000 (0.0199)  time: 3.9196  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1670/1724]  eta: 0:03:31  lr: 0.000200  loss: 2.0208 (3.2523)  loss_n_40: 0.4625 (0.7256)  loss_n_60: 0.4768 (0.7432)  loss_n_80: 0.5280 (0.8097)  loss_n_100: 0.5351 (0.8651)  triple_100: 0.0000 (0.0359)  triple_80: 0.0000 (0.0309)  triple_60: 0.0000 (0.0219)  triple_40: 0.0000 (0.0201)  time: 3.9193  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1680/1724]  eta: 0:02:52  lr: 0.000200  loss: 2.5430 (3.2492)  loss_n_40: 0.4741 (0.7243)  loss_n_60: 0.5533 (0.7426)  loss_n_80: 0.6258 (0.8092)  loss_n_100: 0.6293 (0.8650)  triple_100: 0.0000 (0.0356)  triple_80: 0.0000 (0.0307)  triple_60: 0.0000 (0.0217)  triple_40: 0.0000 (0.0199)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1690/1724]  eta: 0:02:13  lr: 0.000200  loss: 2.6342 (3.2503)  loss_n_40: 0.4877 (0.7231)  loss_n_60: 0.6403 (0.7423)  loss_n_80: 0.7002 (0.8086)  loss_n_100: 0.7815 (0.8643)  triple_100: 0.0000 (0.0354)  triple_80: 0.0000 (0.0336)  triple_60: 0.0000 (0.0231)  triple_40: 0.0000 (0.0198)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1700/1724]  eta: 0:01:34  lr: 0.000200  loss: 3.6048 (3.2742)  loss_n_40: 0.6708 (0.7238)  loss_n_60: 0.8262 (0.7451)  loss_n_80: 0.8823 (0.8132)  loss_n_100: 0.9120 (0.8703)  triple_100: 0.0000 (0.0362)  triple_80: 0.0000 (0.0371)  triple_60: 0.0000 (0.0261)  triple_40: 0.0000 (0.0223)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:14]  [1710/1724]  eta: 0:00:54  lr: 0.000200  loss: 6.3474 (3.2939)  loss_n_40: 0.8669 (0.7250)  loss_n_60: 1.3030 (0.7488)  loss_n_80: 1.8277 (0.8194)  loss_n_100: 2.2833 (0.8788)  triple_100: 0.0000 (0.0360)  triple_80: 0.0000 (0.0375)  triple_60: 0.0000 (0.0260)  triple_40: 0.0000 (0.0222)  time: 3.9171  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1720/1724]  eta: 0:00:15  lr: 0.000200  loss: 6.0712 (3.3060)  loss_n_40: 0.8974 (0.7262)  loss_n_60: 1.2689 (0.7512)  loss_n_80: 1.6804 (0.8234)  loss_n_100: 2.1652 (0.8842)  triple_100: 0.0000 (0.0358)  triple_80: 0.0000 (0.0373)  triple_60: 0.0000 (0.0258)  triple_40: 0.0000 (0.0221)  time: 3.9165  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14]  [1723/1724]  eta: 0:00:03  lr: 0.000200  loss: 5.4780 (3.3086)  loss_n_40: 0.8974 (0.7263)  loss_n_60: 1.1959 (0.7517)  loss_n_80: 1.5592 (0.8243)  loss_n_100: 1.8049 (0.8855)  triple_100: 0.0000 (0.0358)  triple_80: 0.0000 (0.0372)  triple_60: 0.0000 (0.0258)  triple_40: 0.0000 (0.0221)  time: 3.9170  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:14] Total time: 1:52:36 (3.9191 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 5.4780 (3.3086)  loss_n_40: 0.8974 (0.7263)  loss_n_60: 1.1959 (0.7517)  loss_n_80: 1.5592 (0.8243)  loss_n_100: 1.8049 (0.8855)  triple_100: 0.0000 (0.0358)  triple_80: 0.0000 (0.0372)  triple_60: 0.0000 (0.0258)  triple_40: 0.0000 (0.0221)\n",
      "Valid: [epoch:14]  [  0/845]  eta: 0:09:47  loss: 3.8825 (3.8825)  loss_n_40: 0.7315 (0.7315)  loss_n_60: 0.9611 (0.9611)  loss_n_80: 1.0389 (1.0389)  loss_n_100: 1.1510 (1.1510)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.6953  data: 0.3615  max mem: 46473\n",
      "Valid: [epoch:14]  [ 10/845]  eta: 0:05:06  loss: 3.8825 (4.1526)  loss_n_40: 0.7426 (0.8269)  loss_n_60: 0.8778 (0.9181)  loss_n_80: 1.0575 (1.1084)  loss_n_100: 1.2730 (1.2991)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3667  data: 0.0330  max mem: 46473\n",
      "Valid: [epoch:14]  [ 20/845]  eta: 0:04:49  loss: 4.1759 (4.4002)  loss_n_40: 0.7575 (0.8561)  loss_n_60: 0.8778 (0.9683)  loss_n_80: 1.1747 (1.1845)  loss_n_100: 1.3773 (1.3913)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [ 30/845]  eta: 0:04:41  loss: 4.3240 (4.3335)  loss_n_40: 0.7949 (0.8286)  loss_n_60: 0.9051 (0.9432)  loss_n_80: 1.2019 (1.1689)  loss_n_100: 1.4466 (1.3928)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:14]  [ 40/845]  eta: 0:04:35  loss: 4.3240 (4.4342)  loss_n_40: 0.8055 (0.8659)  loss_n_60: 0.9530 (0.9833)  loss_n_80: 1.2019 (1.1860)  loss_n_100: 1.4466 (1.3990)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [ 50/845]  eta: 0:04:31  loss: 4.3656 (4.4440)  loss_n_40: 0.8002 (0.8717)  loss_n_60: 0.9758 (0.9870)  loss_n_80: 1.1288 (1.1860)  loss_n_100: 1.4206 (1.3992)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [ 60/845]  eta: 0:04:26  loss: 4.1988 (4.4442)  loss_n_40: 0.7768 (0.8569)  loss_n_60: 0.9338 (0.9757)  loss_n_80: 1.1259 (1.1936)  loss_n_100: 1.4206 (1.4179)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [ 70/845]  eta: 0:04:22  loss: 4.0842 (4.4090)  loss_n_40: 0.7340 (0.8472)  loss_n_60: 0.8988 (0.9685)  loss_n_80: 1.1089 (1.1835)  loss_n_100: 1.3701 (1.4099)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [ 80/845]  eta: 0:04:18  loss: 4.0076 (4.4114)  loss_n_40: 0.7275 (0.8495)  loss_n_60: 0.8988 (0.9725)  loss_n_80: 1.0758 (1.1831)  loss_n_100: 1.2998 (1.4064)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [ 90/845]  eta: 0:04:15  loss: 4.0771 (4.4248)  loss_n_40: 0.7344 (0.8416)  loss_n_60: 0.9030 (0.9736)  loss_n_80: 1.1242 (1.1923)  loss_n_100: 1.3357 (1.4174)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [100/845]  eta: 0:04:11  loss: 4.2110 (4.4572)  loss_n_40: 0.7709 (0.8463)  loss_n_60: 0.9136 (0.9824)  loss_n_80: 1.1457 (1.2011)  loss_n_100: 1.3864 (1.4273)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [110/845]  eta: 0:04:07  loss: 4.2110 (4.4550)  loss_n_40: 0.8109 (0.8413)  loss_n_60: 0.9136 (0.9798)  loss_n_80: 1.1527 (1.2023)  loss_n_100: 1.4040 (1.4317)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [120/845]  eta: 0:04:04  loss: 4.0539 (4.4348)  loss_n_40: 0.7380 (0.8399)  loss_n_60: 0.8669 (0.9744)  loss_n_80: 1.1009 (1.1963)  loss_n_100: 1.3678 (1.4242)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [130/845]  eta: 0:04:00  loss: 4.0382 (4.4360)  loss_n_40: 0.7155 (0.8399)  loss_n_60: 0.8679 (0.9745)  loss_n_80: 1.0871 (1.1974)  loss_n_100: 1.3116 (1.4242)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [140/845]  eta: 0:03:57  loss: 4.7021 (4.4619)  loss_n_40: 0.7910 (0.8436)  loss_n_60: 0.9916 (0.9798)  loss_n_80: 1.2459 (1.2054)  loss_n_100: 1.3929 (1.4331)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [150/845]  eta: 0:03:53  loss: 4.7021 (4.4716)  loss_n_40: 0.7831 (0.8440)  loss_n_60: 0.9916 (0.9817)  loss_n_80: 1.2674 (1.2093)  loss_n_100: 1.4365 (1.4367)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [160/845]  eta: 0:03:50  loss: 4.0610 (4.4586)  loss_n_40: 0.7462 (0.8389)  loss_n_60: 0.9249 (0.9790)  loss_n_80: 1.1272 (1.2070)  loss_n_100: 1.3410 (1.4338)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [170/845]  eta: 0:03:46  loss: 4.0660 (4.4839)  loss_n_40: 0.7696 (0.8452)  loss_n_60: 0.9664 (0.9863)  loss_n_80: 1.1353 (1.2132)  loss_n_100: 1.3455 (1.4392)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [180/845]  eta: 0:03:43  loss: 4.2917 (4.4708)  loss_n_40: 0.8316 (0.8397)  loss_n_60: 0.9676 (0.9819)  loss_n_80: 1.1235 (1.2113)  loss_n_100: 1.3749 (1.4379)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [190/845]  eta: 0:03:40  loss: 3.9904 (4.4689)  loss_n_40: 0.7297 (0.8397)  loss_n_60: 0.8962 (0.9813)  loss_n_80: 1.1205 (1.2110)  loss_n_100: 1.3640 (1.4368)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [200/845]  eta: 0:03:36  loss: 4.2343 (4.4654)  loss_n_40: 0.7345 (0.8359)  loss_n_60: 0.9025 (0.9797)  loss_n_80: 1.1603 (1.2112)  loss_n_100: 1.4103 (1.4387)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [210/845]  eta: 0:03:33  loss: 4.0926 (4.4668)  loss_n_40: 0.7964 (0.8381)  loss_n_60: 0.9207 (0.9802)  loss_n_80: 1.1603 (1.2108)  loss_n_100: 1.3750 (1.4377)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [220/845]  eta: 0:03:29  loss: 4.0828 (4.4616)  loss_n_40: 0.8134 (0.8356)  loss_n_60: 0.9047 (0.9789)  loss_n_80: 1.2056 (1.2093)  loss_n_100: 1.3512 (1.4378)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [230/845]  eta: 0:03:26  loss: 4.5226 (4.4722)  loss_n_40: 0.7783 (0.8380)  loss_n_60: 0.9363 (0.9817)  loss_n_80: 1.2817 (1.2126)  loss_n_100: 1.4373 (1.4399)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [240/845]  eta: 0:03:23  loss: 4.2792 (4.4761)  loss_n_40: 0.7706 (0.8361)  loss_n_60: 0.9363 (0.9819)  loss_n_80: 1.1789 (1.2147)  loss_n_100: 1.4078 (1.4434)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [250/845]  eta: 0:03:19  loss: 4.1297 (4.4716)  loss_n_40: 0.7538 (0.8341)  loss_n_60: 0.8949 (0.9798)  loss_n_80: 1.1517 (1.2141)  loss_n_100: 1.4078 (1.4436)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [260/845]  eta: 0:03:16  loss: 4.0564 (4.4728)  loss_n_40: 0.7538 (0.8376)  loss_n_60: 0.8933 (0.9811)  loss_n_80: 1.0976 (1.2125)  loss_n_100: 1.3548 (1.4416)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [270/845]  eta: 0:03:12  loss: 4.0076 (4.4624)  loss_n_40: 0.7262 (0.8363)  loss_n_60: 0.8689 (0.9792)  loss_n_80: 1.0842 (1.2089)  loss_n_100: 1.3548 (1.4380)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [280/845]  eta: 0:03:09  loss: 4.0020 (4.4548)  loss_n_40: 0.7383 (0.8332)  loss_n_60: 0.8931 (0.9777)  loss_n_80: 1.1262 (1.2075)  loss_n_100: 1.3819 (1.4364)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:14]  [290/845]  eta: 0:03:06  loss: 4.0937 (4.4500)  loss_n_40: 0.7711 (0.8322)  loss_n_60: 0.8817 (0.9760)  loss_n_80: 1.1274 (1.2057)  loss_n_100: 1.3916 (1.4361)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [300/845]  eta: 0:03:02  loss: 4.0937 (4.4472)  loss_n_40: 0.7970 (0.8314)  loss_n_60: 0.9292 (0.9760)  loss_n_80: 1.1274 (1.2044)  loss_n_100: 1.3916 (1.4354)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [310/845]  eta: 0:02:59  loss: 4.5635 (4.4640)  loss_n_40: 0.7970 (0.8373)  loss_n_60: 0.9634 (0.9805)  loss_n_80: 1.2283 (1.2079)  loss_n_100: 1.4880 (1.4382)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [320/845]  eta: 0:02:56  loss: 4.4009 (4.4578)  loss_n_40: 0.7729 (0.8350)  loss_n_60: 0.9357 (0.9789)  loss_n_80: 1.2219 (1.2068)  loss_n_100: 1.4179 (1.4371)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [330/845]  eta: 0:02:52  loss: 4.1307 (4.4629)  loss_n_40: 0.7298 (0.8355)  loss_n_60: 0.8266 (0.9794)  loss_n_80: 1.1475 (1.2091)  loss_n_100: 1.4055 (1.4389)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [340/845]  eta: 0:02:49  loss: 4.2026 (4.4689)  loss_n_40: 0.7720 (0.8359)  loss_n_60: 0.9623 (0.9808)  loss_n_80: 1.2223 (1.2113)  loss_n_100: 1.4815 (1.4410)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [350/845]  eta: 0:02:45  loss: 4.2845 (4.4606)  loss_n_40: 0.7752 (0.8334)  loss_n_60: 0.9623 (0.9790)  loss_n_80: 1.2009 (1.2093)  loss_n_100: 1.4278 (1.4390)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [360/845]  eta: 0:02:42  loss: 4.0516 (4.4602)  loss_n_40: 0.7507 (0.8339)  loss_n_60: 0.8795 (0.9789)  loss_n_80: 1.1215 (1.2088)  loss_n_100: 1.3715 (1.4386)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [370/845]  eta: 0:02:39  loss: 4.1282 (4.4618)  loss_n_40: 0.7572 (0.8326)  loss_n_60: 0.8519 (0.9779)  loss_n_80: 1.1641 (1.2102)  loss_n_100: 1.4360 (1.4411)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [380/845]  eta: 0:02:35  loss: 4.1282 (4.4619)  loss_n_40: 0.7349 (0.8312)  loss_n_60: 0.8914 (0.9774)  loss_n_80: 1.1641 (1.2108)  loss_n_100: 1.3985 (1.4425)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [390/845]  eta: 0:02:32  loss: 4.0328 (4.4542)  loss_n_40: 0.7277 (0.8283)  loss_n_60: 0.8835 (0.9755)  loss_n_80: 1.0958 (1.2095)  loss_n_100: 1.3596 (1.4409)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [400/845]  eta: 0:02:29  loss: 4.1160 (4.4589)  loss_n_40: 0.7400 (0.8306)  loss_n_60: 0.8959 (0.9770)  loss_n_80: 1.1536 (1.2103)  loss_n_100: 1.3450 (1.4410)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [410/845]  eta: 0:02:25  loss: 4.2646 (4.4553)  loss_n_40: 0.7819 (0.8302)  loss_n_60: 0.8959 (0.9759)  loss_n_80: 1.1575 (1.2093)  loss_n_100: 1.4151 (1.4400)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [420/845]  eta: 0:02:22  loss: 4.2646 (4.4599)  loss_n_40: 0.7884 (0.8335)  loss_n_60: 0.9121 (0.9779)  loss_n_80: 1.2063 (1.2094)  loss_n_100: 1.3540 (1.4391)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [430/845]  eta: 0:02:19  loss: 4.0066 (4.4501)  loss_n_40: 0.7503 (0.8307)  loss_n_60: 0.8550 (0.9752)  loss_n_80: 1.0739 (1.2072)  loss_n_100: 1.2940 (1.4370)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [440/845]  eta: 0:02:15  loss: 4.0532 (4.4515)  loss_n_40: 0.7119 (0.8311)  loss_n_60: 0.8581 (0.9756)  loss_n_80: 1.1046 (1.2076)  loss_n_100: 1.3342 (1.4372)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [450/845]  eta: 0:02:12  loss: 4.0532 (4.4494)  loss_n_40: 0.7283 (0.8312)  loss_n_60: 0.8690 (0.9749)  loss_n_80: 1.1070 (1.2069)  loss_n_100: 1.3473 (1.4365)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [460/845]  eta: 0:02:08  loss: 4.0474 (4.4440)  loss_n_40: 0.7283 (0.8294)  loss_n_60: 0.8552 (0.9738)  loss_n_80: 1.0919 (1.2056)  loss_n_100: 1.3471 (1.4352)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [470/845]  eta: 0:02:05  loss: 4.1371 (4.4439)  loss_n_40: 0.7541 (0.8283)  loss_n_60: 0.9030 (0.9735)  loss_n_80: 1.0994 (1.2055)  loss_n_100: 1.4265 (1.4366)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [480/845]  eta: 0:02:02  loss: 4.4735 (4.4486)  loss_n_40: 0.8050 (0.8282)  loss_n_60: 1.0289 (0.9749)  loss_n_80: 1.2284 (1.2069)  loss_n_100: 1.4717 (1.4385)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [490/845]  eta: 0:01:58  loss: 4.2568 (4.4414)  loss_n_40: 0.7929 (0.8272)  loss_n_60: 1.0289 (0.9736)  loss_n_80: 1.1209 (1.2046)  loss_n_100: 1.3563 (1.4360)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [500/845]  eta: 0:01:55  loss: 4.0308 (4.4429)  loss_n_40: 0.7719 (0.8265)  loss_n_60: 0.8679 (0.9734)  loss_n_80: 1.0862 (1.2056)  loss_n_100: 1.3396 (1.4375)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [510/845]  eta: 0:01:52  loss: 4.2609 (4.4452)  loss_n_40: 0.7891 (0.8258)  loss_n_60: 0.9363 (0.9737)  loss_n_80: 1.1647 (1.2068)  loss_n_100: 1.4571 (1.4389)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [520/845]  eta: 0:01:48  loss: 4.2609 (4.4435)  loss_n_40: 0.7701 (0.8255)  loss_n_60: 0.9128 (0.9731)  loss_n_80: 1.1647 (1.2064)  loss_n_100: 1.4134 (1.4385)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [530/845]  eta: 0:01:45  loss: 4.2695 (4.4446)  loss_n_40: 0.7667 (0.8258)  loss_n_60: 0.8726 (0.9729)  loss_n_80: 1.1928 (1.2066)  loss_n_100: 1.4134 (1.4394)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:14]  [540/845]  eta: 0:01:42  loss: 4.4335 (4.4460)  loss_n_40: 0.7860 (0.8262)  loss_n_60: 0.9911 (0.9730)  loss_n_80: 1.1928 (1.2070)  loss_n_100: 1.4637 (1.4398)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [550/845]  eta: 0:01:38  loss: 4.3689 (4.4468)  loss_n_40: 0.7792 (0.8254)  loss_n_60: 0.9605 (0.9730)  loss_n_80: 1.1389 (1.2076)  loss_n_100: 1.4012 (1.4409)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [560/845]  eta: 0:01:35  loss: 4.1848 (4.4468)  loss_n_40: 0.7790 (0.8249)  loss_n_60: 0.9342 (0.9728)  loss_n_80: 1.1389 (1.2079)  loss_n_100: 1.4012 (1.4413)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [570/845]  eta: 0:01:32  loss: 4.0188 (4.4425)  loss_n_40: 0.7206 (0.8232)  loss_n_60: 0.8760 (0.9713)  loss_n_80: 1.1074 (1.2073)  loss_n_100: 1.3861 (1.4408)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [580/845]  eta: 0:01:28  loss: 4.0352 (4.4404)  loss_n_40: 0.7432 (0.8240)  loss_n_60: 0.8790 (0.9716)  loss_n_80: 1.1334 (1.2061)  loss_n_100: 1.3447 (1.4387)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [590/845]  eta: 0:01:25  loss: 4.2448 (4.4466)  loss_n_40: 0.8149 (0.8261)  loss_n_60: 0.9961 (0.9741)  loss_n_80: 1.1775 (1.2073)  loss_n_100: 1.3590 (1.4392)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [600/845]  eta: 0:01:22  loss: 4.2448 (4.4462)  loss_n_40: 0.8560 (0.8261)  loss_n_60: 0.9973 (0.9742)  loss_n_80: 1.1216 (1.2071)  loss_n_100: 1.3392 (1.4388)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [610/845]  eta: 0:01:18  loss: 4.0572 (4.4455)  loss_n_40: 0.7683 (0.8261)  loss_n_60: 0.8677 (0.9741)  loss_n_80: 1.1106 (1.2070)  loss_n_100: 1.3413 (1.4384)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [620/845]  eta: 0:01:15  loss: 4.0572 (4.4494)  loss_n_40: 0.7683 (0.8274)  loss_n_60: 0.8677 (0.9753)  loss_n_80: 1.1106 (1.2076)  loss_n_100: 1.3442 (1.4392)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [630/845]  eta: 0:01:11  loss: 4.0337 (4.4486)  loss_n_40: 0.7597 (0.8295)  loss_n_60: 0.8726 (0.9754)  loss_n_80: 1.0920 (1.2065)  loss_n_100: 1.2970 (1.4372)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [640/845]  eta: 0:01:08  loss: 4.0251 (4.4472)  loss_n_40: 0.7408 (0.8289)  loss_n_60: 0.8555 (0.9746)  loss_n_80: 1.1051 (1.2062)  loss_n_100: 1.3196 (1.4375)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [650/845]  eta: 0:01:05  loss: 4.1002 (4.4494)  loss_n_40: 0.7408 (0.8283)  loss_n_60: 0.8656 (0.9747)  loss_n_80: 1.1062 (1.2073)  loss_n_100: 1.3740 (1.4391)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [660/845]  eta: 0:01:01  loss: 4.1002 (4.4460)  loss_n_40: 0.7518 (0.8272)  loss_n_60: 0.8798 (0.9736)  loss_n_80: 1.1625 (1.2065)  loss_n_100: 1.3740 (1.4387)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [670/845]  eta: 0:00:58  loss: 4.3207 (4.4490)  loss_n_40: 0.7679 (0.8271)  loss_n_60: 0.9236 (0.9741)  loss_n_80: 1.1675 (1.2077)  loss_n_100: 1.3934 (1.4401)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [680/845]  eta: 0:00:55  loss: 4.3207 (4.4512)  loss_n_40: 0.7694 (0.8285)  loss_n_60: 0.9236 (0.9750)  loss_n_80: 1.1633 (1.2079)  loss_n_100: 1.4387 (1.4400)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [690/845]  eta: 0:00:51  loss: 4.2370 (4.4531)  loss_n_40: 0.7430 (0.8287)  loss_n_60: 0.8883 (0.9753)  loss_n_80: 1.1633 (1.2084)  loss_n_100: 1.4214 (1.4408)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [700/845]  eta: 0:00:48  loss: 4.0277 (4.4473)  loss_n_40: 0.7366 (0.8272)  loss_n_60: 0.8842 (0.9739)  loss_n_80: 1.1115 (1.2069)  loss_n_100: 1.3720 (1.4393)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [710/845]  eta: 0:00:45  loss: 4.0071 (4.4452)  loss_n_40: 0.7420 (0.8268)  loss_n_60: 0.8850 (0.9732)  loss_n_80: 1.0991 (1.2062)  loss_n_100: 1.3237 (1.4391)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [720/845]  eta: 0:00:41  loss: 4.0322 (4.4424)  loss_n_40: 0.7721 (0.8256)  loss_n_60: 0.8928 (0.9724)  loss_n_80: 1.0931 (1.2055)  loss_n_100: 1.3478 (1.4389)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [730/845]  eta: 0:00:38  loss: 4.0756 (4.4474)  loss_n_40: 0.7745 (0.8277)  loss_n_60: 0.9001 (0.9737)  loss_n_80: 1.1064 (1.2068)  loss_n_100: 1.3702 (1.4393)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [740/845]  eta: 0:00:35  loss: 4.2289 (4.4492)  loss_n_40: 0.7852 (0.8286)  loss_n_60: 0.9523 (0.9740)  loss_n_80: 1.1725 (1.2069)  loss_n_100: 1.3929 (1.4397)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [750/845]  eta: 0:00:31  loss: 4.1396 (4.4472)  loss_n_40: 0.7544 (0.8273)  loss_n_60: 0.9114 (0.9734)  loss_n_80: 1.1204 (1.2068)  loss_n_100: 1.4200 (1.4397)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [760/845]  eta: 0:00:28  loss: 4.1624 (4.4474)  loss_n_40: 0.7544 (0.8269)  loss_n_60: 0.9114 (0.9734)  loss_n_80: 1.1204 (1.2071)  loss_n_100: 1.4107 (1.4400)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [770/845]  eta: 0:00:25  loss: 4.0279 (4.4441)  loss_n_40: 0.7530 (0.8259)  loss_n_60: 0.9107 (0.9727)  loss_n_80: 1.1180 (1.2063)  loss_n_100: 1.3692 (1.4391)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [780/845]  eta: 0:00:21  loss: 4.0580 (4.4437)  loss_n_40: 0.7602 (0.8255)  loss_n_60: 0.9352 (0.9730)  loss_n_80: 1.1180 (1.2062)  loss_n_100: 1.3412 (1.4390)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:14]  [790/845]  eta: 0:00:18  loss: 4.1428 (4.4432)  loss_n_40: 0.8046 (0.8262)  loss_n_60: 0.9388 (0.9734)  loss_n_80: 1.1119 (1.2055)  loss_n_100: 1.3412 (1.4381)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [800/845]  eta: 0:00:15  loss: 4.0895 (4.4428)  loss_n_40: 0.7448 (0.8254)  loss_n_60: 0.9307 (0.9733)  loss_n_80: 1.1025 (1.2058)  loss_n_100: 1.3962 (1.4384)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [810/845]  eta: 0:00:11  loss: 4.2518 (4.4491)  loss_n_40: 0.7740 (0.8275)  loss_n_60: 0.9350 (0.9752)  loss_n_80: 1.2065 (1.2072)  loss_n_100: 1.4091 (1.4393)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [820/845]  eta: 0:00:08  loss: 4.2518 (4.4480)  loss_n_40: 0.7649 (0.8266)  loss_n_60: 0.9262 (0.9746)  loss_n_80: 1.2065 (1.2071)  loss_n_100: 1.4230 (1.4397)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [830/845]  eta: 0:00:05  loss: 4.0507 (4.4473)  loss_n_40: 0.7185 (0.8257)  loss_n_60: 0.8641 (0.9744)  loss_n_80: 1.1708 (1.2073)  loss_n_100: 1.4230 (1.4398)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [840/845]  eta: 0:00:01  loss: 4.1928 (4.4484)  loss_n_40: 0.7533 (0.8255)  loss_n_60: 0.8903 (0.9745)  loss_n_80: 1.1708 (1.2078)  loss_n_100: 1.3913 (1.4405)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14]  [844/845]  eta: 0:00:00  loss: 4.2149 (4.4460)  loss_n_40: 0.7533 (0.8249)  loss_n_60: 0.9197 (0.9738)  loss_n_80: 1.1708 (1.2074)  loss_n_100: 1.4478 (1.4399)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:14] Total time: 0:04:42 (0.3348 s / it)\n",
      "Averaged stats: loss: 4.2149 (4.4460)  loss_n_40: 0.7533 (0.8249)  loss_n_60: 0.9197 (0.9738)  loss_n_80: 1.1708 (1.2074)  loss_n_100: 1.4478 (1.4399)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_14_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 1.440%\n",
      "Min loss_n_100: 0.450\n",
      "Best Epoch: 13.000\n",
      "Train: [epoch:15]  [   0/1724]  eta: 2:01:14  lr: 0.000200  loss: 4.6468 (4.6468)  loss_n_40: 0.7962 (0.7962)  loss_n_60: 0.9843 (0.9843)  loss_n_80: 1.2770 (1.2770)  loss_n_100: 1.5892 (1.5892)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.2194  data: 0.4636  max mem: 46473\n",
      "Train: [epoch:15]  [  10/1724]  eta: 1:52:45  lr: 0.000200  loss: 4.2779 (4.2596)  loss_n_40: 0.7657 (0.8183)  loss_n_60: 0.9605 (0.9574)  loss_n_80: 1.1683 (1.1600)  loss_n_100: 1.3615 (1.3239)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9470  data: 0.0423  max mem: 46473\n",
      "Train: [epoch:15]  [  20/1724]  eta: 1:51:42  lr: 0.000200  loss: 3.8350 (4.0023)  loss_n_40: 0.7418 (0.7849)  loss_n_60: 0.8815 (0.9172)  loss_n_80: 1.0468 (1.0787)  loss_n_100: 1.1788 (1.2216)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [  30/1724]  eta: 1:50:53  lr: 0.000200  loss: 3.3253 (3.7234)  loss_n_40: 0.6582 (0.7315)  loss_n_60: 0.7729 (0.8550)  loss_n_80: 0.9414 (1.0042)  loss_n_100: 1.0345 (1.1328)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [  40/1724]  eta: 1:50:10  lr: 0.000200  loss: 2.9124 (3.5314)  loss_n_40: 0.5753 (0.6995)  loss_n_60: 0.6684 (0.8135)  loss_n_80: 0.7846 (0.9520)  loss_n_100: 0.8663 (1.0664)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9174  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [  50/1724]  eta: 1:49:28  lr: 0.000200  loss: 3.0896 (3.6667)  loss_n_40: 0.6124 (0.6961)  loss_n_60: 0.6897 (0.8102)  loss_n_80: 0.8522 (0.9624)  loss_n_100: 0.9394 (1.0905)  triple_100: 0.0000 (0.0326)  triple_80: 0.0000 (0.0402)  triple_60: 0.0000 (0.0348)  triple_40: 0.0000 (0.0000)  time: 3.9178  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [  60/1724]  eta: 1:48:48  lr: 0.000200  loss: 3.8736 (3.7503)  loss_n_40: 0.6677 (0.6950)  loss_n_60: 0.8930 (0.8263)  loss_n_80: 1.0487 (0.9902)  loss_n_100: 1.2559 (1.1350)  triple_100: 0.0000 (0.0411)  triple_80: 0.0000 (0.0336)  triple_60: 0.0000 (0.0291)  triple_40: 0.0000 (0.0000)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [  70/1724]  eta: 1:48:08  lr: 0.000200  loss: 3.8119 (3.7339)  loss_n_40: 0.6646 (0.6917)  loss_n_60: 0.8825 (0.8311)  loss_n_80: 1.0487 (0.9896)  loss_n_100: 1.1923 (1.1323)  triple_100: 0.0000 (0.0353)  triple_80: 0.0000 (0.0288)  triple_60: 0.0000 (0.0250)  triple_40: 0.0000 (0.0000)  time: 3.9194  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [  80/1724]  eta: 1:47:27  lr: 0.000200  loss: 3.2868 (3.6445)  loss_n_40: 0.5968 (0.6755)  loss_n_60: 0.7625 (0.8151)  loss_n_80: 0.8845 (0.9676)  loss_n_100: 1.0314 (1.1082)  triple_100: 0.0000 (0.0310)  triple_80: 0.0000 (0.0253)  triple_60: 0.0000 (0.0219)  triple_40: 0.0000 (0.0000)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [  90/1724]  eta: 1:46:48  lr: 0.000200  loss: 2.7580 (3.5312)  loss_n_40: 0.5251 (0.6610)  loss_n_60: 0.6315 (0.7926)  loss_n_80: 0.7384 (0.9377)  loss_n_100: 0.7929 (1.0703)  triple_100: 0.0000 (0.0276)  triple_80: 0.0000 (0.0225)  triple_60: 0.0000 (0.0195)  triple_40: 0.0000 (0.0000)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 100/1724]  eta: 1:46:08  lr: 0.000200  loss: 2.5306 (3.4279)  loss_n_40: 0.5093 (0.6449)  loss_n_60: 0.6114 (0.7723)  loss_n_80: 0.6766 (0.9112)  loss_n_100: 0.7470 (1.0368)  triple_100: 0.0000 (0.0248)  triple_80: 0.0000 (0.0203)  triple_60: 0.0000 (0.0176)  triple_40: 0.0000 (0.0000)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 110/1724]  eta: 1:45:28  lr: 0.000200  loss: 2.3621 (3.3188)  loss_n_40: 0.4658 (0.6271)  loss_n_60: 0.5584 (0.7507)  loss_n_80: 0.6491 (0.8824)  loss_n_100: 0.6865 (1.0016)  triple_100: 0.0000 (0.0226)  triple_80: 0.0000 (0.0184)  triple_60: 0.0000 (0.0160)  triple_40: 0.0000 (0.0000)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 120/1724]  eta: 1:44:48  lr: 0.000200  loss: 2.3253 (3.2545)  loss_n_40: 0.4597 (0.6192)  loss_n_60: 0.5584 (0.7379)  loss_n_80: 0.6046 (0.8634)  loss_n_100: 0.6476 (0.9778)  triple_100: 0.0000 (0.0246)  triple_80: 0.0000 (0.0169)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0000)  time: 3.9170  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [ 130/1724]  eta: 1:44:08  lr: 0.000200  loss: 2.2851 (3.1749)  loss_n_40: 0.4480 (0.6076)  loss_n_60: 0.5590 (0.7232)  loss_n_80: 0.5864 (0.8413)  loss_n_100: 0.6461 (0.9509)  triple_100: 0.0000 (0.0227)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0135)  triple_40: 0.0000 (0.0000)  time: 3.9150  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [ 140/1724]  eta: 1:43:29  lr: 0.000200  loss: 2.0645 (3.1120)  loss_n_40: 0.4277 (0.6007)  loss_n_60: 0.5233 (0.7117)  loss_n_80: 0.5337 (0.8235)  loss_n_100: 0.5962 (0.9279)  triple_100: 0.0000 (0.0211)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0126)  triple_40: 0.0000 (0.0000)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 150/1724]  eta: 1:42:49  lr: 0.000200  loss: 2.0354 (3.0399)  loss_n_40: 0.4053 (0.5901)  loss_n_60: 0.5061 (0.6978)  loss_n_80: 0.5169 (0.8032)  loss_n_100: 0.5873 (0.9038)  triple_100: 0.0000 (0.0197)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0118)  triple_40: 0.0000 (0.0000)  time: 3.9184  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [ 160/1724]  eta: 1:42:10  lr: 0.000200  loss: 1.9131 (2.9727)  loss_n_40: 0.3986 (0.5802)  loss_n_60: 0.4672 (0.6839)  loss_n_80: 0.4877 (0.7850)  loss_n_100: 0.5390 (0.8815)  triple_100: 0.0000 (0.0185)  triple_80: 0.0000 (0.0127)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0000)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 170/1724]  eta: 1:41:31  lr: 0.000200  loss: 1.8591 (2.9061)  loss_n_40: 0.3920 (0.5701)  loss_n_60: 0.4576 (0.6703)  loss_n_80: 0.4929 (0.7665)  loss_n_100: 0.5217 (0.8595)  triple_100: 0.0000 (0.0174)  triple_80: 0.0000 (0.0120)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0000)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 180/1724]  eta: 1:40:51  lr: 0.000200  loss: 1.8591 (2.8479)  loss_n_40: 0.3782 (0.5609)  loss_n_60: 0.4570 (0.6583)  loss_n_80: 0.4628 (0.7506)  loss_n_100: 0.5307 (0.8406)  triple_100: 0.0000 (0.0164)  triple_80: 0.0000 (0.0113)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0000)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 190/1724]  eta: 1:40:12  lr: 0.000200  loss: 1.8748 (2.8024)  loss_n_40: 0.3914 (0.5563)  loss_n_60: 0.4483 (0.6486)  loss_n_80: 0.4721 (0.7370)  loss_n_100: 0.5097 (0.8235)  triple_100: 0.0000 (0.0169)  triple_80: 0.0000 (0.0107)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0000)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 200/1724]  eta: 1:39:33  lr: 0.000200  loss: 1.8342 (2.7555)  loss_n_40: 0.3866 (0.5482)  loss_n_60: 0.4423 (0.6390)  loss_n_80: 0.4691 (0.7247)  loss_n_100: 0.5022 (0.8084)  triple_100: 0.0000 (0.0161)  triple_80: 0.0000 (0.0102)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0000)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 210/1724]  eta: 1:38:54  lr: 0.000200  loss: 1.7642 (2.7099)  loss_n_40: 0.3832 (0.5421)  loss_n_60: 0.4394 (0.6296)  loss_n_80: 0.4466 (0.7120)  loss_n_100: 0.4761 (0.7928)  triple_100: 0.0000 (0.0153)  triple_80: 0.0000 (0.0097)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0000)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 220/1724]  eta: 1:38:14  lr: 0.000200  loss: 1.8625 (2.6776)  loss_n_40: 0.4153 (0.5362)  loss_n_60: 0.4418 (0.6199)  loss_n_80: 0.4540 (0.7000)  loss_n_100: 0.4715 (0.7776)  triple_100: 0.0000 (0.0177)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0107)  triple_40: 0.0000 (0.0000)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 230/1724]  eta: 1:37:35  lr: 0.000200  loss: 1.9514 (2.6518)  loss_n_40: 0.4461 (0.5331)  loss_n_60: 0.4621 (0.6142)  loss_n_80: 0.5041 (0.6932)  loss_n_100: 0.5277 (0.7690)  triple_100: 0.0000 (0.0169)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0102)  triple_40: 0.0000 (0.0004)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 240/1724]  eta: 1:36:56  lr: 0.000200  loss: 1.9703 (2.6186)  loss_n_40: 0.4161 (0.5272)  loss_n_60: 0.4714 (0.6069)  loss_n_80: 0.5110 (0.6845)  loss_n_100: 0.5626 (0.7593)  triple_100: 0.0000 (0.0162)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0004)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 250/1724]  eta: 1:36:17  lr: 0.000200  loss: 1.8690 (2.5909)  loss_n_40: 0.4145 (0.5247)  loss_n_60: 0.4482 (0.6008)  loss_n_80: 0.4909 (0.6768)  loss_n_100: 0.5260 (0.7496)  triple_100: 0.0000 (0.0156)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0004)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 260/1724]  eta: 1:35:37  lr: 0.000200  loss: 1.9033 (2.5659)  loss_n_40: 0.4339 (0.5218)  loss_n_60: 0.4515 (0.5955)  loss_n_80: 0.4893 (0.6700)  loss_n_100: 0.5134 (0.7412)  triple_100: 0.0000 (0.0150)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0004)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 270/1724]  eta: 1:34:58  lr: 0.000200  loss: 1.8472 (2.5409)  loss_n_40: 0.4215 (0.5190)  loss_n_60: 0.4450 (0.5901)  loss_n_80: 0.4857 (0.6632)  loss_n_100: 0.5051 (0.7324)  triple_100: 0.0000 (0.0144)  triple_80: 0.0000 (0.0126)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0005)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 280/1724]  eta: 1:34:19  lr: 0.000200  loss: 1.7795 (2.5112)  loss_n_40: 0.4144 (0.5152)  loss_n_60: 0.4260 (0.5836)  loss_n_80: 0.4576 (0.6552)  loss_n_100: 0.4715 (0.7223)  triple_100: 0.0000 (0.0139)  triple_80: 0.0000 (0.0122)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0004)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 290/1724]  eta: 1:33:40  lr: 0.000200  loss: 1.6271 (2.4822)  loss_n_40: 0.3709 (0.5109)  loss_n_60: 0.3915 (0.5769)  loss_n_80: 0.4237 (0.6471)  loss_n_100: 0.4356 (0.7119)  triple_100: 0.0000 (0.0134)  triple_80: 0.0000 (0.0117)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0020)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 300/1724]  eta: 1:33:00  lr: 0.000200  loss: 1.5563 (2.4595)  loss_n_40: 0.3620 (0.5074)  loss_n_60: 0.3760 (0.5717)  loss_n_80: 0.4237 (0.6414)  loss_n_100: 0.4311 (0.7048)  triple_100: 0.0000 (0.0130)  triple_80: 0.0000 (0.0114)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0019)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 310/1724]  eta: 1:32:21  lr: 0.000200  loss: 1.7652 (2.4357)  loss_n_40: 0.4003 (0.5035)  loss_n_60: 0.4215 (0.5665)  loss_n_80: 0.4555 (0.6353)  loss_n_100: 0.4833 (0.6973)  triple_100: 0.0000 (0.0126)  triple_80: 0.0000 (0.0110)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0019)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 320/1724]  eta: 1:31:42  lr: 0.000200  loss: 1.6346 (2.4122)  loss_n_40: 0.3806 (0.5001)  loss_n_60: 0.3877 (0.5615)  loss_n_80: 0.4272 (0.6289)  loss_n_100: 0.4494 (0.6897)  triple_100: 0.0000 (0.0122)  triple_80: 0.0000 (0.0106)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0018)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 330/1724]  eta: 1:31:03  lr: 0.000200  loss: 1.5403 (2.3843)  loss_n_40: 0.3550 (0.4955)  loss_n_60: 0.3637 (0.5553)  loss_n_80: 0.3961 (0.6215)  loss_n_100: 0.4191 (0.6811)  triple_100: 0.0000 (0.0118)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0018)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 340/1724]  eta: 1:30:23  lr: 0.000200  loss: 1.4408 (2.3794)  loss_n_40: 0.3389 (0.4917)  loss_n_60: 0.3308 (0.5501)  loss_n_80: 0.3742 (0.6157)  loss_n_100: 0.3833 (0.6746)  triple_100: 0.0000 (0.0161)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0143)  triple_40: 0.0000 (0.0021)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 350/1724]  eta: 1:29:44  lr: 0.000200  loss: 2.4738 (2.4200)  loss_n_40: 0.4256 (0.4911)  loss_n_60: 0.5411 (0.5525)  loss_n_80: 0.6050 (0.6202)  loss_n_100: 0.7707 (0.6826)  triple_100: 0.0000 (0.0239)  triple_80: 0.0000 (0.0220)  triple_60: 0.0000 (0.0220)  triple_40: 0.0000 (0.0058)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 360/1724]  eta: 1:29:05  lr: 0.000200  loss: 2.9801 (2.4395)  loss_n_40: 0.4846 (0.4921)  loss_n_60: 0.6561 (0.5566)  loss_n_80: 0.7694 (0.6270)  loss_n_100: 0.9603 (0.6922)  triple_100: 0.0000 (0.0233)  triple_80: 0.0000 (0.0214)  triple_60: 0.0000 (0.0214)  triple_40: 0.0000 (0.0056)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 370/1724]  eta: 1:28:26  lr: 0.000200  loss: 3.1007 (2.4862)  loss_n_40: 0.5325 (0.4936)  loss_n_60: 0.7130 (0.5614)  loss_n_80: 0.8472 (0.6332)  loss_n_100: 0.9816 (0.7012)  triple_100: 0.0000 (0.0456)  triple_80: 0.0000 (0.0249)  triple_60: 0.0000 (0.0208)  triple_40: 0.0000 (0.0055)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 380/1724]  eta: 1:27:47  lr: 0.000200  loss: 2.7695 (2.4899)  loss_n_40: 0.4998 (0.4944)  loss_n_60: 0.6715 (0.5638)  loss_n_80: 0.7584 (0.6347)  loss_n_100: 0.8634 (0.7028)  triple_100: 0.0000 (0.0444)  triple_80: 0.0000 (0.0242)  triple_60: 0.0000 (0.0202)  triple_40: 0.0000 (0.0054)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 390/1724]  eta: 1:27:07  lr: 0.000200  loss: 2.4728 (2.4906)  loss_n_40: 0.5019 (0.4955)  loss_n_60: 0.6262 (0.5655)  loss_n_80: 0.6465 (0.6351)  loss_n_100: 0.7020 (0.7026)  triple_100: 0.0000 (0.0433)  triple_80: 0.0000 (0.0236)  triple_60: 0.0000 (0.0197)  triple_40: 0.0000 (0.0052)  time: 3.9177  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [ 400/1724]  eta: 1:26:28  lr: 0.000200  loss: 2.3500 (2.4875)  loss_n_40: 0.5019 (0.4959)  loss_n_60: 0.5809 (0.5652)  loss_n_80: 0.6109 (0.6335)  loss_n_100: 0.6304 (0.7002)  triple_100: 0.0000 (0.0422)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0196)  triple_40: 0.0000 (0.0078)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 410/1724]  eta: 1:25:49  lr: 0.000200  loss: 2.2420 (2.4832)  loss_n_40: 0.4226 (0.4955)  loss_n_60: 0.5367 (0.5651)  loss_n_80: 0.5879 (0.6327)  loss_n_100: 0.6445 (0.6995)  triple_100: 0.0000 (0.0412)  triple_80: 0.0000 (0.0225)  triple_60: 0.0000 (0.0192)  triple_40: 0.0000 (0.0076)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 420/1724]  eta: 1:25:10  lr: 0.000200  loss: 2.2553 (2.4788)  loss_n_40: 0.4349 (0.4951)  loss_n_60: 0.5412 (0.5650)  loss_n_80: 0.5951 (0.6321)  loss_n_100: 0.6713 (0.6985)  triple_100: 0.0000 (0.0402)  triple_80: 0.0000 (0.0219)  triple_60: 0.0000 (0.0187)  triple_40: 0.0000 (0.0074)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 430/1724]  eta: 1:24:31  lr: 0.000200  loss: 2.0708 (2.4666)  loss_n_40: 0.4162 (0.4931)  loss_n_60: 0.4964 (0.5631)  loss_n_80: 0.5544 (0.6294)  loss_n_100: 0.6004 (0.6948)  triple_100: 0.0000 (0.0393)  triple_80: 0.0000 (0.0214)  triple_60: 0.0000 (0.0183)  triple_40: 0.0000 (0.0072)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 440/1724]  eta: 1:23:51  lr: 0.000200  loss: 1.8755 (2.4545)  loss_n_40: 0.4027 (0.4920)  loss_n_60: 0.4661 (0.5612)  loss_n_80: 0.4856 (0.6262)  loss_n_100: 0.5176 (0.6909)  triple_100: 0.0000 (0.0384)  triple_80: 0.0000 (0.0209)  triple_60: 0.0000 (0.0179)  triple_40: 0.0000 (0.0070)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 450/1724]  eta: 1:23:12  lr: 0.000200  loss: 1.8541 (2.4428)  loss_n_40: 0.4181 (0.4915)  loss_n_60: 0.4426 (0.5593)  loss_n_80: 0.4581 (0.6230)  loss_n_100: 0.4702 (0.6866)  triple_100: 0.0000 (0.0375)  triple_80: 0.0000 (0.0205)  triple_60: 0.0000 (0.0175)  triple_40: 0.0000 (0.0069)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 460/1724]  eta: 1:22:33  lr: 0.000200  loss: 1.7309 (2.4275)  loss_n_40: 0.3946 (0.4896)  loss_n_60: 0.4259 (0.5565)  loss_n_80: 0.4411 (0.6192)  loss_n_100: 0.4682 (0.6817)  triple_100: 0.0000 (0.0367)  triple_80: 0.0000 (0.0200)  triple_60: 0.0000 (0.0171)  triple_40: 0.0000 (0.0067)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 470/1724]  eta: 1:21:54  lr: 0.000200  loss: 1.6356 (2.4158)  loss_n_40: 0.3946 (0.4891)  loss_n_60: 0.3951 (0.5543)  loss_n_80: 0.4187 (0.6159)  loss_n_100: 0.4455 (0.6776)  triple_100: 0.0000 (0.0359)  triple_80: 0.0000 (0.0196)  triple_60: 0.0000 (0.0167)  triple_40: 0.0000 (0.0066)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 480/1724]  eta: 1:21:14  lr: 0.000200  loss: 1.6356 (2.4011)  loss_n_40: 0.3770 (0.4874)  loss_n_60: 0.3951 (0.5516)  loss_n_80: 0.4230 (0.6121)  loss_n_100: 0.4455 (0.6728)  triple_100: 0.0000 (0.0352)  triple_80: 0.0000 (0.0192)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0065)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 490/1724]  eta: 1:20:35  lr: 0.000200  loss: 1.5453 (2.3827)  loss_n_40: 0.3449 (0.4846)  loss_n_60: 0.3706 (0.5477)  loss_n_80: 0.4004 (0.6075)  loss_n_100: 0.4193 (0.6672)  triple_100: 0.0000 (0.0345)  triple_80: 0.0000 (0.0188)  triple_60: 0.0000 (0.0160)  triple_40: 0.0000 (0.0064)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 500/1724]  eta: 1:19:56  lr: 0.000200  loss: 1.5993 (2.3706)  loss_n_40: 0.3543 (0.4827)  loss_n_60: 0.3762 (0.5449)  loss_n_80: 0.4071 (0.6045)  loss_n_100: 0.4329 (0.6634)  triple_100: 0.0000 (0.0342)  triple_80: 0.0000 (0.0184)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0067)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 510/1724]  eta: 1:19:17  lr: 0.000200  loss: 1.8968 (2.3622)  loss_n_40: 0.3889 (0.4810)  loss_n_60: 0.4460 (0.5433)  loss_n_80: 0.4623 (0.6030)  loss_n_100: 0.4743 (0.6613)  triple_100: 0.0000 (0.0335)  triple_80: 0.0000 (0.0181)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0066)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 520/1724]  eta: 1:18:38  lr: 0.000200  loss: 1.7313 (2.3504)  loss_n_40: 0.3837 (0.4790)  loss_n_60: 0.4235 (0.5408)  loss_n_80: 0.4518 (0.6004)  loss_n_100: 0.4731 (0.6580)  triple_100: 0.0000 (0.0329)  triple_80: 0.0000 (0.0177)  triple_60: 0.0000 (0.0151)  triple_40: 0.0000 (0.0065)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 530/1724]  eta: 1:17:58  lr: 0.000200  loss: 1.9223 (2.3586)  loss_n_40: 0.3854 (0.4781)  loss_n_60: 0.4689 (0.5412)  loss_n_80: 0.5206 (0.6020)  loss_n_100: 0.5474 (0.6614)  triple_100: 0.0000 (0.0329)  triple_80: 0.0000 (0.0186)  triple_60: 0.0000 (0.0180)  triple_40: 0.0000 (0.0064)  time: 3.9153  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 540/1724]  eta: 1:17:19  lr: 0.000200  loss: 2.3835 (2.3580)  loss_n_40: 0.4488 (0.4775)  loss_n_60: 0.5311 (0.5408)  loss_n_80: 0.6237 (0.6023)  loss_n_100: 0.7565 (0.6629)  triple_100: 0.0000 (0.0323)  triple_80: 0.0000 (0.0183)  triple_60: 0.0000 (0.0177)  triple_40: 0.0000 (0.0062)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 550/1724]  eta: 1:16:40  lr: 0.000200  loss: 2.2060 (2.3497)  loss_n_40: 0.4195 (0.4760)  loss_n_60: 0.4823 (0.5389)  loss_n_80: 0.5329 (0.6003)  loss_n_100: 0.6146 (0.6608)  triple_100: 0.0000 (0.0317)  triple_80: 0.0000 (0.0180)  triple_60: 0.0000 (0.0174)  triple_40: 0.0000 (0.0068)  time: 3.9154  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 560/1724]  eta: 1:16:00  lr: 0.000200  loss: 1.8177 (2.3389)  loss_n_40: 0.3900 (0.4744)  loss_n_60: 0.4155 (0.5367)  loss_n_80: 0.4673 (0.5977)  loss_n_100: 0.5240 (0.6577)  triple_100: 0.0000 (0.0311)  triple_80: 0.0000 (0.0176)  triple_60: 0.0000 (0.0170)  triple_40: 0.0000 (0.0066)  time: 3.9151  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [ 570/1724]  eta: 1:15:21  lr: 0.000200  loss: 1.7471 (2.3285)  loss_n_40: 0.3780 (0.4731)  loss_n_60: 0.4093 (0.5346)  loss_n_80: 0.4439 (0.5951)  loss_n_100: 0.4778 (0.6545)  triple_100: 0.0000 (0.0306)  triple_80: 0.0000 (0.0173)  triple_60: 0.0000 (0.0167)  triple_40: 0.0000 (0.0065)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 580/1724]  eta: 1:14:42  lr: 0.000200  loss: 1.6481 (2.3159)  loss_n_40: 0.3753 (0.4712)  loss_n_60: 0.3978 (0.5320)  loss_n_80: 0.4250 (0.5920)  loss_n_100: 0.4565 (0.6508)  triple_100: 0.0000 (0.0301)  triple_80: 0.0000 (0.0170)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0064)  time: 3.9149  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 590/1724]  eta: 1:14:03  lr: 0.000200  loss: 1.4566 (2.3016)  loss_n_40: 0.3389 (0.4691)  loss_n_60: 0.3467 (0.5290)  loss_n_80: 0.3874 (0.5883)  loss_n_100: 0.4058 (0.6464)  triple_100: 0.0000 (0.0295)  triple_80: 0.0000 (0.0167)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0063)  time: 3.9137  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 600/1724]  eta: 1:13:23  lr: 0.000200  loss: 1.4478 (2.2879)  loss_n_40: 0.3475 (0.4672)  loss_n_60: 0.3544 (0.5260)  loss_n_80: 0.3671 (0.5848)  loss_n_100: 0.3761 (0.6421)  triple_100: 0.0000 (0.0291)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0159)  triple_40: 0.0000 (0.0063)  time: 3.9143  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [ 610/1724]  eta: 1:12:44  lr: 0.000200  loss: 1.4287 (2.2726)  loss_n_40: 0.3274 (0.4647)  loss_n_60: 0.3443 (0.5227)  loss_n_80: 0.3684 (0.5810)  loss_n_100: 0.3771 (0.6375)  triple_100: 0.0000 (0.0286)  triple_80: 0.0000 (0.0162)  triple_60: 0.0000 (0.0156)  triple_40: 0.0000 (0.0062)  time: 3.9158  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [ 620/1724]  eta: 1:12:05  lr: 0.000200  loss: 1.3234 (2.2588)  loss_n_40: 0.3041 (0.4625)  loss_n_60: 0.3171 (0.5198)  loss_n_80: 0.3387 (0.5776)  loss_n_100: 0.3427 (0.6334)  triple_100: 0.0000 (0.0281)  triple_80: 0.0000 (0.0159)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0061)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 630/1724]  eta: 1:11:26  lr: 0.000200  loss: 1.4804 (2.2568)  loss_n_40: 0.3318 (0.4613)  loss_n_60: 0.3307 (0.5174)  loss_n_80: 0.3428 (0.5747)  loss_n_100: 0.3820 (0.6300)  triple_100: 0.0000 (0.0290)  triple_80: 0.0000 (0.0180)  triple_60: 0.0000 (0.0175)  triple_40: 0.0000 (0.0090)  time: 3.9152  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [ 640/1724]  eta: 1:10:47  lr: 0.000200  loss: 2.1476 (2.2679)  loss_n_40: 0.4538 (0.4618)  loss_n_60: 0.4638 (0.5194)  loss_n_80: 0.5379 (0.5776)  loss_n_100: 0.5699 (0.6341)  triple_100: 0.0000 (0.0298)  triple_80: 0.0000 (0.0190)  triple_60: 0.0000 (0.0172)  triple_40: 0.0000 (0.0089)  time: 3.9138  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 650/1724]  eta: 1:10:07  lr: 0.000200  loss: 2.8006 (2.2769)  loss_n_40: 0.5670 (0.4639)  loss_n_60: 0.6487 (0.5217)  loss_n_80: 0.7454 (0.5801)  loss_n_100: 0.8588 (0.6374)  triple_100: 0.0000 (0.0294)  triple_80: 0.0000 (0.0187)  triple_60: 0.0000 (0.0170)  triple_40: 0.0000 (0.0087)  time: 3.9146  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 660/1724]  eta: 1:09:28  lr: 0.000200  loss: 2.7034 (2.2813)  loss_n_40: 0.5731 (0.4650)  loss_n_60: 0.6437 (0.5230)  loss_n_80: 0.7107 (0.5816)  loss_n_100: 0.8057 (0.6391)  triple_100: 0.0000 (0.0289)  triple_80: 0.0000 (0.0184)  triple_60: 0.0000 (0.0167)  triple_40: 0.0000 (0.0086)  time: 3.9151  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 670/1724]  eta: 1:08:49  lr: 0.000200  loss: 2.3195 (2.2795)  loss_n_40: 0.4871 (0.4650)  loss_n_60: 0.5614 (0.5228)  loss_n_80: 0.6017 (0.5813)  loss_n_100: 0.6689 (0.6388)  triple_100: 0.0000 (0.0285)  triple_80: 0.0000 (0.0181)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0085)  time: 3.9144  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 680/1724]  eta: 1:08:10  lr: 0.000200  loss: 1.9414 (2.2740)  loss_n_40: 0.4348 (0.4646)  loss_n_60: 0.4689 (0.5218)  loss_n_80: 0.5172 (0.5800)  loss_n_100: 0.5916 (0.6370)  triple_100: 0.0000 (0.0281)  triple_80: 0.0000 (0.0179)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0083)  time: 3.9149  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 690/1724]  eta: 1:07:30  lr: 0.000200  loss: 1.7302 (2.2661)  loss_n_40: 0.3934 (0.4635)  loss_n_60: 0.4111 (0.5203)  loss_n_80: 0.4603 (0.5782)  loss_n_100: 0.4642 (0.6345)  triple_100: 0.0000 (0.0277)  triple_80: 0.0000 (0.0176)  triple_60: 0.0000 (0.0160)  triple_40: 0.0000 (0.0082)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 700/1724]  eta: 1:06:51  lr: 0.000200  loss: 1.7046 (2.2583)  loss_n_40: 0.3848 (0.4628)  loss_n_60: 0.4049 (0.5187)  loss_n_80: 0.4363 (0.5761)  loss_n_100: 0.4628 (0.6320)  triple_100: 0.0000 (0.0273)  triple_80: 0.0000 (0.0174)  triple_60: 0.0000 (0.0158)  triple_40: 0.0000 (0.0083)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 710/1724]  eta: 1:06:12  lr: 0.000200  loss: 1.6307 (2.2511)  loss_n_40: 0.3659 (0.4618)  loss_n_60: 0.3895 (0.5172)  loss_n_80: 0.4210 (0.5743)  loss_n_100: 0.4254 (0.6296)  triple_100: 0.0000 (0.0269)  triple_80: 0.0000 (0.0176)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0082)  time: 3.9157  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 720/1724]  eta: 1:05:33  lr: 0.000200  loss: 1.5146 (2.2408)  loss_n_40: 0.3344 (0.4600)  loss_n_60: 0.3756 (0.5151)  loss_n_80: 0.4030 (0.5719)  loss_n_100: 0.4214 (0.6266)  triple_100: 0.0000 (0.0265)  triple_80: 0.0000 (0.0173)  triple_60: 0.0000 (0.0153)  triple_40: 0.0000 (0.0081)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 730/1724]  eta: 1:04:54  lr: 0.000200  loss: 1.3955 (2.2292)  loss_n_40: 0.3146 (0.4582)  loss_n_60: 0.3418 (0.5126)  loss_n_80: 0.3711 (0.5689)  loss_n_100: 0.3805 (0.6231)  triple_100: 0.0000 (0.0262)  triple_80: 0.0000 (0.0171)  triple_60: 0.0000 (0.0151)  triple_40: 0.0000 (0.0079)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 740/1724]  eta: 1:04:14  lr: 0.000200  loss: 1.3808 (2.2188)  loss_n_40: 0.3197 (0.4568)  loss_n_60: 0.3269 (0.5104)  loss_n_80: 0.3512 (0.5662)  loss_n_100: 0.3640 (0.6199)  triple_100: 0.0000 (0.0258)  triple_80: 0.0000 (0.0169)  triple_60: 0.0000 (0.0149)  triple_40: 0.0000 (0.0078)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 750/1724]  eta: 1:03:35  lr: 0.000200  loss: 1.3185 (2.2076)  loss_n_40: 0.3198 (0.4551)  loss_n_60: 0.3178 (0.5080)  loss_n_80: 0.3435 (0.5634)  loss_n_100: 0.3503 (0.6165)  triple_100: 0.0000 (0.0255)  triple_80: 0.0000 (0.0166)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0077)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 760/1724]  eta: 1:02:56  lr: 0.000200  loss: 1.3439 (2.1970)  loss_n_40: 0.3188 (0.4534)  loss_n_60: 0.3178 (0.5057)  loss_n_80: 0.3482 (0.5608)  loss_n_100: 0.3568 (0.6134)  triple_100: 0.0000 (0.0251)  triple_80: 0.0000 (0.0164)  triple_60: 0.0000 (0.0145)  triple_40: 0.0000 (0.0076)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 770/1724]  eta: 1:02:17  lr: 0.000200  loss: 1.2885 (2.1855)  loss_n_40: 0.3015 (0.4515)  loss_n_60: 0.3101 (0.5032)  loss_n_80: 0.3340 (0.5579)  loss_n_100: 0.3484 (0.6100)  triple_100: 0.0000 (0.0248)  triple_80: 0.0000 (0.0162)  triple_60: 0.0000 (0.0143)  triple_40: 0.0000 (0.0075)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 780/1724]  eta: 1:01:38  lr: 0.000200  loss: 1.5678 (2.1810)  loss_n_40: 0.3305 (0.4507)  loss_n_60: 0.3795 (0.5021)  loss_n_80: 0.4116 (0.5567)  loss_n_100: 0.4102 (0.6084)  triple_100: 0.0000 (0.0247)  triple_80: 0.0000 (0.0168)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0075)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 790/1724]  eta: 1:00:59  lr: 0.000200  loss: 1.7646 (2.1775)  loss_n_40: 0.3859 (0.4500)  loss_n_60: 0.4102 (0.5012)  loss_n_80: 0.4604 (0.5556)  loss_n_100: 0.5059 (0.6073)  triple_100: 0.0000 (0.0249)  triple_80: 0.0000 (0.0169)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0074)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 800/1724]  eta: 1:00:19  lr: 0.000200  loss: 1.7459 (2.1725)  loss_n_40: 0.3702 (0.4491)  loss_n_60: 0.4239 (0.5002)  loss_n_80: 0.4633 (0.5545)  loss_n_100: 0.5145 (0.6059)  triple_100: 0.0000 (0.0246)  triple_80: 0.0000 (0.0167)  triple_60: 0.0000 (0.0140)  triple_40: 0.0000 (0.0073)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 810/1724]  eta: 0:59:40  lr: 0.000200  loss: 1.8419 (2.1701)  loss_n_40: 0.4055 (0.4491)  loss_n_60: 0.4499 (0.4999)  loss_n_80: 0.4789 (0.5539)  loss_n_100: 0.5145 (0.6050)  triple_100: 0.0000 (0.0243)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0072)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 820/1724]  eta: 0:59:01  lr: 0.000200  loss: 1.8372 (2.1643)  loss_n_40: 0.4055 (0.4483)  loss_n_60: 0.4399 (0.4988)  loss_n_80: 0.4715 (0.5524)  loss_n_100: 0.5003 (0.6033)  triple_100: 0.0000 (0.0240)  triple_80: 0.0000 (0.0163)  triple_60: 0.0000 (0.0140)  triple_40: 0.0000 (0.0071)  time: 3.9171  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [ 830/1724]  eta: 0:58:22  lr: 0.000200  loss: 1.6281 (2.1576)  loss_n_40: 0.3676 (0.4473)  loss_n_60: 0.3977 (0.4976)  loss_n_80: 0.4117 (0.5508)  loss_n_100: 0.4167 (0.6012)  triple_100: 0.0000 (0.0237)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0138)  triple_40: 0.0000 (0.0070)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 840/1724]  eta: 0:57:43  lr: 0.000200  loss: 1.5657 (2.1501)  loss_n_40: 0.3629 (0.4463)  loss_n_60: 0.3762 (0.4961)  loss_n_80: 0.4086 (0.5489)  loss_n_100: 0.4167 (0.5989)  triple_100: 0.0000 (0.0234)  triple_80: 0.0000 (0.0159)  triple_60: 0.0000 (0.0137)  triple_40: 0.0000 (0.0070)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 850/1724]  eta: 0:57:03  lr: 0.000200  loss: 1.4682 (2.1421)  loss_n_40: 0.3396 (0.4452)  loss_n_60: 0.3546 (0.4944)  loss_n_80: 0.3828 (0.5469)  loss_n_100: 0.3978 (0.5963)  triple_100: 0.0000 (0.0231)  triple_80: 0.0000 (0.0157)  triple_60: 0.0000 (0.0135)  triple_40: 0.0000 (0.0069)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 860/1724]  eta: 0:56:24  lr: 0.000200  loss: 1.4158 (2.1336)  loss_n_40: 0.3341 (0.4440)  loss_n_60: 0.3445 (0.4926)  loss_n_80: 0.3655 (0.5448)  loss_n_100: 0.3691 (0.5937)  triple_100: 0.0000 (0.0229)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0134)  triple_40: 0.0000 (0.0068)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 870/1724]  eta: 0:55:45  lr: 0.000200  loss: 1.4699 (2.1315)  loss_n_40: 0.3407 (0.4435)  loss_n_60: 0.3539 (0.4915)  loss_n_80: 0.3875 (0.5434)  loss_n_100: 0.3755 (0.5919)  triple_100: 0.0000 (0.0226)  triple_80: 0.0000 (0.0160)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0079)  time: 3.9173  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [ 880/1724]  eta: 0:55:06  lr: 0.000200  loss: 2.1207 (2.1403)  loss_n_40: 0.4348 (0.4440)  loss_n_60: 0.4805 (0.4920)  loss_n_80: 0.5534 (0.5442)  loss_n_100: 0.6098 (0.5930)  triple_100: 0.0000 (0.0291)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0146)  triple_40: 0.0000 (0.0078)  time: 3.9174  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [ 890/1724]  eta: 0:54:27  lr: 0.000200  loss: 2.0802 (2.1372)  loss_n_40: 0.4348 (0.4433)  loss_n_60: 0.4805 (0.4914)  loss_n_80: 0.5350 (0.5436)  loss_n_100: 0.5823 (0.5925)  triple_100: 0.0000 (0.0287)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0144)  triple_40: 0.0000 (0.0077)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 900/1724]  eta: 0:53:48  lr: 0.000200  loss: 1.7490 (2.1318)  loss_n_40: 0.3493 (0.4422)  loss_n_60: 0.4004 (0.4903)  loss_n_80: 0.4589 (0.5424)  loss_n_100: 0.5319 (0.5913)  triple_100: 0.0000 (0.0284)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0076)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 910/1724]  eta: 0:53:08  lr: 0.000200  loss: 1.5915 (2.1251)  loss_n_40: 0.3278 (0.4410)  loss_n_60: 0.3665 (0.4889)  loss_n_80: 0.4319 (0.5408)  loss_n_100: 0.4610 (0.5894)  triple_100: 0.0000 (0.0281)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0075)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 920/1724]  eta: 0:52:29  lr: 0.000200  loss: 1.5219 (2.1191)  loss_n_40: 0.3278 (0.4401)  loss_n_60: 0.3665 (0.4876)  loss_n_80: 0.3997 (0.5393)  loss_n_100: 0.4333 (0.5877)  triple_100: 0.0000 (0.0278)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0139)  triple_40: 0.0000 (0.0075)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 930/1724]  eta: 0:51:50  lr: 0.000200  loss: 1.4941 (2.1128)  loss_n_40: 0.3350 (0.4391)  loss_n_60: 0.3719 (0.4864)  loss_n_80: 0.3892 (0.5377)  loss_n_100: 0.4114 (0.5860)  triple_100: 0.0000 (0.0275)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0138)  triple_40: 0.0000 (0.0074)  time: 3.9206  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [ 940/1724]  eta: 0:51:11  lr: 0.000200  loss: 1.4168 (2.1047)  loss_n_40: 0.3260 (0.4379)  loss_n_60: 0.3364 (0.4846)  loss_n_80: 0.3566 (0.5357)  loss_n_100: 0.3901 (0.5835)  triple_100: 0.0000 (0.0272)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0136)  triple_40: 0.0000 (0.0073)  time: 3.9196  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [ 950/1724]  eta: 0:50:32  lr: 0.000200  loss: 1.3327 (2.0975)  loss_n_40: 0.3179 (0.4368)  loss_n_60: 0.3262 (0.4831)  loss_n_80: 0.3393 (0.5339)  loss_n_100: 0.3444 (0.5814)  triple_100: 0.0000 (0.0269)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0135)  triple_40: 0.0000 (0.0073)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [ 960/1724]  eta: 0:49:53  lr: 0.000200  loss: 1.2951 (2.0897)  loss_n_40: 0.3179 (0.4355)  loss_n_60: 0.3150 (0.4815)  loss_n_80: 0.3297 (0.5319)  loss_n_100: 0.3444 (0.5791)  triple_100: 0.0000 (0.0266)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0133)  triple_40: 0.0000 (0.0072)  time: 3.9193  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [ 970/1724]  eta: 0:49:13  lr: 0.000200  loss: 1.3394 (2.0828)  loss_n_40: 0.3357 (0.4349)  loss_n_60: 0.3150 (0.4801)  loss_n_80: 0.3284 (0.5300)  loss_n_100: 0.3380 (0.5768)  triple_100: 0.0000 (0.0264)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0132)  triple_40: 0.0000 (0.0071)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 980/1724]  eta: 0:48:34  lr: 0.000200  loss: 1.3223 (2.0752)  loss_n_40: 0.3021 (0.4336)  loss_n_60: 0.3139 (0.4785)  loss_n_80: 0.3368 (0.5281)  loss_n_100: 0.3432 (0.5746)  triple_100: 0.0000 (0.0261)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0131)  triple_40: 0.0000 (0.0070)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [ 990/1724]  eta: 0:47:55  lr: 0.000200  loss: 1.2448 (2.0673)  loss_n_40: 0.2831 (0.4324)  loss_n_60: 0.3007 (0.4768)  loss_n_80: 0.3148 (0.5261)  loss_n_100: 0.3298 (0.5722)  triple_100: 0.0000 (0.0258)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0129)  triple_40: 0.0000 (0.0070)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1000/1724]  eta: 0:47:16  lr: 0.000200  loss: 1.2349 (2.0613)  loss_n_40: 0.2927 (0.4315)  loss_n_60: 0.2998 (0.4754)  loss_n_80: 0.3183 (0.5245)  loss_n_100: 0.3356 (0.5704)  triple_100: 0.0000 (0.0256)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0128)  triple_40: 0.0000 (0.0069)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1010/1724]  eta: 0:46:37  lr: 0.000200  loss: 1.2349 (2.0537)  loss_n_40: 0.2872 (0.4301)  loss_n_60: 0.2944 (0.4736)  loss_n_80: 0.3194 (0.5225)  loss_n_100: 0.3315 (0.5680)  triple_100: 0.0000 (0.0255)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0129)  triple_40: 0.0000 (0.0071)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1020/1724]  eta: 0:45:58  lr: 0.000200  loss: 1.4821 (2.0508)  loss_n_40: 0.3093 (0.4297)  loss_n_60: 0.3174 (0.4729)  loss_n_80: 0.3852 (0.5218)  loss_n_100: 0.4290 (0.5675)  triple_100: 0.0000 (0.0252)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0127)  triple_40: 0.0000 (0.0071)  time: 3.9162  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [1030/1724]  eta: 0:45:18  lr: 0.000200  loss: 1.6735 (2.0465)  loss_n_40: 0.3686 (0.4291)  loss_n_60: 0.3894 (0.4720)  loss_n_80: 0.4269 (0.5207)  loss_n_100: 0.4820 (0.5664)  triple_100: 0.0000 (0.0250)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0126)  triple_40: 0.0000 (0.0070)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1040/1724]  eta: 0:44:39  lr: 0.000200  loss: 1.5663 (2.0421)  loss_n_40: 0.3686 (0.4284)  loss_n_60: 0.3821 (0.4711)  loss_n_80: 0.4107 (0.5197)  loss_n_100: 0.4400 (0.5651)  triple_100: 0.0000 (0.0247)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0125)  triple_40: 0.0000 (0.0069)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1050/1724]  eta: 0:44:00  lr: 0.000200  loss: 1.4987 (2.0379)  loss_n_40: 0.3401 (0.4281)  loss_n_60: 0.3613 (0.4703)  loss_n_80: 0.3846 (0.5185)  loss_n_100: 0.4096 (0.5637)  triple_100: 0.0000 (0.0245)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0124)  triple_40: 0.0000 (0.0069)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1060/1724]  eta: 0:43:21  lr: 0.000200  loss: 1.4412 (2.0325)  loss_n_40: 0.3401 (0.4276)  loss_n_60: 0.3460 (0.4691)  loss_n_80: 0.3682 (0.5171)  loss_n_100: 0.3813 (0.5620)  triple_100: 0.0000 (0.0243)  triple_80: 0.0000 (0.0134)  triple_60: 0.0000 (0.0123)  triple_40: 0.0000 (0.0068)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1070/1724]  eta: 0:42:42  lr: 0.000200  loss: 1.3799 (2.0260)  loss_n_40: 0.3237 (0.4266)  loss_n_60: 0.3246 (0.4677)  loss_n_80: 0.3435 (0.5154)  loss_n_100: 0.3760 (0.5600)  triple_100: 0.0000 (0.0240)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0121)  triple_40: 0.0000 (0.0067)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1080/1724]  eta: 0:42:02  lr: 0.000200  loss: 1.2981 (2.0191)  loss_n_40: 0.3119 (0.4255)  loss_n_60: 0.3136 (0.4662)  loss_n_80: 0.3357 (0.5136)  loss_n_100: 0.3478 (0.5580)  triple_100: 0.0000 (0.0238)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0120)  triple_40: 0.0000 (0.0067)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1090/1724]  eta: 0:41:23  lr: 0.000200  loss: 1.1945 (2.0117)  loss_n_40: 0.2910 (0.4244)  loss_n_60: 0.2785 (0.4646)  loss_n_80: 0.2959 (0.5117)  loss_n_100: 0.3086 (0.5558)  triple_100: 0.0000 (0.0236)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0119)  triple_40: 0.0000 (0.0066)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1100/1724]  eta: 0:40:44  lr: 0.000200  loss: 1.1958 (2.0062)  loss_n_40: 0.2948 (0.4237)  loss_n_60: 0.2868 (0.4634)  loss_n_80: 0.3048 (0.5102)  loss_n_100: 0.3126 (0.5540)  triple_100: 0.0000 (0.0234)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0118)  triple_40: 0.0000 (0.0068)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1110/1724]  eta: 0:40:05  lr: 0.000200  loss: 1.2415 (1.9996)  loss_n_40: 0.3217 (0.4227)  loss_n_60: 0.2954 (0.4619)  loss_n_80: 0.3048 (0.5084)  loss_n_100: 0.3314 (0.5521)  triple_100: 0.0000 (0.0232)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0117)  triple_40: 0.0000 (0.0068)  time: 3.9190  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [1120/1724]  eta: 0:39:26  lr: 0.000200  loss: 1.2240 (1.9930)  loss_n_40: 0.2948 (0.4217)  loss_n_60: 0.2871 (0.4605)  loss_n_80: 0.3043 (0.5068)  loss_n_100: 0.3224 (0.5501)  triple_100: 0.0000 (0.0230)  triple_80: 0.0000 (0.0127)  triple_60: 0.0000 (0.0116)  triple_40: 0.0000 (0.0067)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1130/1724]  eta: 0:38:47  lr: 0.000200  loss: 1.2540 (2.0031)  loss_n_40: 0.2948 (0.4209)  loss_n_60: 0.2970 (0.4597)  loss_n_80: 0.3144 (0.5058)  loss_n_100: 0.3264 (0.5492)  triple_100: 0.0000 (0.0266)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0087)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1140/1724]  eta: 0:38:07  lr: 0.000200  loss: 1.9500 (2.0042)  loss_n_40: 0.4191 (0.4213)  loss_n_60: 0.4536 (0.4600)  loss_n_80: 0.5005 (0.5061)  loss_n_100: 0.5482 (0.5497)  triple_100: 0.0000 (0.0264)  triple_80: 0.0000 (0.0164)  triple_60: 0.0000 (0.0156)  triple_40: 0.0000 (0.0086)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1150/1724]  eta: 0:37:28  lr: 0.000200  loss: 2.1009 (2.0065)  loss_n_40: 0.4690 (0.4220)  loss_n_60: 0.4909 (0.4604)  loss_n_80: 0.5307 (0.5063)  loss_n_100: 0.5890 (0.5500)  triple_100: 0.0000 (0.0262)  triple_80: 0.0000 (0.0162)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0100)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1160/1724]  eta: 0:36:49  lr: 0.000200  loss: 1.8284 (2.0042)  loss_n_40: 0.4160 (0.4218)  loss_n_60: 0.4189 (0.4599)  loss_n_80: 0.4734 (0.5058)  loss_n_100: 0.5106 (0.5494)  triple_100: 0.0000 (0.0259)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0153)  triple_40: 0.0000 (0.0099)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1170/1724]  eta: 0:36:10  lr: 0.000200  loss: 1.6212 (2.0007)  loss_n_40: 0.3775 (0.4214)  loss_n_60: 0.3880 (0.4593)  loss_n_80: 0.4162 (0.5050)  loss_n_100: 0.4425 (0.5483)  triple_100: 0.0000 (0.0257)  triple_80: 0.0000 (0.0160)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0099)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1180/1724]  eta: 0:35:31  lr: 0.000200  loss: 1.4917 (1.9964)  loss_n_40: 0.3466 (0.4208)  loss_n_60: 0.3585 (0.4584)  loss_n_80: 0.3849 (0.5040)  loss_n_100: 0.3906 (0.5471)  triple_100: 0.0000 (0.0255)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0151)  triple_40: 0.0000 (0.0098)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1190/1724]  eta: 0:34:52  lr: 0.000200  loss: 1.4917 (1.9918)  loss_n_40: 0.3439 (0.4202)  loss_n_60: 0.3585 (0.4575)  loss_n_80: 0.3738 (0.5028)  loss_n_100: 0.3798 (0.5456)  triple_100: 0.0000 (0.0253)  triple_80: 0.0000 (0.0157)  triple_60: 0.0000 (0.0149)  triple_40: 0.0000 (0.0097)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1200/1724]  eta: 0:34:12  lr: 0.000200  loss: 1.5231 (1.9897)  loss_n_40: 0.3623 (0.4198)  loss_n_60: 0.3754 (0.4570)  loss_n_80: 0.3922 (0.5021)  loss_n_100: 0.4067 (0.5449)  triple_100: 0.0000 (0.0251)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0151)  triple_40: 0.0000 (0.0102)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1210/1724]  eta: 0:33:33  lr: 0.000200  loss: 1.6869 (1.9882)  loss_n_40: 0.3744 (0.4196)  loss_n_60: 0.3925 (0.4567)  loss_n_80: 0.4240 (0.5018)  loss_n_100: 0.4709 (0.5446)  triple_100: 0.0000 (0.0249)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0149)  triple_40: 0.0000 (0.0101)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1220/1724]  eta: 0:32:54  lr: 0.000200  loss: 1.6835 (1.9856)  loss_n_40: 0.3938 (0.4196)  loss_n_60: 0.3925 (0.4563)  loss_n_80: 0.4234 (0.5011)  loss_n_100: 0.4630 (0.5439)  triple_100: 0.0000 (0.0247)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0148)  triple_40: 0.0000 (0.0100)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1230/1724]  eta: 0:32:15  lr: 0.000200  loss: 1.6028 (1.9824)  loss_n_40: 0.3838 (0.4191)  loss_n_60: 0.3892 (0.4557)  loss_n_80: 0.3989 (0.5003)  loss_n_100: 0.4222 (0.5429)  triple_100: 0.0000 (0.0245)  triple_80: 0.0000 (0.0152)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0099)  time: 3.9156  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1240/1724]  eta: 0:31:36  lr: 0.000200  loss: 1.5458 (1.9779)  loss_n_40: 0.3572 (0.4186)  loss_n_60: 0.3747 (0.4549)  loss_n_80: 0.3907 (0.4992)  loss_n_100: 0.4121 (0.5415)  triple_100: 0.0000 (0.0243)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0146)  triple_40: 0.0000 (0.0099)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1250/1724]  eta: 0:30:56  lr: 0.000200  loss: 1.4940 (1.9737)  loss_n_40: 0.3441 (0.4181)  loss_n_60: 0.3608 (0.4540)  loss_n_80: 0.3579 (0.4981)  loss_n_100: 0.3774 (0.5403)  triple_100: 0.0000 (0.0241)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0145)  triple_40: 0.0000 (0.0098)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1260/1724]  eta: 0:30:17  lr: 0.000200  loss: 1.4086 (1.9697)  loss_n_40: 0.3379 (0.4177)  loss_n_60: 0.3382 (0.4533)  loss_n_80: 0.3523 (0.4970)  loss_n_100: 0.3683 (0.5390)  triple_100: 0.0000 (0.0239)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0143)  triple_40: 0.0000 (0.0097)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1270/1724]  eta: 0:29:38  lr: 0.000200  loss: 1.2930 (1.9641)  loss_n_40: 0.3230 (0.4168)  loss_n_60: 0.3058 (0.4521)  loss_n_80: 0.3155 (0.4956)  loss_n_100: 0.3488 (0.5373)  triple_100: 0.0000 (0.0237)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0096)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1280/1724]  eta: 0:28:59  lr: 0.000200  loss: 1.2636 (1.9585)  loss_n_40: 0.2988 (0.4159)  loss_n_60: 0.2954 (0.4509)  loss_n_80: 0.3019 (0.4942)  loss_n_100: 0.3099 (0.5357)  triple_100: 0.0000 (0.0235)  triple_80: 0.0000 (0.0146)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0095)  time: 3.9179  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [1290/1724]  eta: 0:28:20  lr: 0.000200  loss: 1.1902 (1.9523)  loss_n_40: 0.2791 (0.4149)  loss_n_60: 0.2782 (0.4495)  loss_n_80: 0.2971 (0.4927)  loss_n_100: 0.3092 (0.5339)  triple_100: 0.0000 (0.0233)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0140)  triple_40: 0.0000 (0.0095)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1300/1724]  eta: 0:27:41  lr: 0.000200  loss: 1.1333 (1.9468)  loss_n_40: 0.2761 (0.4140)  loss_n_60: 0.2748 (0.4483)  loss_n_80: 0.2888 (0.4911)  loss_n_100: 0.2994 (0.5322)  triple_100: 0.0000 (0.0232)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0095)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1310/1724]  eta: 0:27:01  lr: 0.000200  loss: 1.2808 (1.9435)  loss_n_40: 0.2903 (0.4134)  loss_n_60: 0.3049 (0.4475)  loss_n_80: 0.3227 (0.4903)  loss_n_100: 0.3503 (0.5313)  triple_100: 0.0000 (0.0230)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0095)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1320/1724]  eta: 0:26:22  lr: 0.000200  loss: 1.3846 (1.9426)  loss_n_40: 0.3215 (0.4129)  loss_n_60: 0.3206 (0.4467)  loss_n_80: 0.3522 (0.4895)  loss_n_100: 0.3819 (0.5303)  triple_100: 0.0000 (0.0235)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0144)  triple_40: 0.0000 (0.0098)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1330/1724]  eta: 0:25:43  lr: 0.000200  loss: 1.5815 (1.9410)  loss_n_40: 0.3416 (0.4126)  loss_n_60: 0.3660 (0.4464)  loss_n_80: 0.4157 (0.4892)  loss_n_100: 0.4548 (0.5301)  triple_100: 0.0000 (0.0233)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0143)  triple_40: 0.0000 (0.0097)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1340/1724]  eta: 0:25:04  lr: 0.000200  loss: 1.6786 (1.9383)  loss_n_40: 0.3424 (0.4123)  loss_n_60: 0.3835 (0.4459)  loss_n_80: 0.4209 (0.4886)  loss_n_100: 0.4681 (0.5294)  triple_100: 0.0000 (0.0231)  triple_80: 0.0000 (0.0152)  triple_60: 0.0000 (0.0143)  triple_40: 0.0000 (0.0096)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1350/1724]  eta: 0:24:25  lr: 0.000200  loss: 1.4749 (1.9348)  loss_n_40: 0.3410 (0.4117)  loss_n_60: 0.3392 (0.4452)  loss_n_80: 0.3817 (0.4877)  loss_n_100: 0.4051 (0.5284)  triple_100: 0.0000 (0.0230)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0096)  time: 3.9168  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [1360/1724]  eta: 0:23:46  lr: 0.000200  loss: 1.3967 (1.9308)  loss_n_40: 0.3226 (0.4111)  loss_n_60: 0.3346 (0.4444)  loss_n_80: 0.3462 (0.4867)  loss_n_100: 0.3658 (0.5272)  triple_100: 0.0000 (0.0228)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0095)  time: 3.9179  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [1370/1724]  eta: 0:23:06  lr: 0.000200  loss: 1.4009 (1.9308)  loss_n_40: 0.3360 (0.4110)  loss_n_60: 0.3352 (0.4440)  loss_n_80: 0.3550 (0.4864)  loss_n_100: 0.3658 (0.5269)  triple_100: 0.0000 (0.0231)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0143)  triple_40: 0.0000 (0.0101)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [1380/1724]  eta: 0:22:27  lr: 0.000200  loss: 2.8656 (1.9410)  loss_n_40: 0.4747 (0.4119)  loss_n_60: 0.5695 (0.4460)  loss_n_80: 0.7816 (0.4894)  loss_n_100: 0.8924 (0.5312)  triple_100: 0.0000 (0.0230)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0144)  triple_40: 0.0000 (0.0100)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1390/1724]  eta: 0:21:48  lr: 0.000200  loss: 2.9793 (1.9464)  loss_n_40: 0.4985 (0.4124)  loss_n_60: 0.6513 (0.4471)  loss_n_80: 0.8236 (0.4911)  loss_n_100: 1.0187 (0.5339)  triple_100: 0.0000 (0.0228)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0143)  triple_40: 0.0000 (0.0099)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1400/1724]  eta: 0:21:09  lr: 0.000200  loss: 2.4169 (1.9485)  loss_n_40: 0.4402 (0.4127)  loss_n_60: 0.5487 (0.4477)  loss_n_80: 0.6528 (0.4918)  loss_n_100: 0.7562 (0.5347)  triple_100: 0.0000 (0.0226)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0099)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1410/1724]  eta: 0:20:30  lr: 0.000200  loss: 2.0454 (1.9484)  loss_n_40: 0.4117 (0.4127)  loss_n_60: 0.4749 (0.4478)  loss_n_80: 0.5447 (0.4920)  loss_n_100: 0.5994 (0.5349)  triple_100: 0.0000 (0.0225)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0098)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1420/1724]  eta: 0:19:50  lr: 0.000200  loss: 1.8097 (1.9455)  loss_n_40: 0.3859 (0.4122)  loss_n_60: 0.4147 (0.4472)  loss_n_80: 0.4455 (0.4913)  loss_n_100: 0.4725 (0.5341)  triple_100: 0.0000 (0.0223)  triple_80: 0.0000 (0.0146)  triple_60: 0.0000 (0.0140)  triple_40: 0.0000 (0.0097)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1430/1724]  eta: 0:19:11  lr: 0.000200  loss: 1.5163 (1.9431)  loss_n_40: 0.3673 (0.4121)  loss_n_60: 0.3734 (0.4468)  loss_n_80: 0.3827 (0.4906)  loss_n_100: 0.4133 (0.5333)  triple_100: 0.0000 (0.0222)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0139)  triple_40: 0.0000 (0.0097)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1440/1724]  eta: 0:18:32  lr: 0.000200  loss: 1.5003 (1.9396)  loss_n_40: 0.3664 (0.4115)  loss_n_60: 0.3709 (0.4461)  loss_n_80: 0.3923 (0.4898)  loss_n_100: 0.4012 (0.5324)  triple_100: 0.0000 (0.0220)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0138)  triple_40: 0.0000 (0.0096)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1450/1724]  eta: 0:17:53  lr: 0.000200  loss: 1.4334 (1.9364)  loss_n_40: 0.3239 (0.4110)  loss_n_60: 0.3343 (0.4454)  loss_n_80: 0.3593 (0.4890)  loss_n_100: 0.3858 (0.5314)  triple_100: 0.0000 (0.0218)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0137)  triple_40: 0.0000 (0.0095)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1460/1724]  eta: 0:17:14  lr: 0.000200  loss: 1.3731 (1.9323)  loss_n_40: 0.3187 (0.4104)  loss_n_60: 0.3257 (0.4446)  loss_n_80: 0.3401 (0.4880)  loss_n_100: 0.3724 (0.5302)  triple_100: 0.0000 (0.0217)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0136)  triple_40: 0.0000 (0.0095)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1470/1724]  eta: 0:16:35  lr: 0.000200  loss: 1.3681 (1.9291)  loss_n_40: 0.3187 (0.4101)  loss_n_60: 0.3257 (0.4439)  loss_n_80: 0.3401 (0.4872)  loss_n_100: 0.3490 (0.5293)  triple_100: 0.0000 (0.0215)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0136)  triple_40: 0.0000 (0.0094)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1480/1724]  eta: 0:15:55  lr: 0.000200  loss: 1.4050 (1.9265)  loss_n_40: 0.3117 (0.4094)  loss_n_60: 0.3244 (0.4432)  loss_n_80: 0.3524 (0.4864)  loss_n_100: 0.3731 (0.5283)  triple_100: 0.0000 (0.0214)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0139)  triple_40: 0.0000 (0.0097)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1490/1724]  eta: 0:15:16  lr: 0.000200  loss: 1.7342 (1.9273)  loss_n_40: 0.3561 (0.4095)  loss_n_60: 0.4014 (0.4434)  loss_n_80: 0.4371 (0.4867)  loss_n_100: 0.4749 (0.5289)  triple_100: 0.0000 (0.0213)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0138)  triple_40: 0.0000 (0.0097)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1500/1724]  eta: 0:14:37  lr: 0.000200  loss: 1.9430 (1.9277)  loss_n_40: 0.4236 (0.4096)  loss_n_60: 0.4695 (0.4436)  loss_n_80: 0.5126 (0.4869)  loss_n_100: 0.5750 (0.5291)  triple_100: 0.0000 (0.0212)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0137)  triple_40: 0.0000 (0.0096)  time: 3.9154  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1510/1724]  eta: 0:13:58  lr: 0.000200  loss: 1.7746 (1.9260)  loss_n_40: 0.4066 (0.4095)  loss_n_60: 0.4245 (0.4433)  loss_n_80: 0.4684 (0.4866)  loss_n_100: 0.4903 (0.5286)  triple_100: 0.0000 (0.0211)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0136)  triple_40: 0.0000 (0.0096)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1520/1724]  eta: 0:13:19  lr: 0.000200  loss: 1.5568 (1.9234)  loss_n_40: 0.3606 (0.4091)  loss_n_60: 0.3710 (0.4428)  loss_n_80: 0.4058 (0.4860)  loss_n_100: 0.4267 (0.5279)  triple_100: 0.0000 (0.0209)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0135)  triple_40: 0.0000 (0.0095)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1530/1724]  eta: 0:12:40  lr: 0.000200  loss: 1.4695 (1.9199)  loss_n_40: 0.3289 (0.4086)  loss_n_60: 0.3536 (0.4421)  loss_n_80: 0.3782 (0.4851)  loss_n_100: 0.3942 (0.5268)  triple_100: 0.0000 (0.0208)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0134)  triple_40: 0.0000 (0.0094)  time: 3.9162  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [1540/1724]  eta: 0:12:00  lr: 0.000200  loss: 1.3685 (1.9164)  loss_n_40: 0.3170 (0.4081)  loss_n_60: 0.3346 (0.4414)  loss_n_80: 0.3489 (0.4842)  loss_n_100: 0.3620 (0.5257)  triple_100: 0.0000 (0.0206)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0133)  triple_40: 0.0000 (0.0094)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1550/1724]  eta: 0:11:21  lr: 0.000200  loss: 1.2310 (1.9119)  loss_n_40: 0.3022 (0.4074)  loss_n_60: 0.3088 (0.4405)  loss_n_80: 0.3133 (0.4831)  loss_n_100: 0.3249 (0.5244)  triple_100: 0.0000 (0.0205)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0132)  triple_40: 0.0000 (0.0093)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1560/1724]  eta: 0:10:42  lr: 0.000200  loss: 1.2113 (1.9074)  loss_n_40: 0.2805 (0.4066)  loss_n_60: 0.2921 (0.4396)  loss_n_80: 0.2988 (0.4820)  loss_n_100: 0.3044 (0.5230)  triple_100: 0.0000 (0.0204)  triple_80: 0.0000 (0.0134)  triple_60: 0.0000 (0.0131)  triple_40: 0.0000 (0.0092)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1570/1724]  eta: 0:10:03  lr: 0.000200  loss: 1.2113 (1.9037)  loss_n_40: 0.2817 (0.4061)  loss_n_60: 0.2930 (0.4388)  loss_n_80: 0.3087 (0.4810)  loss_n_100: 0.3237 (0.5219)  triple_100: 0.0000 (0.0202)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0131)  triple_40: 0.0000 (0.0092)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1580/1724]  eta: 0:09:24  lr: 0.000200  loss: 1.2298 (1.8996)  loss_n_40: 0.2974 (0.4054)  loss_n_60: 0.2996 (0.4380)  loss_n_80: 0.3194 (0.4800)  loss_n_100: 0.3237 (0.5207)  triple_100: 0.0000 (0.0201)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0130)  triple_40: 0.0000 (0.0091)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1590/1724]  eta: 0:08:44  lr: 0.000200  loss: 1.1712 (1.8949)  loss_n_40: 0.2756 (0.4046)  loss_n_60: 0.2762 (0.4370)  loss_n_80: 0.3060 (0.4789)  loss_n_100: 0.3058 (0.5193)  triple_100: 0.0000 (0.0200)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0129)  triple_40: 0.0000 (0.0091)  time: 3.9169  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:15]  [1600/1724]  eta: 0:08:05  lr: 0.000200  loss: 1.1712 (1.8907)  loss_n_40: 0.2828 (0.4040)  loss_n_60: 0.2857 (0.4361)  loss_n_80: 0.2987 (0.4778)  loss_n_100: 0.3058 (0.5179)  triple_100: 0.0000 (0.0199)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0128)  triple_40: 0.0000 (0.0090)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1610/1724]  eta: 0:07:26  lr: 0.000200  loss: 1.2861 (1.9085)  loss_n_40: 0.3137 (0.4041)  loss_n_60: 0.3130 (0.4368)  loss_n_80: 0.3174 (0.4790)  loss_n_100: 0.3131 (0.5197)  triple_100: 0.0000 (0.0272)  triple_80: 0.0000 (0.0164)  triple_60: 0.0000 (0.0151)  triple_40: 0.0000 (0.0103)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1620/1724]  eta: 0:06:47  lr: 0.000200  loss: 5.1442 (1.9322)  loss_n_40: 0.6596 (0.4072)  loss_n_60: 1.0282 (0.4421)  loss_n_80: 1.3197 (0.4861)  loss_n_100: 1.5697 (0.5277)  triple_100: 0.0000 (0.0273)  triple_80: 0.0000 (0.0166)  triple_60: 0.0000 (0.0150)  triple_40: 0.0000 (0.0103)  time: 3.9154  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1630/1724]  eta: 0:06:08  lr: 0.000200  loss: 5.1442 (1.9500)  loss_n_40: 0.8565 (0.4098)  loss_n_60: 1.1860 (0.4460)  loss_n_80: 1.4525 (0.4911)  loss_n_100: 1.7050 (0.5337)  triple_100: 0.0000 (0.0278)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0149)  triple_40: 0.0000 (0.0102)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1640/1724]  eta: 0:05:29  lr: 0.000200  loss: 4.2264 (1.9614)  loss_n_40: 0.8149 (0.4121)  loss_n_60: 0.9797 (0.4487)  loss_n_80: 1.1403 (0.4944)  loss_n_100: 1.2672 (0.5373)  triple_100: 0.0000 (0.0276)  triple_80: 0.0000 (0.0164)  triple_60: 0.0000 (0.0148)  triple_40: 0.0000 (0.0101)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1650/1724]  eta: 0:04:49  lr: 0.000200  loss: 3.4829 (1.9698)  loss_n_40: 0.7134 (0.4138)  loss_n_60: 0.8185 (0.4508)  loss_n_80: 0.9242 (0.4968)  loss_n_100: 0.9842 (0.5399)  triple_100: 0.0000 (0.0275)  triple_80: 0.0000 (0.0163)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0101)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1660/1724]  eta: 0:04:10  lr: 0.000200  loss: 2.8348 (1.9734)  loss_n_40: 0.6128 (0.4145)  loss_n_60: 0.6646 (0.4517)  loss_n_80: 0.7895 (0.4979)  loss_n_100: 0.8509 (0.5411)  triple_100: 0.0000 (0.0273)  triple_80: 0.0000 (0.0162)  triple_60: 0.0000 (0.0146)  triple_40: 0.0000 (0.0100)  time: 3.9170  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [1670/1724]  eta: 0:03:31  lr: 0.000200  loss: 2.5257 (1.9773)  loss_n_40: 0.5353 (0.4154)  loss_n_60: 0.5945 (0.4528)  loss_n_80: 0.6687 (0.4990)  loss_n_100: 0.7249 (0.5424)  triple_100: 0.0000 (0.0271)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0145)  triple_40: 0.0000 (0.0100)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1680/1724]  eta: 0:02:52  lr: 0.000200  loss: 2.2810 (1.9782)  loss_n_40: 0.4843 (0.4156)  loss_n_60: 0.5529 (0.4531)  loss_n_80: 0.5927 (0.4993)  loss_n_100: 0.6793 (0.5428)  triple_100: 0.0000 (0.0270)  triple_80: 0.0000 (0.0160)  triple_60: 0.0000 (0.0145)  triple_40: 0.0000 (0.0099)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1690/1724]  eta: 0:02:13  lr: 0.000200  loss: 2.1483 (1.9798)  loss_n_40: 0.4600 (0.4160)  loss_n_60: 0.5158 (0.4536)  loss_n_80: 0.5536 (0.4998)  loss_n_100: 0.6136 (0.5434)  triple_100: 0.0000 (0.0268)  triple_80: 0.0000 (0.0159)  triple_60: 0.0000 (0.0144)  triple_40: 0.0000 (0.0098)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1700/1724]  eta: 0:01:34  lr: 0.000200  loss: 1.9866 (1.9793)  loss_n_40: 0.4398 (0.4162)  loss_n_60: 0.4607 (0.4536)  loss_n_80: 0.5009 (0.4996)  loss_n_100: 0.5434 (0.5433)  triple_100: 0.0000 (0.0266)  triple_80: 0.0000 (0.0159)  triple_60: 0.0000 (0.0143)  triple_40: 0.0000 (0.0098)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1710/1724]  eta: 0:00:54  lr: 0.000200  loss: 1.7474 (1.9776)  loss_n_40: 0.4203 (0.4160)  loss_n_60: 0.4141 (0.4533)  loss_n_80: 0.4416 (0.4992)  loss_n_100: 0.4852 (0.5429)  triple_100: 0.0000 (0.0265)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0097)  time: 3.9174  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:15]  [1720/1724]  eta: 0:00:15  lr: 0.000200  loss: 1.5961 (1.9752)  loss_n_40: 0.3731 (0.4157)  loss_n_60: 0.3859 (0.4529)  loss_n_80: 0.4034 (0.4986)  loss_n_100: 0.4339 (0.5422)  triple_100: 0.0000 (0.0263)  triple_80: 0.0000 (0.0157)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0097)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15]  [1723/1724]  eta: 0:00:03  lr: 0.000200  loss: 1.5961 (1.9747)  loss_n_40: 0.3673 (0.4157)  loss_n_60: 0.3859 (0.4528)  loss_n_80: 0.4034 (0.4985)  loss_n_100: 0.4435 (0.5421)  triple_100: 0.0000 (0.0263)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0097)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:15] Total time: 1:52:34 (3.9177 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.5961 (1.9747)  loss_n_40: 0.3673 (0.4157)  loss_n_60: 0.3859 (0.4528)  loss_n_80: 0.4034 (0.4985)  loss_n_100: 0.4435 (0.5421)  triple_100: 0.0000 (0.0263)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0097)\n",
      "Valid: [epoch:15]  [  0/845]  eta: 0:10:22  loss: 1.6060 (1.6060)  loss_n_40: 0.3376 (0.3376)  loss_n_60: 0.3935 (0.3935)  loss_n_80: 0.3964 (0.3964)  loss_n_100: 0.4785 (0.4785)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7366  data: 0.4005  max mem: 46473\n",
      "Valid: [epoch:15]  [ 10/845]  eta: 0:05:09  loss: 1.7052 (1.5683)  loss_n_40: 0.3376 (0.3438)  loss_n_60: 0.4061 (0.3794)  loss_n_80: 0.4187 (0.3975)  loss_n_100: 0.4865 (0.4476)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3705  data: 0.0365  max mem: 46473\n",
      "Valid: [epoch:15]  [ 20/845]  eta: 0:04:51  loss: 1.6772 (1.6240)  loss_n_40: 0.3348 (0.3598)  loss_n_60: 0.3940 (0.3908)  loss_n_80: 0.4187 (0.4114)  loss_n_100: 0.4865 (0.4619)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [ 30/845]  eta: 0:04:42  loss: 1.6772 (1.6965)  loss_n_40: 0.3704 (0.3818)  loss_n_60: 0.4108 (0.4107)  loss_n_80: 0.4437 (0.4282)  loss_n_100: 0.5003 (0.4757)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [ 40/845]  eta: 0:04:36  loss: 1.4851 (1.6383)  loss_n_40: 0.3440 (0.3706)  loss_n_60: 0.3605 (0.3973)  loss_n_80: 0.3545 (0.4143)  loss_n_100: 0.3959 (0.4562)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [ 50/845]  eta: 0:04:31  loss: 1.4209 (1.6071)  loss_n_40: 0.3185 (0.3605)  loss_n_60: 0.3266 (0.3878)  loss_n_80: 0.3529 (0.4074)  loss_n_100: 0.3734 (0.4514)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [ 60/845]  eta: 0:04:27  loss: 1.4992 (1.5978)  loss_n_40: 0.3373 (0.3604)  loss_n_60: 0.3543 (0.3849)  loss_n_80: 0.3774 (0.4046)  loss_n_100: 0.4185 (0.4480)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [ 70/845]  eta: 0:04:23  loss: 1.4926 (1.5840)  loss_n_40: 0.3373 (0.3569)  loss_n_60: 0.3487 (0.3790)  loss_n_80: 0.3738 (0.3990)  loss_n_100: 0.4185 (0.4422)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0070)  time: 0.3336  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [ 80/845]  eta: 0:04:19  loss: 1.4926 (1.5875)  loss_n_40: 0.3366 (0.3566)  loss_n_60: 0.3487 (0.3798)  loss_n_80: 0.3738 (0.4003)  loss_n_100: 0.4337 (0.4448)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0061)  time: 0.3338  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:15]  [ 90/845]  eta: 0:04:15  loss: 1.4634 (1.5777)  loss_n_40: 0.3090 (0.3537)  loss_n_60: 0.3463 (0.3781)  loss_n_80: 0.3825 (0.3981)  loss_n_100: 0.4275 (0.4425)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0054)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [100/845]  eta: 0:04:11  loss: 1.4660 (1.5761)  loss_n_40: 0.3278 (0.3586)  loss_n_60: 0.3463 (0.3770)  loss_n_80: 0.3680 (0.3956)  loss_n_100: 0.4180 (0.4400)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0049)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [110/845]  eta: 0:04:08  loss: 1.4684 (1.5653)  loss_n_40: 0.3371 (0.3552)  loss_n_60: 0.3469 (0.3744)  loss_n_80: 0.3680 (0.3938)  loss_n_100: 0.4180 (0.4375)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0044)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [120/845]  eta: 0:04:04  loss: 1.3657 (1.5529)  loss_n_40: 0.3177 (0.3529)  loss_n_60: 0.3284 (0.3713)  loss_n_80: 0.3454 (0.3907)  loss_n_100: 0.3779 (0.4339)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0041)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [130/845]  eta: 0:04:00  loss: 1.4059 (1.5577)  loss_n_40: 0.3177 (0.3568)  loss_n_60: 0.3314 (0.3719)  loss_n_80: 0.3616 (0.3915)  loss_n_100: 0.3788 (0.4337)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0038)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [140/845]  eta: 0:03:57  loss: 1.4059 (1.5599)  loss_n_40: 0.3181 (0.3591)  loss_n_60: 0.3314 (0.3725)  loss_n_80: 0.3633 (0.3912)  loss_n_100: 0.3964 (0.4336)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0035)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [150/845]  eta: 0:03:53  loss: 1.4159 (1.5576)  loss_n_40: 0.3165 (0.3579)  loss_n_60: 0.3334 (0.3717)  loss_n_80: 0.3585 (0.3909)  loss_n_100: 0.3964 (0.4337)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0033)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [160/845]  eta: 0:03:50  loss: 1.4433 (1.5542)  loss_n_40: 0.3221 (0.3570)  loss_n_60: 0.3361 (0.3710)  loss_n_80: 0.3585 (0.3900)  loss_n_100: 0.4013 (0.4332)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0031)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [170/845]  eta: 0:03:47  loss: 1.5547 (1.5545)  loss_n_40: 0.3403 (0.3557)  loss_n_60: 0.3673 (0.3710)  loss_n_80: 0.3878 (0.3905)  loss_n_100: 0.4130 (0.4344)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0029)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [180/845]  eta: 0:03:43  loss: 1.5788 (1.5553)  loss_n_40: 0.3403 (0.3572)  loss_n_60: 0.3754 (0.3706)  loss_n_80: 0.3911 (0.3910)  loss_n_100: 0.4137 (0.4337)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0027)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [190/845]  eta: 0:03:40  loss: 1.4871 (1.5549)  loss_n_40: 0.3371 (0.3561)  loss_n_60: 0.3574 (0.3706)  loss_n_80: 0.3907 (0.3915)  loss_n_100: 0.4137 (0.4340)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0026)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [200/845]  eta: 0:03:36  loss: 1.3705 (1.5528)  loss_n_40: 0.3314 (0.3569)  loss_n_60: 0.3300 (0.3701)  loss_n_80: 0.3397 (0.3906)  loss_n_100: 0.3764 (0.4327)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0025)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [210/845]  eta: 0:03:33  loss: 1.2295 (1.5451)  loss_n_40: 0.2790 (0.3574)  loss_n_60: 0.2989 (0.3684)  loss_n_80: 0.3251 (0.3878)  loss_n_100: 0.3483 (0.4292)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0023)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [220/845]  eta: 0:03:29  loss: 1.2295 (1.5390)  loss_n_40: 0.2762 (0.3559)  loss_n_60: 0.2972 (0.3670)  loss_n_80: 0.3171 (0.3863)  loss_n_100: 0.3483 (0.4276)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0022)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [230/845]  eta: 0:03:26  loss: 1.3348 (1.5344)  loss_n_40: 0.3283 (0.3546)  loss_n_60: 0.3124 (0.3663)  loss_n_80: 0.3296 (0.3853)  loss_n_100: 0.3517 (0.4261)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0021)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [240/845]  eta: 0:03:23  loss: 1.3661 (1.5331)  loss_n_40: 0.3313 (0.3549)  loss_n_60: 0.3264 (0.3658)  loss_n_80: 0.3497 (0.3853)  loss_n_100: 0.3665 (0.4251)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0020)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [250/845]  eta: 0:03:19  loss: 1.4864 (1.5319)  loss_n_40: 0.3313 (0.3542)  loss_n_60: 0.3449 (0.3655)  loss_n_80: 0.3759 (0.3851)  loss_n_100: 0.3859 (0.4252)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0020)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [260/845]  eta: 0:03:16  loss: 1.5053 (1.5379)  loss_n_40: 0.3370 (0.3559)  loss_n_60: 0.3502 (0.3671)  loss_n_80: 0.3828 (0.3863)  loss_n_100: 0.4218 (0.4266)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0019)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [270/845]  eta: 0:03:12  loss: 1.7331 (1.5501)  loss_n_40: 0.3592 (0.3591)  loss_n_60: 0.4011 (0.3702)  loss_n_80: 0.4234 (0.3892)  loss_n_100: 0.4591 (0.4298)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0018)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [280/845]  eta: 0:03:09  loss: 1.3304 (1.5420)  loss_n_40: 0.3229 (0.3570)  loss_n_60: 0.3190 (0.3683)  loss_n_80: 0.3381 (0.3873)  loss_n_100: 0.3685 (0.4276)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0018)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [290/845]  eta: 0:03:06  loss: 1.3171 (1.5402)  loss_n_40: 0.3043 (0.3571)  loss_n_60: 0.3125 (0.3682)  loss_n_80: 0.3325 (0.3864)  loss_n_100: 0.3554 (0.4268)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0017)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [300/845]  eta: 0:03:02  loss: 1.5731 (1.5424)  loss_n_40: 0.3450 (0.3577)  loss_n_60: 0.3697 (0.3687)  loss_n_80: 0.3969 (0.3875)  loss_n_100: 0.4194 (0.4268)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0016)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [310/845]  eta: 0:02:59  loss: 1.5731 (1.5443)  loss_n_40: 0.3445 (0.3579)  loss_n_60: 0.3697 (0.3692)  loss_n_80: 0.4004 (0.3880)  loss_n_100: 0.4256 (0.4276)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0016)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [320/845]  eta: 0:02:56  loss: 1.5047 (1.5474)  loss_n_40: 0.3313 (0.3586)  loss_n_60: 0.3503 (0.3699)  loss_n_80: 0.3831 (0.3890)  loss_n_100: 0.4369 (0.4284)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0015)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [330/845]  eta: 0:02:52  loss: 1.6675 (1.5552)  loss_n_40: 0.3629 (0.3620)  loss_n_60: 0.3804 (0.3715)  loss_n_80: 0.4126 (0.3905)  loss_n_100: 0.4494 (0.4297)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0015)  time: 0.3342  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:15]  [340/845]  eta: 0:02:49  loss: 1.7292 (1.5578)  loss_n_40: 0.3812 (0.3628)  loss_n_60: 0.4087 (0.3719)  loss_n_80: 0.4410 (0.3911)  loss_n_100: 0.4852 (0.4306)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0014)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [350/845]  eta: 0:02:45  loss: 1.6557 (1.5621)  loss_n_40: 0.3812 (0.3641)  loss_n_60: 0.3925 (0.3731)  loss_n_80: 0.4127 (0.3920)  loss_n_100: 0.4288 (0.4315)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0014)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [360/845]  eta: 0:02:42  loss: 1.6557 (1.5696)  loss_n_40: 0.3769 (0.3659)  loss_n_60: 0.3925 (0.3750)  loss_n_80: 0.4127 (0.3939)  loss_n_100: 0.4482 (0.4334)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0014)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [370/845]  eta: 0:02:39  loss: 1.5717 (1.5704)  loss_n_40: 0.3506 (0.3654)  loss_n_60: 0.3759 (0.3751)  loss_n_80: 0.3887 (0.3944)  loss_n_100: 0.4370 (0.4341)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0013)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [380/845]  eta: 0:02:35  loss: 1.5250 (1.5690)  loss_n_40: 0.3378 (0.3647)  loss_n_60: 0.3528 (0.3747)  loss_n_80: 0.3851 (0.3942)  loss_n_100: 0.4271 (0.4341)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0013)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [390/845]  eta: 0:02:32  loss: 1.5815 (1.5718)  loss_n_40: 0.3458 (0.3656)  loss_n_60: 0.3747 (0.3755)  loss_n_80: 0.3949 (0.3945)  loss_n_100: 0.4662 (0.4350)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0013)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [400/845]  eta: 0:02:29  loss: 1.5991 (1.5753)  loss_n_40: 0.3687 (0.3665)  loss_n_60: 0.3839 (0.3763)  loss_n_80: 0.4044 (0.3954)  loss_n_100: 0.4654 (0.4358)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0012)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [410/845]  eta: 0:02:25  loss: 1.5934 (1.5755)  loss_n_40: 0.3590 (0.3661)  loss_n_60: 0.3839 (0.3765)  loss_n_80: 0.4001 (0.3955)  loss_n_100: 0.4594 (0.4362)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0012)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [420/845]  eta: 0:02:22  loss: 1.5818 (1.5755)  loss_n_40: 0.3328 (0.3658)  loss_n_60: 0.3770 (0.3766)  loss_n_80: 0.3819 (0.3953)  loss_n_100: 0.4410 (0.4366)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0012)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [430/845]  eta: 0:02:19  loss: 1.5649 (1.5726)  loss_n_40: 0.3307 (0.3648)  loss_n_60: 0.3748 (0.3759)  loss_n_80: 0.3675 (0.3947)  loss_n_100: 0.4155 (0.4360)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0011)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [440/845]  eta: 0:02:15  loss: 1.4412 (1.5714)  loss_n_40: 0.3219 (0.3640)  loss_n_60: 0.3373 (0.3757)  loss_n_80: 0.3558 (0.3946)  loss_n_100: 0.3880 (0.4360)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0011)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [450/845]  eta: 0:02:12  loss: 1.3424 (1.5686)  loss_n_40: 0.2800 (0.3631)  loss_n_60: 0.3244 (0.3749)  loss_n_80: 0.3546 (0.3941)  loss_n_100: 0.3835 (0.4354)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0011)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [460/845]  eta: 0:02:08  loss: 1.4838 (1.5705)  loss_n_40: 0.3214 (0.3642)  loss_n_60: 0.3325 (0.3752)  loss_n_80: 0.3560 (0.3943)  loss_n_100: 0.4136 (0.4357)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0011)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [470/845]  eta: 0:02:05  loss: 1.6180 (1.5771)  loss_n_40: 0.3688 (0.3667)  loss_n_60: 0.3755 (0.3767)  loss_n_80: 0.4215 (0.3957)  loss_n_100: 0.4661 (0.4370)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [480/845]  eta: 0:02:02  loss: 1.5441 (1.5766)  loss_n_40: 0.3401 (0.3660)  loss_n_60: 0.3740 (0.3764)  loss_n_80: 0.3966 (0.3958)  loss_n_100: 0.4563 (0.4374)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [490/845]  eta: 0:01:58  loss: 1.4842 (1.5739)  loss_n_40: 0.3334 (0.3652)  loss_n_60: 0.3406 (0.3758)  loss_n_80: 0.3688 (0.3952)  loss_n_100: 0.4195 (0.4367)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [500/845]  eta: 0:01:55  loss: 1.5504 (1.5768)  loss_n_40: 0.3336 (0.3656)  loss_n_60: 0.3530 (0.3765)  loss_n_80: 0.4077 (0.3960)  loss_n_100: 0.4431 (0.4378)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [510/845]  eta: 0:01:52  loss: 1.7004 (1.5790)  loss_n_40: 0.3473 (0.3662)  loss_n_60: 0.4165 (0.3770)  loss_n_80: 0.4255 (0.3966)  loss_n_100: 0.4562 (0.4382)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [520/845]  eta: 0:01:48  loss: 1.3961 (1.5773)  loss_n_40: 0.3431 (0.3661)  loss_n_60: 0.3191 (0.3765)  loss_n_80: 0.3530 (0.3960)  loss_n_100: 0.3940 (0.4377)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0009)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [530/845]  eta: 0:01:45  loss: 1.3590 (1.5761)  loss_n_40: 0.3038 (0.3661)  loss_n_60: 0.3045 (0.3761)  loss_n_80: 0.3450 (0.3955)  loss_n_100: 0.3617 (0.4374)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0009)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [540/845]  eta: 0:01:42  loss: 1.3764 (1.5794)  loss_n_40: 0.3414 (0.3666)  loss_n_60: 0.3169 (0.3769)  loss_n_80: 0.3433 (0.3965)  loss_n_100: 0.3748 (0.4386)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0009)  time: 0.3336  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [550/845]  eta: 0:01:38  loss: 1.3764 (1.5758)  loss_n_40: 0.3414 (0.3654)  loss_n_60: 0.3185 (0.3761)  loss_n_80: 0.3433 (0.3956)  loss_n_100: 0.3768 (0.4378)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0009)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [560/845]  eta: 0:01:35  loss: 1.2357 (1.5710)  loss_n_40: 0.3022 (0.3643)  loss_n_60: 0.2835 (0.3749)  loss_n_80: 0.3147 (0.3945)  loss_n_100: 0.3385 (0.4365)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0009)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [570/845]  eta: 0:01:32  loss: 1.2757 (1.5707)  loss_n_40: 0.3146 (0.3640)  loss_n_60: 0.2978 (0.3748)  loss_n_80: 0.3297 (0.3945)  loss_n_100: 0.3410 (0.4366)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0009)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [580/845]  eta: 0:01:28  loss: 1.5171 (1.5729)  loss_n_40: 0.3273 (0.3642)  loss_n_60: 0.3565 (0.3753)  loss_n_80: 0.3890 (0.3951)  loss_n_100: 0.4390 (0.4374)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0008)  time: 0.3343  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:15]  [590/845]  eta: 0:01:25  loss: 1.5816 (1.5732)  loss_n_40: 0.3457 (0.3644)  loss_n_60: 0.3786 (0.3753)  loss_n_80: 0.3945 (0.3951)  loss_n_100: 0.4495 (0.4375)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0008)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [600/845]  eta: 0:01:22  loss: 1.5416 (1.5732)  loss_n_40: 0.3457 (0.3647)  loss_n_60: 0.3713 (0.3753)  loss_n_80: 0.3727 (0.3950)  loss_n_100: 0.4291 (0.4373)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0008)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [610/845]  eta: 0:01:18  loss: 1.6003 (1.5759)  loss_n_40: 0.3791 (0.3653)  loss_n_60: 0.3906 (0.3759)  loss_n_80: 0.3998 (0.3956)  loss_n_100: 0.4478 (0.4383)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0008)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [620/845]  eta: 0:01:15  loss: 1.6643 (1.5770)  loss_n_40: 0.3838 (0.3652)  loss_n_60: 0.3921 (0.3762)  loss_n_80: 0.4088 (0.3959)  loss_n_100: 0.4478 (0.4388)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0008)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [630/845]  eta: 0:01:11  loss: 1.6643 (1.5790)  loss_n_40: 0.3674 (0.3658)  loss_n_60: 0.3926 (0.3766)  loss_n_80: 0.4088 (0.3964)  loss_n_100: 0.4478 (0.4394)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0008)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [640/845]  eta: 0:01:08  loss: 1.5678 (1.5783)  loss_n_40: 0.3605 (0.3654)  loss_n_60: 0.3926 (0.3764)  loss_n_80: 0.3878 (0.3964)  loss_n_100: 0.4093 (0.4393)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0008)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [650/845]  eta: 0:01:05  loss: 1.2947 (1.5761)  loss_n_40: 0.3193 (0.3648)  loss_n_60: 0.3000 (0.3759)  loss_n_80: 0.3281 (0.3960)  loss_n_100: 0.3485 (0.4387)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0008)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [660/845]  eta: 0:01:01  loss: 1.3685 (1.5736)  loss_n_40: 0.2989 (0.3641)  loss_n_60: 0.3302 (0.3753)  loss_n_80: 0.3588 (0.3954)  loss_n_100: 0.3684 (0.4381)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0007)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [670/845]  eta: 0:00:58  loss: 1.5224 (1.5740)  loss_n_40: 0.3268 (0.3642)  loss_n_60: 0.3503 (0.3752)  loss_n_80: 0.3749 (0.3956)  loss_n_100: 0.4530 (0.4382)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0007)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [680/845]  eta: 0:00:55  loss: 1.5985 (1.5728)  loss_n_40: 0.3478 (0.3640)  loss_n_60: 0.3885 (0.3750)  loss_n_80: 0.3927 (0.3952)  loss_n_100: 0.4271 (0.4379)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0007)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [690/845]  eta: 0:00:51  loss: 1.5090 (1.5724)  loss_n_40: 0.3310 (0.3637)  loss_n_60: 0.3443 (0.3749)  loss_n_80: 0.3889 (0.3952)  loss_n_100: 0.4218 (0.4379)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0007)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [700/845]  eta: 0:00:48  loss: 1.5967 (1.5734)  loss_n_40: 0.3310 (0.3641)  loss_n_60: 0.3697 (0.3753)  loss_n_80: 0.3889 (0.3953)  loss_n_100: 0.4024 (0.4380)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0007)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [710/845]  eta: 0:00:45  loss: 1.5967 (1.5753)  loss_n_40: 0.3473 (0.3650)  loss_n_60: 0.3729 (0.3759)  loss_n_80: 0.3945 (0.3957)  loss_n_100: 0.4130 (0.4380)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0007)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [720/845]  eta: 0:00:41  loss: 1.5571 (1.5749)  loss_n_40: 0.3268 (0.3652)  loss_n_60: 0.3608 (0.3757)  loss_n_80: 0.3820 (0.3956)  loss_n_100: 0.4118 (0.4378)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0007)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [730/845]  eta: 0:00:38  loss: 1.4253 (1.5778)  loss_n_40: 0.3316 (0.3656)  loss_n_60: 0.3349 (0.3762)  loss_n_80: 0.3527 (0.3964)  loss_n_100: 0.3746 (0.4389)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0007)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [740/845]  eta: 0:00:35  loss: 1.3731 (1.5779)  loss_n_40: 0.3316 (0.3657)  loss_n_60: 0.3239 (0.3762)  loss_n_80: 0.3478 (0.3963)  loss_n_100: 0.3696 (0.4387)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0011)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [750/845]  eta: 0:00:31  loss: 1.5461 (1.5801)  loss_n_40: 0.3401 (0.3661)  loss_n_60: 0.3672 (0.3767)  loss_n_80: 0.3964 (0.3969)  loss_n_100: 0.4422 (0.4394)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0011)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [760/845]  eta: 0:00:28  loss: 1.5770 (1.5791)  loss_n_40: 0.3525 (0.3659)  loss_n_60: 0.3672 (0.3764)  loss_n_80: 0.3978 (0.3967)  loss_n_100: 0.4426 (0.4390)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0011)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [770/845]  eta: 0:00:25  loss: 1.5510 (1.5819)  loss_n_40: 0.3399 (0.3669)  loss_n_60: 0.3618 (0.3774)  loss_n_80: 0.3923 (0.3971)  loss_n_100: 0.4426 (0.4395)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0011)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [780/845]  eta: 0:00:21  loss: 1.2978 (1.5783)  loss_n_40: 0.3045 (0.3659)  loss_n_60: 0.3110 (0.3765)  loss_n_80: 0.3332 (0.3963)  loss_n_100: 0.3594 (0.4386)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0011)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [790/845]  eta: 0:00:18  loss: 1.2486 (1.5789)  loss_n_40: 0.3025 (0.3658)  loss_n_60: 0.3021 (0.3766)  loss_n_80: 0.3332 (0.3965)  loss_n_100: 0.3515 (0.4389)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [800/845]  eta: 0:00:15  loss: 1.4856 (1.5778)  loss_n_40: 0.3240 (0.3652)  loss_n_60: 0.3438 (0.3763)  loss_n_80: 0.3732 (0.3963)  loss_n_100: 0.4391 (0.4389)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [810/845]  eta: 0:00:11  loss: 1.5917 (1.5790)  loss_n_40: 0.3442 (0.3652)  loss_n_60: 0.3847 (0.3766)  loss_n_80: 0.3922 (0.3967)  loss_n_100: 0.4613 (0.4395)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [820/845]  eta: 0:00:08  loss: 1.5717 (1.5773)  loss_n_40: 0.3442 (0.3649)  loss_n_60: 0.3726 (0.3762)  loss_n_80: 0.3922 (0.3963)  loss_n_100: 0.3926 (0.4389)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [830/845]  eta: 0:00:05  loss: 1.5217 (1.5782)  loss_n_40: 0.3449 (0.3648)  loss_n_60: 0.3589 (0.3764)  loss_n_80: 0.3815 (0.3966)  loss_n_100: 0.3936 (0.4394)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3343  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:15]  [840/845]  eta: 0:00:01  loss: 1.6049 (1.5804)  loss_n_40: 0.3527 (0.3654)  loss_n_60: 0.3759 (0.3769)  loss_n_80: 0.4094 (0.3972)  loss_n_100: 0.4415 (0.4399)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15]  [844/845]  eta: 0:00:00  loss: 1.5782 (1.5790)  loss_n_40: 0.3527 (0.3651)  loss_n_60: 0.3602 (0.3766)  loss_n_80: 0.4003 (0.3968)  loss_n_100: 0.4286 (0.4395)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:15] Total time: 0:04:42 (0.3348 s / it)\n",
      "Averaged stats: loss: 1.5782 (1.5790)  loss_n_40: 0.3527 (0.3651)  loss_n_60: 0.3602 (0.3766)  loss_n_80: 0.4003 (0.3968)  loss_n_100: 0.4286 (0.4395)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_15_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.440%\n",
      "Min loss_n_100: 0.440\n",
      "Best Epoch: 15.000\n",
      "Train: [epoch:16]  [   0/1724]  eta: 1:59:45  lr: 0.000200  loss: 1.5153 (1.5153)  loss_n_40: 0.3391 (0.3391)  loss_n_60: 0.3674 (0.3674)  loss_n_80: 0.3865 (0.3865)  loss_n_100: 0.4224 (0.4224)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1677  data: 0.3918  max mem: 46473\n",
      "Train: [epoch:16]  [  10/1724]  eta: 1:52:32  lr: 0.000200  loss: 1.6196 (1.6811)  loss_n_40: 0.3645 (0.3801)  loss_n_60: 0.3999 (0.4066)  loss_n_80: 0.4213 (0.4226)  loss_n_100: 0.4774 (0.4632)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0000)  time: 3.9397  data: 0.0358  max mem: 46473\n",
      "Train: [epoch:16]  [  20/1724]  eta: 1:51:34  lr: 0.000200  loss: 1.6631 (1.6917)  loss_n_40: 0.3869 (0.3850)  loss_n_60: 0.3999 (0.4073)  loss_n_80: 0.4286 (0.4243)  loss_n_100: 0.4774 (0.4706)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0000)  time: 3.9168  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [  30/1724]  eta: 1:50:48  lr: 0.000200  loss: 1.5486 (1.6335)  loss_n_40: 0.3607 (0.3725)  loss_n_60: 0.3767 (0.3910)  loss_n_80: 0.3930 (0.4105)  loss_n_100: 0.4401 (0.4549)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0030)  triple_40: 0.0000 (0.0016)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [  40/1724]  eta: 1:50:06  lr: 0.000200  loss: 1.5344 (1.6265)  loss_n_40: 0.3369 (0.3726)  loss_n_60: 0.3619 (0.3888)  loss_n_80: 0.3930 (0.4112)  loss_n_100: 0.4293 (0.4503)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0012)  time: 3.9174  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [  50/1724]  eta: 1:49:25  lr: 0.000200  loss: 1.5126 (1.6105)  loss_n_40: 0.3555 (0.3710)  loss_n_60: 0.3612 (0.3856)  loss_n_80: 0.3917 (0.4074)  loss_n_100: 0.4207 (0.4436)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0018)  triple_40: 0.0000 (0.0010)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [  60/1724]  eta: 1:48:44  lr: 0.000200  loss: 1.4008 (1.5824)  loss_n_40: 0.3252 (0.3653)  loss_n_60: 0.3337 (0.3791)  loss_n_80: 0.3668 (0.4003)  loss_n_100: 0.3818 (0.4336)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0015)  triple_40: 0.0000 (0.0025)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [  70/1724]  eta: 1:48:04  lr: 0.000200  loss: 1.4008 (1.5692)  loss_n_40: 0.3253 (0.3622)  loss_n_60: 0.3497 (0.3767)  loss_n_80: 0.3668 (0.3978)  loss_n_100: 0.3750 (0.4290)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0013)  triple_40: 0.0000 (0.0022)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [  80/1724]  eta: 1:47:24  lr: 0.000200  loss: 1.3668 (1.5512)  loss_n_40: 0.3378 (0.3594)  loss_n_60: 0.3484 (0.3727)  loss_n_80: 0.3429 (0.3928)  loss_n_100: 0.3564 (0.4231)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0012)  triple_40: 0.0000 (0.0019)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [  90/1724]  eta: 1:46:44  lr: 0.000200  loss: 1.3800 (1.5634)  loss_n_40: 0.3482 (0.3588)  loss_n_60: 0.3353 (0.3711)  loss_n_80: 0.3564 (0.3912)  loss_n_100: 0.3714 (0.4195)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0048)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0152)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 100/1724]  eta: 1:46:04  lr: 0.000200  loss: 1.5324 (1.5861)  loss_n_40: 0.3549 (0.3628)  loss_n_60: 0.3565 (0.3757)  loss_n_80: 0.3955 (0.3974)  loss_n_100: 0.4344 (0.4297)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0043)  triple_60: 0.0000 (0.0025)  triple_40: 0.0000 (0.0137)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 110/1724]  eta: 1:45:25  lr: 0.000200  loss: 1.7744 (1.6087)  loss_n_40: 0.3971 (0.3661)  loss_n_60: 0.4086 (0.3808)  loss_n_80: 0.4565 (0.4046)  loss_n_100: 0.5232 (0.4385)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0039)  triple_60: 0.0000 (0.0022)  triple_40: 0.0000 (0.0125)  time: 3.9147  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 120/1724]  eta: 1:44:45  lr: 0.000200  loss: 1.7441 (1.6144)  loss_n_40: 0.3790 (0.3663)  loss_n_60: 0.4086 (0.3819)  loss_n_80: 0.4500 (0.4064)  loss_n_100: 0.4917 (0.4402)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0115)  time: 3.9157  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 130/1724]  eta: 1:44:06  lr: 0.000200  loss: 1.5212 (1.6023)  loss_n_40: 0.3470 (0.3642)  loss_n_60: 0.3689 (0.3795)  loss_n_80: 0.3852 (0.4043)  loss_n_100: 0.4203 (0.4363)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0055)  triple_60: 0.0000 (0.0019)  triple_40: 0.0000 (0.0106)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 140/1724]  eta: 1:43:26  lr: 0.000200  loss: 1.4280 (1.5979)  loss_n_40: 0.3500 (0.3655)  loss_n_60: 0.3447 (0.3789)  loss_n_80: 0.3661 (0.4032)  loss_n_100: 0.3791 (0.4336)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0018)  triple_40: 0.0000 (0.0098)  time: 3.9168  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [ 150/1724]  eta: 1:42:47  lr: 0.000200  loss: 1.4196 (1.5809)  loss_n_40: 0.3500 (0.3622)  loss_n_60: 0.3447 (0.3753)  loss_n_80: 0.3540 (0.3991)  loss_n_100: 0.3680 (0.4287)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0048)  triple_60: 0.0000 (0.0016)  triple_40: 0.0000 (0.0092)  time: 3.9166  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [ 160/1724]  eta: 1:42:08  lr: 0.000200  loss: 1.3045 (1.5628)  loss_n_40: 0.3029 (0.3588)  loss_n_60: 0.3089 (0.3714)  loss_n_80: 0.3334 (0.3947)  loss_n_100: 0.3513 (0.4233)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0045)  triple_60: 0.0000 (0.0015)  triple_40: 0.0000 (0.0086)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 170/1724]  eta: 1:41:28  lr: 0.000200  loss: 1.2744 (1.5478)  loss_n_40: 0.2999 (0.3566)  loss_n_60: 0.3043 (0.3680)  loss_n_80: 0.3244 (0.3907)  loss_n_100: 0.3513 (0.4187)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0042)  triple_60: 0.0000 (0.0015)  triple_40: 0.0000 (0.0081)  time: 3.9150  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [ 180/1724]  eta: 1:40:49  lr: 0.000200  loss: 1.3013 (1.5331)  loss_n_40: 0.3136 (0.3544)  loss_n_60: 0.3106 (0.3647)  loss_n_80: 0.3147 (0.3868)  loss_n_100: 0.3355 (0.4142)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0040)  triple_60: 0.0000 (0.0014)  triple_40: 0.0000 (0.0077)  time: 3.9155  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 190/1724]  eta: 1:40:09  lr: 0.000200  loss: 1.3013 (1.5229)  loss_n_40: 0.3136 (0.3530)  loss_n_60: 0.3106 (0.3624)  loss_n_80: 0.3126 (0.3844)  loss_n_100: 0.3230 (0.4106)  triple_100: 0.0000 (0.0001)  triple_80: 0.0000 (0.0039)  triple_60: 0.0000 (0.0013)  triple_40: 0.0000 (0.0073)  time: 3.9167  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [ 200/1724]  eta: 1:39:30  lr: 0.000200  loss: 1.3566 (1.5309)  loss_n_40: 0.3297 (0.3533)  loss_n_60: 0.3267 (0.3636)  loss_n_80: 0.3488 (0.3865)  loss_n_100: 0.3683 (0.4136)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0045)  triple_60: 0.0000 (0.0012)  triple_40: 0.0000 (0.0069)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 210/1724]  eta: 1:38:51  lr: 0.000200  loss: 1.5461 (1.5411)  loss_n_40: 0.3666 (0.3540)  loss_n_60: 0.3752 (0.3655)  loss_n_80: 0.3977 (0.3896)  loss_n_100: 0.4442 (0.4178)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0012)  triple_40: 0.0000 (0.0066)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 220/1724]  eta: 1:38:12  lr: 0.000200  loss: 1.5461 (1.5425)  loss_n_40: 0.3432 (0.3535)  loss_n_60: 0.3610 (0.3656)  loss_n_80: 0.4143 (0.3905)  loss_n_100: 0.4455 (0.4193)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0048)  triple_60: 0.0000 (0.0011)  triple_40: 0.0000 (0.0063)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 230/1724]  eta: 1:37:33  lr: 0.000200  loss: 1.4591 (1.5385)  loss_n_40: 0.3195 (0.3529)  loss_n_60: 0.3511 (0.3648)  loss_n_80: 0.3754 (0.3896)  loss_n_100: 0.4028 (0.4182)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0046)  triple_60: 0.0000 (0.0011)  triple_40: 0.0000 (0.0060)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 240/1724]  eta: 1:36:53  lr: 0.000200  loss: 1.4599 (1.5477)  loss_n_40: 0.3443 (0.3538)  loss_n_60: 0.3527 (0.3652)  loss_n_80: 0.3722 (0.3900)  loss_n_100: 0.4021 (0.4183)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0044)  triple_60: 0.0000 (0.0010)  triple_40: 0.0000 (0.0106)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 250/1724]  eta: 1:36:14  lr: 0.000200  loss: 1.4799 (1.5482)  loss_n_40: 0.3512 (0.3547)  loss_n_60: 0.3657 (0.3656)  loss_n_80: 0.3758 (0.3903)  loss_n_100: 0.4039 (0.4180)  triple_100: 0.0000 (0.0042)  triple_80: 0.0000 (0.0042)  triple_60: 0.0000 (0.0010)  triple_40: 0.0000 (0.0102)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 260/1724]  eta: 1:35:35  lr: 0.000200  loss: 1.4699 (1.5445)  loss_n_40: 0.3439 (0.3545)  loss_n_60: 0.3571 (0.3647)  loss_n_80: 0.3758 (0.3897)  loss_n_100: 0.3841 (0.4168)  triple_100: 0.0000 (0.0040)  triple_80: 0.0000 (0.0041)  triple_60: 0.0000 (0.0010)  triple_40: 0.0000 (0.0098)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 270/1724]  eta: 1:34:56  lr: 0.000200  loss: 1.3470 (1.5393)  loss_n_40: 0.3221 (0.3545)  loss_n_60: 0.3179 (0.3635)  loss_n_80: 0.3441 (0.3883)  loss_n_100: 0.3546 (0.4148)  triple_100: 0.0000 (0.0039)  triple_80: 0.0000 (0.0039)  triple_60: 0.0000 (0.0009)  triple_40: 0.0000 (0.0094)  time: 3.9156  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 280/1724]  eta: 1:34:16  lr: 0.000200  loss: 1.3611 (1.5369)  loss_n_40: 0.3669 (0.3560)  loss_n_60: 0.3123 (0.3630)  loss_n_80: 0.3352 (0.3872)  loss_n_100: 0.3504 (0.4129)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0038)  triple_60: 0.0000 (0.0009)  triple_40: 0.0000 (0.0094)  time: 3.9168  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [ 290/1724]  eta: 1:33:37  lr: 0.000200  loss: 1.3323 (1.5293)  loss_n_40: 0.3416 (0.3551)  loss_n_60: 0.3138 (0.3614)  loss_n_80: 0.3301 (0.3851)  loss_n_100: 0.3458 (0.4105)  triple_100: 0.0000 (0.0036)  triple_80: 0.0000 (0.0037)  triple_60: 0.0000 (0.0009)  triple_40: 0.0000 (0.0090)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 300/1724]  eta: 1:32:58  lr: 0.000200  loss: 1.2467 (1.5187)  loss_n_40: 0.3160 (0.3533)  loss_n_60: 0.2983 (0.3591)  loss_n_80: 0.3068 (0.3825)  loss_n_100: 0.3165 (0.4073)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0035)  triple_60: 0.0000 (0.0008)  triple_40: 0.0000 (0.0087)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 310/1724]  eta: 1:32:19  lr: 0.000200  loss: 1.1811 (1.5127)  loss_n_40: 0.2838 (0.3531)  loss_n_60: 0.2858 (0.3578)  loss_n_80: 0.2947 (0.3807)  loss_n_100: 0.3078 (0.4050)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0034)  triple_60: 0.0000 (0.0008)  triple_40: 0.0000 (0.0085)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 320/1724]  eta: 1:31:40  lr: 0.000200  loss: 1.2323 (1.5058)  loss_n_40: 0.3098 (0.3525)  loss_n_60: 0.3008 (0.3562)  loss_n_80: 0.3065 (0.3788)  loss_n_100: 0.3199 (0.4028)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0033)  triple_60: 0.0000 (0.0008)  triple_40: 0.0000 (0.0082)  time: 3.9153  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 330/1724]  eta: 1:31:00  lr: 0.000200  loss: 1.3960 (1.5157)  loss_n_40: 0.3233 (0.3516)  loss_n_60: 0.3178 (0.3565)  loss_n_80: 0.3449 (0.3803)  loss_n_100: 0.3665 (0.4054)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0030)  triple_40: 0.0000 (0.0098)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 340/1724]  eta: 1:30:21  lr: 0.000200  loss: 1.7898 (1.5281)  loss_n_40: 0.3669 (0.3529)  loss_n_60: 0.4064 (0.3583)  loss_n_80: 0.4527 (0.3828)  loss_n_100: 0.5108 (0.4089)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0029)  triple_40: 0.0000 (0.0095)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 350/1724]  eta: 1:29:42  lr: 0.000200  loss: 1.7057 (1.5301)  loss_n_40: 0.3783 (0.3532)  loss_n_60: 0.4001 (0.3587)  loss_n_80: 0.4357 (0.3836)  loss_n_100: 0.4771 (0.4101)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0066)  triple_60: 0.0000 (0.0028)  triple_40: 0.0000 (0.0092)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 360/1724]  eta: 1:29:03  lr: 0.000200  loss: 1.6121 (1.5319)  loss_n_40: 0.3643 (0.3540)  loss_n_60: 0.3812 (0.3595)  loss_n_80: 0.4060 (0.3840)  loss_n_100: 0.4397 (0.4106)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0064)  triple_60: 0.0000 (0.0028)  triple_40: 0.0000 (0.0090)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 370/1724]  eta: 1:28:24  lr: 0.000200  loss: 1.4319 (1.5312)  loss_n_40: 0.3330 (0.3537)  loss_n_60: 0.3445 (0.3592)  loss_n_80: 0.3641 (0.3836)  loss_n_100: 0.3801 (0.4098)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0099)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 380/1724]  eta: 1:27:44  lr: 0.000200  loss: 1.2895 (1.5243)  loss_n_40: 0.3031 (0.3523)  loss_n_60: 0.3127 (0.3578)  loss_n_80: 0.3275 (0.3819)  loss_n_100: 0.3411 (0.4080)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0026)  triple_40: 0.0000 (0.0096)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 390/1724]  eta: 1:27:05  lr: 0.000200  loss: 1.2356 (1.5190)  loss_n_40: 0.2934 (0.3513)  loss_n_60: 0.3009 (0.3569)  loss_n_80: 0.3178 (0.3807)  loss_n_100: 0.3334 (0.4064)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0065)  triple_60: 0.0000 (0.0025)  triple_40: 0.0000 (0.0094)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 400/1724]  eta: 1:26:26  lr: 0.000200  loss: 1.2261 (1.5128)  loss_n_40: 0.2901 (0.3502)  loss_n_60: 0.2922 (0.3556)  loss_n_80: 0.3102 (0.3792)  loss_n_100: 0.3253 (0.4047)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0064)  triple_60: 0.0000 (0.0025)  triple_40: 0.0000 (0.0092)  time: 3.9169  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [ 410/1724]  eta: 1:25:47  lr: 0.000200  loss: 1.2544 (1.5182)  loss_n_40: 0.3053 (0.3510)  loss_n_60: 0.2943 (0.3557)  loss_n_80: 0.3102 (0.3790)  loss_n_100: 0.3373 (0.4043)  triple_100: 0.0000 (0.0071)  triple_80: 0.0000 (0.0073)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0089)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 420/1724]  eta: 1:25:08  lr: 0.000200  loss: 1.9095 (1.5371)  loss_n_40: 0.3970 (0.3527)  loss_n_60: 0.4386 (0.3596)  loss_n_80: 0.4617 (0.3848)  loss_n_100: 0.4828 (0.4120)  triple_100: 0.0000 (0.0074)  triple_80: 0.0000 (0.0071)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0087)  time: 3.9178  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [ 430/1724]  eta: 1:24:29  lr: 0.000200  loss: 1.9296 (1.5427)  loss_n_40: 0.3970 (0.3530)  loss_n_60: 0.4386 (0.3606)  loss_n_80: 0.5344 (0.3869)  loss_n_100: 0.6173 (0.4149)  triple_100: 0.0000 (0.0072)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0085)  time: 3.9194  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [ 440/1724]  eta: 1:23:49  lr: 0.000200  loss: 1.6373 (1.5430)  loss_n_40: 0.3529 (0.3531)  loss_n_60: 0.3847 (0.3608)  loss_n_80: 0.4233 (0.3871)  loss_n_100: 0.4609 (0.4153)  triple_100: 0.0000 (0.0070)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0083)  time: 3.9191  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [ 450/1724]  eta: 1:23:10  lr: 0.000200  loss: 1.4247 (1.5400)  loss_n_40: 0.3462 (0.3526)  loss_n_60: 0.3407 (0.3602)  loss_n_80: 0.3568 (0.3865)  loss_n_100: 0.3840 (0.4147)  triple_100: 0.0000 (0.0069)  triple_80: 0.0000 (0.0066)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0082)  time: 3.9185  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [ 460/1724]  eta: 1:22:31  lr: 0.000200  loss: 1.2998 (1.5335)  loss_n_40: 0.3004 (0.3513)  loss_n_60: 0.3077 (0.3588)  loss_n_80: 0.3328 (0.3849)  loss_n_100: 0.3627 (0.4130)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0065)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0080)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 470/1724]  eta: 1:21:52  lr: 0.000200  loss: 1.2448 (1.5281)  loss_n_40: 0.2978 (0.3505)  loss_n_60: 0.2955 (0.3577)  loss_n_80: 0.3073 (0.3836)  loss_n_100: 0.3207 (0.4114)  triple_100: 0.0000 (0.0066)  triple_80: 0.0000 (0.0063)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0078)  time: 3.9177  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [ 480/1724]  eta: 1:21:13  lr: 0.000200  loss: 1.2037 (1.5206)  loss_n_40: 0.2929 (0.3492)  loss_n_60: 0.2880 (0.3561)  loss_n_80: 0.3021 (0.3817)  loss_n_100: 0.3158 (0.4092)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0076)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 490/1724]  eta: 1:20:34  lr: 0.000200  loss: 1.2084 (1.6042)  loss_n_40: 0.3142 (0.3503)  loss_n_60: 0.2930 (0.3579)  loss_n_80: 0.3066 (0.3841)  loss_n_100: 0.3159 (0.4120)  triple_100: 0.0000 (0.0303)  triple_80: 0.0000 (0.0280)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0218)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 500/1724]  eta: 1:19:54  lr: 0.000200  loss: 4.9274 (1.7532)  loss_n_40: 0.6776 (0.3615)  loss_n_60: 0.9224 (0.3761)  loss_n_80: 1.2240 (0.4079)  loss_n_100: 1.5400 (0.4421)  triple_100: 0.0000 (0.0678)  triple_80: 0.0000 (0.0554)  triple_60: 0.0000 (0.0211)  triple_40: 0.0000 (0.0213)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 510/1724]  eta: 1:19:15  lr: 0.000200  loss: 5.8562 (1.8343)  loss_n_40: 0.9843 (0.3758)  loss_n_60: 1.3294 (0.3950)  loss_n_80: 1.5623 (0.4305)  loss_n_100: 1.8368 (0.4706)  triple_100: 0.0000 (0.0665)  triple_80: 0.0000 (0.0543)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0209)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 520/1724]  eta: 1:18:36  lr: 0.000200  loss: 5.3634 (1.8938)  loss_n_40: 0.8967 (0.3838)  loss_n_60: 1.1592 (0.4080)  loss_n_80: 1.4297 (0.4480)  loss_n_100: 1.7609 (0.4947)  triple_100: 0.0000 (0.0652)  triple_80: 0.0000 (0.0533)  triple_60: 0.0000 (0.0203)  triple_40: 0.0000 (0.0205)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 530/1724]  eta: 1:17:57  lr: 0.000200  loss: 4.3375 (1.9301)  loss_n_40: 0.7303 (0.3898)  loss_n_60: 0.9391 (0.4159)  loss_n_80: 1.1143 (0.4587)  loss_n_100: 1.4594 (0.5093)  triple_100: 0.0000 (0.0640)  triple_80: 0.0000 (0.0523)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0201)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 540/1724]  eta: 1:17:18  lr: 0.000200  loss: 3.5239 (1.9535)  loss_n_40: 0.6030 (0.3939)  loss_n_60: 0.7805 (0.4217)  loss_n_80: 0.9593 (0.4660)  loss_n_100: 1.0999 (0.5185)  triple_100: 0.0000 (0.0628)  triple_80: 0.0000 (0.0513)  triple_60: 0.0000 (0.0196)  triple_40: 0.0000 (0.0198)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 550/1724]  eta: 1:16:39  lr: 0.000200  loss: 2.9199 (1.9644)  loss_n_40: 0.5257 (0.3962)  loss_n_60: 0.6593 (0.4247)  loss_n_80: 0.7504 (0.4698)  loss_n_100: 0.8754 (0.5231)  triple_100: 0.0000 (0.0617)  triple_80: 0.0000 (0.0504)  triple_60: 0.0000 (0.0192)  triple_40: 0.0000 (0.0194)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 560/1724]  eta: 1:15:59  lr: 0.000200  loss: 2.4714 (1.9796)  loss_n_40: 0.5303 (0.4016)  loss_n_60: 0.5722 (0.4291)  loss_n_80: 0.6424 (0.4738)  loss_n_100: 0.7185 (0.5272)  triple_100: 0.0000 (0.0606)  triple_80: 0.0000 (0.0495)  triple_60: 0.0000 (0.0189)  triple_40: 0.0000 (0.0190)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 570/1724]  eta: 1:15:20  lr: 0.000200  loss: 2.4304 (1.9850)  loss_n_40: 0.5303 (0.4039)  loss_n_60: 0.5722 (0.4308)  loss_n_80: 0.6023 (0.4757)  loss_n_100: 0.6726 (0.5293)  triple_100: 0.0000 (0.0595)  triple_80: 0.0000 (0.0486)  triple_60: 0.0000 (0.0185)  triple_40: 0.0000 (0.0187)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 580/1724]  eta: 1:14:41  lr: 0.000200  loss: 2.1254 (1.9877)  loss_n_40: 0.4519 (0.4057)  loss_n_60: 0.4912 (0.4319)  loss_n_80: 0.5407 (0.4766)  loss_n_100: 0.5863 (0.5303)  triple_100: 0.0000 (0.0585)  triple_80: 0.0000 (0.0478)  triple_60: 0.0000 (0.0182)  triple_40: 0.0000 (0.0187)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 590/1724]  eta: 1:14:02  lr: 0.000200  loss: 2.2025 (1.9941)  loss_n_40: 0.4579 (0.4079)  loss_n_60: 0.5040 (0.4337)  loss_n_80: 0.5649 (0.4788)  loss_n_100: 0.6479 (0.5331)  triple_100: 0.0000 (0.0575)  triple_80: 0.0000 (0.0470)  triple_60: 0.0000 (0.0179)  triple_40: 0.0000 (0.0184)  time: 3.9174  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [ 600/1724]  eta: 1:13:23  lr: 0.000200  loss: 2.1511 (1.9932)  loss_n_40: 0.4451 (0.4085)  loss_n_60: 0.4954 (0.4340)  loss_n_80: 0.5452 (0.4791)  loss_n_100: 0.6037 (0.5332)  triple_100: 0.0000 (0.0566)  triple_80: 0.0000 (0.0462)  triple_60: 0.0000 (0.0176)  triple_40: 0.0000 (0.0181)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 610/1724]  eta: 1:12:44  lr: 0.000200  loss: 1.9254 (1.9926)  loss_n_40: 0.4298 (0.4090)  loss_n_60: 0.4459 (0.4343)  loss_n_80: 0.4860 (0.4794)  loss_n_100: 0.5434 (0.5335)  triple_100: 0.0000 (0.0556)  triple_80: 0.0000 (0.0456)  triple_60: 0.0000 (0.0173)  triple_40: 0.0000 (0.0178)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 620/1724]  eta: 1:12:04  lr: 0.000200  loss: 1.8318 (1.9888)  loss_n_40: 0.4153 (0.4093)  loss_n_60: 0.4347 (0.4340)  loss_n_80: 0.4568 (0.4787)  loss_n_100: 0.5036 (0.5326)  triple_100: 0.0000 (0.0547)  triple_80: 0.0000 (0.0448)  triple_60: 0.0000 (0.0170)  triple_40: 0.0000 (0.0175)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 630/1724]  eta: 1:11:25  lr: 0.000200  loss: 1.6706 (1.9832)  loss_n_40: 0.3781 (0.4087)  loss_n_60: 0.3896 (0.4333)  loss_n_80: 0.4091 (0.4778)  loss_n_100: 0.4370 (0.5314)  triple_100: 0.0000 (0.0539)  triple_80: 0.0000 (0.0441)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0172)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 640/1724]  eta: 1:10:46  lr: 0.000200  loss: 1.5348 (1.9749)  loss_n_40: 0.3548 (0.4076)  loss_n_60: 0.3637 (0.4319)  loss_n_80: 0.3942 (0.4761)  loss_n_100: 0.4231 (0.5292)  triple_100: 0.0000 (0.0530)  triple_80: 0.0000 (0.0434)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0170)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 650/1724]  eta: 1:10:07  lr: 0.000200  loss: 1.6818 (1.9785)  loss_n_40: 0.3681 (0.4076)  loss_n_60: 0.3915 (0.4325)  loss_n_80: 0.4306 (0.4768)  loss_n_100: 0.4751 (0.5304)  triple_100: 0.0000 (0.0528)  triple_80: 0.0000 (0.0435)  triple_60: 0.0000 (0.0171)  triple_40: 0.0000 (0.0177)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 660/1724]  eta: 1:09:28  lr: 0.000200  loss: 2.0497 (1.9806)  loss_n_40: 0.4030 (0.4079)  loss_n_60: 0.4852 (0.4335)  loss_n_80: 0.5389 (0.4779)  loss_n_100: 0.6116 (0.5320)  triple_100: 0.0000 (0.0520)  triple_80: 0.0000 (0.0428)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0174)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 670/1724]  eta: 1:08:49  lr: 0.000200  loss: 2.1147 (1.9822)  loss_n_40: 0.4580 (0.4096)  loss_n_60: 0.5021 (0.4347)  loss_n_80: 0.5211 (0.4783)  loss_n_100: 0.5874 (0.5324)  triple_100: 0.0000 (0.0513)  triple_80: 0.0000 (0.0422)  triple_60: 0.0000 (0.0166)  triple_40: 0.0000 (0.0172)  time: 3.9174  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [ 680/1724]  eta: 1:08:09  lr: 0.000200  loss: 1.8604 (1.9801)  loss_n_40: 0.4600 (0.4094)  loss_n_60: 0.4800 (0.4350)  loss_n_80: 0.4669 (0.4782)  loss_n_100: 0.5072 (0.5321)  triple_100: 0.0000 (0.0505)  triple_80: 0.0000 (0.0416)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0169)  time: 3.9184  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [ 690/1724]  eta: 1:07:30  lr: 0.000200  loss: 1.7186 (1.9762)  loss_n_40: 0.3623 (0.4092)  loss_n_60: 0.4201 (0.4347)  loss_n_80: 0.4518 (0.4776)  loss_n_100: 0.4868 (0.5312)  triple_100: 0.0000 (0.0498)  triple_80: 0.0000 (0.0410)  triple_60: 0.0000 (0.0161)  triple_40: 0.0000 (0.0167)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 700/1724]  eta: 1:06:51  lr: 0.000200  loss: 1.5877 (1.9702)  loss_n_40: 0.3755 (0.4088)  loss_n_60: 0.3907 (0.4340)  loss_n_80: 0.4070 (0.4762)  loss_n_100: 0.4314 (0.5294)  triple_100: 0.0000 (0.0491)  triple_80: 0.0000 (0.0404)  triple_60: 0.0000 (0.0159)  triple_40: 0.0000 (0.0164)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 710/1724]  eta: 1:06:12  lr: 0.000200  loss: 1.5759 (1.9673)  loss_n_40: 0.3902 (0.4091)  loss_n_60: 0.3887 (0.4340)  loss_n_80: 0.3933 (0.4757)  loss_n_100: 0.3961 (0.5285)  triple_100: 0.0000 (0.0484)  triple_80: 0.0000 (0.0398)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0162)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 720/1724]  eta: 1:05:33  lr: 0.000200  loss: 1.4607 (1.9608)  loss_n_40: 0.3528 (0.4085)  loss_n_60: 0.3486 (0.4329)  loss_n_80: 0.3900 (0.4743)  loss_n_100: 0.4031 (0.5267)  triple_100: 0.0000 (0.0477)  triple_80: 0.0000 (0.0393)  triple_60: 0.0000 (0.0154)  triple_40: 0.0000 (0.0160)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 730/1724]  eta: 1:04:54  lr: 0.000200  loss: 1.3940 (1.9526)  loss_n_40: 0.3190 (0.4075)  loss_n_60: 0.3341 (0.4316)  loss_n_80: 0.3436 (0.4723)  loss_n_100: 0.3722 (0.5244)  triple_100: 0.0000 (0.0471)  triple_80: 0.0000 (0.0387)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0158)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 740/1724]  eta: 1:04:14  lr: 0.000200  loss: 1.4307 (1.9457)  loss_n_40: 0.3234 (0.4068)  loss_n_60: 0.3271 (0.4304)  loss_n_80: 0.3382 (0.4708)  loss_n_100: 0.3637 (0.5224)  triple_100: 0.0000 (0.0464)  triple_80: 0.0000 (0.0382)  triple_60: 0.0000 (0.0151)  triple_40: 0.0000 (0.0155)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 750/1724]  eta: 1:03:35  lr: 0.000200  loss: 1.4700 (1.9396)  loss_n_40: 0.3412 (0.4062)  loss_n_60: 0.3375 (0.4293)  loss_n_80: 0.3631 (0.4695)  loss_n_100: 0.3852 (0.5209)  triple_100: 0.0000 (0.0458)  triple_80: 0.0000 (0.0377)  triple_60: 0.0000 (0.0149)  triple_40: 0.0000 (0.0153)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 760/1724]  eta: 1:02:56  lr: 0.000200  loss: 1.3446 (1.9305)  loss_n_40: 0.3367 (0.4049)  loss_n_60: 0.3178 (0.4276)  loss_n_80: 0.3415 (0.4674)  loss_n_100: 0.3594 (0.5183)  triple_100: 0.0000 (0.0452)  triple_80: 0.0000 (0.0372)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0151)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 770/1724]  eta: 1:02:17  lr: 0.000200  loss: 1.4907 (1.9363)  loss_n_40: 0.3384 (0.4049)  loss_n_60: 0.3582 (0.4282)  loss_n_80: 0.3717 (0.4682)  loss_n_100: 0.3998 (0.5194)  triple_100: 0.0000 (0.0465)  triple_80: 0.0000 (0.0390)  triple_60: 0.0000 (0.0151)  triple_40: 0.0000 (0.0149)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 780/1724]  eta: 1:01:38  lr: 0.000200  loss: 2.2060 (1.9395)  loss_n_40: 0.4085 (0.4051)  loss_n_60: 0.5107 (0.4292)  loss_n_80: 0.5868 (0.4697)  loss_n_100: 0.6499 (0.5213)  triple_100: 0.0000 (0.0460)  triple_80: 0.0000 (0.0385)  triple_60: 0.0000 (0.0149)  triple_40: 0.0000 (0.0147)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 790/1724]  eta: 1:00:58  lr: 0.000200  loss: 2.2258 (1.9435)  loss_n_40: 0.4190 (0.4058)  loss_n_60: 0.5142 (0.4306)  loss_n_80: 0.5990 (0.4712)  loss_n_100: 0.6678 (0.5232)  triple_100: 0.0000 (0.0454)  triple_80: 0.0000 (0.0380)  triple_60: 0.0000 (0.0148)  triple_40: 0.0000 (0.0146)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 800/1724]  eta: 1:00:19  lr: 0.000200  loss: 2.0812 (1.9427)  loss_n_40: 0.4089 (0.4058)  loss_n_60: 0.4835 (0.4307)  loss_n_80: 0.5289 (0.4714)  loss_n_100: 0.6097 (0.5234)  triple_100: 0.0000 (0.0448)  triple_80: 0.0000 (0.0375)  triple_60: 0.0000 (0.0146)  triple_40: 0.0000 (0.0144)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 810/1724]  eta: 0:59:40  lr: 0.000200  loss: 1.7097 (1.9395)  loss_n_40: 0.3706 (0.4054)  loss_n_60: 0.3989 (0.4304)  loss_n_80: 0.4435 (0.4709)  loss_n_100: 0.4826 (0.5228)  triple_100: 0.0000 (0.0443)  triple_80: 0.0000 (0.0371)  triple_60: 0.0000 (0.0144)  triple_40: 0.0000 (0.0142)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 820/1724]  eta: 0:59:01  lr: 0.000200  loss: 1.5860 (1.9359)  loss_n_40: 0.3663 (0.4053)  loss_n_60: 0.3804 (0.4300)  loss_n_80: 0.4106 (0.4703)  loss_n_100: 0.4347 (0.5218)  triple_100: 0.0000 (0.0437)  triple_80: 0.0000 (0.0366)  triple_60: 0.0000 (0.0142)  triple_40: 0.0000 (0.0140)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 830/1724]  eta: 0:58:22  lr: 0.000200  loss: 1.6477 (1.9330)  loss_n_40: 0.3778 (0.4051)  loss_n_60: 0.3997 (0.4297)  loss_n_80: 0.4185 (0.4699)  loss_n_100: 0.4408 (0.5211)  triple_100: 0.0000 (0.0432)  triple_80: 0.0000 (0.0362)  triple_60: 0.0000 (0.0140)  triple_40: 0.0000 (0.0139)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 840/1724]  eta: 0:57:42  lr: 0.000200  loss: 1.5543 (1.9275)  loss_n_40: 0.3645 (0.4044)  loss_n_60: 0.3658 (0.4287)  loss_n_80: 0.3971 (0.4688)  loss_n_100: 0.4029 (0.5196)  triple_100: 0.0000 (0.0427)  triple_80: 0.0000 (0.0358)  triple_60: 0.0000 (0.0139)  triple_40: 0.0000 (0.0137)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 850/1724]  eta: 0:57:03  lr: 0.000200  loss: 1.5342 (1.9252)  loss_n_40: 0.3631 (0.4044)  loss_n_60: 0.3517 (0.4285)  loss_n_80: 0.3880 (0.4683)  loss_n_100: 0.3921 (0.5186)  triple_100: 0.0000 (0.0422)  triple_80: 0.0000 (0.0357)  triple_60: 0.0000 (0.0140)  triple_40: 0.0000 (0.0135)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 860/1724]  eta: 0:56:24  lr: 0.000200  loss: 1.5210 (1.9200)  loss_n_40: 0.3635 (0.4038)  loss_n_60: 0.3697 (0.4276)  loss_n_80: 0.3895 (0.4672)  loss_n_100: 0.4006 (0.5172)  triple_100: 0.0000 (0.0417)  triple_80: 0.0000 (0.0353)  triple_60: 0.0000 (0.0138)  triple_40: 0.0000 (0.0134)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 870/1724]  eta: 0:55:45  lr: 0.000200  loss: 1.4551 (1.9147)  loss_n_40: 0.3293 (0.4030)  loss_n_60: 0.3460 (0.4268)  loss_n_80: 0.3680 (0.4661)  loss_n_100: 0.3934 (0.5158)  triple_100: 0.0000 (0.0412)  triple_80: 0.0000 (0.0349)  triple_60: 0.0000 (0.0137)  triple_40: 0.0000 (0.0132)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 880/1724]  eta: 0:55:06  lr: 0.000200  loss: 1.3816 (1.9090)  loss_n_40: 0.3210 (0.4023)  loss_n_60: 0.3293 (0.4257)  loss_n_80: 0.3564 (0.4648)  loss_n_100: 0.3728 (0.5143)  triple_100: 0.0000 (0.0407)  triple_80: 0.0000 (0.0345)  triple_60: 0.0000 (0.0135)  triple_40: 0.0000 (0.0131)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 890/1724]  eta: 0:54:27  lr: 0.000200  loss: 1.3104 (1.9036)  loss_n_40: 0.3134 (0.4018)  loss_n_60: 0.3148 (0.4248)  loss_n_80: 0.3306 (0.4636)  loss_n_100: 0.3519 (0.5127)  triple_100: 0.0000 (0.0403)  triple_80: 0.0000 (0.0341)  triple_60: 0.0000 (0.0134)  triple_40: 0.0000 (0.0129)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 900/1724]  eta: 0:53:47  lr: 0.000200  loss: 1.3119 (1.8972)  loss_n_40: 0.3414 (0.4012)  loss_n_60: 0.3148 (0.4236)  loss_n_80: 0.3306 (0.4621)  loss_n_100: 0.3379 (0.5107)  triple_100: 0.0000 (0.0398)  triple_80: 0.0000 (0.0338)  triple_60: 0.0000 (0.0132)  triple_40: 0.0000 (0.0128)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 910/1724]  eta: 0:53:08  lr: 0.000200  loss: 1.2762 (1.8896)  loss_n_40: 0.3015 (0.4000)  loss_n_60: 0.3047 (0.4222)  loss_n_80: 0.3063 (0.4604)  loss_n_100: 0.3166 (0.5085)  triple_100: 0.0000 (0.0394)  triple_80: 0.0000 (0.0334)  triple_60: 0.0000 (0.0131)  triple_40: 0.0000 (0.0126)  time: 3.9176  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [ 920/1724]  eta: 0:52:29  lr: 0.000200  loss: 1.1558 (1.8822)  loss_n_40: 0.2676 (0.3987)  loss_n_60: 0.2807 (0.4207)  loss_n_80: 0.2980 (0.4587)  loss_n_100: 0.3091 (0.5065)  triple_100: 0.0000 (0.0390)  triple_80: 0.0000 (0.0330)  triple_60: 0.0000 (0.0129)  triple_40: 0.0000 (0.0125)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 930/1724]  eta: 0:51:50  lr: 0.000200  loss: 1.1976 (1.8750)  loss_n_40: 0.2818 (0.3977)  loss_n_60: 0.2887 (0.4193)  loss_n_80: 0.3036 (0.4571)  loss_n_100: 0.3110 (0.5045)  triple_100: 0.0000 (0.0385)  triple_80: 0.0000 (0.0327)  triple_60: 0.0000 (0.0128)  triple_40: 0.0000 (0.0124)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 940/1724]  eta: 0:51:11  lr: 0.000200  loss: 1.1674 (1.8676)  loss_n_40: 0.2725 (0.3965)  loss_n_60: 0.2803 (0.4178)  loss_n_80: 0.3013 (0.4554)  loss_n_100: 0.3259 (0.5025)  triple_100: 0.0000 (0.0381)  triple_80: 0.0000 (0.0323)  triple_60: 0.0000 (0.0127)  triple_40: 0.0000 (0.0122)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 950/1724]  eta: 0:50:32  lr: 0.000200  loss: 1.1674 (1.8614)  loss_n_40: 0.2614 (0.3954)  loss_n_60: 0.2788 (0.4165)  loss_n_80: 0.3013 (0.4538)  loss_n_100: 0.3261 (0.5006)  triple_100: 0.0000 (0.0377)  triple_80: 0.0000 (0.0324)  triple_60: 0.0000 (0.0127)  triple_40: 0.0000 (0.0122)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 960/1724]  eta: 0:49:52  lr: 0.000200  loss: 1.3057 (1.8565)  loss_n_40: 0.2946 (0.3945)  loss_n_60: 0.3020 (0.4156)  loss_n_80: 0.3392 (0.4528)  loss_n_100: 0.3613 (0.4995)  triple_100: 0.0000 (0.0373)  triple_80: 0.0000 (0.0320)  triple_60: 0.0000 (0.0125)  triple_40: 0.0000 (0.0121)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 970/1724]  eta: 0:49:13  lr: 0.000200  loss: 1.2918 (1.8506)  loss_n_40: 0.2985 (0.3937)  loss_n_60: 0.3064 (0.4144)  loss_n_80: 0.3383 (0.4515)  loss_n_100: 0.3655 (0.4980)  triple_100: 0.0000 (0.0370)  triple_80: 0.0000 (0.0317)  triple_60: 0.0000 (0.0124)  triple_40: 0.0000 (0.0120)  time: 3.9149  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 980/1724]  eta: 0:48:34  lr: 0.000200  loss: 1.2241 (1.8451)  loss_n_40: 0.2889 (0.3929)  loss_n_60: 0.2888 (0.4134)  loss_n_80: 0.3114 (0.4503)  loss_n_100: 0.3270 (0.4964)  triple_100: 0.0000 (0.0366)  triple_80: 0.0000 (0.0314)  triple_60: 0.0000 (0.0123)  triple_40: 0.0000 (0.0119)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [ 990/1724]  eta: 0:47:55  lr: 0.000200  loss: 1.1385 (1.8386)  loss_n_40: 0.2603 (0.3918)  loss_n_60: 0.2744 (0.4121)  loss_n_80: 0.2974 (0.4488)  loss_n_100: 0.3125 (0.4947)  triple_100: 0.0000 (0.0362)  triple_80: 0.0000 (0.0311)  triple_60: 0.0000 (0.0122)  triple_40: 0.0000 (0.0117)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1000/1724]  eta: 0:47:16  lr: 0.000200  loss: 1.1564 (1.8326)  loss_n_40: 0.2681 (0.3909)  loss_n_60: 0.2767 (0.4110)  loss_n_80: 0.2997 (0.4474)  loss_n_100: 0.3232 (0.4930)  triple_100: 0.0000 (0.0359)  triple_80: 0.0000 (0.0308)  triple_60: 0.0000 (0.0120)  triple_40: 0.0000 (0.0116)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1010/1724]  eta: 0:46:36  lr: 0.000200  loss: 1.3474 (1.8316)  loss_n_40: 0.3211 (0.3903)  loss_n_60: 0.3275 (0.4105)  loss_n_80: 0.3333 (0.4470)  loss_n_100: 0.3670 (0.4927)  triple_100: 0.0000 (0.0357)  triple_80: 0.0000 (0.0312)  triple_60: 0.0000 (0.0121)  triple_40: 0.0000 (0.0122)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1020/1724]  eta: 0:45:57  lr: 0.000200  loss: 1.5160 (1.8286)  loss_n_40: 0.3265 (0.3898)  loss_n_60: 0.3555 (0.4099)  loss_n_80: 0.3772 (0.4464)  loss_n_100: 0.4138 (0.4920)  triple_100: 0.0000 (0.0354)  triple_80: 0.0000 (0.0310)  triple_60: 0.0000 (0.0121)  triple_40: 0.0000 (0.0121)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1030/1724]  eta: 0:45:18  lr: 0.000200  loss: 1.4478 (1.8249)  loss_n_40: 0.3299 (0.3892)  loss_n_60: 0.3524 (0.4093)  loss_n_80: 0.3711 (0.4457)  loss_n_100: 0.4112 (0.4911)  triple_100: 0.0000 (0.0350)  triple_80: 0.0000 (0.0307)  triple_60: 0.0000 (0.0120)  triple_40: 0.0000 (0.0120)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1040/1724]  eta: 0:44:39  lr: 0.000200  loss: 1.4379 (1.8208)  loss_n_40: 0.3101 (0.3885)  loss_n_60: 0.3392 (0.4086)  loss_n_80: 0.3544 (0.4448)  loss_n_100: 0.4055 (0.4902)  triple_100: 0.0000 (0.0347)  triple_80: 0.0000 (0.0304)  triple_60: 0.0000 (0.0119)  triple_40: 0.0000 (0.0118)  time: 3.9156  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1050/1724]  eta: 0:44:00  lr: 0.000200  loss: 1.3348 (1.8161)  loss_n_40: 0.3042 (0.3877)  loss_n_60: 0.3197 (0.4078)  loss_n_80: 0.3474 (0.4438)  loss_n_100: 0.3704 (0.4889)  triple_100: 0.0000 (0.0344)  triple_80: 0.0000 (0.0301)  triple_60: 0.0000 (0.0117)  triple_40: 0.0000 (0.0117)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1060/1724]  eta: 0:43:21  lr: 0.000200  loss: 1.3264 (1.8133)  loss_n_40: 0.3069 (0.3874)  loss_n_60: 0.3217 (0.4072)  loss_n_80: 0.3319 (0.4429)  loss_n_100: 0.3600 (0.4878)  triple_100: 0.0000 (0.0340)  triple_80: 0.0000 (0.0298)  triple_60: 0.0000 (0.0116)  triple_40: 0.0000 (0.0125)  time: 3.9149  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1070/1724]  eta: 0:42:41  lr: 0.000200  loss: 1.3555 (1.8095)  loss_n_40: 0.3443 (0.3870)  loss_n_60: 0.3289 (0.4066)  loss_n_80: 0.3372 (0.4421)  loss_n_100: 0.3611 (0.4867)  triple_100: 0.0000 (0.0337)  triple_80: 0.0000 (0.0295)  triple_60: 0.0000 (0.0115)  triple_40: 0.0000 (0.0124)  time: 3.9134  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1080/1724]  eta: 0:42:02  lr: 0.000200  loss: 1.3013 (1.8043)  loss_n_40: 0.3054 (0.3862)  loss_n_60: 0.3020 (0.4056)  loss_n_80: 0.3243 (0.4409)  loss_n_100: 0.3309 (0.4852)  triple_100: 0.0000 (0.0334)  triple_80: 0.0000 (0.0292)  triple_60: 0.0000 (0.0114)  triple_40: 0.0000 (0.0122)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1090/1724]  eta: 0:41:23  lr: 0.000200  loss: 1.3220 (1.8008)  loss_n_40: 0.2966 (0.3857)  loss_n_60: 0.3180 (0.4051)  loss_n_80: 0.3414 (0.4403)  loss_n_100: 0.3539 (0.4843)  triple_100: 0.0000 (0.0331)  triple_80: 0.0000 (0.0290)  triple_60: 0.0000 (0.0113)  triple_40: 0.0000 (0.0121)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1100/1724]  eta: 0:40:44  lr: 0.000200  loss: 1.3333 (1.7970)  loss_n_40: 0.2935 (0.3849)  loss_n_60: 0.3057 (0.4041)  loss_n_80: 0.3480 (0.4394)  loss_n_100: 0.3794 (0.4832)  triple_100: 0.0000 (0.0328)  triple_80: 0.0000 (0.0288)  triple_60: 0.0000 (0.0115)  triple_40: 0.0000 (0.0123)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1110/1724]  eta: 0:40:05  lr: 0.000200  loss: 1.4053 (1.7947)  loss_n_40: 0.3318 (0.3845)  loss_n_60: 0.3288 (0.4039)  loss_n_80: 0.3561 (0.4390)  loss_n_100: 0.3974 (0.4827)  triple_100: 0.0000 (0.0325)  triple_80: 0.0000 (0.0286)  triple_60: 0.0000 (0.0114)  triple_40: 0.0000 (0.0122)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1120/1724]  eta: 0:39:26  lr: 0.000200  loss: 1.5263 (1.7920)  loss_n_40: 0.3330 (0.3840)  loss_n_60: 0.3636 (0.4034)  loss_n_80: 0.3820 (0.4385)  loss_n_100: 0.4101 (0.4821)  triple_100: 0.0000 (0.0322)  triple_80: 0.0000 (0.0283)  triple_60: 0.0000 (0.0113)  triple_40: 0.0000 (0.0121)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1130/1724]  eta: 0:38:46  lr: 0.000200  loss: 1.3949 (1.7888)  loss_n_40: 0.3291 (0.3836)  loss_n_60: 0.3376 (0.4029)  loss_n_80: 0.3551 (0.4379)  loss_n_100: 0.3748 (0.4812)  triple_100: 0.0000 (0.0319)  triple_80: 0.0000 (0.0281)  triple_60: 0.0000 (0.0112)  triple_40: 0.0000 (0.0120)  time: 3.9171  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [1140/1724]  eta: 0:38:07  lr: 0.000200  loss: 1.3453 (1.7852)  loss_n_40: 0.3141 (0.3831)  loss_n_60: 0.3278 (0.4024)  loss_n_80: 0.3450 (0.4371)  loss_n_100: 0.3663 (0.4802)  triple_100: 0.0000 (0.0316)  triple_80: 0.0000 (0.0278)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0119)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1150/1724]  eta: 0:37:28  lr: 0.000200  loss: 1.2198 (1.7808)  loss_n_40: 0.2777 (0.3825)  loss_n_60: 0.2933 (0.4015)  loss_n_80: 0.3167 (0.4361)  loss_n_100: 0.3244 (0.4789)  triple_100: 0.0000 (0.0314)  triple_80: 0.0000 (0.0276)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0118)  time: 3.9172  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [1160/1724]  eta: 0:36:49  lr: 0.000200  loss: 1.1623 (1.7764)  loss_n_40: 0.2738 (0.3818)  loss_n_60: 0.2844 (0.4007)  loss_n_80: 0.2952 (0.4352)  loss_n_100: 0.3068 (0.4777)  triple_100: 0.0000 (0.0311)  triple_80: 0.0000 (0.0273)  triple_60: 0.0000 (0.0109)  triple_40: 0.0000 (0.0117)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1170/1724]  eta: 0:36:10  lr: 0.000200  loss: 1.1426 (1.7713)  loss_n_40: 0.2738 (0.3812)  loss_n_60: 0.2725 (0.3997)  loss_n_80: 0.2888 (0.4339)  loss_n_100: 0.2986 (0.4762)  triple_100: 0.0000 (0.0308)  triple_80: 0.0000 (0.0271)  triple_60: 0.0000 (0.0108)  triple_40: 0.0000 (0.0116)  time: 3.9162  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [1180/1724]  eta: 0:35:30  lr: 0.000200  loss: 1.1263 (1.7663)  loss_n_40: 0.2675 (0.3805)  loss_n_60: 0.2725 (0.3987)  loss_n_80: 0.2858 (0.4327)  loss_n_100: 0.2986 (0.4747)  triple_100: 0.0000 (0.0306)  triple_80: 0.0000 (0.0269)  triple_60: 0.0000 (0.0107)  triple_40: 0.0000 (0.0115)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1190/1724]  eta: 0:34:51  lr: 0.000200  loss: 1.1245 (1.7619)  loss_n_40: 0.2675 (0.3799)  loss_n_60: 0.2742 (0.3979)  loss_n_80: 0.2865 (0.4317)  loss_n_100: 0.3014 (0.4734)  triple_100: 0.0000 (0.0303)  triple_80: 0.0000 (0.0266)  triple_60: 0.0000 (0.0106)  triple_40: 0.0000 (0.0114)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1200/1724]  eta: 0:34:12  lr: 0.000200  loss: 1.1246 (1.7571)  loss_n_40: 0.2938 (0.3793)  loss_n_60: 0.2855 (0.3970)  loss_n_80: 0.2865 (0.4305)  loss_n_100: 0.2912 (0.4719)  triple_100: 0.0000 (0.0301)  triple_80: 0.0000 (0.0264)  triple_60: 0.0000 (0.0105)  triple_40: 0.0000 (0.0113)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1210/1724]  eta: 0:33:33  lr: 0.000200  loss: 1.0486 (1.7514)  loss_n_40: 0.2498 (0.3784)  loss_n_60: 0.2517 (0.3958)  loss_n_80: 0.2629 (0.4292)  loss_n_100: 0.2732 (0.4703)  triple_100: 0.0000 (0.0298)  triple_80: 0.0000 (0.0262)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0112)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1220/1724]  eta: 0:32:54  lr: 0.000200  loss: 1.0641 (1.7460)  loss_n_40: 0.2596 (0.3776)  loss_n_60: 0.2517 (0.3947)  loss_n_80: 0.2748 (0.4279)  loss_n_100: 0.2847 (0.4688)  triple_100: 0.0000 (0.0296)  triple_80: 0.0000 (0.0260)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0111)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1230/1724]  eta: 0:32:15  lr: 0.000200  loss: 1.0467 (1.7408)  loss_n_40: 0.2596 (0.3767)  loss_n_60: 0.2504 (0.3936)  loss_n_80: 0.2705 (0.4267)  loss_n_100: 0.2799 (0.4673)  triple_100: 0.0000 (0.0293)  triple_80: 0.0000 (0.0258)  triple_60: 0.0000 (0.0103)  triple_40: 0.0000 (0.0110)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1240/1724]  eta: 0:31:35  lr: 0.000200  loss: 1.0124 (1.7360)  loss_n_40: 0.2473 (0.3760)  loss_n_60: 0.2420 (0.3927)  loss_n_80: 0.2605 (0.4256)  loss_n_100: 0.2786 (0.4660)  triple_100: 0.0000 (0.0291)  triple_80: 0.0000 (0.0256)  triple_60: 0.0000 (0.0102)  triple_40: 0.0000 (0.0109)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1250/1724]  eta: 0:30:56  lr: 0.000200  loss: 1.1453 (1.7405)  loss_n_40: 0.2620 (0.3753)  loss_n_60: 0.2677 (0.3920)  loss_n_80: 0.2769 (0.4249)  loss_n_100: 0.2885 (0.4653)  triple_100: 0.0000 (0.0305)  triple_80: 0.0000 (0.0275)  triple_60: 0.0000 (0.0121)  triple_40: 0.0000 (0.0129)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1260/1724]  eta: 0:30:17  lr: 0.000200  loss: 2.0429 (1.7436)  loss_n_40: 0.3834 (0.3759)  loss_n_60: 0.4447 (0.3928)  loss_n_80: 0.4969 (0.4259)  loss_n_100: 0.5625 (0.4667)  triple_100: 0.0000 (0.0303)  triple_80: 0.0000 (0.0273)  triple_60: 0.0000 (0.0120)  triple_40: 0.0000 (0.0128)  time: 3.9158  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [1270/1724]  eta: 0:29:38  lr: 0.000200  loss: 2.0524 (1.7451)  loss_n_40: 0.4358 (0.3763)  loss_n_60: 0.4651 (0.3932)  loss_n_80: 0.5344 (0.4265)  loss_n_100: 0.5755 (0.4674)  triple_100: 0.0000 (0.0301)  triple_80: 0.0000 (0.0270)  triple_60: 0.0000 (0.0120)  triple_40: 0.0000 (0.0127)  time: 3.9157  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1280/1724]  eta: 0:28:59  lr: 0.000200  loss: 1.8108 (1.7451)  loss_n_40: 0.4173 (0.3765)  loss_n_60: 0.4234 (0.3934)  loss_n_80: 0.4658 (0.4266)  loss_n_100: 0.4990 (0.4674)  triple_100: 0.0000 (0.0298)  triple_80: 0.0000 (0.0268)  triple_60: 0.0000 (0.0119)  triple_40: 0.0000 (0.0126)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1290/1724]  eta: 0:28:20  lr: 0.000200  loss: 1.6326 (1.7436)  loss_n_40: 0.3751 (0.3764)  loss_n_60: 0.3862 (0.3933)  loss_n_80: 0.4206 (0.4263)  loss_n_100: 0.4515 (0.4671)  triple_100: 0.0000 (0.0296)  triple_80: 0.0000 (0.0266)  triple_60: 0.0000 (0.0118)  triple_40: 0.0000 (0.0125)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1300/1724]  eta: 0:27:40  lr: 0.000200  loss: 1.4477 (1.7412)  loss_n_40: 0.3356 (0.3761)  loss_n_60: 0.3508 (0.3929)  loss_n_80: 0.3770 (0.4258)  loss_n_100: 0.3983 (0.4664)  triple_100: 0.0000 (0.0294)  triple_80: 0.0000 (0.0264)  triple_60: 0.0000 (0.0117)  triple_40: 0.0000 (0.0124)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1310/1724]  eta: 0:27:01  lr: 0.000200  loss: 1.4403 (1.7392)  loss_n_40: 0.3401 (0.3760)  loss_n_60: 0.3447 (0.3926)  loss_n_80: 0.3644 (0.4254)  loss_n_100: 0.3599 (0.4658)  triple_100: 0.0000 (0.0291)  triple_80: 0.0000 (0.0262)  triple_60: 0.0000 (0.0116)  triple_40: 0.0000 (0.0124)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1320/1724]  eta: 0:26:22  lr: 0.000200  loss: 1.3067 (1.7355)  loss_n_40: 0.3084 (0.3755)  loss_n_60: 0.3171 (0.3920)  loss_n_80: 0.3232 (0.4246)  loss_n_100: 0.3362 (0.4647)  triple_100: 0.0000 (0.0289)  triple_80: 0.0000 (0.0260)  triple_60: 0.0000 (0.0115)  triple_40: 0.0000 (0.0123)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1330/1724]  eta: 0:25:43  lr: 0.000200  loss: 1.2427 (1.7318)  loss_n_40: 0.3032 (0.3748)  loss_n_60: 0.3007 (0.3913)  loss_n_80: 0.3096 (0.4238)  loss_n_100: 0.3215 (0.4637)  triple_100: 0.0000 (0.0287)  triple_80: 0.0000 (0.0258)  triple_60: 0.0000 (0.0114)  triple_40: 0.0000 (0.0122)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1340/1724]  eta: 0:25:04  lr: 0.000200  loss: 1.2471 (1.7287)  loss_n_40: 0.2998 (0.3743)  loss_n_60: 0.2985 (0.3907)  loss_n_80: 0.3187 (0.4231)  loss_n_100: 0.3261 (0.4629)  triple_100: 0.0000 (0.0285)  triple_80: 0.0000 (0.0256)  triple_60: 0.0000 (0.0113)  triple_40: 0.0000 (0.0123)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1350/1724]  eta: 0:24:25  lr: 0.000200  loss: 1.2648 (1.7259)  loss_n_40: 0.2996 (0.3740)  loss_n_60: 0.3082 (0.3901)  loss_n_80: 0.3264 (0.4225)  loss_n_100: 0.3507 (0.4622)  triple_100: 0.0000 (0.0283)  triple_80: 0.0000 (0.0255)  triple_60: 0.0000 (0.0112)  triple_40: 0.0000 (0.0122)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1360/1724]  eta: 0:23:45  lr: 0.000200  loss: 1.2386 (1.7221)  loss_n_40: 0.2835 (0.3734)  loss_n_60: 0.2897 (0.3894)  loss_n_80: 0.3051 (0.4216)  loss_n_100: 0.3382 (0.4612)  triple_100: 0.0000 (0.0281)  triple_80: 0.0000 (0.0253)  triple_60: 0.0000 (0.0112)  triple_40: 0.0000 (0.0121)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1370/1724]  eta: 0:23:06  lr: 0.000200  loss: 1.2391 (1.7191)  loss_n_40: 0.3001 (0.3730)  loss_n_60: 0.3011 (0.3888)  loss_n_80: 0.3067 (0.4209)  loss_n_100: 0.3312 (0.4602)  triple_100: 0.0000 (0.0279)  triple_80: 0.0000 (0.0251)  triple_60: 0.0000 (0.0112)  triple_40: 0.0000 (0.0120)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1380/1724]  eta: 0:22:27  lr: 0.000200  loss: 1.2359 (1.7153)  loss_n_40: 0.2946 (0.3723)  loss_n_60: 0.3062 (0.3882)  loss_n_80: 0.3127 (0.4200)  loss_n_100: 0.3210 (0.4592)  triple_100: 0.0000 (0.0277)  triple_80: 0.0000 (0.0249)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0119)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1390/1724]  eta: 0:21:48  lr: 0.000200  loss: 1.2044 (1.7122)  loss_n_40: 0.2879 (0.3720)  loss_n_60: 0.3013 (0.3876)  loss_n_80: 0.2985 (0.4193)  loss_n_100: 0.3089 (0.4582)  triple_100: 0.0000 (0.0275)  triple_80: 0.0000 (0.0248)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0118)  time: 3.9159  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [1400/1724]  eta: 0:21:09  lr: 0.000200  loss: 1.2044 (1.7086)  loss_n_40: 0.2950 (0.3714)  loss_n_60: 0.2991 (0.3870)  loss_n_80: 0.2985 (0.4184)  loss_n_100: 0.3103 (0.4572)  triple_100: 0.0000 (0.0273)  triple_80: 0.0000 (0.0246)  triple_60: 0.0000 (0.0109)  triple_40: 0.0000 (0.0117)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1410/1724]  eta: 0:20:29  lr: 0.000200  loss: 1.1707 (1.7094)  loss_n_40: 0.2910 (0.3709)  loss_n_60: 0.2815 (0.3864)  loss_n_80: 0.2985 (0.4176)  loss_n_100: 0.3202 (0.4562)  triple_100: 0.0000 (0.0279)  triple_80: 0.0000 (0.0256)  triple_60: 0.0000 (0.0117)  triple_40: 0.0000 (0.0131)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1420/1724]  eta: 0:19:50  lr: 0.000200  loss: 1.4375 (1.7085)  loss_n_40: 0.3021 (0.3707)  loss_n_60: 0.3421 (0.3863)  loss_n_80: 0.3645 (0.4176)  loss_n_100: 0.3828 (0.4562)  triple_100: 0.0000 (0.0277)  triple_80: 0.0000 (0.0255)  triple_60: 0.0000 (0.0116)  triple_40: 0.0000 (0.0130)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1430/1724]  eta: 0:19:11  lr: 0.000200  loss: 1.5516 (1.7084)  loss_n_40: 0.3362 (0.3705)  loss_n_60: 0.3759 (0.3864)  loss_n_80: 0.4139 (0.4177)  loss_n_100: 0.4487 (0.4563)  triple_100: 0.0000 (0.0275)  triple_80: 0.0000 (0.0256)  triple_60: 0.0000 (0.0116)  triple_40: 0.0000 (0.0129)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1440/1724]  eta: 0:18:32  lr: 0.000200  loss: 1.5359 (1.7075)  loss_n_40: 0.3337 (0.3704)  loss_n_60: 0.3625 (0.3863)  loss_n_80: 0.3923 (0.4176)  loss_n_100: 0.4373 (0.4562)  triple_100: 0.0000 (0.0273)  triple_80: 0.0000 (0.0254)  triple_60: 0.0000 (0.0115)  triple_40: 0.0000 (0.0128)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1450/1724]  eta: 0:17:53  lr: 0.000200  loss: 1.5173 (1.7050)  loss_n_40: 0.3337 (0.3700)  loss_n_60: 0.3447 (0.3858)  loss_n_80: 0.3798 (0.4171)  loss_n_100: 0.4032 (0.4555)  triple_100: 0.0000 (0.0271)  triple_80: 0.0000 (0.0252)  triple_60: 0.0000 (0.0115)  triple_40: 0.0000 (0.0127)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1460/1724]  eta: 0:17:14  lr: 0.000200  loss: 1.3024 (1.7023)  loss_n_40: 0.2977 (0.3696)  loss_n_60: 0.3098 (0.3853)  loss_n_80: 0.3315 (0.4165)  loss_n_100: 0.3502 (0.4549)  triple_100: 0.0000 (0.0270)  triple_80: 0.0000 (0.0250)  triple_60: 0.0000 (0.0114)  triple_40: 0.0000 (0.0127)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1470/1724]  eta: 0:16:34  lr: 0.000200  loss: 1.2560 (1.6991)  loss_n_40: 0.2943 (0.3691)  loss_n_60: 0.2945 (0.3847)  loss_n_80: 0.3101 (0.4157)  loss_n_100: 0.3435 (0.4541)  triple_100: 0.0000 (0.0268)  triple_80: 0.0000 (0.0249)  triple_60: 0.0000 (0.0113)  triple_40: 0.0000 (0.0126)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1480/1724]  eta: 0:15:55  lr: 0.000200  loss: 1.1839 (1.6966)  loss_n_40: 0.2862 (0.3689)  loss_n_60: 0.2854 (0.3843)  loss_n_80: 0.3013 (0.4151)  loss_n_100: 0.3165 (0.4532)  triple_100: 0.0000 (0.0266)  triple_80: 0.0000 (0.0247)  triple_60: 0.0000 (0.0112)  triple_40: 0.0000 (0.0125)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1490/1724]  eta: 0:15:16  lr: 0.000200  loss: 1.1675 (1.6931)  loss_n_40: 0.2832 (0.3683)  loss_n_60: 0.2841 (0.3837)  loss_n_80: 0.2926 (0.4143)  loss_n_100: 0.3087 (0.4523)  triple_100: 0.0000 (0.0264)  triple_80: 0.0000 (0.0245)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0124)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1500/1724]  eta: 0:14:37  lr: 0.000200  loss: 1.1801 (1.6898)  loss_n_40: 0.2818 (0.3680)  loss_n_60: 0.2740 (0.3830)  loss_n_80: 0.2893 (0.4135)  loss_n_100: 0.2970 (0.4513)  triple_100: 0.0000 (0.0262)  triple_80: 0.0000 (0.0244)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0123)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1510/1724]  eta: 0:13:58  lr: 0.000200  loss: 1.2059 (1.6873)  loss_n_40: 0.2943 (0.3677)  loss_n_60: 0.2756 (0.3826)  loss_n_80: 0.2958 (0.4129)  loss_n_100: 0.3147 (0.4506)  triple_100: 0.0000 (0.0261)  triple_80: 0.0000 (0.0242)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0123)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1520/1724]  eta: 0:13:19  lr: 0.000200  loss: 1.1728 (1.6840)  loss_n_40: 0.2895 (0.3672)  loss_n_60: 0.2756 (0.3819)  loss_n_80: 0.2875 (0.4121)  loss_n_100: 0.3093 (0.4497)  triple_100: 0.0000 (0.0259)  triple_80: 0.0000 (0.0240)  triple_60: 0.0000 (0.0109)  triple_40: 0.0000 (0.0122)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1530/1724]  eta: 0:12:39  lr: 0.000200  loss: 1.1379 (1.6809)  loss_n_40: 0.2895 (0.3668)  loss_n_60: 0.2717 (0.3814)  loss_n_80: 0.2797 (0.4113)  loss_n_100: 0.2978 (0.4488)  triple_100: 0.0000 (0.0257)  triple_80: 0.0000 (0.0239)  triple_60: 0.0000 (0.0109)  triple_40: 0.0000 (0.0121)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1540/1724]  eta: 0:12:00  lr: 0.000200  loss: 1.1163 (1.6770)  loss_n_40: 0.2825 (0.3661)  loss_n_60: 0.2717 (0.3806)  loss_n_80: 0.2710 (0.4104)  loss_n_100: 0.2849 (0.4477)  triple_100: 0.0000 (0.0256)  triple_80: 0.0000 (0.0237)  triple_60: 0.0000 (0.0108)  triple_40: 0.0000 (0.0121)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1550/1724]  eta: 0:11:21  lr: 0.000200  loss: 1.1477 (1.6738)  loss_n_40: 0.2876 (0.3656)  loss_n_60: 0.2648 (0.3800)  loss_n_80: 0.2792 (0.4097)  loss_n_100: 0.2925 (0.4467)  triple_100: 0.0000 (0.0254)  triple_80: 0.0000 (0.0236)  triple_60: 0.0000 (0.0108)  triple_40: 0.0000 (0.0120)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1560/1724]  eta: 0:10:42  lr: 0.000200  loss: 1.2102 (1.6707)  loss_n_40: 0.2937 (0.3652)  loss_n_60: 0.2704 (0.3794)  loss_n_80: 0.2918 (0.4090)  loss_n_100: 0.3091 (0.4458)  triple_100: 0.0000 (0.0252)  triple_80: 0.0000 (0.0234)  triple_60: 0.0000 (0.0107)  triple_40: 0.0000 (0.0119)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1570/1724]  eta: 0:10:03  lr: 0.000200  loss: 1.0969 (1.6672)  loss_n_40: 0.2584 (0.3646)  loss_n_60: 0.2648 (0.3787)  loss_n_80: 0.2815 (0.4082)  loss_n_100: 0.2871 (0.4449)  triple_100: 0.0000 (0.0251)  triple_80: 0.0000 (0.0233)  triple_60: 0.0000 (0.0106)  triple_40: 0.0000 (0.0118)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1580/1724]  eta: 0:09:24  lr: 0.000200  loss: 1.0450 (1.6642)  loss_n_40: 0.2542 (0.3643)  loss_n_60: 0.2499 (0.3781)  loss_n_80: 0.2636 (0.4074)  loss_n_100: 0.2832 (0.4440)  triple_100: 0.0000 (0.0249)  triple_80: 0.0000 (0.0231)  triple_60: 0.0000 (0.0105)  triple_40: 0.0000 (0.0118)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1590/1724]  eta: 0:08:44  lr: 0.000200  loss: 1.3206 (1.6645)  loss_n_40: 0.2869 (0.3640)  loss_n_60: 0.3068 (0.3780)  loss_n_80: 0.3430 (0.4074)  loss_n_100: 0.3641 (0.4440)  triple_100: 0.0000 (0.0248)  triple_80: 0.0000 (0.0232)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0121)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1600/1724]  eta: 0:08:05  lr: 0.000200  loss: 1.7404 (1.6668)  loss_n_40: 0.3483 (0.3642)  loss_n_60: 0.4089 (0.3786)  loss_n_80: 0.4546 (0.4080)  loss_n_100: 0.5122 (0.4448)  triple_100: 0.0000 (0.0247)  triple_80: 0.0000 (0.0234)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0120)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1610/1724]  eta: 0:07:26  lr: 0.000200  loss: 1.8679 (1.6700)  loss_n_40: 0.3642 (0.3644)  loss_n_60: 0.4331 (0.3791)  loss_n_80: 0.4845 (0.4087)  loss_n_100: 0.5502 (0.4458)  triple_100: 0.0000 (0.0256)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0119)  time: 3.9171  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [1620/1724]  eta: 0:06:47  lr: 0.000200  loss: 2.0576 (1.6727)  loss_n_40: 0.3968 (0.3646)  loss_n_60: 0.4622 (0.3796)  loss_n_80: 0.5581 (0.4097)  loss_n_100: 0.6479 (0.4472)  triple_100: 0.0000 (0.0254)  triple_80: 0.0000 (0.0234)  triple_60: 0.0000 (0.0109)  triple_40: 0.0000 (0.0118)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1630/1724]  eta: 0:06:08  lr: 0.000200  loss: 2.0967 (1.6740)  loss_n_40: 0.3914 (0.3647)  loss_n_60: 0.4368 (0.3799)  loss_n_80: 0.5370 (0.4101)  loss_n_100: 0.6385 (0.4477)  triple_100: 0.0000 (0.0253)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0108)  triple_40: 0.0000 (0.0119)  time: 3.9170  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:16]  [1640/1724]  eta: 0:05:29  lr: 0.000200  loss: 1.6976 (1.6738)  loss_n_40: 0.3777 (0.3648)  loss_n_60: 0.4014 (0.3799)  loss_n_80: 0.4369 (0.4101)  loss_n_100: 0.4808 (0.4478)  triple_100: 0.0000 (0.0251)  triple_80: 0.0000 (0.0234)  triple_60: 0.0000 (0.0108)  triple_40: 0.0000 (0.0118)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1650/1724]  eta: 0:04:49  lr: 0.000200  loss: 1.5650 (1.6727)  loss_n_40: 0.3605 (0.3647)  loss_n_60: 0.3634 (0.3797)  loss_n_80: 0.3777 (0.4099)  loss_n_100: 0.4179 (0.4474)  triple_100: 0.0000 (0.0249)  triple_80: 0.0000 (0.0232)  triple_60: 0.0000 (0.0107)  triple_40: 0.0000 (0.0121)  time: 3.9161  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1660/1724]  eta: 0:04:10  lr: 0.000200  loss: 1.4915 (1.6721)  loss_n_40: 0.3483 (0.3647)  loss_n_60: 0.3567 (0.3797)  loss_n_80: 0.3833 (0.4099)  loss_n_100: 0.3985 (0.4473)  triple_100: 0.0000 (0.0248)  triple_80: 0.0000 (0.0231)  triple_60: 0.0000 (0.0106)  triple_40: 0.0000 (0.0121)  time: 3.9165  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [1670/1724]  eta: 0:03:31  lr: 0.000200  loss: 1.4915 (1.6707)  loss_n_40: 0.3365 (0.3645)  loss_n_60: 0.3567 (0.3795)  loss_n_80: 0.3815 (0.4096)  loss_n_100: 0.4110 (0.4469)  triple_100: 0.0000 (0.0247)  triple_80: 0.0000 (0.0230)  triple_60: 0.0000 (0.0106)  triple_40: 0.0000 (0.0120)  time: 3.9170  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [1680/1724]  eta: 0:02:52  lr: 0.000200  loss: 1.3047 (1.6681)  loss_n_40: 0.3205 (0.3642)  loss_n_60: 0.3134 (0.3790)  loss_n_80: 0.3326 (0.4090)  loss_n_100: 0.3564 (0.4462)  triple_100: 0.0000 (0.0245)  triple_80: 0.0000 (0.0228)  triple_60: 0.0000 (0.0105)  triple_40: 0.0000 (0.0119)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1690/1724]  eta: 0:02:13  lr: 0.000200  loss: 1.4427 (1.6757)  loss_n_40: 0.3261 (0.3642)  loss_n_60: 0.3353 (0.3792)  loss_n_80: 0.3521 (0.4094)  loss_n_100: 0.3734 (0.4468)  triple_100: 0.0000 (0.0260)  triple_80: 0.0000 (0.0243)  triple_60: 0.0000 (0.0121)  triple_40: 0.0000 (0.0136)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1700/1724]  eta: 0:01:34  lr: 0.000200  loss: 1.9678 (1.6775)  loss_n_40: 0.3914 (0.3646)  loss_n_60: 0.4460 (0.3798)  loss_n_80: 0.5132 (0.4101)  loss_n_100: 0.5552 (0.4475)  triple_100: 0.0000 (0.0258)  triple_80: 0.0000 (0.0241)  triple_60: 0.0000 (0.0120)  triple_40: 0.0000 (0.0135)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:16]  [1710/1724]  eta: 0:00:54  lr: 0.000200  loss: 1.9507 (1.6795)  loss_n_40: 0.4415 (0.3653)  loss_n_60: 0.4491 (0.3802)  loss_n_80: 0.5154 (0.4107)  loss_n_100: 0.5389 (0.4480)  triple_100: 0.0000 (0.0257)  triple_80: 0.0000 (0.0242)  triple_60: 0.0000 (0.0120)  triple_40: 0.0000 (0.0134)  time: 3.9167  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [1720/1724]  eta: 0:00:15  lr: 0.000200  loss: 1.9507 (1.6809)  loss_n_40: 0.4637 (0.3659)  loss_n_60: 0.4491 (0.3806)  loss_n_80: 0.5019 (0.4111)  loss_n_100: 0.5309 (0.4484)  triple_100: 0.0000 (0.0255)  triple_80: 0.0000 (0.0242)  triple_60: 0.0000 (0.0119)  triple_40: 0.0000 (0.0134)  time: 3.9172  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16]  [1723/1724]  eta: 0:00:03  lr: 0.000200  loss: 1.9318 (1.6812)  loss_n_40: 0.4637 (0.3660)  loss_n_60: 0.4491 (0.3807)  loss_n_80: 0.5019 (0.4112)  loss_n_100: 0.5242 (0.4484)  triple_100: 0.0000 (0.0255)  triple_80: 0.0000 (0.0241)  triple_60: 0.0000 (0.0119)  triple_40: 0.0000 (0.0133)  time: 3.9180  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:16] Total time: 1:52:33 (3.9173 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.9318 (1.6812)  loss_n_40: 0.4637 (0.3660)  loss_n_60: 0.4491 (0.3807)  loss_n_80: 0.5019 (0.4112)  loss_n_100: 0.5242 (0.4484)  triple_100: 0.0000 (0.0255)  triple_80: 0.0000 (0.0241)  triple_60: 0.0000 (0.0119)  triple_40: 0.0000 (0.0133)\n",
      "Valid: [epoch:16]  [  0/845]  eta: 0:09:47  loss: 1.5030 (1.5030)  loss_n_40: 0.4069 (0.4069)  loss_n_60: 0.3463 (0.3463)  loss_n_80: 0.3729 (0.3729)  loss_n_100: 0.3769 (0.3769)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.6957  data: 0.3590  max mem: 46473\n",
      "Valid: [epoch:16]  [ 10/845]  eta: 0:05:06  loss: 1.5705 (1.6388)  loss_n_40: 0.3912 (0.4489)  loss_n_60: 0.3518 (0.3691)  loss_n_80: 0.3793 (0.4085)  loss_n_100: 0.3939 (0.4122)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3668  data: 0.0327  max mem: 46473\n",
      "Valid: [epoch:16]  [ 20/845]  eta: 0:04:49  loss: 1.6038 (1.6752)  loss_n_40: 0.3978 (0.4448)  loss_n_60: 0.3576 (0.3843)  loss_n_80: 0.4014 (0.4164)  loss_n_100: 0.4092 (0.4297)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [ 30/845]  eta: 0:04:41  loss: 1.6061 (1.6738)  loss_n_40: 0.4047 (0.4486)  loss_n_60: 0.3652 (0.3882)  loss_n_80: 0.4192 (0.4099)  loss_n_100: 0.4124 (0.4272)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [ 40/845]  eta: 0:04:35  loss: 1.6843 (1.6784)  loss_n_40: 0.4087 (0.4377)  loss_n_60: 0.4040 (0.3907)  loss_n_80: 0.4290 (0.4149)  loss_n_100: 0.4423 (0.4352)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3336  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [ 50/845]  eta: 0:04:31  loss: 1.6638 (1.6572)  loss_n_40: 0.3844 (0.4399)  loss_n_60: 0.4061 (0.3873)  loss_n_80: 0.4030 (0.4049)  loss_n_100: 0.4423 (0.4251)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [ 60/845]  eta: 0:04:26  loss: 1.4243 (1.6352)  loss_n_40: 0.3626 (0.4318)  loss_n_60: 0.3259 (0.3812)  loss_n_80: 0.3555 (0.4018)  loss_n_100: 0.3596 (0.4205)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [ 70/845]  eta: 0:04:22  loss: 1.5002 (1.6300)  loss_n_40: 0.3831 (0.4294)  loss_n_60: 0.3381 (0.3799)  loss_n_80: 0.3661 (0.4012)  loss_n_100: 0.3770 (0.4195)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [ 80/845]  eta: 0:04:18  loss: 1.5002 (1.6047)  loss_n_40: 0.3723 (0.4214)  loss_n_60: 0.3335 (0.3736)  loss_n_80: 0.3551 (0.3960)  loss_n_100: 0.3682 (0.4138)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:16]  [ 90/845]  eta: 0:04:15  loss: 1.4219 (1.6066)  loss_n_40: 0.3734 (0.4200)  loss_n_60: 0.3378 (0.3739)  loss_n_80: 0.3682 (0.3981)  loss_n_100: 0.3682 (0.4147)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:16]  [100/845]  eta: 0:04:11  loss: 1.6541 (1.6328)  loss_n_40: 0.4021 (0.4286)  loss_n_60: 0.3870 (0.3800)  loss_n_80: 0.4238 (0.4034)  loss_n_100: 0.4347 (0.4208)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [110/845]  eta: 0:04:07  loss: 1.7547 (1.6388)  loss_n_40: 0.4124 (0.4271)  loss_n_60: 0.4115 (0.3812)  loss_n_80: 0.4464 (0.4061)  loss_n_100: 0.4657 (0.4244)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [120/845]  eta: 0:04:04  loss: 1.7717 (1.6450)  loss_n_40: 0.4124 (0.4305)  loss_n_60: 0.4132 (0.3824)  loss_n_80: 0.4464 (0.4080)  loss_n_100: 0.4403 (0.4241)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:16]  [130/845]  eta: 0:04:00  loss: 1.5984 (1.6403)  loss_n_40: 0.3920 (0.4302)  loss_n_60: 0.3756 (0.3818)  loss_n_80: 0.4090 (0.4065)  loss_n_100: 0.3843 (0.4219)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [140/845]  eta: 0:03:57  loss: 1.5984 (1.6427)  loss_n_40: 0.4128 (0.4324)  loss_n_60: 0.3642 (0.3825)  loss_n_80: 0.3876 (0.4055)  loss_n_100: 0.3974 (0.4223)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [150/845]  eta: 0:03:53  loss: 1.7421 (1.6452)  loss_n_40: 0.4199 (0.4340)  loss_n_60: 0.4101 (0.3830)  loss_n_80: 0.4042 (0.4053)  loss_n_100: 0.4152 (0.4229)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [160/845]  eta: 0:03:50  loss: 1.6463 (1.6409)  loss_n_40: 0.4125 (0.4319)  loss_n_60: 0.3836 (0.3822)  loss_n_80: 0.4073 (0.4047)  loss_n_100: 0.4152 (0.4222)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [170/845]  eta: 0:03:46  loss: 1.5050 (1.6343)  loss_n_40: 0.3697 (0.4282)  loss_n_60: 0.3389 (0.3804)  loss_n_80: 0.3997 (0.4041)  loss_n_100: 0.3981 (0.4216)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [180/845]  eta: 0:03:43  loss: 1.5878 (1.6384)  loss_n_40: 0.3692 (0.4277)  loss_n_60: 0.3583 (0.3812)  loss_n_80: 0.4152 (0.4059)  loss_n_100: 0.4459 (0.4236)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [190/845]  eta: 0:03:40  loss: 1.5482 (1.6279)  loss_n_40: 0.3888 (0.4244)  loss_n_60: 0.3462 (0.3788)  loss_n_80: 0.3950 (0.4040)  loss_n_100: 0.3935 (0.4208)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [200/845]  eta: 0:03:36  loss: 1.3893 (1.6213)  loss_n_40: 0.3591 (0.4231)  loss_n_60: 0.3236 (0.3773)  loss_n_80: 0.3491 (0.4022)  loss_n_100: 0.3548 (0.4187)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:16]  [210/845]  eta: 0:03:33  loss: 1.2908 (1.6122)  loss_n_40: 0.3392 (0.4198)  loss_n_60: 0.3099 (0.3753)  loss_n_80: 0.3407 (0.4004)  loss_n_100: 0.3540 (0.4168)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:16]  [220/845]  eta: 0:03:29  loss: 1.4850 (1.6216)  loss_n_40: 0.3912 (0.4236)  loss_n_60: 0.3494 (0.3777)  loss_n_80: 0.3808 (0.4017)  loss_n_100: 0.3911 (0.4187)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [230/845]  eta: 0:03:26  loss: 1.5541 (1.6205)  loss_n_40: 0.3979 (0.4224)  loss_n_60: 0.3853 (0.3775)  loss_n_80: 0.3946 (0.4019)  loss_n_100: 0.4101 (0.4187)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [240/845]  eta: 0:03:23  loss: 1.7121 (1.6210)  loss_n_40: 0.3979 (0.4226)  loss_n_60: 0.4010 (0.3777)  loss_n_80: 0.4124 (0.4019)  loss_n_100: 0.4157 (0.4188)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [250/845]  eta: 0:03:19  loss: 1.7121 (1.6165)  loss_n_40: 0.4225 (0.4210)  loss_n_60: 0.4010 (0.3766)  loss_n_80: 0.4124 (0.4011)  loss_n_100: 0.4157 (0.4178)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [260/845]  eta: 0:03:16  loss: 1.4890 (1.6182)  loss_n_40: 0.3737 (0.4215)  loss_n_60: 0.3513 (0.3766)  loss_n_80: 0.3838 (0.4020)  loss_n_100: 0.3914 (0.4182)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [270/845]  eta: 0:03:12  loss: 1.5202 (1.6136)  loss_n_40: 0.3729 (0.4195)  loss_n_60: 0.3629 (0.3754)  loss_n_80: 0.3872 (0.4013)  loss_n_100: 0.3956 (0.4173)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [280/845]  eta: 0:03:09  loss: 1.4954 (1.6121)  loss_n_40: 0.3729 (0.4191)  loss_n_60: 0.3495 (0.3754)  loss_n_80: 0.3771 (0.4006)  loss_n_100: 0.3956 (0.4169)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [290/845]  eta: 0:03:06  loss: 1.3729 (1.6026)  loss_n_40: 0.3557 (0.4160)  loss_n_60: 0.3168 (0.3732)  loss_n_80: 0.3448 (0.3988)  loss_n_100: 0.3571 (0.4147)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [300/845]  eta: 0:03:02  loss: 1.2881 (1.6011)  loss_n_40: 0.3253 (0.4151)  loss_n_60: 0.3148 (0.3727)  loss_n_80: 0.3448 (0.3988)  loss_n_100: 0.3558 (0.4146)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [310/845]  eta: 0:02:59  loss: 1.5638 (1.6002)  loss_n_40: 0.3842 (0.4141)  loss_n_60: 0.3652 (0.3724)  loss_n_80: 0.4027 (0.3989)  loss_n_100: 0.4235 (0.4148)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [320/845]  eta: 0:02:56  loss: 1.4866 (1.5958)  loss_n_40: 0.3754 (0.4122)  loss_n_60: 0.3536 (0.3714)  loss_n_80: 0.3866 (0.3983)  loss_n_100: 0.4012 (0.4139)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [330/845]  eta: 0:02:52  loss: 1.4896 (1.5959)  loss_n_40: 0.3858 (0.4119)  loss_n_60: 0.3535 (0.3715)  loss_n_80: 0.3866 (0.3985)  loss_n_100: 0.3962 (0.4140)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [340/845]  eta: 0:02:49  loss: 1.5113 (1.5949)  loss_n_40: 0.3928 (0.4114)  loss_n_60: 0.3535 (0.3712)  loss_n_80: 0.3986 (0.3985)  loss_n_100: 0.3883 (0.4138)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [350/845]  eta: 0:02:45  loss: 1.5186 (1.5970)  loss_n_40: 0.3828 (0.4109)  loss_n_60: 0.3646 (0.3718)  loss_n_80: 0.4004 (0.3994)  loss_n_100: 0.4126 (0.4149)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [360/845]  eta: 0:02:42  loss: 1.7376 (1.6020)  loss_n_40: 0.3773 (0.4128)  loss_n_60: 0.4129 (0.3730)  loss_n_80: 0.4341 (0.4004)  loss_n_100: 0.4732 (0.4159)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [370/845]  eta: 0:02:39  loss: 1.4863 (1.5982)  loss_n_40: 0.3758 (0.4117)  loss_n_60: 0.3745 (0.3722)  loss_n_80: 0.3885 (0.3994)  loss_n_100: 0.4028 (0.4149)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:16]  [380/845]  eta: 0:02:35  loss: 1.4678 (1.5988)  loss_n_40: 0.3747 (0.4122)  loss_n_60: 0.3373 (0.3728)  loss_n_80: 0.3642 (0.3991)  loss_n_100: 0.3482 (0.4147)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [390/845]  eta: 0:02:32  loss: 1.3014 (1.5932)  loss_n_40: 0.3339 (0.4102)  loss_n_60: 0.3169 (0.3716)  loss_n_80: 0.3448 (0.3980)  loss_n_100: 0.3578 (0.4134)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [400/845]  eta: 0:02:29  loss: 1.3874 (1.5948)  loss_n_40: 0.3643 (0.4112)  loss_n_60: 0.3215 (0.3719)  loss_n_80: 0.3454 (0.3979)  loss_n_100: 0.3635 (0.4138)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [410/845]  eta: 0:02:25  loss: 1.6502 (1.5973)  loss_n_40: 0.4118 (0.4116)  loss_n_60: 0.3884 (0.3727)  loss_n_80: 0.3936 (0.3988)  loss_n_100: 0.4001 (0.4142)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [420/845]  eta: 0:02:22  loss: 1.7371 (1.5988)  loss_n_40: 0.4210 (0.4122)  loss_n_60: 0.3949 (0.3731)  loss_n_80: 0.4229 (0.3989)  loss_n_100: 0.4246 (0.4145)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [430/845]  eta: 0:02:19  loss: 1.5969 (1.5995)  loss_n_40: 0.3846 (0.4114)  loss_n_60: 0.3585 (0.3732)  loss_n_80: 0.4084 (0.3995)  loss_n_100: 0.4246 (0.4154)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [440/845]  eta: 0:02:15  loss: 1.5284 (1.5977)  loss_n_40: 0.3752 (0.4109)  loss_n_60: 0.3730 (0.3728)  loss_n_80: 0.3887 (0.3991)  loss_n_100: 0.3931 (0.4149)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [450/845]  eta: 0:02:12  loss: 1.6173 (1.6005)  loss_n_40: 0.4075 (0.4112)  loss_n_60: 0.3730 (0.3735)  loss_n_80: 0.4252 (0.4000)  loss_n_100: 0.4272 (0.4158)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [460/845]  eta: 0:02:08  loss: 1.5560 (1.6006)  loss_n_40: 0.3799 (0.4105)  loss_n_60: 0.3604 (0.3735)  loss_n_80: 0.4071 (0.4003)  loss_n_100: 0.4096 (0.4162)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [470/845]  eta: 0:02:05  loss: 1.6850 (1.6020)  loss_n_40: 0.3799 (0.4111)  loss_n_60: 0.3395 (0.3738)  loss_n_80: 0.3764 (0.4003)  loss_n_100: 0.4111 (0.4168)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [480/845]  eta: 0:02:02  loss: 1.7336 (1.6030)  loss_n_40: 0.4089 (0.4108)  loss_n_60: 0.4075 (0.3741)  loss_n_80: 0.4328 (0.4008)  loss_n_100: 0.4532 (0.4172)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [490/845]  eta: 0:01:58  loss: 1.5298 (1.6007)  loss_n_40: 0.3910 (0.4104)  loss_n_60: 0.3443 (0.3735)  loss_n_80: 0.3800 (0.4003)  loss_n_100: 0.3892 (0.4165)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [500/845]  eta: 0:01:55  loss: 1.4455 (1.5994)  loss_n_40: 0.3640 (0.4095)  loss_n_60: 0.3364 (0.3733)  loss_n_80: 0.3751 (0.4002)  loss_n_100: 0.3890 (0.4164)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [510/845]  eta: 0:01:52  loss: 1.4190 (1.5972)  loss_n_40: 0.3660 (0.4089)  loss_n_60: 0.3327 (0.3727)  loss_n_80: 0.3489 (0.3997)  loss_n_100: 0.3589 (0.4159)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [520/845]  eta: 0:01:48  loss: 1.5773 (1.5973)  loss_n_40: 0.3852 (0.4088)  loss_n_60: 0.3607 (0.3727)  loss_n_80: 0.4042 (0.3998)  loss_n_100: 0.4141 (0.4160)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [530/845]  eta: 0:01:45  loss: 1.5859 (1.5969)  loss_n_40: 0.3852 (0.4086)  loss_n_60: 0.3641 (0.3728)  loss_n_80: 0.4042 (0.3997)  loss_n_100: 0.4190 (0.4159)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [540/845]  eta: 0:01:42  loss: 1.5031 (1.6002)  loss_n_40: 0.3792 (0.4100)  loss_n_60: 0.3499 (0.3738)  loss_n_80: 0.3995 (0.4002)  loss_n_100: 0.3958 (0.4163)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [550/845]  eta: 0:01:38  loss: 1.6324 (1.5999)  loss_n_40: 0.4005 (0.4098)  loss_n_60: 0.3703 (0.3736)  loss_n_80: 0.3997 (0.4003)  loss_n_100: 0.4098 (0.4162)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [560/845]  eta: 0:01:35  loss: 1.6702 (1.6033)  loss_n_40: 0.3903 (0.4107)  loss_n_60: 0.3917 (0.3747)  loss_n_80: 0.4272 (0.4011)  loss_n_100: 0.4114 (0.4168)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [570/845]  eta: 0:01:32  loss: 1.6707 (1.6041)  loss_n_40: 0.3879 (0.4111)  loss_n_60: 0.3928 (0.3749)  loss_n_80: 0.4060 (0.4011)  loss_n_100: 0.4114 (0.4170)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [580/845]  eta: 0:01:28  loss: 1.5320 (1.6047)  loss_n_40: 0.3877 (0.4115)  loss_n_60: 0.3638 (0.3752)  loss_n_80: 0.3800 (0.4011)  loss_n_100: 0.3834 (0.4169)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [590/845]  eta: 0:01:25  loss: 1.6907 (1.6050)  loss_n_40: 0.3966 (0.4119)  loss_n_60: 0.3781 (0.3752)  loss_n_80: 0.4190 (0.4010)  loss_n_100: 0.4272 (0.4169)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [600/845]  eta: 0:01:22  loss: 1.7650 (1.6065)  loss_n_40: 0.4201 (0.4126)  loss_n_60: 0.4067 (0.3755)  loss_n_80: 0.4195 (0.4012)  loss_n_100: 0.4386 (0.4172)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [610/845]  eta: 0:01:18  loss: 1.4941 (1.6047)  loss_n_40: 0.3948 (0.4119)  loss_n_60: 0.3503 (0.3750)  loss_n_80: 0.3778 (0.4009)  loss_n_100: 0.3848 (0.4169)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [620/845]  eta: 0:01:15  loss: 1.4941 (1.6057)  loss_n_40: 0.3874 (0.4117)  loss_n_60: 0.3546 (0.3753)  loss_n_80: 0.3713 (0.4013)  loss_n_100: 0.3749 (0.4174)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:16]  [630/845]  eta: 0:01:11  loss: 1.7390 (1.6082)  loss_n_40: 0.4109 (0.4121)  loss_n_60: 0.3969 (0.3758)  loss_n_80: 0.4416 (0.4020)  loss_n_100: 0.4695 (0.4182)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [640/845]  eta: 0:01:08  loss: 1.5291 (1.6054)  loss_n_40: 0.3789 (0.4113)  loss_n_60: 0.3410 (0.3752)  loss_n_80: 0.3920 (0.4013)  loss_n_100: 0.4125 (0.4175)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [650/845]  eta: 0:01:05  loss: 1.3030 (1.6025)  loss_n_40: 0.3322 (0.4109)  loss_n_60: 0.3121 (0.3746)  loss_n_80: 0.3326 (0.4005)  loss_n_100: 0.3437 (0.4165)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [660/845]  eta: 0:01:01  loss: 1.3299 (1.6016)  loss_n_40: 0.3227 (0.4105)  loss_n_60: 0.3121 (0.3744)  loss_n_80: 0.3374 (0.4003)  loss_n_100: 0.3527 (0.4163)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [670/845]  eta: 0:00:58  loss: 1.4464 (1.6017)  loss_n_40: 0.3693 (0.4111)  loss_n_60: 0.3319 (0.3744)  loss_n_80: 0.3697 (0.4001)  loss_n_100: 0.3838 (0.4161)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [680/845]  eta: 0:00:55  loss: 1.5348 (1.6016)  loss_n_40: 0.3921 (0.4112)  loss_n_60: 0.3506 (0.3743)  loss_n_80: 0.3820 (0.4000)  loss_n_100: 0.3976 (0.4161)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [690/845]  eta: 0:00:51  loss: 1.6464 (1.6021)  loss_n_40: 0.3793 (0.4111)  loss_n_60: 0.3623 (0.3743)  loss_n_80: 0.4366 (0.4003)  loss_n_100: 0.4063 (0.4164)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [700/845]  eta: 0:00:48  loss: 1.2933 (1.6021)  loss_n_40: 0.3213 (0.4114)  loss_n_60: 0.3098 (0.3743)  loss_n_80: 0.3388 (0.4000)  loss_n_100: 0.3516 (0.4164)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [710/845]  eta: 0:00:45  loss: 1.4930 (1.6037)  loss_n_40: 0.3647 (0.4116)  loss_n_60: 0.3535 (0.3749)  loss_n_80: 0.3857 (0.4004)  loss_n_100: 0.3879 (0.4169)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [720/845]  eta: 0:00:41  loss: 1.4930 (1.6027)  loss_n_40: 0.3761 (0.4112)  loss_n_60: 0.3583 (0.3748)  loss_n_80: 0.3868 (0.4001)  loss_n_100: 0.3879 (0.4166)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [730/845]  eta: 0:00:38  loss: 1.5222 (1.6032)  loss_n_40: 0.4032 (0.4113)  loss_n_60: 0.3583 (0.3750)  loss_n_80: 0.3724 (0.4002)  loss_n_100: 0.3711 (0.4167)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [740/845]  eta: 0:00:35  loss: 1.6133 (1.6028)  loss_n_40: 0.4012 (0.4114)  loss_n_60: 0.3594 (0.3749)  loss_n_80: 0.3724 (0.3999)  loss_n_100: 0.4126 (0.4166)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [750/845]  eta: 0:00:31  loss: 1.6133 (1.6035)  loss_n_40: 0.3968 (0.4112)  loss_n_60: 0.3594 (0.3750)  loss_n_80: 0.3974 (0.4002)  loss_n_100: 0.4155 (0.4171)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [760/845]  eta: 0:00:28  loss: 1.5060 (1.6023)  loss_n_40: 0.3899 (0.4108)  loss_n_60: 0.3340 (0.3747)  loss_n_80: 0.3842 (0.4000)  loss_n_100: 0.3853 (0.4168)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [770/845]  eta: 0:00:25  loss: 1.3798 (1.6018)  loss_n_40: 0.3635 (0.4107)  loss_n_60: 0.3247 (0.3746)  loss_n_80: 0.3406 (0.3999)  loss_n_100: 0.3510 (0.4166)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [780/845]  eta: 0:00:21  loss: 1.3204 (1.5982)  loss_n_40: 0.3422 (0.4096)  loss_n_60: 0.2958 (0.3738)  loss_n_80: 0.3388 (0.3991)  loss_n_100: 0.3456 (0.4157)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [790/845]  eta: 0:00:18  loss: 1.4495 (1.6003)  loss_n_40: 0.3682 (0.4101)  loss_n_60: 0.3624 (0.3744)  loss_n_80: 0.3585 (0.3996)  loss_n_100: 0.3564 (0.4163)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [800/845]  eta: 0:00:15  loss: 1.7178 (1.6015)  loss_n_40: 0.4086 (0.4103)  loss_n_60: 0.4045 (0.3747)  loss_n_80: 0.4465 (0.3999)  loss_n_100: 0.4629 (0.4167)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [810/845]  eta: 0:00:11  loss: 1.7792 (1.6032)  loss_n_40: 0.4353 (0.4112)  loss_n_60: 0.4072 (0.3750)  loss_n_80: 0.4441 (0.4002)  loss_n_100: 0.4629 (0.4169)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [820/845]  eta: 0:00:08  loss: 1.7101 (1.6040)  loss_n_40: 0.3861 (0.4112)  loss_n_60: 0.3891 (0.3752)  loss_n_80: 0.4029 (0.4005)  loss_n_100: 0.4133 (0.4171)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [830/845]  eta: 0:00:05  loss: 1.4045 (1.6023)  loss_n_40: 0.3510 (0.4109)  loss_n_60: 0.3268 (0.3748)  loss_n_80: 0.3683 (0.4000)  loss_n_100: 0.3760 (0.4166)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [840/845]  eta: 0:00:01  loss: 1.3895 (1.6021)  loss_n_40: 0.3688 (0.4107)  loss_n_60: 0.3328 (0.3748)  loss_n_80: 0.3510 (0.4000)  loss_n_100: 0.3547 (0.4166)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16]  [844/845]  eta: 0:00:00  loss: 1.5191 (1.6020)  loss_n_40: 0.3844 (0.4106)  loss_n_60: 0.3566 (0.3748)  loss_n_80: 0.3848 (0.4000)  loss_n_100: 0.3760 (0.4166)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:16] Total time: 0:04:42 (0.3348 s / it)\n",
      "Averaged stats: loss: 1.5191 (1.6020)  loss_n_40: 0.3844 (0.4106)  loss_n_60: 0.3566 (0.3748)  loss_n_80: 0.3848 (0.4000)  loss_n_100: 0.3760 (0.4166)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_16_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.417%\n",
      "Min loss_n_100: 0.417\n",
      "Best Epoch: 16.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [   0/1724]  eta: 2:00:52  lr: 0.000200  loss: 1.6381 (1.6381)  loss_n_40: 0.4586 (0.4586)  loss_n_60: 0.3483 (0.3483)  loss_n_80: 0.4082 (0.4082)  loss_n_100: 0.4231 (0.4231)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.2070  data: 0.4511  max mem: 46473\n",
      "Train: [epoch:17]  [  10/1724]  eta: 1:52:43  lr: 0.000200  loss: 1.5732 (1.6457)  loss_n_40: 0.4185 (0.4306)  loss_n_60: 0.3700 (0.3849)  loss_n_80: 0.3926 (0.4071)  loss_n_100: 0.4092 (0.4232)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9458  data: 0.0412  max mem: 46473\n",
      "Train: [epoch:17]  [  20/1724]  eta: 1:51:41  lr: 0.000200  loss: 1.5387 (1.5572)  loss_n_40: 0.3923 (0.4033)  loss_n_60: 0.3603 (0.3679)  loss_n_80: 0.3739 (0.3855)  loss_n_100: 0.3890 (0.4005)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [  30/1724]  eta: 1:50:53  lr: 0.000200  loss: 1.3911 (1.5059)  loss_n_40: 0.3528 (0.3924)  loss_n_60: 0.3320 (0.3566)  loss_n_80: 0.3362 (0.3710)  loss_n_100: 0.3570 (0.3859)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [  40/1724]  eta: 1:50:10  lr: 0.000200  loss: 1.3388 (1.4991)  loss_n_40: 0.3398 (0.3880)  loss_n_60: 0.3189 (0.3515)  loss_n_80: 0.3312 (0.3646)  loss_n_100: 0.3489 (0.3802)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0024)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0124)  time: 3.9185  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [  50/1724]  eta: 1:49:29  lr: 0.000200  loss: 1.2639 (1.4587)  loss_n_40: 0.3360 (0.3780)  loss_n_60: 0.2996 (0.3430)  loss_n_80: 0.3279 (0.3550)  loss_n_100: 0.3370 (0.3708)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0019)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0100)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [  60/1724]  eta: 1:48:48  lr: 0.000200  loss: 1.3226 (1.4487)  loss_n_40: 0.3442 (0.3783)  loss_n_60: 0.3078 (0.3404)  loss_n_80: 0.3279 (0.3523)  loss_n_100: 0.3408 (0.3678)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0016)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0084)  time: 3.9182  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [  70/1724]  eta: 1:48:07  lr: 0.000200  loss: 1.3226 (1.4295)  loss_n_40: 0.3442 (0.3735)  loss_n_60: 0.3206 (0.3359)  loss_n_80: 0.3288 (0.3481)  loss_n_100: 0.3408 (0.3635)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0014)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0072)  time: 3.9176  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [  80/1724]  eta: 1:47:27  lr: 0.000200  loss: 1.1903 (1.4016)  loss_n_40: 0.3134 (0.3661)  loss_n_60: 0.2971 (0.3304)  loss_n_80: 0.2991 (0.3406)  loss_n_100: 0.3106 (0.3553)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0012)  triple_60: 0.0000 (0.0003)  triple_40: 0.0000 (0.0077)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [  90/1724]  eta: 1:46:47  lr: 0.000200  loss: 1.2098 (1.3875)  loss_n_40: 0.3081 (0.3625)  loss_n_60: 0.2914 (0.3268)  loss_n_80: 0.2999 (0.3373)  loss_n_100: 0.3118 (0.3526)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0011)  triple_60: 0.0000 (0.0003)  triple_40: 0.0000 (0.0068)  time: 3.9165  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 100/1724]  eta: 1:46:07  lr: 0.000200  loss: 1.2098 (1.3708)  loss_n_40: 0.2937 (0.3566)  loss_n_60: 0.2821 (0.3230)  loss_n_80: 0.2999 (0.3346)  loss_n_100: 0.3118 (0.3491)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0010)  triple_60: 0.0000 (0.0003)  triple_40: 0.0000 (0.0062)  time: 3.9176  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 110/1724]  eta: 1:45:27  lr: 0.000200  loss: 1.1564 (1.3642)  loss_n_40: 0.3190 (0.3559)  loss_n_60: 0.2789 (0.3216)  loss_n_80: 0.2874 (0.3333)  loss_n_100: 0.3006 (0.3468)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0009)  triple_60: 0.0000 (0.0002)  triple_40: 0.0000 (0.0056)  time: 3.9169  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 120/1724]  eta: 1:44:47  lr: 0.000200  loss: 1.2270 (1.3499)  loss_n_40: 0.3350 (0.3530)  loss_n_60: 0.2789 (0.3183)  loss_n_80: 0.2894 (0.3295)  loss_n_100: 0.3008 (0.3430)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0008)  triple_60: 0.0000 (0.0002)  triple_40: 0.0000 (0.0051)  time: 3.9167  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 130/1724]  eta: 1:44:08  lr: 0.000200  loss: 1.2363 (1.3402)  loss_n_40: 0.3002 (0.3500)  loss_n_60: 0.2898 (0.3160)  loss_n_80: 0.2933 (0.3273)  loss_n_100: 0.3030 (0.3412)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0008)  triple_60: 0.0000 (0.0002)  triple_40: 0.0000 (0.0048)  time: 3.9163  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 140/1724]  eta: 1:43:28  lr: 0.000200  loss: 1.1134 (1.3225)  loss_n_40: 0.2742 (0.3455)  loss_n_60: 0.2726 (0.3121)  loss_n_80: 0.2736 (0.3229)  loss_n_100: 0.2875 (0.3366)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0007)  triple_60: 0.0000 (0.0002)  triple_40: 0.0000 (0.0044)  time: 3.9155  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 150/1724]  eta: 1:42:48  lr: 0.000200  loss: 1.0399 (1.3092)  loss_n_40: 0.2662 (0.3418)  loss_n_60: 0.2434 (0.3093)  loss_n_80: 0.2556 (0.3198)  loss_n_100: 0.2665 (0.3334)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0007)  triple_60: 0.0000 (0.0002)  triple_40: 0.0000 (0.0041)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 160/1724]  eta: 1:42:09  lr: 0.000200  loss: 1.0871 (1.3077)  loss_n_40: 0.2673 (0.3379)  loss_n_60: 0.2608 (0.3070)  loss_n_80: 0.2695 (0.3180)  loss_n_100: 0.2885 (0.3320)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0038)  triple_60: 0.0000 (0.0018)  triple_40: 0.0000 (0.0059)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 170/1724]  eta: 1:41:30  lr: 0.000200  loss: 1.4483 (1.3241)  loss_n_40: 0.3278 (0.3393)  loss_n_60: 0.3146 (0.3105)  loss_n_80: 0.3484 (0.3235)  loss_n_100: 0.3806 (0.3383)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0036)  triple_60: 0.0000 (0.0017)  triple_40: 0.0000 (0.0055)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 180/1724]  eta: 1:40:50  lr: 0.000200  loss: 1.5730 (1.3422)  loss_n_40: 0.3690 (0.3427)  loss_n_60: 0.3655 (0.3145)  loss_n_80: 0.3972 (0.3290)  loss_n_100: 0.4466 (0.3443)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0034)  triple_60: 0.0000 (0.0016)  triple_40: 0.0000 (0.0052)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 190/1724]  eta: 1:40:11  lr: 0.000200  loss: 1.5730 (1.3485)  loss_n_40: 0.3808 (0.3451)  loss_n_60: 0.3509 (0.3158)  loss_n_80: 0.3825 (0.3306)  loss_n_100: 0.4069 (0.3458)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0032)  triple_60: 0.0000 (0.0015)  triple_40: 0.0000 (0.0050)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 200/1724]  eta: 1:39:31  lr: 0.000200  loss: 1.4496 (1.3554)  loss_n_40: 0.3695 (0.3471)  loss_n_60: 0.3458 (0.3176)  loss_n_80: 0.3598 (0.3324)  loss_n_100: 0.3693 (0.3477)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0030)  triple_60: 0.0000 (0.0014)  triple_40: 0.0000 (0.0047)  time: 3.9170  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 210/1724]  eta: 1:38:52  lr: 0.000200  loss: 1.3079 (1.3490)  loss_n_40: 0.3306 (0.3454)  loss_n_60: 0.3093 (0.3164)  loss_n_80: 0.3344 (0.3311)  loss_n_100: 0.3429 (0.3460)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0029)  triple_60: 0.0000 (0.0014)  triple_40: 0.0000 (0.0045)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 220/1724]  eta: 1:38:13  lr: 0.000200  loss: 1.2735 (1.3567)  loss_n_40: 0.2986 (0.3437)  loss_n_60: 0.2955 (0.3160)  loss_n_80: 0.3263 (0.3310)  loss_n_100: 0.3429 (0.3464)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0064)  triple_60: 0.0000 (0.0013)  triple_40: 0.0000 (0.0106)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 230/1724]  eta: 1:37:34  lr: 0.000200  loss: 1.2919 (1.3593)  loss_n_40: 0.2986 (0.3435)  loss_n_60: 0.3072 (0.3167)  loss_n_80: 0.3296 (0.3322)  loss_n_100: 0.3534 (0.3482)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0012)  triple_40: 0.0000 (0.0102)  time: 3.9170  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [ 240/1724]  eta: 1:36:54  lr: 0.000200  loss: 1.3029 (1.4108)  loss_n_40: 0.3028 (0.3426)  loss_n_60: 0.3058 (0.3166)  loss_n_80: 0.3317 (0.3328)  loss_n_100: 0.3466 (0.3487)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0231)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0291)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 250/1724]  eta: 1:36:15  lr: 0.000200  loss: 2.3161 (1.4948)  loss_n_40: 0.4163 (0.3481)  loss_n_60: 0.4057 (0.3289)  loss_n_80: 0.4765 (0.3496)  loss_n_100: 0.5254 (0.3704)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0445)  triple_60: 0.0000 (0.0205)  triple_40: 0.0000 (0.0318)  time: 3.9172  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 260/1724]  eta: 1:35:36  lr: 0.000200  loss: 2.7152 (1.5377)  loss_n_40: 0.4957 (0.3550)  loss_n_60: 0.6195 (0.3393)  loss_n_80: 0.7177 (0.3631)  loss_n_100: 0.8306 (0.3861)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0428)  triple_60: 0.0000 (0.0197)  triple_40: 0.0000 (0.0305)  time: 3.9156  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 270/1724]  eta: 1:34:56  lr: 0.000200  loss: 2.4923 (1.5658)  loss_n_40: 0.5117 (0.3618)  loss_n_60: 0.5767 (0.3459)  loss_n_80: 0.6736 (0.3720)  loss_n_100: 0.7347 (0.3956)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0412)  triple_60: 0.0000 (0.0190)  triple_40: 0.0000 (0.0294)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 280/1724]  eta: 1:34:17  lr: 0.000200  loss: 1.9098 (1.5737)  loss_n_40: 0.4041 (0.3634)  loss_n_60: 0.4329 (0.3481)  loss_n_80: 0.5334 (0.3758)  loss_n_100: 0.5394 (0.3990)  triple_100: 0.0000 (0.0010)  triple_80: 0.0000 (0.0397)  triple_60: 0.0000 (0.0183)  triple_40: 0.0000 (0.0284)  time: 3.9152  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 290/1724]  eta: 1:33:38  lr: 0.000200  loss: 1.5541 (1.5743)  loss_n_40: 0.3515 (0.3635)  loss_n_60: 0.3610 (0.3487)  loss_n_80: 0.4145 (0.3774)  loss_n_100: 0.4332 (0.4002)  triple_100: 0.0000 (0.0010)  triple_80: 0.0000 (0.0384)  triple_60: 0.0000 (0.0177)  triple_40: 0.0000 (0.0274)  time: 3.9150  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 300/1724]  eta: 1:32:58  lr: 0.000200  loss: 1.4994 (1.5705)  loss_n_40: 0.3426 (0.3634)  loss_n_60: 0.3526 (0.3485)  loss_n_80: 0.3969 (0.3774)  loss_n_100: 0.3977 (0.3996)  triple_100: 0.0000 (0.0010)  triple_80: 0.0000 (0.0371)  triple_60: 0.0000 (0.0171)  triple_40: 0.0000 (0.0265)  time: 3.9155  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 310/1724]  eta: 1:32:19  lr: 0.000200  loss: 1.4941 (1.5702)  loss_n_40: 0.3504 (0.3642)  loss_n_60: 0.3526 (0.3489)  loss_n_80: 0.3755 (0.3778)  loss_n_100: 0.3977 (0.4003)  triple_100: 0.0000 (0.0009)  triple_80: 0.0000 (0.0359)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0256)  time: 3.9160  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 320/1724]  eta: 1:31:40  lr: 0.000200  loss: 1.4548 (1.5645)  loss_n_40: 0.3504 (0.3634)  loss_n_60: 0.3222 (0.3481)  loss_n_80: 0.3502 (0.3771)  loss_n_100: 0.3792 (0.3994)  triple_100: 0.0000 (0.0009)  triple_80: 0.0000 (0.0348)  triple_60: 0.0000 (0.0160)  triple_40: 0.0000 (0.0248)  time: 3.9163  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 330/1724]  eta: 1:31:01  lr: 0.000200  loss: 1.3283 (1.5592)  loss_n_40: 0.3142 (0.3631)  loss_n_60: 0.3142 (0.3473)  loss_n_80: 0.3396 (0.3762)  loss_n_100: 0.3600 (0.3984)  triple_100: 0.0000 (0.0009)  triple_80: 0.0000 (0.0337)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0241)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 340/1724]  eta: 1:30:22  lr: 0.000200  loss: 1.4198 (1.5590)  loss_n_40: 0.3293 (0.3632)  loss_n_60: 0.3345 (0.3476)  loss_n_80: 0.3657 (0.3763)  loss_n_100: 0.3747 (0.3987)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0327)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0234)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 350/1724]  eta: 1:29:42  lr: 0.000200  loss: 1.7906 (1.5709)  loss_n_40: 0.3594 (0.3636)  loss_n_60: 0.3919 (0.3503)  loss_n_80: 0.4443 (0.3807)  loss_n_100: 0.5201 (0.4052)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0318)  triple_60: 0.0000 (0.0151)  triple_40: 0.0000 (0.0227)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 360/1724]  eta: 1:29:03  lr: 0.000200  loss: 1.6465 (1.5681)  loss_n_40: 0.3304 (0.3628)  loss_n_60: 0.3660 (0.3499)  loss_n_80: 0.4443 (0.3807)  loss_n_100: 0.5177 (0.4057)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0309)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0221)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 370/1724]  eta: 1:28:24  lr: 0.000200  loss: 1.5265 (1.5668)  loss_n_40: 0.3167 (0.3620)  loss_n_60: 0.3414 (0.3494)  loss_n_80: 0.3658 (0.3802)  loss_n_100: 0.4003 (0.4055)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0314)  triple_60: 0.0000 (0.0148)  triple_40: 0.0000 (0.0221)  time: 3.9195  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 380/1724]  eta: 1:27:45  lr: 0.000200  loss: 1.5549 (1.5666)  loss_n_40: 0.3427 (0.3620)  loss_n_60: 0.3507 (0.3496)  loss_n_80: 0.4003 (0.3807)  loss_n_100: 0.4144 (0.4064)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0306)  triple_60: 0.0000 (0.0144)  triple_40: 0.0000 (0.0215)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 390/1724]  eta: 1:27:06  lr: 0.000200  loss: 1.5383 (1.5646)  loss_n_40: 0.3550 (0.3616)  loss_n_60: 0.3559 (0.3495)  loss_n_80: 0.4003 (0.3809)  loss_n_100: 0.4200 (0.4064)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0298)  triple_60: 0.0000 (0.0140)  triple_40: 0.0000 (0.0210)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 400/1724]  eta: 1:26:27  lr: 0.000200  loss: 1.4712 (1.5609)  loss_n_40: 0.3550 (0.3621)  loss_n_60: 0.3352 (0.3489)  loss_n_80: 0.3584 (0.3801)  loss_n_100: 0.3757 (0.4054)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0291)  triple_60: 0.0000 (0.0137)  triple_40: 0.0000 (0.0205)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 410/1724]  eta: 1:25:47  lr: 0.000200  loss: 1.3127 (1.5558)  loss_n_40: 0.3390 (0.3614)  loss_n_60: 0.3177 (0.3482)  loss_n_80: 0.3312 (0.3791)  loss_n_100: 0.3340 (0.4041)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0283)  triple_60: 0.0000 (0.0133)  triple_40: 0.0000 (0.0200)  time: 3.9185  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 420/1724]  eta: 1:25:08  lr: 0.000200  loss: 1.2530 (1.5476)  loss_n_40: 0.3070 (0.3600)  loss_n_60: 0.2987 (0.3467)  loss_n_80: 0.3171 (0.3774)  loss_n_100: 0.3305 (0.4021)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0277)  triple_60: 0.0000 (0.0130)  triple_40: 0.0000 (0.0195)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 430/1724]  eta: 1:24:29  lr: 0.000200  loss: 1.2611 (1.5433)  loss_n_40: 0.3089 (0.3600)  loss_n_60: 0.3007 (0.3461)  loss_n_80: 0.3171 (0.3764)  loss_n_100: 0.3333 (0.4008)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0270)  triple_60: 0.0000 (0.0127)  triple_40: 0.0000 (0.0190)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 440/1724]  eta: 1:23:50  lr: 0.000200  loss: 1.2591 (1.5355)  loss_n_40: 0.3211 (0.3586)  loss_n_60: 0.3007 (0.3448)  loss_n_80: 0.3081 (0.3747)  loss_n_100: 0.3273 (0.3988)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0264)  triple_60: 0.0000 (0.0124)  triple_40: 0.0000 (0.0186)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 450/1724]  eta: 1:23:11  lr: 0.000200  loss: 1.1126 (1.5263)  loss_n_40: 0.2744 (0.3568)  loss_n_60: 0.2686 (0.3431)  loss_n_80: 0.2804 (0.3726)  loss_n_100: 0.2940 (0.3965)  triple_100: 0.0000 (0.0012)  triple_80: 0.0000 (0.0258)  triple_60: 0.0000 (0.0122)  triple_40: 0.0000 (0.0182)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 460/1724]  eta: 1:22:31  lr: 0.000200  loss: 1.1749 (1.5196)  loss_n_40: 0.2874 (0.3556)  loss_n_60: 0.2764 (0.3418)  loss_n_80: 0.2955 (0.3712)  loss_n_100: 0.3107 (0.3949)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0253)  triple_60: 0.0000 (0.0119)  triple_40: 0.0000 (0.0178)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 470/1724]  eta: 1:21:52  lr: 0.000200  loss: 1.2222 (1.5134)  loss_n_40: 0.2875 (0.3546)  loss_n_60: 0.2878 (0.3407)  loss_n_80: 0.3060 (0.3697)  loss_n_100: 0.3239 (0.3934)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0247)  triple_60: 0.0000 (0.0116)  triple_40: 0.0000 (0.0175)  time: 3.9157  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [ 480/1724]  eta: 1:21:13  lr: 0.000200  loss: 1.2149 (1.5082)  loss_n_40: 0.3027 (0.3537)  loss_n_60: 0.2878 (0.3397)  loss_n_80: 0.3042 (0.3684)  loss_n_100: 0.3204 (0.3921)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0242)  triple_60: 0.0000 (0.0114)  triple_40: 0.0000 (0.0175)  time: 3.9162  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 490/1724]  eta: 1:20:34  lr: 0.000200  loss: 1.1831 (1.5009)  loss_n_40: 0.2571 (0.3520)  loss_n_60: 0.2699 (0.3383)  loss_n_80: 0.2987 (0.3669)  loss_n_100: 0.3179 (0.3905)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0237)  triple_60: 0.0000 (0.0112)  triple_40: 0.0000 (0.0172)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 500/1724]  eta: 1:19:55  lr: 0.000200  loss: 1.0602 (1.4919)  loss_n_40: 0.2507 (0.3501)  loss_n_60: 0.2506 (0.3365)  loss_n_80: 0.2679 (0.3650)  loss_n_100: 0.2764 (0.3883)  triple_100: 0.0000 (0.0011)  triple_80: 0.0000 (0.0233)  triple_60: 0.0000 (0.0109)  triple_40: 0.0000 (0.0168)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 510/1724]  eta: 1:19:15  lr: 0.000200  loss: 1.1296 (1.4903)  loss_n_40: 0.2707 (0.3490)  loss_n_60: 0.2739 (0.3360)  loss_n_80: 0.2894 (0.3645)  loss_n_100: 0.2927 (0.3879)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0231)  triple_60: 0.0000 (0.0114)  triple_40: 0.0000 (0.0170)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 520/1724]  eta: 1:18:36  lr: 0.000200  loss: 1.3692 (1.4991)  loss_n_40: 0.3209 (0.3494)  loss_n_60: 0.3180 (0.3368)  loss_n_80: 0.3523 (0.3655)  loss_n_100: 0.3769 (0.3891)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0267)  triple_60: 0.0000 (0.0113)  triple_40: 0.0000 (0.0169)  time: 3.9174  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 530/1724]  eta: 1:17:57  lr: 0.000200  loss: 1.7162 (1.5028)  loss_n_40: 0.3836 (0.3502)  loss_n_60: 0.4025 (0.3379)  loss_n_80: 0.4370 (0.3669)  loss_n_100: 0.4621 (0.3906)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0262)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0166)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 540/1724]  eta: 1:17:18  lr: 0.000200  loss: 1.6210 (1.5058)  loss_n_40: 0.3624 (0.3510)  loss_n_60: 0.3778 (0.3391)  loss_n_80: 0.4213 (0.3680)  loss_n_100: 0.4481 (0.3915)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0257)  triple_60: 0.0000 (0.0109)  triple_40: 0.0000 (0.0163)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 550/1724]  eta: 1:16:39  lr: 0.000200  loss: 1.4480 (1.5039)  loss_n_40: 0.3426 (0.3508)  loss_n_60: 0.3541 (0.3392)  loss_n_80: 0.3704 (0.3677)  loss_n_100: 0.3797 (0.3910)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0253)  triple_60: 0.0000 (0.0107)  triple_40: 0.0000 (0.0160)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 560/1724]  eta: 1:16:00  lr: 0.000200  loss: 1.3512 (1.5024)  loss_n_40: 0.3213 (0.3508)  loss_n_60: 0.3256 (0.3392)  loss_n_80: 0.3262 (0.3675)  loss_n_100: 0.3493 (0.3907)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0248)  triple_60: 0.0000 (0.0105)  triple_40: 0.0000 (0.0157)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 570/1724]  eta: 1:15:20  lr: 0.000200  loss: 1.2854 (1.4985)  loss_n_40: 0.3084 (0.3503)  loss_n_60: 0.3193 (0.3387)  loss_n_80: 0.3237 (0.3666)  loss_n_100: 0.3368 (0.3896)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0244)  triple_60: 0.0000 (0.0103)  triple_40: 0.0000 (0.0155)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 580/1724]  eta: 1:14:41  lr: 0.000200  loss: 1.3210 (1.4976)  loss_n_40: 0.3388 (0.3506)  loss_n_60: 0.3298 (0.3389)  loss_n_80: 0.3312 (0.3665)  loss_n_100: 0.3368 (0.3893)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0239)  triple_60: 0.0000 (0.0101)  triple_40: 0.0000 (0.0152)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 590/1724]  eta: 1:14:02  lr: 0.000200  loss: 1.3429 (1.4933)  loss_n_40: 0.3401 (0.3499)  loss_n_60: 0.3333 (0.3383)  loss_n_80: 0.3312 (0.3656)  loss_n_100: 0.3360 (0.3881)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0099)  triple_40: 0.0000 (0.0149)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 600/1724]  eta: 1:13:23  lr: 0.000200  loss: 1.1601 (1.4884)  loss_n_40: 0.2844 (0.3491)  loss_n_60: 0.2753 (0.3375)  loss_n_80: 0.2942 (0.3644)  loss_n_100: 0.3056 (0.3868)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0232)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0147)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 610/1724]  eta: 1:12:44  lr: 0.000200  loss: 1.1597 (1.4840)  loss_n_40: 0.2844 (0.3484)  loss_n_60: 0.2745 (0.3368)  loss_n_80: 0.2878 (0.3634)  loss_n_100: 0.2993 (0.3857)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0228)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0144)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 620/1724]  eta: 1:12:05  lr: 0.000200  loss: 1.2292 (1.4797)  loss_n_40: 0.2904 (0.3478)  loss_n_60: 0.2793 (0.3359)  loss_n_80: 0.3127 (0.3625)  loss_n_100: 0.3222 (0.3846)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0224)  triple_60: 0.0000 (0.0095)  triple_40: 0.0000 (0.0142)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 630/1724]  eta: 1:11:25  lr: 0.000200  loss: 1.1680 (1.4737)  loss_n_40: 0.2719 (0.3466)  loss_n_60: 0.2746 (0.3348)  loss_n_80: 0.2963 (0.3611)  loss_n_100: 0.3097 (0.3831)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0221)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0140)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 640/1724]  eta: 1:10:46  lr: 0.000200  loss: 1.0498 (1.4693)  loss_n_40: 0.2616 (0.3458)  loss_n_60: 0.2505 (0.3341)  loss_n_80: 0.2582 (0.3600)  loss_n_100: 0.2711 (0.3818)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0217)  triple_60: 0.0000 (0.0092)  triple_40: 0.0000 (0.0139)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 650/1724]  eta: 1:10:07  lr: 0.000200  loss: 1.1320 (1.4648)  loss_n_40: 0.2777 (0.3448)  loss_n_60: 0.2702 (0.3334)  loss_n_80: 0.2860 (0.3590)  loss_n_100: 0.2936 (0.3807)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0214)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0137)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 660/1724]  eta: 1:09:28  lr: 0.000200  loss: 1.1238 (1.4597)  loss_n_40: 0.2725 (0.3441)  loss_n_60: 0.2702 (0.3325)  loss_n_80: 0.2826 (0.3578)  loss_n_100: 0.2936 (0.3792)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0211)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0135)  time: 3.9181  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 670/1724]  eta: 1:08:49  lr: 0.000200  loss: 1.0773 (1.4554)  loss_n_40: 0.2610 (0.3431)  loss_n_60: 0.2676 (0.3318)  loss_n_80: 0.2737 (0.3568)  loss_n_100: 0.2878 (0.3782)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0207)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0133)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 680/1724]  eta: 1:08:10  lr: 0.000200  loss: 1.0907 (1.4506)  loss_n_40: 0.2655 (0.3421)  loss_n_60: 0.2676 (0.3309)  loss_n_80: 0.2813 (0.3557)  loss_n_100: 0.2999 (0.3771)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0204)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0131)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 690/1724]  eta: 1:07:30  lr: 0.000200  loss: 1.0840 (1.4454)  loss_n_40: 0.2643 (0.3411)  loss_n_60: 0.2586 (0.3300)  loss_n_80: 0.2783 (0.3545)  loss_n_100: 0.2860 (0.3757)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0201)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0129)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 700/1724]  eta: 1:06:51  lr: 0.000200  loss: 1.0508 (1.4397)  loss_n_40: 0.2497 (0.3402)  loss_n_60: 0.2511 (0.3289)  loss_n_80: 0.2491 (0.3530)  loss_n_100: 0.2837 (0.3741)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0198)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0127)  time: 3.9160  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 710/1724]  eta: 1:06:12  lr: 0.000200  loss: 0.9512 (1.4343)  loss_n_40: 0.2493 (0.3392)  loss_n_60: 0.2311 (0.3278)  loss_n_80: 0.2375 (0.3517)  loss_n_100: 0.2540 (0.3727)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0196)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0125)  time: 3.9167  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [ 720/1724]  eta: 1:05:33  lr: 0.000200  loss: 0.9574 (1.4344)  loss_n_40: 0.2266 (0.3383)  loss_n_60: 0.2261 (0.3269)  loss_n_80: 0.2389 (0.3508)  loss_n_100: 0.2540 (0.3719)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0203)  triple_60: 0.0000 (0.0097)  triple_40: 0.0000 (0.0135)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 730/1724]  eta: 1:04:54  lr: 0.000200  loss: 1.4725 (1.4380)  loss_n_40: 0.3225 (0.3386)  loss_n_60: 0.3287 (0.3277)  loss_n_80: 0.3854 (0.3520)  loss_n_100: 0.4229 (0.3735)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0200)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0133)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 740/1724]  eta: 1:04:14  lr: 0.000200  loss: 1.5818 (1.4402)  loss_n_40: 0.3729 (0.3391)  loss_n_60: 0.3686 (0.3283)  loss_n_80: 0.4161 (0.3527)  loss_n_100: 0.4607 (0.3745)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0197)  triple_60: 0.0000 (0.0095)  triple_40: 0.0000 (0.0131)  time: 3.9183  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 750/1724]  eta: 1:03:35  lr: 0.000200  loss: 1.6182 (1.4466)  loss_n_40: 0.3661 (0.3396)  loss_n_60: 0.3686 (0.3290)  loss_n_80: 0.4080 (0.3536)  loss_n_100: 0.4588 (0.3756)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0214)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0136)  time: 3.9173  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 760/1724]  eta: 1:02:56  lr: 0.000200  loss: 1.5591 (1.4472)  loss_n_40: 0.3397 (0.3395)  loss_n_60: 0.3619 (0.3293)  loss_n_80: 0.3888 (0.3540)  loss_n_100: 0.4243 (0.3761)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0211)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0134)  time: 3.9166  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 770/1724]  eta: 1:02:17  lr: 0.000200  loss: 1.4026 (1.4465)  loss_n_40: 0.3246 (0.3394)  loss_n_60: 0.3321 (0.3295)  loss_n_80: 0.3550 (0.3540)  loss_n_100: 0.3736 (0.3761)  triple_100: 0.0000 (0.0040)  triple_80: 0.0000 (0.0208)  triple_60: 0.0000 (0.0095)  triple_40: 0.0000 (0.0132)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 780/1724]  eta: 1:01:38  lr: 0.000200  loss: 1.3541 (1.4460)  loss_n_40: 0.3246 (0.3393)  loss_n_60: 0.3293 (0.3297)  loss_n_80: 0.3452 (0.3540)  loss_n_100: 0.3736 (0.3761)  triple_100: 0.0000 (0.0040)  triple_80: 0.0000 (0.0206)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0131)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 790/1724]  eta: 1:00:59  lr: 0.000200  loss: 1.3328 (1.4444)  loss_n_40: 0.3253 (0.3392)  loss_n_60: 0.3219 (0.3295)  loss_n_80: 0.3331 (0.3537)  loss_n_100: 0.3409 (0.3757)  triple_100: 0.0000 (0.0039)  triple_80: 0.0000 (0.0203)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0129)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 800/1724]  eta: 1:00:19  lr: 0.000200  loss: 1.2419 (1.4422)  loss_n_40: 0.3087 (0.3388)  loss_n_60: 0.3041 (0.3292)  loss_n_80: 0.3109 (0.3532)  loss_n_100: 0.3304 (0.3751)  triple_100: 0.0000 (0.0039)  triple_80: 0.0000 (0.0201)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0127)  time: 3.9184  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 810/1724]  eta: 0:59:40  lr: 0.000200  loss: 1.1730 (1.4385)  loss_n_40: 0.2844 (0.3382)  loss_n_60: 0.2810 (0.3286)  loss_n_80: 0.2854 (0.3523)  loss_n_100: 0.3122 (0.3742)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0198)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0126)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 820/1724]  eta: 0:59:01  lr: 0.000200  loss: 1.1823 (1.4370)  loss_n_40: 0.2981 (0.3382)  loss_n_60: 0.2800 (0.3284)  loss_n_80: 0.2903 (0.3520)  loss_n_100: 0.3062 (0.3737)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0196)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0124)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 830/1724]  eta: 0:58:22  lr: 0.000200  loss: 1.2244 (1.4352)  loss_n_40: 0.3078 (0.3381)  loss_n_60: 0.2916 (0.3282)  loss_n_80: 0.2974 (0.3516)  loss_n_100: 0.3080 (0.3733)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0193)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0123)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 840/1724]  eta: 0:57:43  lr: 0.000200  loss: 1.1349 (1.4320)  loss_n_40: 0.2932 (0.3374)  loss_n_60: 0.2755 (0.3276)  loss_n_80: 0.2834 (0.3509)  loss_n_100: 0.3018 (0.3724)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0191)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0121)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 850/1724]  eta: 0:57:04  lr: 0.000200  loss: 1.1349 (1.4295)  loss_n_40: 0.2703 (0.3370)  loss_n_60: 0.2743 (0.3272)  loss_n_80: 0.2891 (0.3504)  loss_n_100: 0.3046 (0.3718)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0189)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0120)  time: 3.9195  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 860/1724]  eta: 0:56:24  lr: 0.000200  loss: 1.2110 (1.4284)  loss_n_40: 0.2977 (0.3369)  loss_n_60: 0.2954 (0.3272)  loss_n_80: 0.2990 (0.3502)  loss_n_100: 0.3115 (0.3715)  triple_100: 0.0000 (0.0036)  triple_80: 0.0000 (0.0187)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0119)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 870/1724]  eta: 0:55:45  lr: 0.000200  loss: 1.1739 (1.4257)  loss_n_40: 0.2942 (0.3365)  loss_n_60: 0.2861 (0.3268)  loss_n_80: 0.2888 (0.3495)  loss_n_100: 0.3014 (0.3707)  triple_100: 0.0000 (0.0036)  triple_80: 0.0000 (0.0184)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0117)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 880/1724]  eta: 0:55:06  lr: 0.000200  loss: 1.0839 (1.4221)  loss_n_40: 0.2599 (0.3357)  loss_n_60: 0.2621 (0.3261)  loss_n_80: 0.2729 (0.3487)  loss_n_100: 0.2866 (0.3699)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0184)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0116)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 890/1724]  eta: 0:54:27  lr: 0.000200  loss: 1.0520 (1.4192)  loss_n_40: 0.2527 (0.3352)  loss_n_60: 0.2538 (0.3255)  loss_n_80: 0.2607 (0.3480)  loss_n_100: 0.2771 (0.3691)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0182)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0115)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 900/1724]  eta: 0:53:48  lr: 0.000200  loss: 1.0520 (1.4166)  loss_n_40: 0.2598 (0.3347)  loss_n_60: 0.2519 (0.3250)  loss_n_80: 0.2680 (0.3475)  loss_n_100: 0.2868 (0.3685)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0180)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0113)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 910/1724]  eta: 0:53:09  lr: 0.000200  loss: 1.1071 (1.4137)  loss_n_40: 0.2598 (0.3341)  loss_n_60: 0.2598 (0.3245)  loss_n_80: 0.2777 (0.3468)  loss_n_100: 0.2874 (0.3678)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0178)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0112)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 920/1724]  eta: 0:52:29  lr: 0.000200  loss: 1.1129 (1.4103)  loss_n_40: 0.2731 (0.3335)  loss_n_60: 0.2707 (0.3239)  loss_n_80: 0.2777 (0.3460)  loss_n_100: 0.2874 (0.3669)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0176)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0111)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 930/1724]  eta: 0:51:50  lr: 0.000200  loss: 1.0477 (1.4061)  loss_n_40: 0.2583 (0.3327)  loss_n_60: 0.2466 (0.3230)  loss_n_80: 0.2625 (0.3450)  loss_n_100: 0.2716 (0.3658)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0174)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0110)  time: 3.9189  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 940/1724]  eta: 0:51:11  lr: 0.000200  loss: 1.0411 (1.4022)  loss_n_40: 0.2446 (0.3319)  loss_n_60: 0.2466 (0.3222)  loss_n_80: 0.2625 (0.3441)  loss_n_100: 0.2643 (0.3648)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0172)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0108)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [ 950/1724]  eta: 0:50:32  lr: 0.000200  loss: 1.0393 (1.3984)  loss_n_40: 0.2444 (0.3311)  loss_n_60: 0.2478 (0.3214)  loss_n_80: 0.2616 (0.3433)  loss_n_100: 0.2734 (0.3638)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0170)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0107)  time: 3.9194  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [ 960/1724]  eta: 0:49:53  lr: 0.000200  loss: 0.9968 (1.3942)  loss_n_40: 0.2457 (0.3303)  loss_n_60: 0.2341 (0.3206)  loss_n_80: 0.2502 (0.3423)  loss_n_100: 0.2638 (0.3628)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0168)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0106)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 970/1724]  eta: 0:49:14  lr: 0.000200  loss: 1.0173 (1.3908)  loss_n_40: 0.2551 (0.3296)  loss_n_60: 0.2487 (0.3198)  loss_n_80: 0.2512 (0.3415)  loss_n_100: 0.2677 (0.3619)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0167)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0105)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 980/1724]  eta: 0:48:34  lr: 0.000200  loss: 1.0541 (1.3878)  loss_n_40: 0.2624 (0.3289)  loss_n_60: 0.2502 (0.3192)  loss_n_80: 0.2647 (0.3408)  loss_n_100: 0.2848 (0.3612)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0105)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [ 990/1724]  eta: 0:47:55  lr: 0.000200  loss: 1.0441 (1.3845)  loss_n_40: 0.2400 (0.3282)  loss_n_60: 0.2443 (0.3185)  loss_n_80: 0.2673 (0.3400)  loss_n_100: 0.2852 (0.3605)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0163)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0104)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1000/1724]  eta: 0:47:16  lr: 0.000200  loss: 1.2454 (1.3917)  loss_n_40: 0.2963 (0.3285)  loss_n_60: 0.2879 (0.3195)  loss_n_80: 0.2957 (0.3415)  loss_n_100: 0.3269 (0.3623)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0174)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0111)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1010/1724]  eta: 0:46:37  lr: 0.000200  loss: 2.1836 (1.4035)  loss_n_40: 0.3945 (0.3294)  loss_n_60: 0.4445 (0.3214)  loss_n_80: 0.5163 (0.3439)  loss_n_100: 0.5987 (0.3653)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0172)  triple_60: 0.0000 (0.0092)  triple_40: 0.0000 (0.0115)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1020/1724]  eta: 0:45:58  lr: 0.000200  loss: 2.1443 (1.4102)  loss_n_40: 0.4209 (0.3303)  loss_n_60: 0.4993 (0.3229)  loss_n_80: 0.5634 (0.3459)  loss_n_100: 0.6412 (0.3678)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0170)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0114)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1030/1724]  eta: 0:45:19  lr: 0.000200  loss: 1.9307 (1.4148)  loss_n_40: 0.4227 (0.3311)  loss_n_60: 0.4356 (0.3239)  loss_n_80: 0.4934 (0.3472)  loss_n_100: 0.5842 (0.3696)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0169)  triple_60: 0.0000 (0.0092)  triple_40: 0.0000 (0.0113)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1040/1724]  eta: 0:44:39  lr: 0.000200  loss: 1.7831 (1.4177)  loss_n_40: 0.4144 (0.3319)  loss_n_60: 0.3999 (0.3245)  loss_n_80: 0.4576 (0.3481)  loss_n_100: 0.4969 (0.3707)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0167)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0112)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1050/1724]  eta: 0:44:00  lr: 0.000200  loss: 1.5634 (1.4185)  loss_n_40: 0.3977 (0.3323)  loss_n_60: 0.3650 (0.3248)  loss_n_80: 0.3968 (0.3484)  loss_n_100: 0.4200 (0.3709)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0111)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1060/1724]  eta: 0:43:21  lr: 0.000200  loss: 1.3427 (1.4178)  loss_n_40: 0.3204 (0.3325)  loss_n_60: 0.3131 (0.3247)  loss_n_80: 0.3419 (0.3483)  loss_n_100: 0.3432 (0.3707)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0164)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0110)  time: 3.9176  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1070/1724]  eta: 0:42:42  lr: 0.000200  loss: 1.3625 (1.4189)  loss_n_40: 0.3250 (0.3328)  loss_n_60: 0.3003 (0.3249)  loss_n_80: 0.3465 (0.3486)  loss_n_100: 0.3421 (0.3708)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0163)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0110)  time: 3.9182  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1080/1724]  eta: 0:42:03  lr: 0.000200  loss: 1.4276 (1.4194)  loss_n_40: 0.3357 (0.3330)  loss_n_60: 0.3360 (0.3252)  loss_n_80: 0.3659 (0.3487)  loss_n_100: 0.3887 (0.3710)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0162)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0109)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1090/1724]  eta: 0:41:24  lr: 0.000200  loss: 1.3949 (1.4191)  loss_n_40: 0.3357 (0.3330)  loss_n_60: 0.3434 (0.3254)  loss_n_80: 0.3518 (0.3487)  loss_n_100: 0.3694 (0.3710)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0160)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0108)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1100/1724]  eta: 0:40:44  lr: 0.000200  loss: 1.3789 (1.4187)  loss_n_40: 0.3271 (0.3330)  loss_n_60: 0.3434 (0.3255)  loss_n_80: 0.3505 (0.3487)  loss_n_100: 0.3524 (0.3708)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0159)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0107)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1110/1724]  eta: 0:40:05  lr: 0.000200  loss: 1.3201 (1.4183)  loss_n_40: 0.3234 (0.3330)  loss_n_60: 0.3407 (0.3256)  loss_n_80: 0.3377 (0.3487)  loss_n_100: 0.3351 (0.3706)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0157)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0106)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1120/1724]  eta: 0:39:26  lr: 0.000200  loss: 1.2600 (1.4166)  loss_n_40: 0.2917 (0.3328)  loss_n_60: 0.3100 (0.3254)  loss_n_80: 0.3180 (0.3484)  loss_n_100: 0.3159 (0.3701)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0105)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1130/1724]  eta: 0:38:47  lr: 0.000200  loss: 1.1273 (1.4146)  loss_n_40: 0.2798 (0.3324)  loss_n_60: 0.2776 (0.3251)  loss_n_80: 0.2827 (0.3480)  loss_n_100: 0.2807 (0.3695)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0104)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1140/1724]  eta: 0:38:08  lr: 0.000200  loss: 1.0913 (1.4116)  loss_n_40: 0.2624 (0.3317)  loss_n_60: 0.2664 (0.3246)  loss_n_80: 0.2743 (0.3473)  loss_n_100: 0.2807 (0.3687)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0103)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1150/1724]  eta: 0:37:28  lr: 0.000200  loss: 1.0913 (1.4091)  loss_n_40: 0.2624 (0.3313)  loss_n_60: 0.2615 (0.3242)  loss_n_80: 0.2730 (0.3468)  loss_n_100: 0.2826 (0.3679)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0152)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0102)  time: 3.9193  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1160/1724]  eta: 0:36:49  lr: 0.000200  loss: 1.1140 (1.4066)  loss_n_40: 0.2848 (0.3309)  loss_n_60: 0.2815 (0.3237)  loss_n_80: 0.2750 (0.3462)  loss_n_100: 0.2714 (0.3672)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0101)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1170/1724]  eta: 0:36:10  lr: 0.000200  loss: 1.1017 (1.4049)  loss_n_40: 0.2600 (0.3304)  loss_n_60: 0.2664 (0.3234)  loss_n_80: 0.2784 (0.3457)  loss_n_100: 0.2796 (0.3666)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0103)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1180/1724]  eta: 0:35:31  lr: 0.000200  loss: 1.2335 (1.4043)  loss_n_40: 0.2667 (0.3301)  loss_n_60: 0.2799 (0.3233)  loss_n_80: 0.3133 (0.3457)  loss_n_100: 0.3285 (0.3666)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0102)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1190/1724]  eta: 0:34:52  lr: 0.000200  loss: 1.4507 (1.4067)  loss_n_40: 0.3033 (0.3301)  loss_n_60: 0.3550 (0.3238)  loss_n_80: 0.3831 (0.3465)  loss_n_100: 0.3982 (0.3676)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0101)  time: 3.9176  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [1200/1724]  eta: 0:34:13  lr: 0.000200  loss: 1.4586 (1.4075)  loss_n_40: 0.3464 (0.3304)  loss_n_60: 0.3541 (0.3241)  loss_n_80: 0.3841 (0.3468)  loss_n_100: 0.4134 (0.3679)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0101)  time: 3.9170  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1210/1724]  eta: 0:33:33  lr: 0.000200  loss: 1.3133 (1.4059)  loss_n_40: 0.2910 (0.3300)  loss_n_60: 0.3124 (0.3238)  loss_n_80: 0.3307 (0.3465)  loss_n_100: 0.3478 (0.3676)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0100)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1220/1724]  eta: 0:32:54  lr: 0.000200  loss: 1.1943 (1.4047)  loss_n_40: 0.2763 (0.3299)  loss_n_60: 0.2816 (0.3237)  loss_n_80: 0.3120 (0.3462)  loss_n_100: 0.3286 (0.3672)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0146)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0099)  time: 3.9168  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1230/1724]  eta: 0:32:15  lr: 0.000200  loss: 1.3306 (1.4049)  loss_n_40: 0.2962 (0.3299)  loss_n_60: 0.3119 (0.3237)  loss_n_80: 0.3258 (0.3463)  loss_n_100: 0.3379 (0.3673)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0098)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1240/1724]  eta: 0:31:36  lr: 0.000200  loss: 1.4376 (1.4063)  loss_n_40: 0.3237 (0.3301)  loss_n_60: 0.3314 (0.3240)  loss_n_80: 0.3632 (0.3467)  loss_n_100: 0.4030 (0.3680)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0097)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1250/1724]  eta: 0:30:57  lr: 0.000200  loss: 1.3620 (1.4058)  loss_n_40: 0.2949 (0.3300)  loss_n_60: 0.3189 (0.3239)  loss_n_80: 0.3505 (0.3467)  loss_n_100: 0.3927 (0.3680)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0097)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1260/1724]  eta: 0:30:17  lr: 0.000200  loss: 1.2764 (1.4046)  loss_n_40: 0.3011 (0.3299)  loss_n_60: 0.3007 (0.3238)  loss_n_80: 0.3213 (0.3464)  loss_n_100: 0.3496 (0.3677)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0096)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1270/1724]  eta: 0:29:38  lr: 0.000200  loss: 1.2083 (1.4026)  loss_n_40: 0.2891 (0.3295)  loss_n_60: 0.2899 (0.3234)  loss_n_80: 0.2965 (0.3459)  loss_n_100: 0.3080 (0.3671)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0095)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1280/1724]  eta: 0:28:59  lr: 0.000200  loss: 1.1654 (1.4011)  loss_n_40: 0.2874 (0.3293)  loss_n_60: 0.2796 (0.3232)  loss_n_80: 0.2876 (0.3456)  loss_n_100: 0.3071 (0.3667)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0094)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1290/1724]  eta: 0:28:20  lr: 0.000200  loss: 1.2187 (1.4004)  loss_n_40: 0.3041 (0.3294)  loss_n_60: 0.3003 (0.3232)  loss_n_80: 0.3027 (0.3454)  loss_n_100: 0.3074 (0.3664)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0094)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1300/1724]  eta: 0:27:41  lr: 0.000200  loss: 1.0447 (1.3976)  loss_n_40: 0.2485 (0.3288)  loss_n_60: 0.2516 (0.3226)  loss_n_80: 0.2656 (0.3448)  loss_n_100: 0.2763 (0.3657)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0093)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1310/1724]  eta: 0:27:02  lr: 0.000200  loss: 1.0447 (1.3963)  loss_n_40: 0.2438 (0.3283)  loss_n_60: 0.2516 (0.3223)  loss_n_80: 0.2656 (0.3444)  loss_n_100: 0.2745 (0.3652)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0093)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1320/1724]  eta: 0:26:22  lr: 0.000200  loss: 1.2587 (1.3962)  loss_n_40: 0.2715 (0.3283)  loss_n_60: 0.2813 (0.3223)  loss_n_80: 0.3116 (0.3444)  loss_n_100: 0.3463 (0.3654)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0093)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1330/1724]  eta: 0:25:43  lr: 0.000200  loss: 1.3588 (1.3962)  loss_n_40: 0.3139 (0.3283)  loss_n_60: 0.3132 (0.3223)  loss_n_80: 0.3529 (0.3445)  loss_n_100: 0.3735 (0.3655)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0092)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1340/1724]  eta: 0:25:04  lr: 0.000200  loss: 1.2930 (1.3955)  loss_n_40: 0.3072 (0.3283)  loss_n_60: 0.3051 (0.3222)  loss_n_80: 0.3356 (0.3444)  loss_n_100: 0.3430 (0.3653)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0091)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1350/1724]  eta: 0:24:25  lr: 0.000200  loss: 1.2119 (1.3938)  loss_n_40: 0.2989 (0.3280)  loss_n_60: 0.2871 (0.3218)  loss_n_80: 0.3043 (0.3440)  loss_n_100: 0.3197 (0.3649)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0134)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0091)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1360/1724]  eta: 0:23:46  lr: 0.000200  loss: 1.1929 (1.3922)  loss_n_40: 0.2885 (0.3277)  loss_n_60: 0.2871 (0.3216)  loss_n_80: 0.2888 (0.3436)  loss_n_100: 0.3018 (0.3644)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0090)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1370/1724]  eta: 0:23:06  lr: 0.000200  loss: 1.1501 (1.3900)  loss_n_40: 0.2800 (0.3273)  loss_n_60: 0.2703 (0.3212)  loss_n_80: 0.2848 (0.3432)  loss_n_100: 0.3018 (0.3638)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0090)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1380/1724]  eta: 0:22:27  lr: 0.000200  loss: 1.0807 (1.3874)  loss_n_40: 0.2584 (0.3268)  loss_n_60: 0.2600 (0.3206)  loss_n_80: 0.2764 (0.3425)  loss_n_100: 0.2859 (0.3631)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0089)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1390/1724]  eta: 0:21:48  lr: 0.000200  loss: 0.9836 (1.3854)  loss_n_40: 0.2301 (0.3262)  loss_n_60: 0.2397 (0.3202)  loss_n_80: 0.2474 (0.3420)  loss_n_100: 0.2607 (0.3625)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0092)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1400/1724]  eta: 0:21:09  lr: 0.000200  loss: 1.1016 (1.3840)  loss_n_40: 0.2450 (0.3260)  loss_n_60: 0.2669 (0.3199)  loss_n_80: 0.2795 (0.3416)  loss_n_100: 0.2783 (0.3621)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0092)  time: 3.9159  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1410/1724]  eta: 0:20:30  lr: 0.000200  loss: 1.1640 (1.3826)  loss_n_40: 0.2868 (0.3258)  loss_n_60: 0.2765 (0.3197)  loss_n_80: 0.2921 (0.3413)  loss_n_100: 0.3074 (0.3618)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0091)  time: 3.9157  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1420/1724]  eta: 0:19:51  lr: 0.000200  loss: 1.1635 (1.3809)  loss_n_40: 0.2698 (0.3255)  loss_n_60: 0.2751 (0.3193)  loss_n_80: 0.2818 (0.3410)  loss_n_100: 0.2946 (0.3613)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0127)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0091)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1430/1724]  eta: 0:19:11  lr: 0.000200  loss: 1.0122 (1.3782)  loss_n_40: 0.2467 (0.3250)  loss_n_60: 0.2394 (0.3188)  loss_n_80: 0.2515 (0.3403)  loss_n_100: 0.2595 (0.3606)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0126)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0090)  time: 3.9169  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [1440/1724]  eta: 0:18:32  lr: 0.000200  loss: 1.0122 (1.3772)  loss_n_40: 0.2584 (0.3247)  loss_n_60: 0.2408 (0.3185)  loss_n_80: 0.2535 (0.3400)  loss_n_100: 0.2627 (0.3602)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0127)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0090)  time: 3.9169  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1450/1724]  eta: 0:17:53  lr: 0.000200  loss: 1.2044 (1.3766)  loss_n_40: 0.2706 (0.3245)  loss_n_60: 0.2840 (0.3185)  loss_n_80: 0.3077 (0.3399)  loss_n_100: 0.3279 (0.3602)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0126)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0089)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1460/1724]  eta: 0:17:14  lr: 0.000200  loss: 1.2742 (1.3763)  loss_n_40: 0.2868 (0.3245)  loss_n_60: 0.3102 (0.3185)  loss_n_80: 0.3267 (0.3399)  loss_n_100: 0.3521 (0.3602)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0125)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0088)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1470/1724]  eta: 0:16:35  lr: 0.000200  loss: 1.2510 (1.3751)  loss_n_40: 0.3039 (0.3242)  loss_n_60: 0.3056 (0.3183)  loss_n_80: 0.3146 (0.3396)  loss_n_100: 0.3431 (0.3599)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0124)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0088)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1480/1724]  eta: 0:15:55  lr: 0.000200  loss: 1.1411 (1.3735)  loss_n_40: 0.2764 (0.3239)  loss_n_60: 0.2770 (0.3180)  loss_n_80: 0.2881 (0.3392)  loss_n_100: 0.3049 (0.3595)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0123)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0087)  time: 3.9181  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1490/1724]  eta: 0:15:16  lr: 0.000200  loss: 1.0299 (1.3709)  loss_n_40: 0.2420 (0.3234)  loss_n_60: 0.2480 (0.3175)  loss_n_80: 0.2602 (0.3386)  loss_n_100: 0.2760 (0.3588)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0122)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0087)  time: 3.9190  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1500/1724]  eta: 0:14:37  lr: 0.000200  loss: 1.0848 (1.3858)  loss_n_40: 0.2612 (0.3234)  loss_n_60: 0.2573 (0.3177)  loss_n_80: 0.2688 (0.3392)  loss_n_100: 0.2791 (0.3596)  triple_100: 0.0000 (0.0093)  triple_80: 0.0000 (0.0157)  triple_60: 0.0000 (0.0103)  triple_40: 0.0000 (0.0106)  time: 3.9193  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1510/1724]  eta: 0:13:58  lr: 0.000200  loss: 3.1210 (1.4033)  loss_n_40: 0.4006 (0.3258)  loss_n_60: 0.5308 (0.3214)  loss_n_80: 0.6826 (0.3440)  loss_n_100: 0.8778 (0.3656)  triple_100: 0.0000 (0.0100)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0105)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1520/1724]  eta: 0:13:19  lr: 0.000200  loss: 4.2301 (1.4216)  loss_n_40: 0.7336 (0.3288)  loss_n_60: 0.8898 (0.3252)  loss_n_80: 1.1584 (0.3493)  loss_n_100: 1.3621 (0.3721)  triple_100: 0.0000 (0.0099)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0103)  triple_40: 0.0000 (0.0104)  time: 3.9184  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1530/1724]  eta: 0:12:40  lr: 0.000200  loss: 3.5983 (1.4329)  loss_n_40: 0.7191 (0.3308)  loss_n_60: 0.8228 (0.3279)  loss_n_80: 0.9477 (0.3524)  loss_n_100: 1.0893 (0.3759)  triple_100: 0.0000 (0.0099)  triple_80: 0.0000 (0.0154)  triple_60: 0.0000 (0.0102)  triple_40: 0.0000 (0.0104)  time: 3.9179  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1540/1724]  eta: 0:12:00  lr: 0.000200  loss: 2.7056 (1.4401)  loss_n_40: 0.5648 (0.3321)  loss_n_60: 0.6340 (0.3296)  loss_n_80: 0.7057 (0.3544)  loss_n_100: 0.8208 (0.3784)  triple_100: 0.0000 (0.0098)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0102)  triple_40: 0.0000 (0.0103)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1550/1724]  eta: 0:11:21  lr: 0.000200  loss: 2.3239 (1.4451)  loss_n_40: 0.4698 (0.3329)  loss_n_60: 0.5404 (0.3309)  loss_n_80: 0.6060 (0.3559)  loss_n_100: 0.6837 (0.3800)  triple_100: 0.0000 (0.0097)  triple_80: 0.0000 (0.0152)  triple_60: 0.0000 (0.0101)  triple_40: 0.0000 (0.0102)  time: 3.9184  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1560/1724]  eta: 0:10:42  lr: 0.000200  loss: 2.0283 (1.4483)  loss_n_40: 0.4519 (0.3336)  loss_n_60: 0.4768 (0.3318)  loss_n_80: 0.5286 (0.3569)  loss_n_100: 0.5714 (0.3811)  triple_100: 0.0000 (0.0097)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0100)  triple_40: 0.0000 (0.0102)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1570/1724]  eta: 0:10:03  lr: 0.000200  loss: 1.8680 (1.4504)  loss_n_40: 0.4103 (0.3340)  loss_n_60: 0.4527 (0.3324)  loss_n_80: 0.4832 (0.3575)  loss_n_100: 0.5087 (0.3819)  triple_100: 0.0000 (0.0096)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0100)  triple_40: 0.0000 (0.0101)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1580/1724]  eta: 0:09:24  lr: 0.000200  loss: 1.7161 (1.4518)  loss_n_40: 0.3701 (0.3343)  loss_n_60: 0.4088 (0.3328)  loss_n_80: 0.4447 (0.3579)  loss_n_100: 0.4765 (0.3824)  triple_100: 0.0000 (0.0096)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0099)  triple_40: 0.0000 (0.0100)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1590/1724]  eta: 0:08:45  lr: 0.000200  loss: 1.6692 (1.4529)  loss_n_40: 0.3769 (0.3346)  loss_n_60: 0.3898 (0.3331)  loss_n_80: 0.4062 (0.3582)  loss_n_100: 0.4516 (0.3828)  triple_100: 0.0000 (0.0095)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0100)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1600/1724]  eta: 0:08:05  lr: 0.000200  loss: 1.5728 (1.4537)  loss_n_40: 0.3769 (0.3348)  loss_n_60: 0.3727 (0.3334)  loss_n_80: 0.3866 (0.3585)  loss_n_100: 0.4357 (0.3832)  triple_100: 0.0000 (0.0094)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0099)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1610/1724]  eta: 0:07:26  lr: 0.000200  loss: 1.4751 (1.4544)  loss_n_40: 0.3499 (0.3350)  loss_n_60: 0.3454 (0.3336)  loss_n_80: 0.3802 (0.3587)  loss_n_100: 0.4203 (0.3835)  triple_100: 0.0000 (0.0094)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0098)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1620/1724]  eta: 0:06:47  lr: 0.000200  loss: 1.4713 (1.4546)  loss_n_40: 0.3434 (0.3351)  loss_n_60: 0.3454 (0.3336)  loss_n_80: 0.3727 (0.3587)  loss_n_100: 0.4047 (0.3836)  triple_100: 0.0000 (0.0093)  triple_80: 0.0000 (0.0146)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0098)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1630/1724]  eta: 0:06:08  lr: 0.000200  loss: 1.4560 (1.4542)  loss_n_40: 0.3434 (0.3351)  loss_n_60: 0.3490 (0.3336)  loss_n_80: 0.3548 (0.3587)  loss_n_100: 0.4010 (0.3836)  triple_100: 0.0000 (0.0093)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0097)  triple_40: 0.0000 (0.0097)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1640/1724]  eta: 0:05:29  lr: 0.000200  loss: 1.3429 (1.4536)  loss_n_40: 0.3161 (0.3350)  loss_n_60: 0.3161 (0.3335)  loss_n_80: 0.3358 (0.3586)  loss_n_100: 0.3666 (0.3836)  triple_100: 0.0000 (0.0092)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0097)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1650/1724]  eta: 0:04:49  lr: 0.000200  loss: 1.3502 (1.4532)  loss_n_40: 0.3186 (0.3349)  loss_n_60: 0.3161 (0.3335)  loss_n_80: 0.3425 (0.3586)  loss_n_100: 0.3720 (0.3836)  triple_100: 0.0000 (0.0091)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0096)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1660/1724]  eta: 0:04:10  lr: 0.000200  loss: 1.3502 (1.4530)  loss_n_40: 0.3336 (0.3350)  loss_n_60: 0.3104 (0.3334)  loss_n_80: 0.3415 (0.3585)  loss_n_100: 0.3594 (0.3835)  triple_100: 0.0000 (0.0091)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0095)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1670/1724]  eta: 0:03:31  lr: 0.000200  loss: 1.3297 (1.4515)  loss_n_40: 0.3065 (0.3348)  loss_n_60: 0.3057 (0.3331)  loss_n_80: 0.3166 (0.3582)  loss_n_100: 0.3469 (0.3832)  triple_100: 0.0000 (0.0090)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0095)  time: 3.9190  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:17]  [1680/1724]  eta: 0:02:52  lr: 0.000200  loss: 1.1875 (1.4499)  loss_n_40: 0.2757 (0.3345)  loss_n_60: 0.2802 (0.3328)  loss_n_80: 0.3013 (0.3578)  loss_n_100: 0.3339 (0.3828)  triple_100: 0.0000 (0.0090)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0095)  triple_40: 0.0000 (0.0094)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1690/1724]  eta: 0:02:13  lr: 0.000200  loss: 1.1426 (1.4483)  loss_n_40: 0.2669 (0.3342)  loss_n_60: 0.2629 (0.3325)  loss_n_80: 0.2930 (0.3574)  loss_n_100: 0.3159 (0.3824)  triple_100: 0.0000 (0.0089)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0095)  triple_40: 0.0000 (0.0094)  time: 3.9185  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1700/1724]  eta: 0:01:34  lr: 0.000200  loss: 1.1426 (1.4471)  loss_n_40: 0.3001 (0.3341)  loss_n_60: 0.2629 (0.3322)  loss_n_80: 0.2854 (0.3571)  loss_n_100: 0.3139 (0.3821)  triple_100: 0.0000 (0.0089)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0093)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:17]  [1710/1724]  eta: 0:00:54  lr: 0.000200  loss: 1.1781 (1.4454)  loss_n_40: 0.2971 (0.3338)  loss_n_60: 0.2823 (0.3319)  loss_n_80: 0.2910 (0.3567)  loss_n_100: 0.3192 (0.3817)  triple_100: 0.0000 (0.0088)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0093)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1720/1724]  eta: 0:00:15  lr: 0.000200  loss: 1.1560 (1.4445)  loss_n_40: 0.2894 (0.3337)  loss_n_60: 0.2756 (0.3317)  loss_n_80: 0.2885 (0.3565)  loss_n_100: 0.3062 (0.3814)  triple_100: 0.0000 (0.0088)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0094)  time: 3.9191  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17]  [1723/1724]  eta: 0:00:03  lr: 0.000200  loss: 1.1529 (1.4441)  loss_n_40: 0.2831 (0.3336)  loss_n_60: 0.2641 (0.3316)  loss_n_80: 0.2881 (0.3564)  loss_n_100: 0.3061 (0.3813)  triple_100: 0.0000 (0.0088)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0093)  time: 3.9189  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:17] Total time: 1:52:35 (3.9183 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.1529 (1.4441)  loss_n_40: 0.2831 (0.3336)  loss_n_60: 0.2641 (0.3316)  loss_n_80: 0.2881 (0.3564)  loss_n_100: 0.3061 (0.3813)  triple_100: 0.0000 (0.0088)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0093)\n",
      "Valid: [epoch:17]  [  0/845]  eta: 0:10:03  loss: 0.9919 (0.9919)  loss_n_40: 0.2266 (0.2266)  loss_n_60: 0.2317 (0.2317)  loss_n_80: 0.2627 (0.2627)  loss_n_100: 0.2709 (0.2709)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7139  data: 0.3811  max mem: 46473\n",
      "Valid: [epoch:17]  [ 10/845]  eta: 0:05:07  loss: 0.9823 (1.1096)  loss_n_40: 0.2266 (0.2557)  loss_n_60: 0.2302 (0.2550)  loss_n_80: 0.2525 (0.2804)  loss_n_100: 0.2709 (0.3185)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3686  data: 0.0348  max mem: 46473\n",
      "Valid: [epoch:17]  [ 20/845]  eta: 0:04:50  loss: 1.0564 (1.1870)  loss_n_40: 0.2453 (0.2848)  loss_n_60: 0.2366 (0.2753)  loss_n_80: 0.2755 (0.2959)  loss_n_100: 0.2961 (0.3310)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [ 30/845]  eta: 0:04:42  loss: 1.0564 (1.1431)  loss_n_40: 0.2454 (0.2791)  loss_n_60: 0.2455 (0.2641)  loss_n_80: 0.2755 (0.2840)  loss_n_100: 0.2941 (0.3159)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [ 40/845]  eta: 0:04:36  loss: 1.0347 (1.1349)  loss_n_40: 0.2415 (0.2732)  loss_n_60: 0.2381 (0.2614)  loss_n_80: 0.2559 (0.2836)  loss_n_100: 0.2750 (0.3168)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [ 50/845]  eta: 0:04:31  loss: 1.0347 (1.1374)  loss_n_40: 0.2415 (0.2705)  loss_n_60: 0.2381 (0.2631)  loss_n_80: 0.2559 (0.2857)  loss_n_100: 0.2827 (0.3182)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [ 60/845]  eta: 0:04:27  loss: 0.9224 (1.1380)  loss_n_40: 0.2379 (0.2707)  loss_n_60: 0.2245 (0.2645)  loss_n_80: 0.2354 (0.2864)  loss_n_100: 0.2650 (0.3165)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [ 70/845]  eta: 0:04:23  loss: 1.1008 (1.1464)  loss_n_40: 0.2596 (0.2734)  loss_n_60: 0.2561 (0.2656)  loss_n_80: 0.2884 (0.2886)  loss_n_100: 0.3090 (0.3189)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [ 80/845]  eta: 0:04:19  loss: 1.1673 (1.1484)  loss_n_40: 0.2643 (0.2725)  loss_n_60: 0.2572 (0.2661)  loss_n_80: 0.2980 (0.2895)  loss_n_100: 0.3241 (0.3204)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [ 90/845]  eta: 0:04:15  loss: 1.1673 (1.1503)  loss_n_40: 0.2481 (0.2711)  loss_n_60: 0.2650 (0.2664)  loss_n_80: 0.3072 (0.2904)  loss_n_100: 0.3470 (0.3224)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [100/845]  eta: 0:04:11  loss: 1.2600 (1.1691)  loss_n_40: 0.2748 (0.2779)  loss_n_60: 0.2777 (0.2714)  loss_n_80: 0.3064 (0.2932)  loss_n_100: 0.3583 (0.3266)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [110/845]  eta: 0:04:08  loss: 1.2067 (1.1748)  loss_n_40: 0.2792 (0.2800)  loss_n_60: 0.2730 (0.2726)  loss_n_80: 0.3094 (0.2949)  loss_n_100: 0.3426 (0.3273)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [120/845]  eta: 0:04:04  loss: 1.0032 (1.1645)  loss_n_40: 0.2432 (0.2764)  loss_n_60: 0.2376 (0.2704)  loss_n_80: 0.2640 (0.2929)  loss_n_100: 0.2861 (0.3248)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [130/845]  eta: 0:04:01  loss: 0.9958 (1.1698)  loss_n_40: 0.2475 (0.2791)  loss_n_60: 0.2344 (0.2721)  loss_n_80: 0.2597 (0.2937)  loss_n_100: 0.2861 (0.3249)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [140/845]  eta: 0:03:57  loss: 1.2270 (1.1725)  loss_n_40: 0.2700 (0.2806)  loss_n_60: 0.2795 (0.2724)  loss_n_80: 0.2992 (0.2939)  loss_n_100: 0.3354 (0.3257)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [150/845]  eta: 0:03:54  loss: 1.2308 (1.1788)  loss_n_40: 0.2872 (0.2820)  loss_n_60: 0.2824 (0.2738)  loss_n_80: 0.2976 (0.2954)  loss_n_100: 0.3354 (0.3276)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [160/845]  eta: 0:03:50  loss: 1.1509 (1.1766)  loss_n_40: 0.2753 (0.2815)  loss_n_60: 0.2755 (0.2735)  loss_n_80: 0.2974 (0.2950)  loss_n_100: 0.3102 (0.3266)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [170/845]  eta: 0:03:47  loss: 1.0915 (1.1791)  loss_n_40: 0.2548 (0.2827)  loss_n_60: 0.2494 (0.2739)  loss_n_80: 0.2812 (0.2950)  loss_n_100: 0.3085 (0.3274)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:17]  [180/845]  eta: 0:03:43  loss: 1.0901 (1.1749)  loss_n_40: 0.2548 (0.2816)  loss_n_60: 0.2494 (0.2726)  loss_n_80: 0.2804 (0.2939)  loss_n_100: 0.2981 (0.3268)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [190/845]  eta: 0:03:40  loss: 1.1316 (1.1760)  loss_n_40: 0.2674 (0.2821)  loss_n_60: 0.2660 (0.2730)  loss_n_80: 0.2872 (0.2940)  loss_n_100: 0.3044 (0.3268)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [200/845]  eta: 0:03:36  loss: 1.0933 (1.1698)  loss_n_40: 0.2553 (0.2801)  loss_n_60: 0.2524 (0.2715)  loss_n_80: 0.2813 (0.2927)  loss_n_100: 0.3044 (0.3255)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [210/845]  eta: 0:03:33  loss: 1.0627 (1.1666)  loss_n_40: 0.2371 (0.2789)  loss_n_60: 0.2410 (0.2706)  loss_n_80: 0.2698 (0.2922)  loss_n_100: 0.2890 (0.3248)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [220/845]  eta: 0:03:29  loss: 1.0722 (1.1670)  loss_n_40: 0.2540 (0.2784)  loss_n_60: 0.2422 (0.2708)  loss_n_80: 0.2822 (0.2925)  loss_n_100: 0.3061 (0.3252)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [230/845]  eta: 0:03:26  loss: 1.1682 (1.1705)  loss_n_40: 0.2566 (0.2794)  loss_n_60: 0.2705 (0.2713)  loss_n_80: 0.3031 (0.2937)  loss_n_100: 0.3232 (0.3261)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [240/845]  eta: 0:03:23  loss: 1.0976 (1.1693)  loss_n_40: 0.2634 (0.2793)  loss_n_60: 0.2541 (0.2711)  loss_n_80: 0.2838 (0.2933)  loss_n_100: 0.3101 (0.3256)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [250/845]  eta: 0:03:19  loss: 1.1587 (1.1700)  loss_n_40: 0.2634 (0.2799)  loss_n_60: 0.2633 (0.2713)  loss_n_80: 0.2876 (0.2932)  loss_n_100: 0.3351 (0.3256)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [260/845]  eta: 0:03:16  loss: 1.1878 (1.1727)  loss_n_40: 0.2663 (0.2805)  loss_n_60: 0.2671 (0.2717)  loss_n_80: 0.3121 (0.2941)  loss_n_100: 0.3410 (0.3265)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [270/845]  eta: 0:03:12  loss: 1.2291 (1.1706)  loss_n_40: 0.2588 (0.2797)  loss_n_60: 0.2671 (0.2713)  loss_n_80: 0.2999 (0.2936)  loss_n_100: 0.3487 (0.3260)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [280/845]  eta: 0:03:09  loss: 1.0964 (1.1680)  loss_n_40: 0.2368 (0.2785)  loss_n_60: 0.2450 (0.2706)  loss_n_80: 0.2891 (0.2932)  loss_n_100: 0.3168 (0.3257)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [290/845]  eta: 0:03:06  loss: 0.9955 (1.1644)  loss_n_40: 0.2234 (0.2771)  loss_n_60: 0.2346 (0.2698)  loss_n_80: 0.2633 (0.2925)  loss_n_100: 0.2885 (0.3250)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [300/845]  eta: 0:03:02  loss: 0.9955 (1.1619)  loss_n_40: 0.2234 (0.2759)  loss_n_60: 0.2301 (0.2693)  loss_n_80: 0.2633 (0.2921)  loss_n_100: 0.2885 (0.3246)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [310/845]  eta: 0:02:59  loss: 1.1588 (1.1632)  loss_n_40: 0.2547 (0.2756)  loss_n_60: 0.2677 (0.2695)  loss_n_80: 0.3019 (0.2927)  loss_n_100: 0.3411 (0.3254)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [320/845]  eta: 0:02:56  loss: 1.1608 (1.1623)  loss_n_40: 0.2547 (0.2751)  loss_n_60: 0.2677 (0.2692)  loss_n_80: 0.3023 (0.2926)  loss_n_100: 0.3420 (0.3254)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [330/845]  eta: 0:02:52  loss: 1.1608 (1.1631)  loss_n_40: 0.2598 (0.2756)  loss_n_60: 0.2598 (0.2694)  loss_n_80: 0.3000 (0.2927)  loss_n_100: 0.3207 (0.3254)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [340/845]  eta: 0:02:49  loss: 1.2053 (1.1654)  loss_n_40: 0.2623 (0.2761)  loss_n_60: 0.2736 (0.2699)  loss_n_80: 0.3000 (0.2933)  loss_n_100: 0.3207 (0.3261)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [350/845]  eta: 0:02:46  loss: 1.0110 (1.1637)  loss_n_40: 0.2385 (0.2765)  loss_n_60: 0.2260 (0.2695)  loss_n_80: 0.2584 (0.2923)  loss_n_100: 0.2841 (0.3254)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [360/845]  eta: 0:02:42  loss: 0.9557 (1.1610)  loss_n_40: 0.2165 (0.2758)  loss_n_60: 0.2150 (0.2689)  loss_n_80: 0.2474 (0.2915)  loss_n_100: 0.2637 (0.3247)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [370/845]  eta: 0:02:39  loss: 0.9867 (1.1588)  loss_n_40: 0.2419 (0.2750)  loss_n_60: 0.2319 (0.2683)  loss_n_80: 0.2483 (0.2912)  loss_n_100: 0.2846 (0.3242)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [380/845]  eta: 0:02:35  loss: 0.9776 (1.1548)  loss_n_40: 0.2388 (0.2739)  loss_n_60: 0.2299 (0.2675)  loss_n_80: 0.2428 (0.2903)  loss_n_100: 0.2704 (0.3231)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [390/845]  eta: 0:02:32  loss: 1.0579 (1.1541)  loss_n_40: 0.2385 (0.2735)  loss_n_60: 0.2451 (0.2673)  loss_n_80: 0.2785 (0.2903)  loss_n_100: 0.2968 (0.3229)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [400/845]  eta: 0:02:29  loss: 1.1455 (1.1574)  loss_n_40: 0.2619 (0.2744)  loss_n_60: 0.2565 (0.2679)  loss_n_80: 0.3036 (0.2910)  loss_n_100: 0.3426 (0.3240)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [410/845]  eta: 0:02:25  loss: 1.2773 (1.1600)  loss_n_40: 0.2797 (0.2753)  loss_n_60: 0.2955 (0.2685)  loss_n_80: 0.3189 (0.2915)  loss_n_100: 0.3648 (0.3247)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [420/845]  eta: 0:02:22  loss: 1.2208 (1.1601)  loss_n_40: 0.2946 (0.2753)  loss_n_60: 0.2800 (0.2685)  loss_n_80: 0.3129 (0.2916)  loss_n_100: 0.3292 (0.3247)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:17]  [430/845]  eta: 0:02:19  loss: 1.0644 (1.1622)  loss_n_40: 0.2428 (0.2763)  loss_n_60: 0.2548 (0.2690)  loss_n_80: 0.2793 (0.2920)  loss_n_100: 0.2984 (0.3249)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [440/845]  eta: 0:02:15  loss: 1.2130 (1.1634)  loss_n_40: 0.2656 (0.2764)  loss_n_60: 0.2744 (0.2693)  loss_n_80: 0.3088 (0.2924)  loss_n_100: 0.3001 (0.3254)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [450/845]  eta: 0:02:12  loss: 1.2130 (1.1629)  loss_n_40: 0.2656 (0.2760)  loss_n_60: 0.2747 (0.2691)  loss_n_80: 0.3112 (0.2923)  loss_n_100: 0.3547 (0.3255)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [460/845]  eta: 0:02:09  loss: 1.1074 (1.1617)  loss_n_40: 0.2531 (0.2756)  loss_n_60: 0.2584 (0.2687)  loss_n_80: 0.2924 (0.2920)  loss_n_100: 0.3215 (0.3253)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [470/845]  eta: 0:02:05  loss: 1.1011 (1.1608)  loss_n_40: 0.2477 (0.2755)  loss_n_60: 0.2428 (0.2684)  loss_n_80: 0.2834 (0.2918)  loss_n_100: 0.3041 (0.3252)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [480/845]  eta: 0:02:02  loss: 1.1147 (1.1599)  loss_n_40: 0.2477 (0.2750)  loss_n_60: 0.2515 (0.2682)  loss_n_80: 0.2839 (0.2917)  loss_n_100: 0.3216 (0.3250)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [490/845]  eta: 0:01:58  loss: 1.1484 (1.1598)  loss_n_40: 0.2597 (0.2750)  loss_n_60: 0.2607 (0.2681)  loss_n_80: 0.2973 (0.2916)  loss_n_100: 0.3305 (0.3251)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [500/845]  eta: 0:01:55  loss: 1.0593 (1.1609)  loss_n_40: 0.2498 (0.2753)  loss_n_60: 0.2472 (0.2684)  loss_n_80: 0.2761 (0.2918)  loss_n_100: 0.3009 (0.3254)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [510/845]  eta: 0:01:52  loss: 1.0704 (1.1617)  loss_n_40: 0.2498 (0.2754)  loss_n_60: 0.2399 (0.2686)  loss_n_80: 0.2761 (0.2921)  loss_n_100: 0.3102 (0.3256)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [520/845]  eta: 0:01:48  loss: 1.1393 (1.1627)  loss_n_40: 0.2705 (0.2756)  loss_n_60: 0.2601 (0.2689)  loss_n_80: 0.2850 (0.2923)  loss_n_100: 0.3327 (0.3259)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [530/845]  eta: 0:01:45  loss: 1.1393 (1.1631)  loss_n_40: 0.2705 (0.2761)  loss_n_60: 0.2601 (0.2689)  loss_n_80: 0.2850 (0.2923)  loss_n_100: 0.3327 (0.3258)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [540/845]  eta: 0:01:42  loss: 1.2108 (1.1640)  loss_n_40: 0.2714 (0.2768)  loss_n_60: 0.2696 (0.2691)  loss_n_80: 0.3201 (0.2924)  loss_n_100: 0.3446 (0.3257)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [550/845]  eta: 0:01:38  loss: 1.0597 (1.1638)  loss_n_40: 0.2493 (0.2766)  loss_n_60: 0.2365 (0.2691)  loss_n_80: 0.2707 (0.2923)  loss_n_100: 0.2933 (0.3257)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [560/845]  eta: 0:01:35  loss: 1.0597 (1.1627)  loss_n_40: 0.2493 (0.2762)  loss_n_60: 0.2365 (0.2688)  loss_n_80: 0.2646 (0.2922)  loss_n_100: 0.2828 (0.3255)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [570/845]  eta: 0:01:32  loss: 1.0734 (1.1623)  loss_n_40: 0.2522 (0.2762)  loss_n_60: 0.2519 (0.2686)  loss_n_80: 0.2663 (0.2920)  loss_n_100: 0.3057 (0.3255)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [580/845]  eta: 0:01:28  loss: 1.1821 (1.1655)  loss_n_40: 0.2544 (0.2771)  loss_n_60: 0.2713 (0.2694)  loss_n_80: 0.2893 (0.2927)  loss_n_100: 0.3484 (0.3263)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [590/845]  eta: 0:01:25  loss: 1.1250 (1.1635)  loss_n_40: 0.2544 (0.2764)  loss_n_60: 0.2623 (0.2690)  loss_n_80: 0.2890 (0.2923)  loss_n_100: 0.3173 (0.3257)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [600/845]  eta: 0:01:22  loss: 1.0093 (1.1635)  loss_n_40: 0.2322 (0.2769)  loss_n_60: 0.2246 (0.2689)  loss_n_80: 0.2636 (0.2922)  loss_n_100: 0.2848 (0.3255)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [610/845]  eta: 0:01:18  loss: 1.0125 (1.1634)  loss_n_40: 0.2375 (0.2771)  loss_n_60: 0.2338 (0.2689)  loss_n_80: 0.2636 (0.2921)  loss_n_100: 0.2874 (0.3253)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [620/845]  eta: 0:01:15  loss: 1.0891 (1.1632)  loss_n_40: 0.2547 (0.2770)  loss_n_60: 0.2546 (0.2688)  loss_n_80: 0.2686 (0.2922)  loss_n_100: 0.2966 (0.3253)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [630/845]  eta: 0:01:12  loss: 1.1142 (1.1643)  loss_n_40: 0.2548 (0.2778)  loss_n_60: 0.2546 (0.2691)  loss_n_80: 0.2764 (0.2923)  loss_n_100: 0.3099 (0.3252)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [640/845]  eta: 0:01:08  loss: 1.0242 (1.1622)  loss_n_40: 0.2443 (0.2774)  loss_n_60: 0.2313 (0.2686)  loss_n_80: 0.2545 (0.2918)  loss_n_100: 0.2788 (0.3245)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:17]  [650/845]  eta: 0:01:05  loss: 1.0383 (1.1623)  loss_n_40: 0.2443 (0.2773)  loss_n_60: 0.2384 (0.2686)  loss_n_80: 0.2663 (0.2918)  loss_n_100: 0.2918 (0.3246)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [660/845]  eta: 0:01:01  loss: 1.1557 (1.1627)  loss_n_40: 0.2606 (0.2772)  loss_n_60: 0.2643 (0.2687)  loss_n_80: 0.2998 (0.2920)  loss_n_100: 0.3367 (0.3248)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [670/845]  eta: 0:00:58  loss: 1.1557 (1.1653)  loss_n_40: 0.2746 (0.2782)  loss_n_60: 0.2663 (0.2693)  loss_n_80: 0.2998 (0.2926)  loss_n_100: 0.3367 (0.3253)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:17]  [680/845]  eta: 0:00:55  loss: 1.1425 (1.1635)  loss_n_40: 0.2575 (0.2776)  loss_n_60: 0.2560 (0.2688)  loss_n_80: 0.2795 (0.2922)  loss_n_100: 0.3102 (0.3249)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [690/845]  eta: 0:00:51  loss: 0.9792 (1.1613)  loss_n_40: 0.2298 (0.2771)  loss_n_60: 0.2266 (0.2683)  loss_n_80: 0.2501 (0.2917)  loss_n_100: 0.2697 (0.3243)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [700/845]  eta: 0:00:48  loss: 1.0727 (1.1625)  loss_n_40: 0.2560 (0.2772)  loss_n_60: 0.2487 (0.2686)  loss_n_80: 0.2789 (0.2920)  loss_n_100: 0.3076 (0.3247)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [710/845]  eta: 0:00:45  loss: 1.1457 (1.1612)  loss_n_40: 0.2733 (0.2768)  loss_n_60: 0.2599 (0.2683)  loss_n_80: 0.2867 (0.2917)  loss_n_100: 0.3220 (0.3244)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [720/845]  eta: 0:00:41  loss: 1.0070 (1.1609)  loss_n_40: 0.2389 (0.2768)  loss_n_60: 0.2344 (0.2683)  loss_n_80: 0.2520 (0.2916)  loss_n_100: 0.2754 (0.3242)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [730/845]  eta: 0:00:38  loss: 1.0036 (1.1631)  loss_n_40: 0.2324 (0.2774)  loss_n_60: 0.2314 (0.2687)  loss_n_80: 0.2527 (0.2921)  loss_n_100: 0.2840 (0.3248)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [740/845]  eta: 0:00:35  loss: 1.2490 (1.1650)  loss_n_40: 0.2697 (0.2781)  loss_n_60: 0.2794 (0.2691)  loss_n_80: 0.3240 (0.2925)  loss_n_100: 0.3606 (0.3253)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [750/845]  eta: 0:00:31  loss: 1.1925 (1.1632)  loss_n_40: 0.2628 (0.2776)  loss_n_60: 0.2748 (0.2687)  loss_n_80: 0.3095 (0.2921)  loss_n_100: 0.2977 (0.3248)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [760/845]  eta: 0:00:28  loss: 0.9777 (1.1630)  loss_n_40: 0.2286 (0.2776)  loss_n_60: 0.2333 (0.2686)  loss_n_80: 0.2489 (0.2921)  loss_n_100: 0.2768 (0.3247)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [770/845]  eta: 0:00:25  loss: 1.0775 (1.1624)  loss_n_40: 0.2362 (0.2775)  loss_n_60: 0.2476 (0.2686)  loss_n_80: 0.2837 (0.2919)  loss_n_100: 0.3001 (0.3244)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [780/845]  eta: 0:00:21  loss: 1.0775 (1.1620)  loss_n_40: 0.2461 (0.2773)  loss_n_60: 0.2476 (0.2684)  loss_n_80: 0.2837 (0.2919)  loss_n_100: 0.3001 (0.3244)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [790/845]  eta: 0:00:18  loss: 1.1170 (1.1633)  loss_n_40: 0.2542 (0.2776)  loss_n_60: 0.2567 (0.2688)  loss_n_80: 0.2859 (0.2922)  loss_n_100: 0.3177 (0.3247)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [800/845]  eta: 0:00:15  loss: 1.1696 (1.1634)  loss_n_40: 0.2505 (0.2777)  loss_n_60: 0.2603 (0.2688)  loss_n_80: 0.2949 (0.2920)  loss_n_100: 0.3428 (0.3248)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [810/845]  eta: 0:00:11  loss: 1.1026 (1.1622)  loss_n_40: 0.2468 (0.2773)  loss_n_60: 0.2553 (0.2685)  loss_n_80: 0.2645 (0.2918)  loss_n_100: 0.3141 (0.3245)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [820/845]  eta: 0:00:08  loss: 1.1026 (1.1644)  loss_n_40: 0.2575 (0.2779)  loss_n_60: 0.2553 (0.2690)  loss_n_80: 0.2866 (0.2924)  loss_n_100: 0.3141 (0.3251)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [830/845]  eta: 0:00:05  loss: 1.0799 (1.1655)  loss_n_40: 0.2607 (0.2785)  loss_n_60: 0.2472 (0.2694)  loss_n_80: 0.2756 (0.2924)  loss_n_100: 0.3027 (0.3252)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [840/845]  eta: 0:00:01  loss: 1.1717 (1.1656)  loss_n_40: 0.2648 (0.2784)  loss_n_60: 0.2606 (0.2694)  loss_n_80: 0.2996 (0.2925)  loss_n_100: 0.3408 (0.3253)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17]  [844/845]  eta: 0:00:00  loss: 1.0799 (1.1644)  loss_n_40: 0.2607 (0.2780)  loss_n_60: 0.2472 (0.2691)  loss_n_80: 0.2756 (0.2922)  loss_n_100: 0.2963 (0.3250)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:17] Total time: 0:04:43 (0.3350 s / it)\n",
      "Averaged stats: loss: 1.0799 (1.1644)  loss_n_40: 0.2607 (0.2780)  loss_n_60: 0.2472 (0.2691)  loss_n_80: 0.2756 (0.2922)  loss_n_100: 0.2963 (0.3250)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_17_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.325%\n",
      "Min loss_n_100: 0.325\n",
      "Best Epoch: 17.000\n",
      "Train: [epoch:18]  [   0/1724]  eta: 1:59:01  lr: 0.000200  loss: 1.2032 (1.2032)  loss_n_40: 0.2707 (0.2707)  loss_n_60: 0.2772 (0.2772)  loss_n_80: 0.3093 (0.3093)  loss_n_100: 0.3460 (0.3460)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1422  data: 0.3894  max mem: 46473\n",
      "Train: [epoch:18]  [  10/1724]  eta: 1:52:34  lr: 0.000200  loss: 1.1979 (1.1955)  loss_n_40: 0.2707 (0.2799)  loss_n_60: 0.2772 (0.2802)  loss_n_80: 0.3049 (0.3038)  loss_n_100: 0.3260 (0.3317)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9407  data: 0.0356  max mem: 46473\n",
      "Train: [epoch:18]  [  20/1724]  eta: 1:51:37  lr: 0.000200  loss: 1.1568 (1.1848)  loss_n_40: 0.2830 (0.2897)  loss_n_60: 0.2696 (0.2780)  loss_n_80: 0.2913 (0.2966)  loss_n_100: 0.3052 (0.3205)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9201  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [  30/1724]  eta: 1:50:51  lr: 0.000200  loss: 1.1425 (1.1804)  loss_n_40: 0.2816 (0.2943)  loss_n_60: 0.2652 (0.2763)  loss_n_80: 0.2814 (0.2937)  loss_n_100: 0.2948 (0.3162)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [  40/1724]  eta: 1:50:08  lr: 0.000200  loss: 1.1135 (1.1672)  loss_n_40: 0.2816 (0.2958)  loss_n_60: 0.2603 (0.2732)  loss_n_80: 0.2727 (0.2875)  loss_n_100: 0.2960 (0.3108)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9176  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [  50/1724]  eta: 1:49:27  lr: 0.000200  loss: 1.2896 (2.0151)  loss_n_40: 0.3715 (0.3405)  loss_n_60: 0.2938 (0.3517)  loss_n_80: 0.2963 (0.3982)  loss_n_100: 0.3313 (0.4556)  triple_100: 0.0000 (0.1623)  triple_80: 0.0000 (0.1065)  triple_60: 0.0000 (0.0732)  triple_40: 0.0000 (0.1272)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [  60/1724]  eta: 1:48:47  lr: 0.000200  loss: 4.3079 (2.3734)  loss_n_40: 0.6634 (0.4158)  loss_n_60: 0.8353 (0.4519)  loss_n_80: 1.0676 (0.5177)  loss_n_100: 1.2623 (0.5947)  triple_100: 0.0000 (0.1357)  triple_80: 0.0000 (0.0890)  triple_60: 0.0000 (0.0612)  triple_40: 0.0000 (0.1074)  time: 3.9195  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [  70/1724]  eta: 1:48:07  lr: 0.000200  loss: 3.7708 (2.5537)  loss_n_40: 0.7102 (0.4507)  loss_n_60: 0.8684 (0.5071)  loss_n_80: 1.0312 (0.5854)  loss_n_100: 1.1761 (0.6726)  triple_100: 0.0000 (0.1166)  triple_80: 0.0000 (0.0765)  triple_60: 0.0000 (0.0526)  triple_40: 0.0000 (0.0923)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [  80/1724]  eta: 1:47:27  lr: 0.000200  loss: 3.1278 (2.5667)  loss_n_40: 0.5709 (0.4606)  loss_n_60: 0.7166 (0.5214)  loss_n_80: 0.8057 (0.5996)  loss_n_100: 0.9359 (0.6888)  triple_100: 0.0000 (0.1022)  triple_80: 0.0000 (0.0670)  triple_60: 0.0000 (0.0461)  triple_40: 0.0000 (0.0809)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [  90/1724]  eta: 1:46:47  lr: 0.000200  loss: 2.2204 (2.5091)  loss_n_40: 0.4651 (0.4601)  loss_n_60: 0.5343 (0.5179)  loss_n_80: 0.6038 (0.5907)  loss_n_100: 0.6693 (0.6768)  triple_100: 0.0000 (0.0910)  triple_80: 0.0000 (0.0597)  triple_60: 0.0000 (0.0410)  triple_40: 0.0000 (0.0720)  time: 3.9177  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 100/1724]  eta: 1:46:07  lr: 0.000200  loss: 1.9005 (2.4371)  loss_n_40: 0.4229 (0.4537)  loss_n_60: 0.4443 (0.5075)  loss_n_80: 0.4704 (0.5779)  loss_n_100: 0.5165 (0.6605)  triple_100: 0.0000 (0.0820)  triple_80: 0.0000 (0.0538)  triple_60: 0.0000 (0.0370)  triple_40: 0.0000 (0.0649)  time: 3.9177  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 110/1724]  eta: 1:45:27  lr: 0.000200  loss: 1.7284 (2.3749)  loss_n_40: 0.3788 (0.4486)  loss_n_60: 0.4005 (0.4993)  loss_n_80: 0.4314 (0.5662)  loss_n_100: 0.4697 (0.6446)  triple_100: 0.0000 (0.0746)  triple_80: 0.0000 (0.0489)  triple_60: 0.0000 (0.0337)  triple_40: 0.0000 (0.0590)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 120/1724]  eta: 1:44:48  lr: 0.000200  loss: 1.6603 (2.3185)  loss_n_40: 0.3765 (0.4434)  loss_n_60: 0.4005 (0.4913)  loss_n_80: 0.4314 (0.5549)  loss_n_100: 0.4596 (0.6306)  triple_100: 0.0000 (0.0684)  triple_80: 0.0000 (0.0449)  triple_60: 0.0000 (0.0309)  triple_40: 0.0000 (0.0541)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 130/1724]  eta: 1:44:09  lr: 0.000200  loss: 1.4891 (2.2497)  loss_n_40: 0.3733 (0.4367)  loss_n_60: 0.3623 (0.4798)  loss_n_80: 0.3787 (0.5399)  loss_n_100: 0.3939 (0.6101)  triple_100: 0.0000 (0.0632)  triple_80: 0.0000 (0.0414)  triple_60: 0.0000 (0.0285)  triple_40: 0.0000 (0.0500)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 140/1724]  eta: 1:43:29  lr: 0.000200  loss: 1.4561 (2.2002)  loss_n_40: 0.3350 (0.4308)  loss_n_60: 0.3543 (0.4716)  loss_n_80: 0.3675 (0.5290)  loss_n_100: 0.3893 (0.5966)  triple_100: 0.0000 (0.0587)  triple_80: 0.0000 (0.0389)  triple_60: 0.0000 (0.0265)  triple_40: 0.0000 (0.0481)  time: 3.9203  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 150/1724]  eta: 1:42:50  lr: 0.000200  loss: 1.5223 (2.1671)  loss_n_40: 0.3501 (0.4277)  loss_n_60: 0.3620 (0.4674)  loss_n_80: 0.3978 (0.5234)  loss_n_100: 0.4256 (0.5879)  triple_100: 0.0000 (0.0548)  triple_80: 0.0000 (0.0363)  triple_60: 0.0000 (0.0247)  triple_40: 0.0000 (0.0449)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 160/1724]  eta: 1:42:11  lr: 0.000200  loss: 1.6125 (2.1305)  loss_n_40: 0.3612 (0.4234)  loss_n_60: 0.3801 (0.4617)  loss_n_80: 0.4252 (0.5162)  loss_n_100: 0.4312 (0.5784)  triple_100: 0.0000 (0.0514)  triple_80: 0.0000 (0.0341)  triple_60: 0.0000 (0.0232)  triple_40: 0.0000 (0.0421)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 170/1724]  eta: 1:41:32  lr: 0.000200  loss: 1.5219 (2.0895)  loss_n_40: 0.3336 (0.4173)  loss_n_60: 0.3647 (0.4546)  loss_n_80: 0.3901 (0.5079)  loss_n_100: 0.4147 (0.5678)  triple_100: 0.0000 (0.0484)  triple_80: 0.0000 (0.0321)  triple_60: 0.0000 (0.0218)  triple_40: 0.0000 (0.0396)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 180/1724]  eta: 1:40:52  lr: 0.000200  loss: 1.2474 (2.0463)  loss_n_40: 0.3028 (0.4111)  loss_n_60: 0.3143 (0.4469)  loss_n_80: 0.3270 (0.4984)  loss_n_100: 0.3342 (0.5559)  triple_100: 0.0000 (0.0457)  triple_80: 0.0000 (0.0303)  triple_60: 0.0000 (0.0206)  triple_40: 0.0000 (0.0374)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 190/1724]  eta: 1:40:13  lr: 0.000200  loss: 1.3563 (2.0156)  loss_n_40: 0.3069 (0.4074)  loss_n_60: 0.3202 (0.4418)  loss_n_80: 0.3511 (0.4918)  loss_n_100: 0.3636 (0.5475)  triple_100: 0.0000 (0.0433)  triple_80: 0.0000 (0.0287)  triple_60: 0.0000 (0.0196)  triple_40: 0.0000 (0.0355)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 200/1724]  eta: 1:39:34  lr: 0.000200  loss: 1.4508 (1.9970)  loss_n_40: 0.3091 (0.4030)  loss_n_60: 0.3388 (0.4371)  loss_n_80: 0.3700 (0.4867)  loss_n_100: 0.4104 (0.5420)  triple_100: 0.0000 (0.0412)  triple_80: 0.0000 (0.0285)  triple_60: 0.0000 (0.0228)  triple_40: 0.0000 (0.0358)  time: 3.9224  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 210/1724]  eta: 1:38:55  lr: 0.000200  loss: 1.6363 (1.9834)  loss_n_40: 0.3389 (0.4017)  loss_n_60: 0.3629 (0.4350)  loss_n_80: 0.4136 (0.4848)  loss_n_100: 0.4626 (0.5397)  triple_100: 0.0000 (0.0392)  triple_80: 0.0000 (0.0271)  triple_60: 0.0000 (0.0217)  triple_40: 0.0000 (0.0341)  time: 3.9196  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 220/1724]  eta: 1:38:16  lr: 0.000200  loss: 1.6316 (1.9659)  loss_n_40: 0.3579 (0.4006)  loss_n_60: 0.3717 (0.4321)  loss_n_80: 0.4153 (0.4812)  loss_n_100: 0.4648 (0.5354)  triple_100: 0.0000 (0.0375)  triple_80: 0.0000 (0.0259)  triple_60: 0.0000 (0.0208)  triple_40: 0.0000 (0.0325)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 230/1724]  eta: 1:37:36  lr: 0.000200  loss: 1.5900 (1.9497)  loss_n_40: 0.3712 (0.3992)  loss_n_60: 0.3662 (0.4293)  loss_n_80: 0.4146 (0.4780)  loss_n_100: 0.4526 (0.5317)  triple_100: 0.0000 (0.0358)  triple_80: 0.0000 (0.0248)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0311)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 240/1724]  eta: 1:36:57  lr: 0.000200  loss: 1.3808 (1.9266)  loss_n_40: 0.3396 (0.3965)  loss_n_60: 0.3254 (0.4253)  loss_n_80: 0.3480 (0.4727)  loss_n_100: 0.3749 (0.5251)  triple_100: 0.0000 (0.0343)  triple_80: 0.0000 (0.0237)  triple_60: 0.0000 (0.0190)  triple_40: 0.0000 (0.0298)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 250/1724]  eta: 1:36:18  lr: 0.000200  loss: 1.3436 (1.9135)  loss_n_40: 0.3159 (0.3956)  loss_n_60: 0.3196 (0.4235)  loss_n_80: 0.3454 (0.4694)  loss_n_100: 0.3670 (0.5205)  triple_100: 0.0000 (0.0330)  triple_80: 0.0000 (0.0228)  triple_60: 0.0000 (0.0200)  triple_40: 0.0000 (0.0287)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 260/1724]  eta: 1:35:39  lr: 0.000200  loss: 1.4188 (1.8972)  loss_n_40: 0.3434 (0.3942)  loss_n_60: 0.3440 (0.4210)  loss_n_80: 0.3565 (0.4659)  loss_n_100: 0.3882 (0.5157)  triple_100: 0.0000 (0.0317)  triple_80: 0.0000 (0.0219)  triple_60: 0.0000 (0.0193)  triple_40: 0.0000 (0.0276)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 270/1724]  eta: 1:34:59  lr: 0.000200  loss: 1.4568 (1.8808)  loss_n_40: 0.3374 (0.3921)  loss_n_60: 0.3440 (0.4183)  loss_n_80: 0.3728 (0.4625)  loss_n_100: 0.3882 (0.5112)  triple_100: 0.0000 (0.0305)  triple_80: 0.0000 (0.0211)  triple_60: 0.0000 (0.0186)  triple_40: 0.0000 (0.0265)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 280/1724]  eta: 1:34:20  lr: 0.000200  loss: 1.3823 (1.8609)  loss_n_40: 0.3311 (0.3895)  loss_n_60: 0.3331 (0.4148)  loss_n_80: 0.3441 (0.4579)  loss_n_100: 0.3664 (0.5055)  triple_100: 0.0000 (0.0295)  triple_80: 0.0000 (0.0204)  triple_60: 0.0000 (0.0179)  triple_40: 0.0000 (0.0256)  time: 3.9181  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [ 290/1724]  eta: 1:33:41  lr: 0.000200  loss: 1.3893 (1.8499)  loss_n_40: 0.3311 (0.3895)  loss_n_60: 0.3398 (0.4133)  loss_n_80: 0.3380 (0.4553)  loss_n_100: 0.3723 (0.5017)  triple_100: 0.0000 (0.0284)  triple_80: 0.0000 (0.0197)  triple_60: 0.0000 (0.0173)  triple_40: 0.0000 (0.0247)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 300/1724]  eta: 1:33:01  lr: 0.000200  loss: 1.3055 (1.8297)  loss_n_40: 0.3065 (0.3861)  loss_n_60: 0.3130 (0.4095)  loss_n_80: 0.3371 (0.4508)  loss_n_100: 0.3503 (0.4961)  triple_100: 0.0000 (0.0275)  triple_80: 0.0000 (0.0190)  triple_60: 0.0000 (0.0167)  triple_40: 0.0000 (0.0239)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 310/1724]  eta: 1:32:22  lr: 0.000200  loss: 1.2056 (1.8096)  loss_n_40: 0.2833 (0.3831)  loss_n_60: 0.2799 (0.4056)  loss_n_80: 0.3017 (0.4462)  loss_n_100: 0.3128 (0.4904)  triple_100: 0.0000 (0.0266)  triple_80: 0.0000 (0.0184)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0231)  time: 3.9177  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 320/1724]  eta: 1:31:43  lr: 0.000200  loss: 1.0875 (1.7866)  loss_n_40: 0.2601 (0.3790)  loss_n_60: 0.2692 (0.4011)  loss_n_80: 0.2858 (0.4409)  loss_n_100: 0.2897 (0.4839)  triple_100: 0.0000 (0.0258)  triple_80: 0.0000 (0.0178)  triple_60: 0.0000 (0.0157)  triple_40: 0.0000 (0.0224)  time: 3.9179  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 330/1724]  eta: 1:31:03  lr: 0.000200  loss: 1.1030 (1.7695)  loss_n_40: 0.2578 (0.3765)  loss_n_60: 0.2687 (0.3977)  loss_n_80: 0.2823 (0.4369)  loss_n_100: 0.2813 (0.4792)  triple_100: 0.0000 (0.0250)  triple_80: 0.0000 (0.0173)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0217)  time: 3.9176  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 340/1724]  eta: 1:30:24  lr: 0.000200  loss: 1.1030 (1.7487)  loss_n_40: 0.2547 (0.3728)  loss_n_60: 0.2616 (0.3935)  loss_n_80: 0.2823 (0.4320)  loss_n_100: 0.3022 (0.4734)  triple_100: 0.0000 (0.0243)  triple_80: 0.0000 (0.0168)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0211)  time: 3.9170  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 350/1724]  eta: 1:29:45  lr: 0.000200  loss: 1.1534 (1.7680)  loss_n_40: 0.2624 (0.3713)  loss_n_60: 0.2691 (0.3923)  loss_n_80: 0.3027 (0.4310)  loss_n_100: 0.3199 (0.4723)  triple_100: 0.0000 (0.0327)  triple_80: 0.0000 (0.0254)  triple_60: 0.0000 (0.0212)  triple_40: 0.0000 (0.0217)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 360/1724]  eta: 1:29:06  lr: 0.000200  loss: 1.7693 (1.7730)  loss_n_40: 0.3385 (0.3712)  loss_n_60: 0.4062 (0.3938)  loss_n_80: 0.4818 (0.4337)  loss_n_100: 0.5368 (0.4758)  triple_100: 0.0000 (0.0318)  triple_80: 0.0000 (0.0250)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0211)  time: 3.9173  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 370/1724]  eta: 1:28:26  lr: 0.000200  loss: 1.7329 (1.7702)  loss_n_40: 0.3624 (0.3707)  loss_n_60: 0.3977 (0.3938)  loss_n_80: 0.4818 (0.4337)  loss_n_100: 0.5368 (0.4760)  triple_100: 0.0000 (0.0309)  triple_80: 0.0000 (0.0244)  triple_60: 0.0000 (0.0201)  triple_40: 0.0000 (0.0206)  time: 3.9163  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 380/1724]  eta: 1:27:47  lr: 0.000200  loss: 1.5174 (1.7648)  loss_n_40: 0.3456 (0.3705)  loss_n_60: 0.3705 (0.3934)  loss_n_80: 0.4008 (0.4328)  loss_n_100: 0.4392 (0.4748)  triple_100: 0.0000 (0.0301)  triple_80: 0.0000 (0.0237)  triple_60: 0.0000 (0.0196)  triple_40: 0.0000 (0.0200)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 390/1724]  eta: 1:27:08  lr: 0.000200  loss: 1.4404 (1.7546)  loss_n_40: 0.3414 (0.3693)  loss_n_60: 0.3471 (0.3918)  loss_n_80: 0.3662 (0.4305)  loss_n_100: 0.3962 (0.4720)  triple_100: 0.0000 (0.0293)  triple_80: 0.0000 (0.0231)  triple_60: 0.0000 (0.0191)  triple_40: 0.0000 (0.0195)  time: 3.9173  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 400/1724]  eta: 1:26:29  lr: 0.000200  loss: 1.3864 (1.7468)  loss_n_40: 0.3284 (0.3690)  loss_n_60: 0.3244 (0.3906)  loss_n_80: 0.3479 (0.4287)  loss_n_100: 0.3739 (0.4697)  triple_100: 0.0000 (0.0286)  triple_80: 0.0000 (0.0225)  triple_60: 0.0000 (0.0186)  triple_40: 0.0000 (0.0190)  time: 3.9180  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 410/1724]  eta: 1:25:49  lr: 0.000200  loss: 1.3522 (1.7381)  loss_n_40: 0.3284 (0.3685)  loss_n_60: 0.3246 (0.3893)  loss_n_80: 0.3372 (0.4266)  loss_n_100: 0.3507 (0.4671)  triple_100: 0.0000 (0.0279)  triple_80: 0.0000 (0.0220)  triple_60: 0.0000 (0.0181)  triple_40: 0.0000 (0.0186)  time: 3.9178  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 420/1724]  eta: 1:25:10  lr: 0.000200  loss: 1.2769 (1.7258)  loss_n_40: 0.3043 (0.3667)  loss_n_60: 0.3076 (0.3871)  loss_n_80: 0.3234 (0.4238)  loss_n_100: 0.3313 (0.4637)  triple_100: 0.0000 (0.0272)  triple_80: 0.0000 (0.0215)  triple_60: 0.0000 (0.0177)  triple_40: 0.0000 (0.0181)  time: 3.9185  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 430/1724]  eta: 1:24:31  lr: 0.000200  loss: 1.2442 (1.7154)  loss_n_40: 0.3027 (0.3655)  loss_n_60: 0.3026 (0.3853)  loss_n_80: 0.3114 (0.4214)  loss_n_100: 0.3231 (0.4607)  triple_100: 0.0000 (0.0266)  triple_80: 0.0000 (0.0210)  triple_60: 0.0000 (0.0173)  triple_40: 0.0000 (0.0177)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 440/1724]  eta: 1:23:52  lr: 0.000200  loss: 1.1913 (1.7024)  loss_n_40: 0.3027 (0.3635)  loss_n_60: 0.2824 (0.3828)  loss_n_80: 0.2934 (0.4184)  loss_n_100: 0.3112 (0.4570)  triple_100: 0.0000 (0.0260)  triple_80: 0.0000 (0.0205)  triple_60: 0.0000 (0.0169)  triple_40: 0.0000 (0.0173)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 450/1724]  eta: 1:23:12  lr: 0.000200  loss: 1.1874 (1.6914)  loss_n_40: 0.2840 (0.3620)  loss_n_60: 0.2794 (0.3809)  loss_n_80: 0.2903 (0.4158)  loss_n_100: 0.3071 (0.4538)  triple_100: 0.0000 (0.0254)  triple_80: 0.0000 (0.0200)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0169)  time: 3.9171  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 460/1724]  eta: 1:22:33  lr: 0.000200  loss: 1.2917 (1.6836)  loss_n_40: 0.3333 (0.3617)  loss_n_60: 0.3072 (0.3794)  loss_n_80: 0.3170 (0.4139)  loss_n_100: 0.3363 (0.4514)  triple_100: 0.0000 (0.0249)  triple_80: 0.0000 (0.0196)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0165)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 470/1724]  eta: 1:21:54  lr: 0.000200  loss: 1.3106 (1.6753)  loss_n_40: 0.3333 (0.3606)  loss_n_60: 0.3061 (0.3778)  loss_n_80: 0.3286 (0.4119)  loss_n_100: 0.3395 (0.4490)  triple_100: 0.0000 (0.0243)  triple_80: 0.0000 (0.0192)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0163)  time: 3.9175  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 480/1724]  eta: 1:21:15  lr: 0.000200  loss: 1.1875 (1.6644)  loss_n_40: 0.2861 (0.3590)  loss_n_60: 0.2823 (0.3756)  loss_n_80: 0.3035 (0.4094)  loss_n_100: 0.3138 (0.4460)  triple_100: 0.0000 (0.0238)  triple_80: 0.0000 (0.0188)  triple_60: 0.0000 (0.0158)  triple_40: 0.0000 (0.0160)  time: 3.9174  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 490/1724]  eta: 1:20:35  lr: 0.000200  loss: 1.1053 (1.6527)  loss_n_40: 0.2625 (0.3572)  loss_n_60: 0.2562 (0.3732)  loss_n_80: 0.2695 (0.4066)  loss_n_100: 0.2884 (0.4427)  triple_100: 0.0000 (0.0234)  triple_80: 0.0000 (0.0184)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0156)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 500/1724]  eta: 1:19:56  lr: 0.000200  loss: 1.0231 (1.6408)  loss_n_40: 0.2499 (0.3552)  loss_n_60: 0.2487 (0.3709)  loss_n_80: 0.2571 (0.4038)  loss_n_100: 0.2650 (0.4394)  triple_100: 0.0000 (0.0229)  triple_80: 0.0000 (0.0180)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0153)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 510/1724]  eta: 1:19:17  lr: 0.000200  loss: 1.1034 (1.6305)  loss_n_40: 0.2718 (0.3538)  loss_n_60: 0.2635 (0.3689)  loss_n_80: 0.2679 (0.4014)  loss_n_100: 0.2779 (0.4364)  triple_100: 0.0000 (0.0224)  triple_80: 0.0000 (0.0177)  triple_60: 0.0000 (0.0149)  triple_40: 0.0000 (0.0150)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 520/1724]  eta: 1:18:38  lr: 0.000200  loss: 1.0514 (1.6195)  loss_n_40: 0.2590 (0.3519)  loss_n_60: 0.2572 (0.3667)  loss_n_80: 0.2675 (0.3988)  loss_n_100: 0.2704 (0.4333)  triple_100: 0.0000 (0.0220)  triple_80: 0.0000 (0.0174)  triple_60: 0.0000 (0.0146)  triple_40: 0.0000 (0.0147)  time: 3.9204  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [ 530/1724]  eta: 1:17:59  lr: 0.000200  loss: 1.0514 (1.6096)  loss_n_40: 0.2586 (0.3504)  loss_n_60: 0.2568 (0.3648)  loss_n_80: 0.2633 (0.3965)  loss_n_100: 0.2684 (0.4305)  triple_100: 0.0000 (0.0216)  triple_80: 0.0000 (0.0170)  triple_60: 0.0000 (0.0143)  triple_40: 0.0000 (0.0145)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 540/1724]  eta: 1:17:20  lr: 0.000200  loss: 1.0592 (1.5994)  loss_n_40: 0.2626 (0.3487)  loss_n_60: 0.2597 (0.3628)  loss_n_80: 0.2674 (0.3941)  loss_n_100: 0.2723 (0.4276)  triple_100: 0.0000 (0.0212)  triple_80: 0.0000 (0.0167)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0142)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 550/1724]  eta: 1:16:40  lr: 0.000200  loss: 0.9885 (1.5885)  loss_n_40: 0.2413 (0.3469)  loss_n_60: 0.2433 (0.3606)  loss_n_80: 0.2475 (0.3915)  loss_n_100: 0.2550 (0.4245)  triple_100: 0.0000 (0.0208)  triple_80: 0.0000 (0.0164)  triple_60: 0.0000 (0.0138)  triple_40: 0.0000 (0.0139)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 560/1724]  eta: 1:16:01  lr: 0.000200  loss: 0.9841 (1.5789)  loss_n_40: 0.2450 (0.3456)  loss_n_60: 0.2374 (0.3588)  loss_n_80: 0.2469 (0.3892)  loss_n_100: 0.2380 (0.4216)  triple_100: 0.0000 (0.0204)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0136)  triple_40: 0.0000 (0.0137)  time: 3.9191  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 570/1724]  eta: 1:15:22  lr: 0.000200  loss: 1.1550 (1.5727)  loss_n_40: 0.2941 (0.3446)  loss_n_60: 0.2878 (0.3574)  loss_n_80: 0.2731 (0.3874)  loss_n_100: 0.2855 (0.4195)  triple_100: 0.0000 (0.0206)  triple_80: 0.0000 (0.0162)  triple_60: 0.0000 (0.0134)  triple_40: 0.0000 (0.0135)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 580/1724]  eta: 1:14:43  lr: 0.000200  loss: 1.2177 (1.5669)  loss_n_40: 0.2791 (0.3434)  loss_n_60: 0.2877 (0.3562)  loss_n_80: 0.3092 (0.3863)  loss_n_100: 0.3314 (0.4184)  triple_100: 0.0000 (0.0202)  triple_80: 0.0000 (0.0160)  triple_60: 0.0000 (0.0132)  triple_40: 0.0000 (0.0132)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 590/1724]  eta: 1:14:04  lr: 0.000200  loss: 1.2403 (1.5630)  loss_n_40: 0.2682 (0.3424)  loss_n_60: 0.2877 (0.3554)  loss_n_80: 0.3373 (0.3856)  loss_n_100: 0.3580 (0.4176)  triple_100: 0.0000 (0.0200)  triple_80: 0.0000 (0.0159)  triple_60: 0.0000 (0.0130)  triple_40: 0.0000 (0.0130)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 600/1724]  eta: 1:13:24  lr: 0.000200  loss: 1.4687 (1.5700)  loss_n_40: 0.3137 (0.3430)  loss_n_60: 0.3240 (0.3564)  loss_n_80: 0.3660 (0.3871)  loss_n_100: 0.4025 (0.4194)  triple_100: 0.0000 (0.0207)  triple_80: 0.0000 (0.0178)  triple_60: 0.0000 (0.0128)  triple_40: 0.0000 (0.0128)  time: 3.9179  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 610/1724]  eta: 1:12:45  lr: 0.000200  loss: 1.8736 (1.5752)  loss_n_40: 0.3622 (0.3433)  loss_n_60: 0.4227 (0.3576)  loss_n_80: 0.5121 (0.3891)  loss_n_100: 0.6073 (0.4221)  triple_100: 0.0000 (0.0203)  triple_80: 0.0000 (0.0175)  triple_60: 0.0000 (0.0126)  triple_40: 0.0000 (0.0126)  time: 3.9175  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 620/1724]  eta: 1:12:06  lr: 0.000200  loss: 1.8292 (1.5769)  loss_n_40: 0.3622 (0.3438)  loss_n_60: 0.4137 (0.3583)  loss_n_80: 0.4868 (0.3900)  loss_n_100: 0.5146 (0.4228)  triple_100: 0.0000 (0.0200)  triple_80: 0.0000 (0.0173)  triple_60: 0.0000 (0.0124)  triple_40: 0.0000 (0.0124)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 630/1724]  eta: 1:11:27  lr: 0.000200  loss: 1.5648 (1.5744)  loss_n_40: 0.3477 (0.3436)  loss_n_60: 0.3687 (0.3581)  loss_n_80: 0.3906 (0.3896)  loss_n_100: 0.3934 (0.4220)  triple_100: 0.0000 (0.0197)  triple_80: 0.0000 (0.0170)  triple_60: 0.0000 (0.0122)  triple_40: 0.0000 (0.0122)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 640/1724]  eta: 1:10:48  lr: 0.000200  loss: 1.3476 (1.5716)  loss_n_40: 0.3085 (0.3434)  loss_n_60: 0.3195 (0.3579)  loss_n_80: 0.3416 (0.3890)  loss_n_100: 0.3499 (0.4212)  triple_100: 0.0000 (0.0194)  triple_80: 0.0000 (0.0167)  triple_60: 0.0000 (0.0120)  triple_40: 0.0000 (0.0120)  time: 3.9177  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 650/1724]  eta: 1:10:08  lr: 0.000200  loss: 1.1975 (1.5647)  loss_n_40: 0.2762 (0.3422)  loss_n_60: 0.2936 (0.3566)  loss_n_80: 0.3049 (0.3874)  loss_n_100: 0.3169 (0.4192)  triple_100: 0.0000 (0.0191)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0118)  triple_40: 0.0000 (0.0118)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 660/1724]  eta: 1:09:29  lr: 0.000200  loss: 1.1306 (1.5601)  loss_n_40: 0.2684 (0.3416)  loss_n_60: 0.2776 (0.3559)  loss_n_80: 0.2917 (0.3863)  loss_n_100: 0.3035 (0.4180)  triple_100: 0.0000 (0.0188)  triple_80: 0.0000 (0.0162)  triple_60: 0.0000 (0.0116)  triple_40: 0.0000 (0.0116)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 670/1724]  eta: 1:08:50  lr: 0.000200  loss: 1.1222 (1.5543)  loss_n_40: 0.2691 (0.3407)  loss_n_60: 0.2775 (0.3549)  loss_n_80: 0.2832 (0.3850)  loss_n_100: 0.2864 (0.4162)  triple_100: 0.0000 (0.0185)  triple_80: 0.0000 (0.0160)  triple_60: 0.0000 (0.0115)  triple_40: 0.0000 (0.0115)  time: 3.9190  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 680/1724]  eta: 1:08:11  lr: 0.000200  loss: 1.1016 (1.5487)  loss_n_40: 0.2753 (0.3401)  loss_n_60: 0.2775 (0.3540)  loss_n_80: 0.2777 (0.3835)  loss_n_100: 0.2864 (0.4145)  triple_100: 0.0000 (0.0182)  triple_80: 0.0000 (0.0157)  triple_60: 0.0000 (0.0113)  triple_40: 0.0000 (0.0113)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 690/1724]  eta: 1:07:32  lr: 0.000200  loss: 1.1847 (1.5433)  loss_n_40: 0.2771 (0.3393)  loss_n_60: 0.2936 (0.3530)  loss_n_80: 0.2888 (0.3822)  loss_n_100: 0.3094 (0.4130)  triple_100: 0.0000 (0.0180)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0111)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 700/1724]  eta: 1:06:52  lr: 0.000200  loss: 1.1595 (1.5376)  loss_n_40: 0.2705 (0.3384)  loss_n_60: 0.2786 (0.3519)  loss_n_80: 0.2898 (0.3808)  loss_n_100: 0.3069 (0.4113)  triple_100: 0.0000 (0.0177)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0110)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 710/1724]  eta: 1:06:13  lr: 0.000200  loss: 1.0622 (1.5314)  loss_n_40: 0.2705 (0.3374)  loss_n_60: 0.2561 (0.3506)  loss_n_80: 0.2746 (0.3794)  loss_n_100: 0.2835 (0.4096)  triple_100: 0.0000 (0.0175)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0109)  triple_40: 0.0000 (0.0109)  time: 3.9183  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 720/1724]  eta: 1:05:34  lr: 0.000200  loss: 1.0666 (1.5266)  loss_n_40: 0.2630 (0.3369)  loss_n_60: 0.2550 (0.3497)  loss_n_80: 0.2725 (0.3782)  loss_n_100: 0.2879 (0.4082)  triple_100: 0.0000 (0.0172)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0108)  triple_40: 0.0000 (0.0107)  time: 3.9190  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 730/1724]  eta: 1:04:55  lr: 0.000200  loss: 1.0860 (1.5210)  loss_n_40: 0.2630 (0.3362)  loss_n_60: 0.2665 (0.3486)  loss_n_80: 0.2780 (0.3767)  loss_n_100: 0.2926 (0.4066)  triple_100: 0.0000 (0.0170)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0106)  triple_40: 0.0000 (0.0106)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 740/1724]  eta: 1:04:16  lr: 0.000200  loss: 1.0860 (1.5150)  loss_n_40: 0.2654 (0.3353)  loss_n_60: 0.2665 (0.3474)  loss_n_80: 0.2780 (0.3753)  loss_n_100: 0.2807 (0.4048)  triple_100: 0.0000 (0.0168)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0105)  triple_40: 0.0000 (0.0104)  time: 3.9194  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 750/1724]  eta: 1:03:37  lr: 0.000200  loss: 1.1057 (1.5096)  loss_n_40: 0.2695 (0.3345)  loss_n_60: 0.2673 (0.3464)  loss_n_80: 0.2733 (0.3740)  loss_n_100: 0.2775 (0.4032)  triple_100: 0.0000 (0.0165)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0103)  triple_40: 0.0000 (0.0103)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 760/1724]  eta: 1:02:57  lr: 0.000200  loss: 1.1057 (1.5057)  loss_n_40: 0.2767 (0.3341)  loss_n_60: 0.2705 (0.3457)  loss_n_80: 0.2733 (0.3730)  loss_n_100: 0.2846 (0.4021)  triple_100: 0.0000 (0.0163)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0102)  triple_40: 0.0000 (0.0102)  time: 3.9185  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [ 770/1724]  eta: 1:02:18  lr: 0.000200  loss: 1.0043 (1.4985)  loss_n_40: 0.2400 (0.3328)  loss_n_60: 0.2454 (0.3442)  loss_n_80: 0.2544 (0.3712)  loss_n_100: 0.2646 (0.4001)  triple_100: 0.0000 (0.0161)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0101)  triple_40: 0.0000 (0.0100)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 780/1724]  eta: 1:01:39  lr: 0.000200  loss: 1.1581 (1.5014)  loss_n_40: 0.3085 (0.3329)  loss_n_60: 0.2663 (0.3444)  loss_n_80: 0.2722 (0.3716)  loss_n_100: 0.3082 (0.4005)  triple_100: 0.0000 (0.0163)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0102)  triple_40: 0.0000 (0.0109)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 790/1724]  eta: 1:01:00  lr: 0.000200  loss: 1.6208 (1.5038)  loss_n_40: 0.3305 (0.3332)  loss_n_60: 0.3517 (0.3449)  loss_n_80: 0.3882 (0.3722)  loss_n_100: 0.4433 (0.4016)  triple_100: 0.0000 (0.0161)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0105)  triple_40: 0.0000 (0.0108)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 800/1724]  eta: 1:00:21  lr: 0.000200  loss: 1.3883 (1.5014)  loss_n_40: 0.2884 (0.3327)  loss_n_60: 0.3281 (0.3445)  loss_n_80: 0.3621 (0.3718)  loss_n_100: 0.3957 (0.4011)  triple_100: 0.0000 (0.0159)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0104)  triple_40: 0.0000 (0.0107)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 810/1724]  eta: 0:59:41  lr: 0.000200  loss: 1.2520 (1.4983)  loss_n_40: 0.2824 (0.3323)  loss_n_60: 0.2939 (0.3439)  loss_n_80: 0.3238 (0.3712)  loss_n_100: 0.3385 (0.4003)  triple_100: 0.0000 (0.0157)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0102)  triple_40: 0.0000 (0.0105)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 820/1724]  eta: 0:59:02  lr: 0.000200  loss: 1.1578 (1.4942)  loss_n_40: 0.2732 (0.3316)  loss_n_60: 0.2790 (0.3432)  loss_n_80: 0.2972 (0.3703)  loss_n_100: 0.3138 (0.3992)  triple_100: 0.0000 (0.0155)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0101)  triple_40: 0.0000 (0.0104)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 830/1724]  eta: 0:58:23  lr: 0.000200  loss: 1.1299 (1.4906)  loss_n_40: 0.2634 (0.3311)  loss_n_60: 0.2718 (0.3425)  loss_n_80: 0.2856 (0.3695)  loss_n_100: 0.2968 (0.3981)  triple_100: 0.0000 (0.0154)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0100)  triple_40: 0.0000 (0.0103)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 840/1724]  eta: 0:57:44  lr: 0.000200  loss: 1.1299 (1.4871)  loss_n_40: 0.2811 (0.3308)  loss_n_60: 0.2787 (0.3420)  loss_n_80: 0.2767 (0.3686)  loss_n_100: 0.2869 (0.3970)  triple_100: 0.0000 (0.0152)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0099)  triple_40: 0.0000 (0.0101)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 850/1724]  eta: 0:57:05  lr: 0.000200  loss: 1.1088 (1.4829)  loss_n_40: 0.2812 (0.3303)  loss_n_60: 0.2716 (0.3412)  loss_n_80: 0.2741 (0.3675)  loss_n_100: 0.2785 (0.3957)  triple_100: 0.0000 (0.0150)  triple_80: 0.0000 (0.0134)  triple_60: 0.0000 (0.0097)  triple_40: 0.0000 (0.0100)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 860/1724]  eta: 0:56:25  lr: 0.000200  loss: 1.1088 (1.4805)  loss_n_40: 0.2812 (0.3301)  loss_n_60: 0.2687 (0.3409)  loss_n_80: 0.2741 (0.3669)  loss_n_100: 0.2857 (0.3949)  triple_100: 0.0000 (0.0148)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0099)  time: 3.9183  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 870/1724]  eta: 0:55:46  lr: 0.000200  loss: 1.0985 (1.4765)  loss_n_40: 0.2848 (0.3295)  loss_n_60: 0.2654 (0.3401)  loss_n_80: 0.2722 (0.3660)  loss_n_100: 0.2777 (0.3937)  triple_100: 0.0000 (0.0146)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0095)  triple_40: 0.0000 (0.0098)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 880/1724]  eta: 0:55:07  lr: 0.000200  loss: 1.0652 (1.4720)  loss_n_40: 0.2582 (0.3288)  loss_n_60: 0.2535 (0.3392)  loss_n_80: 0.2704 (0.3649)  loss_n_100: 0.2775 (0.3925)  triple_100: 0.0000 (0.0145)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0097)  time: 3.9222  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 890/1724]  eta: 0:54:28  lr: 0.000200  loss: 1.0402 (1.4672)  loss_n_40: 0.2589 (0.3282)  loss_n_60: 0.2534 (0.3382)  loss_n_80: 0.2593 (0.3637)  loss_n_100: 0.2728 (0.3911)  triple_100: 0.0000 (0.0143)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0093)  triple_40: 0.0000 (0.0096)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 900/1724]  eta: 0:53:49  lr: 0.000200  loss: 0.9323 (1.4619)  loss_n_40: 0.2286 (0.3273)  loss_n_60: 0.2245 (0.3371)  loss_n_80: 0.2390 (0.3624)  loss_n_100: 0.2508 (0.3896)  triple_100: 0.0000 (0.0142)  triple_80: 0.0000 (0.0127)  triple_60: 0.0000 (0.0092)  triple_40: 0.0000 (0.0095)  time: 3.9227  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 910/1724]  eta: 0:53:10  lr: 0.000200  loss: 1.1050 (1.4580)  loss_n_40: 0.2580 (0.3268)  loss_n_60: 0.2638 (0.3363)  loss_n_80: 0.2545 (0.3614)  loss_n_100: 0.2814 (0.3885)  triple_100: 0.0000 (0.0140)  triple_80: 0.0000 (0.0125)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0094)  time: 3.9215  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 920/1724]  eta: 0:52:30  lr: 0.000200  loss: 1.0135 (1.4525)  loss_n_40: 0.2507 (0.3259)  loss_n_60: 0.2448 (0.3352)  loss_n_80: 0.2511 (0.3601)  loss_n_100: 0.2625 (0.3869)  triple_100: 0.0000 (0.0139)  triple_80: 0.0000 (0.0124)  triple_60: 0.0000 (0.0090)  triple_40: 0.0000 (0.0093)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 930/1724]  eta: 0:51:51  lr: 0.000200  loss: 0.9027 (1.4470)  loss_n_40: 0.2364 (0.3249)  loss_n_60: 0.2206 (0.3340)  loss_n_80: 0.2233 (0.3587)  loss_n_100: 0.2368 (0.3853)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0123)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0092)  time: 3.9183  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 940/1724]  eta: 0:51:12  lr: 0.000200  loss: 0.9091 (1.4420)  loss_n_40: 0.2306 (0.3241)  loss_n_60: 0.2223 (0.3330)  loss_n_80: 0.2261 (0.3574)  loss_n_100: 0.2384 (0.3839)  triple_100: 0.0000 (0.0136)  triple_80: 0.0000 (0.0122)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0091)  time: 3.9182  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 950/1724]  eta: 0:50:33  lr: 0.000200  loss: 1.0483 (1.4383)  loss_n_40: 0.2475 (0.3235)  loss_n_60: 0.2516 (0.3323)  loss_n_80: 0.2645 (0.3566)  loss_n_100: 0.2780 (0.3829)  triple_100: 0.0000 (0.0134)  triple_80: 0.0000 (0.0120)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0090)  time: 3.9194  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 960/1724]  eta: 0:49:54  lr: 0.000200  loss: 1.0759 (1.4341)  loss_n_40: 0.2475 (0.3226)  loss_n_60: 0.2555 (0.3314)  loss_n_80: 0.2658 (0.3556)  loss_n_100: 0.2830 (0.3818)  triple_100: 0.0000 (0.0133)  triple_80: 0.0000 (0.0119)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0089)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 970/1724]  eta: 0:49:14  lr: 0.000200  loss: 0.9951 (1.4299)  loss_n_40: 0.2379 (0.3218)  loss_n_60: 0.2366 (0.3305)  loss_n_80: 0.2507 (0.3546)  loss_n_100: 0.2668 (0.3807)  triple_100: 0.0000 (0.0131)  triple_80: 0.0000 (0.0118)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0088)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [ 980/1724]  eta: 0:48:35  lr: 0.000200  loss: 1.0073 (1.4264)  loss_n_40: 0.2607 (0.3213)  loss_n_60: 0.2449 (0.3299)  loss_n_80: 0.2530 (0.3537)  loss_n_100: 0.2549 (0.3796)  triple_100: 0.0000 (0.0130)  triple_80: 0.0000 (0.0117)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0087)  time: 3.9207  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [ 990/1724]  eta: 0:47:56  lr: 0.000200  loss: 1.0506 (1.4231)  loss_n_40: 0.2809 (0.3209)  loss_n_60: 0.2520 (0.3292)  loss_n_80: 0.2604 (0.3529)  loss_n_100: 0.2827 (0.3786)  triple_100: 0.0000 (0.0129)  triple_80: 0.0000 (0.0116)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0087)  time: 3.9199  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1000/1724]  eta: 0:47:17  lr: 0.000200  loss: 0.9948 (1.4197)  loss_n_40: 0.2426 (0.3204)  loss_n_60: 0.2367 (0.3284)  loss_n_80: 0.2542 (0.3521)  loss_n_100: 0.2633 (0.3777)  triple_100: 0.0000 (0.0127)  triple_80: 0.0000 (0.0115)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0086)  time: 3.9198  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [1010/1724]  eta: 0:46:38  lr: 0.000200  loss: 1.0617 (1.4169)  loss_n_40: 0.2737 (0.3201)  loss_n_60: 0.2559 (0.3279)  loss_n_80: 0.2662 (0.3514)  loss_n_100: 0.2783 (0.3769)  triple_100: 0.0000 (0.0126)  triple_80: 0.0000 (0.0114)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0085)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1020/1724]  eta: 0:45:59  lr: 0.000200  loss: 1.1003 (1.4141)  loss_n_40: 0.2737 (0.3196)  loss_n_60: 0.2601 (0.3273)  loss_n_80: 0.2675 (0.3507)  loss_n_100: 0.2888 (0.3762)  triple_100: 0.0000 (0.0125)  triple_80: 0.0000 (0.0113)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0084)  time: 3.9189  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1030/1724]  eta: 0:45:19  lr: 0.000200  loss: 1.1003 (1.4114)  loss_n_40: 0.2555 (0.3192)  loss_n_60: 0.2601 (0.3268)  loss_n_80: 0.2675 (0.3501)  loss_n_100: 0.2888 (0.3754)  triple_100: 0.0000 (0.0124)  triple_80: 0.0000 (0.0111)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0083)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1040/1724]  eta: 0:44:40  lr: 0.000200  loss: 1.0274 (1.4082)  loss_n_40: 0.2446 (0.3186)  loss_n_60: 0.2516 (0.3260)  loss_n_80: 0.2526 (0.3492)  loss_n_100: 0.2683 (0.3745)  triple_100: 0.0000 (0.0123)  triple_80: 0.0000 (0.0110)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0084)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1050/1724]  eta: 0:44:01  lr: 0.000200  loss: 1.0274 (1.4051)  loss_n_40: 0.2446 (0.3180)  loss_n_60: 0.2493 (0.3254)  loss_n_80: 0.2553 (0.3485)  loss_n_100: 0.2741 (0.3737)  triple_100: 0.0000 (0.0121)  triple_80: 0.0000 (0.0109)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0083)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1060/1724]  eta: 0:43:22  lr: 0.000200  loss: 1.1510 (1.4036)  loss_n_40: 0.2727 (0.3179)  loss_n_60: 0.2668 (0.3251)  loss_n_80: 0.2925 (0.3482)  loss_n_100: 0.3112 (0.3733)  triple_100: 0.0000 (0.0120)  triple_80: 0.0000 (0.0108)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0082)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1070/1724]  eta: 0:42:43  lr: 0.000200  loss: 1.1558 (1.4010)  loss_n_40: 0.2743 (0.3175)  loss_n_60: 0.2776 (0.3246)  loss_n_80: 0.2949 (0.3476)  loss_n_100: 0.3219 (0.3727)  triple_100: 0.0000 (0.0119)  triple_80: 0.0000 (0.0107)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0081)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1080/1724]  eta: 0:42:03  lr: 0.000200  loss: 1.1121 (1.3987)  loss_n_40: 0.2743 (0.3172)  loss_n_60: 0.2636 (0.3241)  loss_n_80: 0.2781 (0.3470)  loss_n_100: 0.2916 (0.3720)  triple_100: 0.0000 (0.0118)  triple_80: 0.0000 (0.0106)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0081)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1090/1724]  eta: 0:41:24  lr: 0.000200  loss: 1.1262 (1.3964)  loss_n_40: 0.2764 (0.3170)  loss_n_60: 0.2777 (0.3237)  loss_n_80: 0.2823 (0.3464)  loss_n_100: 0.2915 (0.3713)  triple_100: 0.0000 (0.0117)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0080)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1100/1724]  eta: 0:40:45  lr: 0.000200  loss: 1.0445 (1.3929)  loss_n_40: 0.2492 (0.3165)  loss_n_60: 0.2542 (0.3230)  loss_n_80: 0.2666 (0.3455)  loss_n_100: 0.2786 (0.3703)  triple_100: 0.0000 (0.0116)  triple_80: 0.0000 (0.0104)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0079)  time: 3.9201  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1110/1724]  eta: 0:40:06  lr: 0.000200  loss: 0.9515 (1.3891)  loss_n_40: 0.2396 (0.3159)  loss_n_60: 0.2338 (0.3222)  loss_n_80: 0.2401 (0.3445)  loss_n_100: 0.2497 (0.3692)  triple_100: 0.0000 (0.0115)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0079)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1120/1724]  eta: 0:39:27  lr: 0.000200  loss: 0.9853 (1.3856)  loss_n_40: 0.2423 (0.3152)  loss_n_60: 0.2248 (0.3214)  loss_n_80: 0.2383 (0.3437)  loss_n_100: 0.2432 (0.3682)  triple_100: 0.0000 (0.0114)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0079)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1130/1724]  eta: 0:38:47  lr: 0.000200  loss: 1.0448 (1.3837)  loss_n_40: 0.2589 (0.3150)  loss_n_60: 0.2474 (0.3210)  loss_n_80: 0.2604 (0.3432)  loss_n_100: 0.2690 (0.3676)  triple_100: 0.0000 (0.0113)  triple_80: 0.0000 (0.0102)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0078)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1140/1724]  eta: 0:38:08  lr: 0.000200  loss: 0.9994 (1.3799)  loss_n_40: 0.2428 (0.3143)  loss_n_60: 0.2414 (0.3202)  loss_n_80: 0.2566 (0.3423)  loss_n_100: 0.2757 (0.3666)  triple_100: 0.0000 (0.0112)  triple_80: 0.0000 (0.0101)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0078)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1150/1724]  eta: 0:37:29  lr: 0.000200  loss: 1.0581 (1.3778)  loss_n_40: 0.2431 (0.3139)  loss_n_60: 0.2536 (0.3198)  loss_n_80: 0.2726 (0.3418)  loss_n_100: 0.2891 (0.3661)  triple_100: 0.0000 (0.0111)  triple_80: 0.0000 (0.0100)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0077)  time: 3.9201  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1160/1724]  eta: 0:36:50  lr: 0.000200  loss: 1.1083 (1.3756)  loss_n_40: 0.2700 (0.3136)  loss_n_60: 0.2666 (0.3194)  loss_n_80: 0.2820 (0.3412)  loss_n_100: 0.2926 (0.3654)  triple_100: 0.0000 (0.0110)  triple_80: 0.0000 (0.0099)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0076)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1170/1724]  eta: 0:36:11  lr: 0.000200  loss: 1.0405 (1.3732)  loss_n_40: 0.2583 (0.3133)  loss_n_60: 0.2470 (0.3190)  loss_n_80: 0.2552 (0.3407)  loss_n_100: 0.2753 (0.3647)  triple_100: 0.0000 (0.0109)  triple_80: 0.0000 (0.0098)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0076)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1180/1724]  eta: 0:35:32  lr: 0.000200  loss: 1.0203 (1.3700)  loss_n_40: 0.2600 (0.3127)  loss_n_60: 0.2410 (0.3183)  loss_n_80: 0.2576 (0.3399)  loss_n_100: 0.2687 (0.3638)  triple_100: 0.0000 (0.0108)  triple_80: 0.0000 (0.0097)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0075)  time: 3.9201  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1190/1724]  eta: 0:34:52  lr: 0.000200  loss: 0.9969 (1.3665)  loss_n_40: 0.2412 (0.3121)  loss_n_60: 0.2402 (0.3175)  loss_n_80: 0.2517 (0.3390)  loss_n_100: 0.2589 (0.3629)  triple_100: 0.0000 (0.0108)  triple_80: 0.0000 (0.0096)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0075)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1200/1724]  eta: 0:34:13  lr: 0.000200  loss: 0.9236 (1.3635)  loss_n_40: 0.2263 (0.3116)  loss_n_60: 0.2234 (0.3169)  loss_n_80: 0.2341 (0.3383)  loss_n_100: 0.2449 (0.3620)  triple_100: 0.0000 (0.0107)  triple_80: 0.0000 (0.0096)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0074)  time: 3.9196  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1210/1724]  eta: 0:33:34  lr: 0.000200  loss: 0.9420 (1.3608)  loss_n_40: 0.2414 (0.3111)  loss_n_60: 0.2256 (0.3163)  loss_n_80: 0.2341 (0.3376)  loss_n_100: 0.2458 (0.3612)  triple_100: 0.0000 (0.0106)  triple_80: 0.0000 (0.0096)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0073)  time: 3.9212  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1220/1724]  eta: 0:32:55  lr: 0.000200  loss: 0.9680 (1.3576)  loss_n_40: 0.2501 (0.3106)  loss_n_60: 0.2350 (0.3156)  loss_n_80: 0.2296 (0.3368)  loss_n_100: 0.2458 (0.3603)  triple_100: 0.0000 (0.0105)  triple_80: 0.0000 (0.0095)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0073)  time: 3.9217  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1230/1724]  eta: 0:32:16  lr: 0.000200  loss: 1.0870 (1.3594)  loss_n_40: 0.2562 (0.3102)  loss_n_60: 0.2595 (0.3154)  loss_n_80: 0.2853 (0.3366)  loss_n_100: 0.2997 (0.3602)  triple_100: 0.0000 (0.0110)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0083)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1240/1724]  eta: 0:31:36  lr: 0.000200  loss: 1.4791 (1.3640)  loss_n_40: 0.2814 (0.3103)  loss_n_60: 0.3185 (0.3158)  loss_n_80: 0.3631 (0.3375)  loss_n_100: 0.4107 (0.3614)  triple_100: 0.0000 (0.0117)  triple_80: 0.0000 (0.0110)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0083)  time: 3.9191  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [1250/1724]  eta: 0:30:57  lr: 0.000200  loss: 1.6353 (1.3663)  loss_n_40: 0.3296 (0.3105)  loss_n_60: 0.3768 (0.3164)  loss_n_80: 0.4325 (0.3384)  loss_n_100: 0.4690 (0.3624)  triple_100: 0.0000 (0.0116)  triple_80: 0.0000 (0.0109)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0082)  time: 3.9183  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1260/1724]  eta: 0:30:18  lr: 0.000200  loss: 1.5814 (1.3676)  loss_n_40: 0.3313 (0.3107)  loss_n_60: 0.3625 (0.3167)  loss_n_80: 0.3961 (0.3388)  loss_n_100: 0.4514 (0.3630)  triple_100: 0.0000 (0.0115)  triple_80: 0.0000 (0.0108)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0081)  time: 3.9179  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1270/1724]  eta: 0:29:39  lr: 0.000200  loss: 1.4537 (1.3686)  loss_n_40: 0.3397 (0.3109)  loss_n_60: 0.3568 (0.3171)  loss_n_80: 0.3720 (0.3391)  loss_n_100: 0.4346 (0.3635)  triple_100: 0.0000 (0.0114)  triple_80: 0.0000 (0.0108)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0081)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1280/1724]  eta: 0:29:00  lr: 0.000200  loss: 1.4315 (1.3687)  loss_n_40: 0.3243 (0.3111)  loss_n_60: 0.3450 (0.3173)  loss_n_80: 0.3499 (0.3392)  loss_n_100: 0.3767 (0.3636)  triple_100: 0.0000 (0.0113)  triple_80: 0.0000 (0.0107)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0080)  time: 3.9176  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1290/1724]  eta: 0:28:20  lr: 0.000200  loss: 1.2864 (1.3677)  loss_n_40: 0.2954 (0.3109)  loss_n_60: 0.3169 (0.3172)  loss_n_80: 0.3197 (0.3389)  loss_n_100: 0.3374 (0.3633)  triple_100: 0.0000 (0.0112)  triple_80: 0.0000 (0.0106)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0080)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1300/1724]  eta: 0:27:41  lr: 0.000200  loss: 1.1861 (1.3667)  loss_n_40: 0.2848 (0.3109)  loss_n_60: 0.2867 (0.3170)  loss_n_80: 0.2872 (0.3387)  loss_n_100: 0.3250 (0.3630)  triple_100: 0.0000 (0.0111)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0079)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1310/1724]  eta: 0:27:02  lr: 0.000200  loss: 1.2461 (1.3666)  loss_n_40: 0.3034 (0.3111)  loss_n_60: 0.3048 (0.3171)  loss_n_80: 0.3165 (0.3387)  loss_n_100: 0.3366 (0.3629)  triple_100: 0.0000 (0.0111)  triple_80: 0.0000 (0.0104)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0078)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1320/1724]  eta: 0:26:23  lr: 0.000200  loss: 1.2157 (1.3650)  loss_n_40: 0.2930 (0.3109)  loss_n_60: 0.2918 (0.3168)  loss_n_80: 0.3127 (0.3383)  loss_n_100: 0.3182 (0.3625)  triple_100: 0.0000 (0.0110)  triple_80: 0.0000 (0.0104)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0078)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1330/1724]  eta: 0:25:44  lr: 0.000200  loss: 1.0835 (1.3638)  loss_n_40: 0.2706 (0.3108)  loss_n_60: 0.2581 (0.3167)  loss_n_80: 0.2801 (0.3380)  loss_n_100: 0.2848 (0.3621)  triple_100: 0.0000 (0.0109)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0077)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1340/1724]  eta: 0:25:04  lr: 0.000200  loss: 1.1282 (1.3622)  loss_n_40: 0.2701 (0.3106)  loss_n_60: 0.2714 (0.3163)  loss_n_80: 0.2801 (0.3376)  loss_n_100: 0.2944 (0.3617)  triple_100: 0.0000 (0.0108)  triple_80: 0.0000 (0.0102)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0077)  time: 3.9196  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1350/1724]  eta: 0:24:25  lr: 0.000200  loss: 1.0748 (1.3602)  loss_n_40: 0.2549 (0.3103)  loss_n_60: 0.2634 (0.3160)  loss_n_80: 0.2682 (0.3371)  loss_n_100: 0.2900 (0.3612)  triple_100: 0.0000 (0.0107)  triple_80: 0.0000 (0.0101)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0076)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1360/1724]  eta: 0:23:46  lr: 0.000200  loss: 1.0543 (1.3579)  loss_n_40: 0.2542 (0.3100)  loss_n_60: 0.2571 (0.3155)  loss_n_80: 0.2675 (0.3365)  loss_n_100: 0.2816 (0.3605)  triple_100: 0.0000 (0.0107)  triple_80: 0.0000 (0.0100)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0075)  time: 3.9203  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1370/1724]  eta: 0:23:07  lr: 0.000200  loss: 1.2010 (1.3584)  loss_n_40: 0.2905 (0.3100)  loss_n_60: 0.2811 (0.3156)  loss_n_80: 0.2907 (0.3367)  loss_n_100: 0.3069 (0.3607)  triple_100: 0.0000 (0.0106)  triple_80: 0.0000 (0.0100)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0075)  time: 3.9205  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1380/1724]  eta: 0:22:28  lr: 0.000200  loss: 1.4704 (1.3632)  loss_n_40: 0.3242 (0.3104)  loss_n_60: 0.3401 (0.3163)  loss_n_80: 0.3720 (0.3375)  loss_n_100: 0.4180 (0.3616)  triple_100: 0.0000 (0.0117)  triple_80: 0.0000 (0.0109)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0074)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1390/1724]  eta: 0:21:49  lr: 0.000200  loss: 1.6323 (1.3653)  loss_n_40: 0.3654 (0.3108)  loss_n_60: 0.4005 (0.3169)  loss_n_80: 0.4313 (0.3381)  loss_n_100: 0.4644 (0.3624)  triple_100: 0.0000 (0.0116)  triple_80: 0.0000 (0.0108)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0074)  time: 3.9196  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1400/1724]  eta: 0:21:09  lr: 0.000200  loss: 1.5923 (1.3665)  loss_n_40: 0.3594 (0.3112)  loss_n_60: 0.3878 (0.3173)  loss_n_80: 0.4120 (0.3386)  loss_n_100: 0.4431 (0.3626)  triple_100: 0.0000 (0.0116)  triple_80: 0.0000 (0.0107)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0073)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1410/1724]  eta: 0:20:30  lr: 0.000200  loss: 1.4053 (1.3666)  loss_n_40: 0.3151 (0.3113)  loss_n_60: 0.3472 (0.3174)  loss_n_80: 0.3528 (0.3387)  loss_n_100: 0.3691 (0.3627)  triple_100: 0.0000 (0.0115)  triple_80: 0.0000 (0.0106)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0073)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1420/1724]  eta: 0:19:51  lr: 0.000200  loss: 1.2475 (1.3654)  loss_n_40: 0.2788 (0.3110)  loss_n_60: 0.3032 (0.3173)  loss_n_80: 0.3255 (0.3385)  loss_n_100: 0.3399 (0.3624)  triple_100: 0.0000 (0.0114)  triple_80: 0.0000 (0.0106)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0072)  time: 3.9173  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1430/1724]  eta: 0:19:12  lr: 0.000200  loss: 1.1642 (1.3645)  loss_n_40: 0.2738 (0.3109)  loss_n_60: 0.2825 (0.3172)  loss_n_80: 0.2984 (0.3383)  loss_n_100: 0.3155 (0.3621)  triple_100: 0.0000 (0.0113)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0072)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1440/1724]  eta: 0:18:33  lr: 0.000200  loss: 1.2251 (1.3631)  loss_n_40: 0.2789 (0.3106)  loss_n_60: 0.2998 (0.3169)  loss_n_80: 0.3131 (0.3380)  loss_n_100: 0.3215 (0.3618)  triple_100: 0.0000 (0.0112)  triple_80: 0.0000 (0.0104)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0071)  time: 3.9198  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1450/1724]  eta: 0:17:53  lr: 0.000200  loss: 1.1816 (1.3620)  loss_n_40: 0.2752 (0.3104)  loss_n_60: 0.2918 (0.3167)  loss_n_80: 0.3082 (0.3376)  loss_n_100: 0.3051 (0.3613)  triple_100: 0.0000 (0.0113)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0071)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1460/1724]  eta: 0:17:14  lr: 0.000200  loss: 1.3136 (1.3652)  loss_n_40: 0.3119 (0.3105)  loss_n_60: 0.3085 (0.3173)  loss_n_80: 0.3501 (0.3387)  loss_n_100: 0.3796 (0.3629)  triple_100: 0.0000 (0.0113)  triple_80: 0.0000 (0.0106)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0070)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1470/1724]  eta: 0:16:35  lr: 0.000200  loss: 1.8247 (1.3690)  loss_n_40: 0.3195 (0.3107)  loss_n_60: 0.4137 (0.3180)  loss_n_80: 0.4725 (0.3399)  loss_n_100: 0.5648 (0.3647)  triple_100: 0.0000 (0.0112)  triple_80: 0.0000 (0.0106)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0070)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1480/1724]  eta: 0:15:56  lr: 0.000200  loss: 1.5473 (1.3697)  loss_n_40: 0.3210 (0.3108)  loss_n_60: 0.3466 (0.3182)  loss_n_80: 0.4091 (0.3402)  loss_n_100: 0.4536 (0.3651)  triple_100: 0.0000 (0.0111)  triple_80: 0.0000 (0.0105)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0069)  time: 3.9189  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [1490/1724]  eta: 0:15:17  lr: 0.000200  loss: 1.4035 (1.3695)  loss_n_40: 0.2999 (0.3107)  loss_n_60: 0.3251 (0.3181)  loss_n_80: 0.3601 (0.3402)  loss_n_100: 0.4064 (0.3653)  triple_100: 0.0000 (0.0111)  triple_80: 0.0000 (0.0104)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0069)  time: 3.9206  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1500/1724]  eta: 0:14:37  lr: 0.000200  loss: 1.2680 (1.3686)  loss_n_40: 0.2832 (0.3105)  loss_n_60: 0.2984 (0.3180)  loss_n_80: 0.3203 (0.3400)  loss_n_100: 0.3455 (0.3651)  triple_100: 0.0000 (0.0110)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0068)  time: 3.9199  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1510/1724]  eta: 0:13:58  lr: 0.000200  loss: 1.1741 (1.3669)  loss_n_40: 0.2698 (0.3102)  loss_n_60: 0.2848 (0.3176)  loss_n_80: 0.2974 (0.3396)  loss_n_100: 0.3262 (0.3647)  triple_100: 0.0000 (0.0109)  triple_80: 0.0000 (0.0103)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0068)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1520/1724]  eta: 0:13:19  lr: 0.000200  loss: 1.1443 (1.3662)  loss_n_40: 0.2698 (0.3102)  loss_n_60: 0.2747 (0.3175)  loss_n_80: 0.2849 (0.3394)  loss_n_100: 0.3049 (0.3645)  triple_100: 0.0000 (0.0108)  triple_80: 0.0000 (0.0102)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0067)  time: 3.9203  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1530/1724]  eta: 0:12:40  lr: 0.000200  loss: 1.1443 (1.3645)  loss_n_40: 0.2835 (0.3099)  loss_n_60: 0.2747 (0.3173)  loss_n_80: 0.2849 (0.3390)  loss_n_100: 0.2981 (0.3640)  triple_100: 0.0000 (0.0108)  triple_80: 0.0000 (0.0101)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0067)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1540/1724]  eta: 0:12:01  lr: 0.000200  loss: 1.0650 (1.3628)  loss_n_40: 0.2509 (0.3096)  loss_n_60: 0.2616 (0.3169)  loss_n_80: 0.2695 (0.3386)  loss_n_100: 0.2848 (0.3636)  triple_100: 0.0000 (0.0107)  triple_80: 0.0000 (0.0101)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0067)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1550/1724]  eta: 0:11:21  lr: 0.000200  loss: 1.0123 (1.3608)  loss_n_40: 0.2452 (0.3093)  loss_n_60: 0.2422 (0.3165)  loss_n_80: 0.2523 (0.3381)  loss_n_100: 0.2774 (0.3630)  triple_100: 0.0000 (0.0106)  triple_80: 0.0000 (0.0100)  triple_60: 0.0000 (0.0066)  triple_40: 0.0000 (0.0066)  time: 3.9191  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1560/1724]  eta: 0:10:42  lr: 0.000200  loss: 1.0138 (1.3593)  loss_n_40: 0.2402 (0.3090)  loss_n_60: 0.2449 (0.3162)  loss_n_80: 0.2563 (0.3377)  loss_n_100: 0.2819 (0.3626)  triple_100: 0.0000 (0.0106)  triple_80: 0.0000 (0.0100)  triple_60: 0.0000 (0.0066)  triple_40: 0.0000 (0.0066)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1570/1724]  eta: 0:10:03  lr: 0.000200  loss: 1.0013 (1.3568)  loss_n_40: 0.2542 (0.3086)  loss_n_60: 0.2420 (0.3157)  loss_n_80: 0.2482 (0.3371)  loss_n_100: 0.2669 (0.3619)  triple_100: 0.0000 (0.0105)  triple_80: 0.0000 (0.0099)  triple_60: 0.0000 (0.0065)  triple_40: 0.0000 (0.0065)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1580/1724]  eta: 0:09:24  lr: 0.000200  loss: 0.9798 (1.3547)  loss_n_40: 0.2461 (0.3082)  loss_n_60: 0.2396 (0.3153)  loss_n_80: 0.2403 (0.3366)  loss_n_100: 0.2522 (0.3613)  triple_100: 0.0000 (0.0104)  triple_80: 0.0000 (0.0098)  triple_60: 0.0000 (0.0065)  triple_40: 0.0000 (0.0065)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1590/1724]  eta: 0:08:45  lr: 0.000200  loss: 1.0065 (1.3556)  loss_n_40: 0.2485 (0.3080)  loss_n_60: 0.2457 (0.3151)  loss_n_80: 0.2528 (0.3364)  loss_n_100: 0.2759 (0.3612)  triple_100: 0.0000 (0.0107)  triple_80: 0.0000 (0.0098)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0075)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1600/1724]  eta: 0:08:05  lr: 0.000200  loss: 2.6178 (1.3732)  loss_n_40: 0.3844 (0.3096)  loss_n_60: 0.4771 (0.3179)  loss_n_80: 0.6089 (0.3403)  loss_n_100: 0.7246 (0.3660)  triple_100: 0.0000 (0.0138)  triple_80: 0.0000 (0.0113)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0075)  time: 3.9182  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1610/1724]  eta: 0:07:26  lr: 0.000200  loss: 3.5988 (1.3903)  loss_n_40: 0.5977 (0.3119)  loss_n_60: 0.7753 (0.3209)  loss_n_80: 0.9732 (0.3441)  loss_n_100: 1.1271 (0.3705)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0112)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0095)  time: 3.9167  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1620/1724]  eta: 0:06:47  lr: 0.000200  loss: 3.4149 (1.4030)  loss_n_40: 0.6491 (0.3142)  loss_n_60: 0.7661 (0.3237)  loss_n_80: 0.8901 (0.3473)  loss_n_100: 0.9730 (0.3743)  triple_100: 0.0000 (0.0138)  triple_80: 0.0000 (0.0117)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0095)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1630/1724]  eta: 0:06:08  lr: 0.000200  loss: 3.1427 (1.4130)  loss_n_40: 0.6489 (0.3163)  loss_n_60: 0.7706 (0.3265)  loss_n_80: 0.7773 (0.3499)  loss_n_100: 0.8740 (0.3771)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0117)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0094)  time: 3.9177  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1640/1724]  eta: 0:05:29  lr: 0.000200  loss: 2.8050 (1.4200)  loss_n_40: 0.5696 (0.3176)  loss_n_60: 0.7089 (0.3285)  loss_n_80: 0.7271 (0.3517)  loss_n_100: 0.7923 (0.3792)  triple_100: 0.0000 (0.0136)  triple_80: 0.0000 (0.0116)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0094)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1650/1724]  eta: 0:04:50  lr: 0.000200  loss: 2.3404 (1.4251)  loss_n_40: 0.4948 (0.3187)  loss_n_60: 0.5903 (0.3300)  loss_n_80: 0.6009 (0.3531)  loss_n_100: 0.6588 (0.3808)  triple_100: 0.0000 (0.0136)  triple_80: 0.0000 (0.0115)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0093)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1660/1724]  eta: 0:04:10  lr: 0.000200  loss: 2.1681 (1.4289)  loss_n_40: 0.4718 (0.3196)  loss_n_60: 0.5410 (0.3310)  loss_n_80: 0.5620 (0.3540)  loss_n_100: 0.6088 (0.3819)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0114)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0093)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1670/1724]  eta: 0:03:31  lr: 0.000200  loss: 1.8099 (1.4305)  loss_n_40: 0.4301 (0.3200)  loss_n_60: 0.4313 (0.3315)  loss_n_80: 0.4411 (0.3544)  loss_n_100: 0.4970 (0.3824)  triple_100: 0.0000 (0.0134)  triple_80: 0.0000 (0.0115)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0092)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1680/1724]  eta: 0:02:52  lr: 0.000200  loss: 1.5845 (1.4316)  loss_n_40: 0.3890 (0.3205)  loss_n_60: 0.3901 (0.3318)  loss_n_80: 0.3881 (0.3546)  loss_n_100: 0.4391 (0.3827)  triple_100: 0.0000 (0.0133)  triple_80: 0.0000 (0.0114)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0092)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1690/1724]  eta: 0:02:13  lr: 0.000200  loss: 1.5600 (1.4326)  loss_n_40: 0.3862 (0.3209)  loss_n_60: 0.3810 (0.3321)  loss_n_80: 0.3791 (0.3549)  loss_n_100: 0.4166 (0.3830)  triple_100: 0.0000 (0.0132)  triple_80: 0.0000 (0.0113)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0091)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1700/1724]  eta: 0:01:34  lr: 0.000200  loss: 1.4013 (1.4321)  loss_n_40: 0.3249 (0.3209)  loss_n_60: 0.3283 (0.3321)  loss_n_80: 0.3515 (0.3548)  loss_n_100: 0.3973 (0.3830)  triple_100: 0.0000 (0.0132)  triple_80: 0.0000 (0.0113)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0091)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:18]  [1710/1724]  eta: 0:00:54  lr: 0.000200  loss: 1.4500 (1.4336)  loss_n_40: 0.3363 (0.3211)  loss_n_60: 0.3380 (0.3322)  loss_n_80: 0.3565 (0.3548)  loss_n_100: 0.3793 (0.3831)  triple_100: 0.0000 (0.0134)  triple_80: 0.0000 (0.0113)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0093)  time: 3.9191  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18]  [1720/1724]  eta: 0:00:15  lr: 0.000200  loss: 1.7753 (1.4371)  loss_n_40: 0.3946 (0.3218)  loss_n_60: 0.4030 (0.3329)  loss_n_80: 0.4099 (0.3557)  loss_n_100: 0.4895 (0.3843)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0115)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0092)  time: 3.9190  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:18]  [1723/1724]  eta: 0:00:03  lr: 0.000200  loss: 1.8176 (1.4379)  loss_n_40: 0.3971 (0.3220)  loss_n_60: 0.4072 (0.3330)  loss_n_80: 0.4505 (0.3559)  loss_n_100: 0.5033 (0.3845)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0115)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0092)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:18] Total time: 1:52:36 (3.9193 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 1.8176 (1.4379)  loss_n_40: 0.3971 (0.3220)  loss_n_60: 0.4072 (0.3330)  loss_n_80: 0.4505 (0.3559)  loss_n_100: 0.5033 (0.3845)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0115)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0092)\n",
      "Valid: [epoch:18]  [  0/845]  eta: 0:10:20  loss: 1.8451 (1.8451)  loss_n_40: 0.5073 (0.5073)  loss_n_60: 0.4450 (0.4450)  loss_n_80: 0.4444 (0.4444)  loss_n_100: 0.4484 (0.4484)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7340  data: 0.4012  max mem: 46473\n",
      "Valid: [epoch:18]  [ 10/845]  eta: 0:05:09  loss: 1.8451 (1.9227)  loss_n_40: 0.4841 (0.4811)  loss_n_60: 0.4455 (0.4562)  loss_n_80: 0.4444 (0.4788)  loss_n_100: 0.4709 (0.5066)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3708  data: 0.0366  max mem: 46473\n",
      "Valid: [epoch:18]  [ 20/845]  eta: 0:04:51  loss: 1.8066 (1.8871)  loss_n_40: 0.4209 (0.4513)  loss_n_60: 0.4311 (0.4456)  loss_n_80: 0.4424 (0.4795)  loss_n_100: 0.4729 (0.5106)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [ 30/845]  eta: 0:04:42  loss: 1.7681 (1.9163)  loss_n_40: 0.4050 (0.4570)  loss_n_60: 0.4205 (0.4504)  loss_n_80: 0.4591 (0.4830)  loss_n_100: 0.4959 (0.5259)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [ 40/845]  eta: 0:04:36  loss: 1.7403 (1.9289)  loss_n_40: 0.4154 (0.4534)  loss_n_60: 0.4110 (0.4538)  loss_n_80: 0.4591 (0.4886)  loss_n_100: 0.4923 (0.5332)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3337  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [ 50/845]  eta: 0:04:31  loss: 1.7083 (1.9179)  loss_n_40: 0.4287 (0.4548)  loss_n_60: 0.4010 (0.4517)  loss_n_80: 0.4269 (0.4834)  loss_n_100: 0.4637 (0.5279)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [ 60/845]  eta: 0:04:27  loss: 1.7575 (1.9766)  loss_n_40: 0.4646 (0.4607)  loss_n_60: 0.4145 (0.4643)  loss_n_80: 0.4495 (0.5029)  loss_n_100: 0.4680 (0.5488)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0002  max mem: 46473\n",
      "Valid: [epoch:18]  [ 70/845]  eta: 0:04:23  loss: 2.1177 (1.9762)  loss_n_40: 0.4222 (0.4579)  loss_n_60: 0.5036 (0.4643)  loss_n_80: 0.5085 (0.5031)  loss_n_100: 0.5972 (0.5509)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [ 80/845]  eta: 0:04:19  loss: 2.1011 (2.0117)  loss_n_40: 0.4318 (0.4627)  loss_n_60: 0.4806 (0.4721)  loss_n_80: 0.5262 (0.5141)  loss_n_100: 0.5969 (0.5628)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [ 90/845]  eta: 0:04:15  loss: 2.1367 (2.0254)  loss_n_40: 0.5035 (0.4710)  loss_n_60: 0.4841 (0.4754)  loss_n_80: 0.5364 (0.5154)  loss_n_100: 0.5904 (0.5636)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [100/845]  eta: 0:04:11  loss: 2.0676 (2.0232)  loss_n_40: 0.5025 (0.4726)  loss_n_60: 0.4783 (0.4747)  loss_n_80: 0.5034 (0.5143)  loss_n_100: 0.5604 (0.5616)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [110/845]  eta: 0:04:08  loss: 1.7384 (2.0094)  loss_n_40: 0.4573 (0.4707)  loss_n_60: 0.4049 (0.4714)  loss_n_80: 0.4654 (0.5100)  loss_n_100: 0.4664 (0.5573)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [120/845]  eta: 0:04:04  loss: 1.8494 (1.9969)  loss_n_40: 0.4153 (0.4674)  loss_n_60: 0.3974 (0.4683)  loss_n_80: 0.4649 (0.5072)  loss_n_100: 0.5117 (0.5540)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [130/845]  eta: 0:04:01  loss: 1.9675 (2.0273)  loss_n_40: 0.4172 (0.4704)  loss_n_60: 0.4577 (0.4757)  loss_n_80: 0.5404 (0.5162)  loss_n_100: 0.5687 (0.5650)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [140/845]  eta: 0:03:57  loss: 1.9675 (2.0141)  loss_n_40: 0.4431 (0.4678)  loss_n_60: 0.4577 (0.4726)  loss_n_80: 0.4980 (0.5132)  loss_n_100: 0.5577 (0.5605)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [150/845]  eta: 0:03:54  loss: 1.6472 (2.0061)  loss_n_40: 0.4316 (0.4680)  loss_n_60: 0.3994 (0.4703)  loss_n_80: 0.4452 (0.5104)  loss_n_100: 0.4397 (0.5574)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [160/845]  eta: 0:03:50  loss: 1.7882 (1.9986)  loss_n_40: 0.4498 (0.4696)  loss_n_60: 0.4062 (0.4687)  loss_n_80: 0.4171 (0.5068)  loss_n_100: 0.4489 (0.5535)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [170/845]  eta: 0:03:47  loss: 1.6812 (1.9865)  loss_n_40: 0.4468 (0.4698)  loss_n_60: 0.3960 (0.4664)  loss_n_80: 0.4165 (0.5029)  loss_n_100: 0.4293 (0.5474)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [180/845]  eta: 0:03:43  loss: 1.7322 (1.9851)  loss_n_40: 0.4263 (0.4692)  loss_n_60: 0.3907 (0.4661)  loss_n_80: 0.4480 (0.5026)  loss_n_100: 0.4351 (0.5472)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [190/845]  eta: 0:03:40  loss: 1.9299 (1.9895)  loss_n_40: 0.4564 (0.4705)  loss_n_60: 0.4666 (0.4680)  loss_n_80: 0.4726 (0.5034)  loss_n_100: 0.5135 (0.5476)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [200/845]  eta: 0:03:36  loss: 2.0009 (1.9876)  loss_n_40: 0.4580 (0.4693)  loss_n_60: 0.4805 (0.4675)  loss_n_80: 0.5160 (0.5033)  loss_n_100: 0.5583 (0.5475)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [210/845]  eta: 0:03:33  loss: 1.9583 (1.9851)  loss_n_40: 0.4284 (0.4693)  loss_n_60: 0.4461 (0.4668)  loss_n_80: 0.4966 (0.5023)  loss_n_100: 0.5538 (0.5468)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [220/845]  eta: 0:03:30  loss: 1.9578 (1.9831)  loss_n_40: 0.4690 (0.4692)  loss_n_60: 0.4415 (0.4655)  loss_n_80: 0.4870 (0.5007)  loss_n_100: 0.4995 (0.5449)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:18]  [230/845]  eta: 0:03:26  loss: 1.7076 (1.9780)  loss_n_40: 0.4383 (0.4688)  loss_n_60: 0.4056 (0.4645)  loss_n_80: 0.4378 (0.4997)  loss_n_100: 0.4466 (0.5425)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [240/845]  eta: 0:03:23  loss: 1.7648 (1.9727)  loss_n_40: 0.4158 (0.4674)  loss_n_60: 0.4017 (0.4632)  loss_n_80: 0.4638 (0.4984)  loss_n_100: 0.4756 (0.5412)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [250/845]  eta: 0:03:19  loss: 1.8209 (1.9789)  loss_n_40: 0.4324 (0.4683)  loss_n_60: 0.4374 (0.4651)  loss_n_80: 0.4948 (0.5001)  loss_n_100: 0.5126 (0.5429)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [260/845]  eta: 0:03:16  loss: 1.9102 (1.9878)  loss_n_40: 0.4787 (0.4692)  loss_n_60: 0.4444 (0.4668)  loss_n_80: 0.4948 (0.5028)  loss_n_100: 0.5277 (0.5467)  triple_100: 0.0000 (0.0023)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [270/845]  eta: 0:03:13  loss: 1.9619 (1.9855)  loss_n_40: 0.4732 (0.4686)  loss_n_60: 0.4444 (0.4663)  loss_n_80: 0.4771 (0.5026)  loss_n_100: 0.5277 (0.5458)  triple_100: 0.0000 (0.0022)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [280/845]  eta: 0:03:09  loss: 1.7810 (1.9894)  loss_n_40: 0.4162 (0.4699)  loss_n_60: 0.4394 (0.4670)  loss_n_80: 0.4732 (0.5035)  loss_n_100: 0.4949 (0.5468)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [290/845]  eta: 0:03:06  loss: 2.0329 (1.9949)  loss_n_40: 0.4891 (0.4708)  loss_n_60: 0.4711 (0.4683)  loss_n_80: 0.4912 (0.5051)  loss_n_100: 0.5652 (0.5486)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [300/845]  eta: 0:03:02  loss: 2.0329 (1.9948)  loss_n_40: 0.4647 (0.4706)  loss_n_60: 0.4711 (0.4683)  loss_n_80: 0.5191 (0.5052)  loss_n_100: 0.5652 (0.5486)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [310/845]  eta: 0:02:59  loss: 1.9023 (1.9921)  loss_n_40: 0.4647 (0.4715)  loss_n_60: 0.4368 (0.4678)  loss_n_80: 0.4820 (0.5041)  loss_n_100: 0.5496 (0.5467)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [320/845]  eta: 0:02:56  loss: 1.7906 (1.9929)  loss_n_40: 0.4654 (0.4719)  loss_n_60: 0.4245 (0.4676)  loss_n_80: 0.4526 (0.5039)  loss_n_100: 0.4541 (0.5465)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [330/845]  eta: 0:02:52  loss: 1.8797 (1.9906)  loss_n_40: 0.4482 (0.4720)  loss_n_60: 0.4308 (0.4669)  loss_n_80: 0.4790 (0.5031)  loss_n_100: 0.4933 (0.5457)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [340/845]  eta: 0:02:49  loss: 2.0570 (1.9940)  loss_n_40: 0.4765 (0.4724)  loss_n_60: 0.4721 (0.4680)  loss_n_80: 0.4953 (0.5039)  loss_n_100: 0.5564 (0.5470)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3339  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [350/845]  eta: 0:02:46  loss: 1.8387 (1.9895)  loss_n_40: 0.4426 (0.4714)  loss_n_60: 0.4238 (0.4670)  loss_n_80: 0.4890 (0.5027)  loss_n_100: 0.5285 (0.5457)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [360/845]  eta: 0:02:42  loss: 1.7174 (1.9871)  loss_n_40: 0.4120 (0.4710)  loss_n_60: 0.4007 (0.4661)  loss_n_80: 0.4486 (0.5022)  loss_n_100: 0.4758 (0.5452)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [370/845]  eta: 0:02:39  loss: 1.6960 (1.9833)  loss_n_40: 0.4120 (0.4704)  loss_n_60: 0.3864 (0.4652)  loss_n_80: 0.4285 (0.5013)  loss_n_100: 0.4684 (0.5439)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [380/845]  eta: 0:02:35  loss: 1.8888 (1.9877)  loss_n_40: 0.4819 (0.4716)  loss_n_60: 0.4543 (0.4664)  loss_n_80: 0.4571 (0.5021)  loss_n_100: 0.5085 (0.5451)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [390/845]  eta: 0:02:32  loss: 2.1124 (1.9914)  loss_n_40: 0.4850 (0.4716)  loss_n_60: 0.4883 (0.4672)  loss_n_80: 0.5189 (0.5034)  loss_n_100: 0.5671 (0.5468)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [400/845]  eta: 0:02:29  loss: 2.0459 (1.9988)  loss_n_40: 0.4776 (0.4728)  loss_n_60: 0.4801 (0.4689)  loss_n_80: 0.5137 (0.5056)  loss_n_100: 0.5671 (0.5492)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [410/845]  eta: 0:02:25  loss: 2.0636 (2.0033)  loss_n_40: 0.4832 (0.4737)  loss_n_60: 0.5023 (0.4701)  loss_n_80: 0.5292 (0.5069)  loss_n_100: 0.5941 (0.5505)  triple_100: 0.0000 (0.0023)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [420/845]  eta: 0:02:22  loss: 2.0636 (2.0099)  loss_n_40: 0.4869 (0.4741)  loss_n_60: 0.4983 (0.4717)  loss_n_80: 0.5292 (0.5088)  loss_n_100: 0.5982 (0.5531)  triple_100: 0.0000 (0.0023)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [430/845]  eta: 0:02:19  loss: 1.9909 (2.0067)  loss_n_40: 0.4799 (0.4740)  loss_n_60: 0.4662 (0.4707)  loss_n_80: 0.5075 (0.5078)  loss_n_100: 0.5651 (0.5520)  triple_100: 0.0000 (0.0022)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [440/845]  eta: 0:02:15  loss: 1.9301 (2.0089)  loss_n_40: 0.4485 (0.4740)  loss_n_60: 0.4451 (0.4715)  loss_n_80: 0.4956 (0.5086)  loss_n_100: 0.5356 (0.5527)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [450/845]  eta: 0:02:12  loss: 2.0070 (2.0071)  loss_n_40: 0.4268 (0.4735)  loss_n_60: 0.4729 (0.4710)  loss_n_80: 0.4864 (0.5080)  loss_n_100: 0.5284 (0.5525)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [460/845]  eta: 0:02:09  loss: 1.8382 (2.0038)  loss_n_40: 0.4336 (0.4735)  loss_n_60: 0.4404 (0.4702)  loss_n_80: 0.4313 (0.5068)  loss_n_100: 0.4559 (0.5512)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [470/845]  eta: 0:02:05  loss: 1.9429 (2.0032)  loss_n_40: 0.4430 (0.4733)  loss_n_60: 0.4529 (0.4701)  loss_n_80: 0.4638 (0.5066)  loss_n_100: 0.5206 (0.5512)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:18]  [480/845]  eta: 0:02:02  loss: 1.9429 (2.0021)  loss_n_40: 0.4509 (0.4729)  loss_n_60: 0.4576 (0.4697)  loss_n_80: 0.5014 (0.5067)  loss_n_100: 0.5237 (0.5508)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [490/845]  eta: 0:01:58  loss: 1.7842 (1.9996)  loss_n_40: 0.4284 (0.4724)  loss_n_60: 0.4173 (0.4691)  loss_n_80: 0.4763 (0.5061)  loss_n_100: 0.4769 (0.5501)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [500/845]  eta: 0:01:55  loss: 1.7395 (1.9991)  loss_n_40: 0.4205 (0.4720)  loss_n_60: 0.4110 (0.4689)  loss_n_80: 0.4555 (0.5062)  loss_n_100: 0.4897 (0.5500)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [510/845]  eta: 0:01:52  loss: 1.8967 (1.9996)  loss_n_40: 0.4126 (0.4719)  loss_n_60: 0.4450 (0.4690)  loss_n_80: 0.4998 (0.5064)  loss_n_100: 0.5174 (0.5504)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [520/845]  eta: 0:01:48  loss: 1.8967 (1.9983)  loss_n_40: 0.4126 (0.4713)  loss_n_60: 0.4450 (0.4687)  loss_n_80: 0.5045 (0.5062)  loss_n_100: 0.5266 (0.5502)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [530/845]  eta: 0:01:45  loss: 1.9244 (2.0001)  loss_n_40: 0.4361 (0.4713)  loss_n_60: 0.4707 (0.4693)  loss_n_80: 0.4963 (0.5068)  loss_n_100: 0.5261 (0.5510)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [540/845]  eta: 0:01:42  loss: 1.7690 (1.9941)  loss_n_40: 0.4279 (0.4703)  loss_n_60: 0.4368 (0.4680)  loss_n_80: 0.4711 (0.5054)  loss_n_100: 0.4624 (0.5486)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [550/845]  eta: 0:01:38  loss: 1.7283 (1.9948)  loss_n_40: 0.4207 (0.4697)  loss_n_60: 0.3969 (0.4684)  loss_n_80: 0.4486 (0.5058)  loss_n_100: 0.4358 (0.5492)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [560/845]  eta: 0:01:35  loss: 1.9481 (1.9940)  loss_n_40: 0.4306 (0.4700)  loss_n_60: 0.4533 (0.4681)  loss_n_80: 0.4995 (0.5054)  loss_n_100: 0.5440 (0.5488)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [570/845]  eta: 0:01:32  loss: 2.0187 (1.9945)  loss_n_40: 0.4783 (0.4698)  loss_n_60: 0.4670 (0.4681)  loss_n_80: 0.5063 (0.5058)  loss_n_100: 0.5382 (0.5491)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [580/845]  eta: 0:01:28  loss: 1.6508 (1.9892)  loss_n_40: 0.4149 (0.4691)  loss_n_60: 0.3870 (0.4668)  loss_n_80: 0.4209 (0.5043)  loss_n_100: 0.4596 (0.5474)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [590/845]  eta: 0:01:25  loss: 1.6165 (1.9898)  loss_n_40: 0.4269 (0.4693)  loss_n_60: 0.3870 (0.4671)  loss_n_80: 0.4117 (0.5044)  loss_n_100: 0.4329 (0.5473)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [600/845]  eta: 0:01:22  loss: 1.8084 (1.9870)  loss_n_40: 0.4400 (0.4689)  loss_n_60: 0.4331 (0.4663)  loss_n_80: 0.4438 (0.5037)  loss_n_100: 0.4581 (0.5465)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [610/845]  eta: 0:01:18  loss: 1.8964 (1.9847)  loss_n_40: 0.4365 (0.4686)  loss_n_60: 0.4331 (0.4658)  loss_n_80: 0.4750 (0.5031)  loss_n_100: 0.4646 (0.5457)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [620/845]  eta: 0:01:15  loss: 1.6469 (1.9804)  loss_n_40: 0.4287 (0.4683)  loss_n_60: 0.3884 (0.4650)  loss_n_80: 0.4104 (0.5016)  loss_n_100: 0.4192 (0.5439)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [630/845]  eta: 0:01:12  loss: 1.6469 (1.9796)  loss_n_40: 0.4303 (0.4682)  loss_n_60: 0.3959 (0.4648)  loss_n_80: 0.4012 (0.5014)  loss_n_100: 0.4329 (0.5437)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [640/845]  eta: 0:01:08  loss: 1.8895 (1.9798)  loss_n_40: 0.4372 (0.4676)  loss_n_60: 0.4369 (0.4648)  loss_n_80: 0.4891 (0.5017)  loss_n_100: 0.5107 (0.5443)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [650/845]  eta: 0:01:05  loss: 1.8260 (1.9779)  loss_n_40: 0.4367 (0.4675)  loss_n_60: 0.4120 (0.4643)  loss_n_80: 0.4462 (0.5012)  loss_n_100: 0.4888 (0.5435)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [660/845]  eta: 0:01:01  loss: 1.7174 (1.9773)  loss_n_40: 0.4187 (0.4673)  loss_n_60: 0.4012 (0.4642)  loss_n_80: 0.4445 (0.5010)  loss_n_100: 0.4512 (0.5433)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [670/845]  eta: 0:00:58  loss: 1.8972 (1.9784)  loss_n_40: 0.4187 (0.4676)  loss_n_60: 0.4471 (0.4647)  loss_n_80: 0.4625 (0.5012)  loss_n_100: 0.5430 (0.5436)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [680/845]  eta: 0:00:55  loss: 1.9862 (1.9794)  loss_n_40: 0.4674 (0.4678)  loss_n_60: 0.4811 (0.4649)  loss_n_80: 0.5072 (0.5014)  loss_n_100: 0.5430 (0.5439)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [690/845]  eta: 0:00:51  loss: 1.9862 (1.9795)  loss_n_40: 0.4807 (0.4683)  loss_n_60: 0.4624 (0.4650)  loss_n_80: 0.5072 (0.5012)  loss_n_100: 0.5360 (0.5437)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [700/845]  eta: 0:00:48  loss: 2.0198 (1.9793)  loss_n_40: 0.5064 (0.4688)  loss_n_60: 0.4674 (0.4651)  loss_n_80: 0.5010 (0.5009)  loss_n_100: 0.5291 (0.5431)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [710/845]  eta: 0:00:45  loss: 1.9065 (1.9785)  loss_n_40: 0.4490 (0.4687)  loss_n_60: 0.4314 (0.4650)  loss_n_80: 0.4861 (0.5007)  loss_n_100: 0.4650 (0.5427)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [720/845]  eta: 0:00:41  loss: 1.6988 (1.9753)  loss_n_40: 0.4159 (0.4681)  loss_n_60: 0.4049 (0.4642)  loss_n_80: 0.4117 (0.4999)  loss_n_100: 0.4426 (0.5418)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:18]  [730/845]  eta: 0:00:38  loss: 1.8257 (1.9774)  loss_n_40: 0.4359 (0.4682)  loss_n_60: 0.4262 (0.4646)  loss_n_80: 0.4918 (0.5002)  loss_n_100: 0.5270 (0.5423)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [740/845]  eta: 0:00:35  loss: 2.1014 (1.9785)  loss_n_40: 0.4767 (0.4689)  loss_n_60: 0.4757 (0.4647)  loss_n_80: 0.5172 (0.5005)  loss_n_100: 0.5512 (0.5423)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [750/845]  eta: 0:00:31  loss: 1.9909 (1.9782)  loss_n_40: 0.4579 (0.4686)  loss_n_60: 0.4483 (0.4647)  loss_n_80: 0.5005 (0.5005)  loss_n_100: 0.5319 (0.5424)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [760/845]  eta: 0:00:28  loss: 1.7277 (1.9804)  loss_n_40: 0.4212 (0.4687)  loss_n_60: 0.4281 (0.4652)  loss_n_80: 0.4562 (0.5012)  loss_n_100: 0.4818 (0.5433)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [770/845]  eta: 0:00:25  loss: 1.8757 (1.9817)  loss_n_40: 0.4151 (0.4686)  loss_n_60: 0.4645 (0.4655)  loss_n_80: 0.4695 (0.5016)  loss_n_100: 0.5101 (0.5440)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [780/845]  eta: 0:00:21  loss: 1.8757 (1.9811)  loss_n_40: 0.4139 (0.4687)  loss_n_60: 0.4575 (0.4654)  loss_n_80: 0.4845 (0.5014)  loss_n_100: 0.5026 (0.5436)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [790/845]  eta: 0:00:18  loss: 1.9851 (1.9821)  loss_n_40: 0.4365 (0.4688)  loss_n_60: 0.4448 (0.4656)  loss_n_80: 0.4848 (0.5016)  loss_n_100: 0.5600 (0.5441)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [800/845]  eta: 0:00:15  loss: 2.0644 (1.9827)  loss_n_40: 0.4669 (0.4691)  loss_n_60: 0.4893 (0.4657)  loss_n_80: 0.4848 (0.5016)  loss_n_100: 0.5674 (0.5440)  triple_100: 0.0000 (0.0023)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [810/845]  eta: 0:00:11  loss: 1.8221 (1.9810)  loss_n_40: 0.4316 (0.4689)  loss_n_60: 0.4213 (0.4653)  loss_n_80: 0.4659 (0.5011)  loss_n_100: 0.4762 (0.5433)  triple_100: 0.0000 (0.0023)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [820/845]  eta: 0:00:08  loss: 1.8221 (1.9812)  loss_n_40: 0.4316 (0.4689)  loss_n_60: 0.4213 (0.4655)  loss_n_80: 0.4688 (0.5012)  loss_n_100: 0.4660 (0.5434)  triple_100: 0.0000 (0.0023)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [830/845]  eta: 0:00:05  loss: 1.6989 (1.9810)  loss_n_40: 0.4163 (0.4686)  loss_n_60: 0.4103 (0.4654)  loss_n_80: 0.4461 (0.5012)  loss_n_100: 0.4678 (0.5435)  triple_100: 0.0000 (0.0022)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [840/845]  eta: 0:00:01  loss: 1.8039 (1.9805)  loss_n_40: 0.4307 (0.4687)  loss_n_60: 0.4265 (0.4653)  loss_n_80: 0.4593 (0.5012)  loss_n_100: 0.4678 (0.5430)  triple_100: 0.0000 (0.0022)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18]  [844/845]  eta: 0:00:00  loss: 1.6848 (1.9795)  loss_n_40: 0.4307 (0.4684)  loss_n_60: 0.3989 (0.4651)  loss_n_80: 0.4461 (0.5010)  loss_n_100: 0.4576 (0.5428)  triple_100: 0.0000 (0.0022)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:18] Total time: 0:04:43 (0.3350 s / it)\n",
      "Averaged stats: loss: 1.6848 (1.9795)  loss_n_40: 0.4307 (0.4684)  loss_n_60: 0.3989 (0.4651)  loss_n_80: 0.4461 (0.5010)  loss_n_100: 0.4576 (0.5428)  triple_100: 0.0000 (0.0022)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_18_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.543%\n",
      "Min loss_n_100: 0.325\n",
      "Best Epoch: 17.000\n",
      "Train: [epoch:19]  [   0/1724]  eta: 2:00:04  lr: 0.000200  loss: 2.1523 (2.1523)  loss_n_40: 0.5353 (0.5353)  loss_n_60: 0.5280 (0.5280)  loss_n_80: 0.5382 (0.5382)  loss_n_100: 0.5507 (0.5507)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1787  data: 0.4262  max mem: 46473\n",
      "Train: [epoch:19]  [  10/1724]  eta: 1:52:37  lr: 0.000200  loss: 1.8581 (1.9348)  loss_n_40: 0.4952 (0.4799)  loss_n_60: 0.4377 (0.4541)  loss_n_80: 0.4714 (0.4816)  loss_n_100: 0.5122 (0.5193)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9425  data: 0.0389  max mem: 46473\n",
      "Train: [epoch:19]  [  20/1724]  eta: 1:51:36  lr: 0.000200  loss: 1.8670 (1.9198)  loss_n_40: 0.4540 (0.4659)  loss_n_60: 0.4377 (0.4505)  loss_n_80: 0.4714 (0.4825)  loss_n_100: 0.5153 (0.5209)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9174  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [  30/1724]  eta: 1:50:50  lr: 0.000200  loss: 1.7503 (1.8217)  loss_n_40: 0.4311 (0.4469)  loss_n_60: 0.4194 (0.4297)  loss_n_80: 0.4251 (0.4563)  loss_n_100: 0.4266 (0.4889)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9164  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [  40/1724]  eta: 1:50:08  lr: 0.000200  loss: 1.5245 (1.7375)  loss_n_40: 0.3867 (0.4307)  loss_n_60: 0.3664 (0.4102)  loss_n_80: 0.3796 (0.4333)  loss_n_100: 0.4022 (0.4633)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9183  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [  50/1724]  eta: 1:49:26  lr: 0.000200  loss: 1.5440 (1.7192)  loss_n_40: 0.3867 (0.4223)  loss_n_60: 0.3664 (0.4040)  loss_n_80: 0.3833 (0.4287)  loss_n_100: 0.4012 (0.4585)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0057)  time: 3.9185  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [  60/1724]  eta: 1:48:46  lr: 0.000200  loss: 1.5182 (1.6768)  loss_n_40: 0.3569 (0.4104)  loss_n_60: 0.3482 (0.3933)  loss_n_80: 0.3923 (0.4184)  loss_n_100: 0.4175 (0.4500)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0047)  time: 3.9186  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [  70/1724]  eta: 1:48:06  lr: 0.000200  loss: 1.4567 (1.6477)  loss_n_40: 0.3569 (0.4053)  loss_n_60: 0.3447 (0.3866)  loss_n_80: 0.3625 (0.4107)  loss_n_100: 0.3867 (0.4410)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0041)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [  80/1724]  eta: 1:47:26  lr: 0.000200  loss: 1.3901 (1.6074)  loss_n_40: 0.3510 (0.4007)  loss_n_60: 0.3252 (0.3769)  loss_n_80: 0.3421 (0.3987)  loss_n_100: 0.3650 (0.4275)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0036)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [  90/1724]  eta: 1:46:46  lr: 0.000200  loss: 1.2620 (1.5773)  loss_n_40: 0.3379 (0.3946)  loss_n_60: 0.3030 (0.3702)  loss_n_80: 0.3083 (0.3909)  loss_n_100: 0.3270 (0.4185)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0032)  time: 3.9182  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [ 100/1724]  eta: 1:46:07  lr: 0.000200  loss: 1.3591 (1.5989)  loss_n_40: 0.3379 (0.3925)  loss_n_60: 0.3046 (0.3689)  loss_n_80: 0.3284 (0.3914)  loss_n_100: 0.3700 (0.4197)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0263)  time: 3.9182  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 110/1724]  eta: 1:45:27  lr: 0.000200  loss: 1.5404 (1.5960)  loss_n_40: 0.3476 (0.3901)  loss_n_60: 0.3450 (0.3683)  loss_n_80: 0.4060 (0.3923)  loss_n_100: 0.4513 (0.4214)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0240)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 120/1724]  eta: 1:44:47  lr: 0.000200  loss: 1.5877 (1.6030)  loss_n_40: 0.3770 (0.3911)  loss_n_60: 0.3639 (0.3702)  loss_n_80: 0.4060 (0.3951)  loss_n_100: 0.4458 (0.4247)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0220)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 130/1724]  eta: 1:44:08  lr: 0.000200  loss: 1.4025 (1.5903)  loss_n_40: 0.3588 (0.3874)  loss_n_60: 0.3268 (0.3673)  loss_n_80: 0.3596 (0.3925)  loss_n_100: 0.3838 (0.4228)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0203)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 140/1724]  eta: 1:43:28  lr: 0.000200  loss: 1.3323 (1.5781)  loss_n_40: 0.3180 (0.3833)  loss_n_60: 0.3150 (0.3642)  loss_n_80: 0.3300 (0.3894)  loss_n_100: 0.3642 (0.4196)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0189)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 150/1724]  eta: 1:42:49  lr: 0.000200  loss: 1.3196 (1.5720)  loss_n_40: 0.3180 (0.3789)  loss_n_60: 0.2943 (0.3597)  loss_n_80: 0.3200 (0.3850)  loss_n_100: 0.3534 (0.4156)  triple_100: 0.0000 (0.0068)  triple_80: 0.0000 (0.0026)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0200)  time: 3.9179  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 160/1724]  eta: 1:42:10  lr: 0.000200  loss: 1.5002 (1.5790)  loss_n_40: 0.3466 (0.3768)  loss_n_60: 0.3356 (0.3605)  loss_n_80: 0.3779 (0.3882)  loss_n_100: 0.4279 (0.4211)  triple_100: 0.0000 (0.0079)  triple_80: 0.0000 (0.0025)  triple_60: 0.0000 (0.0032)  triple_40: 0.0000 (0.0188)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 170/1724]  eta: 1:41:30  lr: 0.000200  loss: 1.6550 (1.5865)  loss_n_40: 0.3537 (0.3764)  loss_n_60: 0.3727 (0.3624)  loss_n_80: 0.4305 (0.3914)  loss_n_100: 0.4905 (0.4258)  triple_100: 0.0000 (0.0075)  triple_80: 0.0000 (0.0023)  triple_60: 0.0000 (0.0030)  triple_40: 0.0000 (0.0177)  time: 3.9179  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 180/1724]  eta: 1:40:51  lr: 0.000200  loss: 1.5596 (1.5810)  loss_n_40: 0.3456 (0.3749)  loss_n_60: 0.3616 (0.3617)  loss_n_80: 0.4093 (0.3904)  loss_n_100: 0.4548 (0.4252)  triple_100: 0.0000 (0.0071)  triple_80: 0.0000 (0.0022)  triple_60: 0.0000 (0.0028)  triple_40: 0.0000 (0.0167)  time: 3.9170  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 190/1724]  eta: 1:40:12  lr: 0.000200  loss: 1.4470 (1.5746)  loss_n_40: 0.3421 (0.3745)  loss_n_60: 0.3480 (0.3611)  loss_n_80: 0.3552 (0.3885)  loss_n_100: 0.4024 (0.4232)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0021)  triple_60: 0.0000 (0.0027)  triple_40: 0.0000 (0.0158)  time: 3.9174  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 200/1724]  eta: 1:39:32  lr: 0.000200  loss: 1.4579 (1.5723)  loss_n_40: 0.3501 (0.3733)  loss_n_60: 0.3472 (0.3610)  loss_n_80: 0.3538 (0.3880)  loss_n_100: 0.3950 (0.4227)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0020)  triple_60: 0.0000 (0.0025)  triple_40: 0.0000 (0.0163)  time: 3.9173  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 210/1724]  eta: 1:38:53  lr: 0.000200  loss: 1.4166 (1.5610)  loss_n_40: 0.3277 (0.3712)  loss_n_60: 0.3268 (0.3587)  loss_n_80: 0.3520 (0.3854)  loss_n_100: 0.3807 (0.4198)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0019)  triple_60: 0.0000 (0.0024)  triple_40: 0.0000 (0.0156)  time: 3.9161  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 220/1724]  eta: 1:38:14  lr: 0.000200  loss: 1.2329 (1.5461)  loss_n_40: 0.3090 (0.3686)  loss_n_60: 0.2908 (0.3556)  loss_n_80: 0.2958 (0.3817)  loss_n_100: 0.3216 (0.4154)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0018)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0149)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 230/1724]  eta: 1:37:34  lr: 0.000200  loss: 1.1959 (1.5332)  loss_n_40: 0.3069 (0.3661)  loss_n_60: 0.2855 (0.3532)  loss_n_80: 0.2951 (0.3785)  loss_n_100: 0.3168 (0.4117)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0017)  triple_60: 0.0000 (0.0022)  triple_40: 0.0000 (0.0142)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 240/1724]  eta: 1:36:55  lr: 0.000200  loss: 1.2654 (1.5307)  loss_n_40: 0.3092 (0.3641)  loss_n_60: 0.3026 (0.3514)  loss_n_80: 0.3080 (0.3769)  loss_n_100: 0.3313 (0.4105)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0020)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0150)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 250/1724]  eta: 1:36:16  lr: 0.000200  loss: 1.4888 (1.5315)  loss_n_40: 0.3354 (0.3642)  loss_n_60: 0.3297 (0.3516)  loss_n_80: 0.3741 (0.3775)  loss_n_100: 0.4068 (0.4115)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0019)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0144)  time: 3.9177  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 260/1724]  eta: 1:35:37  lr: 0.000200  loss: 1.4223 (1.5257)  loss_n_40: 0.3482 (0.3634)  loss_n_60: 0.3161 (0.3502)  loss_n_80: 0.3589 (0.3762)  loss_n_100: 0.4063 (0.4103)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0019)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0138)  time: 3.9182  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 270/1724]  eta: 1:34:57  lr: 0.000200  loss: 1.3105 (1.5157)  loss_n_40: 0.3289 (0.3616)  loss_n_60: 0.3018 (0.3481)  loss_n_80: 0.3209 (0.3738)  loss_n_100: 0.3508 (0.4076)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0018)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0133)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 280/1724]  eta: 1:34:18  lr: 0.000200  loss: 1.1829 (1.5055)  loss_n_40: 0.3060 (0.3594)  loss_n_60: 0.2810 (0.3462)  loss_n_80: 0.2924 (0.3713)  loss_n_100: 0.3223 (0.4047)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0017)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0129)  time: 3.9172  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 290/1724]  eta: 1:33:39  lr: 0.000200  loss: 1.1599 (1.5069)  loss_n_40: 0.2921 (0.3580)  loss_n_60: 0.2759 (0.3449)  loss_n_80: 0.2898 (0.3697)  loss_n_100: 0.3081 (0.4026)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0154)  time: 3.9166  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 300/1724]  eta: 1:32:59  lr: 0.000200  loss: 1.3546 (1.5070)  loss_n_40: 0.3182 (0.3571)  loss_n_60: 0.3161 (0.3452)  loss_n_80: 0.3610 (0.3705)  loss_n_100: 0.3983 (0.4036)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0149)  time: 3.9169  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 310/1724]  eta: 1:32:20  lr: 0.000200  loss: 1.3921 (1.5007)  loss_n_40: 0.3027 (0.3551)  loss_n_60: 0.3223 (0.3442)  loss_n_80: 0.3610 (0.3695)  loss_n_100: 0.3852 (0.4022)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0055)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0144)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 320/1724]  eta: 1:31:41  lr: 0.000200  loss: 1.2692 (1.4923)  loss_n_40: 0.2959 (0.3534)  loss_n_60: 0.3048 (0.3428)  loss_n_80: 0.3267 (0.3676)  loss_n_100: 0.3387 (0.3999)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0140)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 330/1724]  eta: 1:31:02  lr: 0.000200  loss: 1.1710 (1.4828)  loss_n_40: 0.2868 (0.3515)  loss_n_60: 0.2917 (0.3411)  loss_n_80: 0.2952 (0.3654)  loss_n_100: 0.3139 (0.3970)  triple_100: 0.0000 (0.0042)  triple_80: 0.0000 (0.0052)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0135)  time: 3.9183  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [ 340/1724]  eta: 1:30:23  lr: 0.000200  loss: 1.1389 (1.4749)  loss_n_40: 0.2894 (0.3503)  loss_n_60: 0.2776 (0.3397)  loss_n_80: 0.2798 (0.3634)  loss_n_100: 0.2862 (0.3944)  triple_100: 0.0000 (0.0040)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0131)  time: 3.9178  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 350/1724]  eta: 1:29:43  lr: 0.000200  loss: 1.1602 (1.4669)  loss_n_40: 0.3031 (0.3492)  loss_n_60: 0.2776 (0.3384)  loss_n_80: 0.2874 (0.3614)  loss_n_100: 0.3044 (0.3917)  triple_100: 0.0000 (0.0039)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0128)  time: 3.9165  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 360/1724]  eta: 1:29:04  lr: 0.000200  loss: 1.1602 (1.4581)  loss_n_40: 0.3093 (0.3477)  loss_n_60: 0.2728 (0.3367)  loss_n_80: 0.2874 (0.3594)  loss_n_100: 0.2963 (0.3889)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0047)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0124)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 370/1724]  eta: 1:28:25  lr: 0.000200  loss: 1.2471 (1.4594)  loss_n_40: 0.3148 (0.3471)  loss_n_60: 0.3000 (0.3366)  loss_n_80: 0.3068 (0.3595)  loss_n_100: 0.3325 (0.3891)  triple_100: 0.0000 (0.0037)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0121)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 380/1724]  eta: 1:27:46  lr: 0.000200  loss: 1.3837 (1.4564)  loss_n_40: 0.3068 (0.3462)  loss_n_60: 0.3222 (0.3360)  loss_n_80: 0.3516 (0.3591)  loss_n_100: 0.3898 (0.3888)  triple_100: 0.0000 (0.0036)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0118)  time: 3.9173  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 390/1724]  eta: 1:27:07  lr: 0.000200  loss: 1.1778 (1.4477)  loss_n_40: 0.2875 (0.3443)  loss_n_60: 0.2865 (0.3342)  loss_n_80: 0.3015 (0.3571)  loss_n_100: 0.3376 (0.3864)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0115)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 400/1724]  eta: 1:26:27  lr: 0.000200  loss: 1.0944 (1.4436)  loss_n_40: 0.2707 (0.3429)  loss_n_60: 0.2597 (0.3329)  loss_n_80: 0.2774 (0.3557)  loss_n_100: 0.2925 (0.3848)  triple_100: 0.0000 (0.0036)  triple_80: 0.0000 (0.0063)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0117)  time: 3.9189  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 410/1724]  eta: 1:25:48  lr: 0.000200  loss: 1.3477 (1.4432)  loss_n_40: 0.3057 (0.3425)  loss_n_60: 0.3021 (0.3328)  loss_n_80: 0.3461 (0.3560)  loss_n_100: 0.3679 (0.3853)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0114)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 420/1724]  eta: 1:25:09  lr: 0.000200  loss: 1.4101 (1.4431)  loss_n_40: 0.3263 (0.3426)  loss_n_60: 0.3203 (0.3326)  loss_n_80: 0.3579 (0.3563)  loss_n_100: 0.4066 (0.3856)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0111)  time: 3.9178  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 430/1724]  eta: 1:24:30  lr: 0.000200  loss: 1.4041 (1.4410)  loss_n_40: 0.3476 (0.3424)  loss_n_60: 0.3176 (0.3322)  loss_n_80: 0.3551 (0.3560)  loss_n_100: 0.3699 (0.3850)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0108)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 440/1724]  eta: 1:23:51  lr: 0.000200  loss: 1.2962 (1.4376)  loss_n_40: 0.3248 (0.3422)  loss_n_60: 0.2972 (0.3316)  loss_n_80: 0.3293 (0.3552)  loss_n_100: 0.3424 (0.3839)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0106)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 450/1724]  eta: 1:23:11  lr: 0.000200  loss: 1.2451 (1.4322)  loss_n_40: 0.3144 (0.3415)  loss_n_60: 0.2936 (0.3304)  loss_n_80: 0.3165 (0.3539)  loss_n_100: 0.3232 (0.3821)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0056)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0104)  time: 3.9171  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 460/1724]  eta: 1:22:32  lr: 0.000200  loss: 1.1572 (1.4281)  loss_n_40: 0.2922 (0.3403)  loss_n_60: 0.2699 (0.3292)  loss_n_80: 0.2779 (0.3525)  loss_n_100: 0.2815 (0.3804)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0117)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 470/1724]  eta: 1:21:53  lr: 0.000200  loss: 1.1447 (1.4220)  loss_n_40: 0.2726 (0.3388)  loss_n_60: 0.2683 (0.3280)  loss_n_80: 0.2724 (0.3512)  loss_n_100: 0.2869 (0.3788)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0114)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 480/1724]  eta: 1:21:14  lr: 0.000200  loss: 1.0717 (1.4147)  loss_n_40: 0.2660 (0.3374)  loss_n_60: 0.2567 (0.3266)  loss_n_80: 0.2638 (0.3494)  loss_n_100: 0.2758 (0.3766)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0112)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 490/1724]  eta: 1:20:35  lr: 0.000200  loss: 1.0424 (1.4098)  loss_n_40: 0.2660 (0.3365)  loss_n_60: 0.2506 (0.3257)  loss_n_80: 0.2568 (0.3483)  loss_n_100: 0.2745 (0.3752)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0056)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0110)  time: 3.9176  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 500/1724]  eta: 1:19:55  lr: 0.000200  loss: 1.0147 (1.4021)  loss_n_40: 0.2494 (0.3349)  loss_n_60: 0.2465 (0.3242)  loss_n_80: 0.2543 (0.3464)  loss_n_100: 0.2650 (0.3729)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0055)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0107)  time: 3.9179  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 510/1724]  eta: 1:19:16  lr: 0.000200  loss: 0.9943 (1.3961)  loss_n_40: 0.2452 (0.3339)  loss_n_60: 0.2402 (0.3230)  loss_n_80: 0.2472 (0.3450)  loss_n_100: 0.2589 (0.3711)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0105)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 520/1724]  eta: 1:18:37  lr: 0.000200  loss: 1.0382 (1.3905)  loss_n_40: 0.2643 (0.3328)  loss_n_60: 0.2545 (0.3219)  loss_n_80: 0.2600 (0.3437)  loss_n_100: 0.2612 (0.3693)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0103)  time: 3.9175  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 530/1724]  eta: 1:17:58  lr: 0.000200  loss: 1.0382 (1.3843)  loss_n_40: 0.2702 (0.3319)  loss_n_60: 0.2545 (0.3207)  loss_n_80: 0.2578 (0.3420)  loss_n_100: 0.2640 (0.3673)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0052)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0101)  time: 3.9188  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 540/1724]  eta: 1:17:19  lr: 0.000200  loss: 1.0225 (1.3792)  loss_n_40: 0.2631 (0.3307)  loss_n_60: 0.2498 (0.3195)  loss_n_80: 0.2525 (0.3406)  loss_n_100: 0.2640 (0.3656)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0103)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 550/1724]  eta: 1:16:40  lr: 0.000200  loss: 1.0402 (1.3741)  loss_n_40: 0.2571 (0.3296)  loss_n_60: 0.2446 (0.3185)  loss_n_80: 0.2589 (0.3394)  loss_n_100: 0.2737 (0.3642)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0102)  time: 3.9203  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 560/1724]  eta: 1:16:00  lr: 0.000200  loss: 1.0402 (1.3686)  loss_n_40: 0.2700 (0.3285)  loss_n_60: 0.2446 (0.3173)  loss_n_80: 0.2601 (0.3381)  loss_n_100: 0.2552 (0.3626)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0100)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 570/1724]  eta: 1:15:21  lr: 0.000200  loss: 1.1414 (1.3653)  loss_n_40: 0.2824 (0.3278)  loss_n_60: 0.2677 (0.3167)  loss_n_80: 0.2841 (0.3375)  loss_n_100: 0.2931 (0.3618)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0048)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0098)  time: 3.9185  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [ 580/1724]  eta: 1:14:42  lr: 0.000200  loss: 1.1330 (1.3632)  loss_n_40: 0.2833 (0.3270)  loss_n_60: 0.2742 (0.3158)  loss_n_80: 0.2838 (0.3365)  loss_n_100: 0.2949 (0.3605)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0107)  time: 3.9193  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 590/1724]  eta: 1:14:03  lr: 0.000200  loss: 1.2492 (1.3627)  loss_n_40: 0.2868 (0.3265)  loss_n_60: 0.2864 (0.3158)  loss_n_80: 0.3127 (0.3367)  loss_n_100: 0.3290 (0.3608)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0105)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 600/1724]  eta: 1:13:24  lr: 0.000200  loss: 1.4031 (1.3666)  loss_n_40: 0.3004 (0.3264)  loss_n_60: 0.3208 (0.3162)  loss_n_80: 0.3699 (0.3376)  loss_n_100: 0.4079 (0.3619)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0066)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0103)  time: 3.9190  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 610/1724]  eta: 1:12:45  lr: 0.000200  loss: 1.4205 (1.3659)  loss_n_40: 0.3262 (0.3262)  loss_n_60: 0.3403 (0.3162)  loss_n_80: 0.3541 (0.3376)  loss_n_100: 0.3864 (0.3619)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0065)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0102)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 620/1724]  eta: 1:12:05  lr: 0.000200  loss: 1.3276 (1.3656)  loss_n_40: 0.3262 (0.3262)  loss_n_60: 0.3101 (0.3163)  loss_n_80: 0.3272 (0.3377)  loss_n_100: 0.3469 (0.3617)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0064)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0100)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 630/1724]  eta: 1:11:26  lr: 0.000200  loss: 1.2852 (1.3645)  loss_n_40: 0.3117 (0.3260)  loss_n_60: 0.3101 (0.3163)  loss_n_80: 0.3162 (0.3375)  loss_n_100: 0.3434 (0.3614)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0063)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0099)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 640/1724]  eta: 1:10:47  lr: 0.000200  loss: 1.2388 (1.3618)  loss_n_40: 0.3019 (0.3255)  loss_n_60: 0.3020 (0.3159)  loss_n_80: 0.3013 (0.3369)  loss_n_100: 0.3170 (0.3606)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0097)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 650/1724]  eta: 1:10:08  lr: 0.000200  loss: 1.1437 (1.3588)  loss_n_40: 0.2784 (0.3251)  loss_n_60: 0.2841 (0.3155)  loss_n_80: 0.2843 (0.3360)  loss_n_100: 0.2918 (0.3595)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0095)  time: 3.9172  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 660/1724]  eta: 1:09:29  lr: 0.000200  loss: 1.1437 (1.3559)  loss_n_40: 0.2784 (0.3246)  loss_n_60: 0.2786 (0.3150)  loss_n_80: 0.2843 (0.3354)  loss_n_100: 0.2918 (0.3586)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0039)  triple_40: 0.0000 (0.0094)  time: 3.9167  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 670/1724]  eta: 1:08:49  lr: 0.000200  loss: 1.0719 (1.3510)  loss_n_40: 0.2704 (0.3237)  loss_n_60: 0.2584 (0.3141)  loss_n_80: 0.2658 (0.3342)  loss_n_100: 0.2751 (0.3572)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0039)  triple_40: 0.0000 (0.0093)  time: 3.9177  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 680/1724]  eta: 1:08:10  lr: 0.000200  loss: 0.9968 (1.3461)  loss_n_40: 0.2464 (0.3228)  loss_n_60: 0.2360 (0.3131)  loss_n_80: 0.2471 (0.3329)  loss_n_100: 0.2541 (0.3557)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0038)  triple_40: 0.0000 (0.0091)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 690/1724]  eta: 1:07:31  lr: 0.000200  loss: 0.9585 (1.3445)  loss_n_40: 0.2464 (0.3218)  loss_n_60: 0.2360 (0.3120)  loss_n_80: 0.2328 (0.3316)  loss_n_100: 0.2344 (0.3540)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0112)  time: 3.9187  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 700/1724]  eta: 1:06:52  lr: 0.000200  loss: 1.4239 (1.3556)  loss_n_40: 0.3166 (0.3232)  loss_n_60: 0.3304 (0.3145)  loss_n_80: 0.3235 (0.3350)  loss_n_100: 0.3172 (0.3582)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0111)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 710/1724]  eta: 1:06:13  lr: 0.000200  loss: 1.9925 (1.3667)  loss_n_40: 0.4258 (0.3252)  loss_n_60: 0.4719 (0.3171)  loss_n_80: 0.5164 (0.3380)  loss_n_100: 0.5591 (0.3618)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0109)  time: 3.9189  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 720/1724]  eta: 1:05:34  lr: 0.000200  loss: 1.9463 (1.3756)  loss_n_40: 0.4258 (0.3263)  loss_n_60: 0.4545 (0.3189)  loss_n_80: 0.4986 (0.3404)  loss_n_100: 0.5502 (0.3645)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0064)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0108)  time: 3.9190  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 730/1724]  eta: 1:04:54  lr: 0.000200  loss: 1.6363 (1.3794)  loss_n_40: 0.3389 (0.3269)  loss_n_60: 0.3885 (0.3199)  loss_n_80: 0.4095 (0.3416)  loss_n_100: 0.4607 (0.3660)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0063)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0106)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 740/1724]  eta: 1:04:15  lr: 0.000200  loss: 1.5584 (1.3838)  loss_n_40: 0.3776 (0.3279)  loss_n_60: 0.3674 (0.3210)  loss_n_80: 0.3954 (0.3428)  loss_n_100: 0.4265 (0.3673)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0105)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 750/1724]  eta: 1:03:36  lr: 0.000200  loss: 1.5439 (1.3836)  loss_n_40: 0.3547 (0.3279)  loss_n_60: 0.3528 (0.3212)  loss_n_80: 0.3871 (0.3429)  loss_n_100: 0.4220 (0.3672)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0103)  time: 3.9188  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 760/1724]  eta: 1:02:57  lr: 0.000200  loss: 1.2444 (1.3814)  loss_n_40: 0.2990 (0.3275)  loss_n_60: 0.3020 (0.3209)  loss_n_80: 0.3160 (0.3424)  loss_n_100: 0.3266 (0.3666)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0102)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 770/1724]  eta: 1:02:18  lr: 0.000200  loss: 1.1719 (1.3782)  loss_n_40: 0.2878 (0.3267)  loss_n_60: 0.2905 (0.3203)  loss_n_80: 0.2978 (0.3417)  loss_n_100: 0.3041 (0.3657)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0101)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 780/1724]  eta: 1:01:39  lr: 0.000200  loss: 1.1123 (1.3753)  loss_n_40: 0.2722 (0.3261)  loss_n_60: 0.2726 (0.3198)  loss_n_80: 0.2793 (0.3410)  loss_n_100: 0.2881 (0.3649)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0099)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 790/1724]  eta: 1:00:59  lr: 0.000200  loss: 1.0515 (1.3708)  loss_n_40: 0.2590 (0.3252)  loss_n_60: 0.2606 (0.3189)  loss_n_80: 0.2652 (0.3399)  loss_n_100: 0.2749 (0.3635)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0098)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 800/1724]  eta: 1:00:20  lr: 0.000200  loss: 1.1088 (1.3683)  loss_n_40: 0.2572 (0.3245)  loss_n_60: 0.2630 (0.3183)  loss_n_80: 0.2728 (0.3392)  loss_n_100: 0.2955 (0.3628)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0098)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 810/1724]  eta: 0:59:41  lr: 0.000200  loss: 1.2073 (1.3682)  loss_n_40: 0.2709 (0.3245)  loss_n_60: 0.2762 (0.3184)  loss_n_80: 0.3102 (0.3393)  loss_n_100: 0.3386 (0.3628)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0097)  time: 3.9198  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [ 820/1724]  eta: 0:59:02  lr: 0.000200  loss: 1.2073 (1.3658)  loss_n_40: 0.2884 (0.3241)  loss_n_60: 0.2847 (0.3179)  loss_n_80: 0.3096 (0.3387)  loss_n_100: 0.3132 (0.3622)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0095)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 830/1724]  eta: 0:58:23  lr: 0.000200  loss: 1.2993 (1.3714)  loss_n_40: 0.2884 (0.3243)  loss_n_60: 0.3071 (0.3185)  loss_n_80: 0.3200 (0.3395)  loss_n_100: 0.3344 (0.3633)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0065)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0110)  time: 3.9224  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 840/1724]  eta: 0:57:44  lr: 0.000200  loss: 1.5741 (1.3755)  loss_n_40: 0.3439 (0.3249)  loss_n_60: 0.3675 (0.3195)  loss_n_80: 0.4086 (0.3408)  loss_n_100: 0.4496 (0.3648)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0064)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0109)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 850/1724]  eta: 0:57:04  lr: 0.000200  loss: 1.4954 (1.3760)  loss_n_40: 0.3305 (0.3251)  loss_n_60: 0.3500 (0.3197)  loss_n_80: 0.3701 (0.3410)  loss_n_100: 0.3981 (0.3650)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0063)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0108)  time: 3.9224  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 860/1724]  eta: 0:56:25  lr: 0.000200  loss: 1.3293 (1.3745)  loss_n_40: 0.3197 (0.3249)  loss_n_60: 0.3135 (0.3195)  loss_n_80: 0.3372 (0.3407)  loss_n_100: 0.3518 (0.3645)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0063)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0107)  time: 3.9230  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 870/1724]  eta: 0:55:46  lr: 0.000200  loss: 1.2379 (1.3740)  loss_n_40: 0.3268 (0.3250)  loss_n_60: 0.3004 (0.3195)  loss_n_80: 0.3071 (0.3406)  loss_n_100: 0.3191 (0.3643)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0105)  time: 3.9223  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 880/1724]  eta: 0:55:07  lr: 0.000200  loss: 1.2379 (1.3719)  loss_n_40: 0.3012 (0.3246)  loss_n_60: 0.3004 (0.3192)  loss_n_80: 0.3116 (0.3401)  loss_n_100: 0.3234 (0.3636)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0104)  time: 3.9230  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 890/1724]  eta: 0:54:28  lr: 0.000200  loss: 1.2188 (1.3718)  loss_n_40: 0.2923 (0.3243)  loss_n_60: 0.2898 (0.3189)  loss_n_80: 0.2919 (0.3398)  loss_n_100: 0.3090 (0.3634)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0106)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 900/1724]  eta: 0:53:49  lr: 0.000200  loss: 1.4603 (1.3739)  loss_n_40: 0.3218 (0.3244)  loss_n_60: 0.3396 (0.3194)  loss_n_80: 0.3592 (0.3406)  loss_n_100: 0.3895 (0.3644)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0062)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0105)  time: 3.9219  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 910/1724]  eta: 0:53:10  lr: 0.000200  loss: 1.5640 (1.3756)  loss_n_40: 0.3311 (0.3244)  loss_n_60: 0.3653 (0.3197)  loss_n_80: 0.4051 (0.3413)  loss_n_100: 0.4414 (0.3653)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0061)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0104)  time: 3.9204  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 920/1724]  eta: 0:52:30  lr: 0.000200  loss: 1.4495 (1.3759)  loss_n_40: 0.3214 (0.3245)  loss_n_60: 0.3368 (0.3199)  loss_n_80: 0.3500 (0.3415)  loss_n_100: 0.3971 (0.3655)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0103)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 930/1724]  eta: 0:51:51  lr: 0.000200  loss: 1.2839 (1.3751)  loss_n_40: 0.3110 (0.3243)  loss_n_60: 0.3037 (0.3198)  loss_n_80: 0.3272 (0.3413)  loss_n_100: 0.3580 (0.3653)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0060)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0102)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 940/1724]  eta: 0:51:12  lr: 0.000200  loss: 1.2581 (1.3736)  loss_n_40: 0.3007 (0.3241)  loss_n_60: 0.3014 (0.3195)  loss_n_80: 0.3197 (0.3410)  loss_n_100: 0.3418 (0.3649)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0059)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0101)  time: 3.9209  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [ 950/1724]  eta: 0:50:33  lr: 0.000200  loss: 1.1505 (1.3714)  loss_n_40: 0.2863 (0.3237)  loss_n_60: 0.2791 (0.3192)  loss_n_80: 0.2908 (0.3405)  loss_n_100: 0.3037 (0.3642)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0100)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 960/1724]  eta: 0:49:54  lr: 0.000200  loss: 1.1283 (1.3705)  loss_n_40: 0.2771 (0.3237)  loss_n_60: 0.2724 (0.3190)  loss_n_80: 0.2868 (0.3403)  loss_n_100: 0.3002 (0.3639)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0099)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 970/1724]  eta: 0:49:14  lr: 0.000200  loss: 1.1283 (1.3686)  loss_n_40: 0.2771 (0.3234)  loss_n_60: 0.2726 (0.3187)  loss_n_80: 0.2868 (0.3399)  loss_n_100: 0.2967 (0.3633)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0098)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 980/1724]  eta: 0:48:35  lr: 0.000200  loss: 1.1362 (1.3667)  loss_n_40: 0.2808 (0.3231)  loss_n_60: 0.2779 (0.3184)  loss_n_80: 0.2909 (0.3395)  loss_n_100: 0.2971 (0.3627)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0097)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [ 990/1724]  eta: 0:47:56  lr: 0.000200  loss: 1.1837 (1.3643)  loss_n_40: 0.2847 (0.3228)  loss_n_60: 0.2872 (0.3179)  loss_n_80: 0.2986 (0.3389)  loss_n_100: 0.2965 (0.3619)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0056)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0096)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1000/1724]  eta: 0:47:17  lr: 0.000200  loss: 1.0703 (1.3614)  loss_n_40: 0.2673 (0.3223)  loss_n_60: 0.2566 (0.3173)  loss_n_80: 0.2665 (0.3381)  loss_n_100: 0.2660 (0.3610)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0056)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0095)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1010/1724]  eta: 0:46:38  lr: 0.000200  loss: 1.0491 (1.3584)  loss_n_40: 0.2652 (0.3219)  loss_n_60: 0.2566 (0.3167)  loss_n_80: 0.2532 (0.3374)  loss_n_100: 0.2654 (0.3600)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0055)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0094)  time: 3.9210  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1020/1724]  eta: 0:45:59  lr: 0.000200  loss: 1.0827 (1.3557)  loss_n_40: 0.2652 (0.3214)  loss_n_60: 0.2508 (0.3162)  loss_n_80: 0.2521 (0.3367)  loss_n_100: 0.2654 (0.3592)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0093)  time: 3.9210  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1030/1724]  eta: 0:45:19  lr: 0.000200  loss: 0.9776 (1.3515)  loss_n_40: 0.2511 (0.3205)  loss_n_60: 0.2326 (0.3152)  loss_n_80: 0.2416 (0.3357)  loss_n_100: 0.2499 (0.3580)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0092)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1040/1724]  eta: 0:44:40  lr: 0.000200  loss: 0.9367 (1.3482)  loss_n_40: 0.2304 (0.3200)  loss_n_60: 0.2232 (0.3145)  loss_n_80: 0.2376 (0.3348)  loss_n_100: 0.2392 (0.3570)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0047)  triple_40: 0.0000 (0.0091)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1050/1724]  eta: 0:44:01  lr: 0.000200  loss: 0.9494 (1.3447)  loss_n_40: 0.2304 (0.3193)  loss_n_60: 0.2215 (0.3138)  loss_n_80: 0.2374 (0.3340)  loss_n_100: 0.2402 (0.3561)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0090)  time: 3.9208  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [1060/1724]  eta: 0:43:22  lr: 0.000200  loss: 0.9494 (1.3414)  loss_n_40: 0.2411 (0.3186)  loss_n_60: 0.2241 (0.3131)  loss_n_80: 0.2374 (0.3331)  loss_n_100: 0.2407 (0.3551)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0052)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0089)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1070/1724]  eta: 0:42:43  lr: 0.000200  loss: 0.9268 (1.3374)  loss_n_40: 0.2267 (0.3178)  loss_n_60: 0.2203 (0.3122)  loss_n_80: 0.2295 (0.3322)  loss_n_100: 0.2379 (0.3540)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0052)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0088)  time: 3.9217  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1080/1724]  eta: 0:42:03  lr: 0.000200  loss: 0.9234 (1.3346)  loss_n_40: 0.2267 (0.3172)  loss_n_60: 0.2228 (0.3116)  loss_n_80: 0.2316 (0.3315)  loss_n_100: 0.2400 (0.3533)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0088)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1090/1724]  eta: 0:41:24  lr: 0.000200  loss: 0.9597 (1.3313)  loss_n_40: 0.2316 (0.3166)  loss_n_60: 0.2261 (0.3109)  loss_n_80: 0.2342 (0.3307)  loss_n_100: 0.2412 (0.3524)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0087)  time: 3.9208  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1100/1724]  eta: 0:40:45  lr: 0.000200  loss: 0.8740 (1.3281)  loss_n_40: 0.2268 (0.3160)  loss_n_60: 0.2110 (0.3102)  loss_n_80: 0.2227 (0.3299)  loss_n_100: 0.2325 (0.3514)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0086)  time: 3.9202  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1110/1724]  eta: 0:40:06  lr: 0.000200  loss: 0.8666 (1.3238)  loss_n_40: 0.2199 (0.3151)  loss_n_60: 0.2070 (0.3092)  loss_n_80: 0.2182 (0.3288)  loss_n_100: 0.2240 (0.3502)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0085)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1120/1724]  eta: 0:39:27  lr: 0.000200  loss: 0.8603 (1.3225)  loss_n_40: 0.2199 (0.3147)  loss_n_60: 0.2046 (0.3086)  loss_n_80: 0.2158 (0.3281)  loss_n_100: 0.2177 (0.3493)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0096)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1130/1724]  eta: 0:38:48  lr: 0.000200  loss: 1.2142 (1.3228)  loss_n_40: 0.2748 (0.3147)  loss_n_60: 0.2660 (0.3086)  loss_n_80: 0.2667 (0.3283)  loss_n_100: 0.2715 (0.3496)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0096)  time: 3.9224  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1140/1724]  eta: 0:38:08  lr: 0.000200  loss: 1.3993 (1.3236)  loss_n_40: 0.3307 (0.3150)  loss_n_60: 0.3130 (0.3088)  loss_n_80: 0.3642 (0.3285)  loss_n_100: 0.3713 (0.3499)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0095)  time: 3.9224  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1150/1724]  eta: 0:37:29  lr: 0.000200  loss: 1.4126 (1.3255)  loss_n_40: 0.3382 (0.3156)  loss_n_60: 0.3145 (0.3092)  loss_n_80: 0.3642 (0.3290)  loss_n_100: 0.3713 (0.3504)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0094)  time: 3.9221  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1160/1724]  eta: 0:36:50  lr: 0.000200  loss: 1.3599 (1.3260)  loss_n_40: 0.3254 (0.3155)  loss_n_60: 0.3166 (0.3092)  loss_n_80: 0.3517 (0.3292)  loss_n_100: 0.3754 (0.3506)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0096)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1170/1724]  eta: 0:36:11  lr: 0.000200  loss: 1.3504 (1.3262)  loss_n_40: 0.3087 (0.3155)  loss_n_60: 0.3166 (0.3093)  loss_n_80: 0.3414 (0.3293)  loss_n_100: 0.3674 (0.3508)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0095)  time: 3.9214  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1180/1724]  eta: 0:35:32  lr: 0.000200  loss: 1.2039 (1.3249)  loss_n_40: 0.2650 (0.3151)  loss_n_60: 0.2863 (0.3091)  loss_n_80: 0.3120 (0.3291)  loss_n_100: 0.3322 (0.3505)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0048)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0095)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1190/1724]  eta: 0:34:52  lr: 0.000200  loss: 1.0851 (1.3231)  loss_n_40: 0.2501 (0.3147)  loss_n_60: 0.2603 (0.3088)  loss_n_80: 0.2803 (0.3287)  loss_n_100: 0.2955 (0.3500)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0048)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0094)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1200/1724]  eta: 0:34:13  lr: 0.000200  loss: 1.0509 (1.3206)  loss_n_40: 0.2498 (0.3142)  loss_n_60: 0.2583 (0.3084)  loss_n_80: 0.2678 (0.3281)  loss_n_100: 0.2740 (0.3493)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0048)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0093)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1210/1724]  eta: 0:33:34  lr: 0.000200  loss: 1.0509 (1.3185)  loss_n_40: 0.2498 (0.3138)  loss_n_60: 0.2583 (0.3079)  loss_n_80: 0.2656 (0.3276)  loss_n_100: 0.2764 (0.3486)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0047)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0092)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1220/1724]  eta: 0:32:55  lr: 0.000200  loss: 0.9783 (1.3156)  loss_n_40: 0.2379 (0.3132)  loss_n_60: 0.2373 (0.3073)  loss_n_80: 0.2461 (0.3269)  loss_n_100: 0.2487 (0.3478)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0047)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0091)  time: 3.9204  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1230/1724]  eta: 0:32:16  lr: 0.000200  loss: 0.9703 (1.3127)  loss_n_40: 0.2379 (0.3126)  loss_n_60: 0.2350 (0.3067)  loss_n_80: 0.2394 (0.3262)  loss_n_100: 0.2438 (0.3470)  triple_100: 0.0000 (0.0023)  triple_80: 0.0000 (0.0047)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0091)  time: 3.9211  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1240/1724]  eta: 0:31:37  lr: 0.000200  loss: 0.9729 (1.3104)  loss_n_40: 0.2412 (0.3121)  loss_n_60: 0.2365 (0.3062)  loss_n_80: 0.2429 (0.3256)  loss_n_100: 0.2481 (0.3463)  triple_100: 0.0000 (0.0023)  triple_80: 0.0000 (0.0046)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0090)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1250/1724]  eta: 0:30:57  lr: 0.000200  loss: 0.8877 (1.3069)  loss_n_40: 0.2203 (0.3114)  loss_n_60: 0.2142 (0.3055)  loss_n_80: 0.2218 (0.3247)  loss_n_100: 0.2347 (0.3454)  triple_100: 0.0000 (0.0023)  triple_80: 0.0000 (0.0046)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0089)  time: 3.9199  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1260/1724]  eta: 0:30:18  lr: 0.000200  loss: 0.8877 (1.3080)  loss_n_40: 0.2240 (0.3111)  loss_n_60: 0.2142 (0.3053)  loss_n_80: 0.2203 (0.3245)  loss_n_100: 0.2311 (0.3452)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0090)  time: 3.9201  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1270/1724]  eta: 0:29:39  lr: 0.000200  loss: 1.3974 (1.3184)  loss_n_40: 0.3084 (0.3116)  loss_n_60: 0.3160 (0.3064)  loss_n_80: 0.3649 (0.3262)  loss_n_100: 0.4282 (0.3475)  triple_100: 0.0000 (0.0073)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0046)  triple_40: 0.0000 (0.0090)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1280/1724]  eta: 0:29:00  lr: 0.000200  loss: 1.6790 (1.3214)  loss_n_40: 0.3528 (0.3121)  loss_n_60: 0.3992 (0.3072)  loss_n_80: 0.4419 (0.3272)  loss_n_100: 0.4753 (0.3486)  triple_100: 0.0000 (0.0072)  triple_80: 0.0000 (0.0058)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0089)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1290/1724]  eta: 0:28:21  lr: 0.000200  loss: 1.5575 (1.3235)  loss_n_40: 0.3488 (0.3125)  loss_n_60: 0.3698 (0.3077)  loss_n_80: 0.4118 (0.3278)  loss_n_100: 0.4385 (0.3492)  triple_100: 0.0000 (0.0071)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0088)  time: 3.9194  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [1300/1724]  eta: 0:27:41  lr: 0.000200  loss: 1.4892 (1.3245)  loss_n_40: 0.3548 (0.3128)  loss_n_60: 0.3596 (0.3081)  loss_n_80: 0.3705 (0.3281)  loss_n_100: 0.3927 (0.3495)  triple_100: 0.0000 (0.0071)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0045)  triple_40: 0.0000 (0.0088)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1310/1724]  eta: 0:27:02  lr: 0.000200  loss: 1.3681 (1.3246)  loss_n_40: 0.3333 (0.3129)  loss_n_60: 0.3360 (0.3082)  loss_n_80: 0.3573 (0.3282)  loss_n_100: 0.3657 (0.3494)  triple_100: 0.0000 (0.0070)  triple_80: 0.0000 (0.0057)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0087)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1320/1724]  eta: 0:26:23  lr: 0.000200  loss: 1.2515 (1.3237)  loss_n_40: 0.2960 (0.3127)  loss_n_60: 0.2970 (0.3081)  loss_n_80: 0.3125 (0.3281)  loss_n_100: 0.3209 (0.3492)  triple_100: 0.0000 (0.0070)  triple_80: 0.0000 (0.0056)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0086)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1330/1724]  eta: 0:25:44  lr: 0.000200  loss: 1.1244 (1.3224)  loss_n_40: 0.2708 (0.3126)  loss_n_60: 0.2817 (0.3079)  loss_n_80: 0.2881 (0.3277)  loss_n_100: 0.2846 (0.3487)  triple_100: 0.0000 (0.0069)  triple_80: 0.0000 (0.0056)  triple_60: 0.0000 (0.0044)  triple_40: 0.0000 (0.0086)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1340/1724]  eta: 0:25:05  lr: 0.000200  loss: 1.0962 (1.3211)  loss_n_40: 0.2708 (0.3124)  loss_n_60: 0.2694 (0.3077)  loss_n_80: 0.2724 (0.3274)  loss_n_100: 0.2767 (0.3483)  triple_100: 0.0000 (0.0069)  triple_80: 0.0000 (0.0055)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0085)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1350/1724]  eta: 0:24:25  lr: 0.000200  loss: 1.0962 (1.3190)  loss_n_40: 0.2669 (0.3121)  loss_n_60: 0.2691 (0.3073)  loss_n_80: 0.2724 (0.3270)  loss_n_100: 0.2767 (0.3476)  triple_100: 0.0000 (0.0068)  triple_80: 0.0000 (0.0055)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0084)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1360/1724]  eta: 0:23:46  lr: 0.000200  loss: 1.0349 (1.3173)  loss_n_40: 0.2576 (0.3118)  loss_n_60: 0.2530 (0.3070)  loss_n_80: 0.2662 (0.3266)  loss_n_100: 0.2582 (0.3471)  triple_100: 0.0000 (0.0068)  triple_80: 0.0000 (0.0055)  triple_60: 0.0000 (0.0043)  triple_40: 0.0000 (0.0084)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1370/1724]  eta: 0:23:07  lr: 0.000200  loss: 1.0287 (1.3153)  loss_n_40: 0.2548 (0.3114)  loss_n_60: 0.2492 (0.3066)  loss_n_80: 0.2643 (0.3261)  loss_n_100: 0.2646 (0.3465)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0083)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1380/1724]  eta: 0:22:28  lr: 0.000200  loss: 1.0074 (1.3134)  loss_n_40: 0.2501 (0.3111)  loss_n_60: 0.2492 (0.3062)  loss_n_80: 0.2543 (0.3257)  loss_n_100: 0.2601 (0.3459)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0054)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0083)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1390/1724]  eta: 0:21:49  lr: 0.000200  loss: 1.0105 (1.3116)  loss_n_40: 0.2530 (0.3107)  loss_n_60: 0.2490 (0.3059)  loss_n_80: 0.2501 (0.3252)  loss_n_100: 0.2557 (0.3454)  triple_100: 0.0000 (0.0066)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0082)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1400/1724]  eta: 0:21:09  lr: 0.000200  loss: 1.0137 (1.3094)  loss_n_40: 0.2583 (0.3103)  loss_n_60: 0.2490 (0.3054)  loss_n_80: 0.2542 (0.3247)  loss_n_100: 0.2573 (0.3448)  triple_100: 0.0000 (0.0066)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0042)  triple_40: 0.0000 (0.0081)  time: 3.9201  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1410/1724]  eta: 0:20:30  lr: 0.000200  loss: 1.0377 (1.3073)  loss_n_40: 0.2586 (0.3099)  loss_n_60: 0.2468 (0.3050)  loss_n_80: 0.2598 (0.3242)  loss_n_100: 0.2626 (0.3441)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0053)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0081)  time: 3.9206  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1420/1724]  eta: 0:19:51  lr: 0.000200  loss: 0.9876 (1.3052)  loss_n_40: 0.2587 (0.3096)  loss_n_60: 0.2432 (0.3046)  loss_n_80: 0.2442 (0.3237)  loss_n_100: 0.2464 (0.3435)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0052)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0080)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1430/1724]  eta: 0:19:12  lr: 0.000200  loss: 0.9986 (1.3034)  loss_n_40: 0.2504 (0.3092)  loss_n_60: 0.2432 (0.3042)  loss_n_80: 0.2475 (0.3233)  loss_n_100: 0.2464 (0.3430)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0052)  triple_60: 0.0000 (0.0041)  triple_40: 0.0000 (0.0080)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1440/1724]  eta: 0:18:33  lr: 0.000200  loss: 0.9311 (1.3010)  loss_n_40: 0.2323 (0.3088)  loss_n_60: 0.2269 (0.3037)  loss_n_80: 0.2322 (0.3227)  loss_n_100: 0.2444 (0.3423)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0052)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0079)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1450/1724]  eta: 0:17:53  lr: 0.000200  loss: 0.9591 (1.2994)  loss_n_40: 0.2323 (0.3084)  loss_n_60: 0.2296 (0.3034)  loss_n_80: 0.2388 (0.3223)  loss_n_100: 0.2583 (0.3419)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0052)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0079)  time: 3.9205  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1460/1724]  eta: 0:17:14  lr: 0.000200  loss: 1.0229 (1.2976)  loss_n_40: 0.2533 (0.3081)  loss_n_60: 0.2433 (0.3030)  loss_n_80: 0.2473 (0.3219)  loss_n_100: 0.2641 (0.3414)  triple_100: 0.0000 (0.0063)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0078)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1470/1724]  eta: 0:16:35  lr: 0.000200  loss: 0.9748 (1.2956)  loss_n_40: 0.2459 (0.3077)  loss_n_60: 0.2371 (0.3026)  loss_n_80: 0.2444 (0.3214)  loss_n_100: 0.2517 (0.3408)  triple_100: 0.0000 (0.0063)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0078)  time: 3.9191  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1480/1724]  eta: 0:15:56  lr: 0.000200  loss: 0.9862 (1.2940)  loss_n_40: 0.2514 (0.3074)  loss_n_60: 0.2383 (0.3023)  loss_n_80: 0.2408 (0.3210)  loss_n_100: 0.2554 (0.3404)  triple_100: 0.0000 (0.0062)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0039)  triple_40: 0.0000 (0.0077)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1490/1724]  eta: 0:15:17  lr: 0.000200  loss: 1.0208 (1.2921)  loss_n_40: 0.2525 (0.3071)  loss_n_60: 0.2397 (0.3019)  loss_n_80: 0.2496 (0.3205)  loss_n_100: 0.2677 (0.3398)  triple_100: 0.0000 (0.0062)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0039)  triple_40: 0.0000 (0.0077)  time: 3.9191  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1500/1724]  eta: 0:14:37  lr: 0.000200  loss: 1.0621 (1.2925)  loss_n_40: 0.2689 (0.3070)  loss_n_60: 0.2557 (0.3019)  loss_n_80: 0.2600 (0.3207)  loss_n_100: 0.2776 (0.3401)  triple_100: 0.0000 (0.0061)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0039)  triple_40: 0.0000 (0.0078)  time: 3.9181  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1510/1724]  eta: 0:13:58  lr: 0.000200  loss: 1.4246 (1.2944)  loss_n_40: 0.3019 (0.3072)  loss_n_60: 0.3211 (0.3022)  loss_n_80: 0.3665 (0.3211)  loss_n_100: 0.4010 (0.3407)  triple_100: 0.0000 (0.0061)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0040)  triple_40: 0.0000 (0.0082)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1520/1724]  eta: 0:13:19  lr: 0.000200  loss: 1.6876 (1.3088)  loss_n_40: 0.3387 (0.3077)  loss_n_60: 0.3685 (0.3031)  loss_n_80: 0.4295 (0.3224)  loss_n_100: 0.4514 (0.3421)  triple_100: 0.0000 (0.0090)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0088)  triple_40: 0.0000 (0.0090)  time: 3.9185  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1530/1724]  eta: 0:12:40  lr: 0.000200  loss: 2.0689 (1.3151)  loss_n_40: 0.3668 (0.3086)  loss_n_60: 0.4521 (0.3042)  loss_n_80: 0.5748 (0.3241)  loss_n_100: 0.6048 (0.3439)  triple_100: 0.0000 (0.0098)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0089)  time: 3.9175  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:19]  [1540/1724]  eta: 0:12:01  lr: 0.000200  loss: 2.0187 (1.3205)  loss_n_40: 0.4254 (0.3096)  loss_n_60: 0.4565 (0.3053)  loss_n_80: 0.5313 (0.3254)  loss_n_100: 0.5605 (0.3452)  triple_100: 0.0000 (0.0106)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0088)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1550/1724]  eta: 0:11:21  lr: 0.000200  loss: 2.0093 (1.3270)  loss_n_40: 0.4513 (0.3105)  loss_n_60: 0.4861 (0.3067)  loss_n_80: 0.5313 (0.3274)  loss_n_100: 0.6210 (0.3476)  triple_100: 0.0000 (0.0106)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0088)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1560/1724]  eta: 0:10:42  lr: 0.000200  loss: 2.1581 (1.3328)  loss_n_40: 0.4608 (0.3116)  loss_n_60: 0.4972 (0.3081)  loss_n_80: 0.5754 (0.3291)  loss_n_100: 0.6316 (0.3494)  triple_100: 0.0000 (0.0105)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0086)  triple_40: 0.0000 (0.0087)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1570/1724]  eta: 0:10:03  lr: 0.000200  loss: 1.9492 (1.3356)  loss_n_40: 0.4216 (0.3123)  loss_n_60: 0.4549 (0.3089)  loss_n_80: 0.4878 (0.3299)  loss_n_100: 0.5304 (0.3502)  triple_100: 0.0000 (0.0104)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0087)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1580/1724]  eta: 0:09:24  lr: 0.000200  loss: 1.6799 (1.3376)  loss_n_40: 0.3855 (0.3129)  loss_n_60: 0.4042 (0.3094)  loss_n_80: 0.4357 (0.3304)  loss_n_100: 0.4639 (0.3507)  triple_100: 0.0000 (0.0104)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0086)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1590/1724]  eta: 0:08:45  lr: 0.000200  loss: 1.4615 (1.3383)  loss_n_40: 0.3301 (0.3129)  loss_n_60: 0.3567 (0.3096)  loss_n_80: 0.3619 (0.3306)  loss_n_100: 0.3830 (0.3509)  triple_100: 0.0000 (0.0103)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0086)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1600/1724]  eta: 0:08:06  lr: 0.000200  loss: 1.3386 (1.3380)  loss_n_40: 0.3046 (0.3129)  loss_n_60: 0.3260 (0.3097)  loss_n_80: 0.3417 (0.3306)  loss_n_100: 0.3594 (0.3508)  triple_100: 0.0000 (0.0102)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0085)  time: 3.9196  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1610/1724]  eta: 0:07:26  lr: 0.000200  loss: 1.2962 (1.3381)  loss_n_40: 0.2978 (0.3131)  loss_n_60: 0.3134 (0.3098)  loss_n_80: 0.3334 (0.3307)  loss_n_100: 0.3458 (0.3508)  triple_100: 0.0000 (0.0102)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0085)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1620/1724]  eta: 0:06:47  lr: 0.000200  loss: 1.3397 (1.3384)  loss_n_40: 0.3204 (0.3136)  loss_n_60: 0.3198 (0.3099)  loss_n_80: 0.3207 (0.3307)  loss_n_100: 0.3342 (0.3508)  triple_100: 0.0000 (0.0101)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0084)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1630/1724]  eta: 0:06:08  lr: 0.000200  loss: 1.2223 (1.3368)  loss_n_40: 0.2938 (0.3133)  loss_n_60: 0.2920 (0.3096)  loss_n_80: 0.3000 (0.3303)  loss_n_100: 0.3062 (0.3503)  triple_100: 0.0000 (0.0101)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0084)  time: 3.9190  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1640/1724]  eta: 0:05:29  lr: 0.000200  loss: 1.1397 (1.3361)  loss_n_40: 0.2706 (0.3132)  loss_n_60: 0.2764 (0.3095)  loss_n_80: 0.2934 (0.3302)  loss_n_100: 0.2998 (0.3501)  triple_100: 0.0000 (0.0100)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0083)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1650/1724]  eta: 0:04:50  lr: 0.000200  loss: 1.1411 (1.3350)  loss_n_40: 0.2753 (0.3131)  loss_n_60: 0.2732 (0.3091)  loss_n_80: 0.2660 (0.3297)  loss_n_100: 0.2803 (0.3496)  triple_100: 0.0000 (0.0099)  triple_80: 0.0000 (0.0069)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0085)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1660/1724]  eta: 0:04:10  lr: 0.000200  loss: 1.2145 (1.3343)  loss_n_40: 0.2930 (0.3131)  loss_n_60: 0.2654 (0.3091)  loss_n_80: 0.2660 (0.3296)  loss_n_100: 0.2803 (0.3494)  triple_100: 0.0000 (0.0099)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0084)  time: 3.9185  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:19]  [1670/1724]  eta: 0:03:31  lr: 0.000200  loss: 1.1115 (1.3326)  loss_n_40: 0.2761 (0.3128)  loss_n_60: 0.2654 (0.3087)  loss_n_80: 0.2839 (0.3292)  loss_n_100: 0.2861 (0.3489)  triple_100: 0.0000 (0.0098)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0084)  time: 3.9199  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1680/1724]  eta: 0:02:52  lr: 0.000200  loss: 0.9664 (1.3310)  loss_n_40: 0.2369 (0.3126)  loss_n_60: 0.2336 (0.3084)  loss_n_80: 0.2430 (0.3288)  loss_n_100: 0.2511 (0.3485)  triple_100: 0.0000 (0.0098)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0083)  time: 3.9194  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1690/1724]  eta: 0:02:13  lr: 0.000200  loss: 1.0081 (1.3299)  loss_n_40: 0.2441 (0.3124)  loss_n_60: 0.2454 (0.3081)  loss_n_80: 0.2569 (0.3285)  loss_n_100: 0.2534 (0.3481)  triple_100: 0.0000 (0.0097)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0083)  time: 3.9186  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1700/1724]  eta: 0:01:34  lr: 0.000200  loss: 1.0479 (1.3285)  loss_n_40: 0.2663 (0.3123)  loss_n_60: 0.2468 (0.3079)  loss_n_80: 0.2637 (0.3282)  loss_n_100: 0.2812 (0.3477)  triple_100: 0.0000 (0.0096)  triple_80: 0.0000 (0.0068)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0082)  time: 3.9203  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1710/1724]  eta: 0:00:54  lr: 0.000200  loss: 1.0425 (1.3273)  loss_n_40: 0.2520 (0.3121)  loss_n_60: 0.2468 (0.3076)  loss_n_80: 0.2637 (0.3279)  loss_n_100: 0.2669 (0.3473)  triple_100: 0.0000 (0.0096)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0082)  time: 3.9207  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1720/1724]  eta: 0:00:15  lr: 0.000200  loss: 0.9313 (1.3252)  loss_n_40: 0.2301 (0.3117)  loss_n_60: 0.2250 (0.3072)  loss_n_80: 0.2380 (0.3274)  loss_n_100: 0.2450 (0.3467)  triple_100: 0.0000 (0.0095)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0081)  time: 3.9210  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19]  [1723/1724]  eta: 0:00:03  lr: 0.000200  loss: 0.9089 (1.3245)  loss_n_40: 0.2289 (0.3116)  loss_n_60: 0.2220 (0.3070)  loss_n_80: 0.2293 (0.3273)  loss_n_100: 0.2337 (0.3466)  triple_100: 0.0000 (0.0095)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0081)  time: 3.9209  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:19] Total time: 1:52:37 (3.9196 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 0.9089 (1.3245)  loss_n_40: 0.2289 (0.3116)  loss_n_60: 0.2220 (0.3070)  loss_n_80: 0.2293 (0.3273)  loss_n_100: 0.2337 (0.3466)  triple_100: 0.0000 (0.0095)  triple_80: 0.0000 (0.0067)  triple_60: 0.0000 (0.0078)  triple_40: 0.0000 (0.0081)\n",
      "Valid: [epoch:19]  [  0/845]  eta: 0:09:49  loss: 0.7879 (0.7879)  loss_n_40: 0.1961 (0.1961)  loss_n_60: 0.1964 (0.1964)  loss_n_80: 0.1986 (0.1986)  loss_n_100: 0.1968 (0.1968)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.6981  data: 0.3621  max mem: 46473\n",
      "Valid: [epoch:19]  [ 10/845]  eta: 0:05:06  loss: 0.9570 (1.0084)  loss_n_40: 0.2369 (0.2525)  loss_n_60: 0.2284 (0.2414)  loss_n_80: 0.2518 (0.2620)  loss_n_100: 0.2451 (0.2525)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3675  data: 0.0330  max mem: 46473\n",
      "Valid: [epoch:19]  [ 20/845]  eta: 0:04:50  loss: 0.9204 (1.0083)  loss_n_40: 0.2281 (0.2706)  loss_n_60: 0.2201 (0.2364)  loss_n_80: 0.2412 (0.2500)  loss_n_100: 0.2448 (0.2514)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:19]  [ 30/845]  eta: 0:04:41  loss: 0.9076 (0.9807)  loss_n_40: 0.2243 (0.2570)  loss_n_60: 0.2139 (0.2330)  loss_n_80: 0.2283 (0.2451)  loss_n_100: 0.2266 (0.2457)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [ 40/845]  eta: 0:04:36  loss: 0.8344 (0.9509)  loss_n_40: 0.2171 (0.2460)  loss_n_60: 0.2071 (0.2266)  loss_n_80: 0.2172 (0.2390)  loss_n_100: 0.2186 (0.2393)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [ 50/845]  eta: 0:04:31  loss: 0.8812 (0.9876)  loss_n_40: 0.2171 (0.2537)  loss_n_60: 0.2108 (0.2370)  loss_n_80: 0.2335 (0.2491)  loss_n_100: 0.2295 (0.2479)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [ 60/845]  eta: 0:04:27  loss: 0.8030 (1.0557)  loss_n_40: 0.1913 (0.2448)  loss_n_60: 0.1915 (0.2298)  loss_n_80: 0.2109 (0.2417)  loss_n_100: 0.2101 (0.2402)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0384)  triple_60: 0.0000 (0.0344)  triple_40: 0.0000 (0.0234)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [ 70/845]  eta: 0:04:23  loss: 0.8125 (1.0435)  loss_n_40: 0.2059 (0.2429)  loss_n_60: 0.1915 (0.2309)  loss_n_80: 0.2059 (0.2433)  loss_n_100: 0.2092 (0.2412)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0330)  triple_60: 0.0000 (0.0295)  triple_40: 0.0000 (0.0201)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [ 80/845]  eta: 0:04:19  loss: 0.9271 (1.1284)  loss_n_40: 0.2260 (0.2411)  loss_n_60: 0.2219 (0.2294)  loss_n_80: 0.2344 (0.2411)  loss_n_100: 0.2162 (0.2386)  triple_100: 0.0000 (0.0121)  triple_80: 0.0000 (0.0661)  triple_60: 0.0000 (0.0617)  triple_40: 0.0000 (0.0383)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [ 90/845]  eta: 0:04:15  loss: 0.9271 (1.1196)  loss_n_40: 0.2324 (0.2432)  loss_n_60: 0.2219 (0.2325)  loss_n_80: 0.2344 (0.2440)  loss_n_100: 0.2179 (0.2413)  triple_100: 0.0000 (0.0108)  triple_80: 0.0000 (0.0588)  triple_60: 0.0000 (0.0549)  triple_40: 0.0000 (0.0341)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [100/845]  eta: 0:04:11  loss: 0.9257 (1.1063)  loss_n_40: 0.2381 (0.2445)  loss_n_60: 0.2196 (0.2336)  loss_n_80: 0.2355 (0.2441)  loss_n_100: 0.2334 (0.2412)  triple_100: 0.0000 (0.0097)  triple_80: 0.0000 (0.0530)  triple_60: 0.0000 (0.0495)  triple_40: 0.0000 (0.0307)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [110/845]  eta: 0:04:08  loss: 0.9329 (1.0892)  loss_n_40: 0.2372 (0.2428)  loss_n_60: 0.2196 (0.2326)  loss_n_80: 0.2400 (0.2431)  loss_n_100: 0.2390 (0.2406)  triple_100: 0.0000 (0.0088)  triple_80: 0.0000 (0.0482)  triple_60: 0.0000 (0.0450)  triple_40: 0.0000 (0.0280)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [120/845]  eta: 0:04:04  loss: 0.9329 (1.0748)  loss_n_40: 0.2328 (0.2419)  loss_n_60: 0.2201 (0.2320)  loss_n_80: 0.2400 (0.2422)  loss_n_100: 0.2390 (0.2394)  triple_100: 0.0000 (0.0081)  triple_80: 0.0000 (0.0442)  triple_60: 0.0000 (0.0413)  triple_40: 0.0000 (0.0256)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [130/845]  eta: 0:04:01  loss: 0.8853 (1.0656)  loss_n_40: 0.2217 (0.2413)  loss_n_60: 0.2115 (0.2318)  loss_n_80: 0.2280 (0.2424)  loss_n_100: 0.2278 (0.2400)  triple_100: 0.0000 (0.0075)  triple_80: 0.0000 (0.0408)  triple_60: 0.0000 (0.0382)  triple_40: 0.0000 (0.0237)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [140/845]  eta: 0:03:57  loss: 0.8891 (1.0571)  loss_n_40: 0.2176 (0.2402)  loss_n_60: 0.2115 (0.2317)  loss_n_80: 0.2280 (0.2426)  loss_n_100: 0.2261 (0.2401)  triple_100: 0.0000 (0.0070)  triple_80: 0.0000 (0.0380)  triple_60: 0.0000 (0.0355)  triple_40: 0.0000 (0.0220)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [150/845]  eta: 0:03:54  loss: 0.8906 (1.0532)  loss_n_40: 0.2176 (0.2424)  loss_n_60: 0.2149 (0.2319)  loss_n_80: 0.2257 (0.2425)  loss_n_100: 0.2226 (0.2408)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0354)  triple_60: 0.0000 (0.0331)  triple_40: 0.0000 (0.0206)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [160/845]  eta: 0:03:50  loss: 0.9308 (1.0586)  loss_n_40: 0.2322 (0.2465)  loss_n_60: 0.2208 (0.2338)  loss_n_80: 0.2374 (0.2452)  loss_n_100: 0.2415 (0.2434)  triple_100: 0.0000 (0.0061)  triple_80: 0.0000 (0.0332)  triple_60: 0.0000 (0.0311)  triple_40: 0.0000 (0.0193)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [170/845]  eta: 0:03:47  loss: 0.9180 (1.0516)  loss_n_40: 0.2359 (0.2459)  loss_n_60: 0.2202 (0.2336)  loss_n_80: 0.2354 (0.2447)  loss_n_100: 0.2273 (0.2430)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0313)  triple_60: 0.0000 (0.0292)  triple_40: 0.0000 (0.0181)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [180/845]  eta: 0:03:43  loss: 0.8000 (1.0381)  loss_n_40: 0.2133 (0.2437)  loss_n_60: 0.1880 (0.2316)  loss_n_80: 0.2025 (0.2425)  loss_n_100: 0.1960 (0.2406)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0296)  triple_60: 0.0000 (0.0276)  triple_40: 0.0000 (0.0171)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [190/845]  eta: 0:03:40  loss: 0.8028 (1.0320)  loss_n_40: 0.2133 (0.2428)  loss_n_60: 0.1914 (0.2311)  loss_n_80: 0.2025 (0.2421)  loss_n_100: 0.1960 (0.2403)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0280)  triple_60: 0.0000 (0.0262)  triple_40: 0.0000 (0.0162)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [200/845]  eta: 0:03:36  loss: 0.8130 (1.0359)  loss_n_40: 0.2124 (0.2449)  loss_n_60: 0.1962 (0.2333)  loss_n_80: 0.2017 (0.2441)  loss_n_100: 0.2048 (0.2418)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0266)  triple_60: 0.0000 (0.0249)  triple_40: 0.0000 (0.0154)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [210/845]  eta: 0:03:33  loss: 0.7976 (1.0367)  loss_n_40: 0.2024 (0.2432)  loss_n_60: 0.1942 (0.2320)  loss_n_80: 0.2017 (0.2428)  loss_n_100: 0.2048 (0.2403)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0302)  triple_60: 0.0000 (0.0273)  triple_40: 0.0000 (0.0162)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [220/845]  eta: 0:03:30  loss: 0.8410 (1.0358)  loss_n_40: 0.2142 (0.2433)  loss_n_60: 0.2099 (0.2329)  loss_n_80: 0.2161 (0.2437)  loss_n_100: 0.2124 (0.2411)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0288)  triple_60: 0.0000 (0.0260)  triple_40: 0.0000 (0.0155)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [230/845]  eta: 0:03:26  loss: 0.9429 (1.0336)  loss_n_40: 0.2290 (0.2433)  loss_n_60: 0.2214 (0.2332)  loss_n_80: 0.2441 (0.2442)  loss_n_100: 0.2536 (0.2413)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0276)  triple_60: 0.0000 (0.0249)  triple_40: 0.0000 (0.0148)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [240/845]  eta: 0:03:23  loss: 0.8883 (1.0282)  loss_n_40: 0.2113 (0.2427)  loss_n_60: 0.2106 (0.2326)  loss_n_80: 0.2224 (0.2435)  loss_n_100: 0.2249 (0.2408)  triple_100: 0.0000 (0.0041)  triple_80: 0.0000 (0.0264)  triple_60: 0.0000 (0.0239)  triple_40: 0.0000 (0.0142)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [250/845]  eta: 0:03:19  loss: 1.0362 (1.0354)  loss_n_40: 0.2508 (0.2460)  loss_n_60: 0.2539 (0.2346)  loss_n_80: 0.2590 (0.2457)  loss_n_100: 0.2571 (0.2432)  triple_100: 0.0000 (0.0039)  triple_80: 0.0000 (0.0254)  triple_60: 0.0000 (0.0229)  triple_40: 0.0000 (0.0137)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [260/845]  eta: 0:03:16  loss: 1.0362 (1.0394)  loss_n_40: 0.2545 (0.2479)  loss_n_60: 0.2539 (0.2360)  loss_n_80: 0.2646 (0.2473)  loss_n_100: 0.2706 (0.2449)  triple_100: 0.0000 (0.0038)  triple_80: 0.0000 (0.0244)  triple_60: 0.0000 (0.0220)  triple_40: 0.0000 (0.0131)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [270/845]  eta: 0:03:13  loss: 0.9201 (1.0407)  loss_n_40: 0.2182 (0.2490)  loss_n_60: 0.2233 (0.2371)  loss_n_80: 0.2446 (0.2480)  loss_n_100: 0.2290 (0.2457)  triple_100: 0.0000 (0.0036)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0212)  triple_40: 0.0000 (0.0126)  time: 0.3346  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:19]  [280/845]  eta: 0:03:09  loss: 0.9582 (1.0404)  loss_n_40: 0.2265 (0.2500)  loss_n_60: 0.2292 (0.2373)  loss_n_80: 0.2496 (0.2482)  loss_n_100: 0.2446 (0.2460)  triple_100: 0.0000 (0.0035)  triple_80: 0.0000 (0.0227)  triple_60: 0.0000 (0.0205)  triple_40: 0.0000 (0.0122)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [290/845]  eta: 0:03:06  loss: 0.9582 (1.0377)  loss_n_40: 0.2303 (0.2495)  loss_n_60: 0.2292 (0.2371)  loss_n_80: 0.2455 (0.2482)  loss_n_100: 0.2446 (0.2460)  triple_100: 0.0000 (0.0034)  triple_80: 0.0000 (0.0219)  triple_60: 0.0000 (0.0198)  triple_40: 0.0000 (0.0118)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [300/845]  eta: 0:03:02  loss: 0.8556 (1.0341)  loss_n_40: 0.2167 (0.2490)  loss_n_60: 0.2079 (0.2366)  loss_n_80: 0.2142 (0.2477)  loss_n_100: 0.2184 (0.2458)  triple_100: 0.0000 (0.0033)  triple_80: 0.0000 (0.0212)  triple_60: 0.0000 (0.0191)  triple_40: 0.0000 (0.0114)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [310/845]  eta: 0:02:59  loss: 0.8039 (1.0298)  loss_n_40: 0.2078 (0.2481)  loss_n_60: 0.2001 (0.2361)  loss_n_80: 0.2002 (0.2471)  loss_n_100: 0.1958 (0.2454)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0205)  triple_60: 0.0000 (0.0185)  triple_40: 0.0000 (0.0110)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [320/845]  eta: 0:02:56  loss: 0.9303 (1.0299)  loss_n_40: 0.2267 (0.2491)  loss_n_60: 0.2202 (0.2361)  loss_n_80: 0.2396 (0.2471)  loss_n_100: 0.2433 (0.2459)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0198)  triple_60: 0.0000 (0.0179)  triple_40: 0.0000 (0.0107)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [330/845]  eta: 0:02:52  loss: 0.9690 (1.0361)  loss_n_40: 0.2301 (0.2516)  loss_n_60: 0.2304 (0.2382)  loss_n_80: 0.2495 (0.2487)  loss_n_100: 0.2617 (0.2477)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0192)  triple_60: 0.0000 (0.0174)  triple_40: 0.0000 (0.0104)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [340/845]  eta: 0:02:49  loss: 0.8611 (1.0296)  loss_n_40: 0.2197 (0.2502)  loss_n_60: 0.1988 (0.2370)  loss_n_80: 0.2188 (0.2475)  loss_n_100: 0.2132 (0.2465)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0187)  triple_60: 0.0000 (0.0169)  triple_40: 0.0000 (0.0100)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [350/845]  eta: 0:02:46  loss: 0.7935 (1.0285)  loss_n_40: 0.2021 (0.2506)  loss_n_60: 0.1968 (0.2372)  loss_n_80: 0.2013 (0.2474)  loss_n_100: 0.1986 (0.2462)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0181)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0098)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [360/845]  eta: 0:02:42  loss: 0.9034 (1.0317)  loss_n_40: 0.2290 (0.2521)  loss_n_60: 0.2153 (0.2380)  loss_n_80: 0.2310 (0.2484)  loss_n_100: 0.2417 (0.2475)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0176)  triple_60: 0.0000 (0.0159)  triple_40: 0.0000 (0.0095)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [370/845]  eta: 0:02:39  loss: 0.9440 (1.0305)  loss_n_40: 0.2310 (0.2528)  loss_n_60: 0.2200 (0.2377)  loss_n_80: 0.2424 (0.2480)  loss_n_100: 0.2503 (0.2474)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0172)  triple_60: 0.0000 (0.0155)  triple_40: 0.0000 (0.0092)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [380/845]  eta: 0:02:35  loss: 0.9945 (1.0361)  loss_n_40: 0.2352 (0.2534)  loss_n_60: 0.2381 (0.2384)  loss_n_80: 0.2495 (0.2489)  loss_n_100: 0.2606 (0.2482)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0180)  triple_60: 0.0000 (0.0170)  triple_40: 0.0000 (0.0097)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [390/845]  eta: 0:02:32  loss: 0.9250 (1.0320)  loss_n_40: 0.2224 (0.2524)  loss_n_60: 0.2232 (0.2378)  loss_n_80: 0.2460 (0.2483)  loss_n_100: 0.2334 (0.2475)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0175)  triple_60: 0.0000 (0.0166)  triple_40: 0.0000 (0.0095)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [400/845]  eta: 0:02:29  loss: 0.8534 (1.0528)  loss_n_40: 0.2176 (0.2522)  loss_n_60: 0.2053 (0.2377)  loss_n_80: 0.2246 (0.2483)  loss_n_100: 0.2252 (0.2476)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0246)  triple_60: 0.0000 (0.0242)  triple_40: 0.0000 (0.0138)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [410/845]  eta: 0:02:25  loss: 0.9221 (1.0710)  loss_n_40: 0.2260 (0.2531)  loss_n_60: 0.2155 (0.2380)  loss_n_80: 0.2429 (0.2484)  loss_n_100: 0.2390 (0.2480)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0323)  triple_60: 0.0000 (0.0300)  triple_40: 0.0000 (0.0162)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [420/845]  eta: 0:02:22  loss: 0.9804 (1.0687)  loss_n_40: 0.2336 (0.2534)  loss_n_60: 0.2335 (0.2380)  loss_n_80: 0.2474 (0.2481)  loss_n_100: 0.2437 (0.2477)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0315)  triple_60: 0.0000 (0.0293)  triple_40: 0.0000 (0.0158)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [430/845]  eta: 0:02:19  loss: 0.7706 (1.0634)  loss_n_40: 0.2006 (0.2523)  loss_n_60: 0.1897 (0.2371)  loss_n_80: 0.1975 (0.2474)  loss_n_100: 0.1969 (0.2469)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0308)  triple_60: 0.0000 (0.0286)  triple_40: 0.0000 (0.0154)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [440/845]  eta: 0:02:15  loss: 0.9613 (1.0669)  loss_n_40: 0.2311 (0.2537)  loss_n_60: 0.2287 (0.2385)  loss_n_80: 0.2520 (0.2486)  loss_n_100: 0.2415 (0.2482)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0301)  triple_60: 0.0000 (0.0279)  triple_40: 0.0000 (0.0151)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [450/845]  eta: 0:02:12  loss: 1.0082 (1.0642)  loss_n_40: 0.2430 (0.2537)  loss_n_60: 0.2439 (0.2383)  loss_n_80: 0.2548 (0.2483)  loss_n_100: 0.2575 (0.2479)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0294)  triple_60: 0.0000 (0.0273)  triple_40: 0.0000 (0.0147)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [460/845]  eta: 0:02:09  loss: 0.8485 (1.0613)  loss_n_40: 0.2182 (0.2533)  loss_n_60: 0.2068 (0.2380)  loss_n_80: 0.2187 (0.2480)  loss_n_100: 0.2212 (0.2476)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0288)  triple_60: 0.0000 (0.0267)  triple_40: 0.0000 (0.0144)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [470/845]  eta: 0:02:05  loss: 0.8749 (1.0918)  loss_n_40: 0.2158 (0.2529)  loss_n_60: 0.2070 (0.2379)  loss_n_80: 0.2221 (0.2480)  loss_n_100: 0.2302 (0.2477)  triple_100: 0.0000 (0.0076)  triple_80: 0.0000 (0.0410)  triple_60: 0.0000 (0.0356)  triple_40: 0.0000 (0.0212)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [480/845]  eta: 0:02:02  loss: 0.9785 (1.0913)  loss_n_40: 0.2370 (0.2532)  loss_n_60: 0.2341 (0.2385)  loss_n_80: 0.2436 (0.2484)  loss_n_100: 0.2569 (0.2480)  triple_100: 0.0000 (0.0074)  triple_80: 0.0000 (0.0402)  triple_60: 0.0000 (0.0349)  triple_40: 0.0000 (0.0207)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [490/845]  eta: 0:01:59  loss: 0.9679 (1.0895)  loss_n_40: 0.2370 (0.2530)  loss_n_60: 0.2294 (0.2386)  loss_n_80: 0.2491 (0.2486)  loss_n_100: 0.2515 (0.2483)  triple_100: 0.0000 (0.0073)  triple_80: 0.0000 (0.0393)  triple_60: 0.0000 (0.0342)  triple_40: 0.0000 (0.0203)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [500/845]  eta: 0:01:55  loss: 1.0223 (1.0931)  loss_n_40: 0.2425 (0.2555)  loss_n_60: 0.2516 (0.2395)  loss_n_80: 0.2685 (0.2496)  loss_n_100: 0.2621 (0.2495)  triple_100: 0.0000 (0.0071)  triple_80: 0.0000 (0.0386)  triple_60: 0.0000 (0.0335)  triple_40: 0.0000 (0.0199)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [510/845]  eta: 0:01:52  loss: 1.0223 (1.0928)  loss_n_40: 0.2452 (0.2556)  loss_n_60: 0.2516 (0.2399)  loss_n_80: 0.2685 (0.2502)  loss_n_100: 0.2614 (0.2500)  triple_100: 0.0000 (0.0070)  triple_80: 0.0000 (0.0378)  triple_60: 0.0000 (0.0328)  triple_40: 0.0000 (0.0195)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [520/845]  eta: 0:01:48  loss: 0.9375 (1.0890)  loss_n_40: 0.2304 (0.2552)  loss_n_60: 0.2199 (0.2394)  loss_n_80: 0.2384 (0.2496)  loss_n_100: 0.2301 (0.2495)  triple_100: 0.0000 (0.0069)  triple_80: 0.0000 (0.0371)  triple_60: 0.0000 (0.0322)  triple_40: 0.0000 (0.0191)  time: 0.3343  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:19]  [530/845]  eta: 0:01:45  loss: 0.8037 (1.0876)  loss_n_40: 0.1989 (0.2550)  loss_n_60: 0.1943 (0.2395)  loss_n_80: 0.2097 (0.2497)  loss_n_100: 0.2062 (0.2499)  triple_100: 0.0000 (0.0067)  triple_80: 0.0000 (0.0364)  triple_60: 0.0000 (0.0316)  triple_40: 0.0000 (0.0188)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [540/845]  eta: 0:01:42  loss: 0.8970 (1.0884)  loss_n_40: 0.2270 (0.2552)  loss_n_60: 0.2132 (0.2401)  loss_n_80: 0.2341 (0.2506)  loss_n_100: 0.2252 (0.2507)  triple_100: 0.0000 (0.0066)  triple_80: 0.0000 (0.0357)  triple_60: 0.0000 (0.0310)  triple_40: 0.0000 (0.0184)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [550/845]  eta: 0:01:38  loss: 0.8970 (1.0851)  loss_n_40: 0.2245 (0.2546)  loss_n_60: 0.2132 (0.2397)  loss_n_80: 0.2341 (0.2503)  loss_n_100: 0.2252 (0.2504)  triple_100: 0.0000 (0.0065)  triple_80: 0.0000 (0.0351)  triple_60: 0.0000 (0.0304)  triple_40: 0.0000 (0.0181)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [560/845]  eta: 0:01:35  loss: 0.8568 (1.0821)  loss_n_40: 0.2133 (0.2540)  loss_n_60: 0.2074 (0.2393)  loss_n_80: 0.2162 (0.2500)  loss_n_100: 0.2241 (0.2504)  triple_100: 0.0000 (0.0064)  triple_80: 0.0000 (0.0344)  triple_60: 0.0000 (0.0299)  triple_40: 0.0000 (0.0178)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [570/845]  eta: 0:01:32  loss: 0.9107 (1.0829)  loss_n_40: 0.2365 (0.2546)  loss_n_60: 0.2196 (0.2398)  loss_n_80: 0.2263 (0.2505)  loss_n_100: 0.2241 (0.2508)  triple_100: 0.0000 (0.0063)  triple_80: 0.0000 (0.0340)  triple_60: 0.0000 (0.0294)  triple_40: 0.0000 (0.0175)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [580/845]  eta: 0:01:28  loss: 1.0201 (1.0804)  loss_n_40: 0.2521 (0.2543)  loss_n_60: 0.2410 (0.2396)  loss_n_80: 0.2602 (0.2503)  loss_n_100: 0.2478 (0.2505)  triple_100: 0.0000 (0.0061)  triple_80: 0.0000 (0.0334)  triple_60: 0.0000 (0.0289)  triple_40: 0.0000 (0.0172)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [590/845]  eta: 0:01:25  loss: 0.7939 (1.0751)  loss_n_40: 0.1977 (0.2532)  loss_n_60: 0.1913 (0.2388)  loss_n_80: 0.1982 (0.2494)  loss_n_100: 0.1999 (0.2495)  triple_100: 0.0000 (0.0060)  triple_80: 0.0000 (0.0329)  triple_60: 0.0000 (0.0284)  triple_40: 0.0000 (0.0169)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [600/845]  eta: 0:01:22  loss: 0.7706 (1.0715)  loss_n_40: 0.1977 (0.2527)  loss_n_60: 0.1913 (0.2383)  loss_n_80: 0.1970 (0.2488)  loss_n_100: 0.1944 (0.2490)  triple_100: 0.0000 (0.0059)  triple_80: 0.0000 (0.0323)  triple_60: 0.0000 (0.0279)  triple_40: 0.0000 (0.0166)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [610/845]  eta: 0:01:18  loss: 0.8196 (1.0708)  loss_n_40: 0.2092 (0.2529)  loss_n_60: 0.1954 (0.2384)  loss_n_80: 0.2055 (0.2490)  loss_n_100: 0.2014 (0.2492)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0318)  triple_60: 0.0000 (0.0274)  triple_40: 0.0000 (0.0163)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [620/845]  eta: 0:01:15  loss: 0.8607 (1.0690)  loss_n_40: 0.2194 (0.2527)  loss_n_60: 0.2061 (0.2383)  loss_n_80: 0.2179 (0.2489)  loss_n_100: 0.2178 (0.2490)  triple_100: 0.0000 (0.0058)  triple_80: 0.0000 (0.0313)  triple_60: 0.0000 (0.0270)  triple_40: 0.0000 (0.0161)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [630/845]  eta: 0:01:12  loss: 0.8607 (1.0674)  loss_n_40: 0.2194 (0.2529)  loss_n_60: 0.2075 (0.2382)  loss_n_80: 0.2179 (0.2487)  loss_n_100: 0.2178 (0.2488)  triple_100: 0.0000 (0.0057)  triple_80: 0.0000 (0.0308)  triple_60: 0.0000 (0.0266)  triple_40: 0.0000 (0.0158)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [640/845]  eta: 0:01:08  loss: 0.8544 (1.0750)  loss_n_40: 0.2340 (0.2535)  loss_n_60: 0.2075 (0.2385)  loss_n_80: 0.2174 (0.2488)  loss_n_100: 0.2287 (0.2490)  triple_100: 0.0000 (0.0056)  triple_80: 0.0000 (0.0341)  triple_60: 0.0000 (0.0291)  triple_40: 0.0000 (0.0166)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [650/845]  eta: 0:01:05  loss: 0.8520 (1.0723)  loss_n_40: 0.2071 (0.2531)  loss_n_60: 0.1988 (0.2382)  loss_n_80: 0.2182 (0.2485)  loss_n_100: 0.2220 (0.2486)  triple_100: 0.0000 (0.0055)  triple_80: 0.0000 (0.0336)  triple_60: 0.0000 (0.0286)  triple_40: 0.0000 (0.0163)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [660/845]  eta: 0:01:01  loss: 0.9306 (1.0716)  loss_n_40: 0.2461 (0.2530)  loss_n_60: 0.2297 (0.2383)  loss_n_80: 0.2388 (0.2488)  loss_n_100: 0.2305 (0.2487)  triple_100: 0.0000 (0.0054)  triple_80: 0.0000 (0.0331)  triple_60: 0.0000 (0.0282)  triple_40: 0.0000 (0.0160)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [670/845]  eta: 0:00:58  loss: 0.9691 (1.0710)  loss_n_40: 0.2244 (0.2533)  loss_n_60: 0.2297 (0.2385)  loss_n_80: 0.2452 (0.2489)  loss_n_100: 0.2510 (0.2489)  triple_100: 0.0000 (0.0053)  triple_80: 0.0000 (0.0326)  triple_60: 0.0000 (0.0278)  triple_40: 0.0000 (0.0158)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [680/845]  eta: 0:00:55  loss: 0.8958 (1.0683)  loss_n_40: 0.2237 (0.2529)  loss_n_60: 0.2131 (0.2381)  loss_n_80: 0.2337 (0.2485)  loss_n_100: 0.2205 (0.2485)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0321)  triple_60: 0.0000 (0.0274)  triple_40: 0.0000 (0.0156)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [690/845]  eta: 0:00:51  loss: 0.9191 (1.0680)  loss_n_40: 0.2410 (0.2530)  loss_n_60: 0.2230 (0.2384)  loss_n_80: 0.2337 (0.2488)  loss_n_100: 0.2205 (0.2487)  triple_100: 0.0000 (0.0052)  triple_80: 0.0000 (0.0316)  triple_60: 0.0000 (0.0270)  triple_40: 0.0000 (0.0154)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [700/845]  eta: 0:00:48  loss: 0.9797 (1.0671)  loss_n_40: 0.2395 (0.2529)  loss_n_60: 0.2309 (0.2384)  loss_n_80: 0.2526 (0.2490)  loss_n_100: 0.2524 (0.2489)  triple_100: 0.0000 (0.0051)  triple_80: 0.0000 (0.0312)  triple_60: 0.0000 (0.0266)  triple_40: 0.0000 (0.0151)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [710/845]  eta: 0:00:45  loss: 0.9380 (1.0663)  loss_n_40: 0.2235 (0.2528)  loss_n_60: 0.2222 (0.2385)  loss_n_80: 0.2478 (0.2491)  loss_n_100: 0.2485 (0.2489)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0307)  triple_60: 0.0000 (0.0262)  triple_40: 0.0000 (0.0149)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [720/845]  eta: 0:00:41  loss: 0.9000 (1.0673)  loss_n_40: 0.2276 (0.2542)  loss_n_60: 0.2179 (0.2388)  loss_n_80: 0.2330 (0.2492)  loss_n_100: 0.2320 (0.2492)  triple_100: 0.0000 (0.0050)  triple_80: 0.0000 (0.0303)  triple_60: 0.0000 (0.0258)  triple_40: 0.0000 (0.0147)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [730/845]  eta: 0:00:38  loss: 1.0182 (1.0679)  loss_n_40: 0.2385 (0.2543)  loss_n_60: 0.2448 (0.2393)  loss_n_80: 0.2583 (0.2497)  loss_n_100: 0.2564 (0.2498)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0299)  triple_60: 0.0000 (0.0255)  triple_40: 0.0000 (0.0145)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [740/845]  eta: 0:00:35  loss: 0.9795 (1.0668)  loss_n_40: 0.2335 (0.2545)  loss_n_60: 0.2371 (0.2392)  loss_n_80: 0.2482 (0.2495)  loss_n_100: 0.2515 (0.2497)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0295)  triple_60: 0.0000 (0.0251)  triple_40: 0.0000 (0.0143)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [750/845]  eta: 0:00:31  loss: 0.9509 (1.0763)  loss_n_40: 0.2324 (0.2559)  loss_n_60: 0.2289 (0.2401)  loss_n_80: 0.2447 (0.2503)  loss_n_100: 0.2432 (0.2508)  triple_100: 0.0000 (0.0049)  triple_80: 0.0000 (0.0315)  triple_60: 0.0000 (0.0274)  triple_40: 0.0000 (0.0154)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [760/845]  eta: 0:00:28  loss: 0.8836 (1.0727)  loss_n_40: 0.2294 (0.2551)  loss_n_60: 0.2146 (0.2395)  loss_n_80: 0.2233 (0.2497)  loss_n_100: 0.2162 (0.2501)  triple_100: 0.0000 (0.0048)  triple_80: 0.0000 (0.0311)  triple_60: 0.0000 (0.0270)  triple_40: 0.0000 (0.0152)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [770/845]  eta: 0:00:25  loss: 0.7752 (1.0730)  loss_n_40: 0.1976 (0.2554)  loss_n_60: 0.1890 (0.2400)  loss_n_80: 0.2006 (0.2501)  loss_n_100: 0.2021 (0.2503)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0307)  triple_60: 0.0000 (0.0267)  triple_40: 0.0000 (0.0150)  time: 0.3345  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:19]  [780/845]  eta: 0:00:21  loss: 0.9933 (1.0719)  loss_n_40: 0.2218 (0.2552)  loss_n_60: 0.2417 (0.2400)  loss_n_80: 0.2588 (0.2501)  loss_n_100: 0.2516 (0.2504)  triple_100: 0.0000 (0.0047)  triple_80: 0.0000 (0.0303)  triple_60: 0.0000 (0.0263)  triple_40: 0.0000 (0.0148)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [790/845]  eta: 0:00:18  loss: 0.9524 (1.0704)  loss_n_40: 0.2157 (0.2549)  loss_n_60: 0.2262 (0.2400)  loss_n_80: 0.2505 (0.2500)  loss_n_100: 0.2514 (0.2503)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0299)  triple_60: 0.0000 (0.0260)  triple_40: 0.0000 (0.0146)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [800/845]  eta: 0:00:15  loss: 0.8326 (1.0686)  loss_n_40: 0.2108 (0.2546)  loss_n_60: 0.2030 (0.2398)  loss_n_80: 0.2137 (0.2498)  loss_n_100: 0.2133 (0.2501)  triple_100: 0.0000 (0.0046)  triple_80: 0.0000 (0.0295)  triple_60: 0.0000 (0.0257)  triple_40: 0.0000 (0.0145)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [810/845]  eta: 0:00:11  loss: 0.8556 (1.0661)  loss_n_40: 0.2220 (0.2541)  loss_n_60: 0.2075 (0.2395)  loss_n_80: 0.2199 (0.2495)  loss_n_100: 0.2169 (0.2497)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0292)  triple_60: 0.0000 (0.0254)  triple_40: 0.0000 (0.0143)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [820/845]  eta: 0:00:08  loss: 0.9376 (1.0648)  loss_n_40: 0.2295 (0.2539)  loss_n_60: 0.2205 (0.2394)  loss_n_80: 0.2489 (0.2495)  loss_n_100: 0.2382 (0.2496)  triple_100: 0.0000 (0.0045)  triple_80: 0.0000 (0.0288)  triple_60: 0.0000 (0.0251)  triple_40: 0.0000 (0.0141)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [830/845]  eta: 0:00:05  loss: 0.8372 (1.0619)  loss_n_40: 0.2240 (0.2534)  loss_n_60: 0.2094 (0.2389)  loss_n_80: 0.2111 (0.2490)  loss_n_100: 0.2122 (0.2491)  triple_100: 0.0000 (0.0044)  triple_80: 0.0000 (0.0285)  triple_60: 0.0000 (0.0248)  triple_40: 0.0000 (0.0139)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [840/845]  eta: 0:00:01  loss: 0.7704 (1.0597)  loss_n_40: 0.2020 (0.2528)  loss_n_60: 0.1865 (0.2386)  loss_n_80: 0.1968 (0.2487)  loss_n_100: 0.1968 (0.2489)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0281)  triple_60: 0.0000 (0.0245)  triple_40: 0.0000 (0.0138)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19]  [844/845]  eta: 0:00:00  loss: 0.9021 (1.0595)  loss_n_40: 0.2244 (0.2528)  loss_n_60: 0.2130 (0.2386)  loss_n_80: 0.2292 (0.2488)  loss_n_100: 0.2129 (0.2490)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0280)  triple_60: 0.0000 (0.0243)  triple_40: 0.0000 (0.0137)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:19] Total time: 0:04:43 (0.3351 s / it)\n",
      "Averaged stats: loss: 0.9021 (1.0595)  loss_n_40: 0.2244 (0.2528)  loss_n_60: 0.2130 (0.2386)  loss_n_80: 0.2292 (0.2488)  loss_n_100: 0.2129 (0.2490)  triple_100: 0.0000 (0.0043)  triple_80: 0.0000 (0.0280)  triple_60: 0.0000 (0.0243)  triple_40: 0.0000 (0.0137)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_19_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.249%\n",
      "Min loss_n_100: 0.249\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:20]  [   0/1724]  eta: 2:00:09  lr: 0.000200  loss: 1.0749 (1.0749)  loss_n_40: 0.2649 (0.2649)  loss_n_60: 0.2655 (0.2655)  loss_n_80: 0.2760 (0.2760)  loss_n_100: 0.2684 (0.2684)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1817  data: 0.4275  max mem: 46473\n",
      "Train: [epoch:20]  [  10/1724]  eta: 1:52:41  lr: 0.000200  loss: 1.0998 (1.3096)  loss_n_40: 0.2696 (0.2808)  loss_n_60: 0.2567 (0.2592)  loss_n_80: 0.2760 (0.2746)  loss_n_100: 0.2821 (0.2754)  triple_100: 0.0000 (0.0339)  triple_80: 0.0000 (0.0715)  triple_60: 0.0000 (0.0740)  triple_40: 0.0000 (0.0402)  time: 3.9451  data: 0.0391  max mem: 46473\n",
      "Train: [epoch:20]  [  20/1724]  eta: 1:51:43  lr: 0.000200  loss: 1.3353 (1.4009)  loss_n_40: 0.3045 (0.3049)  loss_n_60: 0.3026 (0.2969)  loss_n_80: 0.3201 (0.3242)  loss_n_100: 0.3268 (0.3487)  triple_100: 0.0000 (0.0289)  triple_80: 0.0000 (0.0375)  triple_60: 0.0000 (0.0388)  triple_40: 0.0000 (0.0210)  time: 3.9216  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [  30/1724]  eta: 1:50:57  lr: 0.000200  loss: 1.6132 (1.9886)  loss_n_40: 0.3463 (0.3317)  loss_n_60: 0.3632 (0.3301)  loss_n_80: 0.4174 (0.3656)  loss_n_100: 0.4737 (0.3927)  triple_100: 0.0000 (0.1325)  triple_80: 0.0000 (0.1932)  triple_60: 0.0000 (0.1180)  triple_40: 0.0000 (0.1249)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [  40/1724]  eta: 1:50:13  lr: 0.000200  loss: 1.8019 (1.9422)  loss_n_40: 0.3642 (0.3502)  loss_n_60: 0.4069 (0.3521)  loss_n_80: 0.4544 (0.3894)  loss_n_100: 0.4842 (0.4207)  triple_100: 0.0000 (0.1002)  triple_80: 0.0000 (0.1460)  triple_60: 0.0000 (0.0892)  triple_40: 0.0000 (0.0944)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [  50/1724]  eta: 1:49:31  lr: 0.000200  loss: 1.6389 (1.8687)  loss_n_40: 0.3620 (0.3521)  loss_n_60: 0.3900 (0.3574)  loss_n_80: 0.4322 (0.3921)  loss_n_100: 0.4723 (0.4215)  triple_100: 0.0000 (0.0805)  triple_80: 0.0000 (0.1174)  triple_60: 0.0000 (0.0717)  triple_40: 0.0000 (0.0759)  time: 3.9190  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [  60/1724]  eta: 1:48:50  lr: 0.000200  loss: 1.4664 (1.7862)  loss_n_40: 0.3434 (0.3482)  loss_n_60: 0.3569 (0.3522)  loss_n_80: 0.3743 (0.3843)  loss_n_100: 0.4022 (0.4126)  triple_100: 0.0000 (0.0673)  triple_80: 0.0000 (0.0982)  triple_60: 0.0000 (0.0600)  triple_40: 0.0000 (0.0635)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [  70/1724]  eta: 1:48:10  lr: 0.000200  loss: 1.3180 (1.7278)  loss_n_40: 0.3113 (0.3471)  loss_n_60: 0.3098 (0.3495)  loss_n_80: 0.3357 (0.3793)  loss_n_100: 0.3380 (0.4038)  triple_100: 0.0000 (0.0579)  triple_80: 0.0000 (0.0843)  triple_60: 0.0000 (0.0515)  triple_40: 0.0000 (0.0545)  time: 3.9204  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [  80/1724]  eta: 1:47:29  lr: 0.000200  loss: 1.2286 (1.6615)  loss_n_40: 0.2987 (0.3399)  loss_n_60: 0.2996 (0.3417)  loss_n_80: 0.3124 (0.3699)  loss_n_100: 0.3169 (0.3924)  triple_100: 0.0000 (0.0507)  triple_80: 0.0000 (0.0739)  triple_60: 0.0000 (0.0451)  triple_40: 0.0000 (0.0478)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [  90/1724]  eta: 1:46:49  lr: 0.000200  loss: 1.1935 (1.6121)  loss_n_40: 0.2987 (0.3379)  loss_n_60: 0.2830 (0.3359)  loss_n_80: 0.3001 (0.3623)  loss_n_100: 0.2979 (0.3823)  triple_100: 0.0000 (0.0451)  triple_80: 0.0000 (0.0658)  triple_60: 0.0000 (0.0402)  triple_40: 0.0000 (0.0425)  time: 3.9177  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 100/1724]  eta: 1:46:10  lr: 0.000200  loss: 1.1935 (1.5720)  loss_n_40: 0.3044 (0.3349)  loss_n_60: 0.2859 (0.3314)  loss_n_80: 0.2975 (0.3567)  loss_n_100: 0.2979 (0.3745)  triple_100: 0.0000 (0.0407)  triple_80: 0.0000 (0.0593)  triple_60: 0.0000 (0.0362)  triple_40: 0.0000 (0.0383)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 110/1724]  eta: 1:45:30  lr: 0.000200  loss: 1.1844 (1.5361)  loss_n_40: 0.2994 (0.3318)  loss_n_60: 0.2842 (0.3273)  loss_n_80: 0.2927 (0.3509)  loss_n_100: 0.3000 (0.3674)  triple_100: 0.0000 (0.0370)  triple_80: 0.0000 (0.0539)  triple_60: 0.0000 (0.0329)  triple_40: 0.0000 (0.0349)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 120/1724]  eta: 1:44:51  lr: 0.000200  loss: 1.1575 (1.5056)  loss_n_40: 0.2894 (0.3294)  loss_n_60: 0.2837 (0.3235)  loss_n_80: 0.2898 (0.3457)  loss_n_100: 0.2956 (0.3614)  triple_100: 0.0000 (0.0339)  triple_80: 0.0000 (0.0495)  triple_60: 0.0000 (0.0302)  triple_40: 0.0000 (0.0320)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 130/1724]  eta: 1:44:11  lr: 0.000200  loss: 1.0955 (1.4739)  loss_n_40: 0.2856 (0.3254)  loss_n_60: 0.2649 (0.3189)  loss_n_80: 0.2692 (0.3401)  loss_n_100: 0.2772 (0.3549)  triple_100: 0.0000 (0.0314)  triple_80: 0.0000 (0.0457)  triple_60: 0.0000 (0.0279)  triple_40: 0.0000 (0.0296)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 140/1724]  eta: 1:43:32  lr: 0.000200  loss: 1.0391 (1.4452)  loss_n_40: 0.2639 (0.3216)  loss_n_60: 0.2475 (0.3145)  loss_n_80: 0.2634 (0.3350)  loss_n_100: 0.2610 (0.3491)  triple_100: 0.0000 (0.0291)  triple_80: 0.0000 (0.0425)  triple_60: 0.0000 (0.0259)  triple_40: 0.0000 (0.0275)  time: 3.9215  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [ 150/1724]  eta: 1:42:53  lr: 0.000200  loss: 1.0515 (1.4395)  loss_n_40: 0.2643 (0.3212)  loss_n_60: 0.2581 (0.3137)  loss_n_80: 0.2633 (0.3339)  loss_n_100: 0.2752 (0.3480)  triple_100: 0.0000 (0.0282)  triple_80: 0.0000 (0.0406)  triple_60: 0.0000 (0.0263)  triple_40: 0.0000 (0.0276)  time: 3.9215  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 160/1724]  eta: 1:42:13  lr: 0.000200  loss: 1.1642 (1.4226)  loss_n_40: 0.2779 (0.3187)  loss_n_60: 0.2761 (0.3113)  loss_n_80: 0.2967 (0.3315)  loss_n_100: 0.3179 (0.3459)  triple_100: 0.0000 (0.0264)  triple_80: 0.0000 (0.0381)  triple_60: 0.0000 (0.0247)  triple_40: 0.0000 (0.0259)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 170/1724]  eta: 1:41:34  lr: 0.000200  loss: 1.2096 (1.4188)  loss_n_40: 0.2990 (0.3193)  loss_n_60: 0.2916 (0.3119)  loss_n_80: 0.3088 (0.3323)  loss_n_100: 0.3257 (0.3470)  triple_100: 0.0000 (0.0249)  triple_80: 0.0000 (0.0358)  triple_60: 0.0000 (0.0232)  triple_40: 0.0000 (0.0244)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 180/1724]  eta: 1:40:55  lr: 0.000200  loss: 1.2509 (1.4067)  loss_n_40: 0.3054 (0.3183)  loss_n_60: 0.3003 (0.3105)  loss_n_80: 0.3193 (0.3306)  loss_n_100: 0.3273 (0.3449)  triple_100: 0.0000 (0.0235)  triple_80: 0.0000 (0.0339)  triple_60: 0.0000 (0.0219)  triple_40: 0.0000 (0.0230)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 190/1724]  eta: 1:40:15  lr: 0.000200  loss: 1.2299 (1.3976)  loss_n_40: 0.3030 (0.3189)  loss_n_60: 0.2770 (0.3095)  loss_n_80: 0.3102 (0.3294)  loss_n_100: 0.3093 (0.3428)  triple_100: 0.0000 (0.0223)  triple_80: 0.0000 (0.0321)  triple_60: 0.0000 (0.0208)  triple_40: 0.0000 (0.0218)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 200/1724]  eta: 1:39:36  lr: 0.000200  loss: 1.1328 (1.3818)  loss_n_40: 0.2860 (0.3168)  loss_n_60: 0.2693 (0.3071)  loss_n_80: 0.2865 (0.3265)  loss_n_100: 0.2864 (0.3392)  triple_100: 0.0000 (0.0212)  triple_80: 0.0000 (0.0305)  triple_60: 0.0000 (0.0198)  triple_40: 0.0000 (0.0207)  time: 3.9196  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 210/1724]  eta: 1:38:57  lr: 0.000200  loss: 1.0895 (1.3677)  loss_n_40: 0.2722 (0.3148)  loss_n_60: 0.2610 (0.3051)  loss_n_80: 0.2718 (0.3239)  loss_n_100: 0.2735 (0.3360)  triple_100: 0.0000 (0.0202)  triple_80: 0.0000 (0.0291)  triple_60: 0.0000 (0.0188)  triple_40: 0.0000 (0.0198)  time: 3.9198  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 220/1724]  eta: 1:38:17  lr: 0.000200  loss: 1.0605 (1.3564)  loss_n_40: 0.2639 (0.3136)  loss_n_60: 0.2593 (0.3037)  loss_n_80: 0.2645 (0.3219)  loss_n_100: 0.2627 (0.3334)  triple_100: 0.0000 (0.0193)  triple_80: 0.0000 (0.0277)  triple_60: 0.0000 (0.0180)  triple_40: 0.0000 (0.0189)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 230/1724]  eta: 1:37:38  lr: 0.000200  loss: 1.0336 (1.3451)  loss_n_40: 0.2617 (0.3122)  loss_n_60: 0.2507 (0.3022)  loss_n_80: 0.2580 (0.3198)  loss_n_100: 0.2575 (0.3308)  triple_100: 0.0000 (0.0184)  triple_80: 0.0000 (0.0265)  triple_60: 0.0000 (0.0172)  triple_40: 0.0000 (0.0180)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 240/1724]  eta: 1:36:58  lr: 0.000200  loss: 1.0814 (1.3374)  loss_n_40: 0.2736 (0.3116)  loss_n_60: 0.2637 (0.3012)  loss_n_80: 0.2698 (0.3186)  loss_n_100: 0.2752 (0.3292)  triple_100: 0.0000 (0.0177)  triple_80: 0.0000 (0.0254)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0173)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 250/1724]  eta: 1:36:19  lr: 0.000200  loss: 1.0624 (1.3238)  loss_n_40: 0.2736 (0.3095)  loss_n_60: 0.2614 (0.2988)  loss_n_80: 0.2639 (0.3156)  loss_n_100: 0.2636 (0.3261)  triple_100: 0.0000 (0.0170)  triple_80: 0.0000 (0.0244)  triple_60: 0.0000 (0.0158)  triple_40: 0.0000 (0.0166)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 260/1724]  eta: 1:35:40  lr: 0.000200  loss: 0.9997 (1.3122)  loss_n_40: 0.2485 (0.3077)  loss_n_60: 0.2341 (0.2969)  loss_n_80: 0.2377 (0.3134)  loss_n_100: 0.2463 (0.3234)  triple_100: 0.0000 (0.0163)  triple_80: 0.0000 (0.0235)  triple_60: 0.0000 (0.0152)  triple_40: 0.0000 (0.0160)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 270/1724]  eta: 1:35:01  lr: 0.000200  loss: 0.9376 (1.3014)  loss_n_40: 0.2434 (0.3063)  loss_n_60: 0.2283 (0.2947)  loss_n_80: 0.2358 (0.3110)  loss_n_100: 0.2408 (0.3210)  triple_100: 0.0000 (0.0157)  triple_80: 0.0000 (0.0226)  triple_60: 0.0000 (0.0147)  triple_40: 0.0000 (0.0154)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 280/1724]  eta: 1:34:21  lr: 0.000200  loss: 1.0516 (1.3077)  loss_n_40: 0.2744 (0.3067)  loss_n_60: 0.2511 (0.2953)  loss_n_80: 0.2506 (0.3120)  loss_n_100: 0.2552 (0.3221)  triple_100: 0.0000 (0.0158)  triple_80: 0.0000 (0.0218)  triple_60: 0.0000 (0.0141)  triple_40: 0.0000 (0.0199)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 290/1724]  eta: 1:33:42  lr: 0.000200  loss: 1.5581 (1.3225)  loss_n_40: 0.3252 (0.3073)  loss_n_60: 0.3580 (0.2988)  loss_n_80: 0.4076 (0.3175)  loss_n_100: 0.4458 (0.3297)  triple_100: 0.0000 (0.0152)  triple_80: 0.0000 (0.0211)  triple_60: 0.0000 (0.0136)  triple_40: 0.0000 (0.0193)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 300/1724]  eta: 1:33:03  lr: 0.000200  loss: 1.5526 (1.3299)  loss_n_40: 0.3252 (0.3080)  loss_n_60: 0.3638 (0.3005)  loss_n_80: 0.4262 (0.3200)  loss_n_100: 0.4650 (0.3330)  triple_100: 0.0000 (0.0147)  triple_80: 0.0000 (0.0219)  triple_60: 0.0000 (0.0132)  triple_40: 0.0000 (0.0186)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 310/1724]  eta: 1:32:23  lr: 0.000200  loss: 1.3205 (1.3259)  loss_n_40: 0.2897 (0.3070)  loss_n_60: 0.3088 (0.3000)  loss_n_80: 0.3317 (0.3198)  loss_n_100: 0.3663 (0.3329)  triple_100: 0.0000 (0.0143)  triple_80: 0.0000 (0.0212)  triple_60: 0.0000 (0.0128)  triple_40: 0.0000 (0.0180)  time: 3.9189  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 320/1724]  eta: 1:31:44  lr: 0.000200  loss: 1.2275 (1.3220)  loss_n_40: 0.2733 (0.3062)  loss_n_60: 0.2845 (0.2994)  loss_n_80: 0.3102 (0.3195)  loss_n_100: 0.3174 (0.3327)  triple_100: 0.0000 (0.0138)  triple_80: 0.0000 (0.0206)  triple_60: 0.0000 (0.0124)  triple_40: 0.0000 (0.0175)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 330/1724]  eta: 1:31:05  lr: 0.000200  loss: 1.2487 (1.3195)  loss_n_40: 0.3045 (0.3058)  loss_n_60: 0.3005 (0.2994)  loss_n_80: 0.3160 (0.3195)  loss_n_100: 0.3165 (0.3325)  triple_100: 0.0000 (0.0134)  triple_80: 0.0000 (0.0199)  triple_60: 0.0000 (0.0120)  triple_40: 0.0000 (0.0169)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 340/1724]  eta: 1:30:26  lr: 0.000200  loss: 1.1425 (1.3131)  loss_n_40: 0.2782 (0.3047)  loss_n_60: 0.2731 (0.2982)  loss_n_80: 0.2906 (0.3184)  loss_n_100: 0.3025 (0.3313)  triple_100: 0.0000 (0.0130)  triple_80: 0.0000 (0.0194)  triple_60: 0.0000 (0.0116)  triple_40: 0.0000 (0.0164)  time: 3.9192  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 350/1724]  eta: 1:29:46  lr: 0.000200  loss: 1.1111 (1.3076)  loss_n_40: 0.2580 (0.3037)  loss_n_60: 0.2658 (0.2974)  loss_n_80: 0.2834 (0.3175)  loss_n_100: 0.2971 (0.3302)  triple_100: 0.0000 (0.0126)  triple_80: 0.0000 (0.0188)  triple_60: 0.0000 (0.0113)  triple_40: 0.0000 (0.0160)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 360/1724]  eta: 1:29:07  lr: 0.000200  loss: 1.0275 (1.2976)  loss_n_40: 0.2530 (0.3020)  loss_n_60: 0.2349 (0.2954)  loss_n_80: 0.2612 (0.3153)  loss_n_100: 0.2617 (0.3278)  triple_100: 0.0000 (0.0123)  triple_80: 0.0000 (0.0183)  triple_60: 0.0000 (0.0110)  triple_40: 0.0000 (0.0155)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 370/1724]  eta: 1:28:28  lr: 0.000200  loss: 1.0275 (1.2953)  loss_n_40: 0.2373 (0.3009)  loss_n_60: 0.2349 (0.2948)  loss_n_80: 0.2612 (0.3149)  loss_n_100: 0.2629 (0.3278)  triple_100: 0.0000 (0.0122)  triple_80: 0.0000 (0.0183)  triple_60: 0.0000 (0.0111)  triple_40: 0.0000 (0.0152)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 380/1724]  eta: 1:27:49  lr: 0.000200  loss: 1.1821 (1.2949)  loss_n_40: 0.2582 (0.3008)  loss_n_60: 0.2771 (0.2950)  loss_n_80: 0.3160 (0.3153)  loss_n_100: 0.3331 (0.3284)  triple_100: 0.0000 (0.0119)  triple_80: 0.0000 (0.0178)  triple_60: 0.0000 (0.0108)  triple_40: 0.0000 (0.0148)  time: 3.9209  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [ 390/1724]  eta: 1:27:09  lr: 0.000200  loss: 1.2581 (1.2925)  loss_n_40: 0.2998 (0.3006)  loss_n_60: 0.3000 (0.2948)  loss_n_80: 0.3201 (0.3151)  loss_n_100: 0.3325 (0.3281)  triple_100: 0.0000 (0.0116)  triple_80: 0.0000 (0.0173)  triple_60: 0.0000 (0.0105)  triple_40: 0.0000 (0.0145)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 400/1724]  eta: 1:26:30  lr: 0.000200  loss: 1.1854 (1.2903)  loss_n_40: 0.2962 (0.3004)  loss_n_60: 0.2892 (0.2946)  loss_n_80: 0.2985 (0.3149)  loss_n_100: 0.3095 (0.3279)  triple_100: 0.0000 (0.0113)  triple_80: 0.0000 (0.0169)  triple_60: 0.0000 (0.0103)  triple_40: 0.0000 (0.0141)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 410/1724]  eta: 1:25:51  lr: 0.000200  loss: 1.0409 (1.2829)  loss_n_40: 0.2575 (0.2991)  loss_n_60: 0.2474 (0.2932)  loss_n_80: 0.2604 (0.3132)  loss_n_100: 0.2719 (0.3261)  triple_100: 0.0000 (0.0110)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0100)  triple_40: 0.0000 (0.0138)  time: 3.9208  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 420/1724]  eta: 1:25:12  lr: 0.000200  loss: 1.0409 (1.2804)  loss_n_40: 0.2730 (0.2986)  loss_n_60: 0.2503 (0.2927)  loss_n_80: 0.2604 (0.3126)  loss_n_100: 0.2667 (0.3254)  triple_100: 0.0000 (0.0108)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0098)  triple_40: 0.0000 (0.0145)  time: 3.9222  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 430/1724]  eta: 1:24:33  lr: 0.000200  loss: 1.1008 (1.2782)  loss_n_40: 0.2790 (0.2982)  loss_n_60: 0.2637 (0.2925)  loss_n_80: 0.2771 (0.3124)  loss_n_100: 0.2946 (0.3252)  triple_100: 0.0000 (0.0105)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0096)  triple_40: 0.0000 (0.0142)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 440/1724]  eta: 1:23:54  lr: 0.000200  loss: 0.9494 (1.2695)  loss_n_40: 0.2206 (0.2964)  loss_n_60: 0.2249 (0.2907)  loss_n_80: 0.2447 (0.3104)  loss_n_100: 0.2535 (0.3231)  triple_100: 0.0000 (0.0103)  triple_80: 0.0000 (0.0154)  triple_60: 0.0000 (0.0094)  triple_40: 0.0000 (0.0139)  time: 3.9223  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 450/1724]  eta: 1:23:14  lr: 0.000200  loss: 0.9011 (1.2639)  loss_n_40: 0.2126 (0.2954)  loss_n_60: 0.2137 (0.2897)  loss_n_80: 0.2281 (0.3092)  loss_n_100: 0.2471 (0.3219)  triple_100: 0.0000 (0.0100)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0135)  time: 3.9220  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 460/1724]  eta: 1:22:35  lr: 0.000200  loss: 0.9266 (1.2579)  loss_n_40: 0.2203 (0.2943)  loss_n_60: 0.2158 (0.2886)  loss_n_80: 0.2335 (0.3079)  loss_n_100: 0.2471 (0.3205)  triple_100: 0.0000 (0.0098)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0133)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 470/1724]  eta: 1:21:56  lr: 0.000200  loss: 0.9074 (1.2519)  loss_n_40: 0.2225 (0.2930)  loss_n_60: 0.2143 (0.2873)  loss_n_80: 0.2254 (0.3063)  loss_n_100: 0.2355 (0.3189)  triple_100: 0.0000 (0.0096)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0091)  triple_40: 0.0000 (0.0130)  time: 3.9203  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 480/1724]  eta: 1:21:17  lr: 0.000200  loss: 0.9071 (1.2449)  loss_n_40: 0.2249 (0.2917)  loss_n_60: 0.2143 (0.2858)  loss_n_80: 0.2248 (0.3048)  loss_n_100: 0.2267 (0.3173)  triple_100: 0.0000 (0.0094)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0089)  triple_40: 0.0000 (0.0128)  time: 3.9210  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 490/1724]  eta: 1:20:38  lr: 0.000200  loss: 0.9268 (1.2397)  loss_n_40: 0.2292 (0.2907)  loss_n_60: 0.2165 (0.2847)  loss_n_80: 0.2339 (0.3036)  loss_n_100: 0.2556 (0.3162)  triple_100: 0.0000 (0.0092)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0087)  triple_40: 0.0000 (0.0125)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 500/1724]  eta: 1:19:58  lr: 0.000200  loss: 0.9667 (1.2337)  loss_n_40: 0.2380 (0.2898)  loss_n_60: 0.2238 (0.2835)  loss_n_80: 0.2400 (0.3022)  loss_n_100: 0.2556 (0.3146)  triple_100: 0.0000 (0.0090)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0085)  triple_40: 0.0000 (0.0123)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 510/1724]  eta: 1:19:19  lr: 0.000200  loss: 0.9382 (1.2290)  loss_n_40: 0.2396 (0.2892)  loss_n_60: 0.2204 (0.2826)  loss_n_80: 0.2263 (0.3011)  loss_n_100: 0.2345 (0.3134)  triple_100: 0.0000 (0.0089)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0083)  triple_40: 0.0000 (0.0120)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 520/1724]  eta: 1:18:40  lr: 0.000200  loss: 0.8964 (1.2228)  loss_n_40: 0.2213 (0.2880)  loss_n_60: 0.2160 (0.2813)  loss_n_80: 0.2263 (0.2996)  loss_n_100: 0.2345 (0.3119)  triple_100: 0.0000 (0.0087)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0118)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 530/1724]  eta: 1:18:01  lr: 0.000200  loss: 0.9552 (1.2188)  loss_n_40: 0.2310 (0.2875)  loss_n_60: 0.2261 (0.2806)  loss_n_80: 0.2414 (0.2987)  loss_n_100: 0.2550 (0.3110)  triple_100: 0.0000 (0.0085)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0116)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 540/1724]  eta: 1:17:22  lr: 0.000200  loss: 1.0189 (1.2142)  loss_n_40: 0.2573 (0.2868)  loss_n_60: 0.2438 (0.2797)  loss_n_80: 0.2536 (0.2976)  loss_n_100: 0.2630 (0.3097)  triple_100: 0.0000 (0.0084)  triple_80: 0.0000 (0.0128)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0114)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 550/1724]  eta: 1:16:42  lr: 0.000200  loss: 0.8558 (1.2081)  loss_n_40: 0.2148 (0.2857)  loss_n_60: 0.2050 (0.2785)  loss_n_80: 0.2137 (0.2962)  loss_n_100: 0.2241 (0.3082)  triple_100: 0.0000 (0.0082)  triple_80: 0.0000 (0.0125)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0112)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 560/1724]  eta: 1:16:03  lr: 0.000200  loss: 0.8558 (1.2029)  loss_n_40: 0.2120 (0.2847)  loss_n_60: 0.2031 (0.2775)  loss_n_80: 0.2128 (0.2950)  loss_n_100: 0.2241 (0.3068)  triple_100: 0.0000 (0.0081)  triple_80: 0.0000 (0.0123)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0110)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 570/1724]  eta: 1:15:24  lr: 0.000200  loss: 0.9412 (1.2000)  loss_n_40: 0.2150 (0.2838)  loss_n_60: 0.2149 (0.2766)  loss_n_80: 0.2392 (0.2942)  loss_n_100: 0.2386 (0.3060)  triple_100: 0.0000 (0.0079)  triple_80: 0.0000 (0.0122)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0118)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 580/1724]  eta: 1:14:45  lr: 0.000200  loss: 0.9609 (1.1969)  loss_n_40: 0.2131 (0.2828)  loss_n_60: 0.2206 (0.2759)  loss_n_80: 0.2511 (0.2937)  loss_n_100: 0.2620 (0.3057)  triple_100: 0.0000 (0.0078)  triple_80: 0.0000 (0.0120)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0116)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 590/1724]  eta: 1:14:06  lr: 0.000200  loss: 0.9506 (1.1937)  loss_n_40: 0.2238 (0.2821)  loss_n_60: 0.2214 (0.2752)  loss_n_80: 0.2456 (0.2931)  loss_n_100: 0.2720 (0.3052)  triple_100: 0.0000 (0.0077)  triple_80: 0.0000 (0.0118)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0114)  time: 3.9218  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 600/1724]  eta: 1:13:27  lr: 0.000200  loss: 0.9425 (1.1892)  loss_n_40: 0.2244 (0.2811)  loss_n_60: 0.2195 (0.2743)  loss_n_80: 0.2384 (0.2921)  loss_n_100: 0.2553 (0.3043)  triple_100: 0.0000 (0.0075)  triple_80: 0.0000 (0.0116)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0112)  time: 3.9234  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 610/1724]  eta: 1:12:47  lr: 0.000200  loss: 0.9593 (1.1948)  loss_n_40: 0.2359 (0.2812)  loss_n_60: 0.2279 (0.2745)  loss_n_80: 0.2373 (0.2927)  loss_n_100: 0.2561 (0.3054)  triple_100: 0.0000 (0.0074)  triple_80: 0.0000 (0.0116)  triple_60: 0.0000 (0.0084)  triple_40: 0.0000 (0.0136)  time: 3.9247  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 620/1724]  eta: 1:12:08  lr: 0.000200  loss: 2.0789 (1.2387)  loss_n_40: 0.3550 (0.2833)  loss_n_60: 0.4569 (0.2787)  loss_n_80: 0.5532 (0.2985)  loss_n_100: 0.5998 (0.3127)  triple_100: 0.0000 (0.0267)  triple_80: 0.0000 (0.0173)  triple_60: 0.0000 (0.0082)  triple_40: 0.0000 (0.0133)  time: 3.9245  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [ 630/1724]  eta: 1:11:29  lr: 0.000200  loss: 2.7182 (1.2658)  loss_n_40: 0.4448 (0.2874)  loss_n_60: 0.6002 (0.2849)  loss_n_80: 0.7443 (0.3062)  loss_n_100: 0.9100 (0.3227)  triple_100: 0.0000 (0.0263)  triple_80: 0.0000 (0.0171)  triple_60: 0.0000 (0.0081)  triple_40: 0.0000 (0.0131)  time: 3.9225  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 640/1724]  eta: 1:10:50  lr: 0.000200  loss: 2.6426 (1.2914)  loss_n_40: 0.5264 (0.2908)  loss_n_60: 0.6002 (0.2897)  loss_n_80: 0.6729 (0.3114)  loss_n_100: 0.7744 (0.3286)  triple_100: 0.0000 (0.0258)  triple_80: 0.0000 (0.0238)  triple_60: 0.0000 (0.0080)  triple_40: 0.0000 (0.0133)  time: 3.9205  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 650/1724]  eta: 1:10:11  lr: 0.000200  loss: 2.4590 (1.3123)  loss_n_40: 0.5184 (0.2941)  loss_n_60: 0.5913 (0.2945)  loss_n_80: 0.6453 (0.3165)  loss_n_100: 0.7105 (0.3341)  triple_100: 0.0000 (0.0254)  triple_80: 0.0000 (0.0267)  triple_60: 0.0000 (0.0079)  triple_40: 0.0000 (0.0131)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 660/1724]  eta: 1:09:31  lr: 0.000200  loss: 2.1690 (1.3248)  loss_n_40: 0.4382 (0.2965)  loss_n_60: 0.5564 (0.2979)  loss_n_80: 0.5916 (0.3203)  loss_n_100: 0.6267 (0.3381)  triple_100: 0.0000 (0.0251)  triple_80: 0.0000 (0.0263)  triple_60: 0.0000 (0.0077)  triple_40: 0.0000 (0.0129)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 670/1724]  eta: 1:08:52  lr: 0.000200  loss: 1.9367 (1.3309)  loss_n_40: 0.4164 (0.2980)  loss_n_60: 0.4630 (0.2998)  loss_n_80: 0.5337 (0.3224)  loss_n_100: 0.5592 (0.3398)  triple_100: 0.0000 (0.0247)  triple_80: 0.0000 (0.0259)  triple_60: 0.0000 (0.0076)  triple_40: 0.0000 (0.0128)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 680/1724]  eta: 1:08:13  lr: 0.000200  loss: 1.5312 (1.3350)  loss_n_40: 0.3509 (0.2997)  loss_n_60: 0.3608 (0.3009)  loss_n_80: 0.4150 (0.3238)  loss_n_100: 0.4035 (0.3407)  triple_100: 0.0000 (0.0243)  triple_80: 0.0000 (0.0255)  triple_60: 0.0000 (0.0075)  triple_40: 0.0000 (0.0126)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 690/1724]  eta: 1:07:34  lr: 0.000200  loss: 1.4615 (1.3374)  loss_n_40: 0.3411 (0.3006)  loss_n_60: 0.3541 (0.3019)  loss_n_80: 0.3908 (0.3248)  loss_n_100: 0.3705 (0.3412)  triple_100: 0.0000 (0.0240)  triple_80: 0.0000 (0.0251)  triple_60: 0.0000 (0.0074)  triple_40: 0.0000 (0.0124)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 700/1724]  eta: 1:06:55  lr: 0.000200  loss: 1.4699 (1.3401)  loss_n_40: 0.3510 (0.3016)  loss_n_60: 0.3581 (0.3029)  loss_n_80: 0.3846 (0.3258)  loss_n_100: 0.3689 (0.3419)  triple_100: 0.0000 (0.0236)  triple_80: 0.0000 (0.0248)  triple_60: 0.0000 (0.0073)  triple_40: 0.0000 (0.0122)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 710/1724]  eta: 1:06:15  lr: 0.000200  loss: 1.5030 (1.3418)  loss_n_40: 0.3618 (0.3025)  loss_n_60: 0.3632 (0.3035)  loss_n_80: 0.3855 (0.3265)  loss_n_100: 0.3910 (0.3423)  triple_100: 0.0000 (0.0233)  triple_80: 0.0000 (0.0244)  triple_60: 0.0000 (0.0072)  triple_40: 0.0000 (0.0120)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 720/1724]  eta: 1:05:36  lr: 0.000200  loss: 1.4536 (1.3427)  loss_n_40: 0.3657 (0.3031)  loss_n_60: 0.3440 (0.3040)  loss_n_80: 0.3730 (0.3270)  loss_n_100: 0.3649 (0.3426)  triple_100: 0.0000 (0.0230)  triple_80: 0.0000 (0.0241)  triple_60: 0.0000 (0.0071)  triple_40: 0.0000 (0.0119)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 730/1724]  eta: 1:04:57  lr: 0.000200  loss: 1.4536 (1.3444)  loss_n_40: 0.3478 (0.3041)  loss_n_60: 0.3440 (0.3046)  loss_n_80: 0.3525 (0.3275)  loss_n_100: 0.3649 (0.3430)  triple_100: 0.0000 (0.0227)  triple_80: 0.0000 (0.0238)  triple_60: 0.0000 (0.0070)  triple_40: 0.0000 (0.0117)  time: 3.9207  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 740/1724]  eta: 1:04:18  lr: 0.000200  loss: 1.3847 (1.3441)  loss_n_40: 0.3423 (0.3045)  loss_n_60: 0.3256 (0.3047)  loss_n_80: 0.3530 (0.3277)  loss_n_100: 0.3708 (0.3429)  triple_100: 0.0000 (0.0224)  triple_80: 0.0000 (0.0234)  triple_60: 0.0000 (0.0069)  triple_40: 0.0000 (0.0115)  time: 3.9212  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 750/1724]  eta: 1:03:39  lr: 0.000200  loss: 1.3847 (1.3453)  loss_n_40: 0.3396 (0.3053)  loss_n_60: 0.3256 (0.3050)  loss_n_80: 0.3609 (0.3282)  loss_n_100: 0.3612 (0.3433)  triple_100: 0.0000 (0.0221)  triple_80: 0.0000 (0.0232)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0114)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 760/1724]  eta: 1:02:59  lr: 0.000200  loss: 1.2802 (1.3442)  loss_n_40: 0.3180 (0.3055)  loss_n_60: 0.2943 (0.3050)  loss_n_80: 0.3273 (0.3281)  loss_n_100: 0.3406 (0.3430)  triple_100: 0.0000 (0.0218)  triple_80: 0.0000 (0.0229)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0112)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 770/1724]  eta: 1:02:20  lr: 0.000200  loss: 1.1343 (1.3430)  loss_n_40: 0.2916 (0.3055)  loss_n_60: 0.2726 (0.3049)  loss_n_80: 0.2834 (0.3279)  loss_n_100: 0.2931 (0.3428)  triple_100: 0.0000 (0.0215)  triple_80: 0.0000 (0.0226)  triple_60: 0.0000 (0.0066)  triple_40: 0.0000 (0.0111)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 780/1724]  eta: 1:01:41  lr: 0.000200  loss: 1.1343 (1.3415)  loss_n_40: 0.2829 (0.3055)  loss_n_60: 0.2726 (0.3047)  loss_n_80: 0.2799 (0.3276)  loss_n_100: 0.2910 (0.3424)  triple_100: 0.0000 (0.0212)  triple_80: 0.0000 (0.0223)  triple_60: 0.0000 (0.0068)  triple_40: 0.0000 (0.0110)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 790/1724]  eta: 1:01:02  lr: 0.000200  loss: 1.1230 (1.3384)  loss_n_40: 0.2786 (0.3052)  loss_n_60: 0.2730 (0.3042)  loss_n_80: 0.2869 (0.3270)  loss_n_100: 0.2792 (0.3415)  triple_100: 0.0000 (0.0209)  triple_80: 0.0000 (0.0220)  triple_60: 0.0000 (0.0067)  triple_40: 0.0000 (0.0108)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 800/1724]  eta: 1:00:23  lr: 0.000200  loss: 1.1498 (1.3371)  loss_n_40: 0.2839 (0.3053)  loss_n_60: 0.2730 (0.3040)  loss_n_80: 0.2861 (0.3268)  loss_n_100: 0.2899 (0.3412)  triple_100: 0.0000 (0.0207)  triple_80: 0.0000 (0.0217)  triple_60: 0.0000 (0.0066)  triple_40: 0.0000 (0.0108)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 810/1724]  eta: 0:59:43  lr: 0.000200  loss: 1.1013 (1.3338)  loss_n_40: 0.2904 (0.3052)  loss_n_60: 0.2620 (0.3033)  loss_n_80: 0.2786 (0.3260)  loss_n_100: 0.2782 (0.3403)  triple_100: 0.0000 (0.0204)  triple_80: 0.0000 (0.0215)  triple_60: 0.0000 (0.0065)  triple_40: 0.0000 (0.0106)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 820/1724]  eta: 0:59:04  lr: 0.000200  loss: 1.0635 (1.3313)  loss_n_40: 0.2712 (0.3048)  loss_n_60: 0.2558 (0.3029)  loss_n_80: 0.2612 (0.3256)  loss_n_100: 0.2642 (0.3397)  triple_100: 0.0000 (0.0202)  triple_80: 0.0000 (0.0212)  triple_60: 0.0000 (0.0064)  triple_40: 0.0000 (0.0105)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 830/1724]  eta: 0:58:25  lr: 0.000200  loss: 1.0111 (1.3277)  loss_n_40: 0.2676 (0.3043)  loss_n_60: 0.2390 (0.3022)  loss_n_80: 0.2612 (0.3248)  loss_n_100: 0.2642 (0.3388)  triple_100: 0.0000 (0.0199)  triple_80: 0.0000 (0.0210)  triple_60: 0.0000 (0.0064)  triple_40: 0.0000 (0.0104)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 840/1724]  eta: 0:57:46  lr: 0.000200  loss: 1.0797 (1.3283)  loss_n_40: 0.2685 (0.3043)  loss_n_60: 0.2584 (0.3021)  loss_n_80: 0.2768 (0.3248)  loss_n_100: 0.2759 (0.3390)  triple_100: 0.0000 (0.0199)  triple_80: 0.0000 (0.0209)  triple_60: 0.0000 (0.0065)  triple_40: 0.0000 (0.0107)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 850/1724]  eta: 0:57:06  lr: 0.000200  loss: 1.3385 (1.3293)  loss_n_40: 0.3114 (0.3048)  loss_n_60: 0.3022 (0.3026)  loss_n_80: 0.3341 (0.3253)  loss_n_100: 0.3595 (0.3394)  triple_100: 0.0000 (0.0197)  triple_80: 0.0000 (0.0207)  triple_60: 0.0000 (0.0065)  triple_40: 0.0000 (0.0106)  time: 3.9194  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 860/1724]  eta: 0:56:27  lr: 0.000200  loss: 1.3181 (1.3294)  loss_n_40: 0.3495 (0.3051)  loss_n_60: 0.3153 (0.3027)  loss_n_80: 0.3208 (0.3253)  loss_n_100: 0.3434 (0.3396)  triple_100: 0.0000 (0.0194)  triple_80: 0.0000 (0.0204)  triple_60: 0.0000 (0.0064)  triple_40: 0.0000 (0.0105)  time: 3.9193  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [ 870/1724]  eta: 0:55:48  lr: 0.000200  loss: 1.2693 (1.3286)  loss_n_40: 0.3291 (0.3052)  loss_n_60: 0.2971 (0.3027)  loss_n_80: 0.3137 (0.3252)  loss_n_100: 0.3330 (0.3394)  triple_100: 0.0000 (0.0192)  triple_80: 0.0000 (0.0202)  triple_60: 0.0000 (0.0063)  triple_40: 0.0000 (0.0103)  time: 3.9192  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 880/1724]  eta: 0:55:09  lr: 0.000200  loss: 1.1768 (1.3258)  loss_n_40: 0.3012 (0.3049)  loss_n_60: 0.2823 (0.3023)  loss_n_80: 0.2904 (0.3246)  loss_n_100: 0.2893 (0.3386)  triple_100: 0.0000 (0.0190)  triple_80: 0.0000 (0.0200)  triple_60: 0.0000 (0.0062)  triple_40: 0.0000 (0.0102)  time: 3.9190  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 890/1724]  eta: 0:54:30  lr: 0.000200  loss: 1.0369 (1.3229)  loss_n_40: 0.2650 (0.3045)  loss_n_60: 0.2483 (0.3017)  loss_n_80: 0.2613 (0.3240)  loss_n_100: 0.2683 (0.3379)  triple_100: 0.0000 (0.0188)  triple_80: 0.0000 (0.0198)  triple_60: 0.0000 (0.0062)  triple_40: 0.0000 (0.0101)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 900/1724]  eta: 0:53:50  lr: 0.000200  loss: 1.0369 (1.3208)  loss_n_40: 0.2641 (0.3043)  loss_n_60: 0.2483 (0.3014)  loss_n_80: 0.2576 (0.3235)  loss_n_100: 0.2683 (0.3374)  triple_100: 0.0000 (0.0186)  triple_80: 0.0000 (0.0195)  triple_60: 0.0000 (0.0061)  triple_40: 0.0000 (0.0100)  time: 3.9205  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 910/1724]  eta: 0:53:11  lr: 0.000200  loss: 1.0718 (1.3178)  loss_n_40: 0.2649 (0.3038)  loss_n_60: 0.2606 (0.3009)  loss_n_80: 0.2576 (0.3228)  loss_n_100: 0.2707 (0.3366)  triple_100: 0.0000 (0.0184)  triple_80: 0.0000 (0.0193)  triple_60: 0.0000 (0.0060)  triple_40: 0.0000 (0.0099)  time: 3.9211  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 920/1724]  eta: 0:52:32  lr: 0.000200  loss: 1.0549 (1.3177)  loss_n_40: 0.2555 (0.3038)  loss_n_60: 0.2446 (0.3007)  loss_n_80: 0.2689 (0.3226)  loss_n_100: 0.2729 (0.3363)  triple_100: 0.0000 (0.0182)  triple_80: 0.0000 (0.0191)  triple_60: 0.0000 (0.0060)  triple_40: 0.0000 (0.0111)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 930/1724]  eta: 0:51:53  lr: 0.000200  loss: 1.1689 (1.3177)  loss_n_40: 0.2741 (0.3038)  loss_n_60: 0.2740 (0.3009)  loss_n_80: 0.3036 (0.3228)  loss_n_100: 0.3020 (0.3365)  triple_100: 0.0000 (0.0180)  triple_80: 0.0000 (0.0189)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0110)  time: 3.9210  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 940/1724]  eta: 0:51:13  lr: 0.000200  loss: 1.2202 (1.3174)  loss_n_40: 0.3034 (0.3040)  loss_n_60: 0.2866 (0.3009)  loss_n_80: 0.3195 (0.3228)  loss_n_100: 0.3328 (0.3364)  triple_100: 0.0000 (0.0178)  triple_80: 0.0000 (0.0187)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0109)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 950/1724]  eta: 0:50:34  lr: 0.000200  loss: 1.1067 (1.3148)  loss_n_40: 0.2806 (0.3036)  loss_n_60: 0.2723 (0.3005)  loss_n_80: 0.2722 (0.3223)  loss_n_100: 0.2767 (0.3358)  triple_100: 0.0000 (0.0176)  triple_80: 0.0000 (0.0185)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0108)  time: 3.9178  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 960/1724]  eta: 0:49:55  lr: 0.000200  loss: 0.9617 (1.3114)  loss_n_40: 0.2390 (0.3029)  loss_n_60: 0.2292 (0.2998)  loss_n_80: 0.2501 (0.3215)  loss_n_100: 0.2553 (0.3350)  triple_100: 0.0000 (0.0174)  triple_80: 0.0000 (0.0183)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0107)  time: 3.9184  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 970/1724]  eta: 0:49:16  lr: 0.000200  loss: 0.9617 (1.3084)  loss_n_40: 0.2376 (0.3026)  loss_n_60: 0.2292 (0.2991)  loss_n_80: 0.2501 (0.3209)  loss_n_100: 0.2573 (0.3342)  triple_100: 0.0000 (0.0172)  triple_80: 0.0000 (0.0181)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0106)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [ 980/1724]  eta: 0:48:37  lr: 0.000200  loss: 1.0136 (1.3069)  loss_n_40: 0.2402 (0.3022)  loss_n_60: 0.2376 (0.2986)  loss_n_80: 0.2561 (0.3204)  loss_n_100: 0.2637 (0.3336)  triple_100: 0.0000 (0.0171)  triple_80: 0.0000 (0.0182)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0111)  time: 3.9197  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [ 990/1724]  eta: 0:47:57  lr: 0.000200  loss: 1.0885 (1.3091)  loss_n_40: 0.2539 (0.3017)  loss_n_60: 0.2483 (0.2982)  loss_n_80: 0.2758 (0.3200)  loss_n_100: 0.2923 (0.3335)  triple_100: 0.0000 (0.0182)  triple_80: 0.0000 (0.0194)  triple_60: 0.0000 (0.0065)  triple_40: 0.0000 (0.0116)  time: 3.9184  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1000/1724]  eta: 0:47:18  lr: 0.000200  loss: 1.4873 (1.3127)  loss_n_40: 0.2725 (0.3020)  loss_n_60: 0.2907 (0.2990)  loss_n_80: 0.3656 (0.3213)  loss_n_100: 0.4019 (0.3353)  triple_100: 0.0000 (0.0180)  triple_80: 0.0000 (0.0192)  triple_60: 0.0000 (0.0064)  triple_40: 0.0000 (0.0115)  time: 3.9180  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1010/1724]  eta: 0:46:39  lr: 0.000200  loss: 1.5768 (1.3144)  loss_n_40: 0.3339 (0.3024)  loss_n_60: 0.3753 (0.2995)  loss_n_80: 0.4140 (0.3219)  loss_n_100: 0.4602 (0.3359)  triple_100: 0.0000 (0.0178)  triple_80: 0.0000 (0.0190)  triple_60: 0.0000 (0.0063)  triple_40: 0.0000 (0.0113)  time: 3.9183  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1020/1724]  eta: 0:46:00  lr: 0.000200  loss: 1.4156 (1.3155)  loss_n_40: 0.3354 (0.3028)  loss_n_60: 0.3410 (0.3000)  loss_n_80: 0.3664 (0.3223)  loss_n_100: 0.3983 (0.3363)  triple_100: 0.0000 (0.0177)  triple_80: 0.0000 (0.0188)  triple_60: 0.0000 (0.0063)  triple_40: 0.0000 (0.0112)  time: 3.9182  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1030/1724]  eta: 0:45:20  lr: 0.000200  loss: 1.3341 (1.3149)  loss_n_40: 0.3240 (0.3030)  loss_n_60: 0.3107 (0.3000)  loss_n_80: 0.3390 (0.3222)  loss_n_100: 0.3517 (0.3362)  triple_100: 0.0000 (0.0175)  triple_80: 0.0000 (0.0187)  triple_60: 0.0000 (0.0062)  triple_40: 0.0000 (0.0111)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1040/1724]  eta: 0:44:41  lr: 0.000200  loss: 1.1703 (1.3139)  loss_n_40: 0.2818 (0.3030)  loss_n_60: 0.2902 (0.2999)  loss_n_80: 0.2820 (0.3221)  loss_n_100: 0.2922 (0.3360)  triple_100: 0.0000 (0.0173)  triple_80: 0.0000 (0.0185)  triple_60: 0.0000 (0.0062)  triple_40: 0.0000 (0.0110)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1050/1724]  eta: 0:44:02  lr: 0.000200  loss: 1.1096 (1.3127)  loss_n_40: 0.2744 (0.3029)  loss_n_60: 0.2706 (0.2998)  loss_n_80: 0.2794 (0.3219)  loss_n_100: 0.2938 (0.3357)  triple_100: 0.0000 (0.0172)  triple_80: 0.0000 (0.0183)  triple_60: 0.0000 (0.0061)  triple_40: 0.0000 (0.0109)  time: 3.9193  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1060/1724]  eta: 0:43:23  lr: 0.000200  loss: 1.2456 (1.3126)  loss_n_40: 0.2903 (0.3029)  loss_n_60: 0.2923 (0.2998)  loss_n_80: 0.3063 (0.3218)  loss_n_100: 0.3162 (0.3355)  triple_100: 0.0000 (0.0170)  triple_80: 0.0000 (0.0185)  triple_60: 0.0000 (0.0061)  triple_40: 0.0000 (0.0112)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1070/1724]  eta: 0:42:44  lr: 0.000200  loss: 1.0817 (1.3099)  loss_n_40: 0.2502 (0.3023)  loss_n_60: 0.2592 (0.2993)  loss_n_80: 0.2756 (0.3212)  loss_n_100: 0.2845 (0.3349)  triple_100: 0.0000 (0.0168)  triple_80: 0.0000 (0.0183)  triple_60: 0.0000 (0.0060)  triple_40: 0.0000 (0.0110)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1080/1724]  eta: 0:42:04  lr: 0.000200  loss: 1.0672 (1.3084)  loss_n_40: 0.2494 (0.3021)  loss_n_60: 0.2592 (0.2991)  loss_n_80: 0.2718 (0.3209)  loss_n_100: 0.2705 (0.3346)  triple_100: 0.0000 (0.0167)  triple_80: 0.0000 (0.0182)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0109)  time: 3.9191  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1090/1724]  eta: 0:41:25  lr: 0.000200  loss: 1.0100 (1.3057)  loss_n_40: 0.2476 (0.3017)  loss_n_60: 0.2593 (0.2986)  loss_n_80: 0.2657 (0.3203)  loss_n_100: 0.2513 (0.3339)  triple_100: 0.0000 (0.0165)  triple_80: 0.0000 (0.0180)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0108)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1100/1724]  eta: 0:40:46  lr: 0.000200  loss: 0.9718 (1.3030)  loss_n_40: 0.2368 (0.3012)  loss_n_60: 0.2369 (0.2981)  loss_n_80: 0.2500 (0.3197)  loss_n_100: 0.2493 (0.3332)  triple_100: 0.0000 (0.0164)  triple_80: 0.0000 (0.0178)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0107)  time: 3.9212  data: 0.0002  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [1110/1724]  eta: 0:40:07  lr: 0.000200  loss: 1.0308 (1.3004)  loss_n_40: 0.2528 (0.3008)  loss_n_60: 0.2408 (0.2976)  loss_n_80: 0.2566 (0.3191)  loss_n_100: 0.2656 (0.3325)  triple_100: 0.0000 (0.0162)  triple_80: 0.0000 (0.0177)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0106)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1120/1724]  eta: 0:39:28  lr: 0.000200  loss: 1.0308 (1.2976)  loss_n_40: 0.2579 (0.3004)  loss_n_60: 0.2408 (0.2971)  loss_n_80: 0.2566 (0.3185)  loss_n_100: 0.2646 (0.3318)  triple_100: 0.0000 (0.0161)  triple_80: 0.0000 (0.0175)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0106)  time: 3.9210  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1130/1724]  eta: 0:38:48  lr: 0.000200  loss: 0.9562 (1.2949)  loss_n_40: 0.2590 (0.3000)  loss_n_60: 0.2339 (0.2965)  loss_n_80: 0.2413 (0.3178)  loss_n_100: 0.2470 (0.3311)  triple_100: 0.0000 (0.0160)  triple_80: 0.0000 (0.0174)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0105)  time: 3.9199  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1140/1724]  eta: 0:38:09  lr: 0.000200  loss: 1.1026 (1.2933)  loss_n_40: 0.2718 (0.2998)  loss_n_60: 0.2676 (0.2963)  loss_n_80: 0.2764 (0.3175)  loss_n_100: 0.2841 (0.3307)  triple_100: 0.0000 (0.0158)  triple_80: 0.0000 (0.0172)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0104)  time: 3.9193  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1150/1724]  eta: 0:37:30  lr: 0.000200  loss: 1.0977 (1.2915)  loss_n_40: 0.2769 (0.2996)  loss_n_60: 0.2632 (0.2960)  loss_n_80: 0.2764 (0.3171)  loss_n_100: 0.2841 (0.3302)  triple_100: 0.0000 (0.0157)  triple_80: 0.0000 (0.0170)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0103)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1160/1724]  eta: 0:36:51  lr: 0.000200  loss: 0.9557 (1.2887)  loss_n_40: 0.2390 (0.2992)  loss_n_60: 0.2262 (0.2954)  loss_n_80: 0.2324 (0.3164)  loss_n_100: 0.2406 (0.3295)  triple_100: 0.0000 (0.0155)  triple_80: 0.0000 (0.0169)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0102)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1170/1724]  eta: 0:36:12  lr: 0.000200  loss: 0.9855 (1.2872)  loss_n_40: 0.2578 (0.2989)  loss_n_60: 0.2293 (0.2949)  loss_n_80: 0.2451 (0.3158)  loss_n_100: 0.2406 (0.3289)  triple_100: 0.0000 (0.0156)  triple_80: 0.0000 (0.0168)  triple_60: 0.0000 (0.0060)  triple_40: 0.0000 (0.0103)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1180/1724]  eta: 0:35:32  lr: 0.000200  loss: 1.1075 (1.2862)  loss_n_40: 0.2692 (0.2987)  loss_n_60: 0.2567 (0.2948)  loss_n_80: 0.2744 (0.3157)  loss_n_100: 0.2890 (0.3288)  triple_100: 0.0000 (0.0155)  triple_80: 0.0000 (0.0166)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0102)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1190/1724]  eta: 0:34:53  lr: 0.000200  loss: 1.1620 (1.2849)  loss_n_40: 0.2779 (0.2986)  loss_n_60: 0.2744 (0.2946)  loss_n_80: 0.2881 (0.3154)  loss_n_100: 0.3120 (0.3285)  triple_100: 0.0000 (0.0154)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0101)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1200/1724]  eta: 0:34:14  lr: 0.000200  loss: 1.0092 (1.2831)  loss_n_40: 0.2528 (0.2983)  loss_n_60: 0.2564 (0.2944)  loss_n_80: 0.2607 (0.3150)  loss_n_100: 0.2681 (0.3281)  triple_100: 0.0000 (0.0152)  triple_80: 0.0000 (0.0163)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0100)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1210/1724]  eta: 0:33:35  lr: 0.000200  loss: 1.0671 (1.2820)  loss_n_40: 0.2643 (0.2983)  loss_n_60: 0.2568 (0.2943)  loss_n_80: 0.2663 (0.3148)  loss_n_100: 0.2798 (0.3278)  triple_100: 0.0000 (0.0151)  triple_80: 0.0000 (0.0162)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0099)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1220/1724]  eta: 0:32:56  lr: 0.000200  loss: 0.9923 (1.2794)  loss_n_40: 0.2517 (0.2978)  loss_n_60: 0.2465 (0.2938)  loss_n_80: 0.2512 (0.3142)  loss_n_100: 0.2557 (0.3271)  triple_100: 0.0000 (0.0150)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0098)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1230/1724]  eta: 0:32:16  lr: 0.000200  loss: 0.9685 (1.2786)  loss_n_40: 0.2321 (0.2974)  loss_n_60: 0.2358 (0.2934)  loss_n_80: 0.2457 (0.3136)  loss_n_100: 0.2442 (0.3265)  triple_100: 0.0000 (0.0149)  triple_80: 0.0000 (0.0165)  triple_60: 0.0000 (0.0059)  triple_40: 0.0000 (0.0105)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1240/1724]  eta: 0:31:37  lr: 0.000200  loss: 1.0620 (1.2774)  loss_n_40: 0.2519 (0.2971)  loss_n_60: 0.2556 (0.2932)  loss_n_80: 0.2668 (0.3135)  loss_n_100: 0.2683 (0.3263)  triple_100: 0.0000 (0.0147)  triple_80: 0.0000 (0.0164)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0104)  time: 3.9221  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1250/1724]  eta: 0:30:58  lr: 0.000200  loss: 1.1547 (1.2773)  loss_n_40: 0.2707 (0.2972)  loss_n_60: 0.2798 (0.2932)  loss_n_80: 0.3003 (0.3135)  loss_n_100: 0.3201 (0.3264)  triple_100: 0.0000 (0.0146)  triple_80: 0.0000 (0.0162)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0103)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1260/1724]  eta: 0:30:19  lr: 0.000200  loss: 1.1833 (1.2760)  loss_n_40: 0.2735 (0.2970)  loss_n_60: 0.2843 (0.2930)  loss_n_80: 0.3005 (0.3133)  loss_n_100: 0.3127 (0.3261)  triple_100: 0.0000 (0.0145)  triple_80: 0.0000 (0.0161)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0102)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1270/1724]  eta: 0:29:39  lr: 0.000200  loss: 1.0025 (1.2747)  loss_n_40: 0.2491 (0.2969)  loss_n_60: 0.2345 (0.2928)  loss_n_80: 0.2571 (0.3130)  loss_n_100: 0.2697 (0.3258)  triple_100: 0.0000 (0.0144)  triple_80: 0.0000 (0.0160)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0101)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1280/1724]  eta: 0:29:00  lr: 0.000200  loss: 1.0025 (1.2727)  loss_n_40: 0.2560 (0.2966)  loss_n_60: 0.2346 (0.2925)  loss_n_80: 0.2472 (0.3125)  loss_n_100: 0.2549 (0.3253)  triple_100: 0.0000 (0.0143)  triple_80: 0.0000 (0.0158)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0101)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1290/1724]  eta: 0:28:21  lr: 0.000200  loss: 1.0149 (1.2715)  loss_n_40: 0.2560 (0.2965)  loss_n_60: 0.2527 (0.2923)  loss_n_80: 0.2477 (0.3122)  loss_n_100: 0.2479 (0.3250)  triple_100: 0.0000 (0.0142)  triple_80: 0.0000 (0.0157)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0100)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1300/1724]  eta: 0:27:42  lr: 0.000200  loss: 1.0572 (1.2711)  loss_n_40: 0.2616 (0.2965)  loss_n_60: 0.2456 (0.2922)  loss_n_80: 0.2514 (0.3120)  loss_n_100: 0.2575 (0.3248)  triple_100: 0.0000 (0.0142)  triple_80: 0.0000 (0.0156)  triple_60: 0.0000 (0.0058)  triple_40: 0.0000 (0.0100)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1310/1724]  eta: 0:27:03  lr: 0.000200  loss: 1.1234 (1.2710)  loss_n_40: 0.2663 (0.2966)  loss_n_60: 0.2568 (0.2923)  loss_n_80: 0.2726 (0.3120)  loss_n_100: 0.3048 (0.3248)  triple_100: 0.0000 (0.0142)  triple_80: 0.0000 (0.0155)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0099)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1320/1724]  eta: 0:26:23  lr: 0.000200  loss: 1.1062 (1.2700)  loss_n_40: 0.2663 (0.2964)  loss_n_60: 0.2617 (0.2922)  loss_n_80: 0.2870 (0.3119)  loss_n_100: 0.3048 (0.3247)  triple_100: 0.0000 (0.0141)  triple_80: 0.0000 (0.0154)  triple_60: 0.0000 (0.0057)  triple_40: 0.0000 (0.0098)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1330/1724]  eta: 0:25:44  lr: 0.000200  loss: 1.0504 (1.2685)  loss_n_40: 0.2441 (0.2961)  loss_n_60: 0.2499 (0.2920)  loss_n_80: 0.2664 (0.3115)  loss_n_100: 0.2839 (0.3243)  triple_100: 0.0000 (0.0140)  triple_80: 0.0000 (0.0153)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0098)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1340/1724]  eta: 0:25:05  lr: 0.000200  loss: 1.0180 (1.2666)  loss_n_40: 0.2441 (0.2957)  loss_n_60: 0.2474 (0.2916)  loss_n_80: 0.2555 (0.3111)  loss_n_100: 0.2651 (0.3239)  triple_100: 0.0000 (0.0139)  triple_80: 0.0000 (0.0151)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0097)  time: 3.9204  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [1350/1724]  eta: 0:24:26  lr: 0.000200  loss: 1.0399 (1.2655)  loss_n_40: 0.2477 (0.2957)  loss_n_60: 0.2539 (0.2915)  loss_n_80: 0.2520 (0.3108)  loss_n_100: 0.2651 (0.3236)  triple_100: 0.0000 (0.0138)  triple_80: 0.0000 (0.0150)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0096)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1360/1724]  eta: 0:23:47  lr: 0.000200  loss: 1.0337 (1.2638)  loss_n_40: 0.2641 (0.2954)  loss_n_60: 0.2542 (0.2912)  loss_n_80: 0.2497 (0.3104)  loss_n_100: 0.2647 (0.3231)  triple_100: 0.0000 (0.0137)  triple_80: 0.0000 (0.0149)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0096)  time: 3.9193  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1370/1724]  eta: 0:23:07  lr: 0.000200  loss: 1.0075 (1.2619)  loss_n_40: 0.2500 (0.2951)  loss_n_60: 0.2504 (0.2909)  loss_n_80: 0.2497 (0.3100)  loss_n_100: 0.2465 (0.3226)  triple_100: 0.0000 (0.0136)  triple_80: 0.0000 (0.0148)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0095)  time: 3.9187  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1380/1724]  eta: 0:22:28  lr: 0.000200  loss: 1.0148 (1.2602)  loss_n_40: 0.2500 (0.2949)  loss_n_60: 0.2496 (0.2906)  loss_n_80: 0.2548 (0.3096)  loss_n_100: 0.2579 (0.3221)  triple_100: 0.0000 (0.0135)  triple_80: 0.0000 (0.0147)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0094)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1390/1724]  eta: 0:21:49  lr: 0.000200  loss: 0.9688 (1.2580)  loss_n_40: 0.2458 (0.2945)  loss_n_60: 0.2309 (0.2902)  loss_n_80: 0.2373 (0.3091)  loss_n_100: 0.2345 (0.3215)  triple_100: 0.0000 (0.0134)  triple_80: 0.0000 (0.0146)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0094)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1400/1724]  eta: 0:21:10  lr: 0.000200  loss: 0.9695 (1.2564)  loss_n_40: 0.2499 (0.2944)  loss_n_60: 0.2290 (0.2899)  loss_n_80: 0.2401 (0.3087)  loss_n_100: 0.2477 (0.3211)  triple_100: 0.0000 (0.0133)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0093)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1410/1724]  eta: 0:20:31  lr: 0.000200  loss: 0.9743 (1.2549)  loss_n_40: 0.2348 (0.2939)  loss_n_60: 0.2293 (0.2894)  loss_n_80: 0.2410 (0.3081)  loss_n_100: 0.2509 (0.3207)  triple_100: 0.0000 (0.0132)  triple_80: 0.0000 (0.0146)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0094)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1420/1724]  eta: 0:19:51  lr: 0.000200  loss: 1.0827 (1.2543)  loss_n_40: 0.2432 (0.2938)  loss_n_60: 0.2455 (0.2893)  loss_n_80: 0.2739 (0.3081)  loss_n_100: 0.2984 (0.3207)  triple_100: 0.0000 (0.0131)  triple_80: 0.0000 (0.0145)  triple_60: 0.0000 (0.0056)  triple_40: 0.0000 (0.0093)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1430/1724]  eta: 0:19:12  lr: 0.000200  loss: 1.1480 (1.2543)  loss_n_40: 0.2847 (0.2938)  loss_n_60: 0.2684 (0.2894)  loss_n_80: 0.2890 (0.3081)  loss_n_100: 0.3149 (0.3208)  triple_100: 0.0000 (0.0130)  triple_80: 0.0000 (0.0144)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0092)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1440/1724]  eta: 0:18:33  lr: 0.000200  loss: 1.1331 (1.2532)  loss_n_40: 0.2828 (0.2937)  loss_n_60: 0.2678 (0.2892)  loss_n_80: 0.2866 (0.3079)  loss_n_100: 0.2953 (0.3206)  triple_100: 0.0000 (0.0129)  triple_80: 0.0000 (0.0143)  triple_60: 0.0000 (0.0055)  triple_40: 0.0000 (0.0092)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1450/1724]  eta: 0:17:54  lr: 0.000200  loss: 1.0713 (1.2525)  loss_n_40: 0.2787 (0.2937)  loss_n_60: 0.2568 (0.2891)  loss_n_80: 0.2667 (0.3077)  loss_n_100: 0.2856 (0.3204)  triple_100: 0.0000 (0.0128)  triple_80: 0.0000 (0.0142)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0091)  time: 3.9203  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1460/1724]  eta: 0:17:15  lr: 0.000200  loss: 0.9841 (1.2505)  loss_n_40: 0.2506 (0.2933)  loss_n_60: 0.2382 (0.2887)  loss_n_80: 0.2468 (0.3073)  loss_n_100: 0.2531 (0.3199)  triple_100: 0.0000 (0.0127)  triple_80: 0.0000 (0.0141)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0090)  time: 3.9194  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1470/1724]  eta: 0:16:35  lr: 0.000200  loss: 0.9540 (1.2489)  loss_n_40: 0.2394 (0.2931)  loss_n_60: 0.2302 (0.2884)  loss_n_80: 0.2399 (0.3069)  loss_n_100: 0.2459 (0.3195)  triple_100: 0.0000 (0.0126)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0054)  triple_40: 0.0000 (0.0090)  time: 3.9200  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1480/1724]  eta: 0:15:56  lr: 0.000200  loss: 0.9482 (1.2473)  loss_n_40: 0.2304 (0.2927)  loss_n_60: 0.2285 (0.2880)  loss_n_80: 0.2362 (0.3064)  loss_n_100: 0.2416 (0.3189)  triple_100: 0.0000 (0.0126)  triple_80: 0.0000 (0.0140)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0093)  time: 3.9217  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1490/1724]  eta: 0:15:17  lr: 0.000200  loss: 0.9482 (1.2457)  loss_n_40: 0.2304 (0.2924)  loss_n_60: 0.2303 (0.2877)  loss_n_80: 0.2370 (0.3061)  loss_n_100: 0.2543 (0.3186)  triple_100: 0.0000 (0.0125)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0093)  time: 3.9219  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1500/1724]  eta: 0:14:38  lr: 0.000200  loss: 1.0160 (1.2441)  loss_n_40: 0.2470 (0.2921)  loss_n_60: 0.2457 (0.2874)  loss_n_80: 0.2550 (0.3057)  loss_n_100: 0.2659 (0.3182)  triple_100: 0.0000 (0.0124)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0053)  triple_40: 0.0000 (0.0092)  time: 3.9225  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1510/1724]  eta: 0:13:59  lr: 0.000200  loss: 0.9838 (1.2424)  loss_n_40: 0.2438 (0.2918)  loss_n_60: 0.2396 (0.2871)  loss_n_80: 0.2454 (0.3053)  loss_n_100: 0.2588 (0.3178)  triple_100: 0.0000 (0.0123)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0091)  time: 3.9238  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1520/1724]  eta: 0:13:19  lr: 0.000200  loss: 0.9838 (1.2409)  loss_n_40: 0.2438 (0.2915)  loss_n_60: 0.2374 (0.2869)  loss_n_80: 0.2447 (0.3050)  loss_n_100: 0.2494 (0.3174)  triple_100: 0.0000 (0.0122)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0091)  time: 3.9236  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1530/1724]  eta: 0:12:40  lr: 0.000200  loss: 1.0070 (1.2404)  loss_n_40: 0.2549 (0.2913)  loss_n_60: 0.2479 (0.2866)  loss_n_80: 0.2488 (0.3048)  loss_n_100: 0.2622 (0.3172)  triple_100: 0.0000 (0.0121)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0093)  time: 3.9223  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1540/1724]  eta: 0:12:01  lr: 0.000200  loss: 1.2001 (1.2420)  loss_n_40: 0.2794 (0.2914)  loss_n_60: 0.2812 (0.2870)  loss_n_80: 0.3077 (0.3053)  loss_n_100: 0.3251 (0.3179)  triple_100: 0.0000 (0.0122)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0092)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1550/1724]  eta: 0:11:22  lr: 0.000200  loss: 1.2512 (1.2422)  loss_n_40: 0.2859 (0.2914)  loss_n_60: 0.3021 (0.2871)  loss_n_80: 0.3194 (0.3055)  loss_n_100: 0.3512 (0.3182)  triple_100: 0.0000 (0.0121)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0092)  time: 3.9218  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1560/1724]  eta: 0:10:43  lr: 0.000200  loss: 1.0988 (1.2416)  loss_n_40: 0.2446 (0.2911)  loss_n_60: 0.2637 (0.2869)  loss_n_80: 0.2837 (0.3052)  loss_n_100: 0.3034 (0.3179)  triple_100: 0.0000 (0.0120)  triple_80: 0.0000 (0.0139)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0093)  time: 3.9226  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1570/1724]  eta: 0:10:03  lr: 0.000200  loss: 1.1288 (1.2415)  loss_n_40: 0.2392 (0.2910)  loss_n_60: 0.2661 (0.2869)  loss_n_80: 0.2888 (0.3053)  loss_n_100: 0.2985 (0.3181)  triple_100: 0.0000 (0.0119)  triple_80: 0.0000 (0.0138)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0092)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1580/1724]  eta: 0:09:24  lr: 0.000200  loss: 1.2700 (1.2424)  loss_n_40: 0.2835 (0.2911)  loss_n_60: 0.3083 (0.2872)  loss_n_80: 0.3327 (0.3057)  loss_n_100: 0.3461 (0.3185)  triple_100: 0.0000 (0.0119)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0092)  time: 3.9230  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:20]  [1590/1724]  eta: 0:08:45  lr: 0.000200  loss: 1.1465 (1.2419)  loss_n_40: 0.2762 (0.2910)  loss_n_60: 0.2776 (0.2872)  loss_n_80: 0.3017 (0.3057)  loss_n_100: 0.3200 (0.3185)  triple_100: 0.0000 (0.0118)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0091)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1600/1724]  eta: 0:08:06  lr: 0.000200  loss: 1.0969 (1.2414)  loss_n_40: 0.2604 (0.2909)  loss_n_60: 0.2629 (0.2871)  loss_n_80: 0.2853 (0.3057)  loss_n_100: 0.2909 (0.3184)  triple_100: 0.0000 (0.0117)  triple_80: 0.0000 (0.0136)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0091)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1610/1724]  eta: 0:07:26  lr: 0.000200  loss: 0.9365 (1.2396)  loss_n_40: 0.2278 (0.2906)  loss_n_60: 0.2344 (0.2868)  loss_n_80: 0.2383 (0.3053)  loss_n_100: 0.2361 (0.3179)  triple_100: 0.0000 (0.0116)  triple_80: 0.0000 (0.0135)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0090)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1620/1724]  eta: 0:06:47  lr: 0.000200  loss: 0.9499 (1.2389)  loss_n_40: 0.2309 (0.2905)  loss_n_60: 0.2304 (0.2867)  loss_n_80: 0.2440 (0.3051)  loss_n_100: 0.2437 (0.3177)  triple_100: 0.0000 (0.0116)  triple_80: 0.0000 (0.0134)  triple_60: 0.0000 (0.0050)  triple_40: 0.0000 (0.0090)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1630/1724]  eta: 0:06:08  lr: 0.000200  loss: 1.0074 (1.2374)  loss_n_40: 0.2570 (0.2903)  loss_n_60: 0.2472 (0.2864)  loss_n_80: 0.2540 (0.3048)  loss_n_100: 0.2538 (0.3173)  triple_100: 0.0000 (0.0115)  triple_80: 0.0000 (0.0133)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0089)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1640/1724]  eta: 0:05:29  lr: 0.000200  loss: 0.8663 (1.2354)  loss_n_40: 0.2239 (0.2899)  loss_n_60: 0.2102 (0.2860)  loss_n_80: 0.2213 (0.3043)  loss_n_100: 0.2259 (0.3167)  triple_100: 0.0000 (0.0114)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0088)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1650/1724]  eta: 0:04:50  lr: 0.000200  loss: 0.8663 (1.2338)  loss_n_40: 0.2213 (0.2897)  loss_n_60: 0.2055 (0.2857)  loss_n_80: 0.2146 (0.3040)  loss_n_100: 0.2256 (0.3163)  triple_100: 0.0000 (0.0113)  triple_80: 0.0000 (0.0132)  triple_60: 0.0000 (0.0049)  triple_40: 0.0000 (0.0088)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1660/1724]  eta: 0:04:10  lr: 0.000200  loss: 0.9168 (1.2328)  loss_n_40: 0.2493 (0.2896)  loss_n_60: 0.2262 (0.2855)  loss_n_80: 0.2299 (0.3037)  loss_n_100: 0.2309 (0.3160)  triple_100: 0.0000 (0.0113)  triple_80: 0.0000 (0.0131)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0087)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1670/1724]  eta: 0:03:31  lr: 0.000200  loss: 1.0052 (1.2315)  loss_n_40: 0.2542 (0.2894)  loss_n_60: 0.2299 (0.2852)  loss_n_80: 0.2418 (0.3034)  loss_n_100: 0.2575 (0.3157)  triple_100: 0.0000 (0.0112)  triple_80: 0.0000 (0.0130)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0088)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1680/1724]  eta: 0:02:52  lr: 0.000200  loss: 0.9867 (1.2300)  loss_n_40: 0.2267 (0.2890)  loss_n_60: 0.2245 (0.2848)  loss_n_80: 0.2435 (0.3031)  loss_n_100: 0.2601 (0.3154)  triple_100: 0.0000 (0.0111)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0088)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1690/1724]  eta: 0:02:13  lr: 0.000200  loss: 0.9377 (1.2287)  loss_n_40: 0.2114 (0.2887)  loss_n_60: 0.2235 (0.2846)  loss_n_80: 0.2435 (0.3029)  loss_n_100: 0.2607 (0.3151)  triple_100: 0.0000 (0.0111)  triple_80: 0.0000 (0.0129)  triple_60: 0.0000 (0.0048)  triple_40: 0.0000 (0.0087)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1700/1724]  eta: 0:01:34  lr: 0.000200  loss: 0.9702 (1.2294)  loss_n_40: 0.2410 (0.2885)  loss_n_60: 0.2196 (0.2843)  loss_n_80: 0.2452 (0.3026)  loss_n_100: 0.2453 (0.3148)  triple_100: 0.0000 (0.0117)  triple_80: 0.0000 (0.0137)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0087)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:20]  [1710/1724]  eta: 0:00:54  lr: 0.000200  loss: 1.2321 (1.2550)  loss_n_40: 0.2921 (0.2890)  loss_n_60: 0.2969 (0.2855)  loss_n_80: 0.3118 (0.3044)  loss_n_100: 0.3250 (0.3173)  triple_100: 0.0000 (0.0263)  triple_80: 0.0000 (0.0188)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0086)  time: 3.9215  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1720/1724]  eta: 0:00:15  lr: 0.000200  loss: 3.2183 (1.2674)  loss_n_40: 0.4393 (0.2903)  loss_n_60: 0.6217 (0.2883)  loss_n_80: 0.7882 (0.3081)  loss_n_100: 0.9718 (0.3222)  triple_100: 0.0000 (0.0261)  triple_80: 0.0000 (0.0187)  triple_60: 0.0000 (0.0052)  triple_40: 0.0000 (0.0086)  time: 3.9207  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20]  [1723/1724]  eta: 0:00:03  lr: 0.000200  loss: 3.2183 (1.2702)  loss_n_40: 0.4640 (0.2906)  loss_n_60: 0.6728 (0.2890)  loss_n_80: 0.8672 (0.3088)  loss_n_100: 1.0638 (0.3234)  triple_100: 0.0000 (0.0261)  triple_80: 0.0000 (0.0187)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0086)  time: 3.9206  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:20] Total time: 1:52:39 (3.9210 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 3.2183 (1.2702)  loss_n_40: 0.4640 (0.2906)  loss_n_60: 0.6728 (0.2890)  loss_n_80: 0.8672 (0.3088)  loss_n_100: 1.0638 (0.3234)  triple_100: 0.0000 (0.0261)  triple_80: 0.0000 (0.0187)  triple_60: 0.0000 (0.0051)  triple_40: 0.0000 (0.0086)\n",
      "Valid: [epoch:20]  [  0/845]  eta: 0:10:19  loss: 2.3167 (2.3167)  loss_n_40: 0.4164 (0.4164)  loss_n_60: 0.5215 (0.5215)  loss_n_80: 0.5854 (0.5854)  loss_n_100: 0.7934 (0.7934)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.7332  data: 0.3981  max mem: 46473\n",
      "Valid: [epoch:20]  [ 10/845]  eta: 0:05:09  loss: 2.5030 (2.6410)  loss_n_40: 0.4164 (0.4433)  loss_n_60: 0.5809 (0.6314)  loss_n_80: 0.6516 (0.6903)  loss_n_100: 0.8329 (0.8760)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3703  data: 0.0363  max mem: 46473\n",
      "Valid: [epoch:20]  [ 20/845]  eta: 0:04:51  loss: 2.5030 (2.6355)  loss_n_40: 0.4671 (0.4691)  loss_n_60: 0.6242 (0.6328)  loss_n_80: 0.6516 (0.6833)  loss_n_100: 0.8625 (0.8503)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3338  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [ 30/845]  eta: 0:04:42  loss: 2.6746 (2.7027)  loss_n_40: 0.4752 (0.4632)  loss_n_60: 0.6366 (0.6552)  loss_n_80: 0.7123 (0.7018)  loss_n_100: 0.8734 (0.8826)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [ 40/845]  eta: 0:04:36  loss: 2.9630 (2.7657)  loss_n_40: 0.4719 (0.4821)  loss_n_60: 0.7424 (0.6667)  loss_n_80: 0.7775 (0.7203)  loss_n_100: 0.9939 (0.8966)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [ 50/845]  eta: 0:04:31  loss: 2.5236 (2.6975)  loss_n_40: 0.4499 (0.4701)  loss_n_60: 0.6093 (0.6505)  loss_n_80: 0.6426 (0.7022)  loss_n_100: 0.8184 (0.8747)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [ 60/845]  eta: 0:04:27  loss: 2.4249 (2.6938)  loss_n_40: 0.4108 (0.4685)  loss_n_60: 0.5902 (0.6507)  loss_n_80: 0.6223 (0.6998)  loss_n_100: 0.8141 (0.8747)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [ 70/845]  eta: 0:04:23  loss: 2.6339 (2.7304)  loss_n_40: 0.4529 (0.4667)  loss_n_60: 0.6613 (0.6607)  loss_n_80: 0.6997 (0.7118)  loss_n_100: 0.9210 (0.8912)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:20]  [ 80/845]  eta: 0:04:19  loss: 2.8246 (2.7414)  loss_n_40: 0.4432 (0.4639)  loss_n_60: 0.6776 (0.6635)  loss_n_80: 0.7685 (0.7162)  loss_n_100: 0.9784 (0.8978)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [ 90/845]  eta: 0:04:15  loss: 2.4243 (2.7021)  loss_n_40: 0.4242 (0.4651)  loss_n_60: 0.5857 (0.6556)  loss_n_80: 0.6122 (0.7029)  loss_n_100: 0.8050 (0.8785)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3340  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [100/845]  eta: 0:04:11  loss: 2.3584 (2.6900)  loss_n_40: 0.4179 (0.4627)  loss_n_60: 0.5666 (0.6519)  loss_n_80: 0.6050 (0.7002)  loss_n_100: 0.7997 (0.8752)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [110/845]  eta: 0:04:08  loss: 2.4836 (2.6993)  loss_n_40: 0.4363 (0.4624)  loss_n_60: 0.6441 (0.6556)  loss_n_80: 0.6480 (0.7041)  loss_n_100: 0.8141 (0.8771)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [120/845]  eta: 0:04:04  loss: 2.8017 (2.7168)  loss_n_40: 0.4737 (0.4643)  loss_n_60: 0.6750 (0.6607)  loss_n_80: 0.7442 (0.7087)  loss_n_100: 0.9488 (0.8830)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [130/845]  eta: 0:04:01  loss: 2.6346 (2.7105)  loss_n_40: 0.4684 (0.4634)  loss_n_60: 0.6650 (0.6592)  loss_n_80: 0.6931 (0.7071)  loss_n_100: 0.8274 (0.8808)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [140/845]  eta: 0:03:57  loss: 2.5121 (2.7097)  loss_n_40: 0.4397 (0.4637)  loss_n_60: 0.6311 (0.6599)  loss_n_80: 0.6511 (0.7060)  loss_n_100: 0.8199 (0.8800)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [150/845]  eta: 0:03:54  loss: 2.5729 (2.7265)  loss_n_40: 0.4449 (0.4639)  loss_n_60: 0.6518 (0.6639)  loss_n_80: 0.6816 (0.7115)  loss_n_100: 0.8938 (0.8872)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [160/845]  eta: 0:03:50  loss: 2.5926 (2.7253)  loss_n_40: 0.4226 (0.4621)  loss_n_60: 0.6011 (0.6624)  loss_n_80: 0.6878 (0.7117)  loss_n_100: 0.8838 (0.8891)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [170/845]  eta: 0:03:47  loss: 2.6393 (2.7267)  loss_n_40: 0.4199 (0.4606)  loss_n_60: 0.6146 (0.6625)  loss_n_80: 0.6898 (0.7127)  loss_n_100: 0.8895 (0.8908)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [180/845]  eta: 0:03:43  loss: 2.6954 (2.7286)  loss_n_40: 0.4661 (0.4628)  loss_n_60: 0.6461 (0.6630)  loss_n_80: 0.7016 (0.7137)  loss_n_100: 0.9269 (0.8891)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [190/845]  eta: 0:03:40  loss: 2.6291 (2.7268)  loss_n_40: 0.4747 (0.4621)  loss_n_60: 0.6358 (0.6629)  loss_n_80: 0.6938 (0.7131)  loss_n_100: 0.8451 (0.8886)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [200/845]  eta: 0:03:36  loss: 2.4173 (2.7133)  loss_n_40: 0.4486 (0.4634)  loss_n_60: 0.5827 (0.6590)  loss_n_80: 0.6284 (0.7084)  loss_n_100: 0.8088 (0.8824)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [210/845]  eta: 0:03:33  loss: 2.3927 (2.7144)  loss_n_40: 0.4527 (0.4640)  loss_n_60: 0.5757 (0.6592)  loss_n_80: 0.6122 (0.7085)  loss_n_100: 0.7968 (0.8827)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [220/845]  eta: 0:03:30  loss: 2.5691 (2.7015)  loss_n_40: 0.4943 (0.4661)  loss_n_60: 0.6153 (0.6561)  loss_n_80: 0.6695 (0.7046)  loss_n_100: 0.7791 (0.8746)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [230/845]  eta: 0:03:26  loss: 2.3726 (2.6880)  loss_n_40: 0.4430 (0.4646)  loss_n_60: 0.5927 (0.6530)  loss_n_80: 0.6182 (0.7006)  loss_n_100: 0.7791 (0.8697)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [240/845]  eta: 0:03:23  loss: 2.5440 (2.6936)  loss_n_40: 0.4224 (0.4648)  loss_n_60: 0.5927 (0.6536)  loss_n_80: 0.6782 (0.7028)  loss_n_100: 0.8255 (0.8724)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [250/845]  eta: 0:03:19  loss: 2.6677 (2.6988)  loss_n_40: 0.4339 (0.4648)  loss_n_60: 0.6451 (0.6542)  loss_n_80: 0.6827 (0.7044)  loss_n_100: 0.8934 (0.8754)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [260/845]  eta: 0:03:16  loss: 2.4371 (2.6895)  loss_n_40: 0.4080 (0.4640)  loss_n_60: 0.5623 (0.6515)  loss_n_80: 0.6392 (0.7015)  loss_n_100: 0.8350 (0.8724)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [270/845]  eta: 0:03:13  loss: 2.4956 (2.6901)  loss_n_40: 0.4351 (0.4636)  loss_n_60: 0.5963 (0.6517)  loss_n_80: 0.6465 (0.7019)  loss_n_100: 0.8235 (0.8729)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [280/845]  eta: 0:03:09  loss: 2.5924 (2.6925)  loss_n_40: 0.4359 (0.4634)  loss_n_60: 0.6168 (0.6520)  loss_n_80: 0.6804 (0.7027)  loss_n_100: 0.8981 (0.8744)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [290/845]  eta: 0:03:06  loss: 2.6421 (2.6922)  loss_n_40: 0.4420 (0.4629)  loss_n_60: 0.6172 (0.6521)  loss_n_80: 0.6938 (0.7025)  loss_n_100: 0.8981 (0.8748)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [300/845]  eta: 0:03:02  loss: 2.6421 (2.6906)  loss_n_40: 0.4420 (0.4635)  loss_n_60: 0.6273 (0.6520)  loss_n_80: 0.6911 (0.7015)  loss_n_100: 0.8397 (0.8737)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [310/845]  eta: 0:02:59  loss: 2.5877 (2.6931)  loss_n_40: 0.4610 (0.4634)  loss_n_60: 0.6525 (0.6530)  loss_n_80: 0.6654 (0.7021)  loss_n_100: 0.8356 (0.8747)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [320/845]  eta: 0:02:56  loss: 2.6423 (2.6968)  loss_n_40: 0.4610 (0.4636)  loss_n_60: 0.6525 (0.6540)  loss_n_80: 0.7027 (0.7032)  loss_n_100: 0.8507 (0.8760)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:20]  [330/845]  eta: 0:02:52  loss: 2.6423 (2.6978)  loss_n_40: 0.4663 (0.4635)  loss_n_60: 0.6727 (0.6546)  loss_n_80: 0.7027 (0.7035)  loss_n_100: 0.8507 (0.8762)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [340/845]  eta: 0:02:49  loss: 2.9050 (2.7070)  loss_n_40: 0.4881 (0.4646)  loss_n_60: 0.7197 (0.6571)  loss_n_80: 0.7451 (0.7063)  loss_n_100: 0.9365 (0.8791)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [350/845]  eta: 0:02:46  loss: 2.9050 (2.7151)  loss_n_40: 0.4871 (0.4648)  loss_n_60: 0.7255 (0.6590)  loss_n_80: 0.7718 (0.7090)  loss_n_100: 0.9493 (0.8824)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [360/845]  eta: 0:02:42  loss: 2.8267 (2.7166)  loss_n_40: 0.4763 (0.4651)  loss_n_60: 0.6635 (0.6590)  loss_n_80: 0.7496 (0.7093)  loss_n_100: 0.9164 (0.8832)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [370/845]  eta: 0:02:39  loss: 2.5609 (2.7132)  loss_n_40: 0.4441 (0.4646)  loss_n_60: 0.6037 (0.6581)  loss_n_80: 0.6659 (0.7083)  loss_n_100: 0.8276 (0.8822)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [380/845]  eta: 0:02:35  loss: 2.3849 (2.7038)  loss_n_40: 0.4128 (0.4637)  loss_n_60: 0.5903 (0.6562)  loss_n_80: 0.6162 (0.7054)  loss_n_100: 0.7871 (0.8785)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [390/845]  eta: 0:02:32  loss: 2.4547 (2.7033)  loss_n_40: 0.4217 (0.4630)  loss_n_60: 0.5953 (0.6561)  loss_n_80: 0.6387 (0.7053)  loss_n_100: 0.8006 (0.8788)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [400/845]  eta: 0:02:29  loss: 2.5273 (2.7004)  loss_n_40: 0.4217 (0.4626)  loss_n_60: 0.5956 (0.6553)  loss_n_80: 0.6587 (0.7044)  loss_n_100: 0.8442 (0.8781)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [410/845]  eta: 0:02:25  loss: 2.6992 (2.7035)  loss_n_40: 0.4592 (0.4631)  loss_n_60: 0.6552 (0.6566)  loss_n_80: 0.7103 (0.7053)  loss_n_100: 0.8315 (0.8785)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [420/845]  eta: 0:02:22  loss: 2.7786 (2.7009)  loss_n_40: 0.4789 (0.4637)  loss_n_60: 0.6819 (0.6557)  loss_n_80: 0.7313 (0.7045)  loss_n_100: 0.8315 (0.8770)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [430/845]  eta: 0:02:19  loss: 2.5830 (2.7013)  loss_n_40: 0.4528 (0.4650)  loss_n_60: 0.6446 (0.6558)  loss_n_80: 0.6827 (0.7043)  loss_n_100: 0.8358 (0.8762)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [440/845]  eta: 0:02:15  loss: 2.5830 (2.7036)  loss_n_40: 0.4765 (0.4657)  loss_n_60: 0.6446 (0.6561)  loss_n_80: 0.6846 (0.7049)  loss_n_100: 0.8447 (0.8770)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [450/845]  eta: 0:02:12  loss: 2.7743 (2.7038)  loss_n_40: 0.4629 (0.4654)  loss_n_60: 0.6757 (0.6560)  loss_n_80: 0.7111 (0.7050)  loss_n_100: 0.8694 (0.8774)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [460/845]  eta: 0:02:09  loss: 2.7743 (2.7035)  loss_n_40: 0.4629 (0.4658)  loss_n_60: 0.6757 (0.6561)  loss_n_80: 0.7161 (0.7047)  loss_n_100: 0.8381 (0.8770)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [470/845]  eta: 0:02:05  loss: 2.6878 (2.7053)  loss_n_40: 0.4753 (0.4661)  loss_n_60: 0.6506 (0.6566)  loss_n_80: 0.7189 (0.7052)  loss_n_100: 0.9090 (0.8775)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [480/845]  eta: 0:02:02  loss: 2.8310 (2.7077)  loss_n_40: 0.4820 (0.4661)  loss_n_60: 0.7044 (0.6573)  loss_n_80: 0.7380 (0.7061)  loss_n_100: 0.9070 (0.8783)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [490/845]  eta: 0:01:59  loss: 2.5502 (2.7038)  loss_n_40: 0.4500 (0.4656)  loss_n_60: 0.6431 (0.6564)  loss_n_80: 0.6728 (0.7049)  loss_n_100: 0.8039 (0.8769)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [500/845]  eta: 0:01:55  loss: 2.5465 (2.7022)  loss_n_40: 0.4388 (0.4655)  loss_n_60: 0.6221 (0.6559)  loss_n_80: 0.6654 (0.7045)  loss_n_100: 0.8039 (0.8764)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [510/845]  eta: 0:01:52  loss: 2.6550 (2.7019)  loss_n_40: 0.4253 (0.4649)  loss_n_60: 0.6221 (0.6558)  loss_n_80: 0.7156 (0.7044)  loss_n_100: 0.8468 (0.8768)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [520/845]  eta: 0:01:48  loss: 2.4517 (2.6997)  loss_n_40: 0.4598 (0.4654)  loss_n_60: 0.6224 (0.6554)  loss_n_80: 0.6390 (0.7035)  loss_n_100: 0.8087 (0.8752)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [530/845]  eta: 0:01:45  loss: 2.8309 (2.7034)  loss_n_40: 0.4779 (0.4662)  loss_n_60: 0.6847 (0.6563)  loss_n_80: 0.7496 (0.7048)  loss_n_100: 0.8796 (0.8762)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [540/845]  eta: 0:01:42  loss: 2.8682 (2.7044)  loss_n_40: 0.4921 (0.4669)  loss_n_60: 0.6971 (0.6567)  loss_n_80: 0.7519 (0.7050)  loss_n_100: 0.8796 (0.8758)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3349  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [550/845]  eta: 0:01:38  loss: 2.6027 (2.7052)  loss_n_40: 0.4324 (0.4664)  loss_n_60: 0.6253 (0.6568)  loss_n_80: 0.6661 (0.7051)  loss_n_100: 0.8373 (0.8769)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [560/845]  eta: 0:01:35  loss: 2.4594 (2.7011)  loss_n_40: 0.4077 (0.4660)  loss_n_60: 0.6029 (0.6559)  loss_n_80: 0.6325 (0.7041)  loss_n_100: 0.8192 (0.8752)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [570/845]  eta: 0:01:32  loss: 2.3878 (2.6974)  loss_n_40: 0.4191 (0.4658)  loss_n_60: 0.5884 (0.6550)  loss_n_80: 0.6194 (0.7030)  loss_n_100: 0.7930 (0.8736)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:20]  [580/845]  eta: 0:01:28  loss: 2.3696 (2.6968)  loss_n_40: 0.4440 (0.4654)  loss_n_60: 0.5884 (0.6548)  loss_n_80: 0.6194 (0.7029)  loss_n_100: 0.7977 (0.8736)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [590/845]  eta: 0:01:25  loss: 2.5062 (2.6969)  loss_n_40: 0.4700 (0.4658)  loss_n_60: 0.6179 (0.6548)  loss_n_80: 0.6536 (0.7028)  loss_n_100: 0.8359 (0.8735)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [600/845]  eta: 0:01:22  loss: 2.6100 (2.6975)  loss_n_40: 0.4700 (0.4659)  loss_n_60: 0.6227 (0.6551)  loss_n_80: 0.6717 (0.7029)  loss_n_100: 0.8854 (0.8736)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [610/845]  eta: 0:01:18  loss: 2.6160 (2.6964)  loss_n_40: 0.4328 (0.4654)  loss_n_60: 0.6396 (0.6546)  loss_n_80: 0.6717 (0.7026)  loss_n_100: 0.8442 (0.8737)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [620/845]  eta: 0:01:15  loss: 2.5913 (2.6975)  loss_n_40: 0.4328 (0.4650)  loss_n_60: 0.6172 (0.6547)  loss_n_80: 0.6635 (0.7030)  loss_n_100: 0.8542 (0.8747)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [630/845]  eta: 0:01:12  loss: 2.9695 (2.7019)  loss_n_40: 0.4835 (0.4653)  loss_n_60: 0.7316 (0.6559)  loss_n_80: 0.7650 (0.7043)  loss_n_100: 0.9444 (0.8765)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [640/845]  eta: 0:01:08  loss: 3.2046 (2.7074)  loss_n_40: 0.4835 (0.4654)  loss_n_60: 0.7880 (0.6574)  loss_n_80: 0.8586 (0.7060)  loss_n_100: 1.0611 (0.8786)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [650/845]  eta: 0:01:05  loss: 3.1367 (2.7125)  loss_n_40: 0.4854 (0.4661)  loss_n_60: 0.7520 (0.6587)  loss_n_80: 0.8506 (0.7076)  loss_n_100: 1.0611 (0.8802)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [660/845]  eta: 0:01:01  loss: 2.8048 (2.7116)  loss_n_40: 0.4962 (0.4664)  loss_n_60: 0.6914 (0.6585)  loss_n_80: 0.7563 (0.7073)  loss_n_100: 0.8901 (0.8794)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [670/845]  eta: 0:00:58  loss: 2.4763 (2.7078)  loss_n_40: 0.4396 (0.4656)  loss_n_60: 0.6014 (0.6574)  loss_n_80: 0.6498 (0.7065)  loss_n_100: 0.8328 (0.8784)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [680/845]  eta: 0:00:55  loss: 2.3930 (2.7035)  loss_n_40: 0.4020 (0.4646)  loss_n_60: 0.5703 (0.6563)  loss_n_80: 0.6328 (0.7053)  loss_n_100: 0.8244 (0.8773)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [690/845]  eta: 0:00:51  loss: 2.4994 (2.7026)  loss_n_40: 0.4237 (0.4643)  loss_n_60: 0.5813 (0.6560)  loss_n_80: 0.6449 (0.7049)  loss_n_100: 0.7903 (0.8774)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [700/845]  eta: 0:00:48  loss: 2.5163 (2.7031)  loss_n_40: 0.4270 (0.4640)  loss_n_60: 0.5923 (0.6561)  loss_n_80: 0.6497 (0.7052)  loss_n_100: 0.8483 (0.8778)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [710/845]  eta: 0:00:45  loss: 2.5163 (2.7037)  loss_n_40: 0.4418 (0.4644)  loss_n_60: 0.6285 (0.6564)  loss_n_80: 0.6594 (0.7053)  loss_n_100: 0.8483 (0.8776)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [720/845]  eta: 0:00:41  loss: 2.5866 (2.7038)  loss_n_40: 0.4736 (0.4647)  loss_n_60: 0.6664 (0.6565)  loss_n_80: 0.6638 (0.7053)  loss_n_100: 0.8466 (0.8773)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [730/845]  eta: 0:00:38  loss: 2.5324 (2.7033)  loss_n_40: 0.4736 (0.4649)  loss_n_60: 0.6377 (0.6564)  loss_n_80: 0.6638 (0.7052)  loss_n_100: 0.8266 (0.8768)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3341  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [740/845]  eta: 0:00:35  loss: 2.6477 (2.7028)  loss_n_40: 0.4677 (0.4653)  loss_n_60: 0.6576 (0.6565)  loss_n_80: 0.6956 (0.7050)  loss_n_100: 0.8296 (0.8761)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3342  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [750/845]  eta: 0:00:31  loss: 2.8566 (2.7054)  loss_n_40: 0.4780 (0.4653)  loss_n_60: 0.6975 (0.6572)  loss_n_80: 0.7627 (0.7059)  loss_n_100: 0.8533 (0.8770)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [760/845]  eta: 0:00:28  loss: 2.8566 (2.7061)  loss_n_40: 0.4780 (0.4657)  loss_n_60: 0.7147 (0.6574)  loss_n_80: 0.7593 (0.7060)  loss_n_100: 0.9142 (0.8771)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [770/845]  eta: 0:00:25  loss: 2.5406 (2.7037)  loss_n_40: 0.4374 (0.4655)  loss_n_60: 0.6299 (0.6568)  loss_n_80: 0.6636 (0.7053)  loss_n_100: 0.8149 (0.8761)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [780/845]  eta: 0:00:21  loss: 2.5105 (2.7040)  loss_n_40: 0.4428 (0.4654)  loss_n_60: 0.6098 (0.6568)  loss_n_80: 0.6636 (0.7055)  loss_n_100: 0.8446 (0.8763)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3347  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [790/845]  eta: 0:00:18  loss: 2.5284 (2.7043)  loss_n_40: 0.4428 (0.4654)  loss_n_60: 0.6329 (0.6568)  loss_n_80: 0.6729 (0.7056)  loss_n_100: 0.8878 (0.8765)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [800/845]  eta: 0:00:15  loss: 2.8606 (2.7078)  loss_n_40: 0.4505 (0.4656)  loss_n_60: 0.6893 (0.6575)  loss_n_80: 0.7384 (0.7067)  loss_n_100: 0.9360 (0.8780)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3344  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [810/845]  eta: 0:00:11  loss: 2.8974 (2.7092)  loss_n_40: 0.4722 (0.4658)  loss_n_60: 0.6893 (0.6579)  loss_n_80: 0.7606 (0.7070)  loss_n_100: 0.9691 (0.8784)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3343  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [820/845]  eta: 0:00:08  loss: 2.6856 (2.7095)  loss_n_40: 0.4722 (0.4657)  loss_n_60: 0.6394 (0.6580)  loss_n_80: 0.6952 (0.7070)  loss_n_100: 0.8612 (0.8788)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:20]  [830/845]  eta: 0:00:05  loss: 2.5498 (2.7083)  loss_n_40: 0.4670 (0.4658)  loss_n_60: 0.6159 (0.6578)  loss_n_80: 0.6561 (0.7067)  loss_n_100: 0.8097 (0.8781)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3348  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [840/845]  eta: 0:00:01  loss: 2.5992 (2.7103)  loss_n_40: 0.4869 (0.4662)  loss_n_60: 0.6427 (0.6584)  loss_n_80: 0.6760 (0.7070)  loss_n_100: 0.8137 (0.8787)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3346  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20]  [844/845]  eta: 0:00:00  loss: 2.9062 (2.7110)  loss_n_40: 0.4869 (0.4662)  loss_n_60: 0.7435 (0.6585)  loss_n_80: 0.7742 (0.7073)  loss_n_100: 0.9141 (0.8791)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 0.3345  data: 0.0001  max mem: 46473\n",
      "Valid: [epoch:20] Total time: 0:04:43 (0.3351 s / it)\n",
      "Averaged stats: loss: 2.9062 (2.7110)  loss_n_40: 0.4869 (0.4662)  loss_n_60: 0.7435 (0.6585)  loss_n_80: 0.7742 (0.7073)  loss_n_100: 0.9141 (0.8791)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/epoch_20_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.879%\n",
      "Min loss_n_100: 0.249\n",
      "Best Epoch: 19.000\n",
      "Train: [epoch:21]  [   0/1724]  eta: 1:59:59  lr: 0.000200  loss: 2.7828 (2.7828)  loss_n_40: 0.4304 (0.4304)  loss_n_60: 0.6738 (0.6738)  loss_n_80: 0.7390 (0.7390)  loss_n_100: 0.9396 (0.9396)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 4.1763  data: 0.4007  max mem: 46473\n",
      "Train: [epoch:21]  [  10/1724]  eta: 1:52:38  lr: 0.000200  loss: 2.6235 (2.5717)  loss_n_40: 0.4467 (0.4780)  loss_n_60: 0.6341 (0.6186)  loss_n_80: 0.6833 (0.6655)  loss_n_100: 0.8440 (0.8096)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9430  data: 0.0366  max mem: 46473\n",
      "Train: [epoch:21]  [  20/1724]  eta: 1:51:39  lr: 0.000200  loss: 2.3789 (2.3941)  loss_n_40: 0.4460 (0.4607)  loss_n_60: 0.5487 (0.5761)  loss_n_80: 0.5927 (0.6123)  loss_n_100: 0.7119 (0.7450)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [  30/1724]  eta: 1:50:54  lr: 0.000200  loss: 1.9968 (2.2603)  loss_n_40: 0.4068 (0.4453)  loss_n_60: 0.4850 (0.5475)  loss_n_80: 0.4939 (0.5743)  loss_n_100: 0.6195 (0.6932)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [  40/1724]  eta: 1:50:11  lr: 0.000200  loss: 1.7558 (2.1321)  loss_n_40: 0.3807 (0.4282)  loss_n_60: 0.4369 (0.5181)  loss_n_80: 0.4368 (0.5407)  loss_n_100: 0.5075 (0.6452)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [  50/1724]  eta: 1:49:30  lr: 0.000200  loss: 1.6708 (2.0288)  loss_n_40: 0.3674 (0.4168)  loss_n_60: 0.4128 (0.4943)  loss_n_80: 0.4186 (0.5129)  loss_n_100: 0.4633 (0.6048)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [  60/1724]  eta: 1:48:50  lr: 0.000200  loss: 1.5701 (1.9428)  loss_n_40: 0.3538 (0.4054)  loss_n_60: 0.3894 (0.4738)  loss_n_80: 0.3945 (0.4909)  loss_n_100: 0.4309 (0.5727)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [  70/1724]  eta: 1:48:11  lr: 0.000200  loss: 1.4734 (1.8864)  loss_n_40: 0.3272 (0.3993)  loss_n_60: 0.3573 (0.4600)  loss_n_80: 0.3750 (0.4761)  loss_n_100: 0.3963 (0.5510)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9236  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [  80/1724]  eta: 1:47:31  lr: 0.000200  loss: 1.3218 (1.8205)  loss_n_40: 0.2974 (0.3875)  loss_n_60: 0.3413 (0.4449)  loss_n_80: 0.3311 (0.4595)  loss_n_100: 0.3671 (0.5286)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9239  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [  90/1724]  eta: 1:46:52  lr: 0.000200  loss: 1.3218 (1.7716)  loss_n_40: 0.3070 (0.3798)  loss_n_60: 0.3350 (0.4342)  loss_n_80: 0.3291 (0.4466)  loss_n_100: 0.3628 (0.5110)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0000)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9230  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 100/1724]  eta: 1:46:12  lr: 0.000200  loss: 1.3356 (1.7379)  loss_n_40: 0.3082 (0.3740)  loss_n_60: 0.3335 (0.4255)  loss_n_80: 0.3371 (0.4373)  loss_n_100: 0.3696 (0.4981)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0029)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 110/1724]  eta: 1:45:33  lr: 0.000200  loss: 1.3344 (1.6953)  loss_n_40: 0.2946 (0.3663)  loss_n_60: 0.3327 (0.4156)  loss_n_80: 0.3368 (0.4266)  loss_n_100: 0.3682 (0.4841)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0027)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9243  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 120/1724]  eta: 1:44:54  lr: 0.000200  loss: 1.2769 (1.6651)  loss_n_40: 0.2906 (0.3617)  loss_n_60: 0.3267 (0.4085)  loss_n_80: 0.3231 (0.4190)  loss_n_100: 0.3413 (0.4735)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0024)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 130/1724]  eta: 1:44:14  lr: 0.000200  loss: 1.2347 (1.6284)  loss_n_40: 0.2780 (0.3549)  loss_n_60: 0.3080 (0.3998)  loss_n_80: 0.3093 (0.4098)  loss_n_100: 0.3348 (0.4618)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0023)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 140/1724]  eta: 1:43:35  lr: 0.000200  loss: 1.2058 (1.6026)  loss_n_40: 0.2822 (0.3517)  loss_n_60: 0.2903 (0.3933)  loss_n_80: 0.3058 (0.4028)  loss_n_100: 0.3234 (0.4527)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0021)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 150/1724]  eta: 1:42:56  lr: 0.000200  loss: 1.1820 (1.5718)  loss_n_40: 0.2725 (0.3462)  loss_n_60: 0.2846 (0.3860)  loss_n_80: 0.2787 (0.3948)  loss_n_100: 0.3089 (0.4429)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0020)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 160/1724]  eta: 1:42:16  lr: 0.000200  loss: 1.1220 (1.5420)  loss_n_40: 0.2496 (0.3405)  loss_n_60: 0.2751 (0.3789)  loss_n_80: 0.2753 (0.3873)  loss_n_100: 0.2930 (0.4334)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0018)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 170/1724]  eta: 1:41:37  lr: 0.000200  loss: 1.0348 (1.5126)  loss_n_40: 0.2403 (0.3351)  loss_n_60: 0.2560 (0.3716)  loss_n_80: 0.2605 (0.3799)  loss_n_100: 0.2729 (0.4243)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0017)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 180/1724]  eta: 1:40:58  lr: 0.000200  loss: 1.0731 (1.4944)  loss_n_40: 0.2569 (0.3321)  loss_n_60: 0.2580 (0.3673)  loss_n_80: 0.2756 (0.3753)  loss_n_100: 0.2813 (0.4180)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0016)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9234  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [ 190/1724]  eta: 1:40:18  lr: 0.000200  loss: 1.0731 (1.4705)  loss_n_40: 0.2569 (0.3278)  loss_n_60: 0.2580 (0.3616)  loss_n_80: 0.2756 (0.3692)  loss_n_100: 0.2903 (0.4104)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0015)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 200/1724]  eta: 1:39:39  lr: 0.000200  loss: 1.0360 (1.4520)  loss_n_40: 0.2379 (0.3245)  loss_n_60: 0.2502 (0.3570)  loss_n_80: 0.2583 (0.3644)  loss_n_100: 0.2829 (0.4045)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0015)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 210/1724]  eta: 1:38:59  lr: 0.000200  loss: 1.0363 (1.4307)  loss_n_40: 0.2413 (0.3204)  loss_n_60: 0.2485 (0.3516)  loss_n_80: 0.2583 (0.3591)  loss_n_100: 0.2827 (0.3981)  triple_100: 0.0000 (0.0000)  triple_80: 0.0000 (0.0015)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0000)  time: 3.9212  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 220/1724]  eta: 1:38:20  lr: 0.000200  loss: 1.0793 (1.4264)  loss_n_40: 0.2575 (0.3188)  loss_n_60: 0.2547 (0.3495)  loss_n_80: 0.2779 (0.3581)  loss_n_100: 0.2862 (0.3967)  triple_100: 0.0000 (0.0005)  triple_80: 0.0000 (0.0014)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0013)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 230/1724]  eta: 1:37:41  lr: 0.000200  loss: 1.5356 (1.4426)  loss_n_40: 0.3042 (0.3200)  loss_n_60: 0.3488 (0.3521)  loss_n_80: 0.4134 (0.3638)  loss_n_100: 0.4587 (0.4036)  triple_100: 0.0000 (0.0005)  triple_80: 0.0000 (0.0013)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0013)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 240/1724]  eta: 1:37:02  lr: 0.000200  loss: 1.5225 (1.4405)  loss_n_40: 0.3193 (0.3192)  loss_n_60: 0.3494 (0.3512)  loss_n_80: 0.4014 (0.3638)  loss_n_100: 0.4576 (0.4034)  triple_100: 0.0000 (0.0005)  triple_80: 0.0000 (0.0013)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0012)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 250/1724]  eta: 1:36:22  lr: 0.000200  loss: 1.3760 (1.4369)  loss_n_40: 0.3022 (0.3185)  loss_n_60: 0.3328 (0.3501)  loss_n_80: 0.3649 (0.3632)  loss_n_100: 0.3766 (0.4022)  triple_100: 0.0000 (0.0005)  triple_80: 0.0000 (0.0012)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0012)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 260/1724]  eta: 1:35:43  lr: 0.000200  loss: 1.2595 (1.4294)  loss_n_40: 0.2917 (0.3175)  loss_n_60: 0.2978 (0.3480)  loss_n_80: 0.3303 (0.3614)  loss_n_100: 0.3535 (0.3997)  triple_100: 0.0000 (0.0004)  triple_80: 0.0000 (0.0012)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0011)  time: 3.9233  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 270/1724]  eta: 1:35:04  lr: 0.000200  loss: 1.1230 (1.4173)  loss_n_40: 0.2647 (0.3155)  loss_n_60: 0.2689 (0.3451)  loss_n_80: 0.2855 (0.3584)  loss_n_100: 0.3071 (0.3958)  triple_100: 0.0000 (0.0004)  triple_80: 0.0000 (0.0011)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0011)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 280/1724]  eta: 1:34:25  lr: 0.000200  loss: 1.0206 (1.4008)  loss_n_40: 0.2410 (0.3125)  loss_n_60: 0.2484 (0.3410)  loss_n_80: 0.2583 (0.3541)  loss_n_100: 0.2750 (0.3907)  triple_100: 0.0000 (0.0004)  triple_80: 0.0000 (0.0011)  triple_60: 0.0000 (0.0000)  triple_40: 0.0000 (0.0010)  time: 3.9248  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 290/1724]  eta: 1:33:46  lr: 0.000200  loss: 1.0206 (1.4071)  loss_n_40: 0.2446 (0.3115)  loss_n_60: 0.2465 (0.3396)  loss_n_80: 0.2545 (0.3531)  loss_n_100: 0.2685 (0.3893)  triple_100: 0.0000 (0.0022)  triple_80: 0.0000 (0.0044)  triple_60: 0.0000 (0.0014)  triple_40: 0.0000 (0.0055)  time: 3.9249  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 300/1724]  eta: 1:33:06  lr: 0.000200  loss: 1.3000 (1.4038)  loss_n_40: 0.2758 (0.3106)  loss_n_60: 0.3050 (0.3386)  loss_n_80: 0.3356 (0.3528)  loss_n_100: 0.3610 (0.3887)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0042)  triple_60: 0.0000 (0.0014)  triple_40: 0.0000 (0.0053)  time: 3.9242  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 310/1724]  eta: 1:32:27  lr: 0.000200  loss: 1.3463 (1.4035)  loss_n_40: 0.2820 (0.3107)  loss_n_60: 0.3172 (0.3383)  loss_n_80: 0.3609 (0.3530)  loss_n_100: 0.3941 (0.3888)  triple_100: 0.0000 (0.0021)  triple_80: 0.0000 (0.0041)  triple_60: 0.0000 (0.0014)  triple_40: 0.0000 (0.0051)  time: 3.9237  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 320/1724]  eta: 1:31:48  lr: 0.000200  loss: 1.4070 (1.4019)  loss_n_40: 0.3011 (0.3108)  loss_n_60: 0.3302 (0.3380)  loss_n_80: 0.3677 (0.3528)  loss_n_100: 0.3941 (0.3881)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0040)  triple_60: 0.0000 (0.0013)  triple_40: 0.0000 (0.0050)  time: 3.9237  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 330/1724]  eta: 1:31:09  lr: 0.000200  loss: 1.2346 (1.3973)  loss_n_40: 0.2995 (0.3100)  loss_n_60: 0.3011 (0.3365)  loss_n_80: 0.3163 (0.3513)  loss_n_100: 0.3376 (0.3861)  triple_100: 0.0000 (0.0020)  triple_80: 0.0000 (0.0044)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0048)  time: 3.9227  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 340/1724]  eta: 1:30:29  lr: 0.000200  loss: 1.2439 (1.3952)  loss_n_40: 0.3016 (0.3101)  loss_n_60: 0.2986 (0.3360)  loss_n_80: 0.3153 (0.3508)  loss_n_100: 0.3378 (0.3853)  triple_100: 0.0000 (0.0019)  triple_80: 0.0000 (0.0042)  triple_60: 0.0000 (0.0022)  triple_40: 0.0000 (0.0047)  time: 3.9222  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 350/1724]  eta: 1:29:50  lr: 0.000200  loss: 1.2303 (1.3901)  loss_n_40: 0.2788 (0.3094)  loss_n_60: 0.2938 (0.3349)  loss_n_80: 0.3127 (0.3496)  loss_n_100: 0.3320 (0.3835)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0041)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0045)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 360/1724]  eta: 1:29:11  lr: 0.000200  loss: 1.1645 (1.3842)  loss_n_40: 0.2768 (0.3087)  loss_n_60: 0.2872 (0.3335)  loss_n_80: 0.2914 (0.3482)  loss_n_100: 0.3037 (0.3816)  triple_100: 0.0000 (0.0018)  triple_80: 0.0000 (0.0040)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0044)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 370/1724]  eta: 1:28:32  lr: 0.000200  loss: 1.1305 (1.3768)  loss_n_40: 0.2761 (0.3076)  loss_n_60: 0.2747 (0.3317)  loss_n_80: 0.2843 (0.3463)  loss_n_100: 0.2996 (0.3792)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0039)  triple_60: 0.0000 (0.0020)  triple_40: 0.0000 (0.0043)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 380/1724]  eta: 1:27:52  lr: 0.000200  loss: 1.1151 (1.3697)  loss_n_40: 0.2634 (0.3065)  loss_n_60: 0.2733 (0.3301)  loss_n_80: 0.2820 (0.3445)  loss_n_100: 0.2900 (0.3769)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0038)  triple_60: 0.0000 (0.0020)  triple_40: 0.0000 (0.0042)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 390/1724]  eta: 1:27:13  lr: 0.000200  loss: 1.1142 (1.3625)  loss_n_40: 0.2611 (0.3055)  loss_n_60: 0.2705 (0.3285)  loss_n_80: 0.2739 (0.3426)  loss_n_100: 0.2868 (0.3745)  triple_100: 0.0000 (0.0017)  triple_80: 0.0000 (0.0037)  triple_60: 0.0000 (0.0019)  triple_40: 0.0000 (0.0041)  time: 3.9205  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 400/1724]  eta: 1:26:34  lr: 0.000200  loss: 1.0566 (1.3572)  loss_n_40: 0.2653 (0.3050)  loss_n_60: 0.2575 (0.3273)  loss_n_80: 0.2645 (0.3412)  loss_n_100: 0.2858 (0.3727)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0036)  triple_60: 0.0000 (0.0019)  triple_40: 0.0000 (0.0040)  time: 3.9199  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 410/1724]  eta: 1:25:54  lr: 0.000200  loss: 1.0159 (1.3499)  loss_n_40: 0.2618 (0.3039)  loss_n_60: 0.2444 (0.3256)  loss_n_80: 0.2534 (0.3393)  loss_n_100: 0.2721 (0.3703)  triple_100: 0.0000 (0.0016)  triple_80: 0.0000 (0.0035)  triple_60: 0.0000 (0.0018)  triple_40: 0.0000 (0.0039)  time: 3.9195  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 420/1724]  eta: 1:25:15  lr: 0.000200  loss: 1.0159 (1.3418)  loss_n_40: 0.2474 (0.3026)  loss_n_60: 0.2444 (0.3237)  loss_n_80: 0.2543 (0.3372)  loss_n_100: 0.2635 (0.3677)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0034)  triple_60: 0.0000 (0.0018)  triple_40: 0.0000 (0.0038)  time: 3.9202  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [ 430/1724]  eta: 1:24:36  lr: 0.000200  loss: 1.0564 (1.3373)  loss_n_40: 0.2600 (0.3020)  loss_n_60: 0.2556 (0.3226)  loss_n_80: 0.2577 (0.3359)  loss_n_100: 0.2722 (0.3660)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0035)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0037)  time: 3.9204  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 440/1724]  eta: 1:23:56  lr: 0.000200  loss: 1.0627 (1.3323)  loss_n_40: 0.2734 (0.3013)  loss_n_60: 0.2577 (0.3214)  loss_n_80: 0.2732 (0.3346)  loss_n_100: 0.2908 (0.3645)  triple_100: 0.0000 (0.0015)  triple_80: 0.0000 (0.0034)  triple_60: 0.0000 (0.0020)  triple_40: 0.0000 (0.0037)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 450/1724]  eta: 1:23:17  lr: 0.000200  loss: 1.0804 (1.3267)  loss_n_40: 0.2531 (0.3003)  loss_n_60: 0.2613 (0.3200)  loss_n_80: 0.2732 (0.3333)  loss_n_100: 0.2886 (0.3627)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0033)  triple_60: 0.0000 (0.0020)  triple_40: 0.0000 (0.0036)  time: 3.9196  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 460/1724]  eta: 1:22:38  lr: 0.000200  loss: 1.0399 (1.3199)  loss_n_40: 0.2508 (0.2993)  loss_n_60: 0.2476 (0.3184)  loss_n_80: 0.2577 (0.3315)  loss_n_100: 0.2756 (0.3606)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0033)  triple_60: 0.0000 (0.0020)  triple_40: 0.0000 (0.0035)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 470/1724]  eta: 1:21:58  lr: 0.000200  loss: 1.0457 (1.3163)  loss_n_40: 0.2544 (0.2989)  loss_n_60: 0.2520 (0.3176)  loss_n_80: 0.2656 (0.3306)  loss_n_100: 0.2756 (0.3593)  triple_100: 0.0000 (0.0014)  triple_80: 0.0000 (0.0032)  triple_60: 0.0000 (0.0019)  triple_40: 0.0000 (0.0034)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 480/1724]  eta: 1:21:19  lr: 0.000200  loss: 1.0532 (1.3105)  loss_n_40: 0.2643 (0.2980)  loss_n_60: 0.2642 (0.3163)  loss_n_80: 0.2663 (0.3291)  loss_n_100: 0.2808 (0.3574)  triple_100: 0.0000 (0.0013)  triple_80: 0.0000 (0.0031)  triple_60: 0.0000 (0.0019)  triple_40: 0.0000 (0.0034)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 490/1724]  eta: 1:20:40  lr: 0.000200  loss: 1.1862 (1.3332)  loss_n_40: 0.2807 (0.3004)  loss_n_60: 0.2898 (0.3204)  loss_n_80: 0.2833 (0.3346)  loss_n_100: 0.2957 (0.3637)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0051)  triple_60: 0.0000 (0.0024)  triple_40: 0.0000 (0.0034)  time: 3.9208  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 500/1724]  eta: 1:20:01  lr: 0.000200  loss: 2.5284 (1.3587)  loss_n_40: 0.4603 (0.3051)  loss_n_60: 0.5851 (0.3264)  loss_n_80: 0.6577 (0.3418)  loss_n_100: 0.7300 (0.3716)  triple_100: 0.0000 (0.0032)  triple_80: 0.0000 (0.0050)  triple_60: 0.0000 (0.0024)  triple_40: 0.0000 (0.0033)  time: 3.9201  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 510/1724]  eta: 1:19:21  lr: 0.000200  loss: 2.1590 (1.3749)  loss_n_40: 0.4403 (0.3080)  loss_n_60: 0.5123 (0.3301)  loss_n_80: 0.5785 (0.3465)  loss_n_100: 0.6370 (0.3768)  triple_100: 0.0000 (0.0031)  triple_80: 0.0000 (0.0049)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0032)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 520/1724]  eta: 1:18:42  lr: 0.000200  loss: 1.9557 (1.3815)  loss_n_40: 0.4033 (0.3090)  loss_n_60: 0.4604 (0.3316)  loss_n_80: 0.5232 (0.3486)  loss_n_100: 0.5649 (0.3790)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0048)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0032)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 530/1724]  eta: 1:18:03  lr: 0.000200  loss: 1.5878 (1.3844)  loss_n_40: 0.3514 (0.3097)  loss_n_60: 0.3706 (0.3323)  loss_n_80: 0.4191 (0.3494)  loss_n_100: 0.4393 (0.3799)  triple_100: 0.0000 (0.0030)  triple_80: 0.0000 (0.0047)  triple_60: 0.0000 (0.0023)  triple_40: 0.0000 (0.0031)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 540/1724]  eta: 1:17:24  lr: 0.000200  loss: 1.4860 (1.3875)  loss_n_40: 0.3579 (0.3110)  loss_n_60: 0.3576 (0.3330)  loss_n_80: 0.3792 (0.3502)  loss_n_100: 0.3957 (0.3806)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0046)  triple_60: 0.0000 (0.0022)  triple_40: 0.0000 (0.0031)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 550/1724]  eta: 1:16:44  lr: 0.000200  loss: 1.3608 (1.3879)  loss_n_40: 0.3290 (0.3115)  loss_n_60: 0.3299 (0.3331)  loss_n_80: 0.3452 (0.3503)  loss_n_100: 0.3639 (0.3805)  triple_100: 0.0000 (0.0029)  triple_80: 0.0000 (0.0045)  triple_60: 0.0000 (0.0022)  triple_40: 0.0000 (0.0030)  time: 3.9229  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 560/1724]  eta: 1:16:05  lr: 0.000200  loss: 1.2454 (1.3850)  loss_n_40: 0.2956 (0.3111)  loss_n_60: 0.2989 (0.3323)  loss_n_80: 0.3154 (0.3496)  loss_n_100: 0.3464 (0.3797)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0044)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0030)  time: 3.9230  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 570/1724]  eta: 1:15:26  lr: 0.000200  loss: 1.1729 (1.3814)  loss_n_40: 0.2718 (0.3105)  loss_n_60: 0.2854 (0.3315)  loss_n_80: 0.2909 (0.3486)  loss_n_100: 0.3231 (0.3787)  triple_100: 0.0000 (0.0028)  triple_80: 0.0000 (0.0044)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0029)  time: 3.9237  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 580/1724]  eta: 1:14:47  lr: 0.000200  loss: 1.1729 (1.3781)  loss_n_40: 0.2718 (0.3099)  loss_n_60: 0.2831 (0.3307)  loss_n_80: 0.2909 (0.3479)  loss_n_100: 0.3136 (0.3777)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0043)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0029)  time: 3.9237  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 590/1724]  eta: 1:14:08  lr: 0.000200  loss: 1.1354 (1.3735)  loss_n_40: 0.2782 (0.3092)  loss_n_60: 0.2706 (0.3296)  loss_n_80: 0.2855 (0.3467)  loss_n_100: 0.3036 (0.3763)  triple_100: 0.0000 (0.0027)  triple_80: 0.0000 (0.0042)  triple_60: 0.0000 (0.0020)  triple_40: 0.0000 (0.0028)  time: 3.9225  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 600/1724]  eta: 1:13:28  lr: 0.000200  loss: 1.1176 (1.3720)  loss_n_40: 0.2740 (0.3091)  loss_n_60: 0.2667 (0.3290)  loss_n_80: 0.2862 (0.3461)  loss_n_100: 0.3031 (0.3755)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0043)  triple_60: 0.0000 (0.0020)  triple_40: 0.0000 (0.0033)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 610/1724]  eta: 1:12:49  lr: 0.000200  loss: 1.0888 (1.3674)  loss_n_40: 0.2599 (0.3082)  loss_n_60: 0.2552 (0.3279)  loss_n_80: 0.2656 (0.3450)  loss_n_100: 0.2877 (0.3742)  triple_100: 0.0000 (0.0026)  triple_80: 0.0000 (0.0043)  triple_60: 0.0000 (0.0020)  triple_40: 0.0000 (0.0033)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 620/1724]  eta: 1:12:10  lr: 0.000200  loss: 0.9989 (1.3628)  loss_n_40: 0.2391 (0.3074)  loss_n_60: 0.2388 (0.3268)  loss_n_80: 0.2571 (0.3439)  loss_n_100: 0.2733 (0.3729)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0042)  triple_60: 0.0000 (0.0019)  triple_40: 0.0000 (0.0032)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 630/1724]  eta: 1:11:31  lr: 0.000200  loss: 1.0375 (1.3578)  loss_n_40: 0.2347 (0.3065)  loss_n_60: 0.2421 (0.3256)  loss_n_80: 0.2610 (0.3426)  loss_n_100: 0.2777 (0.3714)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0041)  triple_60: 0.0000 (0.0019)  triple_40: 0.0000 (0.0032)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 640/1724]  eta: 1:10:51  lr: 0.000200  loss: 1.0375 (1.3533)  loss_n_40: 0.2325 (0.3056)  loss_n_60: 0.2421 (0.3245)  loss_n_80: 0.2584 (0.3414)  loss_n_100: 0.2692 (0.3701)  triple_100: 0.0000 (0.0025)  triple_80: 0.0000 (0.0041)  triple_60: 0.0000 (0.0019)  triple_40: 0.0000 (0.0032)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 650/1724]  eta: 1:10:12  lr: 0.000200  loss: 1.0064 (1.3486)  loss_n_40: 0.2456 (0.3048)  loss_n_60: 0.2433 (0.3234)  loss_n_80: 0.2540 (0.3402)  loss_n_100: 0.2627 (0.3686)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0041)  triple_60: 0.0000 (0.0019)  triple_40: 0.0000 (0.0031)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 660/1724]  eta: 1:09:33  lr: 0.000200  loss: 1.0737 (1.3489)  loss_n_40: 0.2584 (0.3048)  loss_n_60: 0.2667 (0.3232)  loss_n_80: 0.2662 (0.3402)  loss_n_100: 0.2797 (0.3687)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0043)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0031)  time: 3.9236  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [ 670/1724]  eta: 1:08:54  lr: 0.000200  loss: 1.4087 (1.3513)  loss_n_40: 0.2991 (0.3049)  loss_n_60: 0.3312 (0.3236)  loss_n_80: 0.3768 (0.3411)  loss_n_100: 0.4169 (0.3700)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0043)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0030)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 680/1724]  eta: 1:08:15  lr: 0.000200  loss: 1.2931 (1.3493)  loss_n_40: 0.2741 (0.3044)  loss_n_60: 0.3076 (0.3230)  loss_n_80: 0.3437 (0.3407)  loss_n_100: 0.3747 (0.3696)  triple_100: 0.0000 (0.0024)  triple_80: 0.0000 (0.0042)  triple_60: 0.0000 (0.0021)  triple_40: 0.0000 (0.0030)  time: 3.9228  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 690/1724]  eta: 1:07:35  lr: 0.000200  loss: 1.1612 (1.3459)  loss_n_40: 0.2462 (0.3037)  loss_n_60: 0.2686 (0.3222)  loss_n_80: 0.2948 (0.3398)  loss_n_100: 0.3193 (0.3686)  triple_100: 0.0000 (0.0023)  triple_80: 0.0000 (0.0041)  triple_60: 0.0000 (0.0020)  triple_40: 0.0000 (0.0029)  time: 3.9232  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 700/1724]  eta: 1:06:56  lr: 0.000200  loss: 1.2913 (1.3924)  loss_n_40: 0.2874 (0.3055)  loss_n_60: 0.3013 (0.3254)  loss_n_80: 0.3203 (0.3446)  loss_n_100: 0.3437 (0.3749)  triple_100: 0.0000 (0.0182)  triple_80: 0.0000 (0.0099)  triple_60: 0.0000 (0.0028)  triple_40: 0.0000 (0.0111)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 710/1724]  eta: 1:06:17  lr: 0.000200  loss: 3.3296 (1.4270)  loss_n_40: 0.4818 (0.3101)  loss_n_60: 0.7009 (0.3330)  loss_n_80: 0.8621 (0.3546)  loss_n_100: 1.0051 (0.3871)  triple_100: 0.0000 (0.0180)  triple_80: 0.0000 (0.0097)  triple_60: 0.0000 (0.0036)  triple_40: 0.0000 (0.0110)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 720/1724]  eta: 1:05:38  lr: 0.000200  loss: 3.4418 (1.4544)  loss_n_40: 0.5595 (0.3133)  loss_n_60: 0.8059 (0.3392)  loss_n_80: 0.9695 (0.3630)  loss_n_100: 1.1646 (0.3973)  triple_100: 0.0000 (0.0177)  triple_80: 0.0000 (0.0096)  triple_60: 0.0000 (0.0035)  triple_40: 0.0000 (0.0108)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 730/1724]  eta: 1:04:58  lr: 0.000200  loss: 3.2289 (1.4736)  loss_n_40: 0.5036 (0.3162)  loss_n_60: 0.7421 (0.3437)  loss_n_80: 0.8761 (0.3687)  loss_n_100: 0.9997 (0.4040)  triple_100: 0.0000 (0.0175)  triple_80: 0.0000 (0.0095)  triple_60: 0.0000 (0.0035)  triple_40: 0.0000 (0.0107)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 740/1724]  eta: 1:04:19  lr: 0.000200  loss: 2.3485 (1.4825)  loss_n_40: 0.4433 (0.3177)  loss_n_60: 0.5687 (0.3458)  loss_n_80: 0.6446 (0.3714)  loss_n_100: 0.7269 (0.4071)  triple_100: 0.0000 (0.0172)  triple_80: 0.0000 (0.0093)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0105)  time: 3.9210  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 750/1724]  eta: 1:03:40  lr: 0.000200  loss: 1.9223 (1.4884)  loss_n_40: 0.3926 (0.3190)  loss_n_60: 0.4562 (0.3473)  loss_n_80: 0.5099 (0.3730)  loss_n_100: 0.5682 (0.4091)  triple_100: 0.0000 (0.0170)  triple_80: 0.0000 (0.0092)  triple_60: 0.0000 (0.0034)  triple_40: 0.0000 (0.0104)  time: 3.9208  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 760/1724]  eta: 1:03:01  lr: 0.000200  loss: 1.7430 (1.4914)  loss_n_40: 0.3922 (0.3197)  loss_n_60: 0.4120 (0.3481)  loss_n_80: 0.4551 (0.3740)  loss_n_100: 0.5118 (0.4101)  triple_100: 0.0000 (0.0168)  triple_80: 0.0000 (0.0091)  triple_60: 0.0000 (0.0033)  triple_40: 0.0000 (0.0102)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 770/1724]  eta: 1:02:21  lr: 0.000200  loss: 1.6546 (1.4935)  loss_n_40: 0.3756 (0.3206)  loss_n_60: 0.3894 (0.3487)  loss_n_80: 0.4234 (0.3746)  loss_n_100: 0.4563 (0.4107)  triple_100: 0.0000 (0.0166)  triple_80: 0.0000 (0.0090)  triple_60: 0.0000 (0.0033)  triple_40: 0.0000 (0.0101)  time: 3.9216  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 780/1724]  eta: 1:01:42  lr: 0.000200  loss: 1.4936 (1.4930)  loss_n_40: 0.3756 (0.3208)  loss_n_60: 0.3475 (0.3486)  loss_n_80: 0.3964 (0.3745)  loss_n_100: 0.4105 (0.4106)  triple_100: 0.0000 (0.0163)  triple_80: 0.0000 (0.0089)  triple_60: 0.0000 (0.0032)  triple_40: 0.0000 (0.0100)  time: 3.9209  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 790/1724]  eta: 1:01:03  lr: 0.000200  loss: 1.2890 (1.4905)  loss_n_40: 0.2838 (0.3206)  loss_n_60: 0.3086 (0.3482)  loss_n_80: 0.3317 (0.3739)  loss_n_100: 0.3590 (0.4098)  triple_100: 0.0000 (0.0161)  triple_80: 0.0000 (0.0088)  triple_60: 0.0000 (0.0032)  triple_40: 0.0000 (0.0098)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 800/1724]  eta: 1:00:24  lr: 0.000200  loss: 1.2890 (1.4889)  loss_n_40: 0.2838 (0.3205)  loss_n_60: 0.3086 (0.3479)  loss_n_80: 0.3317 (0.3736)  loss_n_100: 0.3675 (0.4095)  triple_100: 0.0000 (0.0159)  triple_80: 0.0000 (0.0086)  triple_60: 0.0000 (0.0032)  triple_40: 0.0000 (0.0097)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 810/1724]  eta: 0:59:44  lr: 0.000200  loss: 1.2942 (1.4863)  loss_n_40: 0.2875 (0.3202)  loss_n_60: 0.3076 (0.3474)  loss_n_80: 0.3424 (0.3730)  loss_n_100: 0.3634 (0.4087)  triple_100: 0.0000 (0.0157)  triple_80: 0.0000 (0.0085)  triple_60: 0.0000 (0.0031)  triple_40: 0.0000 (0.0096)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 820/1724]  eta: 0:59:05  lr: 0.000200  loss: 1.2796 (1.4837)  loss_n_40: 0.2826 (0.3199)  loss_n_60: 0.2941 (0.3468)  loss_n_80: 0.3269 (0.3724)  loss_n_100: 0.3494 (0.4080)  triple_100: 0.0000 (0.0156)  triple_80: 0.0000 (0.0084)  triple_60: 0.0000 (0.0031)  triple_40: 0.0000 (0.0095)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 830/1724]  eta: 0:58:26  lr: 0.000200  loss: 1.1484 (1.4803)  loss_n_40: 0.2615 (0.3195)  loss_n_60: 0.2728 (0.3461)  loss_n_80: 0.2969 (0.3716)  loss_n_100: 0.3164 (0.4070)  triple_100: 0.0000 (0.0154)  triple_80: 0.0000 (0.0083)  triple_60: 0.0000 (0.0031)  triple_40: 0.0000 (0.0094)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 840/1724]  eta: 0:57:47  lr: 0.000200  loss: 1.1484 (1.4769)  loss_n_40: 0.2617 (0.3191)  loss_n_60: 0.2728 (0.3454)  loss_n_80: 0.2933 (0.3708)  loss_n_100: 0.3164 (0.4060)  triple_100: 0.0000 (0.0152)  triple_80: 0.0000 (0.0082)  triple_60: 0.0000 (0.0030)  triple_40: 0.0000 (0.0093)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 850/1724]  eta: 0:57:07  lr: 0.000200  loss: 1.1848 (1.4740)  loss_n_40: 0.2643 (0.3189)  loss_n_60: 0.2768 (0.3447)  loss_n_80: 0.3003 (0.3700)  loss_n_100: 0.3218 (0.4050)  triple_100: 0.0000 (0.0150)  triple_80: 0.0000 (0.0083)  triple_60: 0.0000 (0.0030)  triple_40: 0.0000 (0.0092)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 860/1724]  eta: 0:56:28  lr: 0.000200  loss: 1.3212 (1.5535)  loss_n_40: 0.3183 (0.3209)  loss_n_60: 0.3043 (0.3478)  loss_n_80: 0.3325 (0.3746)  loss_n_100: 0.3400 (0.4110)  triple_100: 0.0000 (0.0360)  triple_80: 0.0000 (0.0275)  triple_60: 0.0000 (0.0130)  triple_40: 0.0000 (0.0226)  time: 3.9197  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 870/1724]  eta: 0:55:49  lr: 0.000200  loss: 4.9691 (1.6265)  loss_n_40: 0.8188 (0.3293)  loss_n_60: 1.0048 (0.3582)  loss_n_80: 1.2482 (0.3878)  loss_n_100: 1.5300 (0.4269)  triple_100: 0.0000 (0.0435)  triple_80: 0.0000 (0.0363)  triple_60: 0.0000 (0.0222)  triple_40: 0.0000 (0.0223)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 880/1724]  eta: 0:55:10  lr: 0.000200  loss: 5.4216 (1.6668)  loss_n_40: 0.9252 (0.3357)  loss_n_60: 1.1976 (0.3674)  loss_n_80: 1.4620 (0.3998)  loss_n_100: 1.6425 (0.4411)  triple_100: 0.0000 (0.0430)  triple_80: 0.0000 (0.0359)  triple_60: 0.0000 (0.0219)  triple_40: 0.0000 (0.0221)  time: 3.9217  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 890/1724]  eta: 0:54:31  lr: 0.000200  loss: 4.9036 (1.6984)  loss_n_40: 0.8083 (0.3412)  loss_n_60: 1.1150 (0.3745)  loss_n_80: 1.2854 (0.4087)  loss_n_100: 1.4698 (0.4511)  triple_100: 0.0000 (0.0440)  triple_80: 0.0000 (0.0355)  triple_60: 0.0000 (0.0217)  triple_40: 0.0000 (0.0218)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 900/1724]  eta: 0:53:51  lr: 0.000200  loss: 4.1475 (1.7248)  loss_n_40: 0.7177 (0.3453)  loss_n_60: 0.9238 (0.3805)  loss_n_80: 1.1312 (0.4165)  loss_n_100: 1.3838 (0.4610)  triple_100: 0.0000 (0.0435)  triple_80: 0.0000 (0.0351)  triple_60: 0.0000 (0.0214)  triple_40: 0.0000 (0.0216)  time: 3.9196  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [ 910/1724]  eta: 0:53:12  lr: 0.000200  loss: 3.3670 (1.7398)  loss_n_40: 0.6260 (0.3478)  loss_n_60: 0.8041 (0.3842)  loss_n_80: 0.8910 (0.4208)  loss_n_100: 1.0491 (0.4663)  triple_100: 0.0000 (0.0430)  triple_80: 0.0000 (0.0347)  triple_60: 0.0000 (0.0216)  triple_40: 0.0000 (0.0213)  time: 3.9199  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 920/1724]  eta: 0:52:33  lr: 0.000200  loss: 2.8580 (1.7485)  loss_n_40: 0.5344 (0.3496)  loss_n_60: 0.6828 (0.3868)  loss_n_80: 0.7487 (0.4236)  loss_n_100: 0.8648 (0.4691)  triple_100: 0.0000 (0.0425)  triple_80: 0.0000 (0.0343)  triple_60: 0.0000 (0.0214)  triple_40: 0.0000 (0.0211)  time: 3.9211  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 930/1724]  eta: 0:51:54  lr: 0.000200  loss: 2.2371 (1.7523)  loss_n_40: 0.4251 (0.3506)  loss_n_60: 0.5409 (0.3882)  loss_n_80: 0.5811 (0.4250)  loss_n_100: 0.6328 (0.4705)  triple_100: 0.0000 (0.0421)  triple_80: 0.0000 (0.0340)  triple_60: 0.0000 (0.0211)  triple_40: 0.0000 (0.0209)  time: 3.9207  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [ 940/1724]  eta: 0:51:14  lr: 0.000200  loss: 1.9085 (1.7545)  loss_n_40: 0.3967 (0.3520)  loss_n_60: 0.4580 (0.3891)  loss_n_80: 0.4989 (0.4257)  loss_n_100: 0.5478 (0.4709)  triple_100: 0.0000 (0.0416)  triple_80: 0.0000 (0.0336)  triple_60: 0.0000 (0.0209)  triple_40: 0.0000 (0.0207)  time: 3.9203  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 950/1724]  eta: 0:50:35  lr: 0.000200  loss: 1.6886 (1.7564)  loss_n_40: 0.3936 (0.3531)  loss_n_60: 0.4134 (0.3897)  loss_n_80: 0.4609 (0.4266)  loss_n_100: 0.4739 (0.4715)  triple_100: 0.0000 (0.0412)  triple_80: 0.0000 (0.0332)  triple_60: 0.0000 (0.0207)  triple_40: 0.0000 (0.0204)  time: 3.9210  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 960/1724]  eta: 0:49:56  lr: 0.000200  loss: 1.6770 (1.7554)  loss_n_40: 0.3636 (0.3536)  loss_n_60: 0.3930 (0.3898)  loss_n_80: 0.4298 (0.4266)  loss_n_100: 0.4524 (0.4711)  triple_100: 0.0000 (0.0408)  triple_80: 0.0000 (0.0329)  triple_60: 0.0000 (0.0205)  triple_40: 0.0000 (0.0202)  time: 3.9206  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 970/1724]  eta: 0:49:17  lr: 0.000200  loss: 1.5343 (1.7546)  loss_n_40: 0.3636 (0.3539)  loss_n_60: 0.3784 (0.3899)  loss_n_80: 0.4054 (0.4267)  loss_n_100: 0.4190 (0.4710)  triple_100: 0.0000 (0.0403)  triple_80: 0.0000 (0.0326)  triple_60: 0.0000 (0.0203)  triple_40: 0.0000 (0.0200)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 980/1724]  eta: 0:48:37  lr: 0.000200  loss: 1.5181 (1.7513)  loss_n_40: 0.3160 (0.3536)  loss_n_60: 0.3741 (0.3895)  loss_n_80: 0.4036 (0.4261)  loss_n_100: 0.4158 (0.4701)  triple_100: 0.0000 (0.0399)  triple_80: 0.0000 (0.0322)  triple_60: 0.0000 (0.0201)  triple_40: 0.0000 (0.0198)  time: 3.9213  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [ 990/1724]  eta: 0:47:58  lr: 0.000200  loss: 1.5519 (1.7505)  loss_n_40: 0.3449 (0.3541)  loss_n_60: 0.3741 (0.3896)  loss_n_80: 0.4030 (0.4261)  loss_n_100: 0.4050 (0.4697)  triple_100: 0.0000 (0.0395)  triple_80: 0.0000 (0.0319)  triple_60: 0.0000 (0.0199)  triple_40: 0.0000 (0.0196)  time: 3.9212  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [1000/1724]  eta: 0:47:19  lr: 0.000200  loss: 1.4774 (1.7478)  loss_n_40: 0.3449 (0.3541)  loss_n_60: 0.3402 (0.3891)  loss_n_80: 0.3802 (0.4257)  loss_n_100: 0.3896 (0.4690)  triple_100: 0.0000 (0.0391)  triple_80: 0.0000 (0.0316)  triple_60: 0.0000 (0.0197)  triple_40: 0.0000 (0.0194)  time: 3.9200  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1010/1724]  eta: 0:46:40  lr: 0.000200  loss: 1.3919 (1.7447)  loss_n_40: 0.3161 (0.3543)  loss_n_60: 0.3190 (0.3886)  loss_n_80: 0.3531 (0.4251)  loss_n_100: 0.3624 (0.4680)  triple_100: 0.0000 (0.0387)  triple_80: 0.0000 (0.0313)  triple_60: 0.0000 (0.0195)  triple_40: 0.0000 (0.0192)  time: 3.9215  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [1020/1724]  eta: 0:46:01  lr: 0.000200  loss: 1.4286 (1.7424)  loss_n_40: 0.3161 (0.3545)  loss_n_60: 0.3356 (0.3883)  loss_n_80: 0.3779 (0.4247)  loss_n_100: 0.3869 (0.4673)  triple_100: 0.0000 (0.0384)  triple_80: 0.0000 (0.0310)  triple_60: 0.0000 (0.0193)  triple_40: 0.0000 (0.0190)  time: 3.9238  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [1030/1724]  eta: 0:45:21  lr: 0.000200  loss: 1.4312 (1.7397)  loss_n_40: 0.3419 (0.3550)  loss_n_60: 0.3432 (0.3878)  loss_n_80: 0.3779 (0.4240)  loss_n_100: 0.3929 (0.4662)  triple_100: 0.0000 (0.0380)  triple_80: 0.0000 (0.0307)  triple_60: 0.0000 (0.0191)  triple_40: 0.0000 (0.0189)  time: 3.9230  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1040/1724]  eta: 0:44:42  lr: 0.000200  loss: 1.4212 (1.7375)  loss_n_40: 0.3419 (0.3549)  loss_n_60: 0.3429 (0.3875)  loss_n_80: 0.3647 (0.4237)  loss_n_100: 0.3860 (0.4658)  triple_100: 0.0000 (0.0376)  triple_80: 0.0000 (0.0304)  triple_60: 0.0000 (0.0189)  triple_40: 0.0000 (0.0187)  time: 3.9218  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1050/1724]  eta: 0:44:03  lr: 0.000200  loss: 1.3197 (1.7328)  loss_n_40: 0.3091 (0.3544)  loss_n_60: 0.3062 (0.3866)  loss_n_80: 0.3280 (0.4227)  loss_n_100: 0.3590 (0.4645)  triple_100: 0.0000 (0.0373)  triple_80: 0.0000 (0.0301)  triple_60: 0.0000 (0.0187)  triple_40: 0.0000 (0.0185)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1060/1724]  eta: 0:43:24  lr: 0.000200  loss: 1.2460 (1.7292)  loss_n_40: 0.3003 (0.3541)  loss_n_60: 0.2941 (0.3859)  loss_n_80: 0.3256 (0.4219)  loss_n_100: 0.3341 (0.4635)  triple_100: 0.0000 (0.0369)  triple_80: 0.0000 (0.0298)  triple_60: 0.0000 (0.0186)  triple_40: 0.0000 (0.0185)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1070/1724]  eta: 0:42:44  lr: 0.000200  loss: 1.2419 (1.7238)  loss_n_40: 0.2931 (0.3534)  loss_n_60: 0.2907 (0.3849)  loss_n_80: 0.3212 (0.4208)  loss_n_100: 0.3273 (0.4620)  triple_100: 0.0000 (0.0366)  triple_80: 0.0000 (0.0295)  triple_60: 0.0000 (0.0184)  triple_40: 0.0000 (0.0183)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1080/1724]  eta: 0:42:05  lr: 0.000200  loss: 1.1702 (1.7196)  loss_n_40: 0.2802 (0.3528)  loss_n_60: 0.2822 (0.3842)  loss_n_80: 0.3007 (0.4199)  loss_n_100: 0.2987 (0.4609)  triple_100: 0.0000 (0.0362)  triple_80: 0.0000 (0.0292)  triple_60: 0.0000 (0.0182)  triple_40: 0.0000 (0.0181)  time: 3.9198  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1090/1724]  eta: 0:41:26  lr: 0.000200  loss: 1.1720 (1.7149)  loss_n_40: 0.2856 (0.3524)  loss_n_60: 0.2766 (0.3833)  loss_n_80: 0.2934 (0.4188)  loss_n_100: 0.3024 (0.4596)  triple_100: 0.0000 (0.0359)  triple_80: 0.0000 (0.0290)  triple_60: 0.0000 (0.0180)  triple_40: 0.0000 (0.0180)  time: 3.9190  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1100/1724]  eta: 0:40:47  lr: 0.000200  loss: 1.1720 (1.7111)  loss_n_40: 0.2894 (0.3522)  loss_n_60: 0.2766 (0.3826)  loss_n_80: 0.2934 (0.4180)  loss_n_100: 0.3033 (0.4584)  triple_100: 0.0000 (0.0356)  triple_80: 0.0000 (0.0287)  triple_60: 0.0000 (0.0179)  triple_40: 0.0000 (0.0178)  time: 3.9214  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1110/1724]  eta: 0:40:08  lr: 0.000200  loss: 1.2830 (1.7069)  loss_n_40: 0.2879 (0.3517)  loss_n_60: 0.3042 (0.3819)  loss_n_80: 0.3286 (0.4171)  loss_n_100: 0.3329 (0.4572)  triple_100: 0.0000 (0.0353)  triple_80: 0.0000 (0.0285)  triple_60: 0.0000 (0.0177)  triple_40: 0.0000 (0.0176)  time: 3.9226  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1120/1724]  eta: 0:39:28  lr: 0.000200  loss: 1.2011 (1.7024)  loss_n_40: 0.2699 (0.3510)  loss_n_60: 0.2788 (0.3810)  loss_n_80: 0.3002 (0.4161)  loss_n_100: 0.3206 (0.4560)  triple_100: 0.0000 (0.0349)  triple_80: 0.0000 (0.0282)  triple_60: 0.0000 (0.0176)  triple_40: 0.0000 (0.0176)  time: 3.9223  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1130/1724]  eta: 0:38:49  lr: 0.000200  loss: 1.2112 (1.6993)  loss_n_40: 0.2785 (0.3508)  loss_n_60: 0.2945 (0.3805)  loss_n_80: 0.3151 (0.4154)  loss_n_100: 0.3297 (0.4552)  triple_100: 0.0000 (0.0346)  triple_80: 0.0000 (0.0279)  triple_60: 0.0000 (0.0174)  triple_40: 0.0000 (0.0174)  time: 3.9222  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1140/1724]  eta: 0:38:10  lr: 0.000200  loss: 1.2416 (1.6947)  loss_n_40: 0.2950 (0.3501)  loss_n_60: 0.2959 (0.3795)  loss_n_80: 0.3257 (0.4144)  loss_n_100: 0.3372 (0.4540)  triple_100: 0.0000 (0.0343)  triple_80: 0.0000 (0.0277)  triple_60: 0.0000 (0.0173)  triple_40: 0.0000 (0.0173)  time: 3.9226  data: 0.0001  max mem: 46473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:21]  [1150/1724]  eta: 0:37:31  lr: 0.000200  loss: 1.1029 (1.6902)  loss_n_40: 0.2519 (0.3497)  loss_n_60: 0.2643 (0.3787)  loss_n_80: 0.2840 (0.4134)  loss_n_100: 0.2894 (0.4527)  triple_100: 0.0000 (0.0340)  triple_80: 0.0000 (0.0275)  triple_60: 0.0000 (0.0171)  triple_40: 0.0000 (0.0171)  time: 3.9229  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1160/1724]  eta: 0:36:51  lr: 0.000200  loss: 1.1387 (1.6867)  loss_n_40: 0.2767 (0.3494)  loss_n_60: 0.2653 (0.3780)  loss_n_80: 0.2899 (0.4126)  loss_n_100: 0.3112 (0.4517)  triple_100: 0.0000 (0.0337)  triple_80: 0.0000 (0.0273)  triple_60: 0.0000 (0.0170)  triple_40: 0.0000 (0.0170)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1170/1724]  eta: 0:36:12  lr: 0.000200  loss: 1.1495 (1.6821)  loss_n_40: 0.2537 (0.3487)  loss_n_60: 0.2638 (0.3771)  loss_n_80: 0.2968 (0.4116)  loss_n_100: 0.3169 (0.4505)  triple_100: 0.0000 (0.0334)  triple_80: 0.0000 (0.0270)  triple_60: 0.0000 (0.0168)  triple_40: 0.0000 (0.0169)  time: 3.9228  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [1180/1724]  eta: 0:35:33  lr: 0.000200  loss: 1.1495 (1.6789)  loss_n_40: 0.2676 (0.3487)  loss_n_60: 0.2677 (0.3764)  loss_n_80: 0.2924 (0.4109)  loss_n_100: 0.3231 (0.4496)  triple_100: 0.0000 (0.0332)  triple_80: 0.0000 (0.0268)  triple_60: 0.0000 (0.0167)  triple_40: 0.0000 (0.0167)  time: 3.9233  data: 0.0002  max mem: 46473\n",
      "Train: [epoch:21]  [1190/1724]  eta: 0:34:54  lr: 0.000200  loss: 1.2876 (1.6757)  loss_n_40: 0.2985 (0.3485)  loss_n_60: 0.2965 (0.3759)  loss_n_80: 0.3019 (0.4101)  loss_n_100: 0.3254 (0.4486)  triple_100: 0.0000 (0.0329)  triple_80: 0.0000 (0.0266)  triple_60: 0.0000 (0.0165)  triple_40: 0.0000 (0.0166)  time: 3.9227  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1200/1724]  eta: 0:34:15  lr: 0.000200  loss: 1.0757 (1.6706)  loss_n_40: 0.2608 (0.3479)  loss_n_60: 0.2588 (0.3748)  loss_n_80: 0.2732 (0.4089)  loss_n_100: 0.2815 (0.4472)  triple_100: 0.0000 (0.0326)  triple_80: 0.0000 (0.0264)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0164)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1210/1724]  eta: 0:33:35  lr: 0.000200  loss: 1.0484 (1.6672)  loss_n_40: 0.2456 (0.3473)  loss_n_60: 0.2513 (0.3740)  loss_n_80: 0.2695 (0.4080)  loss_n_100: 0.2815 (0.4460)  triple_100: 0.0000 (0.0323)  triple_80: 0.0000 (0.0262)  triple_60: 0.0000 (0.0166)  triple_40: 0.0000 (0.0167)  time: 3.9215  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1220/1724]  eta: 0:32:56  lr: 0.000200  loss: 1.3056 (1.6652)  loss_n_40: 0.2911 (0.3471)  loss_n_60: 0.2906 (0.3737)  loss_n_80: 0.3218 (0.4076)  loss_n_100: 0.3448 (0.4457)  triple_100: 0.0000 (0.0321)  triple_80: 0.0000 (0.0260)  triple_60: 0.0000 (0.0164)  triple_40: 0.0000 (0.0166)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1230/1724]  eta: 0:32:17  lr: 0.000200  loss: 1.2769 (1.6621)  loss_n_40: 0.2942 (0.3467)  loss_n_60: 0.3021 (0.3730)  loss_n_80: 0.3218 (0.4070)  loss_n_100: 0.3466 (0.4449)  triple_100: 0.0000 (0.0318)  triple_80: 0.0000 (0.0258)  triple_60: 0.0000 (0.0163)  triple_40: 0.0000 (0.0165)  time: 3.9220  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1240/1724]  eta: 0:31:38  lr: 0.000200  loss: 1.2748 (1.6596)  loss_n_40: 0.2942 (0.3466)  loss_n_60: 0.2907 (0.3726)  loss_n_80: 0.3136 (0.4065)  loss_n_100: 0.3374 (0.4443)  triple_100: 0.0000 (0.0316)  triple_80: 0.0000 (0.0256)  triple_60: 0.0000 (0.0162)  triple_40: 0.0000 (0.0163)  time: 3.9224  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1250/1724]  eta: 0:30:59  lr: 0.000200  loss: 1.1306 (1.6549)  loss_n_40: 0.2674 (0.3459)  loss_n_60: 0.2706 (0.3717)  loss_n_80: 0.2825 (0.4054)  loss_n_100: 0.2886 (0.4430)  triple_100: 0.0000 (0.0313)  triple_80: 0.0000 (0.0254)  triple_60: 0.0000 (0.0160)  triple_40: 0.0000 (0.0162)  time: 3.9219  data: 0.0001  max mem: 46473\n",
      "Train: [epoch:21]  [1260/1724]  eta: 0:30:19  lr: 0.000200  loss: 1.1409 (1.6521)  loss_n_40: 0.2819 (0.3458)  loss_n_60: 0.2706 (0.3712)  loss_n_80: 0.2936 (0.4047)  loss_n_100: 0.3032 (0.4421)  triple_100: 0.0000 (0.0311)  triple_80: 0.0000 (0.0252)  triple_60: 0.0000 (0.0159)  triple_40: 0.0000 (0.0161)  time: 3.9202  data: 0.0001  max mem: 46473\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 223, in <module>\n",
      "    main(args)\n",
      "  File \"train.py\", line 171, in main\n",
      "    train_stats = train_Sinogram(model, criterion, data_loader_train, optimizer, device, epoch, args.teacher_forcing)\n",
      "  File \"/workspace/sunggu/4.Dose_img2img/scripts study/engine.py\", line 86, in train_Sinogram\n",
      "    loss, loss_detail = criterion(n_40=pred_n_40, n_60=pred_n_60, n_80=pred_n_80, n_100=pred_n_100, gt_40=input_n_40, gt_60=input_n_60, gt_80=input_n_80, gt_100=input_n_100)\n",
      "  File \"/home/sunggu/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/workspace/sunggu/4.Dose_img2img/scripts study/losses.py\", line 179, in forward\n",
      "    triple_80 = self.loss_Triple(anchor=n_80.flatten(1), positive=gt_80.flatten(1), negative=n_100.flatten(1)) \\\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--batch-size 4 \\\n",
    "--epochs 1000 \\\n",
    "--min-lr 5e-6 \\\n",
    "--lr 2e-4 \\\n",
    "--data-set 'Sinogram' \\\n",
    "--model-name 'Seqeunce_UNet_Hidden_bottle' \\\n",
    "--criterion 'Perceptual_L1_Triple_Loss' \\\n",
    "--criterion_mode '1:1' \\\n",
    "--output_dir '/workspace/sunggu/4.Dose_img2img/model/Seqeunce_UNet_Hidden_bottle' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle/low2high/' \\\n",
    "--validate-every 5 \\\n",
    "--num_workers 8 \\\n",
    "--multi-gpu-mode 'DataParallel' \\\n",
    "--teacher_forcing \"False\"\n",
    "\n",
    "# --resume '/workspace/sunggu/4.Dose_img2img/model/[Sequence_All_Hidden_Unet]Dose Unet/epoch_6_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Triple Vgg loss로 바꿔서 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "***********************************************\n",
      "Dataset Name:  Sinogram\n",
      "---------- Model ----------\n",
      "Resume From:  \n",
      "Output To:  /workspace/sunggu/4.Dose_img2img/model/Seqeunce_UNet_Hidden_bottle_2\n",
      "Save   To:  /workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle_2/low2high/\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0002\n",
      "Weight Decay:  0.05\n",
      "Batchsize:  4\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  6899\n",
      "Valid [Total]  number =  845\n",
      "Creating model: Seqeunce_UNet_Hidden_bottle\n",
      "Number of Learnable Params: 34916329\n",
      "Start training for 1000 epochs\n",
      "Train: [epoch:0]  [   0/1724]  eta: 3:16:13  lr: 0.000000  loss: 107.6430 (107.6430)  loss_n_40: 4.1078 (4.1078)  loss_n_60: 4.1143 (4.1143)  loss_n_80: 4.2319 (4.2319)  loss_n_100: 4.2886 (4.2886)  triple_100: 24.6620 (24.6620)  triple_80: 25.3873 (25.3873)  triple_60: 23.9624 (23.9624)  triple_40: 16.8887 (16.8887)  time: 6.8294  data: 0.6632  max mem: 39745\n",
      "Train: [epoch:0]  [  10/1724]  eta: 2:48:34  lr: 0.000000  loss: 128.9037 (128.5751)  loss_n_40: 4.4311 (4.4368)  loss_n_60: 4.5317 (4.5230)  loss_n_80: 4.6609 (4.6442)  loss_n_100: 4.6764 (4.6568)  triple_100: 29.9457 (29.4993)  triple_80: 31.1559 (30.9783)  triple_60: 29.6288 (29.5001)  triple_40: 20.5984 (20.3366)  time: 5.9009  data: 0.0604  max mem: 40153\n",
      "Train: [epoch:0]  [  20/1724]  eta: 2:47:36  lr: 0.000000  loss: 132.3326 (131.3056)  loss_n_40: 4.4393 (4.4588)  loss_n_60: 4.5501 (4.5615)  loss_n_80: 4.6728 (4.6855)  loss_n_100: 4.6796 (4.7019)  triple_100: 30.5106 (30.2263)  triple_80: 31.8387 (31.6745)  triple_60: 30.3427 (30.1724)  triple_40: 20.5984 (20.8246)  time: 5.8555  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [  30/1724]  eta: 2:46:42  lr: 0.000000  loss: 133.9468 (132.9353)  loss_n_40: 4.5177 (4.4863)  loss_n_60: 4.6366 (4.5951)  loss_n_80: 4.7782 (4.7179)  loss_n_100: 4.7915 (4.7327)  triple_100: 30.9405 (30.6294)  triple_80: 32.6662 (32.0963)  triple_60: 31.0693 (30.5798)  triple_40: 21.0478 (21.0978)  time: 5.9065  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [  40/1724]  eta: 2:45:44  lr: 0.000000  loss: 132.6626 (132.3717)  loss_n_40: 4.4730 (4.4680)  loss_n_60: 4.6016 (4.5816)  loss_n_80: 4.7325 (4.7046)  loss_n_100: 4.7254 (4.7214)  triple_100: 30.7023 (30.5094)  triple_80: 32.1153 (31.9492)  triple_60: 30.5912 (30.4336)  triple_40: 20.9266 (21.0040)  time: 5.9090  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:0]  [  50/1724]  eta: 2:44:45  lr: 0.000000  loss: 130.7507 (131.8809)  loss_n_40: 4.4075 (4.4636)  loss_n_60: 4.5207 (4.5768)  loss_n_80: 4.6600 (4.7002)  loss_n_100: 4.6803 (4.7162)  triple_100: 30.3961 (30.3759)  triple_80: 31.6309 (31.8467)  triple_60: 29.9911 (30.3307)  triple_40: 20.4928 (20.8708)  time: 5.9069  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [  60/1724]  eta: 2:43:46  lr: 0.000000  loss: 133.4154 (131.9453)  loss_n_40: 4.4711 (4.4615)  loss_n_60: 4.6199 (4.5774)  loss_n_80: 4.7537 (4.6999)  loss_n_100: 4.7289 (4.7154)  triple_100: 30.9738 (30.3915)  triple_80: 32.3233 (31.8629)  triple_60: 30.9269 (30.3571)  triple_40: 20.9072 (20.8796)  time: 5.9055  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [  70/1724]  eta: 2:42:47  lr: 0.000000  loss: 134.8985 (133.3071)  loss_n_40: 4.4883 (4.4811)  loss_n_60: 4.6334 (4.6002)  loss_n_80: 4.7612 (4.7226)  loss_n_100: 4.7828 (4.7387)  triple_100: 31.4538 (30.7275)  triple_80: 32.8018 (32.2032)  triple_60: 30.9328 (30.6946)  triple_40: 21.0914 (21.1393)  time: 5.9043  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [  80/1724]  eta: 2:41:47  lr: 0.000000  loss: 139.7357 (133.5071)  loss_n_40: 4.6009 (4.4832)  loss_n_60: 4.7525 (4.6042)  loss_n_80: 4.8936 (4.7269)  loss_n_100: 4.8715 (4.7431)  triple_100: 32.1742 (30.7841)  triple_80: 34.0748 (32.2602)  triple_60: 32.5484 (30.7453)  triple_40: 21.8196 (21.1601)  time: 5.9041  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [  90/1724]  eta: 2:40:48  lr: 0.000000  loss: 135.6768 (133.7020)  loss_n_40: 4.4893 (4.4860)  loss_n_60: 4.6344 (4.6071)  loss_n_80: 4.7532 (4.7296)  loss_n_100: 4.7719 (4.7455)  triple_100: 31.3297 (30.8223)  triple_80: 32.7817 (32.3047)  triple_60: 31.2182 (30.7940)  triple_40: 21.3178 (21.2128)  time: 5.9041  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 100/1724]  eta: 2:39:49  lr: 0.000000  loss: 122.7481 (132.3866)  loss_n_40: 4.3270 (4.4637)  loss_n_60: 4.3917 (4.5829)  loss_n_80: 4.5026 (4.7057)  loss_n_100: 4.5467 (4.7219)  triple_100: 28.2982 (30.5026)  triple_80: 29.2066 (31.9767)  triple_60: 27.8428 (30.4678)  triple_40: 19.6391 (20.9654)  time: 5.9035  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 110/1724]  eta: 2:38:50  lr: 0.000000  loss: 119.6903 (131.6083)  loss_n_40: 4.3223 (4.4505)  loss_n_60: 4.3910 (4.5678)  loss_n_80: 4.4681 (4.6912)  loss_n_100: 4.4760 (4.7080)  triple_100: 27.2159 (30.3148)  triple_80: 28.8275 (31.7782)  triple_60: 27.3703 (30.2650)  triple_40: 19.1626 (20.8327)  time: 5.9034  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 120/1724]  eta: 2:37:50  lr: 0.000000  loss: 127.6441 (131.0501)  loss_n_40: 4.3521 (4.4403)  loss_n_60: 4.4665 (4.5563)  loss_n_80: 4.5982 (4.6798)  loss_n_100: 4.5991 (4.6974)  triple_100: 29.2014 (30.1891)  triple_80: 30.7084 (31.6307)  triple_60: 29.2008 (30.1227)  triple_40: 19.8471 (20.7338)  time: 5.9031  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 130/1724]  eta: 2:36:51  lr: 0.000000  loss: 128.5046 (131.3671)  loss_n_40: 4.4093 (4.4468)  loss_n_60: 4.5000 (4.5630)  loss_n_80: 4.6316 (4.6861)  loss_n_100: 4.6567 (4.7032)  triple_100: 29.6316 (30.2509)  triple_80: 31.1119 (31.7079)  triple_60: 29.5670 (30.2027)  triple_40: 20.2440 (20.8066)  time: 5.9042  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 140/1724]  eta: 2:35:52  lr: 0.000000  loss: 133.2396 (131.5555)  loss_n_40: 4.4743 (4.4499)  loss_n_60: 4.6114 (4.5671)  loss_n_80: 4.7413 (4.6902)  loss_n_100: 4.7646 (4.7070)  triple_100: 30.5163 (30.2977)  triple_80: 32.2338 (31.7584)  triple_60: 30.7708 (30.2528)  triple_40: 21.5283 (20.8325)  time: 5.9057  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 150/1724]  eta: 2:34:53  lr: 0.000000  loss: 131.9798 (131.6562)  loss_n_40: 4.4259 (4.4507)  loss_n_60: 4.5467 (4.5687)  loss_n_80: 4.6840 (4.6921)  loss_n_100: 4.7040 (4.7092)  triple_100: 30.4488 (30.3276)  triple_80: 31.9566 (31.7878)  triple_60: 30.2704 (30.2771)  triple_40: 20.9164 (20.8431)  time: 5.9052  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 160/1724]  eta: 2:33:54  lr: 0.000000  loss: 131.9798 (131.7479)  loss_n_40: 4.4105 (4.4525)  loss_n_60: 4.5467 (4.5703)  loss_n_80: 4.6840 (4.6936)  loss_n_100: 4.7064 (4.7110)  triple_100: 30.5159 (30.3549)  triple_80: 31.9566 (31.8093)  triple_60: 30.2704 (30.2999)  triple_40: 20.2856 (20.8565)  time: 5.9042  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 170/1724]  eta: 2:32:55  lr: 0.000000  loss: 131.9992 (131.6984)  loss_n_40: 4.4300 (4.4539)  loss_n_60: 4.5760 (4.5703)  loss_n_80: 4.7073 (4.6931)  loss_n_100: 4.7395 (4.7100)  triple_100: 30.8664 (30.3309)  triple_80: 32.0568 (31.7905)  triple_60: 30.5152 (30.2859)  triple_40: 20.3072 (20.8638)  time: 5.9037  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 180/1724]  eta: 2:31:56  lr: 0.000000  loss: 134.2410 (131.6749)  loss_n_40: 4.5024 (4.4536)  loss_n_60: 4.6172 (4.5699)  loss_n_80: 4.7588 (4.6927)  loss_n_100: 4.7737 (4.7092)  triple_100: 30.8747 (30.3165)  triple_80: 32.4953 (31.7828)  triple_60: 30.8711 (30.2795)  triple_40: 20.9822 (20.8707)  time: 5.9041  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 190/1724]  eta: 2:30:57  lr: 0.000000  loss: 130.5331 (131.5064)  loss_n_40: 4.4850 (4.4516)  loss_n_60: 4.5519 (4.5677)  loss_n_80: 4.6922 (4.6905)  loss_n_100: 4.7205 (4.7068)  triple_100: 29.9507 (30.2741)  triple_80: 31.5577 (31.7418)  triple_60: 29.9226 (30.2383)  triple_40: 20.6525 (20.8355)  time: 5.9053  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 200/1724]  eta: 2:29:58  lr: 0.000000  loss: 129.8649 (131.5186)  loss_n_40: 4.4502 (4.4539)  loss_n_60: 4.5464 (4.5685)  loss_n_80: 4.6671 (4.6910)  loss_n_100: 4.6668 (4.7071)  triple_100: 29.7209 (30.2689)  triple_80: 31.3423 (31.7374)  triple_60: 29.8120 (30.2375)  triple_40: 20.6525 (20.8544)  time: 5.9053  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [ 210/1724]  eta: 2:28:59  lr: 0.000000  loss: 134.3821 (131.5028)  loss_n_40: 4.5073 (4.4537)  loss_n_60: 4.6137 (4.5682)  loss_n_80: 4.7512 (4.6905)  loss_n_100: 4.7510 (4.7067)  triple_100: 30.6941 (30.2604)  triple_80: 32.6272 (31.7328)  triple_60: 30.9833 (30.2347)  triple_40: 21.1183 (20.8558)  time: 5.9036  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 220/1724]  eta: 2:28:00  lr: 0.000000  loss: 131.8203 (131.4519)  loss_n_40: 4.4370 (4.4518)  loss_n_60: 4.5261 (4.5667)  loss_n_80: 4.6378 (4.6887)  loss_n_100: 4.6932 (4.7056)  triple_100: 30.6941 (30.2535)  triple_80: 31.6322 (31.7158)  triple_60: 30.1566 (30.2185)  triple_40: 21.0475 (20.8513)  time: 5.9032  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 230/1724]  eta: 2:27:01  lr: 0.000000  loss: 130.6405 (131.3084)  loss_n_40: 4.3823 (4.4493)  loss_n_60: 4.4897 (4.5637)  loss_n_80: 4.6213 (4.6857)  loss_n_100: 4.6423 (4.7024)  triple_100: 30.1123 (30.2175)  triple_80: 31.3583 (31.6788)  triple_60: 29.6790 (30.1823)  triple_40: 20.4861 (20.8286)  time: 5.9041  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 240/1724]  eta: 2:26:02  lr: 0.000000  loss: 128.3364 (131.0935)  loss_n_40: 4.3823 (4.4453)  loss_n_60: 4.4897 (4.5602)  loss_n_80: 4.6213 (4.6823)  loss_n_100: 4.6423 (4.6993)  triple_100: 29.6285 (30.1728)  triple_80: 31.0170 (31.6321)  triple_60: 29.4281 (30.1325)  triple_40: 19.7449 (20.7689)  time: 5.9049  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 250/1724]  eta: 2:25:03  lr: 0.000000  loss: 128.3364 (131.1161)  loss_n_40: 4.4119 (4.4464)  loss_n_60: 4.5273 (4.5612)  loss_n_80: 4.6447 (4.6835)  loss_n_100: 4.6807 (4.7005)  triple_100: 29.8777 (30.1797)  triple_80: 31.0170 (31.6444)  triple_60: 29.4281 (30.1430)  triple_40: 19.7449 (20.7573)  time: 5.9049  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 260/1724]  eta: 2:24:04  lr: 0.000000  loss: 132.4120 (131.3188)  loss_n_40: 4.4541 (4.4491)  loss_n_60: 4.6023 (4.5641)  loss_n_80: 4.7042 (4.6865)  loss_n_100: 4.7473 (4.7039)  triple_100: 30.6564 (30.2322)  triple_80: 32.3783 (31.6939)  triple_60: 30.8606 (30.1902)  triple_40: 20.8731 (20.7990)  time: 5.9045  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 270/1724]  eta: 2:23:05  lr: 0.000000  loss: 132.2695 (131.1043)  loss_n_40: 4.4289 (4.4454)  loss_n_60: 4.5731 (4.5598)  loss_n_80: 4.6995 (4.6823)  loss_n_100: 4.7377 (4.6999)  triple_100: 30.6564 (30.1840)  triple_80: 31.8679 (31.6381)  triple_60: 30.4235 (30.1329)  triple_40: 20.8731 (20.7620)  time: 5.9037  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 280/1724]  eta: 2:22:05  lr: 0.000000  loss: 128.2230 (131.1610)  loss_n_40: 4.3887 (4.4464)  loss_n_60: 4.4654 (4.5607)  loss_n_80: 4.5884 (4.6830)  loss_n_100: 4.6360 (4.7007)  triple_100: 29.3640 (30.1979)  triple_80: 30.9332 (31.6504)  triple_60: 29.3134 (30.1469)  triple_40: 20.4240 (20.7751)  time: 5.9032  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 290/1724]  eta: 2:21:06  lr: 0.000000  loss: 128.2230 (131.1239)  loss_n_40: 4.4069 (4.4450)  loss_n_60: 4.5031 (4.5596)  loss_n_80: 4.6276 (4.6822)  loss_n_100: 4.6365 (4.6998)  triple_100: 29.3426 (30.1903)  triple_80: 30.9332 (31.6442)  triple_60: 29.3134 (30.1384)  triple_40: 20.4240 (20.7646)  time: 5.9041  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 300/1724]  eta: 2:20:07  lr: 0.000000  loss: 128.7486 (131.1086)  loss_n_40: 4.3854 (4.4444)  loss_n_60: 4.4802 (4.5594)  loss_n_80: 4.6125 (4.6821)  loss_n_100: 4.6362 (4.6999)  triple_100: 29.4897 (30.1898)  triple_80: 31.1067 (31.6430)  triple_60: 29.4579 (30.1353)  triple_40: 20.5775 (20.7546)  time: 5.9052  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 310/1724]  eta: 2:19:08  lr: 0.000000  loss: 133.7042 (131.2144)  loss_n_40: 4.4676 (4.4460)  loss_n_60: 4.6193 (4.5613)  loss_n_80: 4.7426 (4.6842)  loss_n_100: 4.7603 (4.7019)  triple_100: 31.0693 (30.2159)  triple_80: 32.2078 (31.6704)  triple_60: 30.8813 (30.1628)  triple_40: 20.7245 (20.7717)  time: 5.9053  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 320/1724]  eta: 2:18:09  lr: 0.000000  loss: 135.8510 (131.3323)  loss_n_40: 4.5312 (4.4485)  loss_n_60: 4.6454 (4.5635)  loss_n_80: 4.7722 (4.6865)  loss_n_100: 4.7998 (4.7045)  triple_100: 31.3230 (30.2464)  triple_80: 32.8943 (31.6991)  triple_60: 31.2714 (30.1882)  triple_40: 21.3625 (20.7957)  time: 5.9046  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 330/1724]  eta: 2:17:10  lr: 0.000000  loss: 134.9111 (131.4334)  loss_n_40: 4.5457 (4.4503)  loss_n_60: 4.6451 (4.5655)  loss_n_80: 4.7571 (4.6887)  loss_n_100: 4.7768 (4.7065)  triple_100: 31.2014 (30.2682)  triple_80: 32.5547 (31.7268)  triple_60: 31.1437 (30.2143)  triple_40: 21.3625 (20.8131)  time: 5.9052  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 340/1724]  eta: 2:16:11  lr: 0.000000  loss: 134.3482 (131.4852)  loss_n_40: 4.4754 (4.4509)  loss_n_60: 4.5911 (4.5658)  loss_n_80: 4.7323 (4.6890)  loss_n_100: 4.7382 (4.7070)  triple_100: 30.7692 (30.2823)  triple_80: 32.4888 (31.7366)  triple_60: 30.9946 (30.2225)  triple_40: 21.2431 (20.8311)  time: 5.9055  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 350/1724]  eta: 2:15:12  lr: 0.000000  loss: 138.1810 (131.7014)  loss_n_40: 4.5208 (4.4546)  loss_n_60: 4.6578 (4.5701)  loss_n_80: 4.7836 (4.6933)  loss_n_100: 4.8262 (4.7112)  triple_100: 31.7711 (30.3342)  triple_80: 33.3828 (31.7934)  triple_60: 31.7179 (30.2785)  triple_40: 21.8452 (20.8662)  time: 5.9043  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 360/1724]  eta: 2:14:13  lr: 0.000000  loss: 136.9609 (131.7809)  loss_n_40: 4.5372 (4.4556)  loss_n_60: 4.6749 (4.5717)  loss_n_80: 4.8182 (4.6949)  loss_n_100: 4.8349 (4.7128)  triple_100: 31.7835 (30.3569)  triple_80: 33.1070 (31.8144)  triple_60: 31.5573 (30.2988)  triple_40: 21.8390 (20.8757)  time: 5.9050  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 370/1724]  eta: 2:13:14  lr: 0.000000  loss: 133.9721 (131.7600)  loss_n_40: 4.5170 (4.4558)  loss_n_60: 4.6453 (4.5716)  loss_n_80: 4.7846 (4.6946)  loss_n_100: 4.7883 (4.7125)  triple_100: 30.8037 (30.3494)  triple_80: 32.7379 (31.8052)  triple_60: 31.0728 (30.2922)  triple_40: 20.9640 (20.8787)  time: 5.9057  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 380/1724]  eta: 2:12:15  lr: 0.000000  loss: 133.9311 (131.7480)  loss_n_40: 4.4844 (4.4559)  loss_n_60: 4.6205 (4.5717)  loss_n_80: 4.7533 (4.6947)  loss_n_100: 4.7744 (4.7125)  triple_100: 30.8037 (30.3471)  triple_80: 32.3216 (31.8044)  triple_60: 30.7202 (30.2906)  triple_40: 20.6633 (20.8712)  time: 5.9049  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 390/1724]  eta: 2:11:16  lr: 0.000000  loss: 131.9094 (131.8033)  loss_n_40: 4.4371 (4.4570)  loss_n_60: 4.5480 (4.5726)  loss_n_80: 4.6823 (4.6958)  loss_n_100: 4.6834 (4.7136)  triple_100: 30.4239 (30.3608)  triple_80: 31.7104 (31.8194)  triple_60: 30.1933 (30.3035)  triple_40: 20.7160 (20.8805)  time: 5.9032  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 400/1724]  eta: 2:10:17  lr: 0.000000  loss: 130.6383 (131.8160)  loss_n_40: 4.4371 (4.4570)  loss_n_60: 4.5480 (4.5727)  loss_n_80: 4.6823 (4.6958)  loss_n_100: 4.6868 (4.7137)  triple_100: 30.0235 (30.3632)  triple_80: 31.5940 (31.8226)  triple_60: 29.9663 (30.3063)  triple_40: 20.7160 (20.8847)  time: 5.9031  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 410/1724]  eta: 2:09:18  lr: 0.000000  loss: 130.6383 (131.9374)  loss_n_40: 4.4787 (4.4590)  loss_n_60: 4.5877 (4.5748)  loss_n_80: 4.7195 (4.6980)  loss_n_100: 4.7235 (4.7158)  triple_100: 30.0235 (30.3910)  triple_80: 31.5940 (31.8520)  triple_60: 29.9663 (30.3356)  triple_40: 20.7519 (20.9111)  time: 5.9049  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 420/1724]  eta: 2:08:19  lr: 0.000000  loss: 138.0119 (131.9031)  loss_n_40: 4.5279 (4.4583)  loss_n_60: 4.6647 (4.5742)  loss_n_80: 4.7934 (4.6972)  loss_n_100: 4.7933 (4.7151)  triple_100: 31.4787 (30.3826)  triple_80: 33.2984 (31.8427)  triple_60: 31.7935 (30.3285)  triple_40: 21.8383 (20.9045)  time: 5.9058  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 430/1724]  eta: 2:07:20  lr: 0.000000  loss: 134.1772 (131.9891)  loss_n_40: 4.4855 (4.4598)  loss_n_60: 4.5819 (4.5759)  loss_n_80: 4.6978 (4.6989)  loss_n_100: 4.6961 (4.7167)  triple_100: 30.6102 (30.4015)  triple_80: 31.9161 (31.8649)  triple_60: 30.7999 (30.3507)  triple_40: 21.4773 (20.9207)  time: 5.9061  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [ 440/1724]  eta: 2:06:21  lr: 0.000000  loss: 134.1772 (131.8984)  loss_n_40: 4.4800 (4.4587)  loss_n_60: 4.5819 (4.5746)  loss_n_80: 4.6978 (4.6974)  loss_n_100: 4.6961 (4.7152)  triple_100: 30.6102 (30.3783)  triple_80: 31.9161 (31.8416)  triple_60: 30.7999 (30.3288)  triple_40: 20.7098 (20.9038)  time: 5.9062  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 450/1724]  eta: 2:05:22  lr: 0.000000  loss: 124.4388 (131.8061)  loss_n_40: 4.3997 (4.4579)  loss_n_60: 4.4560 (4.5733)  loss_n_80: 4.5809 (4.6960)  loss_n_100: 4.5816 (4.7139)  triple_100: 28.2366 (30.3549)  triple_80: 29.8722 (31.8175)  triple_60: 28.5399 (30.3069)  triple_40: 19.6926 (20.8856)  time: 5.9059  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 460/1724]  eta: 2:04:23  lr: 0.000000  loss: 125.7489 (131.7256)  loss_n_40: 4.4203 (4.4566)  loss_n_60: 4.5002 (4.5720)  loss_n_80: 4.5929 (4.6947)  loss_n_100: 4.6153 (4.7128)  triple_100: 28.8805 (30.3371)  triple_80: 30.4037 (31.7998)  triple_60: 28.7705 (30.2886)  triple_40: 19.6926 (20.8639)  time: 5.9062  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 470/1724]  eta: 2:03:24  lr: 0.000000  loss: 130.3223 (131.6546)  loss_n_40: 4.4560 (4.4554)  loss_n_60: 4.5702 (4.5705)  loss_n_80: 4.6663 (4.6934)  loss_n_100: 4.6786 (4.7113)  triple_100: 29.8061 (30.3195)  triple_80: 31.3620 (31.7825)  triple_60: 29.9130 (30.2699)  triple_40: 20.5155 (20.8520)  time: 5.9057  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 480/1724]  eta: 2:02:25  lr: 0.000000  loss: 131.4028 (131.6698)  loss_n_40: 4.4754 (4.4556)  loss_n_60: 4.5756 (4.5706)  loss_n_80: 4.6663 (4.6936)  loss_n_100: 4.6786 (4.7115)  triple_100: 30.2590 (30.3222)  triple_80: 31.6993 (31.7853)  triple_60: 30.0838 (30.2726)  triple_40: 20.7585 (20.8584)  time: 5.9061  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 490/1724]  eta: 2:01:26  lr: 0.000000  loss: 130.5338 (131.5944)  loss_n_40: 4.4844 (4.4551)  loss_n_60: 4.5787 (4.5698)  loss_n_80: 4.6946 (4.6926)  loss_n_100: 4.7008 (4.7104)  triple_100: 29.7522 (30.3028)  triple_80: 31.6515 (31.7666)  triple_60: 30.1016 (30.2552)  triple_40: 20.5893 (20.8418)  time: 5.9063  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 500/1724]  eta: 2:00:27  lr: 0.000000  loss: 130.5338 (131.6300)  loss_n_40: 4.4826 (4.4558)  loss_n_60: 4.5612 (4.5706)  loss_n_80: 4.6884 (4.6934)  loss_n_100: 4.7007 (4.7111)  triple_100: 29.7522 (30.3106)  triple_80: 31.6253 (31.7762)  triple_60: 30.1016 (30.2657)  triple_40: 20.3387 (20.8467)  time: 5.9051  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 510/1724]  eta: 1:59:28  lr: 0.000000  loss: 132.1404 (131.6417)  loss_n_40: 4.4339 (4.4561)  loss_n_60: 4.5180 (4.5709)  loss_n_80: 4.6329 (4.6937)  loss_n_100: 4.6371 (4.7113)  triple_100: 30.5798 (30.3134)  triple_80: 31.6253 (31.7785)  triple_60: 30.1777 (30.2693)  triple_40: 20.3387 (20.8485)  time: 5.9055  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 520/1724]  eta: 1:58:29  lr: 0.000000  loss: 131.0741 (131.6533)  loss_n_40: 4.4715 (4.4565)  loss_n_60: 4.5491 (4.5711)  loss_n_80: 4.6655 (4.6937)  loss_n_100: 4.7085 (4.7116)  triple_100: 30.1602 (30.3149)  triple_80: 31.5918 (31.7798)  triple_60: 30.1299 (30.2700)  triple_40: 21.0807 (20.8557)  time: 5.9072  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 530/1724]  eta: 1:57:30  lr: 0.000000  loss: 129.1855 (131.5895)  loss_n_40: 4.4513 (4.4552)  loss_n_60: 4.5516 (4.5696)  loss_n_80: 4.6676 (4.6923)  loss_n_100: 4.6881 (4.7102)  triple_100: 29.7952 (30.3004)  triple_80: 31.0308 (31.7630)  triple_60: 29.4530 (30.2529)  triple_40: 20.6538 (20.8459)  time: 5.9079  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 540/1724]  eta: 1:56:31  lr: 0.000000  loss: 126.6405 (131.5186)  loss_n_40: 4.4173 (4.4542)  loss_n_60: 4.4887 (4.5686)  loss_n_80: 4.6236 (4.6910)  loss_n_100: 4.6741 (4.7089)  triple_100: 29.0739 (30.2822)  triple_80: 31.0134 (31.7427)  triple_60: 29.3455 (30.2351)  triple_40: 20.1929 (20.8358)  time: 5.9073  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 550/1724]  eta: 1:55:32  lr: 0.000000  loss: 120.8325 (131.3318)  loss_n_40: 4.2399 (4.4502)  loss_n_60: 4.3792 (4.5648)  loss_n_80: 4.5031 (4.6872)  loss_n_100: 4.5034 (4.7053)  triple_100: 27.9889 (30.2395)  triple_80: 29.1464 (31.6947)  triple_60: 27.5072 (30.1876)  triple_40: 18.4110 (20.8026)  time: 5.9068  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 560/1724]  eta: 1:54:33  lr: 0.000000  loss: 125.6942 (131.3749)  loss_n_40: 4.3059 (4.4509)  loss_n_60: 4.4313 (4.5658)  loss_n_80: 4.5747 (4.6883)  loss_n_100: 4.5855 (4.7063)  triple_100: 28.8883 (30.2498)  triple_80: 30.4309 (31.7080)  triple_60: 28.8428 (30.2009)  triple_40: 19.3570 (20.8048)  time: 5.9069  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 570/1724]  eta: 1:53:34  lr: 0.000000  loss: 129.5314 (131.3671)  loss_n_40: 4.4496 (4.4510)  loss_n_60: 4.5436 (4.5656)  loss_n_80: 4.6859 (4.6882)  loss_n_100: 4.6906 (4.7062)  triple_100: 29.9978 (30.2483)  triple_80: 31.5756 (31.7058)  triple_60: 29.9452 (30.1978)  triple_40: 20.3636 (20.8043)  time: 5.9070  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 580/1724]  eta: 1:52:35  lr: 0.000000  loss: 132.7562 (131.3880)  loss_n_40: 4.4507 (4.4515)  loss_n_60: 4.5537 (4.5662)  loss_n_80: 4.6859 (4.6888)  loss_n_100: 4.6906 (4.7067)  triple_100: 30.3578 (30.2532)  triple_80: 31.7942 (31.7129)  triple_60: 30.3730 (30.2041)  triple_40: 20.5895 (20.8047)  time: 5.9074  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 590/1724]  eta: 1:51:36  lr: 0.000000  loss: 133.1126 (131.3908)  loss_n_40: 4.4908 (4.4517)  loss_n_60: 4.5970 (4.5663)  loss_n_80: 4.7389 (4.6891)  loss_n_100: 4.7444 (4.7069)  triple_100: 30.8110 (30.2550)  triple_80: 32.2891 (31.7145)  triple_60: 30.6560 (30.2045)  triple_40: 21.0352 (20.8029)  time: 5.9086  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 600/1724]  eta: 1:50:37  lr: 0.000000  loss: 135.9917 (131.4393)  loss_n_40: 4.5420 (4.4529)  loss_n_60: 4.6641 (4.5675)  loss_n_80: 4.7836 (4.6902)  loss_n_100: 4.8171 (4.7080)  triple_100: 31.1194 (30.2660)  triple_80: 32.7510 (31.7275)  triple_60: 31.1755 (30.2167)  triple_40: 21.7185 (20.8106)  time: 5.9096  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 610/1724]  eta: 1:49:38  lr: 0.000000  loss: 130.4618 (131.4245)  loss_n_40: 4.4460 (4.4526)  loss_n_60: 4.5879 (4.5670)  loss_n_80: 4.7015 (4.6898)  loss_n_100: 4.7280 (4.7076)  triple_100: 30.0360 (30.2621)  triple_80: 31.6912 (31.7234)  triple_60: 30.0673 (30.2120)  triple_40: 21.2158 (20.8101)  time: 5.9092  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 620/1724]  eta: 1:48:39  lr: 0.000000  loss: 130.4618 (131.4362)  loss_n_40: 4.4460 (4.4524)  loss_n_60: 4.5642 (4.5670)  loss_n_80: 4.7007 (4.6898)  loss_n_100: 4.7035 (4.7077)  triple_100: 30.0360 (30.2656)  triple_80: 31.5282 (31.7266)  triple_60: 29.8537 (30.2143)  triple_40: 21.0445 (20.8129)  time: 5.9085  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 630/1724]  eta: 1:47:40  lr: 0.000000  loss: 125.8163 (131.4060)  loss_n_40: 4.3394 (4.4515)  loss_n_60: 4.4643 (4.5661)  loss_n_80: 4.5984 (4.6890)  loss_n_100: 4.6138 (4.7070)  triple_100: 28.9299 (30.2590)  triple_80: 30.3841 (31.7189)  triple_60: 28.7916 (30.2060)  triple_40: 19.9939 (20.8085)  time: 5.9098  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 640/1724]  eta: 1:46:41  lr: 0.000000  loss: 130.7762 (131.4503)  loss_n_40: 4.4108 (4.4523)  loss_n_60: 4.5442 (4.5670)  loss_n_80: 4.6810 (4.6899)  loss_n_100: 4.7123 (4.7079)  triple_100: 30.1585 (30.2700)  triple_80: 31.4893 (31.7301)  triple_60: 29.8909 (30.2171)  triple_40: 20.4418 (20.8159)  time: 5.9103  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 650/1724]  eta: 1:45:42  lr: 0.000000  loss: 135.1996 (131.5004)  loss_n_40: 4.4815 (4.4531)  loss_n_60: 4.6465 (4.5680)  loss_n_80: 4.7626 (4.6909)  loss_n_100: 4.7827 (4.7089)  triple_100: 31.3923 (30.2831)  triple_80: 32.8274 (31.7431)  triple_60: 31.1601 (30.2310)  triple_40: 21.2188 (20.8221)  time: 5.9090  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 660/1724]  eta: 1:44:43  lr: 0.000000  loss: 136.5099 (131.5650)  loss_n_40: 4.4768 (4.4541)  loss_n_60: 4.6491 (4.5690)  loss_n_80: 4.7626 (4.6918)  loss_n_100: 4.7827 (4.7099)  triple_100: 31.5274 (30.2992)  triple_80: 33.0756 (31.7579)  triple_60: 31.6222 (30.2455)  triple_40: 21.8079 (20.8376)  time: 5.9095  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [ 670/1724]  eta: 1:43:44  lr: 0.000000  loss: 133.4076 (131.4992)  loss_n_40: 4.4344 (4.4531)  loss_n_60: 4.5562 (4.5679)  loss_n_80: 4.6760 (4.6908)  loss_n_100: 4.7023 (4.7089)  triple_100: 30.8837 (30.2833)  triple_80: 31.9833 (31.7419)  triple_60: 30.3393 (30.2290)  triple_40: 21.6056 (20.8244)  time: 5.9103  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 680/1724]  eta: 1:42:45  lr: 0.000000  loss: 129.5217 (131.5406)  loss_n_40: 4.4527 (4.4537)  loss_n_60: 4.5590 (4.5685)  loss_n_80: 4.6962 (4.6914)  loss_n_100: 4.7079 (4.7095)  triple_100: 29.8942 (30.2929)  triple_80: 31.3578 (31.7525)  triple_60: 29.9197 (30.2395)  triple_40: 20.0981 (20.8325)  time: 5.9097  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 690/1724]  eta: 1:41:46  lr: 0.000000  loss: 131.0667 (131.5840)  loss_n_40: 4.4674 (4.4546)  loss_n_60: 4.5615 (4.5693)  loss_n_80: 4.6962 (4.6923)  loss_n_100: 4.7040 (4.7104)  triple_100: 30.4450 (30.3046)  triple_80: 31.7162 (31.7633)  triple_60: 30.1599 (30.2500)  triple_40: 21.0390 (20.8395)  time: 5.9095  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 700/1724]  eta: 1:40:47  lr: 0.000000  loss: 129.0079 (131.5671)  loss_n_40: 4.3906 (4.4543)  loss_n_60: 4.5141 (4.5688)  loss_n_80: 4.6432 (4.6917)  loss_n_100: 4.6648 (4.7099)  triple_100: 29.2478 (30.2993)  triple_80: 30.7606 (31.7571)  triple_60: 29.3556 (30.2437)  triple_40: 20.6648 (20.8424)  time: 5.9090  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 710/1724]  eta: 1:39:48  lr: 0.000000  loss: 131.4629 (131.6543)  loss_n_40: 4.4868 (4.4557)  loss_n_60: 4.6097 (4.5703)  loss_n_80: 4.7546 (4.6934)  loss_n_100: 4.7454 (4.7115)  triple_100: 30.2616 (30.3208)  triple_80: 32.0327 (31.7797)  triple_60: 30.3742 (30.2656)  triple_40: 20.9367 (20.8574)  time: 5.9090  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 720/1724]  eta: 1:38:49  lr: 0.000000  loss: 134.1494 (131.6635)  loss_n_40: 4.5170 (4.4560)  loss_n_60: 4.6393 (4.5705)  loss_n_80: 4.7749 (4.6936)  loss_n_100: 4.7843 (4.7117)  triple_100: 30.7140 (30.3242)  triple_80: 32.6900 (31.7814)  triple_60: 31.0932 (30.2670)  triple_40: 21.0914 (20.8591)  time: 5.9098  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 730/1724]  eta: 1:37:50  lr: 0.000000  loss: 132.7330 (131.7045)  loss_n_40: 4.4984 (4.4564)  loss_n_60: 4.5922 (4.5710)  loss_n_80: 4.7270 (4.6941)  loss_n_100: 4.7411 (4.7124)  triple_100: 30.5485 (30.3350)  triple_80: 32.0623 (31.7915)  triple_60: 30.4363 (30.2767)  triple_40: 20.8639 (20.8672)  time: 5.9109  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 740/1724]  eta: 1:36:51  lr: 0.000000  loss: 135.5902 (131.8472)  loss_n_40: 4.5435 (4.4589)  loss_n_60: 4.6822 (4.5737)  loss_n_80: 4.8124 (4.6968)  loss_n_100: 4.8087 (4.7151)  triple_100: 31.0979 (30.3696)  triple_80: 33.1380 (31.8284)  triple_60: 31.4858 (30.3125)  triple_40: 21.1721 (20.8921)  time: 5.9111  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 750/1724]  eta: 1:35:52  lr: 0.000000  loss: 136.0048 (131.8951)  loss_n_40: 4.5460 (4.4597)  loss_n_60: 4.6783 (4.5746)  loss_n_80: 4.8122 (4.6978)  loss_n_100: 4.8142 (4.7159)  triple_100: 31.6428 (30.3806)  triple_80: 33.1380 (31.8409)  triple_60: 31.4858 (30.3243)  triple_40: 21.3732 (20.9012)  time: 5.9102  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 760/1724]  eta: 1:34:53  lr: 0.000000  loss: 132.5575 (131.9120)  loss_n_40: 4.4985 (4.4602)  loss_n_60: 4.5943 (4.5750)  loss_n_80: 4.7233 (4.6981)  loss_n_100: 4.7319 (4.7162)  triple_100: 30.5335 (30.3844)  triple_80: 32.2937 (31.8446)  triple_60: 30.4794 (30.3290)  triple_40: 21.2812 (20.9046)  time: 5.9098  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 770/1724]  eta: 1:33:54  lr: 0.000000  loss: 129.5914 (131.9184)  loss_n_40: 4.4627 (4.4605)  loss_n_60: 4.5370 (4.5750)  loss_n_80: 4.6577 (4.6980)  loss_n_100: 4.6699 (4.7162)  triple_100: 29.9313 (30.3846)  triple_80: 31.5008 (31.8442)  triple_60: 29.8225 (30.3289)  triple_40: 20.7834 (20.9110)  time: 5.9098  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 780/1724]  eta: 1:32:55  lr: 0.000000  loss: 131.0760 (131.8888)  loss_n_40: 4.4627 (4.4600)  loss_n_60: 4.5548 (4.5743)  loss_n_80: 4.6841 (4.6972)  loss_n_100: 4.6907 (4.7154)  triple_100: 30.3246 (30.3757)  triple_80: 31.6091 (31.8341)  triple_60: 30.1626 (30.3201)  triple_40: 20.7834 (20.9119)  time: 5.9091  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 790/1724]  eta: 1:31:56  lr: 0.000000  loss: 130.7312 (131.8259)  loss_n_40: 4.4104 (4.4586)  loss_n_60: 4.5368 (4.5730)  loss_n_80: 4.6609 (4.6959)  loss_n_100: 4.6846 (4.7142)  triple_100: 29.9506 (30.3630)  triple_80: 31.4417 (31.8176)  triple_60: 29.7522 (30.3035)  triple_40: 20.7707 (20.9001)  time: 5.9109  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 800/1724]  eta: 1:30:57  lr: 0.000000  loss: 123.7052 (131.7392)  loss_n_40: 4.3268 (4.4574)  loss_n_60: 4.4229 (4.5714)  loss_n_80: 4.5552 (4.6944)  loss_n_100: 4.5533 (4.7127)  triple_100: 27.9408 (30.3415)  triple_80: 29.4197 (31.7964)  triple_60: 28.1860 (30.2825)  triple_40: 19.1295 (20.8829)  time: 5.9124  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 810/1724]  eta: 1:29:58  lr: 0.000000  loss: 125.1411 (131.7620)  loss_n_40: 4.3889 (4.4579)  loss_n_60: 4.4656 (4.5720)  loss_n_80: 4.5954 (4.6950)  loss_n_100: 4.6257 (4.7132)  triple_100: 28.8383 (30.3458)  triple_80: 30.3312 (31.8034)  triple_60: 28.8068 (30.2893)  triple_40: 19.7402 (20.8853)  time: 5.9108  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 820/1724]  eta: 1:28:59  lr: 0.000000  loss: 134.3718 (131.7516)  loss_n_40: 4.4962 (4.4577)  loss_n_60: 4.6277 (4.5718)  loss_n_80: 4.7536 (4.6948)  loss_n_100: 4.7635 (4.7131)  triple_100: 30.8792 (30.3446)  triple_80: 32.5206 (31.8005)  triple_60: 31.0563 (30.2862)  triple_40: 20.9077 (20.8831)  time: 5.9104  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 830/1724]  eta: 1:28:00  lr: 0.000000  loss: 127.1885 (131.6615)  loss_n_40: 4.3395 (4.4563)  loss_n_60: 4.4740 (4.5703)  loss_n_80: 4.6037 (4.6932)  loss_n_100: 4.6363 (4.7114)  triple_100: 29.5309 (30.3223)  triple_80: 30.6872 (31.7767)  triple_60: 29.0264 (30.2635)  triple_40: 19.4110 (20.8678)  time: 5.9113  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 840/1724]  eta: 1:27:01  lr: 0.000000  loss: 126.9564 (131.6347)  loss_n_40: 4.3473 (4.4558)  loss_n_60: 4.4594 (4.5699)  loss_n_80: 4.5877 (4.6928)  loss_n_100: 4.6145 (4.7109)  triple_100: 29.2124 (30.3150)  triple_80: 30.5868 (31.7711)  triple_60: 28.8769 (30.2575)  triple_40: 19.9152 (20.8616)  time: 5.9117  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 850/1724]  eta: 1:26:02  lr: 0.000000  loss: 130.4723 (131.5833)  loss_n_40: 4.4517 (4.4547)  loss_n_60: 4.5341 (4.5687)  loss_n_80: 4.6665 (4.6917)  loss_n_100: 4.6775 (4.7099)  triple_100: 29.8290 (30.3022)  triple_80: 31.5267 (31.7570)  triple_60: 29.9863 (30.2436)  triple_40: 20.3170 (20.8555)  time: 5.9128  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 860/1724]  eta: 1:25:03  lr: 0.000000  loss: 129.9515 (131.5583)  loss_n_40: 4.4297 (4.4541)  loss_n_60: 4.5505 (4.5681)  loss_n_80: 4.6979 (4.6912)  loss_n_100: 4.6900 (4.7093)  triple_100: 29.8163 (30.2970)  triple_80: 31.6993 (31.7513)  triple_60: 30.0682 (30.2375)  triple_40: 20.2922 (20.8499)  time: 5.9129  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 870/1724]  eta: 1:24:04  lr: 0.000000  loss: 131.1570 (131.5325)  loss_n_40: 4.4297 (4.4537)  loss_n_60: 4.5585 (4.5676)  loss_n_80: 4.6979 (4.6907)  loss_n_100: 4.7040 (4.7088)  triple_100: 30.2446 (30.2908)  triple_80: 31.8889 (31.7455)  triple_60: 30.2391 (30.2317)  triple_40: 20.1242 (20.8437)  time: 5.9123  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 880/1724]  eta: 1:23:05  lr: 0.000000  loss: 128.9261 (131.4824)  loss_n_40: 4.4170 (4.4528)  loss_n_60: 4.5477 (4.5668)  loss_n_80: 4.6590 (4.6899)  loss_n_100: 4.6965 (4.7080)  triple_100: 29.6949 (30.2790)  triple_80: 31.1545 (31.7332)  triple_60: 29.6184 (30.2195)  triple_40: 20.1134 (20.8333)  time: 5.9119  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 890/1724]  eta: 1:22:06  lr: 0.000000  loss: 128.9027 (131.4922)  loss_n_40: 4.4170 (4.4529)  loss_n_60: 4.5477 (4.5671)  loss_n_80: 4.6590 (4.6903)  loss_n_100: 4.6965 (4.7084)  triple_100: 29.5052 (30.2822)  triple_80: 31.1545 (31.7373)  triple_60: 29.6184 (30.2229)  triple_40: 19.6948 (20.8312)  time: 5.9123  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [ 900/1724]  eta: 1:21:07  lr: 0.000000  loss: 134.7157 (131.5104)  loss_n_40: 4.5212 (4.4538)  loss_n_60: 4.6256 (4.5677)  loss_n_80: 4.7291 (4.6907)  loss_n_100: 4.7546 (4.7088)  triple_100: 30.9334 (30.2853)  triple_80: 32.0558 (31.7402)  triple_60: 30.9477 (30.2271)  triple_40: 21.3997 (20.8369)  time: 5.9129  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 910/1724]  eta: 1:20:08  lr: 0.000000  loss: 134.7157 (131.4928)  loss_n_40: 4.5494 (4.4532)  loss_n_60: 4.6410 (4.5672)  loss_n_80: 4.7291 (4.6901)  loss_n_100: 4.7546 (4.7083)  triple_100: 30.9334 (30.2816)  triple_80: 32.0558 (31.7349)  triple_60: 30.9477 (30.2224)  triple_40: 22.0891 (20.8350)  time: 5.9130  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 920/1724]  eta: 1:19:09  lr: 0.000000  loss: 131.1903 (131.4678)  loss_n_40: 4.4345 (4.4529)  loss_n_60: 4.5510 (4.5668)  loss_n_80: 4.6906 (4.6897)  loss_n_100: 4.7021 (4.7077)  triple_100: 30.1649 (30.2733)  triple_80: 31.7350 (31.7280)  triple_60: 30.1303 (30.2167)  triple_40: 20.9236 (20.8327)  time: 5.9142  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 930/1724]  eta: 1:18:10  lr: 0.000000  loss: 132.4545 (131.5213)  loss_n_40: 4.4345 (4.4533)  loss_n_60: 4.5581 (4.5674)  loss_n_80: 4.7012 (4.6904)  loss_n_100: 4.7220 (4.7085)  triple_100: 30.1677 (30.2862)  triple_80: 31.7613 (31.7416)  triple_60: 30.6797 (30.2291)  triple_40: 21.0215 (20.8447)  time: 5.9143  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 940/1724]  eta: 1:17:11  lr: 0.000000  loss: 135.8433 (131.5690)  loss_n_40: 4.4763 (4.4542)  loss_n_60: 4.5858 (4.5683)  loss_n_80: 4.7256 (4.6912)  loss_n_100: 4.7555 (4.7093)  triple_100: 31.4180 (30.2979)  triple_80: 32.7093 (31.7534)  triple_60: 30.9950 (30.2412)  triple_40: 21.9539 (20.8535)  time: 5.9143  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 950/1724]  eta: 1:16:12  lr: 0.000000  loss: 132.3339 (131.5586)  loss_n_40: 4.4763 (4.4540)  loss_n_60: 4.6082 (4.5681)  loss_n_80: 4.7375 (4.6911)  loss_n_100: 4.7555 (4.7092)  triple_100: 30.5783 (30.2959)  triple_80: 32.0449 (31.7511)  triple_60: 30.5362 (30.2388)  triple_40: 20.7152 (20.8502)  time: 5.9134  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 960/1724]  eta: 1:15:13  lr: 0.000000  loss: 130.8881 (131.5429)  loss_n_40: 4.4782 (4.4540)  loss_n_60: 4.5998 (4.5680)  loss_n_80: 4.7192 (4.6909)  loss_n_100: 4.7400 (4.7092)  triple_100: 30.1196 (30.2923)  triple_80: 31.9201 (31.7474)  triple_60: 30.4681 (30.2352)  triple_40: 20.3472 (20.8460)  time: 5.9131  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 970/1724]  eta: 1:14:14  lr: 0.000000  loss: 130.1052 (131.5040)  loss_n_40: 4.4501 (4.4531)  loss_n_60: 4.5756 (4.5672)  loss_n_80: 4.6932 (4.6903)  loss_n_100: 4.7034 (4.7085)  triple_100: 29.7874 (30.2835)  triple_80: 31.4651 (31.7385)  triple_60: 29.9347 (30.2256)  triple_40: 19.9200 (20.8373)  time: 5.9130  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 980/1724]  eta: 1:13:15  lr: 0.000000  loss: 126.6940 (131.5406)  loss_n_40: 4.3450 (4.4537)  loss_n_60: 4.4494 (4.5679)  loss_n_80: 4.5869 (4.6910)  loss_n_100: 4.6282 (4.7092)  triple_100: 29.4446 (30.2924)  triple_80: 30.5307 (31.7485)  triple_60: 28.8144 (30.2357)  triple_40: 19.7929 (20.8421)  time: 5.9132  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [ 990/1724]  eta: 1:12:16  lr: 0.000000  loss: 132.0239 (131.5116)  loss_n_40: 4.4935 (4.4533)  loss_n_60: 4.6306 (4.5674)  loss_n_80: 4.7568 (4.6904)  loss_n_100: 4.7660 (4.7088)  triple_100: 30.5075 (30.2861)  triple_80: 32.3069 (31.7399)  triple_60: 30.8502 (30.2275)  triple_40: 20.3796 (20.8380)  time: 5.9135  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1000/1724]  eta: 1:11:17  lr: 0.000000  loss: 132.5665 (131.5707)  loss_n_40: 4.4939 (4.4542)  loss_n_60: 4.6308 (4.5686)  loss_n_80: 4.7616 (4.6916)  loss_n_100: 4.7630 (4.7099)  triple_100: 30.7225 (30.3010)  triple_80: 32.3714 (31.7557)  triple_60: 30.7741 (30.2428)  triple_40: 20.7947 (20.8471)  time: 5.9128  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1010/1724]  eta: 1:10:18  lr: 0.000000  loss: 132.2124 (131.5563)  loss_n_40: 4.4566 (4.4539)  loss_n_60: 4.5725 (4.5681)  loss_n_80: 4.6942 (4.6911)  loss_n_100: 4.7132 (4.7095)  triple_100: 30.6087 (30.2974)  triple_80: 31.8166 (31.7503)  triple_60: 30.2772 (30.2380)  triple_40: 20.6843 (20.8480)  time: 5.9123  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1020/1724]  eta: 1:09:19  lr: 0.000000  loss: 125.8501 (131.5137)  loss_n_40: 4.3991 (4.4535)  loss_n_60: 4.4750 (4.5675)  loss_n_80: 4.5752 (4.6904)  loss_n_100: 4.6109 (4.7088)  triple_100: 28.8636 (30.2858)  triple_80: 30.2311 (31.7399)  triple_60: 28.8503 (30.2279)  triple_40: 20.0631 (20.8399)  time: 5.9119  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1030/1724]  eta: 1:08:20  lr: 0.000000  loss: 126.2826 (131.4474)  loss_n_40: 4.3526 (4.4524)  loss_n_60: 4.4446 (4.5662)  loss_n_80: 4.5652 (4.6891)  loss_n_100: 4.5990 (4.7076)  triple_100: 29.1490 (30.2698)  triple_80: 30.2756 (31.7220)  triple_60: 28.7805 (30.2104)  triple_40: 20.1083 (20.8300)  time: 5.9120  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1040/1724]  eta: 1:07:20  lr: 0.000000  loss: 126.3376 (131.4054)  loss_n_40: 4.3526 (4.4519)  loss_n_60: 4.4446 (4.5654)  loss_n_80: 4.5692 (4.6884)  loss_n_100: 4.6029 (4.7068)  triple_100: 29.4611 (30.2585)  triple_80: 30.5738 (31.7111)  triple_60: 28.8475 (30.1995)  triple_40: 20.1083 (20.8238)  time: 5.9113  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1050/1724]  eta: 1:06:21  lr: 0.000000  loss: 126.3376 (131.4096)  loss_n_40: 4.4297 (4.4522)  loss_n_60: 4.4878 (4.5656)  loss_n_80: 4.6201 (4.6885)  loss_n_100: 4.6396 (4.7070)  triple_100: 29.4611 (30.2589)  triple_80: 30.5959 (31.7125)  triple_60: 28.8992 (30.2009)  triple_40: 20.2030 (20.8240)  time: 5.9122  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1060/1724]  eta: 1:05:22  lr: 0.000000  loss: 130.6558 (131.4025)  loss_n_40: 4.4297 (4.4521)  loss_n_60: 4.4932 (4.5655)  loss_n_80: 4.6201 (4.6884)  loss_n_100: 4.6593 (4.7070)  triple_100: 30.2435 (30.2580)  triple_80: 31.1390 (31.7105)  triple_60: 29.5992 (30.1991)  triple_40: 20.6658 (20.8218)  time: 5.9145  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1070/1724]  eta: 1:04:23  lr: 0.000000  loss: 131.4444 (131.4105)  loss_n_40: 4.4458 (4.4522)  loss_n_60: 4.5671 (4.5654)  loss_n_80: 4.6945 (4.6883)  loss_n_100: 4.6948 (4.7069)  triple_100: 30.2545 (30.2598)  triple_80: 31.8721 (31.7107)  triple_60: 30.2669 (30.2000)  triple_40: 21.2537 (20.8273)  time: 5.9148  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1080/1724]  eta: 1:03:24  lr: 0.000000  loss: 132.8783 (131.4224)  loss_n_40: 4.4458 (4.4523)  loss_n_60: 4.5882 (4.5657)  loss_n_80: 4.6945 (4.6886)  loss_n_100: 4.6948 (4.7072)  triple_100: 30.5034 (30.2632)  triple_80: 31.9252 (31.7144)  triple_60: 30.2669 (30.2037)  triple_40: 21.7477 (20.8273)  time: 5.9140  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1090/1724]  eta: 1:02:25  lr: 0.000000  loss: 135.7553 (131.4471)  loss_n_40: 4.5403 (4.4530)  loss_n_60: 4.6744 (4.5664)  loss_n_80: 4.8062 (4.6891)  loss_n_100: 4.7882 (4.7077)  triple_100: 30.7425 (30.2684)  triple_80: 32.5362 (31.7196)  triple_60: 31.3756 (30.2094)  triple_40: 21.7871 (20.8335)  time: 5.9151  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1100/1724]  eta: 1:01:26  lr: 0.000000  loss: 135.7553 (131.4856)  loss_n_40: 4.5648 (4.4538)  loss_n_60: 4.6744 (4.5671)  loss_n_80: 4.8062 (4.6898)  loss_n_100: 4.7882 (4.7083)  triple_100: 30.7425 (30.2768)  triple_80: 32.5362 (31.7290)  triple_60: 31.3756 (30.2192)  triple_40: 21.4821 (20.8416)  time: 5.9155  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1110/1724]  eta: 1:00:27  lr: 0.000000  loss: 129.9407 (131.4736)  loss_n_40: 4.4128 (4.4534)  loss_n_60: 4.5536 (4.5667)  loss_n_80: 4.6885 (4.6895)  loss_n_100: 4.6999 (4.7080)  triple_100: 30.1886 (30.2747)  triple_80: 31.4398 (31.7268)  triple_60: 30.0689 (30.2164)  triple_40: 20.1962 (20.8381)  time: 5.9145  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1120/1724]  eta: 0:59:28  lr: 0.000000  loss: 125.8749 (131.4075)  loss_n_40: 4.3422 (4.4524)  loss_n_60: 4.4695 (4.5656)  loss_n_80: 4.5648 (4.6884)  loss_n_100: 4.5763 (4.7069)  triple_100: 28.9541 (30.2584)  triple_80: 30.2071 (31.7097)  triple_60: 28.5924 (30.2000)  triple_40: 19.8888 (20.8262)  time: 5.9149  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [1130/1724]  eta: 0:58:29  lr: 0.000000  loss: 125.8749 (131.4238)  loss_n_40: 4.3722 (4.4527)  loss_n_60: 4.4695 (4.5659)  loss_n_80: 4.5624 (4.6886)  loss_n_100: 4.5763 (4.7073)  triple_100: 28.8490 (30.2628)  triple_80: 30.2071 (31.7133)  triple_60: 28.5924 (30.2037)  triple_40: 19.9409 (20.8296)  time: 5.9138  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1140/1724]  eta: 0:57:30  lr: 0.000000  loss: 133.3479 (131.4334)  loss_n_40: 4.4359 (4.4526)  loss_n_60: 4.5689 (4.5659)  loss_n_80: 4.7070 (4.6886)  loss_n_100: 4.7308 (4.7073)  triple_100: 30.6862 (30.2652)  triple_80: 32.0188 (31.7156)  triple_60: 30.6703 (30.2058)  triple_40: 20.7565 (20.8324)  time: 5.9121  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1150/1724]  eta: 0:56:31  lr: 0.000000  loss: 133.3479 (131.4343)  loss_n_40: 4.4562 (4.4526)  loss_n_60: 4.5689 (4.5659)  loss_n_80: 4.7070 (4.6886)  loss_n_100: 4.7308 (4.7073)  triple_100: 30.8396 (30.2657)  triple_80: 32.2431 (31.7149)  triple_60: 30.6703 (30.2056)  triple_40: 21.2263 (20.8338)  time: 5.9118  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1160/1724]  eta: 0:55:32  lr: 0.000000  loss: 132.4598 (131.4230)  loss_n_40: 4.4798 (4.4526)  loss_n_60: 4.5361 (4.5657)  loss_n_80: 4.6535 (4.6884)  loss_n_100: 4.7061 (4.7070)  triple_100: 30.8396 (30.2618)  triple_80: 31.6245 (31.7117)  triple_60: 30.1854 (30.2025)  triple_40: 20.9599 (20.8334)  time: 5.9116  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1170/1724]  eta: 0:54:33  lr: 0.000000  loss: 132.8730 (131.4466)  loss_n_40: 4.5003 (4.4532)  loss_n_60: 4.5716 (4.5662)  loss_n_80: 4.7052 (4.6889)  loss_n_100: 4.7170 (4.7075)  triple_100: 30.4507 (30.2670)  triple_80: 32.1416 (31.7176)  triple_60: 30.5328 (30.2087)  triple_40: 20.7630 (20.8375)  time: 5.9122  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1180/1724]  eta: 0:53:34  lr: 0.000000  loss: 133.6935 (131.4923)  loss_n_40: 4.5283 (4.4539)  loss_n_60: 4.6338 (4.5670)  loss_n_80: 4.7536 (4.6897)  loss_n_100: 4.7758 (4.7083)  triple_100: 30.9360 (30.2781)  triple_80: 32.4152 (31.7292)  triple_60: 30.9227 (30.2203)  triple_40: 20.9791 (20.8457)  time: 5.9122  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1190/1724]  eta: 0:52:35  lr: 0.000000  loss: 131.3597 (131.4758)  loss_n_40: 4.4559 (4.4537)  loss_n_60: 4.5755 (4.5667)  loss_n_80: 4.7027 (4.6894)  loss_n_100: 4.7258 (4.7081)  triple_100: 30.2197 (30.2743)  triple_80: 31.5873 (31.7250)  triple_60: 30.0194 (30.2160)  triple_40: 20.1388 (20.8426)  time: 5.9111  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1200/1724]  eta: 0:51:36  lr: 0.000000  loss: 128.8626 (131.4979)  loss_n_40: 4.4300 (4.4544)  loss_n_60: 4.5298 (4.5672)  loss_n_80: 4.6522 (4.6898)  loss_n_100: 4.6878 (4.7085)  triple_100: 29.5967 (30.2787)  triple_80: 31.0896 (31.7292)  triple_60: 29.5391 (30.2208)  triple_40: 20.3375 (20.8495)  time: 5.9112  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1210/1724]  eta: 0:50:37  lr: 0.000000  loss: 134.5580 (131.5283)  loss_n_40: 4.5266 (4.4550)  loss_n_60: 4.6273 (4.5678)  loss_n_80: 4.7457 (4.6903)  loss_n_100: 4.7600 (4.7089)  triple_100: 31.0589 (30.2856)  triple_80: 32.6574 (31.7356)  triple_60: 31.2196 (30.2283)  triple_40: 20.9871 (20.8567)  time: 5.9113  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1220/1724]  eta: 0:49:37  lr: 0.000000  loss: 141.4286 (131.6124)  loss_n_40: 4.5909 (4.4562)  loss_n_60: 4.7350 (4.5692)  loss_n_80: 4.8587 (4.6916)  loss_n_100: 4.8767 (4.7103)  triple_100: 32.7699 (30.3066)  triple_80: 34.2170 (31.7560)  triple_60: 32.6596 (30.2485)  triple_40: 22.6982 (20.8741)  time: 5.9114  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1230/1724]  eta: 0:48:38  lr: 0.000000  loss: 133.7879 (131.6021)  loss_n_40: 4.5167 (4.4560)  loss_n_60: 4.6080 (4.5689)  loss_n_80: 4.7187 (4.6914)  loss_n_100: 4.7512 (4.7101)  triple_100: 31.0201 (30.3050)  triple_80: 32.0730 (31.7530)  triple_60: 30.5319 (30.2451)  triple_40: 21.5452 (20.8725)  time: 5.9116  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1240/1724]  eta: 0:47:39  lr: 0.000000  loss: 133.3728 (131.5788)  loss_n_40: 4.4786 (4.4556)  loss_n_60: 4.5633 (4.5685)  loss_n_80: 4.6963 (4.6910)  loss_n_100: 4.7351 (4.7098)  triple_100: 30.6535 (30.2999)  triple_80: 31.9488 (31.7475)  triple_60: 30.4094 (30.2392)  triple_40: 21.0171 (20.8674)  time: 5.9117  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1250/1724]  eta: 0:46:40  lr: 0.000000  loss: 127.6583 (131.5272)  loss_n_40: 4.3597 (4.4546)  loss_n_60: 4.4979 (4.5674)  loss_n_80: 4.6214 (4.6900)  loss_n_100: 4.6355 (4.7088)  triple_100: 29.0803 (30.2880)  triple_80: 30.8547 (31.7349)  triple_60: 29.3303 (30.2264)  triple_40: 19.5482 (20.8572)  time: 5.9115  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1260/1724]  eta: 0:45:41  lr: 0.000000  loss: 121.9013 (131.4773)  loss_n_40: 4.2931 (4.4536)  loss_n_60: 4.3920 (4.5664)  loss_n_80: 4.5506 (4.6890)  loss_n_100: 4.5514 (4.7078)  triple_100: 27.7166 (30.2751)  triple_80: 29.7102 (31.7220)  triple_60: 28.1063 (30.2138)  triple_40: 19.4562 (20.8496)  time: 5.9105  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1270/1724]  eta: 0:44:42  lr: 0.000000  loss: 124.1058 (131.4383)  loss_n_40: 4.3564 (4.4530)  loss_n_60: 4.4527 (4.5658)  loss_n_80: 4.5977 (4.6884)  loss_n_100: 4.6205 (4.7073)  triple_100: 28.8109 (30.2663)  triple_80: 30.1541 (31.7133)  triple_60: 28.5639 (30.2051)  triple_40: 19.3284 (20.8391)  time: 5.9105  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1280/1724]  eta: 0:43:43  lr: 0.000000  loss: 124.1058 (131.4137)  loss_n_40: 4.3564 (4.4527)  loss_n_60: 4.4527 (4.5654)  loss_n_80: 4.5977 (4.6880)  loss_n_100: 4.6205 (4.7069)  triple_100: 28.8109 (30.2608)  triple_80: 30.1541 (31.7067)  triple_60: 28.5639 (30.1987)  triple_40: 19.1888 (20.8345)  time: 5.9103  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1290/1724]  eta: 0:42:44  lr: 0.000000  loss: 121.4614 (131.3628)  loss_n_40: 4.3263 (4.4519)  loss_n_60: 4.4118 (4.5644)  loss_n_80: 4.5272 (4.6870)  loss_n_100: 4.5396 (4.7059)  triple_100: 27.9905 (30.2481)  triple_80: 29.1465 (31.6929)  triple_60: 27.7169 (30.1856)  triple_40: 19.1888 (20.8269)  time: 5.9107  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1300/1724]  eta: 0:41:45  lr: 0.000000  loss: 126.4333 (131.3757)  loss_n_40: 4.3771 (4.4520)  loss_n_60: 4.4838 (4.5647)  loss_n_80: 4.6102 (4.6873)  loss_n_100: 4.6278 (4.7062)  triple_100: 28.9510 (30.2515)  triple_80: 30.4767 (31.6965)  triple_60: 29.0682 (30.1893)  triple_40: 20.0986 (20.8282)  time: 5.9128  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1310/1724]  eta: 0:40:46  lr: 0.000000  loss: 134.6043 (131.4102)  loss_n_40: 4.5131 (4.4524)  loss_n_60: 4.6378 (4.5653)  loss_n_80: 4.7666 (4.6879)  loss_n_100: 4.7673 (4.7067)  triple_100: 30.9985 (30.2604)  triple_80: 32.4879 (31.7058)  triple_60: 30.9462 (30.1982)  triple_40: 21.2596 (20.8335)  time: 5.9142  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1320/1724]  eta: 0:39:47  lr: 0.000000  loss: 133.7682 (131.3889)  loss_n_40: 4.4737 (4.4519)  loss_n_60: 4.5972 (4.5647)  loss_n_80: 4.7062 (4.6874)  loss_n_100: 4.7504 (4.7063)  triple_100: 30.9985 (30.2555)  triple_80: 32.2833 (31.7005)  triple_60: 30.6348 (30.1924)  triple_40: 21.2048 (20.8302)  time: 5.9134  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1330/1724]  eta: 0:38:48  lr: 0.000000  loss: 133.7682 (131.3795)  loss_n_40: 4.5079 (4.4517)  loss_n_60: 4.5972 (4.5645)  loss_n_80: 4.7062 (4.6872)  loss_n_100: 4.7403 (4.7061)  triple_100: 30.5007 (30.2531)  triple_80: 31.9803 (31.6982)  triple_60: 30.6121 (30.1899)  triple_40: 21.2048 (20.8286)  time: 5.9135  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1340/1724]  eta: 0:37:49  lr: 0.000000  loss: 132.2365 (131.3907)  loss_n_40: 4.4728 (4.4518)  loss_n_60: 4.5971 (4.5647)  loss_n_80: 4.6856 (4.6874)  loss_n_100: 4.7047 (4.7063)  triple_100: 30.4747 (30.2554)  triple_80: 31.8591 (31.7009)  triple_60: 30.4276 (30.1928)  triple_40: 21.2035 (20.8314)  time: 5.9149  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1350/1724]  eta: 0:36:49  lr: 0.000000  loss: 134.7601 (131.4565)  loss_n_40: 4.4988 (4.4530)  loss_n_60: 4.6140 (4.5661)  loss_n_80: 4.7533 (4.6888)  loss_n_100: 4.7558 (4.7075)  triple_100: 31.1779 (30.2713)  triple_80: 32.7219 (31.7183)  triple_60: 31.0608 (30.2098)  triple_40: 21.0145 (20.8418)  time: 5.9148  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [1360/1724]  eta: 0:35:50  lr: 0.000000  loss: 139.0761 (131.5213)  loss_n_40: 4.6077 (4.4541)  loss_n_60: 4.7408 (4.5674)  loss_n_80: 4.8680 (4.6900)  loss_n_100: 4.8658 (4.7087)  triple_100: 32.0302 (30.2860)  triple_80: 33.8657 (31.7349)  triple_60: 32.1790 (30.2267)  triple_40: 21.8428 (20.8536)  time: 5.9143  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1370/1724]  eta: 0:34:51  lr: 0.000000  loss: 136.2593 (131.5200)  loss_n_40: 4.5735 (4.4541)  loss_n_60: 4.6843 (4.5673)  loss_n_80: 4.7998 (4.6899)  loss_n_100: 4.8078 (4.7086)  triple_100: 31.4367 (30.2850)  triple_80: 33.1253 (31.7344)  triple_60: 31.6334 (30.2265)  triple_40: 21.8428 (20.8542)  time: 5.9152  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1380/1724]  eta: 0:33:52  lr: 0.000000  loss: 135.4772 (131.5474)  loss_n_40: 4.4936 (4.4546)  loss_n_60: 4.6037 (4.5678)  loss_n_80: 4.7207 (4.6904)  loss_n_100: 4.7229 (4.7091)  triple_100: 31.2511 (30.2915)  triple_80: 32.3317 (31.7410)  triple_60: 30.9652 (30.2332)  triple_40: 21.5813 (20.8597)  time: 5.9154  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1390/1724]  eta: 0:32:53  lr: 0.000000  loss: 133.8560 (131.5603)  loss_n_40: 4.4780 (4.4549)  loss_n_60: 4.6370 (4.5681)  loss_n_80: 4.7625 (4.6907)  loss_n_100: 4.7941 (4.7094)  triple_100: 31.2009 (30.2956)  triple_80: 32.6246 (31.7448)  triple_60: 31.0847 (30.2367)  triple_40: 20.7916 (20.8600)  time: 5.9140  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1400/1724]  eta: 0:31:54  lr: 0.000000  loss: 126.9956 (131.5134)  loss_n_40: 4.3508 (4.4540)  loss_n_60: 4.4834 (4.5672)  loss_n_80: 4.6185 (4.6899)  loss_n_100: 4.6222 (4.7086)  triple_100: 29.3367 (30.2852)  triple_80: 30.6823 (31.7333)  triple_60: 29.2007 (30.2250)  triple_40: 19.3311 (20.8502)  time: 5.9136  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1410/1724]  eta: 0:30:55  lr: 0.000000  loss: 132.1811 (131.5800)  loss_n_40: 4.4616 (4.4552)  loss_n_60: 4.5894 (4.5686)  loss_n_80: 4.7041 (4.6912)  loss_n_100: 4.7273 (4.7099)  triple_100: 30.2378 (30.3014)  triple_80: 32.0622 (31.7509)  triple_60: 30.4868 (30.2421)  triple_40: 20.6430 (20.8607)  time: 5.9138  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1420/1724]  eta: 0:29:56  lr: 0.000000  loss: 137.0591 (131.5748)  loss_n_40: 4.5702 (4.4550)  loss_n_60: 4.6984 (4.5685)  loss_n_80: 4.8148 (4.6911)  loss_n_100: 4.8342 (4.7098)  triple_100: 31.6580 (30.2997)  triple_80: 33.3223 (31.7498)  triple_60: 31.6622 (30.2415)  triple_40: 21.4445 (20.8594)  time: 5.9136  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1430/1724]  eta: 0:28:57  lr: 0.000000  loss: 133.5187 (131.5724)  loss_n_40: 4.5131 (4.4552)  loss_n_60: 4.6085 (4.5685)  loss_n_80: 4.7306 (4.6911)  loss_n_100: 4.7445 (4.7098)  triple_100: 30.5576 (30.2985)  triple_80: 32.2680 (31.7488)  triple_60: 30.7522 (30.2404)  triple_40: 21.3517 (20.8601)  time: 5.9128  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1440/1724]  eta: 0:27:58  lr: 0.000000  loss: 132.9044 (131.6067)  loss_n_40: 4.5131 (4.4558)  loss_n_60: 4.6076 (4.5691)  loss_n_80: 4.7306 (4.6918)  loss_n_100: 4.7449 (4.7105)  triple_100: 30.5576 (30.3065)  triple_80: 32.2680 (31.7578)  triple_60: 30.5373 (30.2490)  triple_40: 21.3906 (20.8662)  time: 5.9118  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1450/1724]  eta: 0:26:59  lr: 0.000000  loss: 131.4345 (131.5579)  loss_n_40: 4.4642 (4.4552)  loss_n_60: 4.5570 (4.5683)  loss_n_80: 4.6787 (4.6909)  loss_n_100: 4.6963 (4.7096)  triple_100: 30.2867 (30.2937)  triple_80: 31.7936 (31.7450)  triple_60: 30.0984 (30.2367)  triple_40: 20.6642 (20.8583)  time: 5.9110  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1460/1724]  eta: 0:26:00  lr: 0.000000  loss: 130.2277 (131.5705)  loss_n_40: 4.4305 (4.4554)  loss_n_60: 4.5512 (4.5685)  loss_n_80: 4.6573 (4.6910)  loss_n_100: 4.6791 (4.7098)  triple_100: 30.0662 (30.2966)  triple_80: 31.4083 (31.7473)  triple_60: 29.8037 (30.2394)  triple_40: 20.6642 (20.8624)  time: 5.9098  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1470/1724]  eta: 0:25:00  lr: 0.000000  loss: 132.2148 (131.5566)  loss_n_40: 4.4728 (4.4551)  loss_n_60: 4.5567 (4.5682)  loss_n_80: 4.6985 (4.6909)  loss_n_100: 4.6952 (4.7096)  triple_100: 30.2419 (30.2937)  triple_80: 32.1620 (31.7446)  triple_60: 30.5826 (30.2363)  triple_40: 21.0769 (20.8581)  time: 5.9103  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1480/1724]  eta: 0:24:01  lr: 0.000000  loss: 132.2148 (131.5308)  loss_n_40: 4.4177 (4.4547)  loss_n_60: 4.5567 (4.5678)  loss_n_80: 4.6985 (4.6904)  loss_n_100: 4.6726 (4.7092)  triple_100: 30.1948 (30.2869)  triple_80: 31.7178 (31.7382)  triple_60: 30.4404 (30.2299)  triple_40: 19.9916 (20.8537)  time: 5.9100  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1490/1724]  eta: 0:23:02  lr: 0.000000  loss: 134.0784 (131.6135)  loss_n_40: 4.5234 (4.4561)  loss_n_60: 4.6286 (4.5693)  loss_n_80: 4.7334 (4.6920)  loss_n_100: 4.7390 (4.7107)  triple_100: 30.6960 (30.3076)  triple_80: 32.0070 (31.7592)  triple_60: 30.7968 (30.2506)  triple_40: 21.3713 (20.8682)  time: 5.9090  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1500/1724]  eta: 0:22:03  lr: 0.000000  loss: 138.6737 (131.6508)  loss_n_40: 4.5578 (4.4567)  loss_n_60: 4.6984 (4.5700)  loss_n_80: 4.8182 (4.6926)  loss_n_100: 4.8236 (4.7113)  triple_100: 32.0310 (30.3165)  triple_80: 33.3493 (31.7680)  triple_60: 31.7511 (30.2595)  triple_40: 22.5957 (20.8762)  time: 5.9096  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1510/1724]  eta: 0:21:04  lr: 0.000000  loss: 136.3623 (131.6489)  loss_n_40: 4.5200 (4.4567)  loss_n_60: 4.6033 (4.5700)  loss_n_80: 4.7365 (4.6926)  loss_n_100: 4.7746 (4.7113)  triple_100: 30.9751 (30.3154)  triple_80: 32.9937 (31.7670)  triple_60: 31.3798 (30.2589)  triple_40: 21.5431 (20.8769)  time: 5.9092  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1520/1724]  eta: 0:20:05  lr: 0.000000  loss: 125.5612 (131.6149)  loss_n_40: 4.4282 (4.4560)  loss_n_60: 4.5303 (4.5693)  loss_n_80: 4.6196 (4.6918)  loss_n_100: 4.6351 (4.7106)  triple_100: 28.5984 (30.3079)  triple_80: 29.7807 (31.7585)  triple_60: 28.8294 (30.2501)  triple_40: 20.3323 (20.8708)  time: 5.9088  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1530/1724]  eta: 0:19:06  lr: 0.000000  loss: 128.3138 (131.5922)  loss_n_40: 4.3848 (4.4556)  loss_n_60: 4.4947 (4.5689)  loss_n_80: 4.6405 (4.6914)  loss_n_100: 4.6709 (4.7103)  triple_100: 29.6320 (30.3030)  triple_80: 30.9789 (31.7526)  triple_60: 29.2285 (30.2442)  triple_40: 20.1351 (20.8662)  time: 5.9089  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1540/1724]  eta: 0:18:07  lr: 0.000000  loss: 128.3138 (131.5513)  loss_n_40: 4.3848 (4.4551)  loss_n_60: 4.4947 (4.5682)  loss_n_80: 4.6405 (4.6908)  loss_n_100: 4.6640 (4.7096)  triple_100: 29.7778 (30.2920)  triple_80: 30.9789 (31.7418)  triple_60: 29.2285 (30.2342)  triple_40: 20.1351 (20.8596)  time: 5.9096  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1550/1724]  eta: 0:17:08  lr: 0.000000  loss: 127.8170 (131.5479)  loss_n_40: 4.3908 (4.4551)  loss_n_60: 4.4832 (4.5682)  loss_n_80: 4.6262 (4.6908)  loss_n_100: 4.6499 (4.7095)  triple_100: 29.5888 (30.2914)  triple_80: 30.8773 (31.7412)  triple_60: 29.3459 (30.2336)  triple_40: 19.9050 (20.8580)  time: 5.9106  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1560/1724]  eta: 0:16:09  lr: 0.000000  loss: 129.6952 (131.5277)  loss_n_40: 4.4259 (4.4548)  loss_n_60: 4.5459 (4.5678)  loss_n_80: 4.6784 (4.6904)  loss_n_100: 4.6766 (4.7092)  triple_100: 29.7431 (30.2868)  triple_80: 31.2630 (31.7354)  triple_60: 29.9775 (30.2281)  triple_40: 20.6112 (20.8553)  time: 5.9108  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1570/1724]  eta: 0:15:10  lr: 0.000000  loss: 130.9438 (131.5389)  loss_n_40: 4.4330 (4.4549)  loss_n_60: 4.5730 (4.5681)  loss_n_80: 4.6804 (4.6906)  loss_n_100: 4.6936 (4.7094)  triple_100: 30.0284 (30.2895)  triple_80: 31.5420 (31.7386)  triple_60: 30.0112 (30.2311)  triple_40: 20.8805 (20.8568)  time: 5.9118  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1580/1724]  eta: 0:14:10  lr: 0.000000  loss: 133.8021 (131.5474)  loss_n_40: 4.4601 (4.4550)  loss_n_60: 4.5730 (4.5682)  loss_n_80: 4.7209 (4.6908)  loss_n_100: 4.7285 (4.7096)  triple_100: 31.0488 (30.2918)  triple_80: 32.4239 (31.7412)  triple_60: 30.6679 (30.2332)  triple_40: 21.2354 (20.8577)  time: 5.9129  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:0]  [1590/1724]  eta: 0:13:11  lr: 0.000000  loss: 134.6358 (131.5645)  loss_n_40: 4.5193 (4.4554)  loss_n_60: 4.6145 (4.5685)  loss_n_80: 4.7405 (4.6911)  loss_n_100: 4.7549 (4.7099)  triple_100: 31.0704 (30.2959)  triple_80: 32.5373 (31.7456)  triple_60: 30.8314 (30.2381)  triple_40: 20.9816 (20.8600)  time: 5.9134  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1600/1724]  eta: 0:12:12  lr: 0.000000  loss: 134.7609 (131.5644)  loss_n_40: 4.5330 (4.4555)  loss_n_60: 4.6474 (4.5685)  loss_n_80: 4.7643 (4.6910)  loss_n_100: 4.8010 (4.7098)  triple_100: 31.3204 (30.2949)  triple_80: 32.6122 (31.7446)  triple_60: 31.0692 (30.2377)  triple_40: 20.9816 (20.8623)  time: 5.9136  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1610/1724]  eta: 0:11:13  lr: 0.000000  loss: 128.2425 (131.5496)  loss_n_40: 4.3892 (4.4551)  loss_n_60: 4.4861 (4.5681)  loss_n_80: 4.6310 (4.6907)  loss_n_100: 4.6390 (4.7094)  triple_100: 29.4224 (30.2911)  triple_80: 31.0318 (31.7404)  triple_60: 29.3584 (30.2330)  triple_40: 20.0441 (20.8617)  time: 5.9131  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1620/1724]  eta: 0:10:14  lr: 0.000000  loss: 127.9509 (131.5367)  loss_n_40: 4.3651 (4.4552)  loss_n_60: 4.4798 (4.5680)  loss_n_80: 4.6067 (4.6905)  loss_n_100: 4.6217 (4.7093)  triple_100: 29.3671 (30.2873)  triple_80: 30.7096 (31.7365)  triple_60: 29.1449 (30.2294)  triple_40: 20.2592 (20.8606)  time: 5.9133  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1630/1724]  eta: 0:09:15  lr: 0.000000  loss: 130.2912 (131.5572)  loss_n_40: 4.4403 (4.4553)  loss_n_60: 4.5763 (4.5682)  loss_n_80: 4.6675 (4.6907)  loss_n_100: 4.6957 (4.7095)  triple_100: 30.2005 (30.2927)  triple_80: 31.3843 (31.7418)  triple_60: 29.7561 (30.2343)  triple_40: 21.0947 (20.8646)  time: 5.9146  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1640/1724]  eta: 0:08:16  lr: 0.000000  loss: 132.1490 (131.5603)  loss_n_40: 4.4188 (4.4554)  loss_n_60: 4.5763 (4.5682)  loss_n_80: 4.6921 (4.6907)  loss_n_100: 4.7024 (4.7095)  triple_100: 30.2005 (30.2932)  triple_80: 31.9517 (31.7413)  triple_60: 30.3753 (30.2343)  triple_40: 21.0947 (20.8677)  time: 5.9145  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1650/1724]  eta: 0:07:17  lr: 0.000000  loss: 131.2093 (131.5657)  loss_n_40: 4.4522 (4.4555)  loss_n_60: 4.5713 (4.5683)  loss_n_80: 4.6899 (4.6908)  loss_n_100: 4.7083 (4.7096)  triple_100: 30.1465 (30.2946)  triple_80: 31.4289 (31.7429)  triple_60: 29.9762 (30.2358)  triple_40: 21.0099 (20.8683)  time: 5.9135  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1660/1724]  eta: 0:06:18  lr: 0.000000  loss: 132.7581 (131.5721)  loss_n_40: 4.4868 (4.4556)  loss_n_60: 4.5885 (4.5685)  loss_n_80: 4.7112 (4.6910)  loss_n_100: 4.7137 (4.7097)  triple_100: 30.4302 (30.2957)  triple_80: 32.0679 (31.7449)  triple_60: 30.5459 (30.2378)  triple_40: 20.9549 (20.8689)  time: 5.9144  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1670/1724]  eta: 0:05:19  lr: 0.000000  loss: 128.9473 (131.5539)  loss_n_40: 4.4044 (4.4553)  loss_n_60: 4.5182 (4.5681)  loss_n_80: 4.6527 (4.6906)  loss_n_100: 4.6647 (4.7093)  triple_100: 29.7298 (30.2906)  triple_80: 31.3079 (31.7402)  triple_60: 29.7554 (30.2334)  triple_40: 20.2538 (20.8664)  time: 5.9162  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1680/1724]  eta: 0:04:20  lr: 0.000000  loss: 126.8328 (131.5200)  loss_n_40: 4.3808 (4.4546)  loss_n_60: 4.4736 (4.5674)  loss_n_80: 4.6058 (4.6899)  loss_n_100: 4.6132 (4.7086)  triple_100: 29.1143 (30.2819)  triple_80: 30.7140 (31.7311)  triple_60: 29.2213 (30.2246)  triple_40: 19.9056 (20.8619)  time: 5.9163  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1690/1724]  eta: 0:03:20  lr: 0.000000  loss: 135.6401 (131.5631)  loss_n_40: 4.4997 (4.4552)  loss_n_60: 4.5930 (4.5680)  loss_n_80: 4.7189 (4.6905)  loss_n_100: 4.7491 (4.7093)  triple_100: 31.3259 (30.2923)  triple_80: 32.5945 (31.7415)  triple_60: 31.0217 (30.2348)  triple_40: 21.9426 (20.8714)  time: 5.9155  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1700/1724]  eta: 0:02:21  lr: 0.000000  loss: 127.7912 (131.5074)  loss_n_40: 4.3813 (4.4544)  loss_n_60: 4.5038 (4.5670)  loss_n_80: 4.6398 (4.6894)  loss_n_100: 4.6466 (4.7082)  triple_100: 29.4090 (30.2784)  triple_80: 31.0356 (31.7265)  triple_60: 29.5147 (30.2204)  triple_40: 20.5541 (20.8630)  time: 5.9156  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1710/1724]  eta: 0:01:22  lr: 0.000000  loss: 127.7912 (131.5283)  loss_n_40: 4.3813 (4.4547)  loss_n_60: 4.5038 (4.5674)  loss_n_80: 4.6398 (4.6898)  loss_n_100: 4.6466 (4.7086)  triple_100: 29.4090 (30.2835)  triple_80: 31.0356 (31.7320)  triple_60: 29.5147 (30.2258)  triple_40: 20.2553 (20.8664)  time: 5.9153  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1720/1724]  eta: 0:00:23  lr: 0.000000  loss: 131.9209 (131.5146)  loss_n_40: 4.4479 (4.4544)  loss_n_60: 4.5930 (4.5670)  loss_n_80: 4.7070 (4.6895)  loss_n_100: 4.7232 (4.7084)  triple_100: 30.4331 (30.2807)  triple_80: 32.0418 (31.7282)  triple_60: 30.4436 (30.2217)  triple_40: 20.9684 (20.8647)  time: 5.9149  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0]  [1723/1724]  eta: 0:00:05  lr: 0.000000  loss: 130.2393 (131.5091)  loss_n_40: 4.4322 (4.4543)  loss_n_60: 4.5593 (4.5670)  loss_n_80: 4.6870 (4.6895)  loss_n_100: 4.7066 (4.7083)  triple_100: 29.8164 (30.2792)  triple_80: 31.5765 (31.7273)  triple_60: 30.1491 (30.2206)  triple_40: 20.0157 (20.8628)  time: 5.9150  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:0] Total time: 2:49:48 (5.9099 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 130.2393 (131.5091)  loss_n_40: 4.4322 (4.4543)  loss_n_60: 4.5593 (4.5670)  loss_n_80: 4.6870 (4.6895)  loss_n_100: 4.7066 (4.7083)  triple_100: 29.8164 (30.2792)  triple_80: 31.5765 (31.7273)  triple_60: 30.1491 (30.2206)  triple_40: 20.0157 (20.8628)\n",
      "Valid: [epoch:0]  [  0/845]  eta: 0:21:09  loss: 75.4982 (75.4982)  loss_n_40: 3.5507 (3.5507)  loss_n_60: 3.5578 (3.5578)  loss_n_80: 3.6035 (3.6035)  loss_n_100: 3.6573 (3.6573)  triple_100: 16.1856 (16.1856)  triple_80: 16.5286 (16.5286)  triple_60: 15.8572 (15.8572)  triple_40: 12.5574 (12.5574)  time: 1.5019  data: 0.5228  max mem: 40153\n",
      "Valid: [epoch:0]  [ 10/845]  eta: 0:14:16  loss: 126.2720 (126.8358)  loss_n_40: 4.4347 (4.5056)  loss_n_60: 4.5655 (4.5336)  loss_n_80: 4.7182 (4.6196)  loss_n_100: 4.7080 (4.6314)  triple_100: 29.1488 (28.6823)  triple_80: 30.9738 (30.1187)  triple_60: 29.4715 (28.9854)  triple_40: 19.0227 (20.7593)  time: 1.0260  data: 0.0477  max mem: 40153\n",
      "Valid: [epoch:0]  [ 20/845]  eta: 0:13:47  loss: 129.4244 (128.9377)  loss_n_40: 4.4367 (4.4749)  loss_n_60: 4.5655 (4.5474)  loss_n_80: 4.7137 (4.6529)  loss_n_100: 4.7134 (4.6640)  triple_100: 29.5997 (29.3257)  triple_80: 31.3323 (30.9073)  triple_60: 29.8695 (29.6094)  triple_40: 19.9692 (20.7561)  time: 0.9785  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [ 30/845]  eta: 0:13:31  loss: 129.7184 (134.3372)  loss_n_40: 4.4434 (4.5570)  loss_n_60: 4.6074 (4.6505)  loss_n_80: 4.7382 (4.7609)  loss_n_100: 4.7205 (4.7676)  triple_100: 29.6312 (30.6695)  triple_80: 31.8717 (32.3766)  triple_60: 30.3187 (31.0006)  triple_40: 19.9692 (21.5546)  time: 0.9787  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [ 40/845]  eta: 0:13:18  loss: 127.7582 (130.4749)  loss_n_40: 4.3899 (4.4831)  loss_n_60: 4.5018 (4.5702)  loss_n_80: 4.6601 (4.6826)  loss_n_100: 4.6496 (4.6943)  triple_100: 29.4520 (29.7990)  triple_80: 31.3816 (31.3626)  triple_60: 29.6690 (30.0010)  triple_40: 19.7556 (20.8820)  time: 0.9788  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [ 50/845]  eta: 0:13:06  loss: 126.9628 (131.6461)  loss_n_40: 4.3513 (4.4902)  loss_n_60: 4.4734 (4.5841)  loss_n_80: 4.6293 (4.6977)  loss_n_100: 4.6168 (4.7108)  triple_100: 28.9989 (30.1265)  triple_80: 30.4181 (31.6724)  triple_60: 28.9048 (30.2683)  triple_40: 20.3134 (21.0961)  time: 0.9790  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [ 60/845]  eta: 0:12:55  loss: 128.1977 (133.0038)  loss_n_40: 4.4304 (4.5041)  loss_n_60: 4.5507 (4.6064)  loss_n_80: 4.6787 (4.7202)  loss_n_100: 4.6607 (4.7347)  triple_100: 29.5017 (30.4783)  triple_80: 31.2997 (32.0228)  triple_60: 29.7213 (30.6117)  triple_40: 20.9783 (21.3258)  time: 0.9791  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:0]  [ 70/845]  eta: 0:12:44  loss: 127.6552 (130.8467)  loss_n_40: 4.4081 (4.4742)  loss_n_60: 4.5507 (4.5685)  loss_n_80: 4.6658 (4.6813)  loss_n_100: 4.6607 (4.6963)  triple_100: 29.2370 (29.9516)  triple_80: 31.1516 (31.4612)  triple_60: 29.6721 (30.0588)  triple_40: 19.3923 (20.9549)  time: 0.9790  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [ 80/845]  eta: 0:12:33  loss: 129.7933 (131.8354)  loss_n_40: 4.4998 (4.4932)  loss_n_60: 4.6537 (4.5910)  loss_n_80: 4.7819 (4.7050)  loss_n_100: 4.7656 (4.7173)  triple_100: 29.9033 (30.1936)  triple_80: 32.0528 (31.7431)  triple_60: 30.5406 (30.3283)  triple_40: 19.3759 (21.0640)  time: 0.9791  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [ 90/845]  eta: 0:12:23  loss: 133.6712 (132.0659)  loss_n_40: 4.5316 (4.4950)  loss_n_60: 4.6743 (4.5951)  loss_n_80: 4.7890 (4.7105)  loss_n_100: 4.7768 (4.7224)  triple_100: 30.7849 (30.2578)  triple_80: 32.6259 (31.8198)  triple_60: 31.0791 (30.3944)  triple_40: 20.4607 (21.0709)  time: 0.9791  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:0]  [100/845]  eta: 0:12:13  loss: 129.6018 (131.1522)  loss_n_40: 4.4415 (4.4736)  loss_n_60: 4.5607 (4.5717)  loss_n_80: 4.6938 (4.6899)  loss_n_100: 4.7060 (4.7025)  triple_100: 29.8686 (30.0536)  triple_80: 31.6401 (31.5851)  triple_60: 30.0655 (30.1373)  triple_40: 19.8411 (20.9385)  time: 0.9787  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [110/845]  eta: 0:12:02  loss: 134.6871 (131.6662)  loss_n_40: 4.5124 (4.4819)  loss_n_60: 4.6542 (4.5808)  loss_n_80: 4.7929 (4.6989)  loss_n_100: 4.7686 (4.7130)  triple_100: 30.9079 (30.1793)  triple_80: 32.5791 (31.7189)  triple_60: 30.7482 (30.2621)  triple_40: 21.5987 (21.0312)  time: 0.9784  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [120/845]  eta: 0:11:52  loss: 135.5815 (131.4799)  loss_n_40: 4.6134 (4.4805)  loss_n_60: 4.7226 (4.5781)  loss_n_80: 4.8327 (4.6960)  loss_n_100: 4.8378 (4.7106)  triple_100: 31.1637 (30.1353)  triple_80: 32.9829 (31.6663)  triple_60: 31.6754 (30.2150)  triple_40: 21.6857 (20.9981)  time: 0.9785  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [130/845]  eta: 0:11:42  loss: 125.8987 (131.5436)  loss_n_40: 4.4019 (4.4814)  loss_n_60: 4.5380 (4.5802)  loss_n_80: 4.6789 (4.6991)  loss_n_100: 4.6675 (4.7135)  triple_100: 29.1138 (30.1570)  triple_80: 30.8541 (31.7036)  triple_60: 29.3502 (30.2407)  triple_40: 20.6336 (20.9681)  time: 0.9782  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [140/845]  eta: 0:11:32  loss: 125.8326 (130.7911)  loss_n_40: 4.4019 (4.4651)  loss_n_60: 4.5380 (4.5636)  loss_n_80: 4.6660 (4.6819)  loss_n_100: 4.6675 (4.6970)  triple_100: 28.9553 (29.9705)  triple_80: 30.7270 (31.5063)  triple_60: 29.1531 (30.0466)  triple_40: 19.9354 (20.8602)  time: 0.9780  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [150/845]  eta: 0:11:22  loss: 130.6406 (131.5922)  loss_n_40: 4.5284 (4.4800)  loss_n_60: 4.6401 (4.5782)  loss_n_80: 4.7656 (4.6963)  loss_n_100: 4.7420 (4.7121)  triple_100: 29.4752 (30.1725)  triple_80: 31.9520 (31.6994)  triple_60: 30.3582 (30.2382)  triple_40: 20.5770 (21.0154)  time: 0.9781  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [160/845]  eta: 0:11:12  loss: 131.7891 (130.7764)  loss_n_40: 4.5528 (4.4652)  loss_n_60: 4.6424 (4.5627)  loss_n_80: 4.7891 (4.6807)  loss_n_100: 4.7671 (4.6975)  triple_100: 30.2464 (29.9855)  triple_80: 31.9520 (31.4910)  triple_60: 30.3582 (30.0316)  triple_40: 20.3734 (20.8623)  time: 0.9781  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [170/845]  eta: 0:11:02  loss: 125.6826 (130.4388)  loss_n_40: 4.3677 (4.4585)  loss_n_60: 4.5093 (4.5562)  loss_n_80: 4.6414 (4.6748)  loss_n_100: 4.6359 (4.6915)  triple_100: 28.7218 (29.9093)  triple_80: 30.4310 (31.4125)  triple_60: 28.7879 (29.9466)  triple_40: 19.0347 (20.7893)  time: 0.9782  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [180/845]  eta: 0:10:52  loss: 126.6728 (130.5039)  loss_n_40: 4.3677 (4.4567)  loss_n_60: 4.5339 (4.5572)  loss_n_80: 4.6816 (4.6754)  loss_n_100: 4.6712 (4.6923)  triple_100: 29.0054 (29.9312)  triple_80: 30.9661 (31.4273)  triple_60: 29.3965 (29.9636)  triple_40: 19.1789 (20.8003)  time: 0.9781  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [190/845]  eta: 0:10:42  loss: 130.6651 (130.5604)  loss_n_40: 4.4681 (4.4585)  loss_n_60: 4.5614 (4.5579)  loss_n_80: 4.7127 (4.6761)  loss_n_100: 4.7111 (4.6927)  triple_100: 29.9443 (29.9405)  triple_80: 31.6883 (31.4415)  triple_60: 29.9562 (29.9731)  triple_40: 20.0033 (20.8202)  time: 0.9778  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [200/845]  eta: 0:10:32  loss: 129.7323 (130.2795)  loss_n_40: 4.4591 (4.4539)  loss_n_60: 4.5386 (4.5534)  loss_n_80: 4.6886 (4.6719)  loss_n_100: 4.6734 (4.6879)  triple_100: 29.4446 (29.8706)  triple_80: 31.3370 (31.3749)  triple_60: 29.8941 (29.9048)  triple_40: 20.0588 (20.7621)  time: 0.9778  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [210/845]  eta: 0:10:22  loss: 124.2241 (129.9355)  loss_n_40: 4.3797 (4.4458)  loss_n_60: 4.4317 (4.5462)  loss_n_80: 4.5742 (4.6648)  loss_n_100: 4.5836 (4.6817)  triple_100: 28.7961 (29.7987)  triple_80: 30.1752 (31.2915)  triple_60: 28.5245 (29.8217)  triple_40: 19.1626 (20.6852)  time: 0.9777  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [220/845]  eta: 0:10:12  loss: 128.5224 (130.3223)  loss_n_40: 4.4009 (4.4511)  loss_n_60: 4.5188 (4.5526)  loss_n_80: 4.6679 (4.6723)  loss_n_100: 4.6649 (4.6888)  triple_100: 29.1566 (29.8994)  triple_80: 30.8425 (31.3968)  triple_60: 29.3785 (29.9172)  triple_40: 20.2666 (20.7441)  time: 0.9777  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [230/845]  eta: 0:10:03  loss: 132.5049 (130.9858)  loss_n_40: 4.5039 (4.4624)  loss_n_60: 4.6204 (4.5650)  loss_n_80: 4.7514 (4.6845)  loss_n_100: 4.7622 (4.7003)  triple_100: 30.6594 (30.0525)  triple_80: 32.2280 (31.5603)  triple_60: 30.4713 (30.0802)  triple_40: 21.0280 (20.8807)  time: 0.9776  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [240/845]  eta: 0:09:53  loss: 133.5226 (131.0620)  loss_n_40: 4.4720 (4.4624)  loss_n_60: 4.5647 (4.5645)  loss_n_80: 4.7033 (4.6844)  loss_n_100: 4.7024 (4.7001)  triple_100: 30.5807 (30.0681)  triple_80: 32.4130 (31.5772)  triple_60: 30.9243 (30.0934)  triple_40: 21.7655 (20.9118)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [250/845]  eta: 0:09:43  loss: 128.1018 (130.1709)  loss_n_40: 4.3818 (4.4474)  loss_n_60: 4.4334 (4.5485)  loss_n_80: 4.5681 (4.6683)  loss_n_100: 4.5962 (4.6845)  triple_100: 28.8170 (29.8596)  triple_80: 30.6495 (31.3473)  triple_60: 29.1250 (29.8648)  triple_40: 20.6542 (20.7506)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [260/845]  eta: 0:09:33  loss: 125.7884 (130.2231)  loss_n_40: 4.3730 (4.4485)  loss_n_60: 4.4334 (4.5497)  loss_n_80: 4.5681 (4.6698)  loss_n_100: 4.5913 (4.6861)  triple_100: 29.0658 (29.8754)  triple_80: 29.9346 (31.3623)  triple_60: 28.3348 (29.8787)  triple_40: 19.6309 (20.7526)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [270/845]  eta: 0:09:23  loss: 128.2056 (130.4935)  loss_n_40: 4.4387 (4.4526)  loss_n_60: 4.5650 (4.5549)  loss_n_80: 4.6989 (4.6748)  loss_n_100: 4.7094 (4.6909)  triple_100: 29.4329 (29.9404)  triple_80: 31.1987 (31.4344)  triple_60: 29.7167 (29.9512)  triple_40: 19.7080 (20.7944)  time: 0.9776  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [280/845]  eta: 0:09:13  loss: 133.7321 (130.7040)  loss_n_40: 4.5358 (4.4552)  loss_n_60: 4.6477 (4.5576)  loss_n_80: 4.7772 (4.6779)  loss_n_100: 4.7688 (4.6939)  triple_100: 30.3772 (29.9907)  triple_80: 32.5552 (31.4884)  triple_60: 31.0448 (30.0018)  triple_40: 21.0980 (20.8384)  time: 0.9776  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [290/845]  eta: 0:09:03  loss: 131.5351 (130.7265)  loss_n_40: 4.5000 (4.4550)  loss_n_60: 4.6261 (4.5573)  loss_n_80: 4.7440 (4.6779)  loss_n_100: 4.7311 (4.6939)  triple_100: 30.1576 (29.9981)  triple_80: 32.1339 (31.4963)  triple_60: 30.4822 (30.0049)  triple_40: 20.9592 (20.8430)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [300/845]  eta: 0:08:54  loss: 128.8164 (130.6645)  loss_n_40: 4.4155 (4.4539)  loss_n_60: 4.5315 (4.5560)  loss_n_80: 4.6447 (4.6765)  loss_n_100: 4.6384 (4.6927)  triple_100: 29.9057 (29.9828)  triple_80: 31.1935 (31.4812)  triple_60: 29.4054 (29.9884)  triple_40: 20.5647 (20.8330)  time: 0.9777  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:0]  [310/845]  eta: 0:08:44  loss: 131.0869 (130.7830)  loss_n_40: 4.4835 (4.4552)  loss_n_60: 4.6272 (4.5588)  loss_n_80: 4.7347 (4.6795)  loss_n_100: 4.7382 (4.6957)  triple_100: 30.0391 (30.0202)  triple_80: 32.0149 (31.5205)  triple_60: 30.4477 (30.0236)  triple_40: 20.1729 (20.8295)  time: 0.9777  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [320/845]  eta: 0:08:34  loss: 131.0869 (130.8465)  loss_n_40: 4.4932 (4.4577)  loss_n_60: 4.6487 (4.5607)  loss_n_80: 4.7652 (4.6810)  loss_n_100: 4.7683 (4.6972)  triple_100: 30.0391 (30.0309)  triple_80: 32.2884 (31.5353)  triple_60: 30.6491 (30.0436)  triple_40: 20.2968 (20.8400)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [330/845]  eta: 0:08:24  loss: 128.9031 (130.7187)  loss_n_40: 4.4814 (4.4549)  loss_n_60: 4.5888 (4.5579)  loss_n_80: 4.7348 (4.6780)  loss_n_100: 4.7328 (4.6946)  triple_100: 29.5795 (30.0019)  triple_80: 31.5162 (31.4992)  triple_60: 29.8093 (30.0099)  triple_40: 19.7486 (20.8223)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [340/845]  eta: 0:08:14  loss: 134.5525 (131.1191)  loss_n_40: 4.5443 (4.4620)  loss_n_60: 4.7006 (4.5653)  loss_n_80: 4.8141 (4.6852)  loss_n_100: 4.8134 (4.7020)  triple_100: 30.9731 (30.1011)  triple_80: 32.9567 (31.5969)  triple_60: 31.3740 (30.1086)  triple_40: 20.4535 (20.8979)  time: 0.9776  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [350/845]  eta: 0:08:04  loss: 129.9444 (131.0226)  loss_n_40: 4.4064 (4.4593)  loss_n_60: 4.5600 (4.5630)  loss_n_80: 4.6834 (4.6829)  loss_n_100: 4.6849 (4.7001)  triple_100: 29.4469 (30.0832)  triple_80: 31.7188 (31.5719)  triple_60: 30.2704 (30.0837)  triple_40: 20.3066 (20.8786)  time: 0.9776  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [360/845]  eta: 0:07:55  loss: 127.2624 (131.0111)  loss_n_40: 4.4064 (4.4596)  loss_n_60: 4.5600 (4.5631)  loss_n_80: 4.6660 (4.6831)  loss_n_100: 4.6776 (4.7007)  triple_100: 29.2606 (30.0833)  triple_80: 30.5827 (31.5708)  triple_60: 29.0016 (30.0797)  triple_40: 19.8069 (20.8708)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [370/845]  eta: 0:07:45  loss: 131.0304 (131.1188)  loss_n_40: 4.5225 (4.4628)  loss_n_60: 4.6382 (4.5664)  loss_n_80: 4.7646 (4.6865)  loss_n_100: 4.7613 (4.7037)  triple_100: 30.2919 (30.1090)  triple_80: 31.9902 (31.6015)  triple_60: 30.3382 (30.1101)  triple_40: 20.4581 (20.8788)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [380/845]  eta: 0:07:35  loss: 138.9446 (131.1207)  loss_n_40: 4.6070 (4.4630)  loss_n_60: 4.7372 (4.5661)  loss_n_80: 4.8495 (4.6863)  loss_n_100: 4.8321 (4.7032)  triple_100: 32.0204 (30.1066)  triple_80: 33.8934 (31.6020)  triple_60: 32.3756 (30.1125)  triple_40: 21.4520 (20.8808)  time: 0.9776  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [390/845]  eta: 0:07:25  loss: 128.4871 (130.9962)  loss_n_40: 4.4727 (4.4612)  loss_n_60: 4.5859 (4.5646)  loss_n_80: 4.7313 (4.6848)  loss_n_100: 4.7284 (4.7017)  triple_100: 29.5400 (30.0791)  triple_80: 31.5153 (31.5745)  triple_60: 29.8255 (30.0826)  triple_40: 19.5218 (20.8477)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [400/845]  eta: 0:07:15  loss: 128.1154 (130.7180)  loss_n_40: 4.4252 (4.4569)  loss_n_60: 4.5762 (4.5601)  loss_n_80: 4.7161 (4.6803)  loss_n_100: 4.7033 (4.6970)  triple_100: 29.3187 (30.0076)  triple_80: 31.3557 (31.5066)  triple_60: 29.7293 (30.0158)  triple_40: 19.4371 (20.7936)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [410/845]  eta: 0:07:05  loss: 129.1469 (131.0491)  loss_n_40: 4.4570 (4.4630)  loss_n_60: 4.5835 (4.5670)  loss_n_80: 4.7180 (4.6873)  loss_n_100: 4.7165 (4.7034)  triple_100: 29.5894 (30.0851)  triple_80: 31.9026 (31.5947)  triple_60: 30.1889 (30.1023)  triple_40: 19.8708 (20.8464)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [420/845]  eta: 0:06:56  loss: 134.6224 (131.1268)  loss_n_40: 4.5784 (4.4649)  loss_n_60: 4.6738 (4.5684)  loss_n_80: 4.7914 (4.6887)  loss_n_100: 4.8093 (4.7051)  triple_100: 30.8252 (30.1081)  triple_80: 32.9550 (31.6132)  triple_60: 31.5064 (30.1193)  triple_40: 20.7420 (20.8591)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [430/845]  eta: 0:06:46  loss: 136.3827 (131.2410)  loss_n_40: 4.5784 (4.4651)  loss_n_60: 4.6738 (4.5698)  loss_n_80: 4.7901 (4.6903)  loss_n_100: 4.7975 (4.7066)  triple_100: 30.8252 (30.1368)  triple_80: 32.8806 (31.6444)  triple_60: 31.3562 (30.1486)  triple_40: 22.0273 (20.8793)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [440/845]  eta: 0:06:36  loss: 129.8966 (130.9996)  loss_n_40: 4.4421 (4.4609)  loss_n_60: 4.5819 (4.5652)  loss_n_80: 4.7197 (4.6858)  loss_n_100: 4.6955 (4.7020)  triple_100: 29.9975 (30.0780)  triple_80: 31.5711 (31.5836)  triple_60: 29.8401 (30.0887)  triple_40: 19.7333 (20.8354)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [450/845]  eta: 0:06:26  loss: 129.2022 (131.1880)  loss_n_40: 4.4421 (4.4642)  loss_n_60: 4.5455 (4.5687)  loss_n_80: 4.6684 (4.6894)  loss_n_100: 4.6825 (4.7056)  triple_100: 29.9360 (30.1241)  triple_80: 31.3920 (31.6308)  triple_60: 29.7752 (30.1343)  triple_40: 20.3158 (20.8710)  time: 0.9773  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:0]  [460/845]  eta: 0:06:16  loss: 129.4529 (131.0640)  loss_n_40: 4.4546 (4.4625)  loss_n_60: 4.5455 (4.5664)  loss_n_80: 4.7002 (4.6869)  loss_n_100: 4.7158 (4.7033)  triple_100: 29.8172 (30.0944)  triple_80: 31.7134 (31.5953)  triple_60: 29.9620 (30.1008)  triple_40: 20.6765 (20.8544)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [470/845]  eta: 0:06:07  loss: 128.2397 (130.9868)  loss_n_40: 4.3910 (4.4590)  loss_n_60: 4.5133 (4.5635)  loss_n_80: 4.6321 (4.6842)  loss_n_100: 4.6497 (4.7008)  triple_100: 29.3091 (30.0773)  triple_80: 31.2247 (31.5750)  triple_60: 29.5597 (30.0805)  triple_40: 19.3221 (20.8464)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [480/845]  eta: 0:05:57  loss: 127.5149 (130.9996)  loss_n_40: 4.3773 (4.4593)  loss_n_60: 4.5133 (4.5637)  loss_n_80: 4.6368 (4.6842)  loss_n_100: 4.6497 (4.7010)  triple_100: 29.1720 (30.0818)  triple_80: 31.2247 (31.5764)  triple_60: 29.5597 (30.0834)  triple_40: 19.3221 (20.8497)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [490/845]  eta: 0:05:47  loss: 128.5105 (130.9670)  loss_n_40: 4.4644 (4.4600)  loss_n_60: 4.5775 (4.5640)  loss_n_80: 4.7302 (4.6845)  loss_n_100: 4.7354 (4.7010)  triple_100: 29.6176 (30.0704)  triple_80: 31.3852 (31.5694)  triple_60: 29.6388 (30.0778)  triple_40: 18.8884 (20.8398)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [500/845]  eta: 0:05:37  loss: 128.5867 (131.0419)  loss_n_40: 4.4644 (4.4613)  loss_n_60: 4.5775 (4.5657)  loss_n_80: 4.7163 (4.6864)  loss_n_100: 4.7354 (4.7025)  triple_100: 29.6005 (30.0849)  triple_80: 31.4487 (31.5943)  triple_60: 29.9286 (30.1012)  triple_40: 19.4598 (20.8455)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [510/845]  eta: 0:05:27  loss: 128.3913 (130.8052)  loss_n_40: 4.4060 (4.4583)  loss_n_60: 4.5278 (4.5619)  loss_n_80: 4.6652 (4.6825)  loss_n_100: 4.6533 (4.6986)  triple_100: 29.2534 (30.0265)  triple_80: 31.1836 (31.5325)  triple_60: 29.6391 (30.0410)  triple_40: 19.4541 (20.8038)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [520/845]  eta: 0:05:18  loss: 126.7246 (130.8198)  loss_n_40: 4.4030 (4.4570)  loss_n_60: 4.5133 (4.5614)  loss_n_80: 4.6594 (4.6822)  loss_n_100: 4.6075 (4.6983)  triple_100: 29.0154 (30.0313)  triple_80: 31.0413 (31.5369)  triple_60: 29.2695 (30.0427)  triple_40: 18.7155 (20.8100)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [530/845]  eta: 0:05:08  loss: 138.0704 (130.9326)  loss_n_40: 4.5530 (4.4601)  loss_n_60: 4.6475 (4.5639)  loss_n_80: 4.7889 (4.6845)  loss_n_100: 4.8054 (4.7002)  triple_100: 31.1852 (30.0526)  triple_80: 33.2206 (31.5631)  triple_60: 31.5733 (30.0725)  triple_40: 21.8416 (20.8357)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [540/845]  eta: 0:04:58  loss: 126.7771 (130.7258)  loss_n_40: 4.4417 (4.4554)  loss_n_60: 4.4646 (4.5593)  loss_n_80: 4.5971 (4.6801)  loss_n_100: 4.5829 (4.6961)  triple_100: 29.4128 (30.0069)  triple_80: 30.2640 (31.5103)  triple_60: 28.5323 (30.0168)  triple_40: 19.4075 (20.8009)  time: 0.9770  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:0]  [550/845]  eta: 0:04:48  loss: 125.6524 (130.7819)  loss_n_40: 4.3119 (4.4556)  loss_n_60: 4.3606 (4.5600)  loss_n_80: 4.5160 (4.6807)  loss_n_100: 4.5500 (4.6968)  triple_100: 28.6093 (30.0228)  triple_80: 30.2528 (31.5245)  triple_60: 28.4373 (30.0312)  triple_40: 18.6075 (20.8102)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [560/845]  eta: 0:04:38  loss: 134.4919 (130.8569)  loss_n_40: 4.5504 (4.4571)  loss_n_60: 4.6743 (4.5614)  loss_n_80: 4.8140 (4.6819)  loss_n_100: 4.8127 (4.6978)  triple_100: 31.1821 (30.0379)  triple_80: 32.8825 (31.5395)  triple_60: 31.1634 (30.0492)  triple_40: 20.4125 (20.8321)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [570/845]  eta: 0:04:29  loss: 134.0899 (130.9291)  loss_n_40: 4.5453 (4.4582)  loss_n_60: 4.6743 (4.5627)  loss_n_80: 4.7846 (4.6833)  loss_n_100: 4.7745 (4.6991)  triple_100: 30.8300 (30.0561)  triple_80: 32.6891 (31.5593)  triple_60: 31.1634 (30.0682)  triple_40: 20.5306 (20.8422)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [580/845]  eta: 0:04:19  loss: 127.9247 (130.9333)  loss_n_40: 4.4172 (4.4577)  loss_n_60: 4.5550 (4.5628)  loss_n_80: 4.6895 (4.6834)  loss_n_100: 4.6926 (4.6993)  triple_100: 29.6116 (30.0582)  triple_80: 31.3538 (31.5628)  triple_60: 29.6284 (30.0711)  triple_40: 20.1552 (20.8381)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [590/845]  eta: 0:04:09  loss: 125.4572 (130.6383)  loss_n_40: 4.3872 (4.4523)  loss_n_60: 4.5262 (4.5573)  loss_n_80: 4.6513 (4.6782)  loss_n_100: 4.6518 (4.6944)  triple_100: 28.5859 (29.9902)  triple_80: 30.8408 (31.4910)  triple_60: 29.3665 (29.9984)  triple_40: 18.3959 (20.7764)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [600/845]  eta: 0:03:59  loss: 125.9979 (130.7345)  loss_n_40: 4.3995 (4.4538)  loss_n_60: 4.5256 (4.5589)  loss_n_80: 4.6513 (4.6797)  loss_n_100: 4.6463 (4.6960)  triple_100: 28.9006 (30.0138)  triple_80: 30.6629 (31.5149)  triple_60: 28.9875 (30.0222)  triple_40: 18.4611 (20.7953)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [610/845]  eta: 0:03:49  loss: 128.5487 (130.8448)  loss_n_40: 4.4737 (4.4556)  loss_n_60: 4.5484 (4.5608)  loss_n_80: 4.6889 (4.6817)  loss_n_100: 4.7113 (4.6977)  triple_100: 29.2443 (30.0374)  triple_80: 31.4460 (31.5438)  triple_60: 29.7873 (30.0511)  triple_40: 19.9797 (20.8166)  time: 0.9772  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:0]  [620/845]  eta: 0:03:40  loss: 129.6801 (130.8539)  loss_n_40: 4.4596 (4.4559)  loss_n_60: 4.5704 (4.5607)  loss_n_80: 4.6889 (4.6817)  loss_n_100: 4.6823 (4.6977)  triple_100: 29.7732 (30.0381)  triple_80: 31.5269 (31.5457)  triple_60: 30.1040 (30.0530)  triple_40: 20.6253 (20.8211)  time: 0.9772  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:0]  [630/845]  eta: 0:03:30  loss: 141.7990 (131.1781)  loss_n_40: 4.7083 (4.4616)  loss_n_60: 4.8081 (4.5667)  loss_n_80: 4.9630 (4.6876)  loss_n_100: 4.9559 (4.7036)  triple_100: 32.5435 (30.1132)  triple_80: 34.6461 (31.6269)  triple_60: 32.8187 (30.1353)  triple_40: 22.3553 (20.8832)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [640/845]  eta: 0:03:20  loss: 140.9691 (131.2793)  loss_n_40: 4.5519 (4.4636)  loss_n_60: 4.6935 (4.5692)  loss_n_80: 4.8195 (4.6900)  loss_n_100: 4.8351 (4.7060)  triple_100: 31.9703 (30.1401)  triple_80: 34.0368 (31.6551)  triple_60: 32.7875 (30.1634)  triple_40: 22.3553 (20.8919)  time: 0.9775  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [650/845]  eta: 0:03:10  loss: 131.0777 (131.3148)  loss_n_40: 4.4903 (4.4645)  loss_n_60: 4.6126 (4.5701)  loss_n_80: 4.7451 (4.6909)  loss_n_100: 4.7492 (4.7069)  triple_100: 30.2953 (30.1471)  triple_80: 31.9613 (31.6649)  triple_60: 30.5338 (30.1742)  triple_40: 20.4472 (20.8962)  time: 0.9777  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [660/845]  eta: 0:03:01  loss: 131.2724 (131.3557)  loss_n_40: 4.5177 (4.4655)  loss_n_60: 4.6684 (4.5714)  loss_n_80: 4.7873 (4.6921)  loss_n_100: 4.8013 (4.7080)  triple_100: 30.5647 (30.1580)  triple_80: 32.3265 (31.6770)  triple_60: 30.7673 (30.1868)  triple_40: 21.2586 (20.8970)  time: 0.9776  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [670/845]  eta: 0:02:51  loss: 138.2480 (131.5424)  loss_n_40: 4.5903 (4.4692)  loss_n_60: 4.6857 (4.5755)  loss_n_80: 4.8192 (4.6962)  loss_n_100: 4.8383 (4.7119)  triple_100: 31.4032 (30.2018)  triple_80: 33.3671 (31.7269)  triple_60: 32.0519 (30.2363)  triple_40: 21.3528 (20.9247)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [680/845]  eta: 0:02:41  loss: 128.8906 (131.5552)  loss_n_40: 4.4850 (4.4699)  loss_n_60: 4.5963 (4.5762)  loss_n_80: 4.7429 (4.6970)  loss_n_100: 4.7308 (4.7127)  triple_100: 30.0315 (30.2060)  triple_80: 31.8691 (31.7334)  triple_60: 30.0504 (30.2417)  triple_40: 19.3296 (20.9182)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [690/845]  eta: 0:02:31  loss: 131.4210 (131.7322)  loss_n_40: 4.4825 (4.4732)  loss_n_60: 4.5963 (4.5797)  loss_n_80: 4.7373 (4.7004)  loss_n_100: 4.7241 (4.7160)  triple_100: 29.9069 (30.2476)  triple_80: 31.8691 (31.7789)  triple_60: 30.3430 (30.2863)  triple_40: 19.8428 (20.9501)  time: 0.9776  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [700/845]  eta: 0:02:21  loss: 129.1126 (131.5926)  loss_n_40: 4.4104 (4.4702)  loss_n_60: 4.5642 (4.5768)  loss_n_80: 4.6863 (4.6977)  loss_n_100: 4.6894 (4.7133)  triple_100: 29.7811 (30.2145)  triple_80: 31.7413 (31.7455)  triple_60: 29.9942 (30.2514)  triple_40: 19.8428 (20.9232)  time: 0.9775  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:0]  [710/845]  eta: 0:02:12  loss: 125.2480 (131.5770)  loss_n_40: 4.3605 (4.4692)  loss_n_60: 4.4277 (4.5757)  loss_n_80: 4.5595 (4.6967)  loss_n_100: 4.5823 (4.7125)  triple_100: 28.8360 (30.2142)  triple_80: 30.6387 (31.7401)  triple_60: 28.9947 (30.2447)  triple_40: 19.1866 (20.9240)  time: 0.9774  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:0]  [720/845]  eta: 0:02:02  loss: 121.3883 (131.2278)  loss_n_40: 4.2709 (4.4638)  loss_n_60: 4.2919 (4.5692)  loss_n_80: 4.4790 (4.6903)  loss_n_100: 4.5086 (4.7061)  triple_100: 27.6443 (30.1281)  triple_80: 29.0950 (31.6482)  triple_60: 27.1323 (30.1554)  triple_40: 19.3629 (20.8666)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [730/845]  eta: 0:01:52  loss: 122.4468 (131.1270)  loss_n_40: 4.3282 (4.4625)  loss_n_60: 4.4182 (4.5678)  loss_n_80: 4.5844 (4.6887)  loss_n_100: 4.5613 (4.7045)  triple_100: 28.1849 (30.1019)  triple_80: 29.7576 (31.6237)  triple_60: 28.1525 (30.1315)  triple_40: 18.4274 (20.8463)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [740/845]  eta: 0:01:42  loss: 127.5905 (131.1668)  loss_n_40: 4.4135 (4.4634)  loss_n_60: 4.5527 (4.5687)  loss_n_80: 4.7073 (4.6897)  loss_n_100: 4.6999 (4.7053)  triple_100: 29.3240 (30.1083)  triple_80: 31.1889 (31.6342)  triple_60: 29.7607 (30.1424)  triple_40: 19.7573 (20.8549)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [750/845]  eta: 0:01:32  loss: 128.7066 (131.1644)  loss_n_40: 4.4135 (4.4628)  loss_n_60: 4.5475 (4.5682)  loss_n_80: 4.6757 (4.6896)  loss_n_100: 4.6892 (4.7052)  triple_100: 29.3626 (30.1097)  triple_80: 31.4200 (31.6357)  triple_60: 29.7607 (30.1400)  triple_40: 20.2449 (20.8532)  time: 0.9775  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [760/845]  eta: 0:01:23  loss: 128.7066 (131.1575)  loss_n_40: 4.4195 (4.4629)  loss_n_60: 4.5628 (4.5687)  loss_n_80: 4.7014 (4.6902)  loss_n_100: 4.6905 (4.7056)  triple_100: 29.3851 (30.1073)  triple_80: 31.4200 (31.6373)  triple_60: 29.6277 (30.1411)  triple_40: 19.7109 (20.8444)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [770/845]  eta: 0:01:13  loss: 127.4583 (131.0524)  loss_n_40: 4.4409 (4.4612)  loss_n_60: 4.5935 (4.5667)  loss_n_80: 4.7208 (4.6882)  loss_n_100: 4.7051 (4.7036)  triple_100: 29.0488 (30.0802)  triple_80: 31.1818 (31.6101)  triple_60: 29.6268 (30.1139)  triple_40: 19.2835 (20.8284)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [780/845]  eta: 0:01:03  loss: 130.7433 (131.1539)  loss_n_40: 4.4874 (4.4629)  loss_n_60: 4.6185 (4.5687)  loss_n_80: 4.7426 (4.6902)  loss_n_100: 4.7571 (4.7054)  triple_100: 29.9523 (30.1033)  triple_80: 31.8769 (31.6354)  triple_60: 30.2688 (30.1396)  triple_40: 19.7794 (20.8484)  time: 0.9775  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:0]  [790/845]  eta: 0:00:53  loss: 130.7433 (131.1476)  loss_n_40: 4.5242 (4.4627)  loss_n_60: 4.7021 (4.5687)  loss_n_80: 4.8315 (4.6901)  loss_n_100: 4.8103 (4.7054)  triple_100: 30.1441 (30.1029)  triple_80: 32.3241 (31.6348)  triple_60: 30.8584 (30.1384)  triple_40: 19.7794 (20.8447)  time: 0.9777  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [800/845]  eta: 0:00:44  loss: 130.3007 (131.1828)  loss_n_40: 4.4434 (4.4639)  loss_n_60: 4.6120 (4.5696)  loss_n_80: 4.7465 (4.6908)  loss_n_100: 4.7472 (4.7059)  triple_100: 30.0465 (30.1071)  triple_80: 32.0117 (31.6417)  triple_60: 30.3691 (30.1471)  triple_40: 19.6535 (20.8567)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [810/845]  eta: 0:00:34  loss: 134.2800 (131.2204)  loss_n_40: 4.5262 (4.4654)  loss_n_60: 4.6234 (4.5708)  loss_n_80: 4.7765 (4.6921)  loss_n_100: 4.7744 (4.7069)  triple_100: 30.9110 (30.1134)  triple_80: 32.8435 (31.6527)  triple_60: 31.1255 (30.1590)  triple_40: 20.5346 (20.8602)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [820/845]  eta: 0:00:24  loss: 128.1573 (131.1756)  loss_n_40: 4.4820 (4.4658)  loss_n_60: 4.6122 (4.5704)  loss_n_80: 4.7397 (4.6915)  loss_n_100: 4.7619 (4.7060)  triple_100: 29.5037 (30.0984)  triple_80: 31.5918 (31.6378)  triple_60: 29.9295 (30.1462)  triple_40: 20.1690 (20.8595)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [830/845]  eta: 0:00:14  loss: 128.5951 (131.1366)  loss_n_40: 4.4820 (4.4659)  loss_n_60: 4.6122 (4.5701)  loss_n_80: 4.7512 (4.6912)  loss_n_100: 4.7619 (4.7058)  triple_100: 29.5037 (30.0894)  triple_80: 31.5918 (31.6288)  triple_60: 30.0693 (30.1372)  triple_40: 20.1690 (20.8483)  time: 0.9774  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [840/845]  eta: 0:00:04  loss: 135.2906 (131.2847)  loss_n_40: 4.5455 (4.4685)  loss_n_60: 4.6766 (4.5729)  loss_n_80: 4.8294 (4.6940)  loss_n_100: 4.8272 (4.7087)  triple_100: 31.0892 (30.1265)  triple_80: 32.9918 (31.6668)  triple_60: 31.4371 (30.1751)  triple_40: 20.4665 (20.8722)  time: 0.9776  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0]  [844/845]  eta: 0:00:00  loss: 134.9107 (131.2680)  loss_n_40: 4.4829 (4.4683)  loss_n_60: 4.6147 (4.5728)  loss_n_80: 4.7512 (4.6939)  loss_n_100: 4.7744 (4.7085)  triple_100: 31.0484 (30.1220)  triple_80: 32.6706 (31.6653)  triple_60: 31.3070 (30.1728)  triple_40: 20.2727 (20.8644)  time: 0.9775  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:0] Total time: 0:13:46 (0.9785 s / it)\n",
      "Averaged stats: loss: 134.9107 (131.2680)  loss_n_40: 4.4829 (4.4683)  loss_n_60: 4.6147 (4.5728)  loss_n_80: 4.7512 (4.6939)  loss_n_100: 4.7744 (4.7085)  triple_100: 31.0484 (30.1220)  triple_80: 32.6706 (31.6653)  triple_60: 31.3070 (30.1728)  triple_40: 20.2727 (20.8644)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle_2/low2high/epoch_0_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 4.708%\n",
      "Min loss_n_100: 4.708\n",
      "Best Epoch: 0.000\n",
      "/home/sunggu/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Train: [epoch:1]  [   0/1724]  eta: 3:01:26  lr: 0.000000  loss: 131.4758 (131.4758)  loss_n_40: 4.5021 (4.5021)  loss_n_60: 4.5974 (4.5974)  loss_n_80: 4.7251 (4.7251)  loss_n_100: 4.7547 (4.7547)  triple_100: 30.1076 (30.1076)  triple_80: 31.8983 (31.8983)  triple_60: 30.4042 (30.4042)  triple_40: 20.4865 (20.4865)  time: 6.3146  data: 0.5652  max mem: 40153\n",
      "Train: [epoch:1]  [  10/1724]  eta: 2:49:39  lr: 0.000000  loss: 131.4758 (129.2577)  loss_n_40: 4.4296 (4.3966)  loss_n_60: 4.5313 (4.5019)  loss_n_80: 4.6762 (4.6309)  loss_n_100: 4.7024 (4.6576)  triple_100: 30.0831 (29.7695)  triple_80: 31.8983 (31.1268)  triple_60: 30.4042 (29.5983)  triple_40: 20.4957 (20.5762)  time: 5.9388  data: 0.0515  max mem: 40153\n",
      "Train: [epoch:1]  [  20/1724]  eta: 2:48:09  lr: 0.000000  loss: 132.0956 (129.4729)  loss_n_40: 4.4296 (4.4102)  loss_n_60: 4.5313 (4.5206)  loss_n_80: 4.6762 (4.6443)  loss_n_100: 4.7024 (4.6682)  triple_100: 30.0831 (29.7899)  triple_80: 32.0550 (31.2031)  triple_60: 30.4314 (29.7011)  triple_40: 20.8665 (20.5356)  time: 5.9011  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [  30/1724]  eta: 2:46:59  lr: 0.000000  loss: 130.0897 (129.3996)  loss_n_40: 4.4149 (4.4117)  loss_n_60: 4.5377 (4.5212)  loss_n_80: 4.6573 (4.6449)  loss_n_100: 4.6763 (4.6690)  triple_100: 30.2005 (29.8084)  triple_80: 31.2978 (31.1794)  triple_60: 29.8243 (29.6817)  triple_40: 20.6923 (20.4833)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [  40/1724]  eta: 2:45:55  lr: 0.000000  loss: 129.7181 (130.1298)  loss_n_40: 4.3175 (4.4181)  loss_n_60: 4.4651 (4.5305)  loss_n_80: 4.6026 (4.6554)  loss_n_100: 4.6329 (4.6774)  triple_100: 29.9361 (29.9632)  triple_80: 31.2699 (31.3664)  triple_60: 29.7279 (29.8654)  triple_40: 20.5121 (20.6534)  time: 5.9029  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [  50/1724]  eta: 2:44:53  lr: 0.000000  loss: 130.9089 (129.6683)  loss_n_40: 4.3728 (4.4099)  loss_n_60: 4.5286 (4.5219)  loss_n_80: 4.6558 (4.6479)  loss_n_100: 4.7059 (4.6705)  triple_100: 30.5138 (29.8832)  triple_80: 31.4732 (31.2631)  triple_60: 30.0256 (29.7454)  triple_40: 19.9418 (20.5263)  time: 5.9033  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [  60/1724]  eta: 2:43:53  lr: 0.000000  loss: 130.9089 (130.0950)  loss_n_40: 4.4237 (4.4175)  loss_n_60: 4.5286 (4.5309)  loss_n_80: 4.6558 (4.6557)  loss_n_100: 4.7059 (4.6778)  triple_100: 30.5138 (29.9783)  triple_80: 31.4732 (31.3631)  triple_60: 30.0256 (29.8514)  triple_40: 20.2565 (20.6204)  time: 5.9044  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [  70/1724]  eta: 2:42:52  lr: 0.000000  loss: 134.3913 (130.9728)  loss_n_40: 4.5477 (4.4338)  loss_n_60: 4.6751 (4.5500)  loss_n_80: 4.7644 (4.6746)  loss_n_100: 4.7626 (4.6960)  triple_100: 31.1208 (30.1924)  triple_80: 32.7625 (31.6023)  triple_60: 31.2307 (30.0883)  triple_40: 21.0958 (20.7354)  time: 5.9044  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [  80/1724]  eta: 2:41:52  lr: 0.000000  loss: 134.2770 (130.7142)  loss_n_40: 4.5477 (4.4311)  loss_n_60: 4.6751 (4.5459)  loss_n_80: 4.7644 (4.6706)  loss_n_100: 4.7605 (4.6921)  triple_100: 30.6379 (30.1353)  triple_80: 31.9121 (31.5408)  triple_60: 30.7256 (30.0239)  triple_40: 21.0958 (20.6745)  time: 5.9036  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [  90/1724]  eta: 2:40:53  lr: 0.000000  loss: 121.4758 (129.6272)  loss_n_40: 4.2696 (4.4131)  loss_n_60: 4.3786 (4.5256)  loss_n_80: 4.4972 (4.6508)  loss_n_100: 4.5161 (4.6732)  triple_100: 27.3087 (29.8771)  triple_80: 28.6980 (31.2683)  triple_60: 27.2117 (29.7502)  triple_40: 18.2857 (20.4689)  time: 5.9046  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 100/1724]  eta: 2:39:53  lr: 0.000000  loss: 122.7796 (129.5977)  loss_n_40: 4.3355 (4.4125)  loss_n_60: 4.4234 (4.5255)  loss_n_80: 4.5547 (4.6501)  loss_n_100: 4.5702 (4.6726)  triple_100: 28.2189 (29.8762)  triple_80: 29.7557 (31.2592)  triple_60: 28.2514 (29.7439)  triple_40: 19.0970 (20.4575)  time: 5.9053  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 110/1724]  eta: 2:38:54  lr: 0.000000  loss: 127.8531 (129.2395)  loss_n_40: 4.3732 (4.4081)  loss_n_60: 4.5140 (4.5206)  loss_n_80: 4.6382 (4.6450)  loss_n_100: 4.6711 (4.6667)  triple_100: 29.6251 (29.7797)  triple_80: 31.0437 (31.1732)  triple_60: 29.5397 (29.6589)  triple_40: 19.7032 (20.3872)  time: 5.9049  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 120/1724]  eta: 2:37:54  lr: 0.000000  loss: 133.3696 (130.3059)  loss_n_40: 4.4957 (4.4272)  loss_n_60: 4.6310 (4.5418)  loss_n_80: 4.7514 (4.6661)  loss_n_100: 4.7605 (4.6867)  triple_100: 31.0531 (30.0367)  triple_80: 32.2211 (31.4506)  triple_60: 30.5842 (29.9264)  triple_40: 20.9839 (20.5704)  time: 5.9045  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [ 130/1724]  eta: 2:36:55  lr: 0.000000  loss: 136.2062 (130.4466)  loss_n_40: 4.5272 (4.4290)  loss_n_60: 4.6730 (4.5446)  loss_n_80: 4.7868 (4.6690)  loss_n_100: 4.7886 (4.6895)  triple_100: 31.6601 (30.0743)  triple_80: 32.9974 (31.4896)  triple_60: 31.5789 (29.9635)  triple_40: 21.2057 (20.5870)  time: 5.9046  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 140/1724]  eta: 2:35:56  lr: 0.000000  loss: 127.7764 (130.3581)  loss_n_40: 4.4132 (4.4315)  loss_n_60: 4.5539 (4.5456)  loss_n_80: 4.6477 (4.6687)  loss_n_100: 4.6425 (4.6884)  triple_100: 29.1569 (30.0338)  triple_80: 30.5705 (31.4573)  triple_60: 29.4543 (29.9440)  triple_40: 20.4389 (20.5889)  time: 5.9045  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 150/1724]  eta: 2:34:56  lr: 0.000000  loss: 134.1258 (131.0334)  loss_n_40: 4.4992 (4.4425)  loss_n_60: 4.5928 (4.5575)  loss_n_80: 4.7306 (4.6805)  loss_n_100: 4.7438 (4.7006)  triple_100: 30.8113 (30.1971)  triple_80: 32.5745 (31.6282)  triple_60: 30.9434 (30.1164)  triple_40: 21.0588 (20.7105)  time: 5.9044  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 160/1724]  eta: 2:33:57  lr: 0.000000  loss: 129.6938 (130.2990)  loss_n_40: 4.4072 (4.4290)  loss_n_60: 4.5017 (4.5433)  loss_n_80: 4.6659 (4.6669)  loss_n_100: 4.6737 (4.6870)  triple_100: 29.7308 (30.0169)  triple_80: 31.4564 (31.4415)  triple_60: 29.6192 (29.9298)  triple_40: 20.6388 (20.5846)  time: 5.9038  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 170/1724]  eta: 2:32:58  lr: 0.000000  loss: 125.9832 (130.4541)  loss_n_40: 4.3578 (4.4319)  loss_n_60: 4.4507 (4.5457)  loss_n_80: 4.5969 (4.6694)  loss_n_100: 4.6193 (4.6896)  triple_100: 28.8543 (30.0574)  triple_80: 30.4149 (31.4796)  triple_60: 28.7157 (29.9663)  triple_40: 20.0578 (20.6142)  time: 5.9033  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 180/1724]  eta: 2:31:59  lr: 0.000000  loss: 134.0664 (130.7043)  loss_n_40: 4.5034 (4.4362)  loss_n_60: 4.6083 (4.5501)  loss_n_80: 4.7370 (4.6739)  loss_n_100: 4.7559 (4.6942)  triple_100: 30.9608 (30.1176)  triple_80: 32.4445 (31.5445)  triple_60: 31.0071 (30.0308)  triple_40: 21.1032 (20.6571)  time: 5.9052  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 190/1724]  eta: 2:31:00  lr: 0.000000  loss: 134.0664 (130.8303)  loss_n_40: 4.5034 (4.4392)  loss_n_60: 4.6235 (4.5529)  loss_n_80: 4.7627 (4.6767)  loss_n_100: 4.7493 (4.6965)  triple_100: 30.9608 (30.1427)  triple_80: 32.4445 (31.5773)  triple_60: 31.0071 (30.0649)  triple_40: 21.1001 (20.6801)  time: 5.9061  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 200/1724]  eta: 2:30:00  lr: 0.000000  loss: 135.4979 (131.0823)  loss_n_40: 4.5071 (4.4441)  loss_n_60: 4.6527 (4.5575)  loss_n_80: 4.7811 (4.6810)  loss_n_100: 4.7948 (4.7007)  triple_100: 31.1000 (30.2003)  triple_80: 32.9345 (31.6352)  triple_60: 31.3761 (30.1242)  triple_40: 21.2191 (20.7393)  time: 5.9053  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 210/1724]  eta: 2:29:01  lr: 0.000000  loss: 134.4892 (131.1259)  loss_n_40: 4.4812 (4.4447)  loss_n_60: 4.5840 (4.5580)  loss_n_80: 4.7149 (4.6811)  loss_n_100: 4.7272 (4.7007)  triple_100: 30.7869 (30.2051)  triple_80: 32.4015 (31.6390)  triple_60: 30.8148 (30.1305)  triple_40: 20.9000 (20.7668)  time: 5.9046  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 220/1724]  eta: 2:28:02  lr: 0.000000  loss: 126.1781 (130.9299)  loss_n_40: 4.3690 (4.4429)  loss_n_60: 4.4617 (4.5551)  loss_n_80: 4.5951 (4.6782)  loss_n_100: 4.6093 (4.6976)  triple_100: 29.0887 (30.1550)  triple_80: 30.3719 (31.5909)  triple_60: 28.7813 (30.0814)  triple_40: 19.9011 (20.7288)  time: 5.9040  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 230/1724]  eta: 2:27:03  lr: 0.000000  loss: 120.5062 (130.4809)  loss_n_40: 4.2987 (4.4340)  loss_n_60: 4.3830 (4.5463)  loss_n_80: 4.5078 (4.6694)  loss_n_100: 4.5265 (4.6891)  triple_100: 27.7983 (30.0468)  triple_80: 29.0574 (31.4767)  triple_60: 27.4633 (29.9685)  triple_40: 19.0883 (20.6501)  time: 5.9040  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 240/1724]  eta: 2:26:04  lr: 0.000000  loss: 123.0187 (130.4459)  loss_n_40: 4.2808 (4.4334)  loss_n_60: 4.3830 (4.5458)  loss_n_80: 4.5044 (4.6688)  loss_n_100: 4.5402 (4.6885)  triple_100: 28.2317 (30.0376)  triple_80: 29.4438 (31.4679)  triple_60: 27.9677 (29.9586)  triple_40: 18.5352 (20.6454)  time: 5.9036  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 250/1724]  eta: 2:25:04  lr: 0.000000  loss: 125.3881 (130.1567)  loss_n_40: 4.2995 (4.4276)  loss_n_60: 4.4335 (4.5405)  loss_n_80: 4.5644 (4.6638)  loss_n_100: 4.5757 (4.6833)  triple_100: 28.6386 (29.9682)  triple_80: 30.2362 (31.4008)  triple_60: 28.5658 (29.8912)  triple_40: 19.5295 (20.5814)  time: 5.9037  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 260/1724]  eta: 2:24:05  lr: 0.000000  loss: 131.3908 (130.4413)  loss_n_40: 4.4609 (4.4320)  loss_n_60: 4.6128 (4.5454)  loss_n_80: 4.7508 (4.6687)  loss_n_100: 4.7381 (4.6882)  triple_100: 30.3058 (30.0351)  triple_80: 32.1946 (31.4705)  triple_60: 30.5918 (29.9616)  triple_40: 20.2573 (20.6397)  time: 5.9048  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 270/1724]  eta: 2:23:06  lr: 0.000000  loss: 139.8455 (130.7628)  loss_n_40: 4.6060 (4.4374)  loss_n_60: 4.7323 (4.5513)  loss_n_80: 4.8466 (4.6745)  loss_n_100: 4.8515 (4.6942)  triple_100: 32.2592 (30.1156)  triple_80: 33.9424 (31.5495)  triple_60: 32.5594 (30.0399)  triple_40: 22.5888 (20.7004)  time: 5.9046  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 280/1724]  eta: 2:22:07  lr: 0.000000  loss: 135.9513 (130.6801)  loss_n_40: 4.5208 (4.4369)  loss_n_60: 4.6600 (4.5502)  loss_n_80: 4.7902 (4.6731)  loss_n_100: 4.7813 (4.6926)  triple_100: 31.1347 (30.0917)  triple_80: 32.9524 (31.5259)  triple_60: 31.3778 (30.0197)  triple_40: 21.5427 (20.6901)  time: 5.9026  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 290/1724]  eta: 2:21:08  lr: 0.000000  loss: 130.1476 (130.7573)  loss_n_40: 4.4597 (4.4389)  loss_n_60: 4.5599 (4.5515)  loss_n_80: 4.6888 (4.6742)  loss_n_100: 4.7007 (4.6934)  triple_100: 29.8378 (30.1018)  triple_80: 31.5355 (31.5411)  triple_60: 30.0191 (30.0382)  triple_40: 21.1510 (20.7184)  time: 5.9021  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 300/1724]  eta: 2:20:09  lr: 0.000000  loss: 130.1476 (130.7645)  loss_n_40: 4.4494 (4.4389)  loss_n_60: 4.4855 (4.5520)  loss_n_80: 4.6202 (4.6748)  loss_n_100: 4.6429 (4.6937)  triple_100: 29.6643 (30.0996)  triple_80: 31.2700 (31.5459)  triple_60: 29.5503 (30.0421)  triple_40: 20.4583 (20.7176)  time: 5.9025  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 310/1724]  eta: 2:19:09  lr: 0.000000  loss: 130.9333 (130.8830)  loss_n_40: 4.4682 (4.4411)  loss_n_60: 4.5728 (4.5543)  loss_n_80: 4.6980 (4.6771)  loss_n_100: 4.7153 (4.6959)  triple_100: 30.1728 (30.1272)  triple_80: 31.7117 (31.5768)  triple_60: 30.0591 (30.0729)  triple_40: 20.5726 (20.7379)  time: 5.9024  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 320/1724]  eta: 2:18:10  lr: 0.000000  loss: 129.4989 (130.7680)  loss_n_40: 4.4419 (4.4383)  loss_n_60: 4.5319 (4.5519)  loss_n_80: 4.6667 (4.6747)  loss_n_100: 4.6809 (4.6939)  triple_100: 30.1178 (30.1025)  triple_80: 31.4275 (31.5495)  triple_60: 29.9203 (30.0455)  triple_40: 20.4564 (20.7116)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 330/1724]  eta: 2:17:11  lr: 0.000000  loss: 129.2158 (130.7421)  loss_n_40: 4.3787 (4.4372)  loss_n_60: 4.5250 (4.5509)  loss_n_80: 4.6377 (4.6738)  loss_n_100: 4.6777 (4.6932)  triple_100: 29.8470 (30.0979)  triple_80: 31.2503 (31.5408)  triple_60: 29.7672 (30.0367)  triple_40: 20.2625 (20.7116)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 340/1724]  eta: 2:16:12  lr: 0.000000  loss: 131.3601 (130.7921)  loss_n_40: 4.4554 (4.4378)  loss_n_60: 4.5528 (4.5515)  loss_n_80: 4.6799 (4.6744)  loss_n_100: 4.6933 (4.6939)  triple_100: 30.1855 (30.1092)  triple_80: 31.6430 (31.5521)  triple_60: 30.0730 (30.0470)  triple_40: 21.3106 (20.7261)  time: 5.9022  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 350/1724]  eta: 2:15:13  lr: 0.000000  loss: 133.3972 (130.8342)  loss_n_40: 4.4477 (4.4389)  loss_n_60: 4.5582 (4.5527)  loss_n_80: 4.6894 (4.6756)  loss_n_100: 4.7039 (4.6949)  triple_100: 30.3636 (30.1200)  triple_80: 32.0697 (31.5635)  triple_60: 30.5149 (30.0576)  triple_40: 21.1951 (20.7311)  time: 5.9033  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [ 360/1724]  eta: 2:14:14  lr: 0.000000  loss: 126.5664 (130.6560)  loss_n_40: 4.3758 (4.4361)  loss_n_60: 4.4909 (4.5494)  loss_n_80: 4.6247 (4.6720)  loss_n_100: 4.6397 (4.6916)  triple_100: 29.0398 (30.0781)  triple_80: 30.7038 (31.5144)  triple_60: 29.1208 (30.0105)  triple_40: 19.7748 (20.7039)  time: 5.9033  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 370/1724]  eta: 2:13:15  lr: 0.000000  loss: 131.8860 (130.8436)  loss_n_40: 4.4957 (4.4391)  loss_n_60: 4.5706 (4.5529)  loss_n_80: 4.7070 (4.6757)  loss_n_100: 4.7221 (4.6952)  triple_100: 30.3778 (30.1245)  triple_80: 31.8275 (31.5649)  triple_60: 30.2040 (30.0593)  triple_40: 20.9333 (20.7319)  time: 5.9038  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 380/1724]  eta: 2:12:15  lr: 0.000000  loss: 135.8672 (130.8341)  loss_n_40: 4.5401 (4.4399)  loss_n_60: 4.6476 (4.5531)  loss_n_80: 4.7843 (4.6759)  loss_n_100: 4.7893 (4.6953)  triple_100: 31.0833 (30.1190)  triple_80: 32.8853 (31.5604)  triple_60: 31.2463 (30.0558)  triple_40: 21.3802 (20.7348)  time: 5.9041  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 390/1724]  eta: 2:11:16  lr: 0.000000  loss: 135.2424 (130.9893)  loss_n_40: 4.4757 (4.4427)  loss_n_60: 4.6348 (4.5561)  loss_n_80: 4.7744 (4.6791)  loss_n_100: 4.7763 (4.6984)  triple_100: 31.0513 (30.1565)  triple_80: 32.4228 (31.6015)  triple_60: 31.1467 (30.0949)  triple_40: 21.2500 (20.7602)  time: 5.9035  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 400/1724]  eta: 2:10:17  lr: 0.000000  loss: 137.0207 (131.0932)  loss_n_40: 4.5010 (4.4446)  loss_n_60: 4.6348 (4.5581)  loss_n_80: 4.7773 (4.6809)  loss_n_100: 4.7984 (4.7005)  triple_100: 31.5515 (30.1847)  triple_80: 33.2926 (31.6248)  triple_60: 31.6233 (30.1182)  triple_40: 21.6578 (20.7814)  time: 5.9031  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 410/1724]  eta: 2:09:18  lr: 0.000000  loss: 134.5981 (131.0782)  loss_n_40: 4.5415 (4.4451)  loss_n_60: 4.6908 (4.5584)  loss_n_80: 4.8002 (4.6812)  loss_n_100: 4.8239 (4.7007)  triple_100: 31.3617 (30.1830)  triple_80: 32.8121 (31.6223)  triple_60: 31.2393 (30.1159)  triple_40: 20.7495 (20.7715)  time: 5.9031  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 420/1724]  eta: 2:08:19  lr: 0.000000  loss: 135.4820 (131.1991)  loss_n_40: 4.5592 (4.4478)  loss_n_60: 4.6504 (4.5611)  loss_n_80: 4.7600 (4.6837)  loss_n_100: 4.7774 (4.7031)  triple_100: 31.3617 (30.2102)  triple_80: 32.6726 (31.6521)  triple_60: 31.2149 (30.1459)  triple_40: 20.7495 (20.7952)  time: 5.9034  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 430/1724]  eta: 2:07:20  lr: 0.000000  loss: 135.4820 (131.2060)  loss_n_40: 4.5266 (4.4478)  loss_n_60: 4.6334 (4.5608)  loss_n_80: 4.7564 (4.6834)  loss_n_100: 4.7526 (4.7026)  triple_100: 31.0789 (30.2084)  triple_80: 32.6726 (31.6512)  triple_60: 31.2149 (30.1467)  triple_40: 21.6530 (20.8051)  time: 5.9039  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 440/1724]  eta: 2:06:21  lr: 0.000000  loss: 132.0687 (131.1920)  loss_n_40: 4.5063 (4.4481)  loss_n_60: 4.6066 (4.5609)  loss_n_80: 4.6797 (4.6834)  loss_n_100: 4.6988 (4.7026)  triple_100: 29.9805 (30.2037)  triple_80: 31.4738 (31.6464)  triple_60: 30.2108 (30.1454)  triple_40: 21.3639 (20.8017)  time: 5.9032  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 450/1724]  eta: 2:05:22  lr: 0.000000  loss: 133.1015 (131.2376)  loss_n_40: 4.4867 (4.4491)  loss_n_60: 4.6066 (4.5621)  loss_n_80: 4.7283 (4.6844)  loss_n_100: 4.7321 (4.7036)  triple_100: 30.5536 (30.2160)  triple_80: 32.1084 (31.6600)  triple_60: 30.7118 (30.1595)  triple_40: 21.3639 (20.8029)  time: 5.9026  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 460/1724]  eta: 2:04:23  lr: 0.000000  loss: 134.5961 (131.2599)  loss_n_40: 4.4867 (4.4498)  loss_n_60: 4.6048 (4.5626)  loss_n_80: 4.7408 (4.6849)  loss_n_100: 4.7454 (4.7039)  triple_100: 31.0240 (30.2188)  triple_80: 32.3296 (31.6631)  triple_60: 30.8662 (30.1643)  triple_40: 21.5183 (20.8126)  time: 5.9040  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 470/1724]  eta: 2:03:24  lr: 0.000000  loss: 131.7979 (131.2242)  loss_n_40: 4.4711 (4.4490)  loss_n_60: 4.5545 (4.5616)  loss_n_80: 4.6817 (4.6839)  loss_n_100: 4.7021 (4.7030)  triple_100: 30.1290 (30.2085)  triple_80: 31.6190 (31.6527)  triple_60: 30.1199 (30.1537)  triple_40: 21.5183 (20.8118)  time: 5.9045  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 480/1724]  eta: 2:02:25  lr: 0.000000  loss: 131.0097 (131.2418)  loss_n_40: 4.3966 (4.4496)  loss_n_60: 4.4952 (4.5622)  loss_n_80: 4.6242 (4.6844)  loss_n_100: 4.6667 (4.7034)  triple_100: 30.1275 (30.2118)  triple_80: 31.4545 (31.6571)  triple_60: 29.8843 (30.1587)  triple_40: 21.3654 (20.8145)  time: 5.9048  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 490/1724]  eta: 2:01:26  lr: 0.000000  loss: 125.8839 (131.1386)  loss_n_40: 4.3646 (4.4480)  loss_n_60: 4.4622 (4.5602)  loss_n_80: 4.5993 (4.6824)  loss_n_100: 4.6306 (4.7016)  triple_100: 29.0141 (30.1875)  triple_80: 30.4523 (31.6292)  triple_60: 28.7960 (30.1315)  triple_40: 19.9901 (20.7982)  time: 5.9036  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 500/1724]  eta: 2:00:27  lr: 0.000000  loss: 132.8435 (131.3897)  loss_n_40: 4.5297 (4.4523)  loss_n_60: 4.5961 (4.5649)  loss_n_80: 4.7344 (4.6871)  loss_n_100: 4.7272 (4.7064)  triple_100: 30.5984 (30.2485)  triple_80: 32.1834 (31.6939)  triple_60: 30.5035 (30.1956)  triple_40: 21.1878 (20.8411)  time: 5.9033  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 510/1724]  eta: 1:59:28  lr: 0.000000  loss: 136.6270 (131.4284)  loss_n_40: 4.5386 (4.4533)  loss_n_60: 4.6826 (4.5658)  loss_n_80: 4.8002 (4.6880)  loss_n_100: 4.7877 (4.7070)  triple_100: 31.1757 (30.2556)  triple_80: 33.2020 (31.7039)  triple_60: 31.8289 (30.2062)  triple_40: 21.5981 (20.8485)  time: 5.9048  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 520/1724]  eta: 1:58:28  lr: 0.000000  loss: 131.0853 (131.5283)  loss_n_40: 4.4525 (4.4550)  loss_n_60: 4.5763 (4.5676)  loss_n_80: 4.7013 (4.6898)  loss_n_100: 4.7070 (4.7089)  triple_100: 30.1645 (30.2811)  triple_80: 31.6892 (31.7291)  triple_60: 30.2292 (30.2300)  triple_40: 20.8966 (20.8668)  time: 5.9051  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 530/1724]  eta: 1:57:29  lr: 0.000000  loss: 136.7428 (131.6566)  loss_n_40: 4.4595 (4.4568)  loss_n_60: 4.6086 (4.5698)  loss_n_80: 4.7307 (4.6920)  loss_n_100: 4.7718 (4.7110)  triple_100: 31.8020 (30.3126)  triple_80: 32.9929 (31.7612)  triple_60: 31.4911 (30.2623)  triple_40: 21.7933 (20.8908)  time: 5.9045  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 540/1724]  eta: 1:56:30  lr: 0.000000  loss: 136.2048 (131.7064)  loss_n_40: 4.5033 (4.4579)  loss_n_60: 4.6369 (4.5711)  loss_n_80: 4.7582 (4.6933)  loss_n_100: 4.7773 (4.7123)  triple_100: 31.1049 (30.3251)  triple_80: 32.7711 (31.7762)  triple_60: 31.3878 (30.2767)  triple_40: 21.4783 (20.8939)  time: 5.9036  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 550/1724]  eta: 1:55:31  lr: 0.000000  loss: 133.2838 (131.7193)  loss_n_40: 4.4685 (4.4575)  loss_n_60: 4.5835 (4.5709)  loss_n_80: 4.7134 (4.6930)  loss_n_100: 4.7523 (4.7120)  triple_100: 30.7791 (30.3284)  triple_80: 32.3021 (31.7761)  triple_60: 30.7272 (30.2772)  triple_40: 21.3756 (20.9042)  time: 5.9025  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 560/1724]  eta: 1:54:32  lr: 0.000000  loss: 131.7126 (131.7265)  loss_n_40: 4.4273 (4.4576)  loss_n_60: 4.5608 (4.5710)  loss_n_80: 4.6779 (4.6931)  loss_n_100: 4.7153 (4.7120)  triple_100: 30.6885 (30.3297)  triple_80: 31.9971 (31.7790)  triple_60: 30.5128 (30.2800)  triple_40: 20.6642 (20.9042)  time: 5.9025  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 570/1724]  eta: 1:53:33  lr: 0.000000  loss: 130.4394 (131.6420)  loss_n_40: 4.4174 (4.4556)  loss_n_60: 4.5464 (4.5692)  loss_n_80: 4.6779 (4.6914)  loss_n_100: 4.7018 (4.7104)  triple_100: 29.9026 (30.3116)  triple_80: 31.5165 (31.7581)  triple_60: 29.9839 (30.2585)  triple_40: 20.0890 (20.8872)  time: 5.9029  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 580/1724]  eta: 1:52:34  lr: 0.000000  loss: 129.7702 (131.6893)  loss_n_40: 4.4174 (4.4566)  loss_n_60: 4.5383 (4.5700)  loss_n_80: 4.6566 (4.6920)  loss_n_100: 4.6915 (4.7110)  triple_100: 30.0805 (30.3219)  triple_80: 31.3914 (31.7667)  triple_60: 29.7023 (30.2680)  triple_40: 20.3482 (20.9029)  time: 5.9041  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [ 590/1724]  eta: 1:51:35  lr: 0.000000  loss: 137.4167 (131.7566)  loss_n_40: 4.5136 (4.4580)  loss_n_60: 4.6662 (4.5714)  loss_n_80: 4.7983 (4.6934)  loss_n_100: 4.8113 (4.7123)  triple_100: 31.5818 (30.3367)  triple_80: 33.3507 (31.7840)  triple_60: 31.6799 (30.2856)  triple_40: 21.6095 (20.9153)  time: 5.9053  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 600/1724]  eta: 1:50:36  lr: 0.000000  loss: 132.2703 (131.6383)  loss_n_40: 4.4555 (4.4563)  loss_n_60: 4.5865 (4.5693)  loss_n_80: 4.7214 (4.6913)  loss_n_100: 4.7431 (4.7102)  triple_100: 30.5009 (30.3074)  triple_80: 31.7106 (31.7535)  triple_60: 30.5165 (30.2553)  triple_40: 20.6086 (20.8952)  time: 5.9045  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 610/1724]  eta: 1:49:37  lr: 0.000000  loss: 128.9006 (131.6936)  loss_n_40: 4.3678 (4.4566)  loss_n_60: 4.5031 (4.5699)  loss_n_80: 4.6262 (4.6920)  loss_n_100: 4.6632 (4.7110)  triple_100: 29.8544 (30.3232)  triple_80: 31.0526 (31.7675)  triple_60: 29.6364 (30.2677)  triple_40: 20.2788 (20.9058)  time: 5.9037  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 620/1724]  eta: 1:48:38  lr: 0.000000  loss: 133.7238 (131.6859)  loss_n_40: 4.4229 (4.4564)  loss_n_60: 4.6056 (4.5698)  loss_n_80: 4.7371 (4.6919)  loss_n_100: 4.7446 (4.7108)  triple_100: 30.5668 (30.3209)  triple_80: 31.9176 (31.7665)  triple_60: 30.6890 (30.2672)  triple_40: 21.2462 (20.9024)  time: 5.9042  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 630/1724]  eta: 1:47:39  lr: 0.000000  loss: 131.0024 (131.6356)  loss_n_40: 4.4897 (4.4557)  loss_n_60: 4.6056 (4.5689)  loss_n_80: 4.7371 (4.6910)  loss_n_100: 4.7446 (4.7100)  triple_100: 30.2286 (30.3101)  triple_80: 31.9040 (31.7527)  triple_60: 30.3077 (30.2539)  triple_40: 20.7211 (20.8932)  time: 5.9039  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 640/1724]  eta: 1:46:40  lr: 0.000000  loss: 130.6953 (131.5985)  loss_n_40: 4.4614 (4.4555)  loss_n_60: 4.5618 (4.5686)  loss_n_80: 4.6705 (4.6907)  loss_n_100: 4.6947 (4.7097)  triple_100: 30.1941 (30.3003)  triple_80: 31.4152 (31.7444)  triple_60: 29.9762 (30.2456)  triple_40: 20.7137 (20.8837)  time: 5.9026  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 650/1724]  eta: 1:45:41  lr: 0.000000  loss: 131.4003 (131.5736)  loss_n_40: 4.4614 (4.4547)  loss_n_60: 4.5705 (4.5679)  loss_n_80: 4.6979 (4.6901)  loss_n_100: 4.7180 (4.7091)  triple_100: 30.1950 (30.2955)  triple_80: 31.8050 (31.7394)  triple_60: 30.2582 (30.2394)  triple_40: 20.7216 (20.8775)  time: 5.9016  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 660/1724]  eta: 1:44:42  lr: 0.000000  loss: 133.6189 (131.4988)  loss_n_40: 4.4662 (4.4538)  loss_n_60: 4.5811 (4.5666)  loss_n_80: 4.7068 (4.6889)  loss_n_100: 4.7218 (4.7078)  triple_100: 30.4369 (30.2760)  triple_80: 31.8908 (31.7203)  triple_60: 30.4568 (30.2208)  triple_40: 20.1742 (20.8647)  time: 5.9016  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 670/1724]  eta: 1:43:43  lr: 0.000000  loss: 122.3439 (131.3660)  loss_n_40: 4.3287 (4.4524)  loss_n_60: 4.4144 (4.5645)  loss_n_80: 4.5330 (4.6866)  loss_n_100: 4.5457 (4.7055)  triple_100: 28.0314 (30.2425)  triple_80: 29.2656 (31.6852)  triple_60: 27.8080 (30.1868)  triple_40: 19.3409 (20.8424)  time: 5.9023  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 680/1724]  eta: 1:42:43  lr: 0.000000  loss: 122.6993 (131.2857)  loss_n_40: 4.3287 (4.4511)  loss_n_60: 4.3913 (4.5629)  loss_n_80: 4.5330 (4.6850)  loss_n_100: 4.5457 (4.7039)  triple_100: 28.2074 (30.2227)  triple_80: 29.3493 (31.6636)  triple_60: 28.0089 (30.1656)  triple_40: 19.4466 (20.8309)  time: 5.9023  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 690/1724]  eta: 1:41:44  lr: 0.000000  loss: 127.3190 (131.2628)  loss_n_40: 4.3977 (4.4504)  loss_n_60: 4.4774 (4.5622)  loss_n_80: 4.5947 (4.6844)  loss_n_100: 4.6269 (4.7035)  triple_100: 29.1144 (30.2179)  triple_80: 30.5013 (31.6584)  triple_60: 29.0073 (30.1592)  triple_40: 20.0680 (20.8268)  time: 5.9017  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 700/1724]  eta: 1:40:45  lr: 0.000000  loss: 126.6169 (131.1941)  loss_n_40: 4.3390 (4.4491)  loss_n_60: 4.4705 (4.5609)  loss_n_80: 4.5947 (4.6831)  loss_n_100: 4.6269 (4.7022)  triple_100: 29.0605 (30.2008)  triple_80: 30.6317 (31.6412)  triple_60: 29.0073 (30.1425)  triple_40: 19.9870 (20.8143)  time: 5.9005  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 710/1724]  eta: 1:39:46  lr: 0.000000  loss: 127.9092 (131.2102)  loss_n_40: 4.3767 (4.4493)  loss_n_60: 4.4789 (4.5611)  loss_n_80: 4.6184 (4.6833)  loss_n_100: 4.6300 (4.7024)  triple_100: 29.1700 (30.2042)  triple_80: 30.9082 (31.6453)  triple_60: 29.4319 (30.1465)  triple_40: 20.0016 (20.8182)  time: 5.9000  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 720/1724]  eta: 1:38:47  lr: 0.000000  loss: 134.8299 (131.2042)  loss_n_40: 4.4962 (4.4494)  loss_n_60: 4.6136 (4.5610)  loss_n_80: 4.7487 (4.6831)  loss_n_100: 4.7671 (4.7021)  triple_100: 31.0642 (30.2006)  triple_80: 32.7252 (31.6416)  triple_60: 31.0512 (30.1436)  triple_40: 21.5433 (20.8227)  time: 5.9017  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 730/1724]  eta: 1:37:48  lr: 0.000000  loss: 136.7657 (131.2492)  loss_n_40: 4.5574 (4.4507)  loss_n_60: 4.6795 (4.5622)  loss_n_80: 4.7929 (4.6842)  loss_n_100: 4.8249 (4.7033)  triple_100: 31.5445 (30.2113)  triple_80: 33.1352 (31.6524)  triple_60: 31.5930 (30.1548)  triple_40: 21.5772 (20.8303)  time: 5.9027  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 740/1724]  eta: 1:36:49  lr: 0.000000  loss: 136.7657 (131.2989)  loss_n_40: 4.5366 (4.4517)  loss_n_60: 4.6370 (4.5631)  loss_n_80: 4.7568 (4.6850)  loss_n_100: 4.7744 (4.7040)  triple_100: 31.5445 (30.2209)  triple_80: 32.8983 (31.6635)  triple_60: 31.4130 (30.1667)  triple_40: 21.5772 (20.8440)  time: 5.9025  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 750/1724]  eta: 1:35:50  lr: 0.000000  loss: 130.6743 (131.2473)  loss_n_40: 4.4336 (4.4511)  loss_n_60: 4.5274 (4.5622)  loss_n_80: 4.6443 (4.6842)  loss_n_100: 4.6588 (4.7031)  triple_100: 30.0945 (30.2087)  triple_80: 31.2645 (31.6496)  triple_60: 29.8037 (30.1529)  triple_40: 21.0799 (20.8355)  time: 5.9021  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 760/1724]  eta: 1:34:51  lr: 0.000000  loss: 125.1461 (131.2294)  loss_n_40: 4.3651 (4.4506)  loss_n_60: 4.4852 (4.5619)  loss_n_80: 4.5921 (4.6840)  loss_n_100: 4.6270 (4.7029)  triple_100: 29.1645 (30.2061)  triple_80: 30.4290 (31.6461)  triple_60: 28.7907 (30.1488)  triple_40: 19.6628 (20.8290)  time: 5.9020  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 770/1724]  eta: 1:33:52  lr: 0.000000  loss: 129.8232 (131.2863)  loss_n_40: 4.4156 (4.4514)  loss_n_60: 4.5729 (4.5630)  loss_n_80: 4.7047 (4.6851)  loss_n_100: 4.7101 (4.7040)  triple_100: 30.0897 (30.2196)  triple_80: 31.6897 (31.6623)  triple_60: 30.1079 (30.1640)  triple_40: 20.5426 (20.8369)  time: 5.9019  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 780/1724]  eta: 1:32:53  lr: 0.000000  loss: 134.0945 (131.2778)  loss_n_40: 4.4467 (4.4509)  loss_n_60: 4.5903 (4.5629)  loss_n_80: 4.7268 (4.6851)  loss_n_100: 4.7379 (4.7039)  triple_100: 31.0369 (30.2187)  triple_80: 32.5412 (31.6621)  triple_60: 31.0192 (30.1630)  triple_40: 20.8769 (20.8312)  time: 5.9012  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 790/1724]  eta: 1:31:54  lr: 0.000000  loss: 127.1984 (131.1645)  loss_n_40: 4.4291 (4.4492)  loss_n_60: 4.5445 (4.5608)  loss_n_80: 4.6725 (4.6830)  loss_n_100: 4.6834 (4.7019)  triple_100: 29.2260 (30.1910)  triple_80: 31.0666 (31.6333)  triple_60: 29.4881 (30.1341)  triple_40: 19.7415 (20.8113)  time: 5.9021  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 800/1724]  eta: 1:30:55  lr: 0.000000  loss: 128.2184 (131.2693)  loss_n_40: 4.4317 (4.4506)  loss_n_60: 4.5445 (4.5625)  loss_n_80: 4.6746 (4.6848)  loss_n_100: 4.6834 (4.7037)  triple_100: 29.7725 (30.2172)  triple_80: 31.0666 (31.6600)  triple_60: 29.4881 (30.1602)  triple_40: 20.3504 (20.8303)  time: 5.9025  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 810/1724]  eta: 1:29:56  lr: 0.000000  loss: 133.6319 (131.2348)  loss_n_40: 4.4466 (4.4497)  loss_n_60: 4.5599 (4.5618)  loss_n_80: 4.6863 (4.6840)  loss_n_100: 4.7362 (4.7030)  triple_100: 30.8224 (30.2101)  triple_80: 32.4416 (31.6515)  triple_60: 30.9051 (30.1518)  triple_40: 20.7736 (20.8228)  time: 5.9018  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [ 820/1724]  eta: 1:28:56  lr: 0.000000  loss: 132.6555 (131.2410)  loss_n_40: 4.4259 (4.4494)  loss_n_60: 4.5548 (4.5617)  loss_n_80: 4.6853 (4.6841)  loss_n_100: 4.6915 (4.7031)  triple_100: 30.3898 (30.2116)  triple_80: 32.1593 (31.6542)  triple_60: 30.5733 (30.1540)  triple_40: 20.7736 (20.8229)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 830/1724]  eta: 1:27:57  lr: 0.000000  loss: 132.6555 (131.2375)  loss_n_40: 4.4714 (4.4493)  loss_n_60: 4.5548 (4.5615)  loss_n_80: 4.6853 (4.6839)  loss_n_100: 4.6906 (4.7030)  triple_100: 30.3898 (30.2106)  triple_80: 32.1593 (31.6526)  triple_60: 30.5733 (30.1525)  triple_40: 20.7212 (20.8240)  time: 5.9016  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 840/1724]  eta: 1:26:58  lr: 0.000000  loss: 134.3058 (131.2723)  loss_n_40: 4.4915 (4.4500)  loss_n_60: 4.6350 (4.5624)  loss_n_80: 4.7668 (4.6849)  loss_n_100: 4.7770 (4.7039)  triple_100: 31.1350 (30.2194)  triple_80: 32.6802 (31.6639)  triple_60: 30.9776 (30.1625)  triple_40: 20.7212 (20.8253)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 850/1724]  eta: 1:25:59  lr: 0.000000  loss: 132.8052 (131.2274)  loss_n_40: 4.4269 (4.4494)  loss_n_60: 4.6027 (4.5617)  loss_n_80: 4.7398 (4.6843)  loss_n_100: 4.7412 (4.7032)  triple_100: 30.7301 (30.2089)  triple_80: 32.2801 (31.6536)  triple_60: 30.5811 (30.1516)  triple_40: 20.7740 (20.8147)  time: 5.9016  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 860/1724]  eta: 1:25:00  lr: 0.000000  loss: 132.8052 (131.2823)  loss_n_40: 4.4269 (4.4503)  loss_n_60: 4.5655 (4.5627)  loss_n_80: 4.6921 (4.6853)  loss_n_100: 4.7242 (4.7043)  triple_100: 30.4611 (30.2232)  triple_80: 32.1498 (31.6678)  triple_60: 30.5392 (30.1653)  triple_40: 20.7740 (20.8235)  time: 5.9025  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 870/1724]  eta: 1:24:01  lr: 0.000000  loss: 135.1958 (131.3334)  loss_n_40: 4.5234 (4.4510)  loss_n_60: 4.6513 (4.5635)  loss_n_80: 4.7722 (4.6861)  loss_n_100: 4.8001 (4.7052)  triple_100: 31.3762 (30.2369)  triple_80: 32.7327 (31.6798)  triple_60: 31.2819 (30.1772)  triple_40: 21.0908 (20.8337)  time: 5.9018  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 880/1724]  eta: 1:23:02  lr: 0.000000  loss: 135.1932 (131.3400)  loss_n_40: 4.4929 (4.4513)  loss_n_60: 4.6483 (4.5638)  loss_n_80: 4.7652 (4.6863)  loss_n_100: 4.7883 (4.7053)  triple_100: 30.8633 (30.2386)  triple_80: 32.2448 (31.6806)  triple_60: 31.1509 (30.1781)  triple_40: 20.8333 (20.8359)  time: 5.9021  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 890/1724]  eta: 1:22:03  lr: 0.000000  loss: 128.5800 (131.3617)  loss_n_40: 4.4156 (4.4516)  loss_n_60: 4.5329 (4.5642)  loss_n_80: 4.6448 (4.6867)  loss_n_100: 4.6621 (4.7059)  triple_100: 29.4288 (30.2456)  triple_80: 30.6538 (31.6868)  triple_60: 29.2937 (30.1838)  triple_40: 20.2965 (20.8371)  time: 5.9033  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 900/1724]  eta: 1:21:04  lr: 0.000000  loss: 128.5800 (131.3299)  loss_n_40: 4.4179 (4.4508)  loss_n_60: 4.5204 (4.5635)  loss_n_80: 4.6543 (4.6861)  loss_n_100: 4.6780 (4.7052)  triple_100: 29.4288 (30.2385)  triple_80: 30.9677 (31.6788)  triple_60: 29.3979 (30.1755)  triple_40: 20.0966 (20.8315)  time: 5.9022  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 910/1724]  eta: 1:20:05  lr: 0.000000  loss: 127.4976 (131.2964)  loss_n_40: 4.3891 (4.4507)  loss_n_60: 4.4799 (4.5629)  loss_n_80: 4.6117 (4.6855)  loss_n_100: 4.6457 (4.7047)  triple_100: 29.4117 (30.2305)  triple_80: 30.7845 (31.6700)  triple_60: 29.0875 (30.1665)  triple_40: 19.8387 (20.8256)  time: 5.9025  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 920/1724]  eta: 1:19:06  lr: 0.000000  loss: 127.1417 (131.2440)  loss_n_40: 4.3891 (4.4502)  loss_n_60: 4.4799 (4.5624)  loss_n_80: 4.6117 (4.6849)  loss_n_100: 4.6457 (4.7041)  triple_100: 29.2105 (30.2173)  triple_80: 30.7845 (31.6570)  triple_60: 29.0875 (30.1541)  triple_40: 19.6909 (20.8141)  time: 5.9034  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 930/1724]  eta: 1:18:07  lr: 0.000000  loss: 127.1417 (131.1989)  loss_n_40: 4.3836 (4.4492)  loss_n_60: 4.4727 (4.5614)  loss_n_80: 4.5914 (4.6839)  loss_n_100: 4.6088 (4.7031)  triple_100: 29.2105 (30.2065)  triple_80: 30.8457 (31.6445)  triple_60: 29.3976 (30.1424)  triple_40: 19.8160 (20.8080)  time: 5.9026  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 940/1724]  eta: 1:17:08  lr: 0.000000  loss: 123.0670 (131.1077)  loss_n_40: 4.3167 (4.4479)  loss_n_60: 4.4607 (4.5597)  loss_n_80: 4.5911 (4.6821)  loss_n_100: 4.5919 (4.7014)  triple_100: 28.7612 (30.1831)  triple_80: 29.8606 (31.6202)  triple_60: 28.4591 (30.1189)  triple_40: 19.9444 (20.7945)  time: 5.9014  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 950/1724]  eta: 1:16:09  lr: 0.000000  loss: 122.5460 (131.0556)  loss_n_40: 4.3729 (4.4474)  loss_n_60: 4.4090 (4.5590)  loss_n_80: 4.5257 (4.6813)  loss_n_100: 4.5218 (4.7005)  triple_100: 28.0514 (30.1691)  triple_80: 29.5999 (31.6065)  triple_60: 27.9859 (30.1061)  triple_40: 19.8445 (20.7858)  time: 5.9012  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 960/1724]  eta: 1:15:10  lr: 0.000000  loss: 132.0163 (131.1056)  loss_n_40: 4.4952 (4.4484)  loss_n_60: 4.5996 (4.5600)  loss_n_80: 4.7208 (4.6824)  loss_n_100: 4.7462 (4.7015)  triple_100: 30.3904 (30.1807)  triple_80: 32.1658 (31.6194)  triple_60: 30.3764 (30.1188)  triple_40: 20.5797 (20.7945)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 970/1724]  eta: 1:14:11  lr: 0.000000  loss: 132.0163 (131.0942)  loss_n_40: 4.5020 (4.4478)  loss_n_60: 4.5923 (4.5595)  loss_n_80: 4.6901 (4.6818)  loss_n_100: 4.7153 (4.7010)  triple_100: 30.5146 (30.1787)  triple_80: 31.8003 (31.6156)  triple_60: 30.2211 (30.1146)  triple_40: 21.2711 (20.7953)  time: 5.9013  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 980/1724]  eta: 1:13:12  lr: 0.000000  loss: 126.1172 (131.0035)  loss_n_40: 4.3612 (4.4459)  loss_n_60: 4.4559 (4.5576)  loss_n_80: 4.5998 (4.6800)  loss_n_100: 4.6115 (4.6992)  triple_100: 28.8100 (30.1566)  triple_80: 30.5279 (31.5931)  triple_60: 28.8407 (30.0919)  triple_40: 19.9102 (20.7792)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [ 990/1724]  eta: 1:12:13  lr: 0.000000  loss: 126.1172 (131.0250)  loss_n_40: 4.3612 (4.4460)  loss_n_60: 4.4559 (4.5579)  loss_n_80: 4.5998 (4.6804)  loss_n_100: 4.6115 (4.6996)  triple_100: 28.8100 (30.1630)  triple_80: 30.5279 (31.5992)  triple_60: 28.8407 (30.0972)  triple_40: 19.9102 (20.7818)  time: 5.9012  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1000/1724]  eta: 1:11:14  lr: 0.000000  loss: 133.7314 (131.0352)  loss_n_40: 4.4216 (4.4464)  loss_n_60: 4.5831 (4.5583)  loss_n_80: 4.7084 (4.6808)  loss_n_100: 4.7410 (4.7000)  triple_100: 31.2745 (30.1667)  triple_80: 32.3310 (31.6020)  triple_60: 30.9184 (30.0998)  triple_40: 20.7643 (20.7813)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1010/1724]  eta: 1:10:15  lr: 0.000000  loss: 129.0638 (131.0147)  loss_n_40: 4.4457 (4.4461)  loss_n_60: 4.5216 (4.5578)  loss_n_80: 4.6382 (4.6803)  loss_n_100: 4.6714 (4.6996)  triple_100: 29.9770 (30.1613)  triple_80: 31.0767 (31.5959)  triple_60: 29.5024 (30.0943)  triple_40: 20.6621 (20.7793)  time: 5.9008  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1020/1724]  eta: 1:09:15  lr: 0.000000  loss: 131.2008 (131.0294)  loss_n_40: 4.4176 (4.4460)  loss_n_60: 4.5478 (4.5577)  loss_n_80: 4.6786 (4.6803)  loss_n_100: 4.6910 (4.6997)  triple_100: 30.0635 (30.1656)  triple_80: 31.6321 (31.5998)  triple_60: 30.0797 (30.0971)  triple_40: 21.0107 (20.7832)  time: 5.9009  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1030/1724]  eta: 1:08:16  lr: 0.000000  loss: 135.1814 (131.1050)  loss_n_40: 4.4950 (4.4473)  loss_n_60: 4.6203 (4.5592)  loss_n_80: 4.7464 (4.6818)  loss_n_100: 4.7654 (4.7011)  triple_100: 31.3595 (30.1845)  triple_80: 32.8497 (31.6199)  triple_60: 31.2520 (30.1163)  triple_40: 21.7220 (20.7948)  time: 5.9022  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1040/1724]  eta: 1:07:17  lr: 0.000000  loss: 134.1573 (131.1089)  loss_n_40: 4.4926 (4.4475)  loss_n_60: 4.5924 (4.5594)  loss_n_80: 4.7124 (4.6819)  loss_n_100: 4.7313 (4.7011)  triple_100: 30.7501 (30.1852)  triple_80: 32.3163 (31.6205)  triple_60: 30.8565 (30.1175)  triple_40: 20.9701 (20.7959)  time: 5.9031  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [1050/1724]  eta: 1:06:18  lr: 0.000000  loss: 130.3113 (131.0839)  loss_n_40: 4.4328 (4.4473)  loss_n_60: 4.5443 (4.5590)  loss_n_80: 4.6585 (4.6815)  loss_n_100: 4.6783 (4.7007)  triple_100: 29.9153 (30.1785)  triple_80: 31.3031 (31.6135)  triple_60: 30.0108 (30.1111)  triple_40: 20.5689 (20.7922)  time: 5.9027  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1060/1724]  eta: 1:05:19  lr: 0.000000  loss: 132.2673 (131.0918)  loss_n_40: 4.4896 (4.4475)  loss_n_60: 4.5799 (4.5593)  loss_n_80: 4.7053 (4.6818)  loss_n_100: 4.7201 (4.7010)  triple_100: 30.4664 (30.1801)  triple_80: 31.5528 (31.6172)  triple_60: 30.3214 (30.1145)  triple_40: 20.6333 (20.7902)  time: 5.9031  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1070/1724]  eta: 1:04:20  lr: 0.000000  loss: 131.4532 (131.0945)  loss_n_40: 4.4551 (4.4474)  loss_n_60: 4.5761 (4.5593)  loss_n_80: 4.7053 (4.6818)  loss_n_100: 4.7219 (4.7010)  triple_100: 30.5317 (30.1804)  triple_80: 31.7404 (31.6175)  triple_60: 30.0316 (30.1147)  triple_40: 20.6333 (20.7924)  time: 5.9036  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1080/1724]  eta: 1:03:21  lr: 0.000000  loss: 131.4532 (131.1138)  loss_n_40: 4.4340 (4.4477)  loss_n_60: 4.5386 (4.5597)  loss_n_80: 4.6819 (4.6822)  loss_n_100: 4.7124 (4.7013)  triple_100: 30.5317 (30.1849)  triple_80: 31.7404 (31.6231)  triple_60: 30.0316 (30.1199)  triple_40: 20.2914 (20.7948)  time: 5.9035  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1090/1724]  eta: 1:02:22  lr: 0.000000  loss: 132.0583 (131.1631)  loss_n_40: 4.4340 (4.4484)  loss_n_60: 4.5644 (4.5606)  loss_n_80: 4.6819 (4.6831)  loss_n_100: 4.7124 (4.7022)  triple_100: 30.6498 (30.1972)  triple_80: 32.0241 (31.6362)  triple_60: 30.3889 (30.1328)  triple_40: 20.4397 (20.8024)  time: 5.9029  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1100/1724]  eta: 1:01:23  lr: 0.000000  loss: 127.8386 (131.1407)  loss_n_40: 4.3807 (4.4479)  loss_n_60: 4.5326 (4.5601)  loss_n_80: 4.6638 (4.6827)  loss_n_100: 4.6579 (4.7018)  triple_100: 29.5022 (30.1922)  triple_80: 31.1611 (31.6313)  triple_60: 29.4510 (30.1274)  triple_40: 20.3409 (20.7974)  time: 5.9029  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1110/1724]  eta: 1:00:24  lr: 0.000000  loss: 125.8374 (131.1368)  loss_n_40: 4.3708 (4.4478)  loss_n_60: 4.4878 (4.5600)  loss_n_80: 4.6142 (4.6826)  loss_n_100: 4.6102 (4.7017)  triple_100: 28.6788 (30.1914)  triple_80: 30.6221 (31.6301)  triple_60: 29.1383 (30.1262)  triple_40: 19.3153 (20.7970)  time: 5.9035  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1120/1724]  eta: 0:59:25  lr: 0.000000  loss: 133.7545 (131.1692)  loss_n_40: 4.5019 (4.4482)  loss_n_60: 4.5874 (4.5606)  loss_n_80: 4.7084 (4.6832)  loss_n_100: 4.7561 (4.7023)  triple_100: 30.9696 (30.1995)  triple_80: 32.2252 (31.6396)  triple_60: 30.5150 (30.1349)  triple_40: 21.0878 (20.8007)  time: 5.9035  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1130/1724]  eta: 0:58:26  lr: 0.000000  loss: 132.8246 (131.1109)  loss_n_40: 4.4096 (4.4472)  loss_n_60: 4.5522 (4.5595)  loss_n_80: 4.6810 (4.6821)  loss_n_100: 4.7178 (4.7012)  triple_100: 30.7748 (30.1854)  triple_80: 32.1728 (31.6244)  triple_60: 30.4841 (30.1197)  triple_40: 20.8111 (20.7914)  time: 5.9036  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1140/1724]  eta: 0:57:27  lr: 0.000000  loss: 129.3322 (131.1467)  loss_n_40: 4.4003 (4.4479)  loss_n_60: 4.5276 (4.5602)  loss_n_80: 4.6633 (4.6829)  loss_n_100: 4.7017 (4.7019)  triple_100: 29.4733 (30.1937)  triple_80: 31.5556 (31.6346)  triple_60: 29.9418 (30.1293)  triple_40: 20.1753 (20.7962)  time: 5.9039  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1150/1724]  eta: 0:56:28  lr: 0.000000  loss: 134.8591 (131.1579)  loss_n_40: 4.4910 (4.4483)  loss_n_60: 4.6217 (4.5605)  loss_n_80: 4.7479 (4.6831)  loss_n_100: 4.7633 (4.7021)  triple_100: 31.1353 (30.1957)  triple_80: 32.6114 (31.6362)  triple_60: 31.1105 (30.1318)  triple_40: 21.7496 (20.8002)  time: 5.9040  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1160/1724]  eta: 0:55:29  lr: 0.000000  loss: 133.9123 (131.1964)  loss_n_40: 4.4616 (4.4490)  loss_n_60: 4.5611 (4.5614)  loss_n_80: 4.7192 (4.6840)  loss_n_100: 4.7201 (4.7029)  triple_100: 30.7491 (30.2044)  triple_80: 32.1163 (31.6476)  triple_60: 30.5056 (30.1428)  triple_40: 21.1440 (20.8044)  time: 5.9039  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1170/1724]  eta: 0:54:30  lr: 0.000000  loss: 132.7104 (131.1996)  loss_n_40: 4.4616 (4.4489)  loss_n_60: 4.5603 (4.5613)  loss_n_80: 4.7192 (4.6839)  loss_n_100: 4.7201 (4.7029)  triple_100: 30.5566 (30.2054)  triple_80: 31.9153 (31.6476)  triple_60: 30.3408 (30.1427)  triple_40: 21.1440 (20.8069)  time: 5.9026  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1180/1724]  eta: 0:53:31  lr: 0.000000  loss: 130.2928 (131.1877)  loss_n_40: 4.3971 (4.4490)  loss_n_60: 4.5187 (4.5612)  loss_n_80: 4.6532 (4.6838)  loss_n_100: 4.6753 (4.7028)  triple_100: 30.2110 (30.2022)  triple_80: 31.4393 (31.6444)  triple_60: 29.8300 (30.1399)  triple_40: 20.5682 (20.8045)  time: 5.9021  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1190/1724]  eta: 0:52:32  lr: 0.000000  loss: 129.8115 (131.2197)  loss_n_40: 4.4458 (4.4497)  loss_n_60: 4.5689 (4.5618)  loss_n_80: 4.6811 (4.6843)  loss_n_100: 4.7167 (4.7034)  triple_100: 29.7512 (30.2106)  triple_80: 30.7814 (31.6522)  triple_60: 29.6483 (30.1478)  triple_40: 20.4042 (20.8099)  time: 5.9030  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1200/1724]  eta: 0:51:33  lr: 0.000000  loss: 135.2224 (131.2443)  loss_n_40: 4.5491 (4.4501)  loss_n_60: 4.6149 (4.5623)  loss_n_80: 4.7425 (4.6848)  loss_n_100: 4.7596 (4.7039)  triple_100: 31.3057 (30.2166)  triple_80: 32.7361 (31.6589)  triple_60: 31.2558 (30.1545)  triple_40: 21.3083 (20.8132)  time: 5.9024  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1210/1724]  eta: 0:50:34  lr: 0.000000  loss: 135.2224 (131.2691)  loss_n_40: 4.5242 (4.4506)  loss_n_60: 4.6404 (4.5628)  loss_n_80: 4.7425 (4.6853)  loss_n_100: 4.7596 (4.7044)  triple_100: 31.2320 (30.2231)  triple_80: 32.7361 (31.6648)  triple_60: 31.2558 (30.1604)  triple_40: 21.3083 (20.8178)  time: 5.9012  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1220/1724]  eta: 0:49:35  lr: 0.000000  loss: 129.7568 (131.2308)  loss_n_40: 4.4905 (4.4500)  loss_n_60: 4.5962 (4.5623)  loss_n_80: 4.6976 (4.6847)  loss_n_100: 4.7062 (4.7039)  triple_100: 29.9584 (30.2142)  triple_80: 31.1770 (31.6556)  triple_60: 29.7591 (30.1515)  triple_40: 20.5463 (20.8086)  time: 5.9005  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1230/1724]  eta: 0:48:36  lr: 0.000000  loss: 131.6493 (131.2452)  loss_n_40: 4.4905 (4.4503)  loss_n_60: 4.5962 (4.5625)  loss_n_80: 4.7057 (4.6850)  loss_n_100: 4.7153 (4.7041)  triple_100: 29.9584 (30.2181)  triple_80: 31.4382 (31.6592)  triple_60: 30.2914 (30.1550)  triple_40: 20.3622 (20.8110)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1240/1724]  eta: 0:47:37  lr: 0.000000  loss: 135.1568 (131.3158)  loss_n_40: 4.5495 (4.4515)  loss_n_60: 4.6443 (4.5638)  loss_n_80: 4.7597 (4.6862)  loss_n_100: 4.7911 (4.7054)  triple_100: 31.0007 (30.2345)  triple_80: 32.5901 (31.6766)  triple_60: 31.0409 (30.1725)  triple_40: 21.6441 (20.8254)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1250/1724]  eta: 0:46:38  lr: 0.000000  loss: 133.7368 (131.2918)  loss_n_40: 4.4560 (4.4510)  loss_n_60: 4.5457 (4.5632)  loss_n_80: 4.6761 (4.6856)  loss_n_100: 4.6771 (4.7047)  triple_100: 30.5727 (30.2273)  triple_80: 31.8209 (31.6691)  triple_60: 30.3577 (30.1657)  triple_40: 21.4206 (20.8251)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1260/1724]  eta: 0:45:39  lr: 0.000000  loss: 129.1380 (131.3240)  loss_n_40: 4.4277 (4.4516)  loss_n_60: 4.5325 (4.5638)  loss_n_80: 4.6491 (4.6861)  loss_n_100: 4.6689 (4.7052)  triple_100: 29.5929 (30.2343)  triple_80: 31.0272 (31.6769)  triple_60: 29.4957 (30.1738)  triple_40: 20.7466 (20.8322)  time: 5.8996  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1270/1724]  eta: 0:44:40  lr: 0.000000  loss: 132.9391 (131.3328)  loss_n_40: 4.4684 (4.4519)  loss_n_60: 4.5809 (4.5639)  loss_n_80: 4.7033 (4.6862)  loss_n_100: 4.7105 (4.7053)  triple_100: 30.6114 (30.2358)  triple_80: 32.1864 (31.6774)  triple_60: 30.5908 (30.1751)  triple_40: 21.2171 (20.8372)  time: 5.8992  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [1280/1724]  eta: 0:43:40  lr: 0.000000  loss: 132.9391 (131.3606)  loss_n_40: 4.4939 (4.4524)  loss_n_60: 4.5809 (4.5644)  loss_n_80: 4.7033 (4.6867)  loss_n_100: 4.7105 (4.7058)  triple_100: 30.5745 (30.2419)  triple_80: 32.1864 (31.6846)  triple_60: 30.5908 (30.1823)  triple_40: 21.2843 (20.8424)  time: 5.8999  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1290/1724]  eta: 0:42:41  lr: 0.000000  loss: 128.3459 (131.3348)  loss_n_40: 4.4356 (4.4520)  loss_n_60: 4.5278 (4.5640)  loss_n_80: 4.6338 (4.6862)  loss_n_100: 4.6626 (4.7053)  triple_100: 29.4012 (30.2363)  triple_80: 30.8820 (31.6780)  triple_60: 29.4365 (30.1757)  triple_40: 20.2048 (20.8374)  time: 5.9008  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1300/1724]  eta: 0:41:42  lr: 0.000000  loss: 125.2286 (131.2691)  loss_n_40: 4.3319 (4.4509)  loss_n_60: 4.4244 (4.5627)  loss_n_80: 4.5603 (4.6850)  loss_n_100: 4.5965 (4.7042)  triple_100: 28.5380 (30.2205)  triple_80: 30.0326 (31.6612)  triple_60: 28.6135 (30.1591)  triple_40: 19.6035 (20.8255)  time: 5.9019  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1310/1724]  eta: 0:40:43  lr: 0.000000  loss: 125.9638 (131.2821)  loss_n_40: 4.3657 (4.4510)  loss_n_60: 4.5024 (4.5630)  loss_n_80: 4.5983 (4.6853)  loss_n_100: 4.6223 (4.7045)  triple_100: 29.2229 (30.2244)  triple_80: 30.3108 (31.6653)  triple_60: 28.8070 (30.1626)  triple_40: 19.6240 (20.8259)  time: 5.9018  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1320/1724]  eta: 0:39:44  lr: 0.000000  loss: 130.3624 (131.3148)  loss_n_40: 4.4473 (4.4518)  loss_n_60: 4.5759 (4.5637)  loss_n_80: 4.6778 (4.6859)  loss_n_100: 4.7106 (4.7051)  triple_100: 30.1760 (30.2321)  triple_80: 31.5214 (31.6729)  triple_60: 29.8600 (30.1705)  triple_40: 20.1912 (20.8329)  time: 5.9003  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1330/1724]  eta: 0:38:45  lr: 0.000000  loss: 132.5155 (131.3508)  loss_n_40: 4.4473 (4.4523)  loss_n_60: 4.5759 (4.5642)  loss_n_80: 4.6974 (4.6865)  loss_n_100: 4.7145 (4.7057)  triple_100: 30.4345 (30.2414)  triple_80: 31.9352 (31.6819)  triple_60: 30.4328 (30.1792)  triple_40: 21.0657 (20.8396)  time: 5.8999  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1340/1724]  eta: 0:37:46  lr: 0.000000  loss: 129.8541 (131.3189)  loss_n_40: 4.3856 (4.4518)  loss_n_60: 4.4707 (4.5637)  loss_n_80: 4.6041 (4.6858)  loss_n_100: 4.6382 (4.7051)  triple_100: 30.0563 (30.2340)  triple_80: 31.3642 (31.6736)  triple_60: 29.7873 (30.1712)  triple_40: 20.6262 (20.8337)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1350/1724]  eta: 0:36:47  lr: 0.000000  loss: 130.9456 (131.3298)  loss_n_40: 4.4876 (4.4520)  loss_n_60: 4.5800 (4.5639)  loss_n_80: 4.7155 (4.6861)  loss_n_100: 4.7387 (4.7053)  triple_100: 30.1490 (30.2369)  triple_80: 31.7711 (31.6768)  triple_60: 30.2817 (30.1741)  triple_40: 20.6262 (20.8348)  time: 5.9023  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1360/1724]  eta: 0:35:48  lr: 0.000000  loss: 137.1253 (131.3197)  loss_n_40: 4.5210 (4.4518)  loss_n_60: 4.6512 (4.5636)  loss_n_80: 4.7758 (4.6858)  loss_n_100: 4.7893 (4.7051)  triple_100: 31.2253 (30.2344)  triple_80: 32.9454 (31.6735)  triple_60: 31.7074 (30.1711)  triple_40: 21.5809 (20.8344)  time: 5.9028  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1370/1724]  eta: 0:34:49  lr: 0.000000  loss: 130.4094 (131.2775)  loss_n_40: 4.4427 (4.4509)  loss_n_60: 4.5388 (4.5628)  loss_n_80: 4.6729 (4.6850)  loss_n_100: 4.6815 (4.7043)  triple_100: 29.8764 (30.2245)  triple_80: 31.5022 (31.6625)  triple_60: 30.0092 (30.1602)  triple_40: 20.7741 (20.8274)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1380/1724]  eta: 0:33:50  lr: 0.000000  loss: 126.8819 (131.2690)  loss_n_40: 4.3335 (4.4506)  loss_n_60: 4.4844 (4.5626)  loss_n_80: 4.6171 (4.6849)  loss_n_100: 4.6433 (4.7042)  triple_100: 29.7763 (30.2238)  triple_80: 30.6116 (31.6615)  triple_60: 29.0992 (30.1582)  triple_40: 19.3987 (20.8231)  time: 5.9011  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1390/1724]  eta: 0:32:51  lr: 0.000000  loss: 132.6023 (131.2771)  loss_n_40: 4.4348 (4.4507)  loss_n_60: 4.5483 (4.5628)  loss_n_80: 4.6629 (4.6850)  loss_n_100: 4.6948 (4.7043)  triple_100: 30.4595 (30.2260)  triple_80: 31.9588 (31.6636)  triple_60: 30.4501 (30.1601)  triple_40: 20.4740 (20.8246)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1400/1724]  eta: 0:31:52  lr: 0.000000  loss: 133.0493 (131.2760)  loss_n_40: 4.4615 (4.4507)  loss_n_60: 4.5851 (4.5628)  loss_n_80: 4.7135 (4.6850)  loss_n_100: 4.7215 (4.7043)  triple_100: 30.6409 (30.2254)  triple_80: 32.4359 (31.6637)  triple_60: 30.8565 (30.1604)  triple_40: 20.7213 (20.8236)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1410/1724]  eta: 0:30:53  lr: 0.000000  loss: 130.9527 (131.2478)  loss_n_40: 4.4615 (4.4502)  loss_n_60: 4.5851 (4.5622)  loss_n_80: 4.7135 (4.6844)  loss_n_100: 4.7215 (4.7037)  triple_100: 30.0200 (30.2185)  triple_80: 31.6870 (31.6565)  triple_60: 30.2251 (30.1533)  triple_40: 20.7213 (20.8189)  time: 5.9020  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1420/1724]  eta: 0:29:54  lr: 0.000000  loss: 130.9431 (131.2609)  loss_n_40: 4.4672 (4.4504)  loss_n_60: 4.5904 (4.5625)  loss_n_80: 4.7209 (4.6847)  loss_n_100: 4.7297 (4.7040)  triple_100: 29.9174 (30.2221)  triple_80: 31.8603 (31.6605)  triple_60: 30.2272 (30.1569)  triple_40: 20.5323 (20.8198)  time: 5.9021  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1430/1724]  eta: 0:28:55  lr: 0.000000  loss: 130.9431 (131.2402)  loss_n_40: 4.4638 (4.4504)  loss_n_60: 4.5777 (4.5622)  loss_n_80: 4.7100 (4.6844)  loss_n_100: 4.7051 (4.7037)  triple_100: 29.9168 (30.2162)  triple_80: 31.8221 (31.6548)  triple_60: 30.2272 (30.1517)  triple_40: 20.5323 (20.8169)  time: 5.9027  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1440/1724]  eta: 0:27:56  lr: 0.000000  loss: 130.1025 (131.2001)  loss_n_40: 4.4157 (4.4500)  loss_n_60: 4.5024 (4.5616)  loss_n_80: 4.6367 (4.6838)  loss_n_100: 4.6777 (4.7030)  triple_100: 29.9168 (30.2054)  triple_80: 31.2836 (31.6440)  triple_60: 29.7418 (30.1415)  triple_40: 19.5522 (20.8108)  time: 5.9036  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1450/1724]  eta: 0:26:57  lr: 0.000000  loss: 131.5088 (131.1837)  loss_n_40: 4.4482 (4.4497)  loss_n_60: 4.5664 (4.5612)  loss_n_80: 4.6889 (4.6834)  loss_n_100: 4.7085 (4.7027)  triple_100: 30.0692 (30.2019)  triple_80: 31.6714 (31.6395)  triple_60: 30.2348 (30.1367)  triple_40: 21.0350 (20.8085)  time: 5.9022  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1460/1724]  eta: 0:25:58  lr: 0.000000  loss: 134.4530 (131.2168)  loss_n_40: 4.4618 (4.4502)  loss_n_60: 4.6160 (4.5618)  loss_n_80: 4.7299 (4.6840)  loss_n_100: 4.7679 (4.7032)  triple_100: 30.9765 (30.2093)  triple_80: 32.5948 (31.6482)  triple_60: 31.0297 (30.1453)  triple_40: 21.1601 (20.8146)  time: 5.9020  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1470/1724]  eta: 0:24:59  lr: 0.000000  loss: 138.8273 (131.2551)  loss_n_40: 4.5883 (4.4509)  loss_n_60: 4.7033 (4.5626)  loss_n_80: 4.8177 (4.6848)  loss_n_100: 4.8330 (4.7039)  triple_100: 31.9464 (30.2181)  triple_80: 33.3366 (31.6582)  triple_60: 31.9111 (30.1551)  triple_40: 21.9146 (20.8215)  time: 5.9032  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1480/1724]  eta: 0:24:00  lr: 0.000000  loss: 134.2820 (131.2525)  loss_n_40: 4.4995 (4.4507)  loss_n_60: 4.5987 (4.5626)  loss_n_80: 4.7180 (4.6848)  loss_n_100: 4.7369 (4.7040)  triple_100: 30.7896 (30.2176)  triple_80: 32.4336 (31.6585)  triple_60: 30.9588 (30.1548)  triple_40: 21.1402 (20.8196)  time: 5.9032  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1490/1724]  eta: 0:23:01  lr: 0.000000  loss: 127.8366 (131.2248)  loss_n_40: 4.3764 (4.4502)  loss_n_60: 4.4675 (4.5621)  loss_n_80: 4.5929 (4.6844)  loss_n_100: 4.6219 (4.7035)  triple_100: 29.3321 (30.2113)  triple_80: 30.9288 (31.6520)  triple_60: 29.3138 (30.1485)  triple_40: 19.9670 (20.8127)  time: 5.9029  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1500/1724]  eta: 0:22:02  lr: 0.000000  loss: 129.4321 (131.2478)  loss_n_40: 4.3951 (4.4506)  loss_n_60: 4.5503 (4.5625)  loss_n_80: 4.6708 (4.6848)  loss_n_100: 4.6846 (4.7039)  triple_100: 29.8888 (30.2161)  triple_80: 31.4688 (31.6581)  triple_60: 29.8798 (30.1542)  triple_40: 20.3109 (20.8176)  time: 5.9033  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1]  [1510/1724]  eta: 0:21:03  lr: 0.000000  loss: 132.9635 (131.2231)  loss_n_40: 4.4604 (4.4498)  loss_n_60: 4.5814 (4.5619)  loss_n_80: 4.6961 (4.6842)  loss_n_100: 4.7300 (4.7033)  triple_100: 30.3019 (30.2111)  triple_80: 32.2151 (31.6521)  triple_60: 30.5895 (30.1482)  triple_40: 20.6854 (20.8125)  time: 5.9040  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1520/1724]  eta: 0:20:04  lr: 0.000000  loss: 132.4575 (131.2574)  loss_n_40: 4.4604 (4.4504)  loss_n_60: 4.5814 (4.5625)  loss_n_80: 4.6961 (4.6849)  loss_n_100: 4.7300 (4.7040)  triple_100: 30.6893 (30.2192)  triple_80: 32.2151 (31.6618)  triple_60: 30.6912 (30.1576)  triple_40: 20.4457 (20.8170)  time: 5.9039  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1530/1724]  eta: 0:19:05  lr: 0.000000  loss: 134.4554 (131.2992)  loss_n_40: 4.5154 (4.4511)  loss_n_60: 4.6411 (4.5634)  loss_n_80: 4.7579 (4.6858)  loss_n_100: 4.7646 (4.7047)  triple_100: 30.8744 (30.2291)  triple_80: 32.6092 (31.6733)  triple_60: 31.1170 (30.1684)  triple_40: 21.1576 (20.8234)  time: 5.9034  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1540/1724]  eta: 0:18:06  lr: 0.000000  loss: 134.5265 (131.3152)  loss_n_40: 4.5127 (4.4513)  loss_n_60: 4.6293 (4.5637)  loss_n_80: 4.7579 (4.6861)  loss_n_100: 4.7688 (4.7050)  triple_100: 31.2163 (30.2339)  triple_80: 32.6092 (31.6781)  triple_60: 31.0393 (30.1727)  triple_40: 20.9850 (20.8245)  time: 5.9023  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1550/1724]  eta: 0:17:07  lr: 0.000000  loss: 134.5265 (131.3306)  loss_n_40: 4.5127 (4.4515)  loss_n_60: 4.6293 (4.5639)  loss_n_80: 4.7511 (4.6862)  loss_n_100: 4.7780 (4.7052)  triple_100: 31.4166 (30.2376)  triple_80: 32.5839 (31.6814)  triple_60: 30.9402 (30.1758)  triple_40: 20.9850 (20.8289)  time: 5.9019  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1560/1724]  eta: 0:16:08  lr: 0.000000  loss: 132.1827 (131.3653)  loss_n_40: 4.4491 (4.4519)  loss_n_60: 4.5338 (4.5644)  loss_n_80: 4.6535 (4.6868)  loss_n_100: 4.6807 (4.7058)  triple_100: 30.6893 (30.2467)  triple_80: 31.6645 (31.6901)  triple_60: 30.0540 (30.1841)  triple_40: 21.7019 (20.8355)  time: 5.9028  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1570/1724]  eta: 0:15:09  lr: 0.000000  loss: 131.2613 (131.3659)  loss_n_40: 4.4491 (4.4520)  loss_n_60: 4.5338 (4.5644)  loss_n_80: 4.6535 (4.6868)  loss_n_100: 4.6807 (4.7059)  triple_100: 30.6893 (30.2467)  triple_80: 31.6645 (31.6900)  triple_60: 29.9589 (30.1840)  triple_40: 20.8036 (20.8361)  time: 5.9035  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1580/1724]  eta: 0:14:10  lr: 0.000000  loss: 131.9965 (131.3778)  loss_n_40: 4.4865 (4.4522)  loss_n_60: 4.6045 (4.5646)  loss_n_80: 4.7344 (4.6870)  loss_n_100: 4.7478 (4.7061)  triple_100: 30.4664 (30.2492)  triple_80: 32.0563 (31.6931)  triple_60: 30.4677 (30.1871)  triple_40: 20.6638 (20.8385)  time: 5.9041  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1590/1724]  eta: 0:13:11  lr: 0.000000  loss: 134.7569 (131.4038)  loss_n_40: 4.5062 (4.4525)  loss_n_60: 4.6328 (4.5650)  loss_n_80: 4.7548 (4.6875)  loss_n_100: 4.7615 (4.7065)  triple_100: 30.9992 (30.2547)  triple_80: 32.6390 (31.7002)  triple_60: 30.9812 (30.1940)  triple_40: 21.1242 (20.8435)  time: 5.9044  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1600/1724]  eta: 0:12:11  lr: 0.000000  loss: 134.7569 (131.4310)  loss_n_40: 4.4942 (4.4529)  loss_n_60: 4.6294 (4.5655)  loss_n_80: 4.7548 (4.6879)  loss_n_100: 4.7615 (4.7069)  triple_100: 30.9992 (30.2615)  triple_80: 32.6390 (31.7066)  triple_60: 30.9812 (30.2002)  triple_40: 21.3227 (20.8495)  time: 5.9027  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1610/1724]  eta: 0:11:12  lr: 0.000000  loss: 128.0572 (131.4229)  loss_n_40: 4.3741 (4.4527)  loss_n_60: 4.4932 (4.5653)  loss_n_80: 4.6187 (4.6877)  loss_n_100: 4.6418 (4.7067)  triple_100: 29.8620 (30.2590)  triple_80: 30.8649 (31.7041)  triple_60: 29.3680 (30.1980)  triple_40: 20.4859 (20.8495)  time: 5.9002  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1620/1724]  eta: 0:10:13  lr: 0.000000  loss: 127.3612 (131.4077)  loss_n_40: 4.3721 (4.4525)  loss_n_60: 4.4650 (4.5650)  loss_n_80: 4.5977 (4.6874)  loss_n_100: 4.6259 (4.7064)  triple_100: 29.2960 (30.2550)  triple_80: 30.7683 (31.7001)  triple_60: 29.1835 (30.1941)  triple_40: 20.2188 (20.8472)  time: 5.9004  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1630/1724]  eta: 0:09:14  lr: 0.000000  loss: 141.6463 (131.4934)  loss_n_40: 4.6506 (4.4540)  loss_n_60: 4.7786 (4.5665)  loss_n_80: 4.9050 (4.6890)  loss_n_100: 4.9211 (4.7079)  triple_100: 32.6401 (30.2755)  triple_80: 34.2900 (31.7214)  triple_60: 32.7444 (30.2151)  triple_40: 22.4758 (20.8639)  time: 5.9022  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1640/1724]  eta: 0:08:15  lr: 0.000000  loss: 135.1355 (131.4608)  loss_n_40: 4.5003 (4.4533)  loss_n_60: 4.6086 (4.5659)  loss_n_80: 4.7418 (4.6883)  loss_n_100: 4.7603 (4.7073)  triple_100: 30.8445 (30.2676)  triple_80: 32.6825 (31.7132)  triple_60: 31.0035 (30.2067)  triple_40: 22.0042 (20.8585)  time: 5.9028  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1650/1724]  eta: 0:07:16  lr: 0.000000  loss: 132.6763 (131.4705)  loss_n_40: 4.3875 (4.4535)  loss_n_60: 4.5498 (4.5660)  loss_n_80: 4.6737 (4.6885)  loss_n_100: 4.7014 (4.7075)  triple_100: 30.6935 (30.2703)  triple_80: 32.0675 (31.7156)  triple_60: 30.5020 (30.2090)  triple_40: 20.6253 (20.8601)  time: 5.9028  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1660/1724]  eta: 0:06:17  lr: 0.000000  loss: 129.5715 (131.4407)  loss_n_40: 4.3875 (4.4530)  loss_n_60: 4.5498 (4.5656)  loss_n_80: 4.6737 (4.6880)  loss_n_100: 4.6987 (4.7070)  triple_100: 29.9354 (30.2631)  triple_80: 31.2650 (31.7082)  triple_60: 29.7078 (30.2021)  triple_40: 20.6253 (20.8537)  time: 5.9025  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1670/1724]  eta: 0:05:18  lr: 0.000000  loss: 129.5715 (131.4469)  loss_n_40: 4.4460 (4.4532)  loss_n_60: 4.5943 (4.5658)  loss_n_80: 4.6782 (4.6882)  loss_n_100: 4.6987 (4.7072)  triple_100: 29.9354 (30.2642)  triple_80: 31.2650 (31.7103)  triple_60: 29.7078 (30.2043)  triple_40: 20.8325 (20.8538)  time: 5.9016  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1680/1724]  eta: 0:04:19  lr: 0.000000  loss: 133.2553 (131.4279)  loss_n_40: 4.4460 (4.4528)  loss_n_60: 4.5943 (4.5654)  loss_n_80: 4.7125 (4.6878)  loss_n_100: 4.7520 (4.7068)  triple_100: 30.6924 (30.2599)  triple_80: 32.2489 (31.7056)  triple_60: 30.6514 (30.1993)  triple_40: 21.1492 (20.8503)  time: 5.9016  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1690/1724]  eta: 0:03:20  lr: 0.000000  loss: 134.7055 (131.4558)  loss_n_40: 4.5492 (4.4533)  loss_n_60: 4.6053 (4.5659)  loss_n_80: 4.7291 (4.6883)  loss_n_100: 4.7566 (4.7073)  triple_100: 30.9240 (30.2666)  triple_80: 32.2637 (31.7123)  triple_60: 30.7577 (30.2059)  triple_40: 21.8127 (20.8561)  time: 5.9014  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1700/1724]  eta: 0:02:21  lr: 0.000000  loss: 140.1842 (131.4773)  loss_n_40: 4.5796 (4.4538)  loss_n_60: 4.7360 (4.5664)  loss_n_80: 4.8582 (4.6888)  loss_n_100: 4.8740 (4.7078)  triple_100: 32.3847 (30.2720)  triple_80: 34.0985 (31.7184)  triple_60: 32.5186 (30.2119)  triple_40: 21.9424 (20.8583)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1710/1724]  eta: 0:01:22  lr: 0.000000  loss: 138.1508 (131.5083)  loss_n_40: 4.5521 (4.4543)  loss_n_60: 4.6927 (4.5670)  loss_n_80: 4.8260 (4.6894)  loss_n_100: 4.8328 (4.7083)  triple_100: 32.0295 (30.2794)  triple_80: 33.4859 (31.7270)  triple_60: 31.8317 (30.2202)  triple_40: 21.8212 (20.8627)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1720/1724]  eta: 0:00:23  lr: 0.000000  loss: 135.8515 (131.5140)  loss_n_40: 4.4956 (4.4544)  loss_n_60: 4.6200 (4.5670)  loss_n_80: 4.7335 (4.6895)  loss_n_100: 4.7545 (4.7084)  triple_100: 31.6010 (30.2811)  triple_80: 32.7084 (31.7282)  triple_60: 31.1700 (30.2214)  triple_40: 21.7402 (20.8639)  time: 5.8995  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:1]  [1723/1724]  eta: 0:00:05  lr: 0.000000  loss: 132.6092 (131.5050)  loss_n_40: 4.4525 (4.4543)  loss_n_60: 4.5748 (4.5669)  loss_n_80: 4.7003 (4.6894)  loss_n_100: 4.7229 (4.7083)  triple_100: 30.3101 (30.2785)  triple_80: 31.8912 (31.7261)  triple_60: 30.3091 (30.2195)  triple_40: 21.1323 (20.8621)  time: 5.8993  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:1] Total time: 2:49:36 (5.9030 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 132.6092 (131.5050)  loss_n_40: 4.4525 (4.4543)  loss_n_60: 4.5748 (4.5669)  loss_n_80: 4.7003 (4.6894)  loss_n_100: 4.7229 (4.7083)  triple_100: 30.3101 (30.2785)  triple_80: 31.8912 (31.7261)  triple_60: 30.3091 (30.2195)  triple_40: 21.1323 (20.8621)\n",
      "Valid: [epoch:1]  [  0/845]  eta: 0:21:18  loss: 161.2621 (161.2621)  loss_n_40: 5.0243 (5.0243)  loss_n_60: 5.1985 (5.1985)  loss_n_80: 5.3137 (5.3137)  loss_n_100: 5.3748 (5.3748)  triple_100: 38.0283 (38.0283)  triple_80: 39.1726 (39.1726)  triple_60: 37.5935 (37.5935)  triple_40: 25.5565 (25.5565)  time: 1.5132  data: 0.5325  max mem: 40153\n",
      "Valid: [epoch:1]  [ 10/845]  eta: 0:14:15  loss: 161.2621 (141.3649)  loss_n_40: 4.9720 (4.6494)  loss_n_60: 5.1466 (4.7992)  loss_n_80: 5.2485 (4.9092)  loss_n_100: 5.2476 (4.9092)  triple_100: 37.3095 (32.5913)  triple_80: 39.0556 (34.2433)  triple_60: 37.4653 (32.7938)  triple_40: 25.5565 (22.4694)  time: 1.0243  data: 0.0485  max mem: 40153\n",
      "Valid: [epoch:1]  [ 20/845]  eta: 0:13:45  loss: 131.1240 (135.4202)  loss_n_40: 4.4998 (4.5490)  loss_n_60: 4.6743 (4.6752)  loss_n_80: 4.7819 (4.7904)  loss_n_100: 4.7768 (4.7891)  triple_100: 30.1377 (31.0700)  triple_80: 32.2994 (32.7571)  triple_60: 30.8786 (31.2875)  triple_40: 19.8138 (21.5019)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [ 30/845]  eta: 0:13:29  loss: 139.0478 (139.5644)  loss_n_40: 4.6264 (4.6145)  loss_n_60: 4.7551 (4.7432)  loss_n_80: 4.8769 (4.8605)  loss_n_100: 4.8459 (4.8603)  triple_100: 31.9359 (32.0875)  triple_80: 33.8400 (33.7828)  triple_60: 32.4055 (32.2946)  triple_40: 22.4291 (22.3211)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [ 40/845]  eta: 0:13:16  loss: 135.1302 (137.4444)  loss_n_40: 4.5463 (4.5733)  loss_n_60: 4.6560 (4.7003)  loss_n_80: 4.8026 (4.8205)  loss_n_100: 4.7970 (4.8264)  triple_100: 31.0608 (31.6304)  triple_80: 32.7765 (33.2889)  triple_60: 31.0995 (31.7798)  triple_40: 21.3192 (21.8247)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [ 50/845]  eta: 0:13:04  loss: 131.7891 (137.7226)  loss_n_40: 4.5284 (4.5821)  loss_n_60: 4.6447 (4.7072)  loss_n_80: 4.7730 (4.8271)  loss_n_100: 4.7753 (4.8327)  triple_100: 30.5647 (31.6515)  triple_80: 32.3265 (33.3704)  triple_60: 30.7673 (31.8706)  triple_40: 20.6802 (21.8811)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [ 60/845]  eta: 0:12:53  loss: 130.2286 (135.6630)  loss_n_40: 4.4731 (4.5343)  loss_n_60: 4.6234 (4.6593)  loss_n_80: 4.7397 (4.7800)  loss_n_100: 4.7420 (4.7906)  triple_100: 30.2464 (31.1858)  triple_80: 31.9520 (32.8407)  triple_60: 30.3375 (31.3303)  triple_40: 20.6253 (21.5418)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [ 70/845]  eta: 0:12:42  loss: 128.2397 (134.9856)  loss_n_40: 4.4041 (4.5233)  loss_n_60: 4.5315 (4.6460)  loss_n_80: 4.6656 (4.7665)  loss_n_100: 4.6644 (4.7787)  triple_100: 29.3091 (31.0266)  triple_80: 30.9144 (32.6731)  triple_60: 29.3341 (31.1692)  triple_40: 19.7885 (21.4022)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [ 80/845]  eta: 0:12:31  loss: 133.4481 (136.2691)  loss_n_40: 4.4907 (4.5485)  loss_n_60: 4.6243 (4.6709)  loss_n_80: 4.7408 (4.7912)  loss_n_100: 4.7471 (4.8041)  triple_100: 30.6470 (31.3470)  triple_80: 32.5978 (32.9904)  triple_60: 31.1560 (31.4796)  triple_40: 20.3706 (21.6375)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [ 90/845]  eta: 0:12:21  loss: 128.8717 (134.1668)  loss_n_40: 4.4387 (4.5129)  loss_n_60: 4.5821 (4.6302)  loss_n_80: 4.6846 (4.7521)  loss_n_100: 4.7033 (4.7668)  triple_100: 29.6961 (30.8643)  triple_80: 31.4137 (32.4561)  triple_60: 29.8093 (30.9306)  triple_40: 19.7080 (21.2539)  time: 0.9764  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:1]  [100/845]  eta: 0:12:11  loss: 128.5224 (133.3553)  loss_n_40: 4.4222 (4.5012)  loss_n_60: 4.5093 (4.6160)  loss_n_80: 4.6414 (4.7387)  loss_n_100: 4.6572 (4.7517)  triple_100: 29.0813 (30.6497)  triple_80: 31.0590 (32.2566)  triple_60: 29.4048 (30.7249)  triple_40: 19.6432 (21.1165)  time: 0.9763  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:1]  [110/845]  eta: 0:12:01  loss: 129.8807 (133.5517)  loss_n_40: 4.4820 (4.5044)  loss_n_60: 4.5935 (4.6182)  loss_n_80: 4.7426 (4.7412)  loss_n_100: 4.7111 (4.7532)  triple_100: 29.9443 (30.6984)  triple_80: 31.6883 (32.3016)  triple_60: 29.9562 (30.7667)  triple_40: 20.5346 (21.1681)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [120/845]  eta: 0:11:51  loss: 129.8807 (133.4698)  loss_n_40: 4.4238 (4.4998)  loss_n_60: 4.5386 (4.6142)  loss_n_80: 4.6807 (4.7382)  loss_n_100: 4.6641 (4.7492)  triple_100: 29.7612 (30.6627)  triple_80: 31.6234 (32.2859)  triple_60: 29.9562 (30.7540)  triple_40: 20.4832 (21.1658)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [130/845]  eta: 0:11:41  loss: 128.2946 (133.2904)  loss_n_40: 4.4131 (4.4951)  loss_n_60: 4.5279 (4.6079)  loss_n_80: 4.6652 (4.7326)  loss_n_100: 4.6593 (4.7444)  triple_100: 29.6898 (30.6232)  triple_80: 31.0319 (32.2334)  triple_60: 29.6851 (30.6957)  triple_40: 20.0033 (21.1582)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [140/845]  eta: 0:11:31  loss: 125.8987 (132.8945)  loss_n_40: 4.3935 (4.4872)  loss_n_60: 4.5256 (4.5996)  loss_n_80: 4.6600 (4.7256)  loss_n_100: 4.6463 (4.7371)  triple_100: 28.8049 (30.5264)  triple_80: 30.8541 (32.1364)  triple_60: 29.1295 (30.5905)  triple_40: 19.0227 (21.0916)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [150/845]  eta: 0:11:21  loss: 127.2974 (132.6435)  loss_n_40: 4.3935 (4.4832)  loss_n_60: 4.5380 (4.5959)  loss_n_80: 4.6816 (4.7223)  loss_n_100: 4.6712 (4.7339)  triple_100: 29.0142 (30.4762)  triple_80: 31.3084 (32.0820)  triple_60: 29.4658 (30.5299)  triple_40: 19.1789 (21.0200)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [160/845]  eta: 0:11:11  loss: 129.1469 (132.9896)  loss_n_40: 4.5342 (4.4906)  loss_n_60: 4.6553 (4.6032)  loss_n_80: 4.7925 (4.7295)  loss_n_100: 4.8093 (4.7415)  triple_100: 30.0726 (30.5624)  triple_80: 31.9026 (32.1687)  triple_60: 30.1889 (30.6187)  triple_40: 20.9007 (21.0751)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [170/845]  eta: 0:11:01  loss: 139.3242 (133.6057)  loss_n_40: 4.6692 (4.5018)  loss_n_60: 4.7602 (4.6151)  loss_n_80: 4.9154 (4.7410)  loss_n_100: 4.9210 (4.7525)  triple_100: 31.9514 (30.7111)  triple_80: 34.1560 (32.3253)  triple_60: 32.6313 (30.7768)  triple_40: 21.9072 (21.1821)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [180/845]  eta: 0:10:51  loss: 134.0232 (133.9938)  loss_n_40: 4.5614 (4.5066)  loss_n_60: 4.6769 (4.6220)  loss_n_80: 4.7924 (4.7471)  loss_n_100: 4.7853 (4.7590)  triple_100: 30.9343 (30.8100)  triple_80: 32.8048 (32.4196)  triple_60: 31.4918 (30.8733)  triple_40: 21.9072 (21.2561)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [190/845]  eta: 0:10:41  loss: 136.5730 (134.0718)  loss_n_40: 4.5980 (4.5081)  loss_n_60: 4.6906 (4.6229)  loss_n_80: 4.8308 (4.7476)  loss_n_100: 4.8363 (4.7597)  triple_100: 31.3009 (30.8205)  triple_80: 33.2656 (32.4315)  triple_60: 31.7475 (30.8894)  triple_40: 20.9279 (21.2920)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [200/845]  eta: 0:10:31  loss: 136.5730 (134.0750)  loss_n_40: 4.4800 (4.5093)  loss_n_60: 4.5755 (4.6228)  loss_n_80: 4.6696 (4.7467)  loss_n_100: 4.6971 (4.7587)  triple_100: 31.0892 (30.8149)  triple_80: 32.6706 (32.4139)  triple_60: 31.3070 (30.8858)  triple_40: 21.6857 (21.3229)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [210/845]  eta: 0:10:21  loss: 129.0960 (133.7707)  loss_n_40: 4.4316 (4.5057)  loss_n_60: 4.5755 (4.6187)  loss_n_80: 4.6696 (4.7424)  loss_n_100: 4.6780 (4.7546)  triple_100: 29.9417 (30.7460)  triple_80: 31.3155 (32.3384)  triple_60: 29.7611 (30.8102)  triple_40: 20.6336 (21.2546)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [220/845]  eta: 0:10:11  loss: 137.7347 (134.2215)  loss_n_40: 4.5496 (4.5143)  loss_n_60: 4.6935 (4.6269)  loss_n_80: 4.8195 (4.7501)  loss_n_100: 4.8351 (4.7631)  triple_100: 31.3446 (30.8532)  triple_80: 33.3732 (32.4458)  triple_60: 31.8020 (30.9217)  triple_40: 22.1436 (21.3465)  time: 0.9770  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:1]  [230/845]  eta: 0:10:02  loss: 132.9671 (133.4263)  loss_n_40: 4.5421 (4.5012)  loss_n_60: 4.6597 (4.6134)  loss_n_80: 4.7418 (4.7350)  loss_n_100: 4.7837 (4.7487)  triple_100: 30.4506 (30.6652)  triple_80: 32.1198 (32.2283)  triple_60: 30.9222 (30.7204)  triple_40: 21.1028 (21.2140)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [240/845]  eta: 0:09:52  loss: 124.9700 (133.0969)  loss_n_40: 4.3639 (4.4954)  loss_n_60: 4.5189 (4.6079)  loss_n_80: 4.6467 (4.7301)  loss_n_100: 4.6429 (4.7434)  triple_100: 28.6325 (30.5911)  triple_80: 30.8822 (32.1537)  triple_60: 29.2215 (30.6434)  triple_40: 18.2309 (21.1319)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [250/845]  eta: 0:09:42  loss: 129.7933 (132.9842)  loss_n_40: 4.5073 (4.4949)  loss_n_60: 4.6382 (4.6056)  loss_n_80: 4.7646 (4.7276)  loss_n_100: 4.7398 (4.7415)  triple_100: 29.9033 (30.5580)  triple_80: 31.9902 (32.1167)  triple_60: 30.3382 (30.6111)  triple_40: 19.7234 (21.1289)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [260/845]  eta: 0:09:32  loss: 136.2430 (133.1629)  loss_n_40: 4.5518 (4.4989)  loss_n_60: 4.6865 (4.6090)  loss_n_80: 4.8319 (4.7311)  loss_n_100: 4.8226 (4.7456)  triple_100: 30.6902 (30.6059)  triple_80: 33.1346 (32.1647)  triple_60: 31.7494 (30.6562)  triple_40: 22.0079 (21.1514)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [270/845]  eta: 0:09:22  loss: 128.1131 (132.7089)  loss_n_40: 4.4169 (4.4906)  loss_n_60: 4.5160 (4.5993)  loss_n_80: 4.6594 (4.7211)  loss_n_100: 4.6537 (4.7358)  triple_100: 29.1712 (30.4945)  triple_80: 30.9485 (32.0409)  triple_60: 29.5470 (30.5377)  triple_40: 19.9692 (21.0890)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [280/845]  eta: 0:09:12  loss: 124.7943 (133.0078)  loss_n_40: 4.3707 (4.4943)  loss_n_60: 4.5126 (4.6042)  loss_n_80: 4.6594 (4.7260)  loss_n_100: 4.6222 (4.7406)  triple_100: 28.2384 (30.5665)  triple_80: 30.6345 (32.1160)  triple_60: 29.0410 (30.6100)  triple_40: 18.9467 (21.1503)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [290/845]  eta: 0:09:03  loss: 126.2608 (132.6219)  loss_n_40: 4.3986 (4.4894)  loss_n_60: 4.5432 (4.5978)  loss_n_80: 4.6818 (4.7194)  loss_n_100: 4.6678 (4.7338)  triple_100: 28.7285 (30.4671)  triple_80: 31.1514 (32.0201)  triple_60: 29.5200 (30.5158)  triple_40: 19.3637 (21.0785)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [300/845]  eta: 0:08:53  loss: 127.9247 (132.9493)  loss_n_40: 4.4009 (4.4941)  loss_n_60: 4.5432 (4.6032)  loss_n_80: 4.6739 (4.7247)  loss_n_100: 4.6649 (4.7394)  triple_100: 29.2696 (30.5507)  triple_80: 31.3229 (32.1020)  triple_60: 29.5750 (30.5950)  triple_40: 20.4186 (21.1403)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [310/845]  eta: 0:08:43  loss: 129.4193 (133.0052)  loss_n_40: 4.4334 (4.4939)  loss_n_60: 4.5704 (4.6032)  loss_n_80: 4.7011 (4.7247)  loss_n_100: 4.6757 (4.7391)  triple_100: 29.3739 (30.5603)  triple_80: 31.5269 (32.1150)  triple_60: 29.9620 (30.6092)  triple_40: 20.8328 (21.1598)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [320/845]  eta: 0:08:33  loss: 129.6725 (132.8946)  loss_n_40: 4.4397 (4.4913)  loss_n_60: 4.5870 (4.6009)  loss_n_80: 4.7049 (4.7225)  loss_n_100: 4.7118 (4.7368)  triple_100: 29.5795 (30.5347)  triple_80: 31.4655 (32.0887)  triple_60: 29.9074 (30.5813)  triple_40: 19.7556 (21.1385)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [330/845]  eta: 0:08:23  loss: 129.6725 (132.7384)  loss_n_40: 4.4091 (4.4897)  loss_n_60: 4.5334 (4.5980)  loss_n_80: 4.6886 (4.7193)  loss_n_100: 4.6663 (4.7334)  triple_100: 29.4446 (30.4869)  triple_80: 31.4655 (32.0418)  triple_60: 29.9074 (30.5399)  triple_40: 19.7556 (21.1293)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [340/845]  eta: 0:08:14  loss: 129.7323 (132.9096)  loss_n_40: 4.4172 (4.4938)  loss_n_60: 4.5484 (4.6020)  loss_n_80: 4.6886 (4.7235)  loss_n_100: 4.6734 (4.7376)  triple_100: 29.4446 (30.5262)  triple_80: 31.6143 (32.0880)  triple_60: 30.0736 (30.5829)  triple_40: 20.5306 (21.1556)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [350/845]  eta: 0:08:04  loss: 127.3566 (132.6761)  loss_n_40: 4.4172 (4.4890)  loss_n_60: 4.5484 (4.5976)  loss_n_80: 4.6796 (4.7191)  loss_n_100: 4.6734 (4.7335)  triple_100: 29.1710 (30.4707)  triple_80: 31.0074 (32.0302)  triple_60: 29.5283 (30.5269)  triple_40: 20.4535 (21.1091)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [360/845]  eta: 0:07:54  loss: 125.4572 (132.7212)  loss_n_40: 4.3949 (4.4896)  loss_n_60: 4.5317 (4.5985)  loss_n_80: 4.6758 (4.7200)  loss_n_100: 4.6657 (4.7341)  triple_100: 28.8881 (30.4776)  triple_80: 30.8408 (32.0453)  triple_60: 29.3665 (30.5415)  triple_40: 19.5380 (21.1146)  time: 0.9770  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:1]  [370/845]  eta: 0:07:44  loss: 128.9961 (132.7976)  loss_n_40: 4.4874 (4.4900)  loss_n_60: 4.6257 (4.5992)  loss_n_80: 4.7426 (4.7208)  loss_n_100: 4.7340 (4.7349)  triple_100: 29.5400 (30.4938)  triple_80: 31.6388 (32.0632)  triple_60: 30.1227 (30.5578)  triple_40: 19.9925 (21.1379)  time: 0.9773  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:1]  [380/845]  eta: 0:07:34  loss: 125.5601 (132.7185)  loss_n_40: 4.4135 (4.4870)  loss_n_60: 4.5464 (4.5965)  loss_n_80: 4.7052 (4.7182)  loss_n_100: 4.6637 (4.7327)  triple_100: 28.3975 (30.4792)  triple_80: 30.7694 (32.0406)  triple_60: 29.2069 (30.5328)  triple_40: 19.9925 (21.1316)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [390/845]  eta: 0:07:25  loss: 123.4252 (132.2779)  loss_n_40: 4.3409 (4.4794)  loss_n_60: 4.4557 (4.5886)  loss_n_80: 4.5941 (4.7102)  loss_n_100: 4.5968 (4.7249)  triple_100: 27.9776 (30.3746)  triple_80: 30.0927 (31.9285)  triple_60: 28.4899 (30.4218)  triple_40: 18.9233 (21.0497)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [400/845]  eta: 0:07:15  loss: 127.0653 (132.0108)  loss_n_40: 4.3924 (4.4753)  loss_n_60: 4.5321 (4.5833)  loss_n_80: 4.6656 (4.7048)  loss_n_100: 4.6526 (4.7196)  triple_100: 28.7010 (30.3051)  triple_80: 31.1280 (31.8575)  triple_60: 29.6159 (30.3531)  triple_40: 19.3358 (21.0122)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [410/845]  eta: 0:07:05  loss: 127.7831 (132.1650)  loss_n_40: 4.4183 (4.4786)  loss_n_60: 4.5571 (4.5867)  loss_n_80: 4.6902 (4.7082)  loss_n_100: 4.7024 (4.7228)  triple_100: 29.3240 (30.3389)  triple_80: 31.3004 (31.9000)  triple_60: 29.6626 (30.3957)  triple_40: 19.9469 (21.0341)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [420/845]  eta: 0:06:55  loss: 136.3186 (132.4190)  loss_n_40: 4.5016 (4.4832)  loss_n_60: 4.5881 (4.5915)  loss_n_80: 4.7356 (4.7130)  loss_n_100: 4.7389 (4.7276)  triple_100: 31.0924 (30.4002)  triple_80: 32.9479 (31.9621)  triple_60: 31.1735 (30.4554)  triple_40: 21.7630 (21.0860)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [430/845]  eta: 0:06:45  loss: 128.5744 (132.3632)  loss_n_40: 4.4110 (4.4822)  loss_n_60: 4.5482 (4.5911)  loss_n_80: 4.6680 (4.7128)  loss_n_100: 4.6732 (4.7273)  triple_100: 29.2912 (30.3868)  triple_80: 31.1836 (31.9551)  triple_60: 29.6391 (30.4461)  triple_40: 19.7794 (21.0617)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [440/845]  eta: 0:06:36  loss: 128.0132 (132.4353)  loss_n_40: 4.4110 (4.4843)  loss_n_60: 4.5614 (4.5932)  loss_n_80: 4.7088 (4.7150)  loss_n_100: 4.6851 (4.7291)  triple_100: 29.1994 (30.4015)  triple_80: 31.1836 (31.9775)  triple_60: 29.6660 (30.4663)  triple_40: 19.7794 (21.0684)  time: 0.9773  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:1]  [450/845]  eta: 0:06:26  loss: 128.6668 (132.3131)  loss_n_40: 4.4308 (4.4813)  loss_n_60: 4.5920 (4.5911)  loss_n_80: 4.7212 (4.7128)  loss_n_100: 4.7149 (4.7273)  triple_100: 29.5017 (30.3784)  triple_80: 31.6181 (31.9502)  triple_60: 30.0456 (30.4393)  triple_40: 19.8207 (21.0327)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [460/845]  eta: 0:06:16  loss: 135.2223 (132.5554)  loss_n_40: 4.4720 (4.4862)  loss_n_60: 4.6034 (4.5956)  loss_n_80: 4.7445 (4.7173)  loss_n_100: 4.7527 (4.7319)  triple_100: 31.0431 (30.4342)  triple_80: 32.4130 (32.0077)  triple_60: 30.9365 (30.4953)  triple_40: 20.7784 (21.0872)  time: 0.9770  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:1]  [470/845]  eta: 0:06:06  loss: 135.9838 (132.4508)  loss_n_40: 4.4720 (4.4835)  loss_n_60: 4.6074 (4.5929)  loss_n_80: 4.7382 (4.7145)  loss_n_100: 4.7068 (4.7294)  triple_100: 31.0726 (30.4111)  triple_80: 32.4130 (31.9788)  triple_60: 30.9365 (30.4670)  triple_40: 22.0482 (21.0736)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [480/845]  eta: 0:05:56  loss: 127.7600 (132.0331)  loss_n_40: 4.4038 (4.4773)  loss_n_60: 4.5312 (4.5855)  loss_n_80: 4.6371 (4.7070)  loss_n_100: 4.6674 (4.7220)  triple_100: 29.4548 (30.3101)  triple_80: 31.2598 (31.8697)  triple_60: 29.6438 (30.3623)  triple_40: 19.4974 (20.9993)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [490/845]  eta: 0:05:47  loss: 119.8542 (131.7436)  loss_n_40: 4.3119 (4.4736)  loss_n_60: 4.3581 (4.5806)  loss_n_80: 4.5160 (4.7019)  loss_n_100: 4.5500 (4.7172)  triple_100: 27.4746 (30.2381)  triple_80: 28.9997 (31.7907)  triple_60: 27.1780 (30.2879)  triple_40: 18.2247 (20.9535)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [500/845]  eta: 0:05:37  loss: 130.9950 (131.8636)  loss_n_40: 4.4835 (4.4763)  loss_n_60: 4.6272 (4.5833)  loss_n_80: 4.7347 (4.7048)  loss_n_100: 4.7387 (4.7196)  triple_100: 29.8994 (30.2640)  triple_80: 32.0149 (31.8255)  triple_60: 30.3430 (30.3210)  triple_40: 19.6502 (20.9691)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [510/845]  eta: 0:05:27  loss: 129.6801 (131.7199)  loss_n_40: 4.4727 (4.4735)  loss_n_60: 4.5963 (4.5808)  loss_n_80: 4.7313 (4.7022)  loss_n_100: 4.7099 (4.7172)  triple_100: 29.5627 (30.2309)  triple_80: 31.6318 (31.7920)  triple_60: 29.8255 (30.2876)  triple_40: 19.6502 (20.9358)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [520/845]  eta: 0:05:17  loss: 128.1154 (131.7546)  loss_n_40: 4.3992 (4.4729)  loss_n_60: 4.5583 (4.5809)  loss_n_80: 4.6871 (4.7024)  loss_n_100: 4.6905 (4.7170)  triple_100: 29.2463 (30.2379)  triple_80: 31.2208 (31.8021)  triple_60: 29.6628 (30.2967)  triple_40: 19.4039 (20.9447)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [530/845]  eta: 0:05:08  loss: 127.4583 (131.6472)  loss_n_40: 4.3992 (4.4716)  loss_n_60: 4.5303 (4.5791)  loss_n_80: 4.6735 (4.7005)  loss_n_100: 4.6728 (4.7151)  triple_100: 29.1118 (30.2093)  triple_80: 31.1818 (31.7735)  triple_60: 29.3807 (30.2694)  triple_40: 19.3990 (20.9287)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [540/845]  eta: 0:04:58  loss: 125.2410 (131.5568)  loss_n_40: 4.4155 (4.4701)  loss_n_60: 4.5303 (4.5772)  loss_n_80: 4.6689 (4.6986)  loss_n_100: 4.6670 (4.7135)  triple_100: 29.0106 (30.1906)  triple_80: 30.4075 (31.7478)  triple_60: 28.5510 (30.2460)  triple_40: 18.7172 (20.9130)  time: 0.9770  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:1]  [550/845]  eta: 0:04:48  loss: 125.2480 (131.3030)  loss_n_40: 4.4233 (4.4655)  loss_n_60: 4.4415 (4.5720)  loss_n_80: 4.6149 (4.6936)  loss_n_100: 4.6379 (4.7088)  triple_100: 29.0446 (30.1330)  triple_80: 30.0846 (31.6825)  triple_60: 28.4816 (30.1793)  triple_40: 19.3857 (20.8683)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [560/845]  eta: 0:04:38  loss: 131.0777 (131.5950)  loss_n_40: 4.4903 (4.4706)  loss_n_60: 4.6114 (4.5775)  loss_n_80: 4.7451 (4.6991)  loss_n_100: 4.7492 (4.7143)  triple_100: 30.2953 (30.2040)  triple_80: 32.0117 (31.7570)  triple_60: 30.3691 (30.2534)  triple_40: 20.7648 (20.9192)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [570/845]  eta: 0:04:28  loss: 131.0777 (131.5798)  loss_n_40: 4.4903 (4.4701)  loss_n_60: 4.6114 (4.5771)  loss_n_80: 4.7451 (4.6987)  loss_n_100: 4.7492 (4.7141)  triple_100: 30.2953 (30.2036)  triple_80: 31.9613 (31.7535)  triple_60: 30.2230 (30.2500)  triple_40: 20.2886 (20.9127)  time: 0.9772  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [580/845]  eta: 0:04:19  loss: 126.4916 (131.4846)  loss_n_40: 4.3986 (4.4690)  loss_n_60: 4.5283 (4.5756)  loss_n_80: 4.6529 (4.6969)  loss_n_100: 4.6497 (4.7121)  triple_100: 28.9160 (30.1786)  triple_80: 30.6629 (31.7290)  triple_60: 29.2692 (30.2281)  triple_40: 19.3221 (20.8953)  time: 0.9774  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:1]  [590/845]  eta: 0:04:09  loss: 129.9526 (131.5452)  loss_n_40: 4.4634 (4.4703)  loss_n_60: 4.6160 (4.5772)  loss_n_80: 4.7514 (4.6986)  loss_n_100: 4.7488 (4.7137)  triple_100: 30.0777 (30.1953)  triple_80: 31.8691 (31.7467)  triple_60: 30.0504 (30.2439)  triple_40: 20.1552 (20.8996)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [600/845]  eta: 0:03:59  loss: 131.1502 (131.4246)  loss_n_40: 4.4932 (4.4693)  loss_n_60: 4.6204 (4.5755)  loss_n_80: 4.7575 (4.6969)  loss_n_100: 4.7524 (4.7120)  triple_100: 30.0777 (30.1651)  triple_80: 32.0945 (31.7171)  triple_60: 30.4196 (30.2145)  triple_40: 20.2237 (20.8742)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [610/845]  eta: 0:03:49  loss: 129.5896 (131.4654)  loss_n_40: 4.4678 (4.4707)  loss_n_60: 4.5480 (4.5762)  loss_n_80: 4.6806 (4.6974)  loss_n_100: 4.6592 (4.7124)  triple_100: 29.8406 (30.1732)  triple_80: 31.2966 (31.7232)  triple_60: 29.6110 (30.2219)  triple_40: 20.2147 (20.8904)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [620/845]  eta: 0:03:39  loss: 139.5119 (131.6985)  loss_n_40: 4.6260 (4.4743)  loss_n_60: 4.7113 (4.5801)  loss_n_80: 4.8417 (4.7012)  loss_n_100: 4.8382 (4.7163)  triple_100: 31.9533 (30.2324)  triple_80: 33.9008 (31.7801)  triple_60: 32.2631 (30.2798)  triple_40: 22.3925 (20.9342)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [630/845]  eta: 0:03:30  loss: 139.7196 (131.8513)  loss_n_40: 4.6908 (4.4760)  loss_n_60: 4.7524 (4.5820)  loss_n_80: 4.8576 (4.7032)  loss_n_100: 4.9129 (4.7183)  triple_100: 32.5292 (30.2697)  triple_80: 34.2119 (31.8178)  triple_60: 32.2155 (30.3161)  triple_40: 22.8778 (20.9682)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [640/845]  eta: 0:03:20  loss: 128.1573 (131.6024)  loss_n_40: 4.3926 (4.4710)  loss_n_60: 4.5323 (4.5766)  loss_n_80: 4.6737 (4.6980)  loss_n_100: 4.6577 (4.7135)  triple_100: 29.3318 (30.2146)  triple_80: 31.5918 (31.7529)  triple_60: 29.7409 (30.2504)  triple_40: 20.1365 (20.9255)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [650/845]  eta: 0:03:10  loss: 130.7433 (131.8848)  loss_n_40: 4.5242 (4.4759)  loss_n_60: 4.6424 (4.5822)  loss_n_80: 4.7891 (4.7035)  loss_n_100: 4.7671 (4.7187)  triple_100: 30.1441 (30.2819)  triple_80: 32.3241 (31.8260)  triple_60: 30.8584 (30.3236)  triple_40: 20.1426 (20.9729)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [660/845]  eta: 0:03:00  loss: 133.6712 (131.9238)  loss_n_40: 4.5528 (4.4768)  loss_n_60: 4.7021 (4.5832)  loss_n_80: 4.8315 (4.7046)  loss_n_100: 4.8103 (4.7198)  triple_100: 30.7849 (30.2929)  triple_80: 32.6259 (31.8383)  triple_60: 31.0791 (30.3358)  triple_40: 21.3022 (20.9725)  time: 0.9773  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:1]  [670/845]  eta: 0:02:51  loss: 131.8086 (131.9507)  loss_n_40: 4.4441 (4.4783)  loss_n_60: 4.5805 (4.5844)  loss_n_80: 4.7216 (4.7057)  loss_n_100: 4.7222 (4.7206)  triple_100: 29.9069 (30.2969)  triple_80: 32.2201 (31.8455)  triple_60: 30.5531 (30.3430)  triple_40: 20.7229 (20.9762)  time: 0.9773  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:1]  [680/845]  eta: 0:02:41  loss: 127.9213 (131.8038)  loss_n_40: 4.4352 (4.4771)  loss_n_60: 4.5655 (4.5821)  loss_n_80: 4.7182 (4.7032)  loss_n_100: 4.7037 (4.7181)  triple_100: 29.1817 (30.2592)  triple_80: 31.1935 (31.8046)  triple_60: 29.4715 (30.3048)  triple_40: 19.8842 (20.9545)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [690/845]  eta: 0:02:31  loss: 128.1894 (131.8870)  loss_n_40: 4.4427 (4.4784)  loss_n_60: 4.5837 (4.5837)  loss_n_80: 4.7182 (4.7048)  loss_n_100: 4.7080 (4.7194)  triple_100: 29.3350 (30.2764)  triple_80: 31.7472 (31.8261)  triple_60: 29.9314 (30.3261)  triple_40: 19.7573 (20.9721)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [700/845]  eta: 0:02:21  loss: 131.5168 (131.8607)  loss_n_40: 4.4772 (4.4784)  loss_n_60: 4.6477 (4.5832)  loss_n_80: 4.7772 (4.7040)  loss_n_100: 4.7688 (4.7187)  triple_100: 30.0078 (30.2672)  triple_80: 32.1770 (31.8157)  triple_60: 30.7080 (30.3184)  triple_40: 20.7031 (20.9751)  time: 0.9773  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:1]  [710/845]  eta: 0:02:11  loss: 134.9113 (132.0384)  loss_n_40: 4.5021 (4.4812)  loss_n_60: 4.6477 (4.5864)  loss_n_80: 4.7772 (4.7074)  loss_n_100: 4.7688 (4.7218)  triple_100: 30.8166 (30.3081)  triple_80: 32.8765 (31.8608)  triple_60: 31.1255 (30.3626)  triple_40: 20.7823 (21.0101)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [720/845]  eta: 0:02:02  loss: 133.4141 (132.0435)  loss_n_40: 4.5058 (4.4820)  loss_n_60: 4.6582 (4.5869)  loss_n_80: 4.7988 (4.7077)  loss_n_100: 4.8086 (4.7221)  triple_100: 30.8166 (30.3080)  triple_80: 32.6935 (31.8617)  triple_60: 30.7949 (30.3639)  triple_40: 21.4233 (21.0110)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [730/845]  eta: 0:01:52  loss: 130.1431 (132.0438)  loss_n_40: 4.4526 (4.4818)  loss_n_60: 4.6120 (4.5870)  loss_n_80: 4.7455 (4.7080)  loss_n_100: 4.7254 (4.7223)  triple_100: 29.9261 (30.3079)  triple_80: 32.0650 (31.8644)  triple_60: 30.2185 (30.3658)  triple_40: 19.1380 (21.0066)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [740/845]  eta: 0:01:42  loss: 129.0318 (132.0671)  loss_n_40: 4.4518 (4.4823)  loss_n_60: 4.6167 (4.5875)  loss_n_80: 4.7455 (4.7087)  loss_n_100: 4.7254 (4.7230)  triple_100: 29.6005 (30.3151)  triple_80: 31.4487 (31.8722)  triple_60: 29.9308 (30.3729)  triple_40: 19.0679 (21.0055)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [750/845]  eta: 0:01:32  loss: 128.4516 (131.9813)  loss_n_40: 4.4596 (4.4809)  loss_n_60: 4.5767 (4.5857)  loss_n_80: 4.7230 (4.7067)  loss_n_100: 4.7279 (4.7211)  triple_100: 29.6005 (30.2928)  triple_80: 31.1374 (31.8464)  triple_60: 29.4308 (30.3484)  triple_40: 21.1008 (20.9994)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [760/845]  eta: 0:01:23  loss: 126.8855 (131.7369)  loss_n_40: 4.3773 (4.4775)  loss_n_60: 4.4277 (4.5814)  loss_n_80: 4.5595 (4.7021)  loss_n_100: 4.5823 (4.7165)  triple_100: 29.2178 (30.2298)  triple_80: 30.6387 (31.7808)  triple_60: 29.1548 (30.2870)  triple_40: 20.0588 (20.9619)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [770/845]  eta: 0:01:13  loss: 124.5016 (131.4220)  loss_n_40: 4.3231 (4.4735)  loss_n_60: 4.3872 (4.5762)  loss_n_80: 4.5266 (4.6967)  loss_n_100: 4.5477 (4.7110)  triple_100: 28.4630 (30.1508)  triple_80: 29.8201 (31.6973)  triple_60: 28.1283 (30.2068)  triple_40: 18.7519 (20.9098)  time: 0.9775  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:1]  [780/845]  eta: 0:01:03  loss: 126.6850 (131.4214)  loss_n_40: 4.4128 (4.4729)  loss_n_60: 4.5493 (4.5760)  loss_n_80: 4.6946 (4.6964)  loss_n_100: 4.6960 (4.7110)  triple_100: 29.0719 (30.1519)  triple_80: 31.0599 (31.6971)  triple_60: 29.3029 (30.2072)  triple_40: 18.3300 (20.9090)  time: 0.9776  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [790/845]  eta: 0:00:53  loss: 129.6912 (131.5104)  loss_n_40: 4.4852 (4.4739)  loss_n_60: 4.6004 (4.5773)  loss_n_80: 4.7822 (4.6980)  loss_n_100: 4.7530 (4.7125)  triple_100: 29.8437 (30.1756)  triple_80: 31.8460 (31.7195)  triple_60: 30.0573 (30.2282)  triple_40: 20.9141 (20.9253)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [800/845]  eta: 0:00:43  loss: 128.1977 (131.4625)  loss_n_40: 4.4079 (4.4725)  loss_n_60: 4.5347 (4.5764)  loss_n_80: 4.6799 (4.6973)  loss_n_100: 4.6776 (4.7118)  triple_100: 29.5021 (30.1655)  triple_80: 31.2475 (31.7100)  triple_60: 29.7213 (30.2166)  triple_40: 20.4581 (20.9123)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [810/845]  eta: 0:00:34  loss: 128.1977 (131.4937)  loss_n_40: 4.4079 (4.4728)  loss_n_60: 4.5347 (4.5769)  loss_n_80: 4.6787 (4.6979)  loss_n_100: 4.6775 (4.7124)  triple_100: 29.3330 (30.1744)  triple_80: 31.2475 (31.7192)  triple_60: 29.7213 (30.2250)  triple_40: 20.3734 (20.9151)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [820/845]  eta: 0:00:24  loss: 128.6637 (131.3921)  loss_n_40: 4.4064 (4.4706)  loss_n_60: 4.5600 (4.5749)  loss_n_80: 4.6834 (4.6959)  loss_n_100: 4.6849 (4.7104)  triple_100: 29.1138 (30.1501)  triple_80: 31.1889 (31.6937)  triple_60: 29.7607 (30.2016)  triple_40: 20.0987 (20.8949)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [830/845]  eta: 0:00:14  loss: 128.6637 (131.4679)  loss_n_40: 4.4515 (4.4720)  loss_n_60: 4.5842 (4.5767)  loss_n_80: 4.6989 (4.6978)  loss_n_100: 4.6892 (4.7121)  triple_100: 29.4469 (30.1681)  triple_80: 31.4548 (31.7166)  triple_60: 29.8862 (30.2240)  triple_40: 19.1159 (20.9005)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [840/845]  eta: 0:00:04  loss: 128.1322 (131.3458)  loss_n_40: 4.4735 (4.4697)  loss_n_60: 4.6122 (4.5744)  loss_n_80: 4.7459 (4.6955)  loss_n_100: 4.7553 (4.7100)  triple_100: 29.5997 (30.1405)  triple_80: 31.4548 (31.6863)  triple_60: 29.8862 (30.1933)  triple_40: 19.6229 (20.8761)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1]  [844/845]  eta: 0:00:00  loss: 128.0752 (131.2680)  loss_n_40: 4.4732 (4.4683)  loss_n_60: 4.5880 (4.5728)  loss_n_80: 4.7360 (4.6939)  loss_n_100: 4.7205 (4.7085)  triple_100: 29.5037 (30.1220)  triple_80: 31.3538 (31.6653)  triple_60: 29.5993 (30.1728)  triple_40: 19.6229 (20.8644)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:1] Total time: 0:13:46 (0.9778 s / it)\n",
      "Averaged stats: loss: 128.0752 (131.2680)  loss_n_40: 4.4732 (4.4683)  loss_n_60: 4.5880 (4.5728)  loss_n_80: 4.7360 (4.6939)  loss_n_100: 4.7205 (4.7085)  triple_100: 29.5037 (30.1220)  triple_80: 31.3538 (31.6653)  triple_60: 29.5993 (30.1728)  triple_40: 19.6229 (20.8644)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle_2/low2high/epoch_1_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 4.708%\n",
      "Min loss_n_100: 4.708\n",
      "Best Epoch: 0.000\n",
      "Train: [epoch:2]  [   0/1724]  eta: 3:00:36  lr: 0.000020  loss: 110.1208 (110.1208)  loss_n_40: 4.1042 (4.1042)  loss_n_60: 4.1797 (4.1797)  loss_n_80: 4.2850 (4.2850)  loss_n_100: 4.3189 (4.3189)  triple_100: 24.9626 (24.9626)  triple_80: 26.1759 (26.1759)  triple_60: 24.9330 (24.9330)  triple_40: 17.1616 (17.1616)  time: 6.2857  data: 0.5484  max mem: 40153\n",
      "Train: [epoch:2]  [  10/1724]  eta: 2:49:35  lr: 0.000020  loss: 125.0816 (127.2536)  loss_n_40: 4.1826 (4.2423)  loss_n_60: 4.3281 (4.4309)  loss_n_80: 4.4691 (4.5347)  loss_n_100: 4.4898 (4.5314)  triple_100: 28.8436 (29.4147)  triple_80: 29.9061 (30.5652)  triple_60: 28.8157 (29.4078)  triple_40: 20.1676 (20.1267)  time: 5.9367  data: 0.0500  max mem: 40153\n",
      "Train: [epoch:2]  [  20/1724]  eta: 2:48:08  lr: 0.000020  loss: 122.6476 (122.6414)  loss_n_40: 4.1438 (4.1154)  loss_n_60: 4.3126 (4.3252)  loss_n_80: 4.4099 (4.4167)  loss_n_100: 4.3911 (4.3975)  triple_100: 28.1199 (28.3132)  triple_80: 29.4751 (29.4312)  triple_60: 28.1402 (28.4906)  triple_40: 18.9570 (19.1517)  time: 5.9022  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [  30/1724]  eta: 2:47:00  lr: 0.000020  loss: 119.3666 (121.4091)  loss_n_40: 3.9532 (4.0620)  loss_n_60: 4.2495 (4.2992)  loss_n_80: 4.2921 (4.3726)  loss_n_100: 4.3002 (4.3596)  triple_100: 27.9681 (28.1240)  triple_80: 28.8743 (29.1660)  triple_60: 27.8665 (28.3372)  triple_40: 17.6037 (18.6887)  time: 5.9037  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [  40/1724]  eta: 2:45:56  lr: 0.000020  loss: 118.2752 (120.5999)  loss_n_40: 3.9186 (4.0226)  loss_n_60: 4.2247 (4.2740)  loss_n_80: 4.2833 (4.3390)  loss_n_100: 4.2943 (4.3431)  triple_100: 28.2053 (28.1047)  triple_80: 29.1000 (29.0401)  triple_60: 27.7589 (28.1912)  triple_40: 17.4789 (18.2851)  time: 5.9042  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:2]  [  50/1724]  eta: 2:44:54  lr: 0.000020  loss: 122.4644 (121.8548)  loss_n_40: 4.0099 (4.0379)  loss_n_60: 4.2807 (4.2967)  loss_n_80: 4.3766 (4.3679)  loss_n_100: 4.4215 (4.3791)  triple_100: 28.9610 (28.5195)  triple_80: 30.1718 (29.4864)  triple_60: 29.3436 (28.5731)  triple_40: 17.7918 (18.1942)  time: 5.9038  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [  60/1724]  eta: 2:43:53  lr: 0.000020  loss: 124.8589 (121.2907)  loss_n_40: 4.0803 (4.0419)  loss_n_60: 4.3314 (4.2874)  loss_n_80: 4.4356 (4.3615)  loss_n_100: 4.4611 (4.3707)  triple_100: 29.6623 (28.3447)  triple_80: 30.5003 (29.4103)  triple_60: 29.4825 (28.4693)  triple_40: 17.7918 (18.0049)  time: 5.9033  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [  70/1724]  eta: 2:42:52  lr: 0.000020  loss: 119.8280 (121.1558)  loss_n_40: 4.1637 (4.0595)  loss_n_60: 4.2698 (4.2856)  loss_n_80: 4.3774 (4.3681)  loss_n_100: 4.3165 (4.3708)  triple_100: 27.4188 (28.2212)  triple_80: 29.7934 (29.5474)  triple_60: 28.3177 (28.3879)  triple_40: 17.2924 (17.9153)  time: 5.9029  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [  80/1724]  eta: 2:41:52  lr: 0.000020  loss: 118.1164 (120.8742)  loss_n_40: 4.2411 (4.0843)  loss_n_60: 4.3136 (4.2865)  loss_n_80: 4.4504 (4.3793)  loss_n_100: 4.4129 (4.3781)  triple_100: 27.0541 (28.0851)  triple_80: 29.9393 (29.6253)  triple_60: 27.2072 (28.1863)  triple_40: 17.2057 (17.8493)  time: 5.9031  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [  90/1724]  eta: 2:40:52  lr: 0.000020  loss: 119.0330 (121.1916)  loss_n_40: 4.2523 (4.0984)  loss_n_60: 4.2133 (4.2840)  loss_n_80: 4.4059 (4.3906)  loss_n_100: 4.4211 (4.3912)  triple_100: 27.2722 (28.1299)  triple_80: 30.5047 (29.8493)  triple_60: 26.4944 (28.1342)  triple_40: 17.8467 (17.9140)  time: 5.9021  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 100/1724]  eta: 2:39:52  lr: 0.000020  loss: 128.1447 (121.9937)  loss_n_40: 4.2188 (4.1058)  loss_n_60: 4.2433 (4.2841)  loss_n_80: 4.4587 (4.4026)  loss_n_100: 4.5371 (4.4086)  triple_100: 29.8679 (28.3091)  triple_80: 32.6913 (30.1714)  triple_60: 28.9509 (28.2457)  triple_40: 19.1689 (18.0665)  time: 5.9023  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 110/1724]  eta: 2:38:52  lr: 0.000020  loss: 128.2948 (122.2308)  loss_n_40: 4.1145 (4.1006)  loss_n_60: 4.2433 (4.2773)  loss_n_80: 4.4587 (4.4089)  loss_n_100: 4.5371 (4.4221)  triple_100: 30.0528 (28.3801)  triple_80: 33.3459 (30.3495)  triple_60: 28.9509 (28.1905)  triple_40: 19.1689 (18.1017)  time: 5.9024  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 120/1724]  eta: 2:37:53  lr: 0.000020  loss: 124.0141 (122.4749)  loss_n_40: 4.0089 (4.0940)  loss_n_60: 4.1165 (4.2641)  loss_n_80: 4.3859 (4.4062)  loss_n_100: 4.4585 (4.4279)  triple_100: 29.0545 (28.4552)  triple_80: 32.0273 (30.4876)  triple_60: 27.5378 (28.1571)  triple_40: 18.2076 (18.1829)  time: 5.9017  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 130/1724]  eta: 2:36:53  lr: 0.000020  loss: 126.7276 (122.7483)  loss_n_40: 3.9723 (4.0879)  loss_n_60: 4.0885 (4.2502)  loss_n_80: 4.3922 (4.4047)  loss_n_100: 4.4911 (4.4373)  triple_100: 29.5659 (28.5676)  triple_80: 32.3028 (30.6431)  triple_60: 27.7233 (28.1225)  triple_40: 19.0822 (18.2350)  time: 5.9013  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 140/1724]  eta: 2:35:53  lr: 0.000020  loss: 126.9142 (122.7057)  loss_n_40: 3.9460 (4.0792)  loss_n_60: 4.0372 (4.2351)  loss_n_80: 4.3372 (4.3993)  loss_n_100: 4.5643 (4.4451)  triple_100: 29.5659 (28.6217)  triple_80: 32.6767 (30.6985)  triple_60: 27.7000 (28.0232)  triple_40: 18.5844 (18.2036)  time: 5.9008  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 150/1724]  eta: 2:34:54  lr: 0.000020  loss: 123.5130 (122.9105)  loss_n_40: 3.8704 (4.0668)  loss_n_60: 3.9810 (4.2212)  loss_n_80: 4.2538 (4.3927)  loss_n_100: 4.5210 (4.4519)  triple_100: 29.5421 (28.7442)  triple_80: 31.6430 (30.8065)  triple_60: 27.0622 (28.0123)  triple_40: 18.4719 (18.2150)  time: 5.9002  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 160/1724]  eta: 2:33:54  lr: 0.000020  loss: 124.0062 (122.8391)  loss_n_40: 3.8879 (4.0600)  loss_n_60: 3.9810 (4.2118)  loss_n_80: 4.2979 (4.3908)  loss_n_100: 4.5688 (4.4636)  triple_100: 30.0611 (28.8061)  triple_80: 31.6430 (30.8445)  triple_60: 27.0622 (27.9171)  triple_40: 17.9157 (18.1451)  time: 5.8996  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 170/1724]  eta: 2:32:55  lr: 0.000020  loss: 121.6730 (122.7946)  loss_n_40: 3.9060 (4.0521)  loss_n_60: 4.0375 (4.2009)  loss_n_80: 4.3076 (4.3853)  loss_n_100: 4.5695 (4.4694)  triple_100: 29.5942 (28.8652)  triple_80: 31.0564 (30.8760)  triple_60: 26.5800 (27.8382)  triple_40: 17.3453 (18.1074)  time: 5.8993  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 180/1724]  eta: 2:31:55  lr: 0.000020  loss: 122.6090 (122.6959)  loss_n_40: 3.9056 (4.0423)  loss_n_60: 3.9768 (4.1894)  loss_n_80: 4.2668 (4.3801)  loss_n_100: 4.5566 (4.4749)  triple_100: 30.0476 (28.9103)  triple_80: 31.9065 (30.8949)  triple_60: 26.5800 (27.7470)  triple_40: 17.3453 (18.0571)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 190/1724]  eta: 2:30:56  lr: 0.000020  loss: 124.2827 (122.8553)  loss_n_40: 3.8689 (4.0331)  loss_n_60: 3.9926 (4.1794)  loss_n_80: 4.2845 (4.3747)  loss_n_100: 4.5485 (4.4780)  triple_100: 30.8502 (29.0195)  triple_80: 32.0821 (30.9841)  triple_60: 27.2288 (27.7374)  triple_40: 17.3572 (18.0491)  time: 5.8997  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 200/1724]  eta: 2:29:56  lr: 0.000020  loss: 124.8837 (122.9073)  loss_n_40: 3.8485 (4.0220)  loss_n_60: 3.9824 (4.1678)  loss_n_80: 4.2445 (4.3671)  loss_n_100: 4.4752 (4.4786)  triple_100: 31.0525 (29.0955)  triple_80: 32.1343 (31.0371)  triple_60: 27.2589 (27.7063)  triple_40: 18.0386 (18.0329)  time: 5.8995  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 210/1724]  eta: 2:28:57  lr: 0.000020  loss: 126.3763 (123.1325)  loss_n_40: 3.8485 (4.0134)  loss_n_60: 3.9594 (4.1575)  loss_n_80: 4.2049 (4.3600)  loss_n_100: 4.4752 (4.4778)  triple_100: 31.2613 (29.2116)  triple_80: 32.4658 (31.1334)  triple_60: 27.5239 (27.7266)  triple_40: 17.7003 (18.0521)  time: 5.8986  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 220/1724]  eta: 2:27:58  lr: 0.000020  loss: 126.3763 (123.3472)  loss_n_40: 3.8676 (4.0049)  loss_n_60: 4.0108 (4.1489)  loss_n_80: 4.2806 (4.3548)  loss_n_100: 4.5304 (4.4784)  triple_100: 31.2613 (29.3249)  triple_80: 32.6918 (31.2267)  triple_60: 27.5351 (27.7470)  triple_40: 17.7003 (18.0617)  time: 5.8993  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 230/1724]  eta: 2:26:58  lr: 0.000020  loss: 122.9337 (123.1022)  loss_n_40: 3.7934 (3.9919)  loss_n_60: 3.9299 (4.1350)  loss_n_80: 4.1825 (4.3447)  loss_n_100: 4.3882 (4.4744)  triple_100: 30.2959 (29.3260)  triple_80: 32.0602 (31.1924)  triple_60: 27.1315 (27.6440)  triple_40: 17.2614 (17.9937)  time: 5.8992  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 240/1724]  eta: 2:25:59  lr: 0.000020  loss: 122.8190 (123.3736)  loss_n_40: 3.7934 (3.9866)  loss_n_60: 3.9299 (4.1311)  loss_n_80: 4.2280 (4.3433)  loss_n_100: 4.4715 (4.4777)  triple_100: 30.2959 (29.4574)  triple_80: 31.5461 (31.2939)  triple_60: 26.8748 (27.6885)  triple_40: 16.8389 (17.9951)  time: 5.8990  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 250/1724]  eta: 2:25:00  lr: 0.000020  loss: 126.4498 (123.5149)  loss_n_40: 3.8378 (3.9800)  loss_n_60: 4.0084 (4.1249)  loss_n_80: 4.2728 (4.3405)  loss_n_100: 4.5689 (4.4786)  triple_100: 31.8154 (29.5489)  triple_80: 33.0148 (31.3669)  triple_60: 27.7287 (27.6924)  triple_40: 17.0559 (17.9827)  time: 5.8982  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 260/1724]  eta: 2:24:01  lr: 0.000020  loss: 130.7506 (124.0081)  loss_n_40: 3.7902 (3.9749)  loss_n_60: 3.9137 (4.1201)  loss_n_80: 4.2192 (4.3372)  loss_n_100: 4.4498 (4.4773)  triple_100: 33.0164 (29.7154)  triple_80: 34.2296 (31.5223)  triple_60: 29.3542 (27.8054)  triple_40: 18.6700 (18.0555)  time: 5.8974  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 270/1724]  eta: 2:23:01  lr: 0.000020  loss: 135.3561 (124.2225)  loss_n_40: 3.8391 (3.9695)  loss_n_60: 4.0014 (4.1159)  loss_n_80: 4.2796 (4.3354)  loss_n_100: 4.4822 (4.4768)  triple_100: 33.3616 (29.8043)  triple_80: 35.1354 (31.6049)  triple_60: 30.7369 (27.8468)  triple_40: 19.3720 (18.0688)  time: 5.8979  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 280/1724]  eta: 2:22:02  lr: 0.000020  loss: 131.1660 (124.3752)  loss_n_40: 3.7535 (3.9616)  loss_n_60: 3.9469 (4.1069)  loss_n_80: 4.2103 (4.3275)  loss_n_100: 4.3804 (4.4709)  triple_100: 32.1272 (29.8755)  triple_80: 33.8850 (31.6613)  triple_60: 29.0290 (27.8745)  triple_40: 19.1546 (18.0968)  time: 5.8988  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 290/1724]  eta: 2:21:03  lr: 0.000020  loss: 131.1660 (124.6953)  loss_n_40: 3.7540 (3.9581)  loss_n_60: 3.9469 (4.1032)  loss_n_80: 4.1856 (4.3244)  loss_n_100: 4.3695 (4.4691)  triple_100: 32.1272 (29.9812)  triple_80: 33.8850 (31.7627)  triple_60: 29.0290 (27.9508)  triple_40: 19.3699 (18.1457)  time: 5.8985  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [ 300/1724]  eta: 2:20:04  lr: 0.000020  loss: 137.1348 (125.0680)  loss_n_40: 3.8224 (3.9538)  loss_n_60: 3.9688 (4.0991)  loss_n_80: 4.2144 (4.3216)  loss_n_100: 4.3861 (4.4670)  triple_100: 34.0090 (30.1046)  triple_80: 35.6753 (31.8805)  triple_60: 30.9925 (28.0354)  triple_40: 20.2028 (18.2059)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 310/1724]  eta: 2:19:05  lr: 0.000020  loss: 135.0521 (125.1296)  loss_n_40: 3.7857 (3.9472)  loss_n_60: 3.9000 (4.0920)  loss_n_80: 4.2059 (4.3162)  loss_n_100: 4.3632 (4.4622)  triple_100: 32.8247 (30.1431)  triple_80: 34.7927 (31.9141)  triple_60: 30.6379 (28.0383)  triple_40: 19.8736 (18.2164)  time: 5.8988  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 320/1724]  eta: 2:18:06  lr: 0.000020  loss: 128.7724 (125.5295)  loss_n_40: 3.7458 (3.9441)  loss_n_60: 3.8898 (4.0882)  loss_n_80: 4.1302 (4.3132)  loss_n_100: 4.2986 (4.4589)  triple_100: 32.1439 (30.2623)  triple_80: 34.0530 (32.0288)  triple_60: 28.6503 (28.1287)  triple_40: 19.8736 (18.3052)  time: 5.9006  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 330/1724]  eta: 2:17:06  lr: 0.000020  loss: 133.4502 (125.9566)  loss_n_40: 3.8398 (3.9422)  loss_n_60: 3.9931 (4.0857)  loss_n_80: 4.1945 (4.3118)  loss_n_100: 4.3444 (4.4577)  triple_100: 32.8631 (30.3821)  triple_80: 34.8705 (32.1498)  triple_60: 30.3917 (28.2286)  triple_40: 21.0815 (18.3988)  time: 5.9009  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 340/1724]  eta: 2:16:07  lr: 0.000020  loss: 140.9720 (126.3353)  loss_n_40: 3.8253 (3.9377)  loss_n_60: 3.9287 (4.0787)  loss_n_80: 4.1783 (4.3053)  loss_n_100: 4.3222 (4.4512)  triple_100: 34.2174 (30.4856)  triple_80: 36.2014 (32.2531)  triple_60: 32.2644 (28.3179)  triple_40: 22.1757 (18.5059)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 350/1724]  eta: 2:15:08  lr: 0.000020  loss: 136.8689 (126.6816)  loss_n_40: 3.7712 (3.9332)  loss_n_60: 3.8471 (4.0713)  loss_n_80: 4.0660 (4.2981)  loss_n_100: 4.2158 (4.4439)  triple_100: 33.7224 (30.5810)  triple_80: 35.2275 (32.3433)  triple_60: 30.9615 (28.3960)  triple_40: 22.4214 (18.6147)  time: 5.9021  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 360/1724]  eta: 2:14:09  lr: 0.000020  loss: 134.9945 (126.8303)  loss_n_40: 3.7698 (3.9285)  loss_n_60: 3.8041 (4.0643)  loss_n_80: 4.0603 (4.2919)  loss_n_100: 4.1675 (4.4376)  triple_100: 32.7655 (30.6258)  triple_80: 34.5186 (32.3864)  triple_60: 30.1234 (28.4214)  triple_40: 21.5649 (18.6744)  time: 5.9023  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 370/1724]  eta: 2:13:10  lr: 0.000020  loss: 134.4051 (127.1671)  loss_n_40: 3.7667 (3.9244)  loss_n_60: 3.8120 (4.0582)  loss_n_80: 4.0624 (4.2872)  loss_n_100: 4.1494 (4.4327)  triple_100: 32.4681 (30.7230)  triple_80: 34.5186 (32.4777)  triple_60: 29.7078 (28.4882)  triple_40: 21.5006 (18.7757)  time: 5.9009  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 380/1724]  eta: 2:12:11  lr: 0.000020  loss: 142.6989 (127.6752)  loss_n_40: 3.8144 (3.9225)  loss_n_60: 3.8664 (4.0535)  loss_n_80: 4.0972 (4.2829)  loss_n_100: 4.1969 (4.4273)  triple_100: 34.4777 (30.8531)  triple_80: 36.6669 (32.6081)  triple_60: 32.3648 (28.6049)  triple_40: 23.3260 (18.9228)  time: 5.9002  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 390/1724]  eta: 2:11:12  lr: 0.000020  loss: 138.1962 (127.8948)  loss_n_40: 3.7967 (3.9182)  loss_n_60: 3.8274 (4.0458)  loss_n_80: 4.0607 (4.2753)  loss_n_100: 4.1741 (4.4186)  triple_100: 33.8175 (30.9102)  triple_80: 35.5028 (32.6646)  triple_60: 30.5009 (28.6455)  triple_40: 23.2078 (19.0167)  time: 5.9000  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 400/1724]  eta: 2:10:13  lr: 0.000020  loss: 134.9167 (128.0684)  loss_n_40: 3.7474 (3.9139)  loss_n_60: 3.7000 (4.0386)  loss_n_80: 3.9505 (4.2689)  loss_n_100: 4.0636 (4.4118)  triple_100: 32.8760 (30.9606)  triple_80: 34.2982 (32.7102)  triple_60: 29.3370 (28.6678)  triple_40: 23.0121 (19.0966)  time: 5.9010  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 410/1724]  eta: 2:09:14  lr: 0.000020  loss: 128.5085 (128.0323)  loss_n_40: 3.7411 (3.9074)  loss_n_60: 3.7018 (4.0308)  loss_n_80: 3.9802 (4.2632)  loss_n_100: 4.1102 (4.4055)  triple_100: 31.4946 (30.9651)  triple_80: 33.2369 (32.7113)  triple_60: 28.3683 (28.6290)  triple_40: 20.9552 (19.1199)  time: 5.9018  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 420/1724]  eta: 2:08:15  lr: 0.000020  loss: 135.5392 (128.2537)  loss_n_40: 3.7405 (3.9036)  loss_n_60: 3.7152 (4.0245)  loss_n_80: 3.9966 (4.2570)  loss_n_100: 4.1133 (4.3983)  triple_100: 32.8825 (31.0232)  triple_80: 34.5569 (32.7695)  triple_60: 29.4404 (28.6726)  triple_40: 22.0673 (19.2049)  time: 5.9009  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 430/1724]  eta: 2:07:16  lr: 0.000020  loss: 138.1700 (128.3623)  loss_n_40: 3.7405 (3.9011)  loss_n_60: 3.8165 (4.0192)  loss_n_80: 3.9966 (4.2524)  loss_n_100: 4.0892 (4.3932)  triple_100: 33.6816 (31.0538)  triple_80: 35.7034 (32.8012)  triple_60: 30.8022 (28.6825)  triple_40: 22.6415 (19.2589)  time: 5.9010  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 440/1724]  eta: 2:06:17  lr: 0.000020  loss: 126.9163 (128.3698)  loss_n_40: 3.7194 (3.8963)  loss_n_60: 3.6747 (4.0116)  loss_n_80: 3.9464 (4.2455)  loss_n_100: 4.0467 (4.3855)  triple_100: 30.5066 (31.0576)  triple_80: 32.6835 (32.8088)  triple_60: 27.8572 (28.6661)  triple_40: 20.7759 (19.2984)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 450/1724]  eta: 2:05:18  lr: 0.000020  loss: 126.9163 (128.4228)  loss_n_40: 3.6550 (3.8910)  loss_n_60: 3.6457 (4.0030)  loss_n_80: 3.9076 (4.2377)  loss_n_100: 4.0034 (4.3771)  triple_100: 30.6749 (31.0762)  triple_80: 32.6835 (32.8234)  triple_60: 27.6818 (28.6572)  triple_40: 20.7759 (19.3573)  time: 5.8991  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 460/1724]  eta: 2:04:19  lr: 0.000020  loss: 131.1425 (128.5946)  loss_n_40: 3.6612 (3.8872)  loss_n_60: 3.6116 (3.9957)  loss_n_80: 3.8938 (4.2312)  loss_n_100: 4.0034 (4.3699)  triple_100: 31.8721 (31.1212)  triple_80: 33.3732 (32.8684)  triple_60: 28.7582 (28.6787)  triple_40: 23.2903 (19.4424)  time: 5.8963  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 470/1724]  eta: 2:03:20  lr: 0.000020  loss: 133.6401 (128.6283)  loss_n_40: 3.6777 (3.8821)  loss_n_60: 3.6435 (3.9892)  loss_n_80: 3.8811 (4.2246)  loss_n_100: 3.9786 (4.3618)  triple_100: 32.7853 (31.1354)  triple_80: 34.4856 (32.8836)  triple_60: 29.5920 (28.6797)  triple_40: 22.3619 (19.4719)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 480/1724]  eta: 2:02:20  lr: 0.000020  loss: 131.5905 (128.7128)  loss_n_40: 3.6704 (3.8782)  loss_n_60: 3.6435 (3.9828)  loss_n_80: 3.8383 (4.2180)  loss_n_100: 3.9513 (4.3541)  triple_100: 31.9678 (31.1586)  triple_80: 33.8613 (32.9093)  triple_60: 29.0394 (28.6896)  triple_40: 22.1036 (19.5223)  time: 5.8950  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 490/1724]  eta: 2:01:21  lr: 0.000020  loss: 132.6177 (128.7861)  loss_n_40: 3.6863 (3.8742)  loss_n_60: 3.6897 (3.9763)  loss_n_80: 3.8986 (4.2115)  loss_n_100: 3.9788 (4.3470)  triple_100: 31.9678 (31.1817)  triple_80: 34.0085 (32.9318)  triple_60: 29.0559 (28.6966)  triple_40: 21.9773 (19.5669)  time: 5.8957  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 500/1724]  eta: 2:00:22  lr: 0.000020  loss: 132.6177 (128.8479)  loss_n_40: 3.7030 (3.8701)  loss_n_60: 3.6897 (3.9701)  loss_n_80: 3.8986 (4.2052)  loss_n_100: 3.9980 (4.3397)  triple_100: 31.9622 (31.1977)  triple_80: 34.0085 (32.9514)  triple_60: 29.3456 (28.7043)  triple_40: 20.9725 (19.6094)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 510/1724]  eta: 1:59:23  lr: 0.000020  loss: 130.2649 (128.8572)  loss_n_40: 3.6053 (3.8645)  loss_n_60: 3.5425 (3.9623)  loss_n_80: 3.8254 (4.1981)  loss_n_100: 3.9604 (4.3319)  triple_100: 31.3334 (31.2074)  triple_80: 33.6753 (32.9593)  triple_60: 28.9148 (28.6905)  triple_40: 20.9725 (19.6433)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 520/1724]  eta: 1:58:24  lr: 0.000020  loss: 131.6209 (128.8945)  loss_n_40: 3.5646 (3.8592)  loss_n_60: 3.5215 (3.9543)  loss_n_80: 3.7947 (4.1903)  loss_n_100: 3.9005 (4.3234)  triple_100: 32.0803 (31.2227)  triple_80: 33.8254 (32.9715)  triple_60: 28.7665 (28.6864)  triple_40: 20.6643 (19.6867)  time: 5.8975  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [ 530/1724]  eta: 1:57:25  lr: 0.000020  loss: 132.3674 (129.0057)  loss_n_40: 3.5741 (3.8546)  loss_n_60: 3.5562 (3.9472)  loss_n_80: 3.8053 (4.1835)  loss_n_100: 3.9316 (4.3158)  triple_100: 32.6090 (31.2553)  triple_80: 33.4952 (33.0024)  triple_60: 28.9257 (28.7013)  triple_40: 21.3924 (19.7456)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 540/1724]  eta: 1:56:26  lr: 0.000020  loss: 132.3674 (129.0763)  loss_n_40: 3.6133 (3.8503)  loss_n_60: 3.5930 (3.9406)  loss_n_80: 3.8021 (4.1766)  loss_n_100: 3.8911 (4.3079)  triple_100: 32.5102 (31.2738)  triple_80: 33.7317 (33.0222)  triple_60: 28.9257 (28.7107)  triple_40: 21.9188 (19.7942)  time: 5.8973  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 550/1724]  eta: 1:55:27  lr: 0.000020  loss: 130.5916 (129.0946)  loss_n_40: 3.5464 (3.8454)  loss_n_60: 3.5395 (3.9330)  loss_n_80: 3.7811 (4.1683)  loss_n_100: 3.8505 (4.2986)  triple_100: 31.4824 (31.2803)  triple_80: 33.7013 (33.0279)  triple_60: 28.3622 (28.7112)  triple_40: 22.4495 (19.8300)  time: 5.8985  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 560/1724]  eta: 1:54:28  lr: 0.000020  loss: 127.2046 (129.0964)  loss_n_40: 3.4795 (3.8396)  loss_n_60: 3.4604 (3.9248)  loss_n_80: 3.6285 (4.1598)  loss_n_100: 3.6807 (4.2890)  triple_100: 30.8426 (31.2830)  triple_80: 32.4376 (33.0318)  triple_60: 27.8979 (28.7059)  triple_40: 21.7867 (19.8625)  time: 5.8992  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 570/1724]  eta: 1:53:29  lr: 0.000020  loss: 130.9044 (129.2359)  loss_n_40: 3.5231 (3.8364)  loss_n_60: 3.4682 (3.9203)  loss_n_80: 3.7599 (4.1552)  loss_n_100: 3.8274 (4.2836)  triple_100: 31.8589 (31.3243)  triple_80: 33.4881 (33.0733)  triple_60: 28.3952 (28.7349)  triple_40: 22.1355 (19.9079)  time: 5.9001  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 580/1724]  eta: 1:52:30  lr: 0.000020  loss: 130.9264 (129.2362)  loss_n_40: 3.5231 (3.8306)  loss_n_60: 3.4682 (3.9112)  loss_n_80: 3.7583 (4.1456)  loss_n_100: 3.8435 (4.2732)  triple_100: 31.9568 (31.3274)  triple_80: 33.4881 (33.0726)  triple_60: 28.3952 (28.7285)  triple_40: 22.1355 (19.9470)  time: 5.8999  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 590/1724]  eta: 1:51:31  lr: 0.000020  loss: 126.9541 (129.1567)  loss_n_40: 3.4906 (3.8251)  loss_n_60: 3.4107 (3.9043)  loss_n_80: 3.6138 (4.1386)  loss_n_100: 3.7427 (4.2656)  triple_100: 31.2585 (31.3153)  triple_80: 33.0653 (33.0591)  triple_60: 28.1917 (28.7038)  triple_40: 20.7694 (19.9447)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 600/1724]  eta: 1:50:32  lr: 0.000020  loss: 125.3678 (129.0971)  loss_n_40: 3.5081 (3.8205)  loss_n_60: 3.4118 (3.8971)  loss_n_80: 3.6446 (4.1309)  loss_n_100: 3.7427 (4.2570)  triple_100: 30.8051 (31.3034)  triple_80: 32.6427 (33.0473)  triple_60: 26.9048 (28.6856)  triple_40: 19.9455 (19.9553)  time: 5.8998  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 610/1724]  eta: 1:49:33  lr: 0.000020  loss: 126.7675 (129.0918)  loss_n_40: 3.5567 (3.8154)  loss_n_60: 3.3736 (3.8892)  loss_n_80: 3.5781 (4.1226)  loss_n_100: 3.6811 (4.2480)  triple_100: 31.2167 (31.3072)  triple_80: 32.6427 (33.0454)  triple_60: 27.1867 (28.6791)  triple_40: 21.8856 (19.9848)  time: 5.9006  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 620/1724]  eta: 1:48:34  lr: 0.000020  loss: 125.3295 (129.0114)  loss_n_40: 3.4135 (3.8089)  loss_n_60: 3.3018 (3.8802)  loss_n_80: 3.5416 (4.1134)  loss_n_100: 3.6312 (4.2383)  triple_100: 30.4031 (31.2959)  triple_80: 32.3844 (33.0280)  triple_60: 28.0847 (28.6529)  triple_40: 21.3880 (19.9937)  time: 5.8998  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 630/1724]  eta: 1:47:35  lr: 0.000020  loss: 123.5024 (128.8614)  loss_n_40: 3.3764 (3.8021)  loss_n_60: 3.2464 (3.8701)  loss_n_80: 3.4232 (4.1031)  loss_n_100: 3.5242 (4.2274)  triple_100: 30.0981 (31.2623)  triple_80: 31.3447 (32.9910)  triple_60: 26.9284 (28.6105)  triple_40: 19.9423 (19.9950)  time: 5.9002  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 640/1724]  eta: 1:46:36  lr: 0.000020  loss: 119.0258 (128.6720)  loss_n_40: 3.3176 (3.7949)  loss_n_60: 3.2469 (3.8609)  loss_n_80: 3.4194 (4.0931)  loss_n_100: 3.5242 (4.2165)  triple_100: 29.2867 (31.2226)  triple_80: 30.7184 (32.9463)  triple_60: 25.6649 (28.5661)  triple_40: 19.0451 (19.9716)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 650/1724]  eta: 1:45:37  lr: 0.000020  loss: 119.4132 (128.5644)  loss_n_40: 3.3451 (3.7885)  loss_n_60: 3.2645 (3.8520)  loss_n_80: 3.4696 (4.0837)  loss_n_100: 3.5507 (4.2067)  triple_100: 29.4084 (31.2045)  triple_80: 30.7184 (32.9202)  triple_60: 25.7517 (28.5345)  triple_40: 19.6136 (19.9744)  time: 5.9018  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 660/1724]  eta: 1:44:38  lr: 0.000020  loss: 119.8317 (128.4167)  loss_n_40: 3.3349 (3.7817)  loss_n_60: 3.2043 (3.8411)  loss_n_80: 3.4046 (4.0724)  loss_n_100: 3.5389 (4.1951)  triple_100: 29.3541 (31.1719)  triple_80: 30.3261 (32.8793)  triple_60: 26.0165 (28.4921)  triple_40: 20.7195 (19.9830)  time: 5.9026  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 670/1724]  eta: 1:43:39  lr: 0.000020  loss: 115.4005 (128.2058)  loss_n_40: 3.3349 (3.7754)  loss_n_60: 3.1647 (3.8320)  loss_n_80: 3.4046 (4.0631)  loss_n_100: 3.5068 (4.1853)  triple_100: 28.4705 (31.1266)  triple_80: 29.7590 (32.8276)  triple_60: 24.6626 (28.4349)  triple_40: 19.1924 (19.9609)  time: 5.9016  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 680/1724]  eta: 1:42:40  lr: 0.000020  loss: 115.4005 (128.0235)  loss_n_40: 3.3339 (3.7690)  loss_n_60: 3.1645 (3.8223)  loss_n_80: 3.3936 (4.0529)  loss_n_100: 3.5068 (4.1744)  triple_100: 28.2909 (31.0855)  triple_80: 29.5956 (32.7826)  triple_60: 25.3732 (28.3887)  triple_40: 18.9136 (19.9482)  time: 5.9004  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 690/1724]  eta: 1:41:41  lr: 0.000020  loss: 113.3244 (127.7310)  loss_n_40: 3.2359 (3.7610)  loss_n_60: 3.1039 (3.8117)  loss_n_80: 3.3094 (4.0422)  loss_n_100: 3.4180 (4.1631)  triple_100: 27.6437 (31.0194)  triple_80: 28.9483 (32.7123)  triple_60: 24.3015 (28.3141)  triple_40: 18.9136 (19.9071)  time: 5.9001  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 700/1724]  eta: 1:40:42  lr: 0.000020  loss: 106.5369 (127.4416)  loss_n_40: 3.1476 (3.7533)  loss_n_60: 3.0166 (3.8009)  loss_n_80: 3.1987 (4.0303)  loss_n_100: 3.2547 (4.1504)  triple_100: 26.0031 (30.9481)  triple_80: 27.0585 (32.6363)  triple_60: 23.5252 (28.2481)  triple_40: 16.7817 (19.8741)  time: 5.9002  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 710/1724]  eta: 1:39:43  lr: 0.000020  loss: 106.5369 (127.1322)  loss_n_40: 3.1736 (3.7454)  loss_n_60: 2.9557 (3.7892)  loss_n_80: 3.1258 (4.0178)  loss_n_100: 3.2174 (4.1373)  triple_100: 26.1748 (30.8750)  triple_80: 27.2395 (32.5539)  triple_60: 23.5252 (28.1731)  triple_40: 17.4200 (19.8404)  time: 5.9002  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 720/1724]  eta: 1:38:44  lr: 0.000020  loss: 105.6281 (126.7900)  loss_n_40: 3.1447 (3.7370)  loss_n_60: 2.9008 (3.7775)  loss_n_80: 3.0284 (4.0049)  loss_n_100: 3.0944 (4.1236)  triple_100: 25.5767 (30.7911)  triple_80: 26.4033 (32.4639)  triple_60: 22.0680 (28.0933)  triple_40: 17.3996 (19.7987)  time: 5.8996  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 730/1724]  eta: 1:37:45  lr: 0.000020  loss: 101.6463 (126.4644)  loss_n_40: 3.1182 (3.7290)  loss_n_60: 2.9008 (3.7667)  loss_n_80: 3.0163 (3.9933)  loss_n_100: 3.0916 (4.1114)  triple_100: 24.6753 (30.7147)  triple_80: 25.7912 (32.3801)  triple_60: 22.0185 (28.0169)  triple_40: 16.5404 (19.7523)  time: 5.9000  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 740/1724]  eta: 1:36:46  lr: 0.000020  loss: 99.6830 (126.1156)  loss_n_40: 3.1649 (3.7220)  loss_n_60: 2.9365 (3.7561)  loss_n_80: 3.0491 (3.9813)  loss_n_100: 3.1253 (4.0985)  triple_100: 24.3904 (30.6287)  triple_80: 25.8281 (32.2903)  triple_60: 21.8405 (27.9393)  triple_40: 16.5784 (19.6993)  time: 5.9004  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 750/1724]  eta: 1:35:47  lr: 0.000020  loss: 100.9146 (125.8141)  loss_n_40: 3.2462 (3.7154)  loss_n_60: 2.9260 (3.7460)  loss_n_80: 3.0134 (3.9699)  loss_n_100: 3.0885 (4.0861)  triple_100: 24.3203 (30.5546)  triple_80: 25.8281 (32.2126)  triple_60: 22.4620 (27.8724)  triple_40: 16.0251 (19.6571)  time: 5.9003  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [ 760/1724]  eta: 1:34:48  lr: 0.000020  loss: 95.2420 (125.3564)  loss_n_40: 3.0907 (3.7059)  loss_n_60: 2.8510 (3.7327)  loss_n_80: 2.9928 (3.9556)  loss_n_100: 3.0318 (4.0714)  triple_100: 23.3265 (30.4475)  triple_80: 24.6019 (32.0933)  triple_60: 20.7165 (27.7633)  triple_40: 14.6425 (19.5867)  time: 5.9008  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 770/1724]  eta: 1:33:49  lr: 0.000020  loss: 90.9340 (124.9308)  loss_n_40: 2.9930 (3.6971)  loss_n_60: 2.7203 (3.7209)  loss_n_80: 2.9054 (3.9428)  loss_n_100: 2.9584 (4.0581)  triple_100: 22.3612 (30.3489)  triple_80: 23.2020 (31.9879)  triple_60: 19.8885 (27.6650)  triple_40: 13.5527 (19.5102)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 780/1724]  eta: 1:32:50  lr: 0.000020  loss: 92.7743 (124.5700)  loss_n_40: 3.0186 (3.6895)  loss_n_60: 2.8342 (3.7109)  loss_n_80: 2.9954 (3.9318)  loss_n_100: 3.0461 (4.0464)  triple_100: 22.8011 (30.2620)  triple_80: 23.6843 (31.8979)  triple_60: 20.1427 (27.5838)  triple_40: 13.8152 (19.4477)  time: 5.9005  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 790/1724]  eta: 1:31:51  lr: 0.000020  loss: 96.0395 (124.2031)  loss_n_40: 3.0056 (3.6817)  loss_n_60: 2.8351 (3.7003)  loss_n_80: 3.0091 (3.9207)  loss_n_100: 3.0461 (4.0345)  triple_100: 23.1637 (30.1778)  triple_80: 24.0751 (31.8065)  triple_60: 20.3942 (27.4987)  triple_40: 14.0750 (19.3828)  time: 5.9001  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 800/1724]  eta: 1:30:52  lr: 0.000020  loss: 90.4634 (123.7862)  loss_n_40: 2.9630 (3.6727)  loss_n_60: 2.7834 (3.6880)  loss_n_80: 3.0026 (3.9074)  loss_n_100: 3.0233 (4.0207)  triple_100: 22.6638 (30.0832)  triple_80: 23.7747 (31.7019)  triple_60: 20.1632 (27.4033)  triple_40: 13.8512 (19.3089)  time: 5.9004  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 810/1724]  eta: 1:29:53  lr: 0.000020  loss: 90.2209 (123.4031)  loss_n_40: 2.9505 (3.6648)  loss_n_60: 2.6777 (3.6760)  loss_n_80: 2.8724 (3.8939)  loss_n_100: 2.9771 (4.0064)  triple_100: 22.5591 (29.9933)  triple_80: 23.7747 (31.6049)  triple_60: 19.6850 (27.3200)  triple_40: 13.9973 (19.2438)  time: 5.9009  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 820/1724]  eta: 1:28:54  lr: 0.000020  loss: 90.1766 (122.9808)  loss_n_40: 2.9215 (3.6555)  loss_n_60: 2.6619 (3.6636)  loss_n_80: 2.8116 (3.8806)  loss_n_100: 2.8583 (3.9924)  triple_100: 22.1900 (29.8967)  triple_80: 22.9878 (31.5003)  triple_60: 19.9902 (27.2233)  triple_40: 13.8626 (19.1683)  time: 5.9002  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 830/1724]  eta: 1:27:55  lr: 0.000020  loss: 89.8688 (122.5670)  loss_n_40: 2.8566 (3.6462)  loss_n_60: 2.6314 (3.6505)  loss_n_80: 2.8066 (3.8669)  loss_n_100: 2.8566 (3.9781)  triple_100: 22.3078 (29.8035)  triple_80: 23.2900 (31.3988)  triple_60: 19.8046 (27.1297)  triple_40: 12.9114 (19.0933)  time: 5.8998  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 840/1724]  eta: 1:26:56  lr: 0.000020  loss: 86.4785 (122.1164)  loss_n_40: 2.8073 (3.6352)  loss_n_60: 2.5338 (3.6360)  loss_n_80: 2.7427 (3.8517)  loss_n_100: 2.8007 (3.9626)  triple_100: 21.8988 (29.7029)  triple_80: 22.6979 (31.2877)  triple_60: 19.0758 (27.0251)  triple_40: 12.9114 (19.0152)  time: 5.8997  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 850/1724]  eta: 1:25:57  lr: 0.000020  loss: 83.9047 (121.6807)  loss_n_40: 2.6851 (3.6249)  loss_n_60: 2.3687 (3.6214)  loss_n_80: 2.5913 (3.8364)  loss_n_100: 2.6522 (3.9466)  triple_100: 21.2317 (29.6027)  triple_80: 21.8597 (31.1787)  triple_60: 18.4478 (26.9258)  triple_40: 12.8348 (18.9442)  time: 5.9005  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 860/1724]  eta: 1:24:58  lr: 0.000020  loss: 86.1378 (121.2962)  loss_n_40: 2.7156 (3.6154)  loss_n_60: 2.3687 (3.6094)  loss_n_80: 2.5007 (3.8236)  loss_n_100: 2.5648 (3.9333)  triple_100: 21.5495 (29.5183)  triple_80: 22.1543 (31.0864)  triple_60: 18.9137 (26.8405)  triple_40: 13.0290 (18.8693)  time: 5.9018  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 870/1724]  eta: 1:23:59  lr: 0.000020  loss: 89.0497 (120.9022)  loss_n_40: 2.7156 (3.6044)  loss_n_60: 2.4870 (3.5953)  loss_n_80: 2.6275 (3.8090)  loss_n_100: 2.6476 (3.9184)  triple_100: 22.2456 (29.4312)  triple_80: 23.1368 (30.9895)  triple_60: 19.3732 (26.7496)  triple_40: 12.9099 (18.8048)  time: 5.9005  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 880/1724]  eta: 1:23:00  lr: 0.000020  loss: 87.3584 (120.5005)  loss_n_40: 2.6419 (3.5934)  loss_n_60: 2.3033 (3.5810)  loss_n_80: 2.4570 (3.7941)  loss_n_100: 2.5543 (3.9031)  triple_100: 22.1119 (29.3408)  triple_80: 22.6939 (30.8901)  triple_60: 19.0714 (26.6590)  triple_40: 12.9099 (18.7391)  time: 5.9003  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 890/1724]  eta: 1:22:01  lr: 0.000020  loss: 86.5904 (120.1240)  loss_n_40: 2.6432 (3.5831)  loss_n_60: 2.3058 (3.5684)  loss_n_80: 2.4774 (3.7807)  loss_n_100: 2.5767 (3.8892)  triple_100: 21.5829 (29.2553)  triple_80: 22.5001 (30.7990)  triple_60: 18.8300 (26.5748)  triple_40: 12.9505 (18.6735)  time: 5.9003  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 900/1724]  eta: 1:21:02  lr: 0.000020  loss: 84.9643 (119.7306)  loss_n_40: 2.6210 (3.5728)  loss_n_60: 2.4249 (3.5553)  loss_n_80: 2.5100 (3.7666)  loss_n_100: 2.5923 (3.8744)  triple_100: 21.4386 (29.1645)  triple_80: 22.5001 (30.7021)  triple_60: 18.8300 (26.4879)  triple_40: 12.6422 (18.6070)  time: 5.8991  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 910/1724]  eta: 1:20:03  lr: 0.000020  loss: 81.9606 (119.3425)  loss_n_40: 2.5669 (3.5612)  loss_n_60: 2.4018 (3.5416)  loss_n_80: 2.4956 (3.7521)  loss_n_100: 2.5635 (3.8594)  triple_100: 20.4482 (29.0775)  triple_80: 21.6105 (30.6079)  triple_60: 18.1222 (26.4022)  triple_40: 12.4030 (18.5406)  time: 5.9001  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 920/1724]  eta: 1:19:04  lr: 0.000020  loss: 83.7309 (118.9803)  loss_n_40: 2.4767 (3.5489)  loss_n_60: 2.2697 (3.5278)  loss_n_80: 2.4464 (3.7377)  loss_n_100: 2.5330 (3.8447)  triple_100: 20.9903 (28.9985)  triple_80: 22.0678 (30.5193)  triple_60: 18.3708 (26.3218)  triple_40: 12.7654 (18.4816)  time: 5.9005  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:2]  [ 930/1724]  eta: 1:18:05  lr: 0.000020  loss: 88.3941 (118.6316)  loss_n_40: 2.5052 (3.5386)  loss_n_60: 2.2954 (3.5158)  loss_n_80: 2.4308 (3.7249)  loss_n_100: 2.5169 (3.8316)  triple_100: 21.6756 (28.9205)  triple_80: 23.0791 (30.4348)  triple_60: 19.4881 (26.2452)  triple_40: 12.9295 (18.4203)  time: 5.9008  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:2]  [ 940/1724]  eta: 1:17:06  lr: 0.000020  loss: 86.3250 (118.2747)  loss_n_40: 2.4967 (3.5268)  loss_n_60: 2.2717 (3.5022)  loss_n_80: 2.4179 (3.7109)  loss_n_100: 2.5116 (3.8173)  triple_100: 21.4867 (28.8419)  triple_80: 22.3574 (30.3470)  triple_60: 18.7974 (26.1645)  triple_40: 13.2023 (18.3642)  time: 5.9018  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 950/1724]  eta: 1:16:06  lr: 0.000020  loss: 84.5352 (117.9299)  loss_n_40: 2.4026 (3.5152)  loss_n_60: 2.2225 (3.4894)  loss_n_80: 2.3653 (3.6972)  loss_n_100: 2.4634 (3.8034)  triple_100: 21.4151 (28.7655)  triple_80: 22.3292 (30.2632)  triple_60: 18.6969 (26.0904)  triple_40: 12.8936 (18.3056)  time: 5.9009  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 960/1724]  eta: 1:15:07  lr: 0.000020  loss: 84.4409 (117.5922)  loss_n_40: 2.4026 (3.5045)  loss_n_60: 2.2977 (3.4774)  loss_n_80: 2.3706 (3.6844)  loss_n_100: 2.4634 (3.7902)  triple_100: 21.3402 (28.6908)  triple_80: 22.0137 (30.1815)  triple_60: 18.8720 (26.0167)  triple_40: 12.3613 (18.2466)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 970/1724]  eta: 1:14:08  lr: 0.000020  loss: 82.2445 (117.2083)  loss_n_40: 2.3860 (3.4927)  loss_n_60: 2.2276 (3.4645)  loss_n_80: 2.3323 (3.6706)  loss_n_100: 2.4181 (3.7760)  triple_100: 20.7755 (28.6035)  triple_80: 21.5460 (30.0886)  triple_60: 18.4431 (25.9327)  triple_40: 12.1289 (18.1797)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [ 980/1724]  eta: 1:13:09  lr: 0.000020  loss: 80.1222 (116.8475)  loss_n_40: 2.2569 (3.4799)  loss_n_60: 2.1742 (3.4512)  loss_n_80: 2.3000 (3.6568)  loss_n_100: 2.3283 (3.7620)  triple_100: 20.3435 (28.5245)  triple_80: 21.2891 (30.0017)  triple_60: 17.6065 (25.8535)  triple_40: 11.9239 (18.1180)  time: 5.8998  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [ 990/1724]  eta: 1:12:10  lr: 0.000020  loss: 81.5313 (116.5057)  loss_n_40: 2.3011 (3.4687)  loss_n_60: 2.2033 (3.4397)  loss_n_80: 2.3577 (3.6445)  loss_n_100: 2.4312 (3.7494)  triple_100: 20.3435 (28.4483)  triple_80: 21.4762 (29.9207)  triple_60: 18.3123 (25.7800)  triple_40: 12.0624 (18.0544)  time: 5.9010  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1000/1724]  eta: 1:11:11  lr: 0.000020  loss: 83.0578 (116.1501)  loss_n_40: 2.3118 (3.4567)  loss_n_60: 2.2100 (3.4269)  loss_n_80: 2.3175 (3.6310)  loss_n_100: 2.3795 (3.7357)  triple_100: 20.6896 (28.3689)  triple_80: 21.8071 (29.8342)  triple_60: 18.5137 (25.7021)  triple_40: 12.0279 (17.9946)  time: 5.8995  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1010/1724]  eta: 1:10:12  lr: 0.000020  loss: 79.9107 (115.7846)  loss_n_40: 2.1753 (3.4447)  loss_n_60: 2.1046 (3.4144)  loss_n_80: 2.2633 (3.6179)  loss_n_100: 2.3484 (3.7222)  triple_100: 20.1853 (28.2877)  triple_80: 21.0890 (29.7473)  triple_60: 17.9144 (25.6213)  triple_40: 11.8282 (17.9290)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1020/1724]  eta: 1:09:13  lr: 0.000020  loss: 78.9256 (115.4470)  loss_n_40: 2.1807 (3.4329)  loss_n_60: 2.1261 (3.4020)  loss_n_80: 2.2758 (3.6047)  loss_n_100: 2.3249 (3.7087)  triple_100: 20.0828 (28.2130)  triple_80: 21.0672 (29.6654)  triple_60: 17.9144 (25.5493)  triple_40: 12.0403 (17.8711)  time: 5.8992  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1030/1724]  eta: 1:08:14  lr: 0.000020  loss: 82.1868 (115.1241)  loss_n_40: 2.2607 (3.4212)  loss_n_60: 2.2619 (3.3901)  loss_n_80: 2.3587 (3.5920)  loss_n_100: 2.4151 (3.6959)  triple_100: 20.9472 (28.1432)  triple_80: 21.7413 (29.5883)  triple_60: 18.4399 (25.4808)  triple_40: 12.0403 (17.8126)  time: 5.9001  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1040/1724]  eta: 1:07:15  lr: 0.000020  loss: 82.4273 (114.8192)  loss_n_40: 2.2607 (3.4100)  loss_n_60: 2.2070 (3.3782)  loss_n_80: 2.3587 (3.5798)  loss_n_100: 2.4360 (3.6836)  triple_100: 21.3483 (28.0774)  triple_80: 21.9077 (29.5163)  triple_60: 18.6360 (25.4140)  triple_40: 12.1465 (17.7599)  time: 5.9008  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1050/1724]  eta: 1:06:16  lr: 0.000020  loss: 82.5232 (114.5164)  loss_n_40: 2.2310 (3.3989)  loss_n_60: 2.1735 (3.3670)  loss_n_80: 2.3338 (3.5679)  loss_n_100: 2.4360 (3.6715)  triple_100: 21.4442 (28.0120)  triple_80: 22.4200 (29.4446)  triple_60: 18.3047 (25.3508)  triple_40: 12.1465 (17.7038)  time: 5.9008  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1060/1724]  eta: 1:05:17  lr: 0.000020  loss: 80.2700 (114.1870)  loss_n_40: 2.1268 (3.3863)  loss_n_60: 2.0874 (3.3543)  loss_n_80: 2.2082 (3.5546)  loss_n_100: 2.2834 (3.6581)  triple_100: 20.3358 (27.9400)  triple_80: 20.9244 (29.3654)  triple_60: 18.3047 (25.2800)  triple_40: 11.7767 (17.6482)  time: 5.9000  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1070/1724]  eta: 1:04:18  lr: 0.000020  loss: 79.5325 (113.8640)  loss_n_40: 2.1202 (3.3756)  loss_n_60: 2.0779 (3.3430)  loss_n_80: 2.2201 (3.5424)  loss_n_100: 2.2907 (3.6455)  triple_100: 19.7052 (27.8658)  triple_80: 20.6779 (29.2868)  triple_60: 17.6205 (25.2115)  triple_40: 11.9548 (17.5933)  time: 5.8994  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1080/1724]  eta: 1:03:19  lr: 0.000020  loss: 79.5325 (113.5707)  loss_n_40: 2.1134 (3.3644)  loss_n_60: 2.0779 (3.3319)  loss_n_80: 2.2211 (3.5303)  loss_n_100: 2.3049 (3.6332)  triple_100: 19.9915 (27.8004)  triple_80: 20.8844 (29.2156)  triple_60: 18.0647 (25.1517)  triple_40: 12.0660 (17.5433)  time: 5.9001  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1090/1724]  eta: 1:02:20  lr: 0.000020  loss: 80.9235 (113.2762)  loss_n_40: 2.0669 (3.3533)  loss_n_60: 2.0315 (3.3209)  loss_n_80: 2.1661 (3.5187)  loss_n_100: 2.2877 (3.6215)  triple_100: 20.7069 (27.7353)  triple_80: 21.3363 (29.1464)  triple_60: 18.2803 (25.0897)  triple_40: 12.1374 (17.4905)  time: 5.9011  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1100/1724]  eta: 1:01:21  lr: 0.000020  loss: 81.0691 (113.0100)  loss_n_40: 2.1447 (3.3433)  loss_n_60: 2.0885 (3.3107)  loss_n_80: 2.2631 (3.5081)  loss_n_100: 2.3369 (3.6107)  triple_100: 20.6495 (27.6760)  triple_80: 21.4882 (29.0836)  triple_60: 18.4732 (25.0331)  triple_40: 12.1374 (17.4445)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1110/1724]  eta: 1:00:22  lr: 0.000020  loss: 78.7567 (112.6822)  loss_n_40: 2.1314 (3.3321)  loss_n_60: 2.0758 (3.2988)  loss_n_80: 2.1924 (3.4952)  loss_n_100: 2.2694 (3.5975)  triple_100: 19.7787 (27.5996)  triple_80: 20.8201 (29.0018)  triple_60: 17.7470 (24.9637)  triple_40: 12.0256 (17.3935)  time: 5.9012  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1120/1724]  eta: 0:59:23  lr: 0.000020  loss: 77.1544 (112.3846)  loss_n_40: 2.0674 (3.3212)  loss_n_60: 1.9870 (3.2882)  loss_n_80: 2.0723 (3.4841)  loss_n_100: 2.1404 (3.5861)  triple_100: 19.3806 (27.5330)  triple_80: 20.5357 (28.9314)  triple_60: 17.2533 (24.8996)  triple_40: 11.5529 (17.3410)  time: 5.9010  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1130/1724]  eta: 0:58:24  lr: 0.000020  loss: 78.1249 (112.0982)  loss_n_40: 2.1163 (3.3114)  loss_n_60: 1.9975 (3.2781)  loss_n_80: 2.1310 (3.4733)  loss_n_100: 2.1918 (3.5751)  triple_100: 19.9653 (27.4677)  triple_80: 20.5602 (28.8627)  triple_60: 17.5854 (24.8393)  triple_40: 11.8354 (17.2906)  time: 5.9013  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1140/1724]  eta: 0:57:25  lr: 0.000020  loss: 77.4110 (111.7810)  loss_n_40: 2.0937 (3.2999)  loss_n_60: 1.9614 (3.2660)  loss_n_80: 2.0697 (3.4604)  loss_n_100: 2.1863 (3.5618)  triple_100: 19.2013 (27.3946)  triple_80: 20.5048 (28.7838)  triple_60: 17.5854 (24.7725)  triple_40: 11.8974 (17.2420)  time: 5.9005  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1150/1724]  eta: 0:56:26  lr: 0.000020  loss: 76.0309 (111.4834)  loss_n_40: 1.9345 (3.2888)  loss_n_60: 1.8588 (3.2546)  loss_n_80: 2.0051 (3.4483)  loss_n_100: 2.0480 (3.5496)  triple_100: 19.0977 (27.3269)  triple_80: 19.7201 (28.7106)  triple_60: 17.3888 (24.7096)  triple_40: 11.8974 (17.1951)  time: 5.8988  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1160/1724]  eta: 0:55:27  lr: 0.000020  loss: 76.1754 (111.2019)  loss_n_40: 1.9881 (3.2788)  loss_n_60: 1.9390 (3.2446)  loss_n_80: 2.0125 (3.4376)  loss_n_100: 2.0884 (3.5386)  triple_100: 19.2023 (27.2626)  triple_80: 19.8616 (28.6430)  triple_60: 17.5450 (24.6503)  triple_40: 11.8186 (17.1465)  time: 5.8983  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1170/1724]  eta: 0:54:28  lr: 0.000020  loss: 74.2239 (110.9089)  loss_n_40: 2.0213 (3.2676)  loss_n_60: 1.9813 (3.2330)  loss_n_80: 2.0670 (3.4252)  loss_n_100: 2.0999 (3.5259)  triple_100: 18.8949 (27.1958)  triple_80: 19.6542 (28.5704)  triple_60: 16.9571 (24.5895)  triple_40: 11.8780 (17.1015)  time: 5.8982  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1180/1724]  eta: 0:53:29  lr: 0.000020  loss: 76.0790 (110.6196)  loss_n_40: 1.9934 (3.2572)  loss_n_60: 1.9813 (3.2225)  loss_n_80: 2.0670 (3.4140)  loss_n_100: 2.0999 (3.5145)  triple_100: 19.2019 (27.1292)  triple_80: 19.6964 (28.5004)  triple_60: 17.1409 (24.5287)  triple_40: 11.8652 (17.0531)  time: 5.8980  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1190/1724]  eta: 0:52:30  lr: 0.000020  loss: 76.0790 (110.3229)  loss_n_40: 1.9044 (3.2464)  loss_n_60: 1.8864 (3.2114)  loss_n_80: 1.9755 (3.4022)  loss_n_100: 2.0610 (3.5025)  triple_100: 18.5322 (27.0611)  triple_80: 19.6964 (28.4275)  triple_60: 17.2254 (24.4664)  triple_40: 11.4547 (17.0055)  time: 5.8984  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:2]  [1200/1724]  eta: 0:51:31  lr: 0.000020  loss: 74.2438 (110.0314)  loss_n_40: 1.9119 (3.2356)  loss_n_60: 1.8864 (3.2004)  loss_n_80: 1.9755 (3.3904)  loss_n_100: 2.0266 (3.4905)  triple_100: 18.5114 (26.9944)  triple_80: 19.4445 (28.3562)  triple_60: 17.0126 (24.4061)  triple_40: 11.1079 (16.9577)  time: 5.8992  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1210/1724]  eta: 0:50:32  lr: 0.000020  loss: 76.0332 (109.7470)  loss_n_40: 2.0112 (3.2251)  loss_n_60: 1.9357 (3.1898)  loss_n_80: 1.9839 (3.3792)  loss_n_100: 1.9984 (3.4791)  triple_100: 19.1749 (26.9303)  triple_80: 19.8115 (28.2872)  triple_60: 17.0762 (24.3461)  triple_40: 11.0256 (16.9102)  time: 5.8983  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [1220/1724]  eta: 0:49:33  lr: 0.000020  loss: 76.1263 (109.4840)  loss_n_40: 2.0068 (3.2153)  loss_n_60: 1.9467 (3.1797)  loss_n_80: 2.0091 (3.3685)  loss_n_100: 2.0718 (3.4683)  triple_100: 19.2749 (26.8705)  triple_80: 19.8754 (28.2229)  triple_60: 17.5737 (24.2914)  triple_40: 11.4741 (16.8674)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1230/1724]  eta: 0:48:34  lr: 0.000020  loss: 75.6974 (109.2121)  loss_n_40: 2.0068 (3.2053)  loss_n_60: 1.9467 (3.1692)  loss_n_80: 2.0110 (3.3572)  loss_n_100: 2.0718 (3.4567)  triple_100: 19.0331 (26.8070)  triple_80: 19.8019 (28.1552)  triple_60: 17.1609 (24.2356)  triple_40: 11.6935 (16.8259)  time: 5.8976  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:2]  [1240/1724]  eta: 0:47:35  lr: 0.000020  loss: 74.3635 (108.9420)  loss_n_40: 2.0318 (3.1960)  loss_n_60: 1.8726 (3.1595)  loss_n_80: 2.0110 (3.3466)  loss_n_100: 2.0615 (3.4458)  triple_100: 18.7189 (26.7427)  triple_80: 19.5634 (28.0887)  triple_60: 16.9009 (24.1799)  triple_40: 11.4674 (16.7828)  time: 5.8981  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1250/1724]  eta: 0:46:36  lr: 0.000020  loss: 73.0589 (108.6630)  loss_n_40: 1.9786 (3.1856)  loss_n_60: 1.8711 (3.1490)  loss_n_80: 1.9297 (3.3354)  loss_n_100: 1.9660 (3.4343)  triple_100: 18.1629 (26.6774)  triple_80: 18.7765 (28.0194)  triple_60: 16.6187 (24.1226)  triple_40: 11.4674 (16.7395)  time: 5.8981  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1260/1724]  eta: 0:45:37  lr: 0.000020  loss: 73.6969 (108.4015)  loss_n_40: 1.8838 (3.1758)  loss_n_60: 1.8711 (3.1387)  loss_n_80: 1.9297 (3.3243)  loss_n_100: 1.9504 (3.4229)  triple_100: 18.6451 (26.6155)  triple_80: 19.1628 (27.9536)  triple_60: 16.6187 (24.0696)  triple_40: 11.5126 (16.7011)  time: 5.8973  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1270/1724]  eta: 0:44:38  lr: 0.000020  loss: 76.1434 (108.1529)  loss_n_40: 1.8838 (3.1657)  loss_n_60: 1.8490 (3.1286)  loss_n_80: 1.9494 (3.3137)  loss_n_100: 2.0271 (3.4121)  triple_100: 18.8075 (26.5593)  triple_80: 19.7829 (27.8935)  triple_60: 17.1257 (24.0180)  triple_40: 11.7981 (16.6620)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1280/1724]  eta: 0:43:39  lr: 0.000020  loss: 76.2107 (107.9064)  loss_n_40: 1.8443 (3.1562)  loss_n_60: 1.8490 (3.1192)  loss_n_80: 2.0020 (3.3037)  loss_n_100: 2.0271 (3.4020)  triple_100: 19.5409 (26.5032)  triple_80: 20.3095 (27.8341)  triple_60: 17.4695 (23.9673)  triple_40: 11.5275 (16.6206)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1290/1724]  eta: 0:42:40  lr: 0.000020  loss: 78.6112 (107.6881)  loss_n_40: 2.0262 (3.1486)  loss_n_60: 2.0128 (3.1117)  loss_n_80: 2.0794 (3.2955)  loss_n_100: 2.1475 (3.3935)  triple_100: 19.8096 (26.4532)  triple_80: 20.8514 (27.7830)  triple_60: 17.9662 (23.9234)  triple_40: 11.1540 (16.5793)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1300/1724]  eta: 0:41:41  lr: 0.000020  loss: 75.0781 (107.4266)  loss_n_40: 1.9613 (3.1391)  loss_n_60: 1.7863 (3.1019)  loss_n_80: 1.9120 (3.2851)  loss_n_100: 1.9975 (3.3829)  triple_100: 19.0394 (26.3936)  triple_80: 19.6948 (27.7191)  triple_60: 17.2535 (23.8684)  triple_40: 11.1349 (16.5365)  time: 5.8952  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1310/1724]  eta: 0:40:42  lr: 0.000020  loss: 70.4403 (107.1611)  loss_n_40: 1.8129 (3.1292)  loss_n_60: 1.7431 (3.0918)  loss_n_80: 1.8384 (3.2744)  loss_n_100: 1.9243 (3.3720)  triple_100: 17.8253 (26.3316)  triple_80: 18.7000 (27.6538)  triple_60: 15.9619 (23.8130)  triple_40: 10.8517 (16.4952)  time: 5.8958  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1320/1724]  eta: 0:39:43  lr: 0.000020  loss: 73.3817 (106.9199)  loss_n_40: 1.8828 (3.1205)  loss_n_60: 1.8611 (3.0829)  loss_n_80: 2.0264 (3.2649)  loss_n_100: 2.0834 (3.3623)  triple_100: 18.1149 (26.2755)  triple_80: 19.2134 (27.5951)  triple_60: 16.7272 (23.7636)  triple_40: 11.3470 (16.4550)  time: 5.8961  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1330/1724]  eta: 0:38:44  lr: 0.000020  loss: 77.4631 (106.7034)  loss_n_40: 1.9934 (3.1125)  loss_n_60: 1.9892 (3.0752)  loss_n_80: 2.0473 (3.2567)  loss_n_100: 2.1366 (3.3539)  triple_100: 19.6798 (26.2264)  triple_80: 20.7568 (27.5443)  triple_60: 17.4299 (23.7195)  triple_40: 11.1802 (16.4148)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1340/1724]  eta: 0:37:45  lr: 0.000020  loss: 77.3969 (106.4812)  loss_n_40: 1.9546 (3.1039)  loss_n_60: 1.9801 (3.0664)  loss_n_80: 2.1020 (3.2472)  loss_n_100: 2.2154 (3.3442)  triple_100: 19.5702 (26.1751)  triple_80: 20.5949 (27.4897)  triple_60: 17.8525 (23.6756)  triple_40: 11.1802 (16.3792)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1350/1724]  eta: 0:36:46  lr: 0.000020  loss: 74.0412 (106.2467)  loss_n_40: 1.8507 (3.0947)  loss_n_60: 1.8559 (3.0572)  loss_n_80: 1.9627 (3.2374)  loss_n_100: 2.0044 (3.3342)  triple_100: 18.6220 (26.1214)  triple_80: 19.6503 (27.4332)  triple_60: 17.1676 (23.6279)  triple_40: 11.2289 (16.3407)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1360/1724]  eta: 0:35:47  lr: 0.000020  loss: 75.4251 (106.0390)  loss_n_40: 1.9256 (3.0869)  loss_n_60: 1.9219 (3.0494)  loss_n_80: 2.0312 (3.2291)  loss_n_100: 2.0920 (3.3258)  triple_100: 19.0240 (26.0741)  triple_80: 19.8289 (27.3839)  triple_60: 17.4330 (23.5859)  triple_40: 11.1883 (16.3039)  time: 5.8979  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1370/1724]  eta: 0:34:48  lr: 0.000020  loss: 76.7464 (105.8051)  loss_n_40: 1.9622 (3.0779)  loss_n_60: 1.9219 (3.0401)  loss_n_80: 1.9972 (3.2191)  loss_n_100: 2.0510 (3.3157)  triple_100: 19.0240 (26.0196)  triple_80: 19.9693 (27.3257)  triple_60: 17.4386 (23.5374)  triple_40: 11.4514 (16.2695)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1380/1724]  eta: 0:33:49  lr: 0.000020  loss: 72.6788 (105.5621)  loss_n_40: 1.7660 (3.0680)  loss_n_60: 1.6745 (3.0302)  loss_n_80: 1.7586 (3.2089)  loss_n_100: 1.8303 (3.3053)  triple_100: 18.3328 (25.9650)  triple_80: 19.0858 (27.2666)  triple_60: 16.7164 (23.4866)  triple_40: 11.2040 (16.2314)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1390/1724]  eta: 0:32:50  lr: 0.000020  loss: 69.9950 (105.3323)  loss_n_40: 1.7660 (3.0599)  loss_n_60: 1.6745 (3.0220)  loss_n_80: 1.7586 (3.2000)  loss_n_100: 1.8303 (3.2961)  triple_100: 18.0362 (25.9111)  triple_80: 18.8270 (27.2101)  triple_60: 16.4178 (23.4395)  triple_40: 10.9441 (16.1937)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1400/1724]  eta: 0:31:51  lr: 0.000020  loss: 71.1870 (105.1081)  loss_n_40: 2.0509 (3.0525)  loss_n_60: 1.9143 (3.0138)  loss_n_80: 1.9815 (3.1911)  loss_n_100: 2.0601 (3.2870)  triple_100: 18.0362 (25.8574)  triple_80: 18.9952 (27.1538)  triple_60: 16.4681 (23.3930)  triple_40: 11.0897 (16.1594)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1410/1724]  eta: 0:30:52  lr: 0.000020  loss: 73.7809 (104.8836)  loss_n_40: 2.0509 (3.0458)  loss_n_60: 1.8911 (3.0064)  loss_n_80: 1.9714 (3.1831)  loss_n_100: 2.0089 (3.2788)  triple_100: 18.3657 (25.8044)  triple_80: 19.4327 (27.0993)  triple_60: 16.7914 (23.3454)  triple_40: 10.9041 (16.1204)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1420/1724]  eta: 0:29:53  lr: 0.000020  loss: 75.7836 (104.6875)  loss_n_40: 2.0267 (3.0396)  loss_n_60: 1.8911 (2.9998)  loss_n_80: 1.9870 (3.1757)  loss_n_100: 2.0908 (3.2711)  triple_100: 19.1335 (25.7568)  triple_80: 19.8849 (27.0514)  triple_60: 17.3239 (23.3069)  triple_40: 10.7641 (16.0862)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1430/1724]  eta: 0:28:54  lr: 0.000020  loss: 76.3786 (104.4848)  loss_n_40: 2.0322 (3.0322)  loss_n_60: 1.9658 (2.9918)  loss_n_80: 2.0203 (3.1671)  loss_n_100: 2.0908 (3.2624)  triple_100: 19.4280 (25.7093)  triple_80: 20.2694 (27.0009)  triple_60: 17.6783 (23.2658)  triple_40: 11.5053 (16.0552)  time: 5.8943  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1440/1724]  eta: 0:27:55  lr: 0.000020  loss: 76.7595 (104.2696)  loss_n_40: 1.9230 (3.0243)  loss_n_60: 1.8006 (2.9835)  loss_n_80: 1.8480 (3.1581)  loss_n_100: 1.9214 (3.2531)  triple_100: 18.9842 (25.6581)  triple_80: 20.2076 (26.9471)  triple_60: 17.7151 (23.2218)  triple_40: 11.9006 (16.0236)  time: 5.8945  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [1450/1724]  eta: 0:26:56  lr: 0.000020  loss: 70.4540 (104.0416)  loss_n_40: 1.8376 (3.0163)  loss_n_60: 1.6890 (2.9750)  loss_n_80: 1.7782 (3.1490)  loss_n_100: 1.8511 (3.2438)  triple_100: 17.8137 (25.6039)  triple_80: 18.7302 (26.8901)  triple_60: 15.9805 (23.1740)  triple_40: 11.4513 (15.9895)  time: 5.8953  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1460/1724]  eta: 0:25:57  lr: 0.000020  loss: 69.1294 (103.8105)  loss_n_40: 1.7283 (3.0074)  loss_n_60: 1.6568 (2.9658)  loss_n_80: 1.7367 (3.1392)  loss_n_100: 1.8155 (3.2339)  triple_100: 17.6952 (25.5510)  triple_80: 18.4723 (26.8332)  triple_60: 15.8957 (23.1257)  triple_40: 10.9731 (15.9542)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1470/1724]  eta: 0:24:58  lr: 0.000020  loss: 73.3281 (103.6322)  loss_n_40: 1.7765 (3.0003)  loss_n_60: 1.7562 (2.9585)  loss_n_80: 1.8821 (3.1314)  loss_n_100: 1.9164 (3.2259)  triple_100: 18.5945 (25.5090)  triple_80: 19.5107 (26.7904)  triple_60: 17.1331 (23.0904)  triple_40: 11.1020 (15.9263)  time: 5.8961  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1480/1724]  eta: 0:23:59  lr: 0.000020  loss: 72.5525 (103.4146)  loss_n_40: 1.7765 (2.9914)  loss_n_60: 1.7493 (2.9494)  loss_n_80: 1.8338 (3.1219)  loss_n_100: 1.8785 (3.2162)  triple_100: 18.4030 (25.4589)  triple_80: 19.1313 (26.7365)  triple_60: 16.6140 (23.0442)  triple_40: 11.4948 (15.8962)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1490/1724]  eta: 0:23:00  lr: 0.000020  loss: 72.3098 (103.2134)  loss_n_40: 1.6881 (2.9831)  loss_n_60: 1.6186 (2.9410)  loss_n_80: 1.7583 (3.1130)  loss_n_100: 1.8312 (3.2072)  triple_100: 18.2590 (25.4129)  triple_80: 18.9452 (26.6873)  triple_60: 16.1913 (23.0031)  triple_40: 11.4185 (15.8659)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1500/1724]  eta: 0:22:01  lr: 0.000020  loss: 74.8713 (103.0205)  loss_n_40: 1.8292 (2.9763)  loss_n_60: 1.7193 (2.9337)  loss_n_80: 1.8326 (3.1051)  loss_n_100: 1.8781 (3.1991)  triple_100: 18.6095 (25.3666)  triple_80: 19.5531 (26.6394)  triple_60: 17.1050 (22.9637)  triple_40: 11.6489 (15.8365)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1510/1724]  eta: 0:21:02  lr: 0.000020  loss: 74.5318 (102.8202)  loss_n_40: 1.8529 (2.9686)  loss_n_60: 1.7826 (2.9257)  loss_n_80: 1.8460 (3.0967)  loss_n_100: 1.9119 (3.1903)  triple_100: 17.9777 (25.3186)  triple_80: 19.3650 (26.5900)  triple_60: 17.1050 (22.9219)  triple_40: 11.6676 (15.8083)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1520/1724]  eta: 0:20:03  lr: 0.000020  loss: 71.5541 (102.6188)  loss_n_40: 1.7225 (2.9602)  loss_n_60: 1.6862 (2.9173)  loss_n_80: 1.8177 (3.0879)  loss_n_100: 1.8853 (3.1814)  triple_100: 17.9390 (25.2730)  triple_80: 18.8478 (26.5416)  triple_60: 16.4158 (22.8795)  triple_40: 11.1673 (15.7778)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1530/1724]  eta: 0:19:04  lr: 0.000020  loss: 71.8905 (102.4361)  loss_n_40: 1.7225 (2.9526)  loss_n_60: 1.6862 (2.9096)  loss_n_80: 1.8114 (3.0799)  loss_n_100: 1.8853 (3.1733)  triple_100: 18.3244 (25.2314)  triple_80: 19.1688 (26.4976)  triple_60: 16.4464 (22.8415)  triple_40: 11.2054 (15.7502)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1540/1724]  eta: 0:18:05  lr: 0.000020  loss: 72.9200 (102.2519)  loss_n_40: 1.7676 (2.9456)  loss_n_60: 1.7187 (2.9024)  loss_n_80: 1.8307 (3.0723)  loss_n_100: 1.9120 (3.1656)  triple_100: 18.7009 (25.1890)  triple_80: 19.5156 (26.4530)  triple_60: 16.6922 (22.8037)  triple_40: 11.2494 (15.7203)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1550/1724]  eta: 0:17:06  lr: 0.000020  loss: 74.2086 (102.0756)  loss_n_40: 1.9413 (2.9394)  loss_n_60: 1.8139 (2.8959)  loss_n_80: 1.9310 (3.0651)  loss_n_100: 1.9812 (3.1582)  triple_100: 18.4638 (25.1467)  triple_80: 19.7892 (26.4096)  triple_60: 17.2547 (22.7689)  triple_40: 11.1288 (15.6918)  time: 5.8963  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1560/1724]  eta: 0:16:07  lr: 0.000020  loss: 73.8808 (101.8911)  loss_n_40: 1.8816 (2.9327)  loss_n_60: 1.8595 (2.8888)  loss_n_80: 1.8528 (3.0574)  loss_n_100: 1.9239 (3.1503)  triple_100: 18.3929 (25.1027)  triple_80: 19.0751 (26.3635)  triple_60: 17.2547 (22.7315)  triple_40: 11.2552 (15.6643)  time: 5.8947  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1570/1724]  eta: 0:15:08  lr: 0.000020  loss: 70.1971 (101.6987)  loss_n_40: 1.7477 (2.9251)  loss_n_60: 1.6222 (2.8809)  loss_n_80: 1.7158 (3.0491)  loss_n_100: 1.7770 (3.1418)  triple_100: 17.6291 (25.0570)  triple_80: 18.4301 (26.3159)  triple_60: 16.3020 (22.6914)  triple_40: 11.2450 (15.6376)  time: 5.8944  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1580/1724]  eta: 0:14:09  lr: 0.000020  loss: 71.2232 (101.5118)  loss_n_40: 1.7499 (2.9181)  loss_n_60: 1.6092 (2.8737)  loss_n_80: 1.6939 (3.0414)  loss_n_100: 1.7621 (3.1339)  triple_100: 18.0462 (25.0139)  triple_80: 18.6111 (26.2703)  triple_60: 16.2825 (22.6535)  triple_40: 11.1488 (15.6071)  time: 5.8945  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1590/1724]  eta: 0:13:10  lr: 0.000020  loss: 74.3333 (101.3367)  loss_n_40: 1.7499 (2.9109)  loss_n_60: 1.6226 (2.8663)  loss_n_80: 1.7457 (3.0336)  loss_n_100: 1.7954 (3.1259)  triple_100: 18.4496 (24.9726)  triple_80: 19.6712 (26.2276)  triple_60: 16.7174 (22.6170)  triple_40: 11.1488 (15.5829)  time: 5.8938  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1600/1724]  eta: 0:12:11  lr: 0.000020  loss: 70.9471 (101.1522)  loss_n_40: 1.6508 (2.9037)  loss_n_60: 1.6000 (2.8588)  loss_n_80: 1.7179 (3.0256)  loss_n_100: 1.7950 (3.1177)  triple_100: 17.8178 (24.9296)  triple_80: 18.7001 (26.1819)  triple_60: 16.1509 (22.5784)  triple_40: 11.6641 (15.5565)  time: 5.8927  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1610/1724]  eta: 0:11:12  lr: 0.000020  loss: 70.4942 (100.9586)  loss_n_40: 1.7206 (2.8965)  loss_n_60: 1.5936 (2.8509)  loss_n_80: 1.6853 (3.0172)  loss_n_100: 1.7204 (3.1089)  triple_100: 17.4024 (24.8828)  triple_80: 18.3811 (26.1331)  triple_60: 16.1344 (22.5389)  triple_40: 11.4637 (15.5304)  time: 5.8924  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1620/1724]  eta: 0:10:13  lr: 0.000020  loss: 69.6422 (100.7651)  loss_n_40: 1.6514 (2.8894)  loss_n_60: 1.5199 (2.8436)  loss_n_80: 1.6093 (3.0095)  loss_n_100: 1.6568 (3.1010)  triple_100: 17.4024 (24.8373)  triple_80: 18.3158 (26.0858)  triple_60: 15.7515 (22.4986)  triple_40: 10.9160 (15.5002)  time: 5.8921  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1630/1724]  eta: 0:09:14  lr: 0.000020  loss: 69.8117 (100.5947)  loss_n_40: 1.6755 (2.8828)  loss_n_60: 1.6151 (2.8369)  loss_n_80: 1.7227 (3.0024)  loss_n_100: 1.7878 (3.0937)  triple_100: 17.9742 (24.7977)  triple_80: 18.6606 (26.0445)  triple_60: 15.9524 (22.4643)  triple_40: 10.8174 (15.4725)  time: 5.8922  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1640/1724]  eta: 0:08:15  lr: 0.000020  loss: 71.8296 (100.4307)  loss_n_40: 1.7490 (2.8764)  loss_n_60: 1.6947 (2.8305)  loss_n_80: 1.8151 (2.9956)  loss_n_100: 1.8858 (3.0866)  triple_100: 18.0159 (24.7592)  triple_80: 19.1049 (26.0053)  triple_60: 16.3603 (22.4312)  triple_40: 11.1286 (15.4458)  time: 5.8925  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1650/1724]  eta: 0:07:16  lr: 0.000020  loss: 71.8656 (100.2512)  loss_n_40: 1.7483 (2.8697)  loss_n_60: 1.6924 (2.8234)  loss_n_80: 1.8151 (2.9880)  loss_n_100: 1.8941 (3.0789)  triple_100: 17.9111 (24.7171)  triple_80: 18.8280 (25.9611)  triple_60: 16.4068 (22.3935)  triple_40: 11.2331 (15.4196)  time: 5.8914  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1660/1724]  eta: 0:06:17  lr: 0.000020  loss: 70.5146 (100.0719)  loss_n_40: 1.7047 (2.8630)  loss_n_60: 1.6663 (2.8164)  loss_n_80: 1.7879 (2.9804)  loss_n_100: 1.8292 (3.0710)  triple_100: 17.9066 (24.6744)  triple_80: 18.6471 (25.9163)  triple_60: 16.0802 (22.3567)  triple_40: 11.3078 (15.3938)  time: 5.8908  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1670/1724]  eta: 0:05:18  lr: 0.000020  loss: 70.9705 (99.9143)  loss_n_40: 1.7549 (2.8571)  loss_n_60: 1.6747 (2.8105)  loss_n_80: 1.7975 (2.9742)  loss_n_100: 1.8550 (3.0645)  triple_100: 18.0251 (24.6375)  triple_80: 18.8290 (25.8785)  triple_60: 16.2733 (22.3246)  triple_40: 10.7695 (15.3674)  time: 5.8921  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:2]  [1680/1724]  eta: 0:04:19  lr: 0.000020  loss: 70.0569 (99.7291)  loss_n_40: 1.7411 (2.8500)  loss_n_60: 1.6747 (2.8031)  loss_n_80: 1.8009 (2.9663)  loss_n_100: 1.8550 (3.0566)  triple_100: 17.7150 (24.5943)  triple_80: 18.5989 (25.8321)  triple_60: 16.1394 (22.2855)  triple_40: 10.7666 (15.3412)  time: 5.8940  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1690/1724]  eta: 0:03:20  lr: 0.000020  loss: 69.3268 (99.5434)  loss_n_40: 1.6188 (2.8431)  loss_n_60: 1.5152 (2.7957)  loss_n_80: 1.6204 (2.9585)  loss_n_100: 1.6727 (3.0484)  triple_100: 17.5313 (24.5498)  triple_80: 18.1897 (25.7856)  triple_60: 15.8778 (22.2461)  triple_40: 10.9839 (15.3162)  time: 5.8934  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1700/1724]  eta: 0:02:21  lr: 0.000020  loss: 68.1405 (99.3566)  loss_n_40: 1.6188 (2.8359)  loss_n_60: 1.5344 (2.7882)  loss_n_80: 1.6204 (2.9506)  loss_n_100: 1.6727 (3.0403)  triple_100: 16.8647 (24.5062)  triple_80: 17.7905 (25.7392)  triple_60: 15.6003 (22.2068)  triple_40: 11.1010 (15.2893)  time: 5.8933  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1710/1724]  eta: 0:01:22  lr: 0.000020  loss: 68.2064 (99.1880)  loss_n_40: 1.6213 (2.8295)  loss_n_60: 1.5788 (2.7817)  loss_n_80: 1.6470 (2.9437)  loss_n_100: 1.7040 (3.0332)  triple_100: 17.0986 (24.4659)  triple_80: 18.1994 (25.6977)  triple_60: 15.5634 (22.1722)  triple_40: 10.8084 (15.2642)  time: 5.8947  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1720/1724]  eta: 0:00:23  lr: 0.000020  loss: 68.6408 (99.0018)  loss_n_40: 1.6873 (2.8235)  loss_n_60: 1.5971 (2.7753)  loss_n_80: 1.6964 (2.9366)  loss_n_100: 1.6989 (3.0257)  triple_100: 16.9891 (24.4196)  triple_80: 18.2745 (25.6511)  triple_60: 15.8000 (22.1336)  triple_40: 10.6942 (15.2365)  time: 5.8958  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2]  [1723/1724]  eta: 0:00:05  lr: 0.000020  loss: 68.2064 (98.9450)  loss_n_40: 1.6213 (2.8213)  loss_n_60: 1.5295 (2.7730)  loss_n_80: 1.6058 (2.9343)  loss_n_100: 1.6564 (3.0232)  triple_100: 16.7923 (24.4055)  triple_80: 18.2277 (25.6369)  triple_60: 15.5634 (22.1215)  triple_40: 10.5335 (15.2293)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:2] Total time: 2:49:29 (5.8990 s / it)\n",
      "Averaged stats: lr: 0.000020  loss: 68.2064 (98.9450)  loss_n_40: 1.6213 (2.8213)  loss_n_60: 1.5295 (2.7730)  loss_n_80: 1.6058 (2.9343)  loss_n_100: 1.6564 (3.0232)  triple_100: 16.7923 (24.4055)  triple_80: 18.2277 (25.6369)  triple_60: 15.5634 (22.1215)  triple_40: 10.5335 (15.2293)\n",
      "Valid: [epoch:2]  [  0/845]  eta: 0:21:36  loss: 55.2241 (55.2241)  loss_n_40: 1.2736 (1.2736)  loss_n_60: 1.1337 (1.1337)  loss_n_80: 1.2177 (1.2177)  loss_n_100: 1.2841 (1.2841)  triple_100: 13.9548 (13.9548)  triple_80: 14.2734 (14.2734)  triple_60: 12.4740 (12.4740)  triple_40: 9.6128 (9.6128)  time: 1.5341  data: 0.5519  max mem: 40153\n",
      "Valid: [epoch:2]  [ 10/845]  eta: 0:14:16  loss: 58.4604 (65.5285)  loss_n_40: 1.3249 (1.6096)  loss_n_60: 1.2359 (1.4886)  loss_n_80: 1.3400 (1.5796)  loss_n_100: 1.3775 (1.6254)  triple_100: 15.2252 (16.3309)  triple_80: 15.6437 (17.2303)  triple_60: 13.3285 (15.0063)  triple_40: 11.0204 (10.6577)  time: 1.0253  data: 0.0503  max mem: 40153\n",
      "Valid: [epoch:2]  [ 20/845]  eta: 0:13:46  loss: 67.0370 (67.4662)  loss_n_40: 1.3276 (1.7042)  loss_n_60: 1.3092 (1.5977)  loss_n_80: 1.4264 (1.6874)  loss_n_100: 1.5162 (1.7295)  triple_100: 16.7498 (16.8632)  triple_80: 17.6875 (17.8463)  triple_60: 15.1931 (15.4713)  triple_40: 10.9580 (10.5667)  time: 0.9749  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [ 30/845]  eta: 0:13:29  loss: 71.1907 (69.7132)  loss_n_40: 1.7390 (1.8067)  loss_n_60: 1.6911 (1.7123)  loss_n_80: 1.8817 (1.7884)  loss_n_100: 1.9740 (1.8278)  triple_100: 17.4703 (17.4015)  triple_80: 18.5446 (18.4585)  triple_60: 16.9480 (16.0800)  triple_40: 10.5188 (10.6380)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [ 40/845]  eta: 0:13:16  loss: 67.8711 (68.6533)  loss_n_40: 1.5505 (1.7502)  loss_n_60: 1.4735 (1.6534)  loss_n_80: 1.6936 (1.7345)  loss_n_100: 1.7586 (1.7751)  triple_100: 17.1783 (17.1686)  triple_80: 17.8527 (18.1587)  triple_60: 15.4635 (15.8181)  triple_40: 10.5188 (10.5946)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [ 50/845]  eta: 0:13:04  loss: 65.5256 (68.3356)  loss_n_40: 1.4306 (1.7481)  loss_n_60: 1.4275 (1.6347)  loss_n_80: 1.5112 (1.7114)  loss_n_100: 1.5639 (1.7464)  triple_100: 16.9083 (17.0058)  triple_80: 17.4518 (18.0271)  triple_60: 15.1047 (15.7332)  triple_40: 10.7870 (10.7288)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [ 60/845]  eta: 0:12:52  loss: 71.1219 (69.1398)  loss_n_40: 1.4308 (1.7569)  loss_n_60: 1.3932 (1.6463)  loss_n_80: 1.5729 (1.7269)  loss_n_100: 1.6555 (1.7651)  triple_100: 17.5357 (17.2511)  triple_80: 18.9186 (18.2596)  triple_60: 15.9341 (15.9303)  triple_40: 10.9516 (10.8037)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [ 70/845]  eta: 0:12:42  loss: 69.3945 (69.6281)  loss_n_40: 1.4308 (1.7583)  loss_n_60: 1.3617 (1.6538)  loss_n_80: 1.5714 (1.7342)  loss_n_100: 1.6184 (1.7728)  triple_100: 17.5357 (17.3618)  triple_80: 18.9141 (18.3823)  triple_60: 14.9971 (16.0704)  triple_40: 10.5250 (10.8945)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [ 80/845]  eta: 0:12:31  loss: 65.2537 (70.0200)  loss_n_40: 1.5611 (1.7757)  loss_n_60: 1.5507 (1.6741)  loss_n_80: 1.7250 (1.7558)  loss_n_100: 1.8427 (1.7978)  triple_100: 16.6471 (17.5153)  triple_80: 17.5037 (18.5124)  triple_60: 14.7919 (16.1792)  triple_40: 10.4008 (10.8097)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [ 90/845]  eta: 0:12:21  loss: 67.2120 (70.1516)  loss_n_40: 1.6673 (1.7657)  loss_n_60: 1.6403 (1.6634)  loss_n_80: 1.7614 (1.7485)  loss_n_100: 1.8898 (1.7929)  triple_100: 16.9769 (17.5633)  triple_80: 18.3424 (18.5403)  triple_60: 15.5434 (16.1820)  triple_40: 10.6149 (10.8956)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [100/845]  eta: 0:12:10  loss: 68.2056 (70.0656)  loss_n_40: 1.6297 (1.7626)  loss_n_60: 1.5681 (1.6588)  loss_n_80: 1.7067 (1.7453)  loss_n_100: 1.8233 (1.7915)  triple_100: 16.9769 (17.5596)  triple_80: 18.6166 (18.5200)  triple_60: 16.0078 (16.1645)  triple_40: 10.8447 (10.8634)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [110/845]  eta: 0:12:00  loss: 62.6436 (69.8697)  loss_n_40: 1.4328 (1.7420)  loss_n_60: 1.4100 (1.6367)  loss_n_80: 1.6084 (1.7263)  loss_n_100: 1.6862 (1.7721)  triple_100: 15.8068 (17.5262)  triple_80: 16.4755 (18.4710)  triple_60: 13.9569 (16.0898)  triple_40: 10.9493 (10.9056)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [120/845]  eta: 0:11:50  loss: 70.3079 (70.0642)  loss_n_40: 1.4659 (1.7445)  loss_n_60: 1.4166 (1.6435)  loss_n_80: 1.6045 (1.7330)  loss_n_100: 1.6514 (1.7787)  triple_100: 17.8869 (17.5723)  triple_80: 18.5190 (18.5240)  triple_60: 15.7807 (16.1490)  triple_40: 10.9493 (10.9191)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [130/845]  eta: 0:11:40  loss: 70.9834 (70.3672)  loss_n_40: 1.6151 (1.7621)  loss_n_60: 1.5090 (1.6555)  loss_n_80: 1.6407 (1.7420)  loss_n_100: 1.7348 (1.7860)  triple_100: 18.1950 (17.6134)  triple_80: 19.0641 (18.5838)  triple_60: 16.3626 (16.2298)  triple_40: 11.5650 (10.9946)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [140/845]  eta: 0:11:30  loss: 70.9535 (70.5175)  loss_n_40: 1.5285 (1.7698)  loss_n_60: 1.4352 (1.6584)  loss_n_80: 1.6220 (1.7418)  loss_n_100: 1.7146 (1.7851)  triple_100: 17.3505 (17.6141)  triple_80: 18.1791 (18.5919)  triple_60: 15.8269 (16.2894)  triple_40: 11.3981 (11.0671)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [150/845]  eta: 0:11:20  loss: 59.6946 (70.0132)  loss_n_40: 1.4016 (1.7477)  loss_n_60: 1.2084 (1.6364)  loss_n_80: 1.2717 (1.7218)  loss_n_100: 1.3049 (1.7643)  triple_100: 15.3163 (17.4912)  triple_80: 15.6814 (18.4584)  triple_60: 13.7722 (16.1472)  triple_40: 10.7598 (11.0462)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [160/845]  eta: 0:11:10  loss: 59.6946 (70.1892)  loss_n_40: 1.2320 (1.7494)  loss_n_60: 1.1721 (1.6432)  loss_n_80: 1.3025 (1.7300)  loss_n_100: 1.3243 (1.7734)  triple_100: 15.3163 (17.5468)  triple_80: 15.6814 (18.5174)  triple_60: 13.4614 (16.1878)  triple_40: 10.7308 (11.0411)  time: 0.9760  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:2]  [170/845]  eta: 0:11:00  loss: 71.6256 (70.4000)  loss_n_40: 1.5743 (1.7548)  loss_n_60: 1.6639 (1.6512)  loss_n_80: 1.8231 (1.7387)  loss_n_100: 1.8605 (1.7824)  triple_100: 17.4385 (17.5939)  triple_80: 18.1917 (18.5803)  triple_60: 16.7916 (16.2437)  triple_40: 11.0520 (11.0549)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [180/845]  eta: 0:10:50  loss: 71.5601 (70.6533)  loss_n_40: 1.5622 (1.7514)  loss_n_60: 1.5590 (1.6530)  loss_n_80: 1.7544 (1.7432)  loss_n_100: 1.8361 (1.7872)  triple_100: 18.2504 (17.6824)  triple_80: 19.0969 (18.6669)  triple_60: 16.7916 (16.3007)  triple_40: 11.2447 (11.0684)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [190/845]  eta: 0:10:41  loss: 72.7587 (70.6645)  loss_n_40: 1.5115 (1.7504)  loss_n_60: 1.5552 (1.6548)  loss_n_80: 1.6110 (1.7461)  loss_n_100: 1.6626 (1.7907)  triple_100: 18.2504 (17.6938)  triple_80: 19.2595 (18.6846)  triple_60: 16.1928 (16.3050)  triple_40: 11.2447 (11.0390)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [200/845]  eta: 0:10:31  loss: 72.5297 (70.8306)  loss_n_40: 1.6068 (1.7566)  loss_n_60: 1.6314 (1.6616)  loss_n_80: 1.7982 (1.7527)  loss_n_100: 1.9076 (1.7969)  triple_100: 18.5845 (17.7244)  triple_80: 19.3937 (18.7304)  triple_60: 16.2276 (16.3455)  triple_40: 10.6987 (11.0625)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [210/845]  eta: 0:10:21  loss: 71.5704 (70.7502)  loss_n_40: 1.4915 (1.7503)  loss_n_60: 1.4406 (1.6549)  loss_n_80: 1.6535 (1.7479)  loss_n_100: 1.7257 (1.7914)  triple_100: 18.3223 (17.7138)  triple_80: 19.3597 (18.7183)  triple_60: 15.5375 (16.3119)  triple_40: 10.6987 (11.0618)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [220/845]  eta: 0:10:11  loss: 66.6663 (70.5585)  loss_n_40: 1.4820 (1.7482)  loss_n_60: 1.4302 (1.6547)  loss_n_80: 1.5298 (1.7476)  loss_n_100: 1.6141 (1.7914)  triple_100: 16.8176 (17.6670)  triple_80: 16.9214 (18.6728)  triple_60: 15.2718 (16.2703)  triple_40: 10.5128 (11.0065)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [230/845]  eta: 0:10:01  loss: 65.5922 (70.2662)  loss_n_40: 1.4364 (1.7377)  loss_n_60: 1.3058 (1.6419)  loss_n_80: 1.4502 (1.7348)  loss_n_100: 1.4923 (1.7778)  triple_100: 15.8737 (17.5837)  triple_80: 16.9214 (18.5876)  triple_60: 14.5186 (16.1908)  triple_40: 10.1903 (11.0119)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [240/845]  eta: 0:09:51  loss: 64.6025 (70.3361)  loss_n_40: 1.4186 (1.7427)  loss_n_60: 1.2898 (1.6489)  loss_n_80: 1.4502 (1.7416)  loss_n_100: 1.4887 (1.7844)  triple_100: 15.9480 (17.5924)  triple_80: 17.3461 (18.6139)  triple_60: 14.5186 (16.2112)  triple_40: 10.6282 (11.0010)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [250/845]  eta: 0:09:41  loss: 64.4834 (70.0720)  loss_n_40: 1.4078 (1.7304)  loss_n_60: 1.3606 (1.6393)  loss_n_80: 1.4585 (1.7334)  loss_n_100: 1.5486 (1.7768)  triple_100: 15.9480 (17.5423)  triple_80: 17.0730 (18.5529)  triple_60: 14.6318 (16.1355)  triple_40: 10.3624 (10.9613)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [260/845]  eta: 0:09:32  loss: 61.3820 (70.1935)  loss_n_40: 1.4078 (1.7347)  loss_n_60: 1.3704 (1.6469)  loss_n_80: 1.4680 (1.7404)  loss_n_100: 1.5578 (1.7837)  triple_100: 15.7029 (17.5678)  triple_80: 16.2256 (18.5893)  triple_60: 14.0226 (16.1746)  triple_40: 10.3624 (10.9562)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [270/845]  eta: 0:09:22  loss: 67.2425 (70.2938)  loss_n_40: 1.5544 (1.7344)  loss_n_60: 1.5868 (1.6478)  loss_n_80: 1.6883 (1.7421)  loss_n_100: 1.7525 (1.7861)  triple_100: 16.6785 (17.5994)  triple_80: 18.0751 (18.6218)  triple_60: 16.0026 (16.1975)  triple_40: 11.0972 (10.9647)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [280/845]  eta: 0:09:12  loss: 67.2425 (70.1433)  loss_n_40: 1.5088 (1.7245)  loss_n_60: 1.5390 (1.6388)  loss_n_80: 1.6923 (1.7347)  loss_n_100: 1.7771 (1.7790)  triple_100: 17.6382 (17.5721)  triple_80: 18.0751 (18.5878)  triple_60: 16.0026 (16.1531)  triple_40: 11.0972 (10.9533)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [290/845]  eta: 0:09:02  loss: 67.1067 (70.2173)  loss_n_40: 1.5088 (1.7301)  loss_n_60: 1.5390 (1.6456)  loss_n_80: 1.6923 (1.7416)  loss_n_100: 1.7771 (1.7857)  triple_100: 16.8772 (17.5922)  triple_80: 17.9801 (18.6147)  triple_60: 15.0137 (16.1683)  triple_40: 10.3605 (10.9392)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [300/845]  eta: 0:08:52  loss: 66.9914 (70.0682)  loss_n_40: 1.3550 (1.7226)  loss_n_60: 1.3351 (1.6398)  loss_n_80: 1.4158 (1.7369)  loss_n_100: 1.4821 (1.7811)  triple_100: 16.7761 (17.5594)  triple_80: 17.6707 (18.5808)  triple_60: 15.1273 (16.1240)  triple_40: 10.2022 (10.9234)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [310/845]  eta: 0:08:43  loss: 59.3953 (69.7329)  loss_n_40: 1.2659 (1.7084)  loss_n_60: 1.2020 (1.6246)  loss_n_80: 1.3223 (1.7224)  loss_n_100: 1.3284 (1.7664)  triple_100: 14.9938 (17.4787)  triple_80: 15.4518 (18.4874)  triple_60: 13.3857 (16.0357)  triple_40: 10.5069 (10.9095)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [320/845]  eta: 0:08:33  loss: 59.3953 (69.9446)  loss_n_40: 1.3149 (1.7211)  loss_n_60: 1.2151 (1.6362)  loss_n_80: 1.3223 (1.7321)  loss_n_100: 1.3284 (1.7754)  triple_100: 15.2196 (17.5190)  triple_80: 15.4624 (18.5390)  triple_60: 13.2359 (16.0949)  triple_40: 10.8883 (10.9269)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [330/845]  eta: 0:08:23  loss: 74.3895 (69.8552)  loss_n_40: 1.4944 (1.7167)  loss_n_60: 1.4907 (1.6315)  loss_n_80: 1.6982 (1.7277)  loss_n_100: 1.7795 (1.7716)  triple_100: 19.0420 (17.5043)  triple_80: 20.0696 (18.5137)  triple_60: 16.5639 (16.0716)  triple_40: 11.0966 (10.9181)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [340/845]  eta: 0:08:13  loss: 65.3495 (69.7510)  loss_n_40: 1.4044 (1.7123)  loss_n_60: 1.3478 (1.6270)  loss_n_80: 1.4677 (1.7238)  loss_n_100: 1.5561 (1.7684)  triple_100: 16.1927 (17.4849)  triple_80: 17.6963 (18.4885)  triple_60: 14.5161 (16.0440)  triple_40: 10.3241 (10.9022)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [350/845]  eta: 0:08:03  loss: 65.3495 (69.5655)  loss_n_40: 1.3872 (1.7024)  loss_n_60: 1.3242 (1.6175)  loss_n_80: 1.5376 (1.7156)  loss_n_100: 1.5798 (1.7603)  triple_100: 16.1927 (17.4455)  triple_80: 17.4918 (18.4442)  triple_60: 14.3262 (15.9902)  triple_40: 10.7874 (10.8899)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [360/845]  eta: 0:07:54  loss: 67.2658 (69.7208)  loss_n_40: 1.5050 (1.7147)  loss_n_60: 1.4725 (1.6290)  loss_n_80: 1.6374 (1.7258)  loss_n_100: 1.7116 (1.7694)  triple_100: 17.4579 (17.4665)  triple_80: 18.5522 (18.4818)  triple_60: 14.7435 (16.0355)  triple_40: 10.8620 (10.8980)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [370/845]  eta: 0:07:44  loss: 73.1091 (69.7542)  loss_n_40: 1.5843 (1.7118)  loss_n_60: 1.6056 (1.6274)  loss_n_80: 1.8111 (1.7257)  loss_n_100: 1.8340 (1.7695)  triple_100: 18.4470 (17.4809)  triple_80: 19.5449 (18.4987)  triple_60: 16.3987 (16.0378)  triple_40: 10.9848 (10.9023)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [380/845]  eta: 0:07:34  loss: 64.3365 (69.5518)  loss_n_40: 1.4671 (1.7053)  loss_n_60: 1.3902 (1.6211)  loss_n_80: 1.5500 (1.7196)  loss_n_100: 1.6258 (1.7640)  triple_100: 15.5909 (17.4395)  triple_80: 17.2253 (18.4454)  triple_60: 14.6697 (15.9854)  triple_40: 10.4349 (10.8714)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [390/845]  eta: 0:07:24  loss: 67.1959 (69.6492)  loss_n_40: 1.4671 (1.7096)  loss_n_60: 1.5199 (1.6269)  loss_n_80: 1.6355 (1.7251)  loss_n_100: 1.7276 (1.7693)  triple_100: 16.8011 (17.4602)  triple_80: 17.9385 (18.4769)  triple_60: 15.1097 (16.0160)  triple_40: 10.0452 (10.8651)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [400/845]  eta: 0:07:14  loss: 67.8850 (69.5398)  loss_n_40: 1.6391 (1.7070)  loss_n_60: 1.5857 (1.6259)  loss_n_80: 1.7344 (1.7242)  loss_n_100: 1.8316 (1.7684)  triple_100: 16.8011 (17.4328)  triple_80: 18.2494 (18.4526)  triple_60: 15.4519 (15.9913)  triple_40: 10.0452 (10.8377)  time: 0.9757  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:2]  [410/845]  eta: 0:07:05  loss: 63.3031 (69.5080)  loss_n_40: 1.3575 (1.7068)  loss_n_60: 1.2876 (1.6248)  loss_n_80: 1.3806 (1.7228)  loss_n_100: 1.4593 (1.7668)  triple_100: 15.9566 (17.4190)  triple_80: 16.8463 (18.4414)  triple_60: 14.2159 (15.9864)  triple_40: 10.0605 (10.8400)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [420/845]  eta: 0:06:55  loss: 67.5376 (69.7298)  loss_n_40: 1.6803 (1.7129)  loss_n_60: 1.6400 (1.6304)  loss_n_80: 1.8383 (1.7287)  loss_n_100: 1.9424 (1.7732)  triple_100: 17.2422 (17.4779)  triple_80: 18.2372 (18.4995)  triple_60: 16.3181 (16.0374)  triple_40: 10.9034 (10.8698)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [430/845]  eta: 0:06:45  loss: 68.4588 (69.6580)  loss_n_40: 1.4620 (1.7108)  loss_n_60: 1.3827 (1.6275)  loss_n_80: 1.5557 (1.7258)  loss_n_100: 1.6180 (1.7701)  triple_100: 17.8552 (17.4618)  triple_80: 18.3821 (18.4799)  triple_60: 15.4165 (16.0184)  triple_40: 11.0599 (10.8638)  time: 0.9758  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:2]  [440/845]  eta: 0:06:35  loss: 64.7699 (69.5846)  loss_n_40: 1.4194 (1.7077)  loss_n_60: 1.2890 (1.6256)  loss_n_80: 1.4418 (1.7244)  loss_n_100: 1.4947 (1.7691)  triple_100: 16.4904 (17.4543)  triple_80: 17.3348 (18.4663)  triple_60: 14.4799 (15.9970)  triple_40: 10.1992 (10.8402)  time: 0.9761  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:2]  [450/845]  eta: 0:06:25  loss: 65.7880 (69.5347)  loss_n_40: 1.4212 (1.7027)  loss_n_60: 1.3657 (1.6214)  loss_n_80: 1.5768 (1.7211)  loss_n_100: 1.6196 (1.7660)  triple_100: 16.9304 (17.4521)  triple_80: 17.6801 (18.4563)  triple_60: 14.8135 (15.9802)  triple_40: 10.2753 (10.8350)  time: 0.9762  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:2]  [460/845]  eta: 0:06:16  loss: 63.8837 (69.4669)  loss_n_40: 1.3981 (1.6978)  loss_n_60: 1.3136 (1.6167)  loss_n_80: 1.4355 (1.7165)  loss_n_100: 1.4626 (1.7610)  triple_100: 16.3746 (17.4351)  triple_80: 17.3716 (18.4364)  triple_60: 14.8135 (15.9614)  triple_40: 11.0526 (10.8420)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [470/845]  eta: 0:06:06  loss: 67.8037 (69.5082)  loss_n_40: 1.5471 (1.6995)  loss_n_60: 1.4406 (1.6185)  loss_n_80: 1.6281 (1.7183)  loss_n_100: 1.7221 (1.7632)  triple_100: 17.6742 (17.4461)  triple_80: 18.0939 (18.4477)  triple_60: 14.6263 (15.9728)  triple_40: 11.0590 (10.8421)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [480/845]  eta: 0:05:56  loss: 74.8007 (69.5824)  loss_n_40: 1.5834 (1.6963)  loss_n_60: 1.5503 (1.6166)  loss_n_80: 1.7107 (1.7179)  loss_n_100: 1.8180 (1.7637)  triple_100: 19.0065 (17.4793)  triple_80: 19.5882 (18.4735)  triple_60: 17.0858 (15.9833)  triple_40: 11.2368 (10.8519)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [490/845]  eta: 0:05:46  loss: 74.8007 (69.6914)  loss_n_40: 1.6096 (1.7027)  loss_n_60: 1.5657 (1.6231)  loss_n_80: 1.7673 (1.7235)  loss_n_100: 1.8651 (1.7690)  triple_100: 19.1484 (17.5043)  triple_80: 19.9503 (18.5023)  triple_60: 17.0858 (16.0141)  triple_40: 10.8020 (10.8522)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [500/845]  eta: 0:05:37  loss: 74.1349 (69.7253)  loss_n_40: 1.6999 (1.7066)  loss_n_60: 1.7276 (1.6258)  loss_n_80: 1.8755 (1.7256)  loss_n_100: 2.0102 (1.7705)  triple_100: 18.6557 (17.5046)  triple_80: 19.9148 (18.5092)  triple_60: 17.5385 (16.0232)  triple_40: 10.5706 (10.8598)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [510/845]  eta: 0:05:27  loss: 71.1226 (69.7200)  loss_n_40: 1.5682 (1.7067)  loss_n_60: 1.5659 (1.6263)  loss_n_80: 1.7399 (1.7261)  loss_n_100: 1.8703 (1.7708)  triple_100: 17.9254 (17.5035)  triple_80: 19.0491 (18.5102)  triple_60: 16.2237 (16.0240)  triple_40: 10.5173 (10.8523)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [520/845]  eta: 0:05:17  loss: 69.4977 (69.6883)  loss_n_40: 1.4286 (1.7047)  loss_n_60: 1.4553 (1.6254)  loss_n_80: 1.6556 (1.7253)  loss_n_100: 1.7364 (1.7700)  triple_100: 17.7482 (17.5002)  triple_80: 18.6322 (18.5054)  triple_60: 15.8124 (16.0155)  triple_40: 10.3674 (10.8418)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [530/845]  eta: 0:05:07  loss: 69.0678 (69.7183)  loss_n_40: 1.5244 (1.7061)  loss_n_60: 1.4718 (1.6270)  loss_n_80: 1.6672 (1.7269)  loss_n_100: 1.7581 (1.7716)  triple_100: 17.7482 (17.5068)  triple_80: 18.3609 (18.5132)  triple_60: 15.5788 (16.0212)  triple_40: 10.7713 (10.8456)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [540/845]  eta: 0:04:57  loss: 63.5858 (69.6379)  loss_n_40: 1.4914 (1.7019)  loss_n_60: 1.4667 (1.6224)  loss_n_80: 1.6363 (1.7226)  loss_n_100: 1.7107 (1.7676)  triple_100: 16.1997 (17.4899)  triple_80: 16.9918 (18.4919)  triple_60: 14.3948 (16.0003)  triple_40: 10.9122 (10.8414)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [550/845]  eta: 0:04:48  loss: 68.0574 (69.6618)  loss_n_40: 1.4878 (1.7034)  loss_n_60: 1.4999 (1.6236)  loss_n_80: 1.6084 (1.7240)  loss_n_100: 1.7075 (1.7695)  triple_100: 17.0331 (17.4974)  triple_80: 17.9750 (18.4982)  triple_60: 15.1651 (16.0049)  triple_40: 10.6271 (10.8409)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [560/845]  eta: 0:04:38  loss: 68.0574 (69.5801)  loss_n_40: 1.4625 (1.6982)  loss_n_60: 1.3807 (1.6179)  loss_n_80: 1.5735 (1.7192)  loss_n_100: 1.6308 (1.7648)  triple_100: 17.5544 (17.4818)  triple_80: 17.9750 (18.4775)  triple_60: 15.1651 (15.9788)  triple_40: 10.8494 (10.8419)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [570/845]  eta: 0:04:28  loss: 69.8914 (69.7395)  loss_n_40: 1.4702 (1.7014)  loss_n_60: 1.4449 (1.6226)  loss_n_80: 1.6286 (1.7248)  loss_n_100: 1.6816 (1.7707)  triple_100: 17.8899 (17.5275)  triple_80: 18.9160 (18.5279)  triple_60: 15.3857 (16.0141)  triple_40: 11.1884 (10.8505)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [580/845]  eta: 0:04:18  loss: 74.0713 (69.6812)  loss_n_40: 1.5555 (1.6981)  loss_n_60: 1.4934 (1.6191)  loss_n_80: 1.7112 (1.7214)  loss_n_100: 1.8027 (1.7673)  triple_100: 19.3786 (17.5137)  triple_80: 20.1247 (18.5098)  triple_60: 16.3043 (15.9999)  triple_40: 11.3710 (10.8517)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [590/845]  eta: 0:04:09  loss: 75.5924 (69.8583)  loss_n_40: 1.6462 (1.7059)  loss_n_60: 1.6304 (1.6258)  loss_n_80: 1.7637 (1.7274)  loss_n_100: 1.8801 (1.7732)  triple_100: 19.1074 (17.5484)  triple_80: 19.9730 (18.5525)  triple_60: 16.9735 (16.0465)  triple_40: 11.3710 (10.8785)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [600/845]  eta: 0:03:59  loss: 77.6619 (69.9183)  loss_n_40: 1.6689 (1.7068)  loss_n_60: 1.6786 (1.6266)  loss_n_80: 1.8629 (1.7285)  loss_n_100: 1.9924 (1.7744)  triple_100: 19.2988 (17.5649)  triple_80: 20.6713 (18.5684)  triple_60: 18.1811 (16.0579)  triple_40: 11.4029 (10.8910)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [610/845]  eta: 0:03:49  loss: 72.7000 (69.9442)  loss_n_40: 1.5899 (1.7072)  loss_n_60: 1.5988 (1.6276)  loss_n_80: 1.7594 (1.7299)  loss_n_100: 1.8967 (1.7763)  triple_100: 18.7896 (17.5784)  triple_80: 19.8068 (18.5791)  triple_60: 16.1962 (16.0612)  triple_40: 10.8310 (10.8845)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [620/845]  eta: 0:03:39  loss: 73.2451 (69.9972)  loss_n_40: 1.5970 (1.7080)  loss_n_60: 1.5851 (1.6281)  loss_n_80: 1.7319 (1.7307)  loss_n_100: 1.8385 (1.7773)  triple_100: 18.9300 (17.5930)  triple_80: 19.7381 (18.5930)  triple_60: 16.4315 (16.0708)  triple_40: 11.0886 (10.8964)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [630/845]  eta: 0:03:30  loss: 65.6973 (70.0130)  loss_n_40: 1.4369 (1.7084)  loss_n_60: 1.3327 (1.6273)  loss_n_80: 1.5116 (1.7297)  loss_n_100: 1.5789 (1.7761)  triple_100: 16.7921 (17.5940)  triple_80: 17.8197 (18.5940)  triple_60: 14.4699 (16.0739)  triple_40: 11.0769 (10.9096)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [640/845]  eta: 0:03:20  loss: 61.2376 (69.9041)  loss_n_40: 1.2769 (1.7061)  loss_n_60: 1.1886 (1.6237)  loss_n_80: 1.3239 (1.7259)  loss_n_100: 1.3790 (1.7722)  triple_100: 15.2608 (17.5685)  triple_80: 16.1289 (18.5617)  triple_60: 13.3566 (16.0461)  triple_40: 10.3316 (10.8999)  time: 0.9753  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:2]  [650/845]  eta: 0:03:10  loss: 64.9962 (69.9479)  loss_n_40: 1.4938 (1.7067)  loss_n_60: 1.4829 (1.6242)  loss_n_80: 1.6286 (1.7267)  loss_n_100: 1.7035 (1.7732)  triple_100: 16.3134 (17.5826)  triple_80: 17.5436 (18.5747)  triple_60: 14.7439 (16.0569)  triple_40: 10.4721 (10.9029)  time: 0.9753  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [660/845]  eta: 0:03:00  loss: 71.8758 (69.9743)  loss_n_40: 1.5217 (1.7086)  loss_n_60: 1.4954 (1.6262)  loss_n_80: 1.6822 (1.7281)  loss_n_100: 1.7641 (1.7744)  triple_100: 18.4594 (17.5859)  triple_80: 19.3317 (18.5803)  triple_60: 16.2171 (16.0666)  triple_40: 10.9595 (10.9043)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [670/845]  eta: 0:02:50  loss: 71.8758 (69.9744)  loss_n_40: 1.4442 (1.7060)  loss_n_60: 1.3753 (1.6242)  loss_n_80: 1.4933 (1.7269)  loss_n_100: 1.5389 (1.7733)  triple_100: 17.7344 (17.5900)  triple_80: 18.4849 (18.5829)  triple_60: 16.2171 (16.0615)  triple_40: 11.4466 (10.9095)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [680/845]  eta: 0:02:41  loss: 73.1014 (70.1633)  loss_n_40: 1.5670 (1.7127)  loss_n_60: 1.5084 (1.6306)  loss_n_80: 1.6734 (1.7328)  loss_n_100: 1.7790 (1.7790)  triple_100: 18.9631 (17.6296)  triple_80: 19.8103 (18.6292)  triple_60: 16.3428 (16.1107)  triple_40: 11.7379 (10.9387)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [690/845]  eta: 0:02:31  loss: 76.4468 (70.2162)  loss_n_40: 1.7106 (1.7147)  loss_n_60: 1.6747 (1.6329)  loss_n_80: 1.8388 (1.7350)  loss_n_100: 1.9253 (1.7812)  triple_100: 19.3733 (17.6397)  triple_80: 20.2796 (18.6428)  triple_60: 17.1473 (16.1249)  triple_40: 11.3117 (10.9451)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [700/845]  eta: 0:02:21  loss: 65.4987 (70.1704)  loss_n_40: 1.5484 (1.7129)  loss_n_60: 1.5471 (1.6303)  loss_n_80: 1.6105 (1.7325)  loss_n_100: 1.6712 (1.7788)  triple_100: 16.8435 (17.6306)  triple_80: 17.4147 (18.6301)  triple_60: 14.7061 (16.1112)  triple_40: 10.7139 (10.9440)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [710/845]  eta: 0:02:11  loss: 61.7538 (70.0909)  loss_n_40: 1.3771 (1.7112)  loss_n_60: 1.2759 (1.6281)  loss_n_80: 1.3970 (1.7303)  loss_n_100: 1.4833 (1.7767)  triple_100: 16.0248 (17.6128)  triple_80: 16.4325 (18.6081)  triple_60: 13.8001 (16.0918)  triple_40: 10.3014 (10.9318)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [720/845]  eta: 0:02:02  loss: 61.9155 (70.0325)  loss_n_40: 1.3771 (1.7095)  loss_n_60: 1.2571 (1.6265)  loss_n_80: 1.3908 (1.7289)  loss_n_100: 1.4634 (1.7752)  triple_100: 15.3082 (17.5965)  triple_80: 16.2458 (18.5939)  triple_60: 13.8611 (16.0767)  triple_40: 10.2036 (10.9253)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [730/845]  eta: 0:01:52  loss: 67.1535 (70.0338)  loss_n_40: 1.5021 (1.7089)  loss_n_60: 1.4578 (1.6259)  loss_n_80: 1.6454 (1.7284)  loss_n_100: 1.7053 (1.7747)  triple_100: 17.5106 (17.5980)  triple_80: 18.3972 (18.5957)  triple_60: 15.0552 (16.0780)  triple_40: 10.5891 (10.9241)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [740/845]  eta: 0:01:42  loss: 68.5463 (70.0543)  loss_n_40: 1.4976 (1.7086)  loss_n_60: 1.3657 (1.6249)  loss_n_80: 1.5762 (1.7273)  loss_n_100: 1.6364 (1.7734)  triple_100: 17.5333 (17.6014)  triple_80: 18.5151 (18.5977)  triple_60: 15.1523 (16.0803)  triple_40: 11.0975 (10.9406)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [750/845]  eta: 0:01:32  loss: 68.5463 (70.0712)  loss_n_40: 1.4192 (1.7087)  loss_n_60: 1.3342 (1.6255)  loss_n_80: 1.4752 (1.7277)  loss_n_100: 1.5160 (1.7735)  triple_100: 17.5333 (17.6041)  triple_80: 18.5151 (18.6012)  triple_60: 15.1523 (16.0855)  triple_40: 11.0939 (10.9451)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [760/845]  eta: 0:01:23  loss: 74.6466 (70.1298)  loss_n_40: 1.5069 (1.7109)  loss_n_60: 1.5137 (1.6287)  loss_n_80: 1.7218 (1.7309)  loss_n_100: 1.7877 (1.7768)  triple_100: 19.5441 (17.6206)  triple_80: 20.2361 (18.6202)  triple_60: 16.5000 (16.1003)  triple_40: 10.8848 (10.9415)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [770/845]  eta: 0:01:13  loss: 70.8450 (70.1523)  loss_n_40: 1.5218 (1.7117)  loss_n_60: 1.5229 (1.6296)  loss_n_80: 1.7267 (1.7317)  loss_n_100: 1.8095 (1.7777)  triple_100: 18.3160 (17.6283)  triple_80: 18.5372 (18.6257)  triple_60: 16.0052 (16.1060)  triple_40: 10.7992 (10.9416)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [780/845]  eta: 0:01:03  loss: 66.7651 (70.1061)  loss_n_40: 1.4754 (1.7102)  loss_n_60: 1.4615 (1.6281)  loss_n_80: 1.5952 (1.7304)  loss_n_100: 1.6871 (1.7766)  triple_100: 17.1124 (17.6230)  triple_80: 17.9111 (18.6154)  triple_60: 14.9082 (16.0921)  triple_40: 10.4222 (10.9303)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [790/845]  eta: 0:00:53  loss: 69.6695 (70.1051)  loss_n_40: 1.4171 (1.7074)  loss_n_60: 1.3575 (1.6258)  loss_n_80: 1.5649 (1.7289)  loss_n_100: 1.6077 (1.7756)  triple_100: 17.8433 (17.6318)  triple_80: 18.4920 (18.6187)  triple_60: 15.4183 (16.0870)  triple_40: 10.5002 (10.9299)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [800/845]  eta: 0:00:43  loss: 70.8753 (70.1070)  loss_n_40: 1.4286 (1.7069)  loss_n_60: 1.3575 (1.6266)  loss_n_80: 1.5649 (1.7300)  loss_n_100: 1.6077 (1.7767)  triple_100: 18.1467 (17.6359)  triple_80: 19.1821 (18.6236)  triple_60: 15.4183 (16.0876)  triple_40: 10.6031 (10.9197)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [810/845]  eta: 0:00:34  loss: 68.3317 (70.1032)  loss_n_40: 1.4747 (1.7070)  loss_n_60: 1.4579 (1.6265)  loss_n_80: 1.5747 (1.7299)  loss_n_100: 1.6308 (1.7764)  triple_100: 17.4221 (17.6351)  triple_80: 18.2948 (18.6230)  triple_60: 14.9417 (16.0870)  triple_40: 10.5737 (10.9182)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [820/845]  eta: 0:00:24  loss: 69.1602 (70.1710)  loss_n_40: 1.5361 (1.7088)  loss_n_60: 1.4758 (1.6290)  loss_n_80: 1.6716 (1.7326)  loss_n_100: 1.7349 (1.7793)  triple_100: 17.8045 (17.6544)  triple_80: 18.7092 (18.6437)  triple_60: 15.4947 (16.1041)  triple_40: 10.9998 (10.9192)  time: 0.9756  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:2]  [830/845]  eta: 0:00:14  loss: 71.7426 (70.2116)  loss_n_40: 1.5536 (1.7077)  loss_n_60: 1.4542 (1.6280)  loss_n_80: 1.6476 (1.7322)  loss_n_100: 1.7349 (1.7791)  triple_100: 18.5564 (17.6693)  triple_80: 19.5090 (18.6561)  triple_60: 15.9588 (16.1112)  triple_40: 11.1052 (10.9280)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [840/845]  eta: 0:00:04  loss: 71.9840 (70.2427)  loss_n_40: 1.5596 (1.7080)  loss_n_60: 1.5774 (1.6283)  loss_n_80: 1.6750 (1.7328)  loss_n_100: 1.7462 (1.7797)  triple_100: 18.9165 (17.6796)  triple_80: 19.5090 (18.6658)  triple_60: 16.1552 (16.1188)  triple_40: 11.3810 (10.9296)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2]  [844/845]  eta: 0:00:00  loss: 67.4998 (70.2330)  loss_n_40: 1.4930 (1.7068)  loss_n_60: 1.4296 (1.6271)  loss_n_80: 1.6064 (1.7317)  loss_n_100: 1.6667 (1.7786)  triple_100: 17.7301 (17.6776)  triple_80: 18.6029 (18.6628)  triple_60: 14.8922 (16.1155)  triple_40: 11.2571 (10.9329)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:2] Total time: 0:13:45 (0.9767 s / it)\n",
      "Averaged stats: loss: 67.4998 (70.2330)  loss_n_40: 1.4930 (1.7068)  loss_n_60: 1.4296 (1.6271)  loss_n_80: 1.6064 (1.7317)  loss_n_100: 1.6667 (1.7786)  triple_100: 17.7301 (17.6776)  triple_80: 18.6029 (18.6628)  triple_60: 14.8922 (16.1155)  triple_40: 11.2571 (10.9329)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle_2/low2high/epoch_2_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 1.779%\n",
      "Min loss_n_100: 1.779\n",
      "Best Epoch: 2.000\n",
      "Train: [epoch:3]  [   0/1724]  eta: 3:01:44  lr: 0.000040  loss: 61.2764 (61.2764)  loss_n_40: 1.4143 (1.4143)  loss_n_60: 1.3535 (1.3535)  loss_n_80: 1.4681 (1.4681)  loss_n_100: 1.5086 (1.5086)  triple_100: 15.3536 (15.3536)  triple_80: 16.2780 (16.2780)  triple_60: 13.8391 (13.8391)  triple_40: 10.0612 (10.0612)  time: 6.3253  data: 0.5990  max mem: 40153\n",
      "Train: [epoch:3]  [  10/1724]  eta: 2:49:26  lr: 0.000040  loss: 72.9521 (70.9446)  loss_n_40: 1.8355 (1.7429)  loss_n_60: 1.7308 (1.6674)  loss_n_80: 1.7670 (1.7619)  loss_n_100: 1.8145 (1.8106)  triple_100: 17.9680 (17.9008)  triple_80: 19.0761 (18.8567)  triple_60: 16.6711 (16.3110)  triple_40: 10.9705 (10.8933)  time: 5.9312  data: 0.0546  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [  20/1724]  eta: 2:47:53  lr: 0.000040  loss: 68.7338 (68.4915)  loss_n_40: 1.7128 (1.6878)  loss_n_60: 1.5914 (1.5950)  loss_n_80: 1.6774 (1.6936)  loss_n_100: 1.7383 (1.7437)  triple_100: 17.1743 (17.2914)  triple_80: 17.9745 (18.1815)  triple_60: 15.7980 (15.6754)  triple_40: 10.7391 (10.6229)  time: 5.8911  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [  30/1724]  eta: 2:46:43  lr: 0.000040  loss: 66.9315 (68.5763)  loss_n_40: 1.5933 (1.6942)  loss_n_60: 1.5631 (1.5972)  loss_n_80: 1.6551 (1.6976)  loss_n_100: 1.7135 (1.7494)  triple_100: 17.2481 (17.3536)  triple_80: 17.9745 (18.2108)  triple_60: 15.4212 (15.6793)  triple_40: 10.5471 (10.5942)  time: 5.8909  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [  40/1724]  eta: 2:45:38  lr: 0.000040  loss: 68.3798 (68.1234)  loss_n_40: 1.7119 (1.7031)  loss_n_60: 1.6872 (1.6060)  loss_n_80: 1.7482 (1.7027)  loss_n_100: 1.7726 (1.7528)  triple_100: 17.4291 (17.2123)  triple_80: 18.2034 (18.0733)  triple_60: 15.4985 (15.5601)  triple_40: 10.1208 (10.5132)  time: 5.8915  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [  50/1724]  eta: 2:44:36  lr: 0.000040  loss: 66.7015 (68.0265)  loss_n_40: 1.6305 (1.7052)  loss_n_60: 1.5662 (1.6113)  loss_n_80: 1.6620 (1.7077)  loss_n_100: 1.7269 (1.7593)  triple_100: 16.9409 (17.1798)  triple_80: 17.4303 (18.0569)  triple_60: 14.9561 (15.5468)  triple_40: 10.1023 (10.4595)  time: 5.8914  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [  60/1724]  eta: 2:43:34  lr: 0.000040  loss: 66.6911 (68.4053)  loss_n_40: 1.6305 (1.7297)  loss_n_60: 1.5662 (1.6375)  loss_n_80: 1.6620 (1.7304)  loss_n_100: 1.7126 (1.7810)  triple_100: 17.0662 (17.2272)  triple_80: 17.4303 (18.1626)  triple_60: 15.0294 (15.6460)  triple_40: 10.2738 (10.4910)  time: 5.8913  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [  70/1724]  eta: 2:42:34  lr: 0.000040  loss: 66.8513 (68.1636)  loss_n_40: 1.6797 (1.7237)  loss_n_60: 1.5860 (1.6328)  loss_n_80: 1.7005 (1.7256)  loss_n_100: 1.7209 (1.7753)  triple_100: 17.0255 (17.1435)  triple_80: 17.9009 (18.0944)  triple_60: 15.2403 (15.5918)  triple_40: 10.5816 (10.4765)  time: 5.8919  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [  80/1724]  eta: 2:41:34  lr: 0.000040  loss: 68.2055 (68.4888)  loss_n_40: 1.6797 (1.7309)  loss_n_60: 1.5860 (1.6429)  loss_n_80: 1.7206 (1.7381)  loss_n_100: 1.8203 (1.7885)  triple_100: 17.3118 (17.2343)  triple_80: 18.2397 (18.1989)  triple_60: 15.3476 (15.6536)  triple_40: 10.5314 (10.5017)  time: 5.8916  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [  90/1724]  eta: 2:40:34  lr: 0.000040  loss: 68.9060 (68.5581)  loss_n_40: 1.6650 (1.7204)  loss_n_60: 1.6054 (1.6329)  loss_n_80: 1.7262 (1.7309)  loss_n_100: 1.7835 (1.7821)  triple_100: 17.4321 (17.2735)  triple_80: 18.1607 (18.2211)  triple_60: 15.6254 (15.6489)  triple_40: 10.7867 (10.5483)  time: 5.8916  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 100/1724]  eta: 2:39:34  lr: 0.000040  loss: 67.0299 (68.4066)  loss_n_40: 1.5557 (1.7077)  loss_n_60: 1.4955 (1.6178)  loss_n_80: 1.5556 (1.7159)  loss_n_100: 1.6087 (1.7671)  triple_100: 16.9391 (17.2376)  triple_80: 17.5889 (18.1603)  triple_60: 15.1114 (15.6091)  triple_40: 11.0093 (10.5912)  time: 5.8919  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 110/1724]  eta: 2:38:35  lr: 0.000040  loss: 65.3586 (68.1030)  loss_n_40: 1.6690 (1.7124)  loss_n_60: 1.5268 (1.6140)  loss_n_80: 1.6364 (1.7138)  loss_n_100: 1.7039 (1.7649)  triple_100: 16.4069 (17.1475)  triple_80: 17.1064 (18.0422)  triple_60: 14.8998 (15.5255)  triple_40: 10.3145 (10.5826)  time: 5.8922  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 120/1724]  eta: 2:37:35  lr: 0.000040  loss: 63.7789 (67.8884)  loss_n_40: 1.6979 (1.7191)  loss_n_60: 1.5565 (1.6148)  loss_n_80: 1.6487 (1.7105)  loss_n_100: 1.7039 (1.7616)  triple_100: 15.8524 (17.0643)  triple_80: 16.8243 (17.9600)  triple_60: 14.6098 (15.4866)  triple_40: 10.1383 (10.5715)  time: 5.8925  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 130/1724]  eta: 2:36:36  lr: 0.000040  loss: 63.9684 (67.8386)  loss_n_40: 1.7341 (1.7280)  loss_n_60: 1.5565 (1.6199)  loss_n_80: 1.6487 (1.7125)  loss_n_100: 1.7000 (1.7633)  triple_100: 15.8040 (17.0341)  triple_80: 16.8243 (17.9313)  triple_60: 15.0347 (15.4771)  triple_40: 10.3154 (10.5724)  time: 5.8924  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 140/1724]  eta: 2:35:37  lr: 0.000040  loss: 64.7800 (67.8221)  loss_n_40: 1.7341 (1.7445)  loss_n_60: 1.5955 (1.6321)  loss_n_80: 1.6284 (1.7199)  loss_n_100: 1.6971 (1.7713)  triple_100: 16.4869 (16.9987)  triple_80: 17.1655 (17.9049)  triple_60: 14.8872 (15.4802)  triple_40: 10.3154 (10.5704)  time: 5.8920  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 150/1724]  eta: 2:34:38  lr: 0.000040  loss: 64.7800 (67.5804)  loss_n_40: 1.6722 (1.7361)  loss_n_60: 1.5297 (1.6237)  loss_n_80: 1.6284 (1.7114)  loss_n_100: 1.6951 (1.7632)  triple_100: 15.9599 (16.9398)  triple_80: 17.0539 (17.8381)  triple_60: 14.6878 (15.4154)  triple_40: 10.2702 (10.5527)  time: 5.8925  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 160/1724]  eta: 2:33:39  lr: 0.000040  loss: 62.3086 (67.2842)  loss_n_40: 1.5641 (1.7313)  loss_n_60: 1.4610 (1.6181)  loss_n_80: 1.5469 (1.7037)  loss_n_100: 1.6187 (1.7553)  triple_100: 15.5383 (16.8563)  triple_80: 16.3626 (17.7522)  triple_60: 14.1082 (15.3501)  triple_40: 10.2198 (10.5171)  time: 5.8928  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [ 170/1724]  eta: 2:32:39  lr: 0.000040  loss: 60.8888 (66.9436)  loss_n_40: 1.5641 (1.7254)  loss_n_60: 1.4610 (1.6112)  loss_n_80: 1.5511 (1.6955)  loss_n_100: 1.6116 (1.7459)  triple_100: 15.0673 (16.7584)  triple_80: 16.0525 (17.6613)  triple_60: 13.8838 (15.2680)  triple_40: 9.5585 (10.4778)  time: 5.8919  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [ 180/1724]  eta: 2:31:40  lr: 0.000040  loss: 61.9157 (66.7833)  loss_n_40: 1.5912 (1.7179)  loss_n_60: 1.4840 (1.6045)  loss_n_80: 1.5528 (1.6893)  loss_n_100: 1.6116 (1.7396)  triple_100: 15.4641 (16.7221)  triple_80: 16.4925 (17.6227)  triple_60: 14.0358 (15.2277)  triple_40: 9.5585 (10.4596)  time: 5.8917  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 190/1724]  eta: 2:30:41  lr: 0.000040  loss: 63.3347 (66.5952)  loss_n_40: 1.5516 (1.7144)  loss_n_60: 1.5159 (1.6010)  loss_n_80: 1.5844 (1.6849)  loss_n_100: 1.6691 (1.7343)  triple_100: 16.0314 (16.6682)  triple_80: 17.0538 (17.5729)  triple_60: 14.2515 (15.1775)  triple_40: 9.8666 (10.4419)  time: 5.8928  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 200/1724]  eta: 2:29:42  lr: 0.000040  loss: 64.5213 (66.4977)  loss_n_40: 1.4841 (1.7048)  loss_n_60: 1.4202 (1.5930)  loss_n_80: 1.5668 (1.6787)  loss_n_100: 1.6346 (1.7279)  triple_100: 16.1569 (16.6508)  triple_80: 17.2786 (17.5549)  triple_60: 14.5621 (15.1485)  triple_40: 9.9898 (10.4392)  time: 5.8940  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 210/1724]  eta: 2:28:43  lr: 0.000040  loss: 64.1670 (66.3653)  loss_n_40: 1.4681 (1.6963)  loss_n_60: 1.3507 (1.5844)  loss_n_80: 1.4718 (1.6705)  loss_n_100: 1.5206 (1.7194)  triple_100: 16.0188 (16.6166)  triple_80: 17.0271 (17.5196)  triple_60: 14.5621 (15.1166)  triple_40: 10.1166 (10.4420)  time: 5.8933  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 220/1724]  eta: 2:27:44  lr: 0.000040  loss: 64.1064 (66.3514)  loss_n_40: 1.4794 (1.6970)  loss_n_60: 1.3870 (1.5861)  loss_n_80: 1.4738 (1.6707)  loss_n_100: 1.5206 (1.7183)  triple_100: 15.9611 (16.5925)  triple_80: 16.6867 (17.5096)  triple_60: 14.1491 (15.1177)  triple_40: 10.1166 (10.4596)  time: 5.8929  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 230/1724]  eta: 2:26:45  lr: 0.000040  loss: 62.0125 (66.3099)  loss_n_40: 1.5969 (1.7004)  loss_n_60: 1.5340 (1.5901)  loss_n_80: 1.6354 (1.6737)  loss_n_100: 1.6822 (1.7206)  triple_100: 15.9437 (16.5690)  triple_80: 16.4255 (17.4943)  triple_60: 13.9432 (15.1094)  triple_40: 10.1523 (10.4524)  time: 5.8931  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 240/1724]  eta: 2:25:46  lr: 0.000040  loss: 62.3462 (66.3057)  loss_n_40: 1.6617 (1.7014)  loss_n_60: 1.6031 (1.5947)  loss_n_80: 1.6625 (1.6785)  loss_n_100: 1.6750 (1.7247)  triple_100: 15.6626 (16.5613)  triple_80: 16.7083 (17.5036)  triple_60: 14.1063 (15.1122)  triple_40: 9.9329 (10.4293)  time: 5.8939  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [ 250/1724]  eta: 2:24:47  lr: 0.000040  loss: 64.6288 (66.2538)  loss_n_40: 1.6599 (1.6987)  loss_n_60: 1.6031 (1.5933)  loss_n_80: 1.6625 (1.6767)  loss_n_100: 1.6609 (1.7225)  triple_100: 15.8790 (16.5360)  triple_80: 17.1783 (17.4885)  triple_60: 14.9477 (15.1058)  triple_40: 10.2368 (10.4322)  time: 5.8948  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 260/1724]  eta: 2:23:48  lr: 0.000040  loss: 61.9259 (66.0803)  loss_n_40: 1.5023 (1.6902)  loss_n_60: 1.4892 (1.5864)  loss_n_80: 1.5571 (1.6706)  loss_n_100: 1.6035 (1.7165)  triple_100: 15.7258 (16.4973)  triple_80: 16.4823 (17.4512)  triple_60: 13.9537 (15.0613)  triple_40: 10.2577 (10.4068)  time: 5.8936  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 270/1724]  eta: 2:22:49  lr: 0.000040  loss: 62.7027 (66.0516)  loss_n_40: 1.5462 (1.6886)  loss_n_60: 1.4892 (1.5855)  loss_n_80: 1.5571 (1.6699)  loss_n_100: 1.6264 (1.7158)  triple_100: 15.7258 (16.4891)  triple_80: 16.9172 (17.4460)  triple_60: 14.3870 (15.0550)  triple_40: 10.1888 (10.4017)  time: 5.8931  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 280/1724]  eta: 2:21:50  lr: 0.000040  loss: 62.1775 (65.9119)  loss_n_40: 1.5462 (1.6822)  loss_n_60: 1.4355 (1.5792)  loss_n_80: 1.5240 (1.6637)  loss_n_100: 1.5737 (1.7102)  triple_100: 15.5585 (16.4594)  triple_80: 16.4252 (17.4109)  triple_60: 14.3870 (15.0200)  triple_40: 10.0808 (10.3863)  time: 5.8938  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 290/1724]  eta: 2:20:51  lr: 0.000040  loss: 62.1775 (65.9285)  loss_n_40: 1.5748 (1.6872)  loss_n_60: 1.4545 (1.5846)  loss_n_80: 1.5240 (1.6670)  loss_n_100: 1.5737 (1.7127)  triple_100: 15.2954 (16.4405)  triple_80: 16.3051 (17.4073)  triple_60: 14.3461 (15.0311)  triple_40: 10.0808 (10.3981)  time: 5.8932  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 300/1724]  eta: 2:19:52  lr: 0.000040  loss: 60.2182 (65.6876)  loss_n_40: 1.5154 (1.6771)  loss_n_60: 1.3859 (1.5745)  loss_n_80: 1.4881 (1.6578)  loss_n_100: 1.5504 (1.7037)  triple_100: 15.2027 (16.3910)  triple_80: 16.1535 (17.3480)  triple_60: 13.2204 (14.9673)  triple_40: 9.9126 (10.3682)  time: 5.8923  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 310/1724]  eta: 2:18:53  lr: 0.000040  loss: 60.4702 (65.5816)  loss_n_40: 1.3547 (1.6691)  loss_n_60: 1.2880 (1.5681)  loss_n_80: 1.4026 (1.6522)  loss_n_100: 1.4373 (1.6980)  triple_100: 15.2027 (16.3692)  triple_80: 15.9306 (17.3244)  triple_60: 13.3040 (14.9409)  triple_40: 9.9126 (10.3597)  time: 5.8910  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 320/1724]  eta: 2:17:54  lr: 0.000040  loss: 62.6301 (65.4775)  loss_n_40: 1.3836 (1.6622)  loss_n_60: 1.3747 (1.5627)  loss_n_80: 1.4526 (1.6470)  loss_n_100: 1.4978 (1.6929)  triple_100: 15.6908 (16.3493)  triple_80: 16.3341 (17.2994)  triple_60: 14.3363 (14.9183)  triple_40: 10.0714 (10.3457)  time: 5.8905  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 330/1724]  eta: 2:16:55  lr: 0.000040  loss: 63.7346 (65.4613)  loss_n_40: 1.4761 (1.6582)  loss_n_60: 1.3896 (1.5593)  loss_n_80: 1.4677 (1.6435)  loss_n_100: 1.5127 (1.6895)  triple_100: 15.6456 (16.3387)  triple_80: 16.7680 (17.2872)  triple_60: 14.5097 (14.9180)  triple_40: 10.1472 (10.3669)  time: 5.8921  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 340/1724]  eta: 2:15:56  lr: 0.000040  loss: 59.9376 (65.2656)  loss_n_40: 1.4175 (1.6524)  loss_n_60: 1.3486 (1.5540)  loss_n_80: 1.4236 (1.6381)  loss_n_100: 1.4569 (1.6846)  triple_100: 15.1418 (16.2937)  triple_80: 16.1412 (17.2344)  triple_60: 13.4350 (14.8706)  triple_40: 9.6114 (10.3377)  time: 5.8931  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 350/1724]  eta: 2:14:57  lr: 0.000040  loss: 57.1213 (65.1530)  loss_n_40: 1.4175 (1.6491)  loss_n_60: 1.3405 (1.5506)  loss_n_80: 1.4087 (1.6342)  loss_n_100: 1.4700 (1.6803)  triple_100: 14.0891 (16.2606)  triple_80: 14.8682 (17.2005)  triple_60: 12.9652 (14.8478)  triple_40: 9.2139 (10.3297)  time: 5.8923  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 360/1724]  eta: 2:13:58  lr: 0.000040  loss: 59.1272 (65.1917)  loss_n_40: 1.5057 (1.6479)  loss_n_60: 1.4488 (1.5511)  loss_n_80: 1.5062 (1.6344)  loss_n_100: 1.5538 (1.6809)  triple_100: 15.4568 (16.2730)  triple_80: 15.8006 (17.2113)  triple_60: 13.2800 (14.8632)  triple_40: 9.2284 (10.3299)  time: 5.8903  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 370/1724]  eta: 2:12:59  lr: 0.000040  loss: 60.9927 (65.0979)  loss_n_40: 1.4234 (1.6412)  loss_n_60: 1.3273 (1.5452)  loss_n_80: 1.4099 (1.6287)  loss_n_100: 1.4785 (1.6753)  triple_100: 15.4713 (16.2547)  triple_80: 16.2669 (17.1880)  triple_60: 14.1940 (14.8449)  triple_40: 9.8250 (10.3199)  time: 5.8899  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 380/1724]  eta: 2:12:00  lr: 0.000040  loss: 60.9927 (65.1128)  loss_n_40: 1.3815 (1.6375)  loss_n_60: 1.3569 (1.5429)  loss_n_80: 1.4552 (1.6264)  loss_n_100: 1.5069 (1.6731)  triple_100: 15.4713 (16.2603)  triple_80: 16.0763 (17.1920)  triple_60: 14.1940 (14.8541)  triple_40: 9.8794 (10.3264)  time: 5.8895  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 390/1724]  eta: 2:11:01  lr: 0.000040  loss: 60.7772 (65.0406)  loss_n_40: 1.3815 (1.6319)  loss_n_60: 1.3569 (1.5387)  loss_n_80: 1.4578 (1.6226)  loss_n_100: 1.5425 (1.6696)  triple_100: 15.0854 (16.2509)  triple_80: 15.9244 (17.1755)  triple_60: 13.8534 (14.8358)  triple_40: 10.2431 (10.3155)  time: 5.8892  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 400/1724]  eta: 2:10:02  lr: 0.000040  loss: 60.7772 (65.0116)  loss_n_40: 1.3974 (1.6279)  loss_n_60: 1.3817 (1.5356)  loss_n_80: 1.4658 (1.6194)  loss_n_100: 1.5288 (1.6665)  triple_100: 15.7302 (16.2449)  triple_80: 15.9422 (17.1661)  triple_60: 13.8534 (14.8314)  triple_40: 10.1188 (10.3197)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 410/1724]  eta: 2:09:03  lr: 0.000040  loss: 62.1031 (64.9155)  loss_n_40: 1.4085 (1.6223)  loss_n_60: 1.3817 (1.5303)  loss_n_80: 1.4658 (1.6137)  loss_n_100: 1.5029 (1.6608)  triple_100: 15.7513 (16.2200)  triple_80: 16.4410 (17.1367)  triple_60: 13.9846 (14.8125)  triple_40: 10.3460 (10.3191)  time: 5.8900  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [ 420/1724]  eta: 2:08:04  lr: 0.000040  loss: 59.6660 (64.8345)  loss_n_40: 1.3508 (1.6170)  loss_n_60: 1.3358 (1.5256)  loss_n_80: 1.4435 (1.6091)  loss_n_100: 1.4700 (1.6563)  triple_100: 15.4332 (16.2033)  triple_80: 15.9971 (17.1153)  triple_60: 13.8600 (14.7971)  triple_40: 9.9563 (10.3107)  time: 5.8899  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [ 430/1724]  eta: 2:07:05  lr: 0.000040  loss: 61.9276 (64.7932)  loss_n_40: 1.4074 (1.6149)  loss_n_60: 1.3837 (1.5239)  loss_n_80: 1.4812 (1.6068)  loss_n_100: 1.5181 (1.6539)  triple_100: 15.5328 (16.1908)  triple_80: 16.2280 (17.1011)  triple_60: 14.2288 (14.7917)  triple_40: 9.7609 (10.3100)  time: 5.8893  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 440/1724]  eta: 2:06:06  lr: 0.000040  loss: 65.3677 (64.8212)  loss_n_40: 1.5732 (1.6140)  loss_n_60: 1.5450 (1.5232)  loss_n_80: 1.6771 (1.6056)  loss_n_100: 1.7248 (1.6529)  triple_100: 16.5695 (16.1995)  triple_80: 17.1324 (17.1053)  triple_60: 15.1873 (14.8028)  triple_40: 10.0951 (10.3180)  time: 5.8897  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 450/1724]  eta: 2:05:07  lr: 0.000040  loss: 66.4709 (64.8340)  loss_n_40: 1.5627 (1.6129)  loss_n_60: 1.4406 (1.5218)  loss_n_80: 1.5338 (1.6045)  loss_n_100: 1.6133 (1.6519)  triple_100: 16.7152 (16.2057)  triple_80: 17.1651 (17.1066)  triple_60: 15.0233 (14.8052)  triple_40: 10.7382 (10.3253)  time: 5.8909  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 460/1724]  eta: 2:04:08  lr: 0.000040  loss: 58.7457 (64.7159)  loss_n_40: 1.4911 (1.6073)  loss_n_60: 1.4149 (1.5170)  loss_n_80: 1.4861 (1.5995)  loss_n_100: 1.5421 (1.6475)  triple_100: 15.4352 (16.1842)  triple_80: 15.6745 (17.0742)  triple_60: 13.5036 (14.7787)  triple_40: 9.7969 (10.3074)  time: 5.8913  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 470/1724]  eta: 2:03:09  lr: 0.000040  loss: 58.7457 (64.6569)  loss_n_40: 1.4278 (1.6041)  loss_n_60: 1.3059 (1.5143)  loss_n_80: 1.3851 (1.5966)  loss_n_100: 1.4497 (1.6448)  triple_100: 15.3780 (16.1756)  triple_80: 15.6745 (17.0589)  triple_60: 13.5036 (14.7686)  triple_40: 9.4418 (10.2940)  time: 5.8912  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [ 480/1724]  eta: 2:02:10  lr: 0.000040  loss: 61.9044 (64.5703)  loss_n_40: 1.3974 (1.5985)  loss_n_60: 1.2774 (1.5091)  loss_n_80: 1.3567 (1.5914)  loss_n_100: 1.4280 (1.6399)  triple_100: 15.6627 (16.1610)  triple_80: 16.3417 (17.0354)  triple_60: 14.1269 (14.7490)  triple_40: 9.8036 (10.2860)  time: 5.8910  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 490/1724]  eta: 2:01:11  lr: 0.000040  loss: 60.6544 (64.4692)  loss_n_40: 1.2516 (1.5934)  loss_n_60: 1.1869 (1.5052)  loss_n_80: 1.3489 (1.5876)  loss_n_100: 1.4280 (1.6366)  triple_100: 14.8732 (16.1416)  triple_80: 15.8470 (17.0071)  triple_60: 13.5840 (14.7261)  triple_40: 9.9357 (10.2716)  time: 5.8912  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 500/1724]  eta: 2:00:12  lr: 0.000040  loss: 56.7445 (64.3663)  loss_n_40: 1.2404 (1.5874)  loss_n_60: 1.1857 (1.5002)  loss_n_80: 1.2724 (1.5826)  loss_n_100: 1.3347 (1.6318)  triple_100: 15.0910 (16.1202)  triple_80: 15.4357 (16.9801)  triple_60: 13.0584 (14.7052)  triple_40: 9.0360 (10.2590)  time: 5.8908  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 510/1724]  eta: 1:59:13  lr: 0.000040  loss: 59.9394 (64.3611)  loss_n_40: 1.2095 (1.5861)  loss_n_60: 1.2221 (1.4994)  loss_n_80: 1.2849 (1.5818)  loss_n_100: 1.3611 (1.6313)  triple_100: 15.4066 (16.1199)  triple_80: 15.7583 (16.9780)  triple_60: 13.6427 (14.7066)  triple_40: 9.1644 (10.2580)  time: 5.8897  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 520/1724]  eta: 1:58:14  lr: 0.000040  loss: 61.7735 (64.3360)  loss_n_40: 1.4952 (1.5831)  loss_n_60: 1.4391 (1.4976)  loss_n_80: 1.5526 (1.5795)  loss_n_100: 1.5793 (1.6290)  triple_100: 16.0445 (16.1147)  triple_80: 16.5247 (16.9686)  triple_60: 14.4621 (14.7074)  triple_40: 10.1351 (10.2561)  time: 5.8903  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 530/1724]  eta: 1:57:15  lr: 0.000040  loss: 58.9965 (64.2729)  loss_n_40: 1.2266 (1.5780)  loss_n_60: 1.2964 (1.4933)  loss_n_80: 1.3951 (1.5752)  loss_n_100: 1.4712 (1.6248)  triple_100: 15.2833 (16.1038)  triple_80: 15.7465 (16.9512)  triple_60: 13.8931 (14.6967)  triple_40: 10.0201 (10.2499)  time: 5.8915  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 540/1724]  eta: 1:56:16  lr: 0.000040  loss: 62.5799 (64.2723)  loss_n_40: 1.3778 (1.5761)  loss_n_60: 1.3323 (1.4923)  loss_n_80: 1.4061 (1.5740)  loss_n_100: 1.4989 (1.6238)  triple_100: 15.4365 (16.1065)  triple_80: 16.2745 (16.9508)  triple_60: 14.5833 (14.7018)  triple_40: 9.9981 (10.2471)  time: 5.8929  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 550/1724]  eta: 1:55:17  lr: 0.000040  loss: 62.9824 (64.2096)  loss_n_40: 1.3778 (1.5714)  loss_n_60: 1.3420 (1.4886)  loss_n_80: 1.4303 (1.5704)  loss_n_100: 1.4989 (1.6206)  triple_100: 15.7673 (16.0969)  triple_80: 16.4870 (16.9352)  triple_60: 14.5833 (14.6886)  triple_40: 9.9482 (10.2380)  time: 5.8927  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 560/1724]  eta: 1:54:18  lr: 0.000040  loss: 58.1805 (64.1553)  loss_n_40: 1.2778 (1.5678)  loss_n_60: 1.2032 (1.4866)  loss_n_80: 1.2787 (1.5686)  loss_n_100: 1.3599 (1.6189)  triple_100: 14.9817 (16.0869)  triple_80: 15.3663 (16.9219)  triple_60: 13.3782 (14.6781)  triple_40: 9.5741 (10.2266)  time: 5.8914  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 570/1724]  eta: 1:53:19  lr: 0.000040  loss: 60.9964 (64.0948)  loss_n_40: 1.2778 (1.5641)  loss_n_60: 1.2424 (1.4832)  loss_n_80: 1.3059 (1.5651)  loss_n_100: 1.3737 (1.6155)  triple_100: 14.9817 (16.0738)  triple_80: 15.6617 (16.9063)  triple_60: 14.3899 (14.6670)  triple_40: 9.7469 (10.2198)  time: 5.8911  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 580/1724]  eta: 1:52:20  lr: 0.000040  loss: 60.6754 (63.9658)  loss_n_40: 1.2247 (1.5587)  loss_n_60: 1.2267 (1.4782)  loss_n_80: 1.2849 (1.5598)  loss_n_100: 1.3412 (1.6106)  triple_100: 15.0722 (16.0428)  triple_80: 15.6617 (16.8702)  triple_60: 14.1295 (14.6399)  triple_40: 9.5306 (10.2056)  time: 5.8914  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 590/1724]  eta: 1:51:22  lr: 0.000040  loss: 54.2398 (63.8243)  loss_n_40: 1.1387 (1.5518)  loss_n_60: 1.1271 (1.4731)  loss_n_80: 1.2121 (1.5545)  loss_n_100: 1.3030 (1.6056)  triple_100: 14.0500 (16.0121)  triple_80: 14.4254 (16.8324)  triple_60: 12.6247 (14.6078)  triple_40: 9.0123 (10.1869)  time: 5.8922  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 600/1724]  eta: 1:50:23  lr: 0.000040  loss: 54.2398 (63.7327)  loss_n_40: 1.1108 (1.5472)  loss_n_60: 1.1281 (1.4694)  loss_n_80: 1.2246 (1.5511)  loss_n_100: 1.3030 (1.6025)  triple_100: 14.3286 (15.9965)  triple_80: 14.4762 (16.8117)  triple_60: 12.6247 (14.5873)  triple_40: 8.9994 (10.1671)  time: 5.8928  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [ 610/1724]  eta: 1:49:24  lr: 0.000040  loss: 53.7975 (63.5980)  loss_n_40: 1.0578 (1.5394)  loss_n_60: 1.0919 (1.4633)  loss_n_80: 1.1988 (1.5454)  loss_n_100: 1.2847 (1.5971)  triple_100: 14.0047 (15.9715)  triple_80: 14.3358 (16.7804)  triple_60: 12.5597 (14.5576)  triple_40: 8.7340 (10.1434)  time: 5.8926  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 620/1724]  eta: 1:48:25  lr: 0.000040  loss: 58.1727 (63.6172)  loss_n_40: 1.0770 (1.5376)  loss_n_60: 1.1442 (1.4629)  loss_n_80: 1.2990 (1.5451)  loss_n_100: 1.3877 (1.5970)  triple_100: 15.1548 (15.9812)  triple_80: 15.5299 (16.7877)  triple_60: 13.3813 (14.5647)  triple_40: 9.0357 (10.1410)  time: 5.8919  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 630/1724]  eta: 1:47:26  lr: 0.000040  loss: 58.1727 (63.5397)  loss_n_40: 1.1745 (1.5313)  loss_n_60: 1.2098 (1.4582)  loss_n_80: 1.3093 (1.5407)  loss_n_100: 1.3868 (1.5929)  triple_100: 15.0757 (15.9688)  triple_80: 15.5299 (16.7688)  triple_60: 13.3666 (14.5487)  triple_40: 9.4454 (10.1305)  time: 5.8919  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 640/1724]  eta: 1:46:27  lr: 0.000040  loss: 55.1186 (63.4576)  loss_n_40: 1.0410 (1.5251)  loss_n_60: 1.0556 (1.4534)  loss_n_80: 1.1573 (1.5366)  loss_n_100: 1.2265 (1.5893)  triple_100: 14.6375 (15.9585)  triple_80: 14.9425 (16.7503)  triple_60: 12.8461 (14.5291)  triple_40: 9.0649 (10.1153)  time: 5.8928  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 650/1724]  eta: 1:45:28  lr: 0.000040  loss: 55.1186 (63.3809)  loss_n_40: 1.2242 (1.5214)  loss_n_60: 1.1960 (1.4507)  loss_n_80: 1.2743 (1.5338)  loss_n_100: 1.3456 (1.5867)  triple_100: 14.5490 (15.9427)  triple_80: 14.9425 (16.7304)  triple_60: 12.8461 (14.5140)  triple_40: 8.8101 (10.1011)  time: 5.8919  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 660/1724]  eta: 1:44:29  lr: 0.000040  loss: 55.6938 (63.2747)  loss_n_40: 1.2242 (1.5157)  loss_n_60: 1.1960 (1.4459)  loss_n_80: 1.2743 (1.5289)  loss_n_100: 1.3456 (1.5820)  triple_100: 14.5440 (15.9203)  triple_80: 14.8352 (16.7026)  triple_60: 12.9821 (14.4916)  triple_40: 8.9111 (10.0877)  time: 5.8914  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 670/1724]  eta: 1:43:30  lr: 0.000040  loss: 57.2013 (63.2617)  loss_n_40: 1.2276 (1.5145)  loss_n_60: 1.2227 (1.4452)  loss_n_80: 1.3241 (1.5280)  loss_n_100: 1.3708 (1.5813)  triple_100: 15.1586 (15.9191)  triple_80: 15.3378 (16.7004)  triple_60: 13.1488 (14.4924)  triple_40: 9.2676 (10.0807)  time: 5.8918  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 680/1724]  eta: 1:42:31  lr: 0.000040  loss: 57.9098 (63.1904)  loss_n_40: 1.2640 (1.5125)  loss_n_60: 1.2104 (1.4431)  loss_n_80: 1.3163 (1.5258)  loss_n_100: 1.4162 (1.5794)  triple_100: 14.8707 (15.9061)  triple_80: 15.2514 (16.6817)  triple_60: 13.4341 (14.4778)  triple_40: 8.8723 (10.0641)  time: 5.8910  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 690/1724]  eta: 1:41:32  lr: 0.000040  loss: 54.9080 (63.0894)  loss_n_40: 1.2054 (1.5104)  loss_n_60: 1.1167 (1.4406)  loss_n_80: 1.2039 (1.5227)  loss_n_100: 1.3101 (1.5765)  triple_100: 14.4715 (15.8820)  triple_80: 14.6007 (16.6535)  triple_60: 12.5686 (14.4549)  triple_40: 8.7263 (10.0488)  time: 5.8901  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 700/1724]  eta: 1:40:33  lr: 0.000040  loss: 54.5214 (63.0460)  loss_n_40: 1.2264 (1.5091)  loss_n_60: 1.1647 (1.4397)  loss_n_80: 1.2039 (1.5215)  loss_n_100: 1.3285 (1.5756)  triple_100: 14.3257 (15.8732)  triple_80: 14.4802 (16.6414)  triple_60: 12.2134 (14.4468)  triple_40: 8.7263 (10.0387)  time: 5.8892  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [ 710/1724]  eta: 1:39:34  lr: 0.000040  loss: 56.3231 (62.9828)  loss_n_40: 1.2617 (1.5081)  loss_n_60: 1.2389 (1.4386)  loss_n_80: 1.3329 (1.5196)  loss_n_100: 1.4246 (1.5741)  triple_100: 14.5715 (15.8567)  triple_80: 15.1886 (16.6213)  triple_60: 13.4537 (14.4338)  triple_40: 8.9453 (10.0306)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 720/1724]  eta: 1:38:35  lr: 0.000040  loss: 59.5380 (62.9448)  loss_n_40: 1.2879 (1.5053)  loss_n_60: 1.2900 (1.4366)  loss_n_80: 1.3766 (1.5177)  loss_n_100: 1.4927 (1.5726)  triple_100: 14.6662 (15.8536)  triple_80: 15.5247 (16.6120)  triple_60: 13.8945 (14.4261)  triple_40: 8.9453 (10.0210)  time: 5.8898  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 730/1724]  eta: 1:37:36  lr: 0.000040  loss: 57.1762 (62.8826)  loss_n_40: 1.2879 (1.5031)  loss_n_60: 1.2900 (1.4351)  loss_n_80: 1.3766 (1.5160)  loss_n_100: 1.4934 (1.5715)  triple_100: 14.8592 (15.8380)  triple_80: 15.0425 (16.5933)  triple_60: 12.9867 (14.4147)  triple_40: 9.2256 (10.0110)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 740/1724]  eta: 1:36:37  lr: 0.000040  loss: 54.0100 (62.7809)  loss_n_40: 1.0607 (1.4979)  loss_n_60: 1.0573 (1.4306)  loss_n_80: 1.1338 (1.5115)  loss_n_100: 1.2416 (1.5674)  triple_100: 14.1797 (15.8189)  triple_80: 14.2773 (16.5672)  triple_60: 12.3266 (14.3921)  triple_40: 8.6120 (9.9952)  time: 5.8905  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 750/1724]  eta: 1:35:38  lr: 0.000040  loss: 56.5136 (62.7653)  loss_n_40: 1.0862 (1.4948)  loss_n_60: 1.0868 (1.4290)  loss_n_80: 1.1767 (1.5099)  loss_n_100: 1.2564 (1.5662)  triple_100: 14.6768 (15.8201)  triple_80: 15.0512 (16.5643)  triple_60: 12.8722 (14.3913)  triple_40: 8.6120 (9.9897)  time: 5.8910  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 760/1724]  eta: 1:34:40  lr: 0.000040  loss: 55.4480 (62.6301)  loss_n_40: 1.0621 (1.4886)  loss_n_60: 1.0574 (1.4238)  loss_n_80: 1.1794 (1.5050)  loss_n_100: 1.2641 (1.5617)  triple_100: 14.2051 (15.7929)  triple_80: 14.3677 (16.5298)  triple_60: 12.2264 (14.3601)  triple_40: 8.4000 (9.9682)  time: 5.8910  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 770/1724]  eta: 1:33:41  lr: 0.000040  loss: 50.2640 (62.4666)  loss_n_40: 0.9690 (1.4811)  loss_n_60: 0.9339 (1.4170)  loss_n_80: 1.0221 (1.4982)  loss_n_100: 1.1144 (1.5552)  triple_100: 13.2052 (15.7587)  triple_80: 13.2576 (16.4874)  triple_60: 11.7179 (14.3234)  triple_40: 8.0555 (9.9456)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 780/1724]  eta: 1:32:42  lr: 0.000040  loss: 53.0573 (62.4129)  loss_n_40: 0.9690 (1.4774)  loss_n_60: 0.9846 (1.4141)  loss_n_80: 1.0775 (1.4952)  loss_n_100: 1.1559 (1.5525)  triple_100: 13.9197 (15.7490)  triple_80: 13.8846 (16.4721)  triple_60: 11.9081 (14.3128)  triple_40: 8.4199 (9.9399)  time: 5.8891  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 790/1724]  eta: 1:31:43  lr: 0.000040  loss: 53.1520 (62.3137)  loss_n_40: 1.0683 (1.4731)  loss_n_60: 1.0717 (1.4103)  loss_n_80: 1.1675 (1.4910)  loss_n_100: 1.2695 (1.5485)  triple_100: 13.9197 (15.7252)  triple_80: 13.9748 (16.4441)  triple_60: 12.2597 (14.2924)  triple_40: 8.8462 (9.9289)  time: 5.8902  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 800/1724]  eta: 1:30:44  lr: 0.000040  loss: 54.5609 (62.2399)  loss_n_40: 1.1253 (1.4693)  loss_n_60: 1.1541 (1.4072)  loss_n_80: 1.2289 (1.4881)  loss_n_100: 1.2695 (1.5459)  triple_100: 13.7993 (15.7123)  triple_80: 14.4374 (16.4259)  triple_60: 12.6867 (14.2761)  triple_40: 8.7437 (9.9149)  time: 5.8910  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 810/1724]  eta: 1:29:45  lr: 0.000040  loss: 53.9802 (62.1205)  loss_n_40: 1.1044 (1.4635)  loss_n_60: 1.1093 (1.4023)  loss_n_80: 1.2034 (1.4831)  loss_n_100: 1.2796 (1.5412)  triple_100: 14.0272 (15.6885)  triple_80: 14.2810 (16.3954)  triple_60: 12.6026 (14.2496)  triple_40: 8.5016 (9.8969)  time: 5.8913  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 820/1724]  eta: 1:28:46  lr: 0.000040  loss: 52.8742 (62.0645)  loss_n_40: 1.0675 (1.4603)  loss_n_60: 1.0797 (1.3997)  loss_n_80: 1.1725 (1.4807)  loss_n_100: 1.2657 (1.5393)  triple_100: 14.0272 (15.6802)  triple_80: 14.0698 (16.3815)  triple_60: 12.1259 (14.2372)  triple_40: 8.2068 (9.8856)  time: 5.8912  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 830/1724]  eta: 1:27:47  lr: 0.000040  loss: 54.4448 (62.0194)  loss_n_40: 1.0900 (1.4569)  loss_n_60: 1.0797 (1.3971)  loss_n_80: 1.1725 (1.4781)  loss_n_100: 1.2798 (1.5371)  triple_100: 14.3547 (15.6741)  triple_80: 14.5213 (16.3699)  triple_60: 12.3992 (14.2280)  triple_40: 8.2688 (9.8781)  time: 5.8909  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 840/1724]  eta: 1:26:48  lr: 0.000040  loss: 54.2015 (61.9155)  loss_n_40: 1.0478 (1.4525)  loss_n_60: 1.0739 (1.3936)  loss_n_80: 1.1556 (1.4749)  loss_n_100: 1.2766 (1.5345)  triple_100: 14.2229 (15.6555)  triple_80: 14.5213 (16.3456)  triple_60: 12.3992 (14.2024)  triple_40: 8.2106 (9.8565)  time: 5.8906  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 850/1724]  eta: 1:25:49  lr: 0.000040  loss: 52.8586 (61.8343)  loss_n_40: 1.0145 (1.4497)  loss_n_60: 0.9591 (1.3913)  loss_n_80: 1.0524 (1.4724)  loss_n_100: 1.1478 (1.5322)  triple_100: 14.2089 (15.6377)  triple_80: 14.4129 (16.3245)  triple_60: 12.0673 (14.1847)  triple_40: 8.1215 (9.8417)  time: 5.8925  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 860/1724]  eta: 1:24:50  lr: 0.000040  loss: 51.3969 (61.7497)  loss_n_40: 1.0371 (1.4467)  loss_n_60: 1.0101 (1.3888)  loss_n_80: 1.0883 (1.4701)  loss_n_100: 1.1756 (1.5302)  triple_100: 13.4947 (15.6207)  triple_80: 13.6655 (16.3039)  triple_60: 11.6508 (14.1637)  triple_40: 8.1849 (9.8258)  time: 5.8929  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 870/1724]  eta: 1:23:51  lr: 0.000040  loss: 51.3969 (61.6448)  loss_n_40: 1.0190 (1.4425)  loss_n_60: 1.0120 (1.3853)  loss_n_80: 1.0928 (1.4667)  loss_n_100: 1.1756 (1.5271)  triple_100: 13.4947 (15.5991)  triple_80: 13.7977 (16.2783)  triple_60: 11.8004 (14.1398)  triple_40: 8.0260 (9.8060)  time: 5.8920  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [ 880/1724]  eta: 1:22:52  lr: 0.000040  loss: 54.0603 (61.6123)  loss_n_40: 1.1662 (1.4422)  loss_n_60: 1.1062 (1.3850)  loss_n_80: 1.2079 (1.4662)  loss_n_100: 1.2995 (1.5266)  triple_100: 13.7148 (15.5887)  triple_80: 14.2243 (16.2685)  triple_60: 12.5501 (14.1342)  triple_40: 8.4394 (9.8009)  time: 5.8924  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 890/1724]  eta: 1:21:53  lr: 0.000040  loss: 55.0092 (61.5184)  loss_n_40: 1.2214 (1.4404)  loss_n_60: 1.1917 (1.3832)  loss_n_80: 1.2550 (1.4642)  loss_n_100: 1.3068 (1.5248)  triple_100: 13.7495 (15.5636)  triple_80: 14.3751 (16.2427)  triple_60: 12.7916 (14.1143)  triple_40: 8.3766 (9.7852)  time: 5.8921  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 900/1724]  eta: 1:20:55  lr: 0.000040  loss: 49.1876 (61.4252)  loss_n_40: 1.1275 (1.4369)  loss_n_60: 1.1129 (1.3801)  loss_n_80: 1.2106 (1.4610)  loss_n_100: 1.3042 (1.5218)  triple_100: 12.9776 (15.5417)  triple_80: 13.2019 (16.2192)  triple_60: 11.4151 (14.0944)  triple_40: 7.6734 (9.7700)  time: 5.8922  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 910/1724]  eta: 1:19:56  lr: 0.000040  loss: 52.7079 (61.3745)  loss_n_40: 1.0350 (1.4336)  loss_n_60: 1.0279 (1.3773)  loss_n_80: 1.1201 (1.4582)  loss_n_100: 1.1992 (1.5192)  triple_100: 13.0014 (15.5318)  triple_80: 13.7914 (16.2058)  triple_60: 12.1763 (14.0855)  triple_40: 7.9283 (9.7630)  time: 5.8922  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 920/1724]  eta: 1:18:57  lr: 0.000040  loss: 52.3898 (61.3051)  loss_n_40: 1.0350 (1.4304)  loss_n_60: 1.0279 (1.3747)  loss_n_80: 1.1139 (1.4557)  loss_n_100: 1.1992 (1.5170)  triple_100: 14.1295 (15.5193)  triple_80: 14.1634 (16.1893)  triple_60: 11.9770 (14.0701)  triple_40: 8.1768 (9.7486)  time: 5.8918  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 930/1724]  eta: 1:17:58  lr: 0.000040  loss: 52.4444 (61.2144)  loss_n_40: 0.9649 (1.4255)  loss_n_60: 0.9878 (1.3706)  loss_n_80: 1.0940 (1.4517)  loss_n_100: 1.1829 (1.5133)  triple_100: 13.9196 (15.5020)  triple_80: 14.0827 (16.1674)  triple_60: 12.1590 (14.0514)  triple_40: 8.1768 (9.7324)  time: 5.8916  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [ 940/1724]  eta: 1:16:59  lr: 0.000040  loss: 54.4341 (61.1697)  loss_n_40: 0.9922 (1.4238)  loss_n_60: 1.0398 (1.3689)  loss_n_80: 1.1488 (1.4499)  loss_n_100: 1.2293 (1.5118)  triple_100: 13.9363 (15.4902)  triple_80: 14.3540 (16.1535)  triple_60: 12.6424 (14.0424)  triple_40: 8.3755 (9.7292)  time: 5.8915  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 950/1724]  eta: 1:16:00  lr: 0.000040  loss: 52.4081 (61.0893)  loss_n_40: 1.1491 (1.4208)  loss_n_60: 1.1392 (1.3664)  loss_n_80: 1.2134 (1.4475)  loss_n_100: 1.3289 (1.5098)  triple_100: 13.6733 (15.4748)  triple_80: 13.9636 (16.1337)  triple_60: 11.9367 (14.0234)  triple_40: 8.2212 (9.7128)  time: 5.8918  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 960/1724]  eta: 1:15:01  lr: 0.000040  loss: 49.9935 (60.9902)  loss_n_40: 1.0945 (1.4177)  loss_n_60: 1.0874 (1.3640)  loss_n_80: 1.1989 (1.4453)  loss_n_100: 1.2980 (1.5080)  triple_100: 13.2120 (15.4559)  triple_80: 13.3518 (16.1106)  triple_60: 11.3533 (13.9983)  triple_40: 7.1195 (9.6903)  time: 5.8915  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 970/1724]  eta: 1:14:02  lr: 0.000040  loss: 48.7957 (60.9084)  loss_n_40: 1.0365 (1.4154)  loss_n_60: 0.9992 (1.3621)  loss_n_80: 1.1086 (1.4436)  loss_n_100: 1.2120 (1.5066)  triple_100: 12.8617 (15.4384)  triple_80: 13.1097 (16.0903)  triple_60: 11.1490 (13.9792)  triple_40: 7.1404 (9.6728)  time: 5.8908  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 980/1724]  eta: 1:13:03  lr: 0.000040  loss: 50.8652 (60.8321)  loss_n_40: 1.0445 (1.4123)  loss_n_60: 1.0433 (1.3597)  loss_n_80: 1.1415 (1.4414)  loss_n_100: 1.2235 (1.5047)  triple_100: 13.3625 (15.4241)  triple_80: 13.9332 (16.0728)  triple_60: 11.6007 (13.9612)  triple_40: 7.2185 (9.6559)  time: 5.8898  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [ 990/1724]  eta: 1:12:04  lr: 0.000040  loss: 51.9971 (60.7738)  loss_n_40: 1.0851 (1.4099)  loss_n_60: 1.0917 (1.3578)  loss_n_80: 1.1664 (1.4396)  loss_n_100: 1.2624 (1.5031)  triple_100: 13.3625 (15.4125)  triple_80: 13.9293 (16.0585)  triple_60: 11.7668 (13.9478)  triple_40: 7.7892 (9.6445)  time: 5.8880  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1000/1724]  eta: 1:11:05  lr: 0.000040  loss: 50.7847 (60.6757)  loss_n_40: 0.9567 (1.4061)  loss_n_60: 1.0165 (1.3545)  loss_n_80: 1.1143 (1.4363)  loss_n_100: 1.2109 (1.5000)  triple_100: 13.1086 (15.3913)  triple_80: 13.6607 (16.0344)  triple_60: 11.6315 (13.9257)  triple_40: 7.5688 (9.6274)  time: 5.8879  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1010/1724]  eta: 1:10:06  lr: 0.000040  loss: 50.3357 (60.6058)  loss_n_40: 1.0306 (1.4032)  loss_n_60: 1.0457 (1.3527)  loss_n_80: 1.1430 (1.4348)  loss_n_100: 1.2553 (1.4989)  triple_100: 13.2648 (15.3805)  triple_80: 13.6250 (16.0191)  triple_60: 11.6315 (13.9089)  triple_40: 7.2894 (9.6076)  time: 5.8894  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1020/1724]  eta: 1:09:07  lr: 0.000040  loss: 51.1119 (60.4968)  loss_n_40: 1.0145 (1.3985)  loss_n_60: 1.0343 (1.3490)  loss_n_80: 1.1430 (1.4315)  loss_n_100: 1.2382 (1.4959)  triple_100: 13.5504 (15.3606)  triple_80: 13.6250 (15.9947)  triple_60: 11.5514 (13.8832)  triple_40: 7.1556 (9.5835)  time: 5.8909  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [1030/1724]  eta: 1:08:08  lr: 0.000040  loss: 48.0994 (60.3966)  loss_n_40: 0.8773 (1.3941)  loss_n_60: 0.9418 (1.3453)  loss_n_80: 1.0488 (1.4278)  loss_n_100: 1.1348 (1.4924)  triple_100: 12.9130 (15.3400)  triple_80: 12.9529 (15.9700)  triple_60: 11.1012 (13.8607)  triple_40: 7.1556 (9.5662)  time: 5.8901  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [1040/1724]  eta: 1:07:09  lr: 0.000040  loss: 45.5782 (60.2750)  loss_n_40: 0.8449 (1.3891)  loss_n_60: 0.8636 (1.3410)  loss_n_80: 0.9881 (1.4238)  loss_n_100: 1.0892 (1.4887)  triple_100: 12.3069 (15.3165)  triple_80: 12.3524 (15.9409)  triple_60: 10.5612 (13.8316)  triple_40: 7.1934 (9.5434)  time: 5.8890  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1050/1724]  eta: 1:06:11  lr: 0.000040  loss: 46.9640 (60.1915)  loss_n_40: 0.8780 (1.3856)  loss_n_60: 0.9150 (1.3383)  loss_n_80: 1.0066 (1.4212)  loss_n_100: 1.1059 (1.4863)  triple_100: 12.5566 (15.2992)  triple_80: 12.7311 (15.9207)  triple_60: 10.8332 (13.8123)  triple_40: 7.3404 (9.5279)  time: 5.8884  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1060/1724]  eta: 1:05:12  lr: 0.000040  loss: 48.6423 (60.1302)  loss_n_40: 0.8895 (1.3823)  loss_n_60: 0.9398 (1.3358)  loss_n_80: 1.0198 (1.4188)  loss_n_100: 1.1242 (1.4840)  triple_100: 13.2571 (15.2876)  triple_80: 13.2829 (15.9065)  triple_60: 11.0344 (13.7987)  triple_40: 7.4410 (9.5165)  time: 5.8871  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1070/1724]  eta: 1:04:13  lr: 0.000040  loss: 50.3186 (60.0781)  loss_n_40: 0.9151 (1.3806)  loss_n_60: 0.9516 (1.3348)  loss_n_80: 1.0571 (1.4180)  loss_n_100: 1.1780 (1.4835)  triple_100: 13.6961 (15.2778)  triple_80: 13.7787 (15.8949)  triple_60: 11.4035 (13.7863)  triple_40: 7.4410 (9.5021)  time: 5.8864  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1080/1724]  eta: 1:03:14  lr: 0.000040  loss: 49.9636 (59.9674)  loss_n_40: 0.9982 (1.3769)  loss_n_60: 1.0049 (1.3312)  loss_n_80: 1.1197 (1.4143)  loss_n_100: 1.1837 (1.4800)  triple_100: 13.1712 (15.2530)  triple_80: 13.6380 (15.8661)  triple_60: 11.4351 (13.7613)  triple_40: 7.6503 (9.4845)  time: 5.8873  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1090/1724]  eta: 1:02:15  lr: 0.000040  loss: 49.9636 (59.9123)  loss_n_40: 0.9982 (1.3741)  loss_n_60: 1.0049 (1.3290)  loss_n_80: 1.0986 (1.4118)  loss_n_100: 1.1675 (1.4775)  triple_100: 13.0679 (15.2401)  triple_80: 13.5971 (15.8515)  triple_60: 11.5272 (13.7516)  triple_40: 7.9998 (9.4766)  time: 5.8880  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [1100/1724]  eta: 1:01:16  lr: 0.000040  loss: 52.8972 (59.8470)  loss_n_40: 1.0575 (1.3719)  loss_n_60: 1.0315 (1.3270)  loss_n_80: 1.1269 (1.4097)  loss_n_100: 1.1694 (1.4753)  triple_100: 13.3925 (15.2246)  triple_80: 13.9207 (15.8341)  triple_60: 12.5616 (13.7375)  triple_40: 8.7954 (9.4669)  time: 5.8878  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [1110/1724]  eta: 1:00:17  lr: 0.000040  loss: 55.6423 (59.8049)  loss_n_40: 1.1559 (1.3704)  loss_n_60: 1.1722 (1.3259)  loss_n_80: 1.2538 (1.4086)  loss_n_100: 1.3378 (1.4746)  triple_100: 13.7411 (15.2176)  triple_80: 14.4437 (15.8236)  triple_60: 12.9412 (13.7274)  triple_40: 8.7940 (9.4567)  time: 5.8871  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1120/1724]  eta: 0:59:18  lr: 0.000040  loss: 53.3409 (59.7394)  loss_n_40: 1.0597 (1.3679)  loss_n_60: 1.0791 (1.3238)  loss_n_80: 1.1710 (1.4065)  loss_n_100: 1.2549 (1.4726)  triple_100: 13.7672 (15.2036)  triple_80: 14.2131 (15.8074)  triple_60: 12.4394 (13.7140)  triple_40: 7.9994 (9.4435)  time: 5.8869  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1130/1724]  eta: 0:58:19  lr: 0.000040  loss: 49.3167 (59.6686)  loss_n_40: 0.9560 (1.3660)  loss_n_60: 0.9952 (1.3219)  loss_n_80: 1.0958 (1.4045)  loss_n_100: 1.2159 (1.4708)  triple_100: 13.3662 (15.1874)  triple_80: 13.4765 (15.7885)  triple_60: 11.0733 (13.6975)  triple_40: 7.2799 (9.4320)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1140/1724]  eta: 0:57:20  lr: 0.000040  loss: 49.3167 (59.5865)  loss_n_40: 1.0965 (1.3630)  loss_n_60: 1.0427 (1.3193)  loss_n_80: 1.1129 (1.4020)  loss_n_100: 1.1993 (1.4684)  triple_100: 13.1697 (15.1691)  triple_80: 13.4765 (15.7684)  triple_60: 11.2108 (13.6792)  triple_40: 7.4591 (9.4171)  time: 5.8894  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1150/1724]  eta: 0:56:21  lr: 0.000040  loss: 50.3656 (59.5188)  loss_n_40: 1.0195 (1.3604)  loss_n_60: 1.0139 (1.3173)  loss_n_80: 1.1081 (1.4000)  loss_n_100: 1.1588 (1.4667)  triple_100: 13.1697 (15.1553)  triple_80: 13.5277 (15.7521)  triple_60: 11.6895 (13.6639)  triple_40: 7.3525 (9.4031)  time: 5.8880  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1160/1724]  eta: 0:55:22  lr: 0.000040  loss: 49.6294 (59.4405)  loss_n_40: 0.9552 (1.3573)  loss_n_60: 0.9338 (1.3143)  loss_n_80: 1.0403 (1.3971)  loss_n_100: 1.1468 (1.4640)  triple_100: 13.2174 (15.1388)  triple_80: 13.4163 (15.7325)  triple_60: 11.2866 (13.6456)  triple_40: 7.2173 (9.3909)  time: 5.8878  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [1170/1724]  eta: 0:54:23  lr: 0.000040  loss: 47.3021 (59.3424)  loss_n_40: 0.8328 (1.3536)  loss_n_60: 0.8331 (1.3109)  loss_n_80: 0.9560 (1.3938)  loss_n_100: 1.0634 (1.4609)  triple_100: 12.7018 (15.1181)  triple_80: 12.7966 (15.7080)  triple_60: 10.7789 (13.6231)  triple_40: 7.2173 (9.3740)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1180/1724]  eta: 0:53:24  lr: 0.000040  loss: 46.9383 (59.2724)  loss_n_40: 0.8574 (1.3504)  loss_n_60: 0.8966 (1.3083)  loss_n_80: 1.0062 (1.3914)  loss_n_100: 1.1029 (1.4587)  triple_100: 12.4879 (15.1049)  triple_80: 12.6226 (15.6915)  triple_60: 10.6606 (13.6066)  triple_40: 7.0420 (9.3607)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1190/1724]  eta: 0:52:25  lr: 0.000040  loss: 47.7639 (59.2300)  loss_n_40: 0.9388 (1.3487)  loss_n_60: 0.9818 (1.3068)  loss_n_80: 1.0783 (1.3898)  loss_n_100: 1.1597 (1.4572)  triple_100: 13.2617 (15.0959)  triple_80: 13.1897 (15.6805)  triple_60: 11.0911 (13.5978)  triple_40: 7.1623 (9.3532)  time: 5.8899  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1200/1724]  eta: 0:51:27  lr: 0.000040  loss: 50.0976 (59.1719)  loss_n_40: 1.0206 (1.3469)  loss_n_60: 1.0237 (1.3051)  loss_n_80: 1.1198 (1.3880)  loss_n_100: 1.2343 (1.4555)  triple_100: 13.5975 (15.0833)  triple_80: 13.5083 (15.6654)  triple_60: 11.5662 (13.5854)  triple_40: 7.8234 (9.3423)  time: 5.8907  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1210/1724]  eta: 0:50:28  lr: 0.000040  loss: 48.4116 (59.1373)  loss_n_40: 0.9804 (1.3451)  loss_n_60: 0.9892 (1.3040)  loss_n_80: 1.0776 (1.3870)  loss_n_100: 1.1658 (1.4547)  triple_100: 12.9528 (15.0772)  triple_80: 13.0392 (15.6575)  triple_60: 11.3140 (13.5777)  triple_40: 7.8234 (9.3341)  time: 5.8906  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1220/1724]  eta: 0:49:29  lr: 0.000040  loss: 47.5484 (59.0429)  loss_n_40: 0.8256 (1.3414)  loss_n_60: 0.8605 (1.3006)  loss_n_80: 0.9794 (1.3836)  loss_n_100: 1.0657 (1.4515)  triple_100: 12.6366 (15.0564)  triple_80: 12.6857 (15.6334)  triple_60: 11.1392 (13.5564)  triple_40: 7.1221 (9.3195)  time: 5.8904  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1230/1724]  eta: 0:48:30  lr: 0.000040  loss: 48.5412 (58.9754)  loss_n_40: 0.9424 (1.3388)  loss_n_60: 0.9052 (1.2983)  loss_n_80: 0.9905 (1.3813)  loss_n_100: 1.1042 (1.4493)  triple_100: 12.7773 (15.0422)  triple_80: 12.6726 (15.6155)  triple_60: 11.3549 (13.5403)  triple_40: 7.7460 (9.3096)  time: 5.8895  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1240/1724]  eta: 0:47:31  lr: 0.000040  loss: 49.1329 (58.8857)  loss_n_40: 0.9917 (1.3356)  loss_n_60: 0.9811 (1.2955)  loss_n_80: 1.0663 (1.3785)  loss_n_100: 1.1351 (1.4466)  triple_100: 12.7773 (15.0217)  triple_80: 12.6726 (15.5920)  triple_60: 10.9504 (13.5198)  triple_40: 7.3692 (9.2959)  time: 5.8882  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1250/1724]  eta: 0:46:32  lr: 0.000040  loss: 45.7338 (58.8042)  loss_n_40: 0.8856 (1.3326)  loss_n_60: 0.9005 (1.2928)  loss_n_80: 1.0111 (1.3758)  loss_n_100: 1.0898 (1.4441)  triple_100: 12.2522 (15.0042)  triple_80: 12.3628 (15.5712)  triple_60: 10.4357 (13.5004)  triple_40: 6.9435 (9.2830)  time: 5.8885  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1260/1724]  eta: 0:45:33  lr: 0.000040  loss: 44.3375 (58.7004)  loss_n_40: 0.7664 (1.3279)  loss_n_60: 0.8163 (1.2889)  loss_n_80: 0.9330 (1.3720)  loss_n_100: 1.0401 (1.4405)  triple_100: 11.8409 (14.9835)  triple_80: 11.9987 (15.5468)  triple_60: 10.1946 (13.4774)  triple_40: 6.7791 (9.2634)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1270/1724]  eta: 0:44:34  lr: 0.000040  loss: 44.6179 (58.6159)  loss_n_40: 0.7454 (1.3242)  loss_n_60: 0.8163 (1.2856)  loss_n_80: 0.9325 (1.3688)  loss_n_100: 1.0136 (1.4375)  triple_100: 12.0488 (14.9657)  triple_80: 12.0498 (15.5254)  triple_60: 10.2500 (13.4582)  triple_40: 6.8065 (9.2503)  time: 5.8905  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1280/1724]  eta: 0:43:35  lr: 0.000040  loss: 46.1293 (58.5520)  loss_n_40: 0.8140 (1.3215)  loss_n_60: 0.8393 (1.2834)  loss_n_80: 0.9498 (1.3666)  loss_n_100: 1.0461 (1.4354)  triple_100: 12.3328 (14.9517)  triple_80: 12.4929 (15.5086)  triple_60: 10.8714 (13.4440)  triple_40: 7.1599 (9.2408)  time: 5.8908  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1290/1724]  eta: 0:42:36  lr: 0.000040  loss: 45.9191 (58.4738)  loss_n_40: 0.7776 (1.3179)  loss_n_60: 0.8283 (1.2803)  loss_n_80: 0.9379 (1.3635)  loss_n_100: 1.0076 (1.4323)  triple_100: 11.8082 (14.9349)  triple_80: 11.9913 (15.4896)  triple_60: 10.6036 (13.4273)  triple_40: 6.9970 (9.2279)  time: 5.8909  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1300/1724]  eta: 0:41:37  lr: 0.000040  loss: 44.7667 (58.3898)  loss_n_40: 0.7375 (1.3145)  loss_n_60: 0.8174 (1.2773)  loss_n_80: 0.9204 (1.3606)  loss_n_100: 1.0033 (1.4295)  triple_100: 12.3475 (14.9173)  triple_80: 12.3808 (15.4695)  triple_60: 10.2990 (13.4086)  triple_40: 6.8759 (9.2126)  time: 5.8897  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1310/1724]  eta: 0:40:38  lr: 0.000040  loss: 46.6961 (58.3436)  loss_n_40: 0.7747 (1.3124)  loss_n_60: 0.8268 (1.2756)  loss_n_80: 0.9482 (1.3590)  loss_n_100: 1.0510 (1.4280)  triple_100: 12.5492 (14.9081)  triple_80: 12.5752 (15.4581)  triple_60: 11.1557 (13.3981)  triple_40: 7.1298 (9.2042)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1320/1724]  eta: 0:39:40  lr: 0.000040  loss: 49.2407 (58.2793)  loss_n_40: 0.8628 (1.3098)  loss_n_60: 0.9681 (1.2734)  loss_n_80: 1.1029 (1.3570)  loss_n_100: 1.2069 (1.4263)  triple_100: 13.0990 (14.8961)  triple_80: 13.2663 (15.4429)  triple_60: 11.2497 (13.3827)  triple_40: 7.2735 (9.1910)  time: 5.8897  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1330/1724]  eta: 0:38:41  lr: 0.000040  loss: 46.7781 (58.1943)  loss_n_40: 0.7972 (1.3063)  loss_n_60: 0.8759 (1.2705)  loss_n_80: 0.9627 (1.3541)  loss_n_100: 1.0679 (1.4235)  triple_100: 12.8608 (14.8782)  triple_80: 12.9650 (15.4224)  triple_60: 10.8166 (13.3642)  triple_40: 6.8049 (9.1752)  time: 5.8891  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1340/1724]  eta: 0:37:42  lr: 0.000040  loss: 45.7235 (58.1183)  loss_n_40: 0.7972 (1.3033)  loss_n_60: 0.7836 (1.2678)  loss_n_80: 0.9079 (1.3513)  loss_n_100: 1.0016 (1.4208)  triple_100: 12.5421 (14.8608)  triple_80: 12.4704 (15.4019)  triple_60: 10.4896 (13.3470)  triple_40: 6.8114 (9.1654)  time: 5.8894  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1350/1724]  eta: 0:36:43  lr: 0.000040  loss: 45.7235 (58.0310)  loss_n_40: 0.8302 (1.2997)  loss_n_60: 0.8717 (1.2647)  loss_n_80: 0.9547 (1.3483)  loss_n_100: 1.0350 (1.4179)  triple_100: 12.5421 (14.8431)  triple_80: 12.4704 (15.3809)  triple_60: 10.4525 (13.3271)  triple_40: 6.8269 (9.1493)  time: 5.8887  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1360/1724]  eta: 0:35:44  lr: 0.000040  loss: 48.8170 (57.9815)  loss_n_40: 0.8426 (1.2973)  loss_n_60: 0.9343 (1.2627)  loss_n_80: 1.0367 (1.3464)  loss_n_100: 1.1229 (1.4162)  triple_100: 12.9512 (14.8335)  triple_80: 13.0378 (15.3690)  triple_60: 10.8408 (13.3161)  triple_40: 7.2268 (9.1403)  time: 5.8884  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1370/1724]  eta: 0:34:45  lr: 0.000040  loss: 51.2182 (57.9130)  loss_n_40: 0.8551 (1.2946)  loss_n_60: 0.9284 (1.2604)  loss_n_80: 1.0367 (1.3441)  loss_n_100: 1.1321 (1.4140)  triple_100: 13.1383 (14.8185)  triple_80: 13.6657 (15.3522)  triple_60: 11.7881 (13.3008)  triple_40: 7.4840 (9.1284)  time: 5.8890  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1380/1724]  eta: 0:33:46  lr: 0.000040  loss: 47.1217 (57.8440)  loss_n_40: 0.7550 (1.2919)  loss_n_60: 0.8726 (1.2579)  loss_n_80: 1.0022 (1.3415)  loss_n_100: 1.0923 (1.4114)  triple_100: 12.9039 (14.8023)  triple_80: 12.8999 (15.3346)  triple_60: 10.6902 (13.2866)  triple_40: 6.9539 (9.1179)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1390/1724]  eta: 0:32:47  lr: 0.000040  loss: 45.6894 (57.7582)  loss_n_40: 0.7850 (1.2895)  loss_n_60: 0.8311 (1.2554)  loss_n_80: 0.8860 (1.3389)  loss_n_100: 0.9477 (1.4088)  triple_100: 12.1847 (14.7812)  triple_80: 12.3493 (15.3120)  triple_60: 10.5425 (13.2671)  triple_40: 6.8932 (9.1054)  time: 5.8886  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [1400/1724]  eta: 0:31:48  lr: 0.000040  loss: 45.8663 (57.6963)  loss_n_40: 0.8382 (1.2873)  loss_n_60: 0.8654 (1.2533)  loss_n_80: 0.9408 (1.3367)  loss_n_100: 0.9993 (1.4066)  triple_100: 12.1847 (14.7666)  triple_80: 12.3493 (15.2957)  triple_60: 10.7475 (13.2541)  triple_40: 6.8932 (9.0961)  time: 5.8886  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [1410/1724]  eta: 0:30:49  lr: 0.000040  loss: 47.3356 (57.6397)  loss_n_40: 0.8382 (1.2854)  loss_n_60: 0.8598 (1.2514)  loss_n_80: 0.9465 (1.3347)  loss_n_100: 1.0300 (1.4046)  triple_100: 12.5714 (14.7525)  triple_80: 12.8744 (15.2806)  triple_60: 10.9738 (13.2421)  triple_40: 7.4157 (9.0884)  time: 5.8879  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1420/1724]  eta: 0:29:50  lr: 0.000040  loss: 45.5450 (57.5748)  loss_n_40: 0.7925 (1.2825)  loss_n_60: 0.8264 (1.2490)  loss_n_80: 0.9232 (1.3323)  loss_n_100: 0.9860 (1.4023)  triple_100: 12.1659 (14.7389)  triple_80: 12.2259 (15.2646)  triple_60: 10.5905 (13.2276)  triple_40: 7.2916 (9.0776)  time: 5.8879  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1430/1724]  eta: 0:28:51  lr: 0.000040  loss: 44.8231 (57.5107)  loss_n_40: 0.7505 (1.2797)  loss_n_60: 0.7991 (1.2466)  loss_n_80: 0.8951 (1.3299)  loss_n_100: 0.9843 (1.3999)  triple_100: 12.4655 (14.7247)  triple_80: 12.5165 (15.2486)  triple_60: 10.1266 (13.2140)  triple_40: 7.1381 (9.0673)  time: 5.8887  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1440/1724]  eta: 0:27:53  lr: 0.000040  loss: 45.8919 (57.4469)  loss_n_40: 0.7528 (1.2773)  loss_n_60: 0.8870 (1.2444)  loss_n_80: 0.9903 (1.3278)  loss_n_100: 1.0861 (1.3980)  triple_100: 12.5750 (14.7107)  triple_80: 12.5416 (15.2318)  triple_60: 10.6235 (13.1990)  triple_40: 7.1381 (9.0579)  time: 5.8880  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1450/1724]  eta: 0:26:54  lr: 0.000040  loss: 46.3669 (57.3875)  loss_n_40: 0.8092 (1.2747)  loss_n_60: 0.9062 (1.2423)  loss_n_80: 0.9903 (1.3257)  loss_n_100: 1.0861 (1.3961)  triple_100: 12.4699 (14.6986)  triple_80: 12.5416 (15.2174)  triple_60: 10.5804 (13.1858)  triple_40: 6.9121 (9.0467)  time: 5.8880  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1460/1724]  eta: 0:25:55  lr: 0.000040  loss: 44.4214 (57.3008)  loss_n_40: 0.6912 (1.2709)  loss_n_60: 0.7327 (1.2390)  loss_n_80: 0.8417 (1.3224)  loss_n_100: 0.9232 (1.3928)  triple_100: 12.1101 (14.6805)  triple_80: 12.1410 (15.1966)  triple_60: 10.4841 (13.1669)  triple_40: 6.7368 (9.0317)  time: 5.8887  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1470/1724]  eta: 0:24:56  lr: 0.000040  loss: 44.0110 (57.2381)  loss_n_40: 0.7451 (1.2688)  loss_n_60: 0.7779 (1.2371)  loss_n_80: 0.8829 (1.3205)  loss_n_100: 0.9697 (1.3910)  triple_100: 11.8744 (14.6654)  triple_80: 12.0686 (15.1802)  triple_60: 10.1281 (13.1532)  triple_40: 6.7497 (9.0218)  time: 5.8884  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1480/1724]  eta: 0:23:57  lr: 0.000040  loss: 46.4132 (57.1684)  loss_n_40: 0.8242 (1.2662)  loss_n_60: 0.8715 (1.2348)  loss_n_80: 0.9941 (1.3182)  loss_n_100: 1.0367 (1.3888)  triple_100: 12.1347 (14.6505)  triple_80: 12.3053 (15.1630)  triple_60: 10.5328 (13.1375)  triple_40: 6.9175 (9.0095)  time: 5.8890  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1490/1724]  eta: 0:22:58  lr: 0.000040  loss: 46.1644 (57.0870)  loss_n_40: 0.7799 (1.2629)  loss_n_60: 0.8275 (1.2319)  loss_n_80: 0.9277 (1.3155)  loss_n_100: 1.0147 (1.3862)  triple_100: 12.3126 (14.6337)  triple_80: 12.4845 (15.1437)  triple_60: 10.4942 (13.1181)  triple_40: 6.7850 (8.9951)  time: 5.8897  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1500/1724]  eta: 0:21:59  lr: 0.000040  loss: 45.6740 (57.0177)  loss_n_40: 0.7265 (1.2597)  loss_n_60: 0.7815 (1.2293)  loss_n_80: 0.9010 (1.3130)  loss_n_100: 0.9856 (1.3837)  triple_100: 12.3126 (14.6195)  triple_80: 12.4845 (15.1273)  triple_60: 10.3944 (13.1030)  triple_40: 6.7850 (8.9823)  time: 5.8890  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1510/1724]  eta: 0:21:00  lr: 0.000040  loss: 47.7899 (56.9594)  loss_n_40: 0.7151 (1.2574)  loss_n_60: 0.8090 (1.2273)  loss_n_80: 0.9138 (1.3109)  loss_n_100: 0.9989 (1.3817)  triple_100: 12.6965 (14.6062)  triple_80: 13.0798 (15.1129)  triple_60: 11.1666 (13.0906)  triple_40: 6.8352 (8.9724)  time: 5.8879  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1520/1724]  eta: 0:20:01  lr: 0.000040  loss: 46.7953 (56.8902)  loss_n_40: 0.8387 (1.2551)  loss_n_60: 0.8240 (1.2251)  loss_n_80: 0.9304 (1.3087)  loss_n_100: 1.0191 (1.3795)  triple_100: 12.4575 (14.5906)  triple_80: 12.5570 (15.0951)  triple_60: 10.5739 (13.0746)  triple_40: 6.8352 (8.9614)  time: 5.8880  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1530/1724]  eta: 0:19:02  lr: 0.000040  loss: 46.0650 (56.8230)  loss_n_40: 0.7511 (1.2522)  loss_n_60: 0.8240 (1.2225)  loss_n_80: 0.9103 (1.3061)  loss_n_100: 0.9608 (1.3769)  triple_100: 11.9622 (14.5754)  triple_80: 12.2674 (15.0781)  triple_60: 10.5739 (13.0604)  triple_40: 6.9238 (8.9512)  time: 5.8879  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1540/1724]  eta: 0:18:03  lr: 0.000040  loss: 46.7442 (56.7841)  loss_n_40: 0.8759 (1.2512)  loss_n_60: 0.8578 (1.2214)  loss_n_80: 0.9340 (1.3049)  loss_n_100: 0.9957 (1.3758)  triple_100: 12.4578 (14.5654)  triple_80: 12.6024 (15.0667)  triple_60: 11.0061 (13.0513)  triple_40: 7.7122 (8.9474)  time: 5.8870  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1550/1724]  eta: 0:17:04  lr: 0.000040  loss: 46.4020 (56.7163)  loss_n_40: 0.9085 (1.2488)  loss_n_60: 0.8314 (1.2193)  loss_n_80: 0.8942 (1.3029)  loss_n_100: 0.9836 (1.3738)  triple_100: 12.5505 (14.5512)  triple_80: 12.6024 (15.0504)  triple_60: 10.8194 (13.0361)  triple_40: 7.0601 (8.9338)  time: 5.8877  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1560/1724]  eta: 0:16:06  lr: 0.000040  loss: 45.7979 (56.6553)  loss_n_40: 0.8033 (1.2464)  loss_n_60: 0.8112 (1.2171)  loss_n_80: 0.8942 (1.3007)  loss_n_100: 0.9836 (1.3717)  triple_100: 12.1589 (14.5386)  triple_80: 12.2744 (15.0351)  triple_60: 10.7427 (13.0222)  triple_40: 6.6642 (8.9236)  time: 5.8902  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1570/1724]  eta: 0:15:07  lr: 0.000040  loss: 44.7405 (56.5703)  loss_n_40: 0.7119 (1.2429)  loss_n_60: 0.7493 (1.2140)  loss_n_80: 0.8359 (1.2975)  loss_n_100: 0.9123 (1.3686)  triple_100: 12.1589 (14.5206)  triple_80: 12.0321 (15.0144)  triple_60: 9.9909 (13.0035)  triple_40: 6.4819 (8.9088)  time: 5.8902  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1580/1724]  eta: 0:14:08  lr: 0.000040  loss: 42.7531 (56.5165)  loss_n_40: 0.6886 (1.2406)  loss_n_60: 0.7232 (1.2120)  loss_n_80: 0.8168 (1.2955)  loss_n_100: 0.8974 (1.3666)  triple_100: 11.5566 (14.5088)  triple_80: 11.5609 (15.0010)  triple_60: 9.9068 (12.9917)  triple_40: 6.5717 (8.9003)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1590/1724]  eta: 0:13:09  lr: 0.000040  loss: 42.8574 (56.4453)  loss_n_40: 0.6750 (1.2378)  loss_n_60: 0.7232 (1.2095)  loss_n_80: 0.8413 (1.2930)  loss_n_100: 0.9224 (1.3641)  triple_100: 11.5795 (14.4926)  triple_80: 11.6412 (14.9830)  triple_60: 9.9104 (12.9763)  triple_40: 6.6323 (8.8890)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1600/1724]  eta: 0:12:10  lr: 0.000040  loss: 42.5879 (56.3721)  loss_n_40: 0.6700 (1.2356)  loss_n_60: 0.7188 (1.2073)  loss_n_80: 0.8186 (1.2907)  loss_n_100: 0.8930 (1.3617)  triple_100: 11.5795 (14.4737)  triple_80: 11.6412 (14.9634)  triple_60: 9.9104 (12.9600)  triple_40: 6.5061 (8.8797)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1610/1724]  eta: 0:11:11  lr: 0.000040  loss: 44.3361 (56.3068)  loss_n_40: 0.8029 (1.2335)  loss_n_60: 0.8022 (1.2052)  loss_n_80: 0.8478 (1.2885)  loss_n_100: 0.9205 (1.3594)  triple_100: 11.3239 (14.4566)  triple_80: 11.6925 (14.9454)  triple_60: 10.3656 (12.9462)  triple_40: 7.0642 (8.8721)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1620/1724]  eta: 0:10:12  lr: 0.000040  loss: 43.6894 (56.2297)  loss_n_40: 0.8429 (1.2312)  loss_n_60: 0.8299 (1.2030)  loss_n_80: 0.9157 (1.2863)  loss_n_100: 0.9928 (1.3573)  triple_100: 11.3918 (14.4390)  triple_80: 11.4831 (14.9257)  triple_60: 9.8480 (12.9276)  triple_40: 7.0397 (8.8594)  time: 5.8884  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:3]  [1630/1724]  eta: 0:09:13  lr: 0.000040  loss: 41.1739 (56.1467)  loss_n_40: 0.7857 (1.2282)  loss_n_60: 0.7738 (1.2003)  loss_n_80: 0.8624 (1.2836)  loss_n_100: 0.9525 (1.3546)  triple_100: 11.3904 (14.4211)  triple_80: 11.2768 (14.9053)  triple_60: 9.4381 (12.9088)  triple_40: 6.3759 (8.8448)  time: 5.8892  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1640/1724]  eta: 0:08:14  lr: 0.000040  loss: 41.9383 (56.0667)  loss_n_40: 0.6907 (1.2253)  loss_n_60: 0.7195 (1.1977)  loss_n_80: 0.8204 (1.2811)  loss_n_100: 0.8864 (1.3522)  triple_100: 11.4529 (14.4048)  triple_80: 11.5395 (14.8865)  triple_60: 9.6176 (12.8903)  triple_40: 6.3124 (8.8288)  time: 5.8902  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1650/1724]  eta: 0:07:15  lr: 0.000040  loss: 43.1822 (55.9904)  loss_n_40: 0.7377 (1.2228)  loss_n_60: 0.7311 (1.1952)  loss_n_80: 0.8305 (1.2787)  loss_n_100: 0.9139 (1.3498)  triple_100: 11.5777 (14.3876)  triple_80: 11.7778 (14.8672)  triple_60: 9.9200 (12.8725)  triple_40: 6.3565 (8.8166)  time: 5.8897  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1660/1724]  eta: 0:06:17  lr: 0.000040  loss: 43.1340 (55.9170)  loss_n_40: 0.6902 (1.2201)  loss_n_60: 0.7169 (1.1927)  loss_n_80: 0.8213 (1.2760)  loss_n_100: 0.9042 (1.3472)  triple_100: 11.5777 (14.3709)  triple_80: 11.5981 (14.8487)  triple_60: 9.9581 (12.8563)  triple_40: 6.4746 (8.8051)  time: 5.8905  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:3]  [1670/1724]  eta: 0:05:18  lr: 0.000040  loss: 42.7490 (55.8585)  loss_n_40: 0.6767 (1.2181)  loss_n_60: 0.7128 (1.1908)  loss_n_80: 0.8213 (1.2743)  loss_n_100: 0.9018 (1.3455)  triple_100: 11.5813 (14.3580)  triple_80: 11.5981 (14.8342)  triple_60: 9.7876 (12.8427)  triple_40: 6.4686 (8.7949)  time: 5.8921  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1680/1724]  eta: 0:04:19  lr: 0.000040  loss: 43.0906 (55.7934)  loss_n_40: 0.6767 (1.2155)  loss_n_60: 0.7147 (1.1885)  loss_n_80: 0.8458 (1.2720)  loss_n_100: 0.9439 (1.3431)  triple_100: 11.5385 (14.3429)  triple_80: 11.9160 (14.8178)  triple_60: 9.9922 (12.8286)  triple_40: 6.4894 (8.7851)  time: 5.8927  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1690/1724]  eta: 0:03:20  lr: 0.000040  loss: 43.0906 (55.7239)  loss_n_40: 0.6869 (1.2131)  loss_n_60: 0.7147 (1.1862)  loss_n_80: 0.8458 (1.2697)  loss_n_100: 0.9439 (1.3409)  triple_100: 11.6860 (14.3274)  triple_80: 11.9160 (14.8004)  triple_60: 10.1027 (12.8128)  triple_40: 6.4788 (8.7733)  time: 5.8930  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1700/1724]  eta: 0:02:21  lr: 0.000040  loss: 42.5485 (55.6530)  loss_n_40: 0.6965 (1.2104)  loss_n_60: 0.7553 (1.1838)  loss_n_80: 0.8503 (1.2673)  loss_n_100: 0.9311 (1.3387)  triple_100: 11.7949 (14.3123)  triple_80: 11.7559 (14.7833)  triple_60: 9.8319 (12.7963)  triple_40: 6.2767 (8.7609)  time: 5.8921  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1710/1724]  eta: 0:01:22  lr: 0.000040  loss: 42.1490 (55.5831)  loss_n_40: 0.6929 (1.2083)  loss_n_60: 0.7540 (1.1819)  loss_n_80: 0.8659 (1.2656)  loss_n_100: 0.9820 (1.3371)  triple_100: 11.6570 (14.2971)  triple_80: 11.6343 (14.7657)  triple_60: 9.4774 (12.7792)  triple_40: 6.1696 (8.7482)  time: 5.8904  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1720/1724]  eta: 0:00:23  lr: 0.000040  loss: 42.7890 (55.5250)  loss_n_40: 0.6656 (1.2061)  loss_n_60: 0.7206 (1.1799)  loss_n_80: 0.8282 (1.2636)  loss_n_100: 0.8946 (1.3352)  triple_100: 11.6762 (14.2842)  triple_80: 11.6749 (14.7512)  triple_60: 9.8789 (12.7662)  triple_40: 6.1781 (8.7386)  time: 5.8908  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3]  [1723/1724]  eta: 0:00:05  lr: 0.000040  loss: 42.9300 (55.5095)  loss_n_40: 0.6576 (1.2054)  loss_n_60: 0.7206 (1.1793)  loss_n_80: 0.8264 (1.2630)  loss_n_100: 0.8946 (1.3346)  triple_100: 11.7710 (14.2811)  triple_80: 11.8982 (14.7475)  triple_60: 9.8914 (12.7628)  triple_40: 6.3253 (8.7359)  time: 5.8903  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:3] Total time: 2:49:15 (5.8909 s / it)\n",
      "Averaged stats: lr: 0.000040  loss: 42.9300 (55.5095)  loss_n_40: 0.6576 (1.2054)  loss_n_60: 0.7206 (1.1793)  loss_n_80: 0.8264 (1.2630)  loss_n_100: 0.8946 (1.3346)  triple_100: 11.7710 (14.2811)  triple_80: 11.8982 (14.7475)  triple_60: 9.8914 (12.7628)  triple_40: 6.3253 (8.7359)\n",
      "Valid: [epoch:3]  [  0/845]  eta: 0:21:19  loss: 44.7652 (44.7652)  loss_n_40: 0.7034 (0.7034)  loss_n_60: 0.7519 (0.7519)  loss_n_80: 0.8788 (0.8788)  loss_n_100: 1.0078 (1.0078)  triple_100: 12.7286 (12.7286)  triple_80: 12.5080 (12.5080)  triple_60: 10.1030 (10.1030)  triple_40: 6.0837 (6.0837)  time: 1.5144  data: 0.5401  max mem: 40153\n",
      "Valid: [epoch:3]  [ 10/845]  eta: 0:14:14  loss: 44.7652 (47.0127)  loss_n_40: 0.6365 (0.7594)  loss_n_60: 0.7375 (0.8259)  loss_n_80: 0.8872 (0.9557)  loss_n_100: 1.0037 (1.0579)  triple_100: 12.7286 (12.9446)  triple_80: 12.5080 (12.9788)  triple_60: 10.1030 (10.7042)  triple_40: 6.0837 (6.7861)  time: 1.0231  data: 0.0492  max mem: 40153\n",
      "Valid: [epoch:3]  [ 20/845]  eta: 0:13:45  loss: 43.1715 (46.5631)  loss_n_40: 0.5902 (0.7377)  loss_n_60: 0.7014 (0.8031)  loss_n_80: 0.8577 (0.9181)  loss_n_100: 0.9878 (1.0160)  triple_100: 12.4219 (12.7185)  triple_80: 12.2895 (12.7394)  triple_60: 9.9113 (10.6830)  triple_40: 5.9534 (6.9472)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [ 30/845]  eta: 0:13:28  loss: 42.3947 (46.7703)  loss_n_40: 0.5902 (0.7616)  loss_n_60: 0.6811 (0.8211)  loss_n_80: 0.7944 (0.9345)  loss_n_100: 0.8737 (1.0331)  triple_100: 12.1722 (12.7442)  triple_80: 11.8552 (12.7696)  triple_60: 9.7829 (10.7300)  triple_40: 5.9468 (6.9762)  time: 0.9748  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [ 40/845]  eta: 0:13:15  loss: 40.2637 (45.9248)  loss_n_40: 0.6044 (0.7551)  loss_n_60: 0.6127 (0.8101)  loss_n_80: 0.7035 (0.9221)  loss_n_100: 0.7859 (1.0138)  triple_100: 10.8430 (12.4756)  triple_80: 10.8710 (12.5360)  triple_60: 9.2187 (10.5035)  triple_40: 6.1942 (6.9086)  time: 0.9749  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [ 50/845]  eta: 0:13:03  loss: 39.3274 (45.6465)  loss_n_40: 0.5670 (0.7712)  loss_n_60: 0.5912 (0.8108)  loss_n_80: 0.6851 (0.9183)  loss_n_100: 0.7585 (1.0071)  triple_100: 10.8340 (12.3443)  triple_80: 10.8614 (12.4150)  triple_60: 9.0254 (10.4264)  triple_40: 6.1767 (6.9534)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [ 60/845]  eta: 0:12:52  loss: 42.2865 (47.0103)  loss_n_40: 0.5795 (0.8455)  loss_n_60: 0.7135 (0.8804)  loss_n_80: 0.8618 (0.9864)  loss_n_100: 0.9440 (1.0744)  triple_100: 11.3573 (12.6042)  triple_80: 11.5558 (12.7374)  triple_60: 9.5928 (10.7142)  triple_40: 6.3319 (7.1678)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [ 70/845]  eta: 0:12:41  loss: 44.4800 (46.2661)  loss_n_40: 0.6218 (0.8100)  loss_n_60: 0.7250 (0.8494)  loss_n_80: 0.8618 (0.9553)  loss_n_100: 0.9796 (1.0421)  triple_100: 12.4649 (12.4496)  triple_80: 12.3647 (12.5709)  triple_60: 10.2936 (10.5676)  triple_40: 6.3319 (7.0212)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [ 80/845]  eta: 0:12:31  loss: 40.3722 (45.5420)  loss_n_40: 0.6157 (0.7832)  loss_n_60: 0.6844 (0.8220)  loss_n_80: 0.7849 (0.9269)  loss_n_100: 0.8653 (1.0132)  triple_100: 10.9266 (12.2852)  triple_80: 11.0412 (12.3907)  triple_60: 9.5849 (10.4160)  triple_40: 6.0408 (6.9048)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [ 90/845]  eta: 0:12:20  loss: 40.4090 (45.1138)  loss_n_40: 0.6157 (0.7640)  loss_n_60: 0.6296 (0.8042)  loss_n_80: 0.7199 (0.9093)  loss_n_100: 0.7923 (0.9953)  triple_100: 10.9041 (12.2016)  triple_80: 10.9476 (12.2986)  triple_60: 9.3382 (10.3253)  triple_40: 6.0185 (6.8155)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [100/845]  eta: 0:12:10  loss: 38.9752 (44.8201)  loss_n_40: 0.5965 (0.7601)  loss_n_60: 0.5973 (0.7994)  loss_n_80: 0.6689 (0.9032)  loss_n_100: 0.7327 (0.9876)  triple_100: 10.5963 (12.1035)  triple_80: 10.5886 (12.2065)  triple_60: 9.0825 (10.2600)  triple_40: 6.2512 (6.7998)  time: 0.9765  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:3]  [110/845]  eta: 0:12:00  loss: 39.4915 (44.6642)  loss_n_40: 0.5965 (0.7570)  loss_n_60: 0.6076 (0.7955)  loss_n_80: 0.7015 (0.8997)  loss_n_100: 0.7728 (0.9850)  triple_100: 10.5978 (12.0737)  triple_80: 10.8627 (12.1720)  triple_60: 9.0669 (10.2189)  triple_40: 6.2932 (6.7624)  time: 0.9765  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:3]  [120/845]  eta: 0:11:50  loss: 42.8815 (44.5133)  loss_n_40: 0.5961 (0.7478)  loss_n_60: 0.6939 (0.7886)  loss_n_80: 0.8077 (0.8939)  loss_n_100: 0.9203 (0.9802)  triple_100: 11.9617 (12.0605)  triple_80: 11.9185 (12.1501)  triple_60: 9.7333 (10.1824)  triple_40: 6.2219 (6.7099)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [130/845]  eta: 0:11:40  loss: 42.6189 (44.7657)  loss_n_40: 0.5867 (0.7625)  loss_n_60: 0.6939 (0.7993)  loss_n_80: 0.7822 (0.9023)  loss_n_100: 0.8614 (0.9877)  triple_100: 11.9617 (12.0834)  triple_80: 11.8921 (12.1887)  triple_60: 9.5632 (10.2517)  triple_40: 6.0347 (6.7901)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [140/845]  eta: 0:11:30  loss: 41.7006 (44.6771)  loss_n_40: 0.6000 (0.7565)  loss_n_60: 0.6820 (0.7950)  loss_n_80: 0.8166 (0.8997)  loss_n_100: 0.9148 (0.9866)  triple_100: 11.9218 (12.0926)  triple_80: 11.8655 (12.1834)  triple_60: 9.8241 (10.2248)  triple_40: 5.9909 (6.7386)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [150/845]  eta: 0:11:20  loss: 42.7302 (44.8725)  loss_n_40: 0.6336 (0.7708)  loss_n_60: 0.7196 (0.8097)  loss_n_80: 0.8489 (0.9164)  loss_n_100: 0.9823 (1.0052)  triple_100: 12.0180 (12.1452)  triple_80: 11.8655 (12.2328)  triple_60: 9.8553 (10.2438)  triple_40: 5.8741 (6.7486)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [160/845]  eta: 0:11:10  loss: 41.9297 (44.7232)  loss_n_40: 0.6065 (0.7583)  loss_n_60: 0.7162 (0.8000)  loss_n_80: 0.8303 (0.9073)  loss_n_100: 0.9374 (0.9964)  triple_100: 12.0180 (12.1319)  triple_80: 11.8056 (12.2082)  triple_60: 9.6735 (10.2180)  triple_40: 5.9899 (6.7030)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [170/845]  eta: 0:11:00  loss: 40.8941 (44.6479)  loss_n_40: 0.5824 (0.7622)  loss_n_60: 0.6321 (0.8005)  loss_n_80: 0.7063 (0.9064)  loss_n_100: 0.7776 (0.9954)  triple_100: 11.0824 (12.0949)  triple_80: 11.1232 (12.1695)  triple_60: 9.4614 (10.1982)  triple_40: 6.1404 (6.7208)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [180/845]  eta: 0:10:51  loss: 40.8941 (44.7894)  loss_n_40: 0.6139 (0.7701)  loss_n_60: 0.6975 (0.8085)  loss_n_80: 0.8309 (0.9162)  loss_n_100: 0.9387 (1.0065)  triple_100: 11.0824 (12.1400)  triple_80: 11.1306 (12.2132)  triple_60: 9.4571 (10.2140)  triple_40: 6.2039 (6.7210)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [190/845]  eta: 0:10:41  loss: 43.6093 (45.1290)  loss_n_40: 0.6779 (0.7808)  loss_n_60: 0.7693 (0.8189)  loss_n_80: 0.9301 (0.9272)  loss_n_100: 1.0562 (1.0190)  triple_100: 12.5102 (12.2236)  triple_80: 12.3139 (12.2919)  triple_60: 9.7692 (10.2855)  triple_40: 6.0560 (6.7822)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [200/845]  eta: 0:10:31  loss: 41.4860 (45.3267)  loss_n_40: 0.6596 (0.7920)  loss_n_60: 0.7162 (0.8287)  loss_n_80: 0.8337 (0.9359)  loss_n_100: 0.9273 (1.0274)  triple_100: 11.4973 (12.2483)  triple_80: 11.3303 (12.3287)  triple_60: 9.5382 (10.3331)  triple_40: 6.0560 (6.8326)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [210/845]  eta: 0:10:21  loss: 42.2145 (45.3658)  loss_n_40: 0.5895 (0.7976)  loss_n_60: 0.6695 (0.8317)  loss_n_80: 0.7744 (0.9372)  loss_n_100: 0.8779 (1.0277)  triple_100: 11.4185 (12.2353)  triple_80: 11.8762 (12.3306)  triple_60: 10.1568 (10.3560)  triple_40: 6.3064 (6.8497)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [220/845]  eta: 0:10:11  loss: 43.6600 (45.7305)  loss_n_40: 0.6553 (0.8153)  loss_n_60: 0.7058 (0.8474)  loss_n_80: 0.7968 (0.9511)  loss_n_100: 0.8857 (1.0413)  triple_100: 11.8602 (12.2904)  triple_80: 12.0424 (12.4007)  triple_60: 10.1274 (10.4489)  triple_40: 6.3712 (6.9356)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [230/845]  eta: 0:10:01  loss: 43.1723 (45.9240)  loss_n_40: 0.6704 (0.8255)  loss_n_60: 0.7169 (0.8565)  loss_n_80: 0.8109 (0.9597)  loss_n_100: 0.9274 (1.0496)  triple_100: 12.1752 (12.3248)  triple_80: 12.0424 (12.4452)  triple_60: 9.7483 (10.4936)  triple_40: 6.3024 (6.9691)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [240/845]  eta: 0:09:51  loss: 40.1913 (46.0548)  loss_n_40: 0.6193 (0.8309)  loss_n_60: 0.6726 (0.8602)  loss_n_80: 0.7864 (0.9623)  loss_n_100: 0.8525 (1.0518)  triple_100: 11.3363 (12.3365)  triple_80: 11.2765 (12.4611)  triple_60: 9.4226 (10.5282)  triple_40: 6.1596 (7.0238)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [250/845]  eta: 0:09:42  loss: 40.8826 (46.0061)  loss_n_40: 0.6060 (0.8276)  loss_n_60: 0.6695 (0.8575)  loss_n_80: 0.7647 (0.9597)  loss_n_100: 0.8319 (1.0493)  triple_100: 11.0481 (12.3355)  triple_80: 11.2437 (12.4550)  triple_60: 9.5136 (10.5174)  triple_40: 6.1596 (7.0042)  time: 0.9768  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:3]  [260/845]  eta: 0:09:32  loss: 40.6403 (45.8070)  loss_n_40: 0.5900 (0.8213)  loss_n_60: 0.6138 (0.8511)  loss_n_80: 0.7033 (0.9528)  loss_n_100: 0.7667 (1.0419)  triple_100: 10.9818 (12.2870)  triple_80: 10.9183 (12.4025)  triple_60: 9.3927 (10.4718)  triple_40: 6.2309 (6.9785)  time: 0.9767  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:3]  [270/845]  eta: 0:09:22  loss: 39.0725 (45.6967)  loss_n_40: 0.5571 (0.8208)  loss_n_60: 0.5889 (0.8486)  loss_n_80: 0.6727 (0.9494)  loss_n_100: 0.7324 (1.0375)  triple_100: 10.6998 (12.2472)  triple_80: 10.7832 (12.3683)  triple_60: 9.1433 (10.4502)  triple_40: 6.2309 (6.9748)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [280/845]  eta: 0:09:12  loss: 39.7570 (45.6544)  loss_n_40: 0.5492 (0.8206)  loss_n_60: 0.6111 (0.8485)  loss_n_80: 0.7580 (0.9485)  loss_n_100: 0.8516 (1.0363)  triple_100: 10.8771 (12.2269)  triple_80: 10.8427 (12.3551)  triple_60: 9.2297 (10.4478)  triple_40: 6.2681 (6.9708)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [290/845]  eta: 0:09:02  loss: 38.5360 (45.5312)  loss_n_40: 0.5347 (0.8155)  loss_n_60: 0.5221 (0.8423)  loss_n_80: 0.6034 (0.9410)  loss_n_100: 0.6667 (1.0277)  triple_100: 10.1547 (12.1837)  triple_80: 10.2681 (12.3124)  triple_60: 8.9165 (10.4277)  triple_40: 6.3158 (6.9809)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [300/845]  eta: 0:08:53  loss: 39.4464 (45.4803)  loss_n_40: 0.5703 (0.8090)  loss_n_60: 0.5740 (0.8376)  loss_n_80: 0.6632 (0.9370)  loss_n_100: 0.7433 (1.0242)  triple_100: 10.0473 (12.1899)  triple_80: 10.2377 (12.3116)  triple_60: 9.2947 (10.4178)  triple_40: 6.2911 (6.9531)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [310/845]  eta: 0:08:43  loss: 41.5259 (45.3228)  loss_n_40: 0.5749 (0.8004)  loss_n_60: 0.6419 (0.8300)  loss_n_80: 0.7550 (0.9296)  loss_n_100: 0.8331 (1.0164)  triple_100: 11.5704 (12.1574)  triple_80: 11.6197 (12.2755)  triple_60: 9.4736 (10.3846)  triple_40: 6.1521 (6.9290)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [320/845]  eta: 0:08:33  loss: 40.1470 (45.3922)  loss_n_40: 0.5454 (0.8061)  loss_n_60: 0.6442 (0.8340)  loss_n_80: 0.7561 (0.9330)  loss_n_100: 0.8344 (1.0190)  triple_100: 10.9647 (12.1584)  triple_80: 11.0616 (12.2884)  triple_60: 9.4660 (10.4047)  triple_40: 6.1521 (6.9485)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [330/845]  eta: 0:08:23  loss: 42.4796 (45.3843)  loss_n_40: 0.6549 (0.8061)  loss_n_60: 0.7021 (0.8342)  loss_n_80: 0.8144 (0.9339)  loss_n_100: 0.9154 (1.0207)  triple_100: 11.9821 (12.1678)  triple_80: 11.8979 (12.2910)  triple_60: 9.8475 (10.3976)  triple_40: 6.1160 (6.9331)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [340/845]  eta: 0:08:13  loss: 43.1719 (45.3146)  loss_n_40: 0.6295 (0.8029)  loss_n_60: 0.7021 (0.8328)  loss_n_80: 0.8169 (0.9334)  loss_n_100: 0.9210 (1.0208)  triple_100: 11.9821 (12.1606)  triple_80: 11.9557 (12.2807)  triple_60: 9.7346 (10.3781)  triple_40: 6.0974 (6.9052)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [350/845]  eta: 0:08:04  loss: 41.5829 (45.2616)  loss_n_40: 0.6295 (0.8011)  loss_n_60: 0.7105 (0.8314)  loss_n_80: 0.8166 (0.9329)  loss_n_100: 0.8916 (1.0208)  triple_100: 11.4244 (12.1573)  triple_80: 11.3872 (12.2727)  triple_60: 9.5118 (10.3597)  triple_40: 6.0295 (6.8856)  time: 0.9764  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:3]  [360/845]  eta: 0:07:54  loss: 41.9590 (45.3786)  loss_n_40: 0.6591 (0.8057)  loss_n_60: 0.7234 (0.8351)  loss_n_80: 0.8238 (0.9364)  loss_n_100: 0.9525 (1.0244)  triple_100: 11.8798 (12.1810)  triple_80: 11.6412 (12.3025)  triple_60: 9.5118 (10.3901)  triple_40: 6.0004 (6.9033)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [370/845]  eta: 0:07:44  loss: 43.6358 (45.4112)  loss_n_40: 0.6575 (0.8058)  loss_n_60: 0.7311 (0.8357)  loss_n_80: 0.8656 (0.9376)  loss_n_100: 0.9782 (1.0261)  triple_100: 12.1545 (12.1966)  triple_80: 12.3074 (12.3155)  triple_60: 9.9696 (10.3955)  triple_40: 5.9435 (6.8985)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [380/845]  eta: 0:07:34  loss: 44.3860 (45.3891)  loss_n_40: 0.6364 (0.8047)  loss_n_60: 0.7289 (0.8352)  loss_n_80: 0.8507 (0.9378)  loss_n_100: 0.9422 (1.0269)  triple_100: 12.3694 (12.2017)  triple_80: 12.4286 (12.3150)  triple_60: 9.9696 (10.3863)  triple_40: 6.0571 (6.8814)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [390/845]  eta: 0:07:24  loss: 39.6838 (45.3504)  loss_n_40: 0.6364 (0.8047)  loss_n_60: 0.6900 (0.8343)  loss_n_80: 0.7984 (0.9365)  loss_n_100: 0.8830 (1.0254)  triple_100: 11.0566 (12.1870)  triple_80: 11.0071 (12.2999)  triple_60: 8.9919 (10.3773)  triple_40: 6.1446 (6.8852)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [400/845]  eta: 0:07:15  loss: 38.6398 (45.3387)  loss_n_40: 0.5734 (0.8034)  loss_n_60: 0.5969 (0.8330)  loss_n_80: 0.6841 (0.9348)  loss_n_100: 0.7530 (1.0233)  triple_100: 10.6050 (12.1795)  triple_80: 10.6908 (12.2932)  triple_60: 8.9919 (10.3777)  triple_40: 6.0056 (6.8939)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [410/845]  eta: 0:07:05  loss: 41.1701 (45.2920)  loss_n_40: 0.5855 (0.8009)  loss_n_60: 0.6660 (0.8326)  loss_n_80: 0.7923 (0.9351)  loss_n_100: 0.9054 (1.0242)  triple_100: 11.5801 (12.1791)  triple_80: 11.4472 (12.2896)  triple_60: 9.4353 (10.3635)  triple_40: 5.9551 (6.8669)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [420/845]  eta: 0:06:55  loss: 41.1701 (45.1906)  loss_n_40: 0.5747 (0.7951)  loss_n_60: 0.6796 (0.8277)  loss_n_80: 0.8319 (0.9302)  loss_n_100: 0.9512 (1.0193)  triple_100: 11.2289 (12.1598)  triple_80: 11.4073 (12.2674)  triple_60: 9.5128 (10.3425)  triple_40: 5.9276 (6.8486)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [430/845]  eta: 0:06:45  loss: 41.5712 (45.1248)  loss_n_40: 0.5747 (0.7909)  loss_n_60: 0.6507 (0.8242)  loss_n_80: 0.7663 (0.9273)  loss_n_100: 0.8652 (1.0165)  triple_100: 11.3181 (12.1528)  triple_80: 11.4073 (12.2572)  triple_60: 9.5128 (10.3257)  triple_40: 5.9823 (6.8301)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [440/845]  eta: 0:06:35  loss: 44.3061 (45.1106)  loss_n_40: 0.5915 (0.7907)  loss_n_60: 0.6961 (0.8238)  loss_n_80: 0.7981 (0.9273)  loss_n_100: 0.8945 (1.0165)  triple_100: 12.4010 (12.1527)  triple_80: 12.2784 (12.2575)  triple_60: 9.7365 (10.3191)  triple_40: 6.0861 (6.8229)  time: 0.9768  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:3]  [450/845]  eta: 0:06:26  loss: 42.9146 (45.2356)  loss_n_40: 0.5876 (0.7992)  loss_n_60: 0.6533 (0.8301)  loss_n_80: 0.7872 (0.9329)  loss_n_100: 0.9018 (1.0213)  triple_100: 12.0492 (12.1633)  triple_80: 12.1020 (12.2828)  triple_60: 9.7365 (10.3483)  triple_40: 6.3266 (6.8578)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [460/845]  eta: 0:06:16  loss: 41.8561 (45.3075)  loss_n_40: 0.5900 (0.8007)  loss_n_60: 0.6460 (0.8315)  loss_n_80: 0.7695 (0.9345)  loss_n_100: 0.8656 (1.0227)  triple_100: 11.7927 (12.1820)  triple_80: 11.6091 (12.3015)  triple_60: 9.7666 (10.3638)  triple_40: 6.3266 (6.8709)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [470/845]  eta: 0:06:06  loss: 41.8561 (45.3982)  loss_n_40: 0.6025 (0.8076)  loss_n_60: 0.6782 (0.8367)  loss_n_80: 0.7936 (0.9391)  loss_n_100: 0.8981 (1.0271)  triple_100: 12.0606 (12.1926)  triple_80: 11.8002 (12.3180)  triple_60: 9.7666 (10.3841)  triple_40: 6.2474 (6.8930)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [480/845]  eta: 0:05:56  loss: 43.5250 (45.4622)  loss_n_40: 0.6081 (0.8089)  loss_n_60: 0.7452 (0.8383)  loss_n_80: 0.8640 (0.9408)  loss_n_100: 0.9767 (1.0289)  triple_100: 12.0871 (12.2087)  triple_80: 12.1691 (12.3341)  triple_60: 9.9875 (10.3984)  triple_40: 6.1070 (6.9040)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [490/845]  eta: 0:05:47  loss: 41.0980 (45.3535)  loss_n_40: 0.5947 (0.8041)  loss_n_60: 0.6336 (0.8338)  loss_n_80: 0.7312 (0.9362)  loss_n_100: 0.8018 (1.0240)  triple_100: 11.1727 (12.1853)  triple_80: 11.2558 (12.3087)  triple_60: 9.5559 (10.3763)  triple_40: 6.1232 (6.8851)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [500/845]  eta: 0:05:37  loss: 42.0862 (45.3798)  loss_n_40: 0.5827 (0.8032)  loss_n_60: 0.6428 (0.8334)  loss_n_80: 0.7587 (0.9360)  loss_n_100: 0.8237 (1.0239)  triple_100: 11.5497 (12.1971)  triple_80: 11.6760 (12.3194)  triple_60: 9.6613 (10.3842)  triple_40: 6.0389 (6.8825)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [510/845]  eta: 0:05:27  loss: 45.1698 (45.3622)  loss_n_40: 0.6812 (0.8024)  loss_n_60: 0.7377 (0.8332)  loss_n_80: 0.8902 (0.9362)  loss_n_100: 0.9970 (1.0248)  triple_100: 12.7930 (12.2016)  triple_80: 12.7432 (12.3192)  triple_60: 10.3394 (10.3774)  triple_40: 6.0237 (6.8673)  time: 0.9771  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [520/845]  eta: 0:05:17  loss: 40.4563 (45.2722)  loss_n_40: 0.6247 (0.7988)  loss_n_60: 0.7058 (0.8304)  loss_n_80: 0.8314 (0.9336)  loss_n_100: 0.9630 (1.0224)  triple_100: 11.3395 (12.1829)  triple_80: 10.9524 (12.2970)  triple_60: 9.6120 (10.3552)  triple_40: 6.0596 (6.8520)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [530/845]  eta: 0:05:07  loss: 39.8188 (45.2962)  loss_n_40: 0.5647 (0.7974)  loss_n_60: 0.6453 (0.8295)  loss_n_80: 0.7450 (0.9327)  loss_n_100: 0.8286 (1.0213)  triple_100: 10.9827 (12.1898)  triple_80: 10.9205 (12.3042)  triple_60: 9.4301 (10.3652)  triple_40: 6.1358 (6.8561)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [540/845]  eta: 0:04:58  loss: 39.7185 (45.2683)  loss_n_40: 0.5903 (0.7976)  loss_n_60: 0.6353 (0.8290)  loss_n_80: 0.7314 (0.9316)  loss_n_100: 0.7811 (1.0199)  triple_100: 10.8493 (12.1776)  triple_80: 10.9205 (12.2943)  triple_60: 9.4301 (10.3618)  triple_40: 6.1011 (6.8563)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [550/845]  eta: 0:04:48  loss: 39.2243 (45.2363)  loss_n_40: 0.5528 (0.7956)  loss_n_60: 0.5649 (0.8268)  loss_n_80: 0.6398 (0.9292)  loss_n_100: 0.7095 (1.0173)  triple_100: 10.5755 (12.1689)  triple_80: 10.4746 (12.2836)  triple_60: 9.0023 (10.3564)  triple_40: 6.1476 (6.8584)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [560/845]  eta: 0:04:38  loss: 39.8956 (45.2468)  loss_n_40: 0.6482 (0.7979)  loss_n_60: 0.6230 (0.8284)  loss_n_80: 0.7136 (0.9305)  loss_n_100: 0.7909 (1.0186)  triple_100: 10.9400 (12.1644)  triple_80: 11.0604 (12.2843)  triple_60: 8.9911 (10.3609)  triple_40: 6.2761 (6.8619)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [570/845]  eta: 0:04:28  loss: 42.2485 (45.2112)  loss_n_40: 0.6464 (0.7948)  loss_n_60: 0.7455 (0.8262)  loss_n_80: 0.8834 (0.9285)  loss_n_100: 1.0050 (1.0168)  triple_100: 11.3298 (12.1615)  triple_80: 11.4744 (12.2785)  triple_60: 9.6064 (10.3534)  triple_40: 6.2761 (6.8517)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [580/845]  eta: 0:04:18  loss: 40.1715 (45.1902)  loss_n_40: 0.6201 (0.7958)  loss_n_60: 0.7063 (0.8267)  loss_n_80: 0.8192 (0.9289)  loss_n_100: 0.9274 (1.0172)  triple_100: 11.2283 (12.1508)  triple_80: 11.1794 (12.2716)  triple_60: 9.1498 (10.3485)  triple_40: 6.1087 (6.8506)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [590/845]  eta: 0:04:09  loss: 41.3648 (45.2585)  loss_n_40: 0.6145 (0.7992)  loss_n_60: 0.6735 (0.8292)  loss_n_80: 0.7790 (0.9313)  loss_n_100: 0.8770 (1.0195)  triple_100: 11.4983 (12.1622)  triple_80: 11.4674 (12.2869)  triple_60: 9.1589 (10.3639)  triple_40: 6.1049 (6.8664)  time: 0.9764  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:3]  [600/845]  eta: 0:03:59  loss: 42.1827 (45.2055)  loss_n_40: 0.6239 (0.7967)  loss_n_60: 0.6748 (0.8270)  loss_n_80: 0.8089 (0.9296)  loss_n_100: 0.8872 (1.0180)  triple_100: 11.7838 (12.1550)  triple_80: 11.8444 (12.2767)  triple_60: 9.4907 (10.3489)  triple_40: 6.1049 (6.8535)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [610/845]  eta: 0:03:49  loss: 41.8057 (45.1532)  loss_n_40: 0.6069 (0.7941)  loss_n_60: 0.6652 (0.8249)  loss_n_80: 0.7721 (0.9275)  loss_n_100: 0.8741 (1.0160)  triple_100: 11.7191 (12.1470)  triple_80: 11.6419 (12.2647)  triple_60: 9.2227 (10.3366)  triple_40: 6.0377 (6.8423)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [620/845]  eta: 0:03:39  loss: 40.1566 (45.1841)  loss_n_40: 0.5284 (0.7982)  loss_n_60: 0.6134 (0.8271)  loss_n_80: 0.6936 (0.9288)  loss_n_100: 0.7690 (1.0168)  triple_100: 11.1587 (12.1376)  triple_80: 11.1479 (12.2651)  triple_60: 9.2227 (10.3486)  triple_40: 6.0724 (6.8619)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [630/845]  eta: 0:03:30  loss: 45.3264 (45.2096)  loss_n_40: 0.5816 (0.7971)  loss_n_60: 0.6962 (0.8270)  loss_n_80: 0.8374 (0.9289)  loss_n_100: 0.9553 (1.0170)  triple_100: 12.1128 (12.1504)  triple_80: 12.5593 (12.2767)  triple_60: 10.3883 (10.3564)  triple_40: 6.1540 (6.8561)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [640/845]  eta: 0:03:20  loss: 43.6511 (45.1802)  loss_n_40: 0.5739 (0.7971)  loss_n_60: 0.6962 (0.8262)  loss_n_80: 0.7825 (0.9276)  loss_n_100: 0.8753 (1.0152)  triple_100: 12.1128 (12.1347)  triple_80: 12.1389 (12.2660)  triple_60: 10.3883 (10.3529)  triple_40: 6.1433 (6.8604)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [650/845]  eta: 0:03:10  loss: 40.9040 (45.2614)  loss_n_40: 0.5827 (0.8040)  loss_n_60: 0.6429 (0.8319)  loss_n_80: 0.7362 (0.9328)  loss_n_100: 0.8053 (1.0203)  triple_100: 11.2726 (12.1433)  triple_80: 11.3028 (12.2817)  triple_60: 9.6436 (10.3710)  triple_40: 6.1363 (6.8764)  time: 0.9764  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:3]  [660/845]  eta: 0:03:00  loss: 41.4625 (45.2321)  loss_n_40: 0.5960 (0.8023)  loss_n_60: 0.6606 (0.8312)  loss_n_80: 0.7583 (0.9322)  loss_n_100: 0.8437 (1.0199)  triple_100: 11.6266 (12.1402)  triple_80: 11.5627 (12.2763)  triple_60: 9.6436 (10.3637)  triple_40: 6.1205 (6.8663)  time: 0.9766  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:3]  [670/845]  eta: 0:02:51  loss: 41.4625 (45.2118)  loss_n_40: 0.5925 (0.8004)  loss_n_60: 0.6606 (0.8300)  loss_n_80: 0.8005 (0.9314)  loss_n_100: 0.8880 (1.0194)  triple_100: 11.7269 (12.1430)  triple_80: 11.5627 (12.2750)  triple_60: 9.5907 (10.3578)  triple_40: 6.0727 (6.8547)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [680/845]  eta: 0:02:41  loss: 40.8350 (45.1850)  loss_n_40: 0.5594 (0.8000)  loss_n_60: 0.6248 (0.8297)  loss_n_80: 0.7374 (0.9310)  loss_n_100: 0.8244 (1.0191)  triple_100: 11.5517 (12.1369)  triple_80: 11.5259 (12.2667)  triple_60: 9.3902 (10.3494)  triple_40: 6.0727 (6.8523)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [690/845]  eta: 0:02:31  loss: 41.4950 (45.1283)  loss_n_40: 0.5940 (0.7987)  loss_n_60: 0.6248 (0.8283)  loss_n_80: 0.7354 (0.9297)  loss_n_100: 0.7985 (1.0180)  triple_100: 11.2724 (12.1256)  triple_80: 11.1223 (12.2523)  triple_60: 9.0677 (10.3334)  triple_40: 6.0179 (6.8423)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [700/845]  eta: 0:02:21  loss: 41.5679 (45.0697)  loss_n_40: 0.5694 (0.7956)  loss_n_60: 0.6385 (0.8260)  loss_n_80: 0.7354 (0.9275)  loss_n_100: 0.7985 (1.0158)  triple_100: 11.2724 (12.1152)  triple_80: 11.1223 (12.2395)  triple_60: 9.3387 (10.3208)  triple_40: 5.9770 (6.8293)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [710/845]  eta: 0:02:11  loss: 42.8446 (45.0530)  loss_n_40: 0.5570 (0.7947)  loss_n_60: 0.6593 (0.8260)  loss_n_80: 0.7944 (0.9278)  loss_n_100: 0.8973 (1.0162)  triple_100: 12.0894 (12.1163)  triple_80: 12.0733 (12.2391)  triple_60: 9.8758 (10.3161)  triple_40: 5.9049 (6.8169)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [720/845]  eta: 0:02:02  loss: 42.8446 (45.1206)  loss_n_40: 0.5434 (0.7983)  loss_n_60: 0.6442 (0.8289)  loss_n_80: 0.7599 (0.9302)  loss_n_100: 0.8582 (1.0184)  triple_100: 11.9605 (12.1265)  triple_80: 12.0733 (12.2521)  triple_60: 9.8758 (10.3333)  triple_40: 5.9238 (6.8329)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [730/845]  eta: 0:01:52  loss: 40.2208 (45.1803)  loss_n_40: 0.5954 (0.8014)  loss_n_60: 0.6175 (0.8312)  loss_n_80: 0.7358 (0.9323)  loss_n_100: 0.8334 (1.0205)  triple_100: 11.3185 (12.1361)  triple_80: 11.3650 (12.2644)  triple_60: 9.1194 (10.3458)  triple_40: 6.1732 (6.8486)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [740/845]  eta: 0:01:42  loss: 40.5907 (45.2142)  loss_n_40: 0.6013 (0.8038)  loss_n_60: 0.6175 (0.8329)  loss_n_80: 0.7358 (0.9341)  loss_n_100: 0.8428 (1.0224)  triple_100: 11.4300 (12.1434)  triple_80: 11.4431 (12.2698)  triple_60: 9.0183 (10.3504)  triple_40: 6.1732 (6.8575)  time: 0.9767  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:3]  [750/845]  eta: 0:01:32  loss: 45.3482 (45.3057)  loss_n_40: 0.6381 (0.8088)  loss_n_60: 0.7608 (0.8376)  loss_n_80: 0.8722 (0.9386)  loss_n_100: 0.9723 (1.0270)  triple_100: 12.8642 (12.1588)  triple_80: 12.7956 (12.2909)  triple_60: 10.0552 (10.3710)  triple_40: 6.2448 (6.8729)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [760/845]  eta: 0:01:23  loss: 42.4251 (45.2998)  loss_n_40: 0.6381 (0.8087)  loss_n_60: 0.6803 (0.8377)  loss_n_80: 0.8256 (0.9386)  loss_n_100: 0.9549 (1.0271)  triple_100: 12.2381 (12.1585)  triple_80: 12.0909 (12.2888)  triple_60: 9.8383 (10.3697)  triple_40: 5.9879 (6.8706)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [770/845]  eta: 0:01:13  loss: 41.3719 (45.2633)  loss_n_40: 0.6292 (0.8068)  loss_n_60: 0.6803 (0.8360)  loss_n_80: 0.8256 (0.9374)  loss_n_100: 0.9447 (1.0260)  triple_100: 11.7574 (12.1549)  triple_80: 11.5785 (12.2826)  triple_60: 9.5583 (10.3597)  triple_40: 5.9879 (6.8599)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [780/845]  eta: 0:01:03  loss: 41.6955 (45.2466)  loss_n_40: 0.6049 (0.8052)  loss_n_60: 0.6993 (0.8353)  loss_n_80: 0.8326 (0.9368)  loss_n_100: 0.9447 (1.0256)  triple_100: 11.7731 (12.1552)  triple_80: 11.7512 (12.2817)  triple_60: 9.5572 (10.3559)  triple_40: 6.1134 (6.8510)  time: 0.9770  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [790/845]  eta: 0:00:53  loss: 41.6955 (45.2257)  loss_n_40: 0.5897 (0.8044)  loss_n_60: 0.6993 (0.8352)  loss_n_80: 0.8409 (0.9368)  loss_n_100: 0.8761 (1.0257)  triple_100: 11.6440 (12.1536)  triple_80: 11.4967 (12.2785)  triple_60: 9.5528 (10.3489)  triple_40: 6.3226 (6.8425)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [800/845]  eta: 0:00:43  loss: 41.7885 (45.1768)  loss_n_40: 0.5872 (0.8028)  loss_n_60: 0.6591 (0.8335)  loss_n_80: 0.7910 (0.9350)  loss_n_100: 0.8761 (1.0240)  triple_100: 11.2389 (12.1442)  triple_80: 11.3468 (12.2667)  triple_60: 9.4001 (10.3379)  triple_40: 6.2057 (6.8328)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [810/845]  eta: 0:00:34  loss: 41.7885 (45.1776)  loss_n_40: 0.6164 (0.8014)  loss_n_60: 0.6603 (0.8328)  loss_n_80: 0.7910 (0.9345)  loss_n_100: 0.8754 (1.0235)  triple_100: 11.4524 (12.1485)  triple_80: 11.3468 (12.2695)  triple_60: 9.5527 (10.3391)  triple_40: 6.1951 (6.8283)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [820/845]  eta: 0:00:24  loss: 44.8326 (45.1913)  loss_n_40: 0.6064 (0.8014)  loss_n_60: 0.6976 (0.8330)  loss_n_80: 0.8403 (0.9347)  loss_n_100: 0.9418 (1.0238)  triple_100: 12.1876 (12.1534)  triple_80: 12.1712 (12.2734)  triple_60: 9.9455 (10.3413)  triple_40: 6.1833 (6.8304)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [830/845]  eta: 0:00:14  loss: 42.7363 (45.1774)  loss_n_40: 0.5705 (0.8007)  loss_n_60: 0.6867 (0.8327)  loss_n_80: 0.8403 (0.9346)  loss_n_100: 0.9418 (1.0238)  triple_100: 12.2003 (12.1527)  triple_80: 11.8702 (12.2720)  triple_60: 9.8323 (10.3368)  triple_40: 5.8915 (6.8240)  time: 0.9766  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:3]  [840/845]  eta: 0:00:04  loss: 43.7789 (45.1724)  loss_n_40: 0.5948 (0.8001)  loss_n_60: 0.6867 (0.8332)  loss_n_80: 0.8586 (0.9355)  loss_n_100: 0.9623 (1.0251)  triple_100: 12.3262 (12.1575)  triple_80: 12.3157 (12.2742)  triple_60: 9.9303 (10.3330)  triple_40: 5.9301 (6.8138)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3]  [844/845]  eta: 0:00:00  loss: 41.7907 (45.1420)  loss_n_40: 0.5819 (0.7992)  loss_n_60: 0.6786 (0.8323)  loss_n_80: 0.7891 (0.9345)  loss_n_100: 0.8942 (1.0241)  triple_100: 11.7470 (12.1499)  triple_80: 11.6548 (12.2663)  triple_60: 9.6232 (10.3264)  triple_40: 5.9922 (6.8093)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:3] Total time: 0:13:45 (0.9773 s / it)\n",
      "Averaged stats: loss: 41.7907 (45.1420)  loss_n_40: 0.5819 (0.7992)  loss_n_60: 0.6786 (0.8323)  loss_n_80: 0.7891 (0.9345)  loss_n_100: 0.8942 (1.0241)  triple_100: 11.7470 (12.1499)  triple_80: 11.6548 (12.2663)  triple_60: 9.6232 (10.3264)  triple_40: 5.9922 (6.8093)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle_2/low2high/epoch_3_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 1.024%\n",
      "Min loss_n_100: 1.024\n",
      "Best Epoch: 3.000\n",
      "Train: [epoch:4]  [   0/1724]  eta: 3:01:06  lr: 0.000060  loss: 40.5248 (40.5248)  loss_n_40: 0.5379 (0.5379)  loss_n_60: 0.6062 (0.6062)  loss_n_80: 0.7068 (0.7068)  loss_n_100: 0.7757 (0.7757)  triple_100: 11.0878 (11.0878)  triple_80: 11.2390 (11.2390)  triple_60: 9.4629 (9.4629)  triple_40: 6.1085 (6.1085)  time: 6.3031  data: 0.5723  max mem: 40153\n",
      "Train: [epoch:4]  [  10/1724]  eta: 2:49:32  lr: 0.000060  loss: 48.6324 (48.8256)  loss_n_40: 0.9025 (0.9020)  loss_n_60: 0.9388 (0.9458)  loss_n_80: 1.0732 (1.0931)  loss_n_100: 1.1697 (1.2027)  triple_100: 13.4826 (13.2208)  triple_80: 13.6389 (13.2817)  triple_60: 10.9828 (10.9780)  triple_40: 7.4833 (7.2015)  time: 5.9352  data: 0.0522  max mem: 40153\n",
      "Train: [epoch:4]  [  20/1724]  eta: 2:48:04  lr: 0.000060  loss: 48.6324 (48.5817)  loss_n_40: 0.8671 (0.8717)  loss_n_60: 0.9388 (0.9252)  loss_n_80: 1.0732 (1.0558)  loss_n_100: 1.1666 (1.1614)  triple_100: 13.3316 (13.1373)  triple_80: 13.2905 (13.2195)  triple_60: 10.9828 (11.0544)  triple_40: 7.2481 (7.1564)  time: 5.8991  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [  30/1724]  eta: 2:46:55  lr: 0.000060  loss: 49.0603 (49.0071)  loss_n_40: 0.8178 (0.8961)  loss_n_60: 0.8928 (0.9383)  loss_n_80: 1.0250 (1.0559)  loss_n_100: 1.1192 (1.1550)  triple_100: 13.0715 (13.1251)  triple_80: 12.9902 (13.2793)  triple_60: 11.3058 (11.2322)  triple_40: 7.4883 (7.3251)  time: 5.8997  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [  40/1724]  eta: 2:45:50  lr: 0.000060  loss: 50.6943 (49.5022)  loss_n_40: 1.0347 (0.9434)  loss_n_60: 0.9823 (0.9671)  loss_n_80: 1.0571 (1.0734)  loss_n_100: 1.1471 (1.1678)  triple_100: 12.9610 (13.1357)  triple_80: 13.1455 (13.3512)  triple_60: 11.8753 (11.3883)  triple_40: 8.0750 (7.4753)  time: 5.8995  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [  50/1724]  eta: 2:44:48  lr: 0.000060  loss: 48.5231 (49.0368)  loss_n_40: 0.8140 (0.9274)  loss_n_60: 0.8886 (0.9502)  loss_n_80: 0.9956 (1.0561)  loss_n_100: 1.0804 (1.1499)  triple_100: 12.9075 (13.0337)  triple_80: 13.1455 (13.2343)  triple_60: 11.2856 (11.2705)  triple_40: 6.9632 (7.4148)  time: 5.8988  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [  60/1724]  eta: 2:43:46  lr: 0.000060  loss: 45.3464 (48.5492)  loss_n_40: 0.7475 (0.9230)  loss_n_60: 0.7959 (0.9408)  loss_n_80: 0.8825 (1.0435)  loss_n_100: 0.9740 (1.1360)  triple_100: 12.4822 (12.8994)  triple_80: 12.4841 (13.1004)  triple_60: 10.3920 (11.1617)  triple_40: 6.6214 (7.3443)  time: 5.8985  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [  70/1724]  eta: 2:42:46  lr: 0.000060  loss: 44.0553 (48.2586)  loss_n_40: 0.7951 (0.9268)  loss_n_60: 0.7924 (0.9384)  loss_n_80: 0.8672 (1.0374)  loss_n_100: 0.9624 (1.1280)  triple_100: 12.0505 (12.7883)  triple_80: 12.3265 (12.9989)  triple_60: 10.1985 (11.0977)  triple_40: 6.5241 (7.3431)  time: 5.8990  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [  80/1724]  eta: 2:41:46  lr: 0.000060  loss: 44.0553 (48.0043)  loss_n_40: 0.7512 (0.9119)  loss_n_60: 0.7704 (0.9254)  loss_n_80: 0.8602 (1.0228)  loss_n_100: 0.9479 (1.1117)  triple_100: 12.0505 (12.7270)  triple_80: 12.3265 (12.9361)  triple_60: 10.3625 (11.0646)  triple_40: 6.7236 (7.3048)  time: 5.8993  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [  90/1724]  eta: 2:40:46  lr: 0.000060  loss: 43.6775 (47.5385)  loss_n_40: 0.6947 (0.9002)  loss_n_60: 0.7287 (0.9142)  loss_n_80: 0.8488 (1.0128)  loss_n_100: 0.9278 (1.1015)  triple_100: 11.8720 (12.6267)  triple_80: 12.0103 (12.8340)  triple_60: 9.8064 (10.9455)  triple_40: 6.3636 (7.2035)  time: 5.8990  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 100/1724]  eta: 2:39:46  lr: 0.000060  loss: 43.3508 (47.2555)  loss_n_40: 0.8087 (0.8999)  loss_n_60: 0.7963 (0.9103)  loss_n_80: 0.8911 (1.0089)  loss_n_100: 0.9903 (1.0972)  triple_100: 11.8250 (12.5549)  triple_80: 11.8651 (12.7590)  triple_60: 9.6817 (10.8649)  triple_40: 6.2021 (7.1603)  time: 5.8982  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 110/1724]  eta: 2:38:46  lr: 0.000060  loss: 42.2324 (46.9433)  loss_n_40: 0.8087 (0.8932)  loss_n_60: 0.7882 (0.9016)  loss_n_80: 0.8963 (0.9997)  loss_n_100: 0.9925 (1.0876)  triple_100: 11.7412 (12.4885)  triple_80: 11.7402 (12.6869)  triple_60: 9.6817 (10.7918)  triple_40: 6.2385 (7.0940)  time: 5.8974  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 120/1724]  eta: 2:37:46  lr: 0.000060  loss: 42.4909 (46.6378)  loss_n_40: 0.7427 (0.8834)  loss_n_60: 0.7477 (0.8934)  loss_n_80: 0.8639 (0.9915)  loss_n_100: 0.9484 (1.0789)  triple_100: 11.7412 (12.4317)  triple_80: 11.8635 (12.6187)  triple_60: 9.6819 (10.7162)  triple_40: 6.0890 (7.0240)  time: 5.8978  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 130/1724]  eta: 2:36:46  lr: 0.000060  loss: 43.1734 (46.4703)  loss_n_40: 0.7474 (0.8830)  loss_n_60: 0.7477 (0.8906)  loss_n_80: 0.8639 (0.9887)  loss_n_100: 0.9484 (1.0766)  triple_100: 11.8730 (12.3850)  triple_80: 11.9950 (12.5640)  triple_60: 9.7198 (10.6622)  triple_40: 6.1556 (7.0202)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 140/1724]  eta: 2:35:47  lr: 0.000060  loss: 44.4323 (46.4943)  loss_n_40: 0.8162 (0.8825)  loss_n_60: 0.8541 (0.8914)  loss_n_80: 0.9854 (0.9905)  loss_n_100: 1.0846 (1.0791)  triple_100: 12.1888 (12.4053)  triple_80: 12.1256 (12.5759)  triple_60: 10.0200 (10.6643)  triple_40: 6.2309 (7.0053)  time: 5.8951  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 150/1724]  eta: 2:34:47  lr: 0.000060  loss: 45.4615 (46.4724)  loss_n_40: 0.8458 (0.8859)  loss_n_60: 0.8785 (0.8929)  loss_n_80: 0.9894 (0.9911)  loss_n_100: 1.0979 (1.0788)  triple_100: 12.4754 (12.3870)  triple_80: 12.4421 (12.5622)  triple_60: 10.3735 (10.6593)  triple_40: 6.7540 (7.0152)  time: 5.8955  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 160/1724]  eta: 2:33:48  lr: 0.000060  loss: 44.7264 (46.4514)  loss_n_40: 0.8072 (0.8879)  loss_n_60: 0.8030 (0.8922)  loss_n_80: 0.9203 (0.9889)  loss_n_100: 1.0031 (1.0758)  triple_100: 12.0276 (12.3745)  triple_80: 12.2251 (12.5502)  triple_60: 10.3735 (10.6606)  triple_40: 6.7840 (7.0213)  time: 5.8953  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 170/1724]  eta: 2:32:48  lr: 0.000060  loss: 43.3987 (46.2733)  loss_n_40: 0.7657 (0.8857)  loss_n_60: 0.7842 (0.8875)  loss_n_80: 0.8888 (0.9838)  loss_n_100: 0.9610 (1.0706)  triple_100: 11.8185 (12.3220)  triple_80: 11.9164 (12.4960)  triple_60: 9.9306 (10.6174)  triple_40: 6.4887 (7.0103)  time: 5.8961  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 180/1724]  eta: 2:31:49  lr: 0.000060  loss: 42.8051 (46.1679)  loss_n_40: 0.8005 (0.8859)  loss_n_60: 0.7890 (0.8878)  loss_n_80: 0.9071 (0.9844)  loss_n_100: 1.0018 (1.0719)  triple_100: 11.4923 (12.2963)  triple_80: 11.5145 (12.4714)  triple_60: 9.5179 (10.5880)  triple_40: 6.3495 (6.9822)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 190/1724]  eta: 2:30:50  lr: 0.000060  loss: 42.5956 (45.9376)  loss_n_40: 0.7482 (0.8793)  loss_n_60: 0.7825 (0.8799)  loss_n_80: 0.8943 (0.9762)  loss_n_100: 0.9863 (1.0628)  triple_100: 11.3408 (12.2369)  triple_80: 11.4941 (12.4125)  triple_60: 9.4545 (10.5353)  triple_40: 6.2073 (6.9546)  time: 5.8954  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [ 200/1724]  eta: 2:29:50  lr: 0.000060  loss: 40.7651 (45.7184)  loss_n_40: 0.6383 (0.8708)  loss_n_60: 0.6732 (0.8722)  loss_n_80: 0.7644 (0.9681)  loss_n_100: 0.8326 (1.0535)  triple_100: 10.9197 (12.1769)  triple_80: 11.1632 (12.3589)  triple_60: 9.4545 (10.4916)  triple_40: 6.1451 (6.9263)  time: 5.8949  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 210/1724]  eta: 2:28:51  lr: 0.000060  loss: 40.8545 (45.5231)  loss_n_40: 0.6629 (0.8665)  loss_n_60: 0.6927 (0.8667)  loss_n_80: 0.7645 (0.9622)  loss_n_100: 0.8456 (1.0471)  triple_100: 10.7836 (12.1281)  triple_80: 11.0121 (12.3086)  triple_60: 9.4725 (10.4484)  triple_40: 6.1030 (6.8954)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 220/1724]  eta: 2:27:52  lr: 0.000060  loss: 41.5165 (45.4062)  loss_n_40: 0.7214 (0.8624)  loss_n_60: 0.7243 (0.8628)  loss_n_80: 0.8340 (0.9577)  loss_n_100: 0.9057 (1.0421)  triple_100: 10.9973 (12.0988)  triple_80: 11.3076 (12.2762)  triple_60: 9.4670 (10.4247)  triple_40: 6.0881 (6.8816)  time: 5.8975  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 230/1724]  eta: 2:26:53  lr: 0.000060  loss: 40.9724 (45.2845)  loss_n_40: 0.6695 (0.8587)  loss_n_60: 0.6916 (0.8587)  loss_n_80: 0.8001 (0.9529)  loss_n_100: 0.8910 (1.0371)  triple_100: 11.0292 (12.0644)  triple_80: 11.1709 (12.2402)  triple_60: 9.4670 (10.3998)  triple_40: 6.3001 (6.8726)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 240/1724]  eta: 2:25:54  lr: 0.000060  loss: 41.4518 (45.1952)  loss_n_40: 0.6695 (0.8558)  loss_n_60: 0.7338 (0.8558)  loss_n_80: 0.8238 (0.9499)  loss_n_100: 0.8987 (1.0337)  triple_100: 11.1823 (12.0424)  triple_80: 11.3480 (12.2166)  triple_60: 9.6831 (10.3816)  triple_40: 6.1515 (6.8594)  time: 5.8961  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 250/1724]  eta: 2:24:55  lr: 0.000060  loss: 41.4518 (45.0489)  loss_n_40: 0.7332 (0.8517)  loss_n_60: 0.7312 (0.8512)  loss_n_80: 0.7881 (0.9450)  loss_n_100: 0.8774 (1.0287)  triple_100: 11.2456 (12.0062)  triple_80: 11.3864 (12.1760)  triple_60: 9.6631 (10.3486)  triple_40: 6.0937 (6.8414)  time: 5.8978  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 260/1724]  eta: 2:23:56  lr: 0.000060  loss: 41.1993 (44.9476)  loss_n_40: 0.7458 (0.8485)  loss_n_60: 0.7541 (0.8482)  loss_n_80: 0.8552 (0.9415)  loss_n_100: 0.9413 (1.0249)  triple_100: 11.4191 (11.9782)  triple_80: 11.3455 (12.1468)  triple_60: 9.5670 (10.3294)  triple_40: 6.2452 (6.8300)  time: 5.8987  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 270/1724]  eta: 2:22:56  lr: 0.000060  loss: 42.2389 (44.9903)  loss_n_40: 0.7207 (0.8527)  loss_n_60: 0.7541 (0.8513)  loss_n_80: 0.8552 (0.9440)  loss_n_100: 0.9413 (1.0270)  triple_100: 11.4191 (11.9787)  triple_80: 11.7246 (12.1512)  triple_60: 9.7918 (10.3394)  triple_40: 6.2954 (6.8461)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 280/1724]  eta: 2:21:57  lr: 0.000060  loss: 41.6783 (44.8344)  loss_n_40: 0.6306 (0.8461)  loss_n_60: 0.6950 (0.8458)  loss_n_80: 0.7765 (0.9380)  loss_n_100: 0.8514 (1.0204)  triple_100: 11.1146 (11.9372)  triple_80: 11.3428 (12.1118)  triple_60: 9.6279 (10.3114)  triple_40: 5.9752 (6.8236)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 290/1724]  eta: 2:20:58  lr: 0.000060  loss: 40.4683 (44.7301)  loss_n_40: 0.6484 (0.8417)  loss_n_60: 0.6863 (0.8421)  loss_n_80: 0.7725 (0.9340)  loss_n_100: 0.8466 (1.0160)  triple_100: 11.0333 (11.9128)  triple_80: 11.1226 (12.0877)  triple_60: 9.3440 (10.2934)  triple_40: 5.9685 (6.8024)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 300/1724]  eta: 2:19:59  lr: 0.000060  loss: 40.5997 (44.6523)  loss_n_40: 0.7064 (0.8412)  loss_n_60: 0.7497 (0.8403)  loss_n_80: 0.8513 (0.9315)  loss_n_100: 0.8972 (1.0129)  triple_100: 10.9590 (11.8861)  triple_80: 11.0801 (12.0656)  triple_60: 9.6084 (10.2793)  triple_40: 6.0282 (6.7954)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 310/1724]  eta: 2:19:00  lr: 0.000060  loss: 41.6651 (44.5164)  loss_n_40: 0.6980 (0.8378)  loss_n_60: 0.7497 (0.8366)  loss_n_80: 0.8287 (0.9278)  loss_n_100: 0.9088 (1.0092)  triple_100: 10.8403 (11.8548)  triple_80: 11.0791 (12.0320)  triple_60: 9.5727 (10.2468)  triple_40: 5.9540 (6.7713)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 320/1724]  eta: 2:18:01  lr: 0.000060  loss: 39.9139 (44.4426)  loss_n_40: 0.6931 (0.8378)  loss_n_60: 0.6859 (0.8355)  loss_n_80: 0.7971 (0.9260)  loss_n_100: 0.9017 (1.0072)  triple_100: 10.8472 (11.8322)  triple_80: 10.9465 (12.0072)  triple_60: 9.2873 (10.2308)  triple_40: 5.9936 (6.7658)  time: 5.8957  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 330/1724]  eta: 2:17:02  lr: 0.000060  loss: 42.9752 (44.4044)  loss_n_40: 0.7717 (0.8356)  loss_n_60: 0.8012 (0.8335)  loss_n_80: 0.8681 (0.9240)  loss_n_100: 0.9197 (1.0052)  triple_100: 11.1969 (11.8256)  triple_80: 11.3992 (11.9985)  triple_60: 9.9372 (10.2227)  triple_40: 6.2887 (6.7594)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 340/1724]  eta: 2:16:03  lr: 0.000060  loss: 41.8435 (44.2929)  loss_n_40: 0.6472 (0.8310)  loss_n_60: 0.6989 (0.8297)  loss_n_80: 0.7943 (0.9199)  loss_n_100: 0.8753 (1.0008)  triple_100: 11.1969 (11.7981)  triple_80: 11.3992 (11.9713)  triple_60: 9.6073 (10.2007)  triple_40: 6.1530 (6.7415)  time: 5.8975  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 350/1724]  eta: 2:15:04  lr: 0.000060  loss: 39.4299 (44.1709)  loss_n_40: 0.5763 (0.8270)  loss_n_60: 0.6519 (0.8258)  loss_n_80: 0.7527 (0.9158)  loss_n_100: 0.8362 (0.9966)  triple_100: 10.7340 (11.7697)  triple_80: 10.8562 (11.9399)  triple_60: 9.2387 (10.1720)  triple_40: 5.7760 (6.7243)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 360/1724]  eta: 2:14:05  lr: 0.000060  loss: 40.8701 (44.1208)  loss_n_40: 0.6134 (0.8247)  loss_n_60: 0.6717 (0.8243)  loss_n_80: 0.7527 (0.9142)  loss_n_100: 0.8473 (0.9951)  triple_100: 10.9478 (11.7600)  triple_80: 11.3028 (11.9283)  triple_60: 9.3921 (10.1619)  triple_40: 6.0020 (6.7123)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 370/1724]  eta: 2:13:06  lr: 0.000060  loss: 41.6499 (44.0645)  loss_n_40: 0.6375 (0.8207)  loss_n_60: 0.6872 (0.8215)  loss_n_80: 0.8150 (0.9116)  loss_n_100: 0.9123 (0.9926)  triple_100: 11.3857 (11.7537)  triple_80: 11.4030 (11.9186)  triple_60: 9.6826 (10.1513)  triple_40: 6.0027 (6.6944)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 380/1724]  eta: 2:12:07  lr: 0.000060  loss: 40.7689 (44.0313)  loss_n_40: 0.6336 (0.8177)  loss_n_60: 0.6728 (0.8190)  loss_n_80: 0.7591 (0.9092)  loss_n_100: 0.8436 (0.9902)  triple_100: 11.1017 (11.7479)  triple_80: 11.0761 (11.9115)  triple_60: 9.4312 (10.1460)  triple_40: 6.0191 (6.6898)  time: 5.8975  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:4]  [ 390/1724]  eta: 2:11:08  lr: 0.000060  loss: 41.3762 (43.9940)  loss_n_40: 0.7519 (0.8211)  loss_n_60: 0.7558 (0.8204)  loss_n_80: 0.8406 (0.9100)  loss_n_100: 0.9273 (0.9905)  triple_100: 10.8538 (11.7309)  triple_80: 11.0761 (11.8973)  triple_60: 9.4312 (10.1369)  triple_40: 6.3208 (6.6870)  time: 5.8971  data: 0.0003  max mem: 40153\n",
      "Train: [epoch:4]  [ 400/1724]  eta: 2:10:09  lr: 0.000060  loss: 41.3762 (43.8942)  loss_n_40: 0.7519 (0.8185)  loss_n_60: 0.7558 (0.8176)  loss_n_80: 0.8406 (0.9069)  loss_n_100: 0.9201 (0.9871)  triple_100: 10.8016 (11.7051)  triple_80: 10.9322 (11.8708)  triple_60: 9.3749 (10.1137)  triple_40: 5.9510 (6.6745)  time: 5.8962  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:4]  [ 410/1724]  eta: 2:09:10  lr: 0.000060  loss: 39.1028 (43.7868)  loss_n_40: 0.5552 (0.8133)  loss_n_60: 0.5866 (0.8130)  loss_n_80: 0.6835 (0.9024)  loss_n_100: 0.7651 (0.9826)  triple_100: 10.5914 (11.6838)  triple_80: 10.6084 (11.8462)  triple_60: 9.0904 (10.0904)  triple_40: 5.7645 (6.6550)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 420/1724]  eta: 2:08:11  lr: 0.000060  loss: 38.4077 (43.6995)  loss_n_40: 0.5649 (0.8098)  loss_n_60: 0.5988 (0.8099)  loss_n_80: 0.6925 (0.8994)  loss_n_100: 0.7669 (0.9797)  triple_100: 10.6569 (11.6660)  triple_80: 10.6141 (11.8249)  triple_60: 8.9837 (10.0699)  triple_40: 5.8442 (6.6399)  time: 5.8960  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [ 430/1724]  eta: 2:07:12  lr: 0.000060  loss: 39.2397 (43.6248)  loss_n_40: 0.5743 (0.8071)  loss_n_60: 0.6236 (0.8072)  loss_n_80: 0.6957 (0.8965)  loss_n_100: 0.7773 (0.9764)  triple_100: 10.6337 (11.6454)  triple_80: 10.9098 (11.8052)  triple_60: 9.0271 (10.0555)  triple_40: 5.8842 (6.6315)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 440/1724]  eta: 2:06:13  lr: 0.000060  loss: 40.0945 (43.5608)  loss_n_40: 0.5656 (0.8055)  loss_n_60: 0.6058 (0.8048)  loss_n_80: 0.6824 (0.8935)  loss_n_100: 0.7323 (0.9730)  triple_100: 10.4467 (11.6247)  triple_80: 10.4724 (11.7854)  triple_60: 9.2895 (10.0431)  triple_40: 6.1477 (6.6309)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 450/1724]  eta: 2:05:14  lr: 0.000060  loss: 38.7941 (43.4903)  loss_n_40: 0.6330 (0.8034)  loss_n_60: 0.6561 (0.8026)  loss_n_80: 0.7550 (0.8913)  loss_n_100: 0.8479 (0.9709)  triple_100: 10.4935 (11.6084)  triple_80: 10.4724 (11.7668)  triple_60: 9.0692 (10.0257)  triple_40: 6.1477 (6.6213)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 460/1724]  eta: 2:04:15  lr: 0.000060  loss: 39.7913 (43.4479)  loss_n_40: 0.6545 (0.8031)  loss_n_60: 0.6781 (0.8019)  loss_n_80: 0.7550 (0.8901)  loss_n_100: 0.8479 (0.9692)  triple_100: 10.7040 (11.5941)  triple_80: 10.7599 (11.7549)  triple_60: 9.2380 (10.0182)  triple_40: 6.1273 (6.6164)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 470/1724]  eta: 2:03:16  lr: 0.000060  loss: 39.8770 (43.4111)  loss_n_40: 0.6545 (0.8001)  loss_n_60: 0.6812 (0.7998)  loss_n_80: 0.7548 (0.8879)  loss_n_100: 0.8107 (0.9668)  triple_100: 10.7223 (11.5866)  triple_80: 10.9126 (11.7480)  triple_60: 9.3418 (10.0145)  triple_40: 6.1273 (6.6074)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 480/1724]  eta: 2:02:17  lr: 0.000060  loss: 39.2237 (43.3543)  loss_n_40: 0.6881 (0.8013)  loss_n_60: 0.7272 (0.7998)  loss_n_80: 0.8295 (0.8881)  loss_n_100: 0.8902 (0.9671)  triple_100: 10.6124 (11.5713)  triple_80: 10.6475 (11.7342)  triple_60: 9.1021 (9.9977)  triple_40: 5.9435 (6.5948)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 490/1724]  eta: 2:01:18  lr: 0.000060  loss: 39.2237 (43.2983)  loss_n_40: 0.8916 (0.8050)  loss_n_60: 0.7965 (0.8010)  loss_n_80: 0.8819 (0.8887)  loss_n_100: 0.9446 (0.9675)  triple_100: 10.3209 (11.5507)  triple_80: 10.6059 (11.7153)  triple_60: 8.9695 (9.9827)  triple_40: 5.9504 (6.5873)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 500/1724]  eta: 2:00:19  lr: 0.000060  loss: 41.5914 (43.2966)  loss_n_40: 0.9903 (0.8117)  loss_n_60: 0.9326 (0.8066)  loss_n_80: 1.0102 (0.8942)  loss_n_100: 1.0847 (0.9735)  triple_100: 10.5875 (11.5433)  triple_80: 10.8963 (11.7101)  triple_60: 9.3192 (9.9755)  triple_40: 6.1647 (6.5817)  time: 5.8973  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 510/1724]  eta: 1:59:20  lr: 0.000060  loss: 44.1778 (43.3704)  loss_n_40: 0.9903 (0.8150)  loss_n_60: 1.0488 (0.8114)  loss_n_80: 1.1265 (0.8997)  loss_n_100: 1.2201 (0.9794)  triple_100: 11.7859 (11.5636)  triple_80: 11.9881 (11.7346)  triple_60: 10.0906 (9.9914)  triple_40: 5.9649 (6.5754)  time: 5.8957  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 520/1724]  eta: 1:58:21  lr: 0.000060  loss: 44.9634 (43.4023)  loss_n_40: 0.8970 (0.8157)  loss_n_60: 0.9473 (0.8129)  loss_n_80: 1.0634 (0.9017)  loss_n_100: 1.1696 (0.9820)  triple_100: 12.0456 (11.5759)  triple_80: 12.4131 (11.7469)  triple_60: 10.3438 (9.9966)  triple_40: 5.9649 (6.5706)  time: 5.8951  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 530/1724]  eta: 1:57:22  lr: 0.000060  loss: 44.5766 (43.4098)  loss_n_40: 0.7165 (0.8150)  loss_n_60: 0.8585 (0.8136)  loss_n_80: 0.9942 (0.9028)  loss_n_100: 1.1084 (0.9834)  triple_100: 11.9486 (11.5824)  triple_80: 12.2462 (11.7533)  triple_60: 10.0616 (9.9971)  triple_40: 5.9129 (6.5622)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 540/1724]  eta: 1:56:23  lr: 0.000060  loss: 43.5396 (43.4063)  loss_n_40: 0.6859 (0.8163)  loss_n_60: 0.7792 (0.8150)  loss_n_80: 0.8921 (0.9044)  loss_n_100: 0.9935 (0.9850)  triple_100: 11.6547 (11.5777)  triple_80: 11.6848 (11.7502)  triple_60: 9.7330 (9.9950)  triple_40: 5.9075 (6.5627)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 550/1724]  eta: 1:55:24  lr: 0.000060  loss: 41.8664 (43.3869)  loss_n_40: 0.7053 (0.8161)  loss_n_60: 0.7792 (0.8155)  loss_n_80: 0.8921 (0.9051)  loss_n_100: 0.9935 (0.9858)  triple_100: 11.1659 (11.5710)  triple_80: 11.4280 (11.7455)  triple_60: 9.5918 (9.9909)  triple_40: 6.1698 (6.5571)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 560/1724]  eta: 1:54:25  lr: 0.000060  loss: 40.8350 (43.3482)  loss_n_40: 0.7105 (0.8161)  loss_n_60: 0.7743 (0.8155)  loss_n_80: 0.8899 (0.9051)  loss_n_100: 0.9579 (0.9857)  triple_100: 11.0126 (11.5577)  triple_80: 11.1689 (11.7347)  triple_60: 9.5434 (9.9816)  triple_40: 6.0578 (6.5516)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 570/1724]  eta: 1:53:26  lr: 0.000060  loss: 40.4621 (43.2969)  loss_n_40: 0.7207 (0.8154)  loss_n_60: 0.7743 (0.8154)  loss_n_80: 0.8687 (0.9049)  loss_n_100: 0.9489 (0.9856)  triple_100: 10.8201 (11.5438)  triple_80: 11.0862 (11.7200)  triple_60: 9.3587 (9.9701)  triple_40: 6.0436 (6.5418)  time: 5.8987  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 580/1724]  eta: 1:52:27  lr: 0.000060  loss: 41.3736 (43.2900)  loss_n_40: 0.8119 (0.8166)  loss_n_60: 0.8524 (0.8163)  loss_n_80: 0.9212 (0.9057)  loss_n_100: 0.9839 (0.9862)  triple_100: 10.7849 (11.5390)  triple_80: 11.0222 (11.7160)  triple_60: 9.4345 (9.9681)  triple_40: 6.0436 (6.5422)  time: 5.8979  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 590/1724]  eta: 1:51:28  lr: 0.000060  loss: 42.4369 (43.2613)  loss_n_40: 0.8132 (0.8167)  loss_n_60: 0.8524 (0.8164)  loss_n_80: 0.9212 (0.9056)  loss_n_100: 0.9839 (0.9859)  triple_100: 10.7849 (11.5291)  triple_80: 11.2256 (11.7073)  triple_60: 9.8058 (9.9633)  triple_40: 6.0082 (6.5370)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 600/1724]  eta: 1:50:29  lr: 0.000060  loss: 42.2439 (43.2463)  loss_n_40: 0.7470 (0.8163)  loss_n_60: 0.8179 (0.8163)  loss_n_80: 0.9008 (0.9053)  loss_n_100: 0.9606 (0.9857)  triple_100: 10.7774 (11.5238)  triple_80: 11.0347 (11.7024)  triple_60: 9.7459 (9.9621)  triple_40: 5.8819 (6.5345)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 610/1724]  eta: 1:49:30  lr: 0.000060  loss: 41.1753 (43.2069)  loss_n_40: 0.6945 (0.8144)  loss_n_60: 0.7321 (0.8149)  loss_n_80: 0.8384 (0.9039)  loss_n_100: 0.9192 (0.9840)  triple_100: 10.8478 (11.5127)  triple_80: 11.0143 (11.6928)  triple_60: 9.6146 (9.9547)  triple_40: 6.1794 (6.5294)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 620/1724]  eta: 1:48:31  lr: 0.000060  loss: 39.7732 (43.1603)  loss_n_40: 0.6001 (0.8118)  loss_n_60: 0.6448 (0.8127)  loss_n_80: 0.7422 (0.9015)  loss_n_100: 0.8375 (0.9816)  triple_100: 10.6944 (11.5022)  triple_80: 10.7899 (11.6803)  triple_60: 9.1759 (9.9450)  triple_40: 6.1238 (6.5253)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 630/1724]  eta: 1:47:32  lr: 0.000060  loss: 39.5791 (43.1142)  loss_n_40: 0.5796 (0.8092)  loss_n_60: 0.6406 (0.8106)  loss_n_80: 0.7264 (0.8992)  loss_n_100: 0.7999 (0.9791)  triple_100: 10.6487 (11.4920)  triple_80: 10.6624 (11.6681)  triple_60: 9.2921 (9.9364)  triple_40: 6.0448 (6.5196)  time: 5.8958  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 640/1724]  eta: 1:46:33  lr: 0.000060  loss: 41.5072 (43.0972)  loss_n_40: 0.5796 (0.8073)  loss_n_60: 0.6565 (0.8091)  loss_n_80: 0.7344 (0.8976)  loss_n_100: 0.8103 (0.9774)  triple_100: 11.0977 (11.4887)  triple_80: 11.1443 (11.6643)  triple_60: 9.6140 (9.9341)  triple_40: 6.0466 (6.5188)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 650/1724]  eta: 1:45:34  lr: 0.000060  loss: 40.4023 (43.0357)  loss_n_40: 0.5593 (0.8042)  loss_n_60: 0.6175 (0.8064)  loss_n_80: 0.7123 (0.8949)  loss_n_100: 0.8035 (0.9746)  triple_100: 11.0580 (11.4745)  triple_80: 11.0978 (11.6490)  triple_60: 9.5649 (9.9222)  triple_40: 5.8740 (6.5101)  time: 5.8968  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [ 660/1724]  eta: 1:44:35  lr: 0.000060  loss: 38.7311 (42.9800)  loss_n_40: 0.5593 (0.8025)  loss_n_60: 0.6136 (0.8046)  loss_n_80: 0.7034 (0.8927)  loss_n_100: 0.7774 (0.9721)  triple_100: 10.4389 (11.4583)  triple_80: 10.4925 (11.6333)  triple_60: 9.1331 (9.9108)  triple_40: 5.8591 (6.5058)  time: 5.8980  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 670/1724]  eta: 1:43:36  lr: 0.000060  loss: 39.0649 (42.9406)  loss_n_40: 0.6190 (0.8011)  loss_n_60: 0.6757 (0.8033)  loss_n_80: 0.7570 (0.8915)  loss_n_100: 0.8274 (0.9708)  triple_100: 10.6140 (11.4489)  triple_80: 10.7689 (11.6230)  triple_60: 9.1331 (9.9015)  triple_40: 6.0767 (6.5005)  time: 5.8979  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 680/1724]  eta: 1:42:37  lr: 0.000060  loss: 38.9987 (42.9148)  loss_n_40: 0.6508 (0.8010)  loss_n_60: 0.7661 (0.8029)  loss_n_80: 0.8332 (0.8909)  loss_n_100: 0.9383 (0.9702)  triple_100: 10.4574 (11.4425)  triple_80: 10.4469 (11.6150)  triple_60: 9.0555 (9.8950)  triple_40: 6.0767 (6.4972)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 690/1724]  eta: 1:41:38  lr: 0.000060  loss: 38.5079 (42.8625)  loss_n_40: 0.5636 (0.7985)  loss_n_60: 0.6694 (0.8009)  loss_n_80: 0.7659 (0.8890)  loss_n_100: 0.8332 (0.9682)  triple_100: 10.4574 (11.4314)  triple_80: 10.4469 (11.6025)  triple_60: 9.0555 (9.8836)  triple_40: 5.8676 (6.4883)  time: 5.8978  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 700/1724]  eta: 1:40:39  lr: 0.000060  loss: 39.5205 (42.8341)  loss_n_40: 0.5497 (0.7959)  loss_n_60: 0.6230 (0.7992)  loss_n_80: 0.7255 (0.8874)  loss_n_100: 0.8362 (0.9666)  triple_100: 10.8948 (11.4281)  triple_80: 10.8469 (11.5979)  triple_60: 9.1005 (9.8778)  triple_40: 5.8676 (6.4811)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 710/1724]  eta: 1:39:40  lr: 0.000060  loss: 39.8208 (42.7788)  loss_n_40: 0.5706 (0.7943)  loss_n_60: 0.6328 (0.7973)  loss_n_80: 0.7425 (0.8854)  loss_n_100: 0.8362 (0.9644)  triple_100: 10.8948 (11.4142)  triple_80: 10.8499 (11.5839)  triple_60: 9.2579 (9.8657)  triple_40: 6.0294 (6.4736)  time: 5.8973  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 720/1724]  eta: 1:38:41  lr: 0.000060  loss: 38.1016 (42.7267)  loss_n_40: 0.5871 (0.7924)  loss_n_60: 0.6328 (0.7957)  loss_n_80: 0.7425 (0.8837)  loss_n_100: 0.8248 (0.9626)  triple_100: 9.8970 (11.4012)  triple_80: 10.2342 (11.5703)  triple_60: 8.6923 (9.8544)  triple_40: 5.8522 (6.4664)  time: 5.8979  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 730/1724]  eta: 1:37:42  lr: 0.000060  loss: 37.7872 (42.6676)  loss_n_40: 0.5408 (0.7902)  loss_n_60: 0.6174 (0.7935)  loss_n_80: 0.7317 (0.8814)  loss_n_100: 0.7884 (0.9603)  triple_100: 10.0459 (11.3876)  triple_80: 10.2342 (11.5554)  triple_60: 8.7484 (9.8415)  triple_40: 5.5571 (6.4577)  time: 5.8980  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 740/1724]  eta: 1:36:43  lr: 0.000060  loss: 39.2598 (42.6439)  loss_n_40: 0.5276 (0.7891)  loss_n_60: 0.6128 (0.7924)  loss_n_80: 0.7147 (0.8802)  loss_n_100: 0.8048 (0.9590)  triple_100: 10.4803 (11.3825)  triple_80: 10.6759 (11.5491)  triple_60: 9.1197 (9.8366)  triple_40: 5.6250 (6.4551)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 750/1724]  eta: 1:35:44  lr: 0.000060  loss: 39.2674 (42.5938)  loss_n_40: 0.5919 (0.7889)  loss_n_60: 0.6468 (0.7915)  loss_n_80: 0.7576 (0.8791)  loss_n_100: 0.8444 (0.9577)  triple_100: 10.5437 (11.3680)  triple_80: 10.6997 (11.5349)  triple_60: 9.1389 (9.8245)  triple_40: 5.7456 (6.4492)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 760/1724]  eta: 1:34:45  lr: 0.000060  loss: 38.2705 (42.5242)  loss_n_40: 0.5919 (0.7862)  loss_n_60: 0.6527 (0.7891)  loss_n_80: 0.7387 (0.8766)  loss_n_100: 0.8195 (0.9552)  triple_100: 10.0819 (11.3517)  triple_80: 10.2385 (11.5173)  triple_60: 8.6912 (9.8090)  triple_40: 5.7709 (6.4390)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 770/1724]  eta: 1:33:46  lr: 0.000060  loss: 37.5953 (42.4763)  loss_n_40: 0.5610 (0.7853)  loss_n_60: 0.6069 (0.7881)  loss_n_80: 0.6814 (0.8755)  loss_n_100: 0.7825 (0.9540)  triple_100: 10.1618 (11.3394)  triple_80: 10.2864 (11.5045)  triple_60: 8.6258 (9.7981)  triple_40: 5.5511 (6.4314)  time: 5.8980  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 780/1724]  eta: 1:32:47  lr: 0.000060  loss: 38.3538 (42.4251)  loss_n_40: 0.6310 (0.7842)  loss_n_60: 0.6696 (0.7868)  loss_n_80: 0.7570 (0.8742)  loss_n_100: 0.8204 (0.9525)  triple_100: 10.5588 (11.3270)  triple_80: 10.6612 (11.4917)  triple_60: 8.7406 (9.7861)  triple_40: 5.6142 (6.4227)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 790/1724]  eta: 1:31:48  lr: 0.000060  loss: 38.3538 (42.3682)  loss_n_40: 0.5844 (0.7819)  loss_n_60: 0.6250 (0.7848)  loss_n_80: 0.7174 (0.8722)  loss_n_100: 0.8097 (0.9505)  triple_100: 10.3863 (11.3143)  triple_80: 10.5597 (11.4783)  triple_60: 8.7406 (9.7737)  triple_40: 5.6142 (6.4125)  time: 5.8970  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 800/1724]  eta: 1:30:49  lr: 0.000060  loss: 36.6600 (42.3010)  loss_n_40: 0.5885 (0.7814)  loss_n_60: 0.6121 (0.7835)  loss_n_80: 0.7107 (0.8704)  loss_n_100: 0.7891 (0.9485)  triple_100: 9.9767 (11.2947)  triple_80: 10.0552 (11.4583)  triple_60: 8.5404 (9.7579)  triple_40: 5.7399 (6.4063)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 810/1724]  eta: 1:29:50  lr: 0.000060  loss: 35.7880 (42.2324)  loss_n_40: 0.5287 (0.7779)  loss_n_60: 0.5576 (0.7806)  loss_n_80: 0.6377 (0.8675)  loss_n_100: 0.6987 (0.9455)  triple_100: 9.7055 (11.2793)  triple_80: 9.7397 (11.4414)  triple_60: 8.3709 (9.7438)  triple_40: 5.6485 (6.3963)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 820/1724]  eta: 1:28:51  lr: 0.000060  loss: 35.8965 (42.1870)  loss_n_40: 0.4765 (0.7762)  loss_n_60: 0.5189 (0.7787)  loss_n_80: 0.6105 (0.8655)  loss_n_100: 0.6722 (0.9433)  triple_100: 9.9372 (11.2682)  triple_80: 9.9245 (11.4290)  triple_60: 8.4671 (9.7346)  triple_40: 5.6308 (6.3916)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 830/1724]  eta: 1:27:52  lr: 0.000060  loss: 36.6697 (42.1253)  loss_n_40: 0.4956 (0.7741)  loss_n_60: 0.5546 (0.7767)  loss_n_80: 0.6374 (0.8634)  loss_n_100: 0.7241 (0.9413)  triple_100: 9.9889 (11.2545)  triple_80: 10.0696 (11.4137)  triple_60: 8.4183 (9.7197)  triple_40: 5.5648 (6.3820)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 840/1724]  eta: 1:26:53  lr: 0.000060  loss: 37.2386 (42.0930)  loss_n_40: 0.5228 (0.7724)  loss_n_60: 0.5871 (0.7753)  loss_n_80: 0.6634 (0.8620)  loss_n_100: 0.7241 (0.9397)  triple_100: 10.2695 (11.2478)  triple_80: 10.2772 (11.4060)  triple_60: 8.6082 (9.7135)  triple_40: 5.5695 (6.3762)  time: 5.8978  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 850/1724]  eta: 1:25:54  lr: 0.000060  loss: 38.2146 (42.0458)  loss_n_40: 0.5300 (0.7704)  loss_n_60: 0.6025 (0.7735)  loss_n_80: 0.6843 (0.8602)  loss_n_100: 0.7701 (0.9379)  triple_100: 10.5774 (11.2375)  triple_80: 10.5586 (11.3942)  triple_60: 8.9321 (9.7029)  triple_40: 5.7568 (6.3690)  time: 5.8985  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 860/1724]  eta: 1:24:55  lr: 0.000060  loss: 37.7878 (41.9956)  loss_n_40: 0.5399 (0.7690)  loss_n_60: 0.5989 (0.7720)  loss_n_80: 0.6843 (0.8584)  loss_n_100: 0.7701 (0.9361)  triple_100: 10.2544 (11.2240)  triple_80: 10.2782 (11.3807)  triple_60: 8.9321 (9.6927)  triple_40: 5.8185 (6.3628)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 870/1724]  eta: 1:23:56  lr: 0.000060  loss: 36.0678 (41.9282)  loss_n_40: 0.5252 (0.7664)  loss_n_60: 0.5521 (0.7696)  loss_n_80: 0.6481 (0.8559)  loss_n_100: 0.7302 (0.9334)  triple_100: 9.7563 (11.2080)  triple_80: 9.8142 (11.3636)  triple_60: 8.3738 (9.6782)  triple_40: 5.5911 (6.3529)  time: 5.8999  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 880/1724]  eta: 1:22:57  lr: 0.000060  loss: 35.9448 (41.8871)  loss_n_40: 0.5648 (0.7657)  loss_n_60: 0.5864 (0.7685)  loss_n_80: 0.6623 (0.8547)  loss_n_100: 0.7318 (0.9321)  triple_100: 9.8758 (11.1980)  triple_80: 9.9497 (11.3539)  triple_60: 8.3738 (9.6696)  triple_40: 5.4943 (6.3446)  time: 5.8990  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [ 890/1724]  eta: 1:21:58  lr: 0.000060  loss: 37.1922 (41.8461)  loss_n_40: 0.5761 (0.7648)  loss_n_60: 0.5974 (0.7673)  loss_n_80: 0.7130 (0.8535)  loss_n_100: 0.7972 (0.9308)  triple_100: 10.1587 (11.1887)  triple_80: 10.3609 (11.3444)  triple_60: 8.5678 (9.6604)  triple_40: 5.4943 (6.3362)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 900/1724]  eta: 1:20:59  lr: 0.000060  loss: 37.2866 (41.8054)  loss_n_40: 0.5626 (0.7635)  loss_n_60: 0.5974 (0.7662)  loss_n_80: 0.6891 (0.8524)  loss_n_100: 0.7960 (0.9296)  triple_100: 10.3653 (11.1791)  triple_80: 10.4593 (11.3348)  triple_60: 8.7283 (9.6517)  triple_40: 5.4179 (6.3280)  time: 5.8955  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 910/1724]  eta: 1:20:00  lr: 0.000060  loss: 37.8038 (41.7635)  loss_n_40: 0.5740 (0.7618)  loss_n_60: 0.6221 (0.7647)  loss_n_80: 0.6981 (0.8508)  loss_n_100: 0.7699 (0.9279)  triple_100: 10.3381 (11.1699)  triple_80: 10.4482 (11.3248)  triple_60: 8.8437 (9.6433)  triple_40: 5.4179 (6.3202)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 920/1724]  eta: 1:19:01  lr: 0.000060  loss: 38.4703 (41.7324)  loss_n_40: 0.5740 (0.7605)  loss_n_60: 0.6213 (0.7636)  loss_n_80: 0.7013 (0.8497)  loss_n_100: 0.7768 (0.9267)  triple_100: 10.3381 (11.1630)  triple_80: 10.4702 (11.3175)  triple_60: 8.8873 (9.6365)  triple_40: 5.6993 (6.3149)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 930/1724]  eta: 1:18:02  lr: 0.000060  loss: 38.2610 (41.6858)  loss_n_40: 0.5488 (0.7586)  loss_n_60: 0.6045 (0.7621)  loss_n_80: 0.7012 (0.8482)  loss_n_100: 0.7768 (0.9252)  triple_100: 10.3401 (11.1528)  triple_80: 10.5171 (11.3066)  triple_60: 8.8397 (9.6260)  triple_40: 5.6639 (6.3064)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 940/1724]  eta: 1:17:03  lr: 0.000060  loss: 37.1313 (41.6422)  loss_n_40: 0.5488 (0.7573)  loss_n_60: 0.5826 (0.7609)  loss_n_80: 0.6825 (0.8470)  loss_n_100: 0.7635 (0.9239)  triple_100: 10.2331 (11.1422)  triple_80: 10.3330 (11.2955)  triple_60: 8.5933 (9.6160)  triple_40: 5.4537 (6.2994)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 950/1724]  eta: 1:16:04  lr: 0.000060  loss: 37.8403 (41.6053)  loss_n_40: 0.5258 (0.7549)  loss_n_60: 0.5979 (0.7591)  loss_n_80: 0.6764 (0.8451)  loss_n_100: 0.7515 (0.9221)  triple_100: 10.1719 (11.1356)  triple_80: 10.2598 (11.2871)  triple_60: 8.7941 (9.6092)  triple_40: 5.6320 (6.2923)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 960/1724]  eta: 1:15:05  lr: 0.000060  loss: 36.7205 (41.5552)  loss_n_40: 0.5451 (0.7536)  loss_n_60: 0.5979 (0.7577)  loss_n_80: 0.6764 (0.8434)  loss_n_100: 0.7366 (0.9202)  triple_100: 9.9829 (11.1212)  triple_80: 10.0989 (11.2730)  triple_60: 8.7130 (9.5991)  triple_40: 5.5928 (6.2870)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 970/1724]  eta: 1:14:06  lr: 0.000060  loss: 36.2637 (41.5050)  loss_n_40: 0.5555 (0.7515)  loss_n_60: 0.5962 (0.7559)  loss_n_80: 0.6604 (0.8415)  loss_n_100: 0.7227 (0.9182)  triple_100: 9.7228 (11.1092)  triple_80: 9.9412 (11.2605)  triple_60: 8.5124 (9.5883)  triple_40: 5.5928 (6.2798)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 980/1724]  eta: 1:13:07  lr: 0.000060  loss: 37.1039 (41.4628)  loss_n_40: 0.5323 (0.7503)  loss_n_60: 0.5962 (0.7545)  loss_n_80: 0.6840 (0.8400)  loss_n_100: 0.7625 (0.9165)  triple_100: 9.9973 (11.0977)  triple_80: 10.0748 (11.2488)  triple_60: 8.5323 (9.5795)  triple_40: 5.6375 (6.2755)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [ 990/1724]  eta: 1:12:08  lr: 0.000060  loss: 37.1749 (41.4331)  loss_n_40: 0.6358 (0.7503)  loss_n_60: 0.6414 (0.7539)  loss_n_80: 0.7114 (0.8392)  loss_n_100: 0.7995 (0.9156)  triple_100: 10.0974 (11.0889)  triple_80: 10.2877 (11.2396)  triple_60: 8.6998 (9.5728)  triple_40: 5.7942 (6.2727)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1000/1724]  eta: 1:11:09  lr: 0.000060  loss: 37.0923 (41.3809)  loss_n_40: 0.6358 (0.7495)  loss_n_60: 0.6418 (0.7526)  loss_n_80: 0.7156 (0.8375)  loss_n_100: 0.7967 (0.9136)  triple_100: 9.5808 (11.0720)  triple_80: 9.8139 (11.2240)  triple_60: 8.6998 (9.5619)  triple_40: 5.6439 (6.2697)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1010/1724]  eta: 1:10:10  lr: 0.000060  loss: 36.4503 (41.3362)  loss_n_40: 0.5344 (0.7473)  loss_n_60: 0.5794 (0.7507)  loss_n_80: 0.6722 (0.8357)  loss_n_100: 0.7417 (0.9118)  triple_100: 9.5617 (11.0631)  triple_80: 9.6924 (11.2138)  triple_60: 8.2715 (9.5520)  triple_40: 5.4626 (6.2618)  time: 5.8987  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1020/1724]  eta: 1:09:11  lr: 0.000060  loss: 36.8287 (41.3022)  loss_n_40: 0.5344 (0.7461)  loss_n_60: 0.5884 (0.7496)  loss_n_80: 0.6840 (0.8345)  loss_n_100: 0.7641 (0.9106)  triple_100: 10.2107 (11.0561)  triple_80: 10.2430 (11.2057)  triple_60: 8.4378 (9.5442)  triple_40: 5.3447 (6.2554)  time: 5.8974  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1030/1724]  eta: 1:08:12  lr: 0.000060  loss: 37.6215 (41.2669)  loss_n_40: 0.5558 (0.7444)  loss_n_60: 0.5884 (0.7481)  loss_n_80: 0.6834 (0.8330)  loss_n_100: 0.7641 (0.9090)  triple_100: 9.9527 (11.0479)  triple_80: 10.2221 (11.1967)  triple_60: 8.6437 (9.5366)  triple_40: 5.6347 (6.2512)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1040/1724]  eta: 1:07:13  lr: 0.000060  loss: 36.3395 (41.2230)  loss_n_40: 0.5558 (0.7440)  loss_n_60: 0.5838 (0.7471)  loss_n_80: 0.6641 (0.8317)  loss_n_100: 0.7351 (0.9077)  triple_100: 9.7224 (11.0353)  triple_80: 9.8000 (11.1840)  triple_60: 8.5381 (9.5271)  triple_40: 5.6921 (6.2461)  time: 5.8963  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1050/1724]  eta: 1:06:14  lr: 0.000060  loss: 36.1081 (41.1793)  loss_n_40: 0.5240 (0.7426)  loss_n_60: 0.5922 (0.7459)  loss_n_80: 0.6641 (0.8304)  loss_n_100: 0.7351 (0.9064)  triple_100: 9.5907 (11.0258)  triple_80: 9.6772 (11.1733)  triple_60: 8.4231 (9.5169)  triple_40: 5.4475 (6.2380)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1060/1724]  eta: 1:05:15  lr: 0.000060  loss: 36.4671 (41.1478)  loss_n_40: 0.5530 (0.7410)  loss_n_60: 0.5957 (0.7446)  loss_n_80: 0.6743 (0.8291)  loss_n_100: 0.7499 (0.9050)  triple_100: 9.9292 (11.0197)  triple_80: 9.9101 (11.1661)  triple_60: 8.4293 (9.5106)  triple_40: 5.3718 (6.2317)  time: 5.8970  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1070/1724]  eta: 1:04:16  lr: 0.000060  loss: 35.5961 (41.0918)  loss_n_40: 0.5297 (0.7395)  loss_n_60: 0.5751 (0.7429)  loss_n_80: 0.6634 (0.8271)  loss_n_100: 0.7435 (0.9028)  triple_100: 9.6057 (11.0043)  triple_80: 9.6075 (11.1506)  triple_60: 8.2951 (9.4988)  triple_40: 5.5472 (6.2258)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1080/1724]  eta: 1:03:17  lr: 0.000060  loss: 35.2847 (41.0486)  loss_n_40: 0.4891 (0.7378)  loss_n_60: 0.5374 (0.7412)  loss_n_80: 0.6118 (0.8254)  loss_n_100: 0.6630 (0.9011)  triple_100: 9.4948 (10.9947)  triple_80: 9.5839 (11.1399)  triple_60: 8.2951 (9.4893)  triple_40: 5.5463 (6.2192)  time: 5.8955  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1090/1724]  eta: 1:02:18  lr: 0.000060  loss: 35.6433 (41.0044)  loss_n_40: 0.5436 (0.7369)  loss_n_60: 0.5645 (0.7401)  loss_n_80: 0.6502 (0.8241)  loss_n_100: 0.7249 (0.8997)  triple_100: 9.7305 (10.9836)  triple_80: 9.8466 (11.1274)  triple_60: 8.3763 (9.4790)  triple_40: 5.5224 (6.2136)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1100/1724]  eta: 1:01:20  lr: 0.000060  loss: 36.0687 (40.9659)  loss_n_40: 0.5620 (0.7354)  loss_n_60: 0.5871 (0.7387)  loss_n_80: 0.6798 (0.8226)  loss_n_100: 0.7477 (0.8982)  triple_100: 9.7305 (10.9747)  triple_80: 9.6399 (11.1174)  triple_60: 8.3414 (9.4703)  triple_40: 5.5496 (6.2086)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1110/1724]  eta: 1:00:21  lr: 0.000060  loss: 36.2651 (40.9274)  loss_n_40: 0.5204 (0.7342)  loss_n_60: 0.5871 (0.7375)  loss_n_80: 0.6798 (0.8213)  loss_n_100: 0.7649 (0.8968)  triple_100: 9.6113 (10.9650)  triple_80: 9.7987 (11.1071)  triple_60: 8.4334 (9.4621)  triple_40: 5.5233 (6.2035)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1120/1724]  eta: 0:59:22  lr: 0.000060  loss: 36.4026 (40.8858)  loss_n_40: 0.4891 (0.7321)  loss_n_60: 0.5641 (0.7358)  loss_n_80: 0.6462 (0.8196)  loss_n_100: 0.7150 (0.8951)  triple_100: 9.9153 (10.9561)  triple_80: 9.9272 (11.0973)  triple_60: 8.5084 (9.4537)  triple_40: 5.3775 (6.1960)  time: 5.8965  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [1130/1724]  eta: 0:58:23  lr: 0.000060  loss: 36.0853 (40.8411)  loss_n_40: 0.5009 (0.7300)  loss_n_60: 0.5506 (0.7342)  loss_n_80: 0.6340 (0.8180)  loss_n_100: 0.7141 (0.8935)  triple_100: 9.9459 (10.9460)  triple_80: 9.9350 (11.0863)  triple_60: 8.4075 (9.4441)  triple_40: 5.3116 (6.1890)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1140/1724]  eta: 0:57:24  lr: 0.000060  loss: 34.7600 (40.7921)  loss_n_40: 0.4778 (0.7283)  loss_n_60: 0.5211 (0.7328)  loss_n_80: 0.6081 (0.8164)  loss_n_100: 0.6840 (0.8920)  triple_100: 9.5315 (10.9338)  triple_80: 9.5064 (11.0734)  triple_60: 8.2864 (9.4336)  triple_40: 5.3667 (6.1818)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1150/1724]  eta: 0:56:25  lr: 0.000060  loss: 39.2900 (40.8056)  loss_n_40: 0.6691 (0.7295)  loss_n_60: 0.7090 (0.7339)  loss_n_80: 0.7999 (0.8176)  loss_n_100: 0.8843 (0.8933)  triple_100: 10.6804 (10.9385)  triple_80: 10.6983 (11.0784)  triple_60: 8.8110 (9.4357)  triple_40: 5.6705 (6.1787)  time: 5.8957  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1160/1724]  eta: 0:55:26  lr: 0.000060  loss: 39.7756 (40.7882)  loss_n_40: 0.8952 (0.7314)  loss_n_60: 0.8543 (0.7347)  loss_n_80: 0.9131 (0.8181)  loss_n_100: 0.9769 (0.8936)  triple_100: 10.8195 (10.9300)  triple_80: 10.8724 (11.0720)  triple_60: 9.1699 (9.4316)  triple_40: 5.9236 (6.1769)  time: 5.8951  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1170/1724]  eta: 0:54:27  lr: 0.000060  loss: 38.4158 (40.7710)  loss_n_40: 0.8204 (0.7327)  loss_n_60: 0.7961 (0.7355)  loss_n_80: 0.8812 (0.8187)  loss_n_100: 0.9375 (0.8939)  triple_100: 10.0396 (10.9233)  triple_80: 10.4180 (11.0668)  triple_60: 8.9063 (9.4274)  triple_40: 5.7255 (6.1726)  time: 5.8961  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1180/1724]  eta: 0:53:28  lr: 0.000060  loss: 37.8623 (40.7389)  loss_n_40: 0.7417 (0.7332)  loss_n_60: 0.6864 (0.7352)  loss_n_80: 0.7807 (0.8181)  loss_n_100: 0.8626 (0.8932)  triple_100: 10.0396 (10.9134)  triple_80: 10.1693 (11.0570)  triple_60: 8.7029 (9.4191)  triple_40: 5.6607 (6.1697)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1190/1724]  eta: 0:52:29  lr: 0.000060  loss: 37.3730 (40.7256)  loss_n_40: 0.6225 (0.7326)  loss_n_60: 0.6641 (0.7346)  loss_n_80: 0.7410 (0.8175)  loss_n_100: 0.8024 (0.8925)  triple_100: 10.1898 (10.9102)  triple_80: 10.1543 (11.0535)  triple_60: 8.6216 (9.4169)  triple_40: 5.7733 (6.1679)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1200/1724]  eta: 0:51:30  lr: 0.000060  loss: 37.8737 (40.6985)  loss_n_40: 0.6504 (0.7336)  loss_n_60: 0.6590 (0.7344)  loss_n_80: 0.7194 (0.8172)  loss_n_100: 0.7934 (0.8920)  triple_100: 10.1898 (10.9018)  triple_80: 10.1543 (11.0445)  triple_60: 8.7157 (9.4095)  triple_40: 5.6061 (6.1655)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1210/1724]  eta: 0:50:31  lr: 0.000060  loss: 36.2218 (40.6662)  loss_n_40: 0.5916 (0.7323)  loss_n_60: 0.6242 (0.7332)  loss_n_80: 0.7121 (0.8160)  loss_n_100: 0.7843 (0.8909)  triple_100: 9.6845 (10.8952)  triple_80: 9.8186 (11.0366)  triple_60: 8.5022 (9.4018)  triple_40: 5.5789 (6.1601)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1220/1724]  eta: 0:49:32  lr: 0.000060  loss: 35.2770 (40.6216)  loss_n_40: 0.6126 (0.7325)  loss_n_60: 0.6036 (0.7325)  loss_n_80: 0.6940 (0.8151)  loss_n_100: 0.7423 (0.8898)  triple_100: 9.5556 (10.8817)  triple_80: 9.5339 (11.0230)  triple_60: 8.1140 (9.3901)  triple_40: 5.5214 (6.1569)  time: 5.8948  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1230/1724]  eta: 0:48:33  lr: 0.000060  loss: 35.4184 (40.6032)  loss_n_40: 0.7140 (0.7324)  loss_n_60: 0.6582 (0.7320)  loss_n_80: 0.7271 (0.8146)  loss_n_100: 0.7803 (0.8892)  triple_100: 9.2778 (10.8766)  triple_80: 9.3255 (11.0179)  triple_60: 8.1301 (9.3857)  triple_40: 5.6708 (6.1547)  time: 5.8949  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1240/1724]  eta: 0:47:34  lr: 0.000060  loss: 38.8491 (40.5918)  loss_n_40: 0.7006 (0.7324)  loss_n_60: 0.6906 (0.7318)  loss_n_80: 0.7538 (0.8144)  loss_n_100: 0.8218 (0.8890)  triple_100: 10.6281 (10.8746)  triple_80: 10.7145 (11.0153)  triple_60: 8.9674 (9.3827)  triple_40: 5.6708 (6.1517)  time: 5.8953  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1250/1724]  eta: 0:46:35  lr: 0.000060  loss: 38.3542 (40.5695)  loss_n_40: 0.6206 (0.7313)  loss_n_60: 0.6276 (0.7308)  loss_n_80: 0.7045 (0.8135)  loss_n_100: 0.7684 (0.8880)  triple_100: 10.4351 (10.8700)  triple_80: 10.4417 (11.0100)  triple_60: 8.9674 (9.3780)  triple_40: 5.6529 (6.1479)  time: 5.8951  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1260/1724]  eta: 0:45:36  lr: 0.000060  loss: 37.1550 (40.5459)  loss_n_40: 0.5771 (0.7305)  loss_n_60: 0.5929 (0.7300)  loss_n_80: 0.6662 (0.8126)  loss_n_100: 0.7298 (0.8871)  triple_100: 10.0464 (10.8644)  triple_80: 10.1942 (11.0039)  triple_60: 8.8592 (9.3732)  triple_40: 5.6534 (6.1442)  time: 5.8950  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1270/1724]  eta: 0:44:37  lr: 0.000060  loss: 37.1550 (40.5252)  loss_n_40: 0.6127 (0.7306)  loss_n_60: 0.6276 (0.7296)  loss_n_80: 0.6980 (0.8120)  loss_n_100: 0.7646 (0.8864)  triple_100: 9.8250 (10.8583)  triple_80: 10.1650 (10.9983)  triple_60: 8.4629 (9.3679)  triple_40: 5.6790 (6.1420)  time: 5.8946  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1280/1724]  eta: 0:43:38  lr: 0.000060  loss: 36.9532 (40.4920)  loss_n_40: 0.5448 (0.7293)  loss_n_60: 0.5912 (0.7285)  loss_n_80: 0.6788 (0.8109)  loss_n_100: 0.7656 (0.8853)  triple_100: 9.9588 (10.8514)  triple_80: 10.1616 (10.9901)  triple_60: 8.4207 (9.3602)  triple_40: 5.5914 (6.1364)  time: 5.8941  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1290/1724]  eta: 0:42:39  lr: 0.000060  loss: 35.8139 (40.4536)  loss_n_40: 0.5211 (0.7281)  loss_n_60: 0.5518 (0.7272)  loss_n_80: 0.6210 (0.8095)  loss_n_100: 0.6931 (0.8838)  triple_100: 9.6840 (10.8418)  triple_80: 9.6709 (10.9799)  triple_60: 8.2048 (9.3516)  triple_40: 5.2804 (6.1317)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1300/1724]  eta: 0:41:40  lr: 0.000060  loss: 36.0079 (40.4313)  loss_n_40: 0.5404 (0.7274)  loss_n_60: 0.5630 (0.7266)  loss_n_80: 0.6432 (0.8088)  loss_n_100: 0.7058 (0.8832)  triple_100: 9.7995 (10.8371)  triple_80: 9.9024 (10.9745)  triple_60: 8.4599 (9.3467)  triple_40: 5.4235 (6.1269)  time: 5.8961  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1310/1724]  eta: 0:40:41  lr: 0.000060  loss: 36.8102 (40.4006)  loss_n_40: 0.5404 (0.7263)  loss_n_60: 0.5865 (0.7255)  loss_n_80: 0.6585 (0.8077)  loss_n_100: 0.7386 (0.8820)  triple_100: 10.1254 (10.8299)  triple_80: 10.1655 (10.9669)  triple_60: 8.6061 (9.3398)  triple_40: 5.3775 (6.1225)  time: 5.8939  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1320/1724]  eta: 0:39:42  lr: 0.000060  loss: 35.1686 (40.3625)  loss_n_40: 0.5222 (0.7253)  loss_n_60: 0.5536 (0.7243)  loss_n_80: 0.6325 (0.8063)  loss_n_100: 0.6995 (0.8805)  triple_100: 9.4333 (10.8192)  triple_80: 9.4731 (10.9560)  triple_60: 8.1609 (9.3315)  triple_40: 5.4407 (6.1195)  time: 5.8935  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1330/1724]  eta: 0:38:43  lr: 0.000060  loss: 35.1686 (40.3317)  loss_n_40: 0.5189 (0.7242)  loss_n_60: 0.5263 (0.7233)  loss_n_80: 0.6089 (0.8052)  loss_n_100: 0.6750 (0.8792)  triple_100: 9.3769 (10.8111)  triple_80: 9.4616 (10.9479)  triple_60: 8.1906 (9.3250)  triple_40: 5.5768 (6.1158)  time: 5.8948  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1340/1724]  eta: 0:37:44  lr: 0.000060  loss: 35.4154 (40.2967)  loss_n_40: 0.5380 (0.7233)  loss_n_60: 0.5635 (0.7222)  loss_n_80: 0.6366 (0.8040)  loss_n_100: 0.7081 (0.8780)  triple_100: 9.6386 (10.8018)  triple_80: 9.5844 (10.9381)  triple_60: 8.2318 (9.3173)  triple_40: 5.4847 (6.1120)  time: 5.8955  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1350/1724]  eta: 0:36:45  lr: 0.000060  loss: 34.3626 (40.2533)  loss_n_40: 0.5879 (0.7226)  loss_n_60: 0.5635 (0.7211)  loss_n_80: 0.6209 (0.8028)  loss_n_100: 0.6807 (0.8767)  triple_100: 9.3619 (10.7904)  triple_80: 9.3634 (10.9262)  triple_60: 7.9267 (9.3067)  triple_40: 5.4110 (6.1067)  time: 5.8955  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1360/1724]  eta: 0:35:46  lr: 0.000060  loss: 34.3626 (40.2131)  loss_n_40: 0.5895 (0.7222)  loss_n_60: 0.5653 (0.7204)  loss_n_80: 0.6215 (0.8019)  loss_n_100: 0.6807 (0.8756)  triple_100: 9.2652 (10.7796)  triple_80: 9.2853 (10.9154)  triple_60: 7.9699 (9.2979)  triple_40: 5.2767 (6.1002)  time: 5.8973  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [1370/1724]  eta: 0:34:47  lr: 0.000060  loss: 35.4895 (40.1963)  loss_n_40: 0.7818 (0.7241)  loss_n_60: 0.7542 (0.7219)  loss_n_80: 0.8143 (0.8034)  loss_n_100: 0.8781 (0.8772)  triple_100: 9.2652 (10.7724)  triple_80: 9.6241 (10.9095)  triple_60: 8.2803 (9.2929)  triple_40: 5.2518 (6.0949)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1380/1724]  eta: 0:33:48  lr: 0.000060  loss: 37.9171 (40.1857)  loss_n_40: 0.9048 (0.7262)  loss_n_60: 0.8702 (0.7234)  loss_n_80: 0.9710 (0.8049)  loss_n_100: 1.0541 (0.8786)  triple_100: 9.5102 (10.7654)  triple_80: 9.7872 (10.9036)  triple_60: 8.5796 (9.2889)  triple_40: 5.5703 (6.0947)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1390/1724]  eta: 0:32:49  lr: 0.000060  loss: 38.6430 (40.1740)  loss_n_40: 0.8280 (0.7275)  loss_n_60: 0.8382 (0.7240)  loss_n_80: 0.9492 (0.8055)  loss_n_100: 1.0073 (0.8792)  triple_100: 9.7421 (10.7602)  triple_80: 10.0394 (10.8991)  triple_60: 8.6586 (9.2852)  triple_40: 5.8639 (6.0933)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1400/1724]  eta: 0:31:50  lr: 0.000060  loss: 37.4969 (40.1559)  loss_n_40: 0.8178 (0.7286)  loss_n_60: 0.7575 (0.7245)  loss_n_80: 0.7898 (0.8058)  loss_n_100: 0.9110 (0.8796)  triple_100: 9.8974 (10.7532)  triple_80: 10.1270 (10.8924)  triple_60: 8.6066 (9.2805)  triple_40: 5.6992 (6.0912)  time: 5.8951  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1410/1724]  eta: 0:30:51  lr: 0.000060  loss: 37.1563 (40.1376)  loss_n_40: 0.7394 (0.7286)  loss_n_60: 0.7134 (0.7245)  loss_n_80: 0.7727 (0.8057)  loss_n_100: 0.8413 (0.8793)  triple_100: 9.8538 (10.7477)  triple_80: 9.9050 (10.8871)  triple_60: 8.5718 (9.2768)  triple_40: 5.5363 (6.0878)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1420/1724]  eta: 0:29:52  lr: 0.000060  loss: 37.4017 (40.1156)  loss_n_40: 0.6414 (0.7279)  loss_n_60: 0.6606 (0.7238)  loss_n_80: 0.7420 (0.8050)  loss_n_100: 0.8100 (0.8786)  triple_100: 9.8538 (10.7424)  triple_80: 9.9398 (10.8818)  triple_60: 8.5718 (9.2727)  triple_40: 5.4756 (6.0834)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1430/1724]  eta: 0:28:53  lr: 0.000060  loss: 37.0724 (40.1031)  loss_n_40: 0.6244 (0.7279)  loss_n_60: 0.6606 (0.7238)  loss_n_80: 0.7420 (0.8048)  loss_n_100: 0.8100 (0.8783)  triple_100: 9.9745 (10.7386)  triple_80: 10.0356 (10.8781)  triple_60: 8.5008 (9.2704)  triple_40: 5.4034 (6.0810)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1440/1724]  eta: 0:27:54  lr: 0.000060  loss: 35.5364 (40.0668)  loss_n_40: 0.5558 (0.7269)  loss_n_60: 0.5792 (0.7228)  loss_n_80: 0.6599 (0.8036)  loss_n_100: 0.7206 (0.8770)  triple_100: 9.3922 (10.7289)  triple_80: 9.6437 (10.8685)  triple_60: 8.2643 (9.2629)  triple_40: 5.4008 (6.0762)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1450/1724]  eta: 0:26:55  lr: 0.000060  loss: 34.5529 (40.0326)  loss_n_40: 0.5361 (0.7262)  loss_n_60: 0.5511 (0.7219)  loss_n_80: 0.6304 (0.8026)  loss_n_100: 0.6955 (0.8758)  triple_100: 9.3313 (10.7194)  triple_80: 9.4113 (10.8591)  triple_60: 8.1680 (9.2554)  triple_40: 5.3986 (6.0721)  time: 5.8973  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1460/1724]  eta: 0:25:56  lr: 0.000060  loss: 35.9589 (40.0124)  loss_n_40: 0.5681 (0.7257)  loss_n_60: 0.5980 (0.7214)  loss_n_80: 0.6913 (0.8020)  loss_n_100: 0.7540 (0.8752)  triple_100: 9.7608 (10.7151)  triple_80: 9.7800 (10.8545)  triple_60: 8.2149 (9.2507)  triple_40: 5.3668 (6.0676)  time: 5.8981  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1470/1724]  eta: 0:24:57  lr: 0.000060  loss: 35.2059 (39.9727)  loss_n_40: 0.5700 (0.7250)  loss_n_60: 0.5960 (0.7206)  loss_n_80: 0.6957 (0.8011)  loss_n_100: 0.7777 (0.8741)  triple_100: 9.7195 (10.7044)  triple_80: 9.7760 (10.8437)  triple_60: 7.9583 (9.2415)  triple_40: 5.3251 (6.0624)  time: 5.8978  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1480/1724]  eta: 0:23:58  lr: 0.000060  loss: 35.2059 (39.9589)  loss_n_40: 0.5559 (0.7246)  loss_n_60: 0.5960 (0.7202)  loss_n_80: 0.6839 (0.8006)  loss_n_100: 0.7545 (0.8736)  triple_100: 9.7790 (10.7006)  triple_80: 9.8988 (10.8405)  triple_60: 8.2955 (9.2391)  triple_40: 5.3927 (6.0596)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1490/1724]  eta: 0:22:59  lr: 0.000060  loss: 36.8154 (39.9300)  loss_n_40: 0.5220 (0.7238)  loss_n_60: 0.5815 (0.7193)  loss_n_80: 0.6472 (0.7996)  loss_n_100: 0.7110 (0.8724)  triple_100: 9.9151 (10.6927)  triple_80: 10.1302 (10.8326)  triple_60: 8.7744 (9.2332)  triple_40: 5.4398 (6.0564)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1500/1724]  eta: 0:22:00  lr: 0.000060  loss: 36.0332 (39.9171)  loss_n_40: 0.6111 (0.7236)  loss_n_60: 0.6124 (0.7191)  loss_n_80: 0.6884 (0.7993)  loss_n_100: 0.7213 (0.8721)  triple_100: 9.7440 (10.6902)  triple_80: 9.8130 (10.8299)  triple_60: 8.4062 (9.2301)  triple_40: 5.3677 (6.0527)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1510/1724]  eta: 0:21:01  lr: 0.000060  loss: 36.1765 (39.8931)  loss_n_40: 0.6322 (0.7229)  loss_n_60: 0.5968 (0.7183)  loss_n_80: 0.6884 (0.7984)  loss_n_100: 0.7626 (0.8711)  triple_100: 9.7440 (10.6836)  triple_80: 9.8852 (10.8231)  triple_60: 8.4174 (9.2253)  triple_40: 5.5137 (6.0505)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1520/1724]  eta: 0:20:02  lr: 0.000060  loss: 35.7803 (39.8687)  loss_n_40: 0.6356 (0.7231)  loss_n_60: 0.5968 (0.7183)  loss_n_80: 0.6764 (0.7982)  loss_n_100: 0.7626 (0.8709)  triple_100: 9.6863 (10.6771)  triple_80: 9.7166 (10.8164)  triple_60: 8.3177 (9.2194)  triple_40: 5.5423 (6.0454)  time: 5.8963  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1530/1724]  eta: 0:19:04  lr: 0.000060  loss: 35.2944 (39.8404)  loss_n_40: 0.7652 (0.7230)  loss_n_60: 0.7150 (0.7179)  loss_n_80: 0.7275 (0.7977)  loss_n_100: 0.8035 (0.8703)  triple_100: 9.6751 (10.6696)  triple_80: 9.6872 (10.8091)  triple_60: 8.1625 (9.2126)  triple_40: 5.1631 (6.0401)  time: 5.8952  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1540/1724]  eta: 0:18:05  lr: 0.000060  loss: 35.1594 (39.8126)  loss_n_40: 0.6230 (0.7233)  loss_n_60: 0.6438 (0.7175)  loss_n_80: 0.7089 (0.7970)  loss_n_100: 0.7592 (0.8694)  triple_100: 9.2798 (10.6591)  triple_80: 9.4880 (10.7996)  triple_60: 8.1625 (9.2070)  triple_40: 5.4600 (6.0396)  time: 5.8952  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1550/1724]  eta: 0:17:06  lr: 0.000060  loss: 35.7067 (39.7959)  loss_n_40: 0.6393 (0.7228)  loss_n_60: 0.6226 (0.7168)  loss_n_80: 0.6666 (0.7962)  loss_n_100: 0.7150 (0.8685)  triple_100: 9.2743 (10.6542)  triple_80: 9.6218 (10.7947)  triple_60: 8.5040 (9.2040)  triple_40: 5.8738 (6.0387)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1560/1724]  eta: 0:16:07  lr: 0.000060  loss: 35.8444 (39.7673)  loss_n_40: 0.6046 (0.7222)  loss_n_60: 0.6130 (0.7160)  loss_n_80: 0.6575 (0.7951)  loss_n_100: 0.6993 (0.8673)  triple_100: 9.2920 (10.6454)  triple_80: 9.6218 (10.7862)  triple_60: 8.2362 (9.1981)  triple_40: 5.8164 (6.0370)  time: 5.8955  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1570/1724]  eta: 0:15:08  lr: 0.000060  loss: 34.6450 (39.7404)  loss_n_40: 0.5433 (0.7210)  loss_n_60: 0.5378 (0.7148)  loss_n_80: 0.6184 (0.7938)  loss_n_100: 0.6607 (0.8659)  triple_100: 9.0714 (10.6371)  triple_80: 9.3185 (10.7785)  triple_60: 8.2214 (9.1932)  triple_40: 5.8121 (6.0361)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1580/1724]  eta: 0:14:09  lr: 0.000060  loss: 36.1406 (39.7244)  loss_n_40: 0.5433 (0.7199)  loss_n_60: 0.5381 (0.7139)  loss_n_80: 0.6079 (0.7929)  loss_n_100: 0.6609 (0.8649)  triple_100: 9.5505 (10.6335)  triple_80: 9.7655 (10.7747)  triple_60: 8.4870 (9.1903)  triple_40: 5.6983 (6.0342)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1590/1724]  eta: 0:13:10  lr: 0.000060  loss: 36.1406 (39.7014)  loss_n_40: 0.5519 (0.7191)  loss_n_60: 0.5567 (0.7131)  loss_n_80: 0.6168 (0.7921)  loss_n_100: 0.6649 (0.8640)  triple_100: 9.5549 (10.6271)  triple_80: 9.7416 (10.7682)  triple_60: 8.4374 (9.1850)  triple_40: 5.7088 (6.0328)  time: 5.8961  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1600/1724]  eta: 0:12:11  lr: 0.000060  loss: 36.3991 (39.6892)  loss_n_40: 0.6072 (0.7191)  loss_n_60: 0.5935 (0.7128)  loss_n_80: 0.6453 (0.7915)  loss_n_100: 0.6936 (0.8632)  triple_100: 9.5306 (10.6223)  triple_80: 9.6636 (10.7639)  triple_60: 8.3774 (9.1831)  triple_40: 5.8184 (6.0333)  time: 5.8959  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:4]  [1610/1724]  eta: 0:11:12  lr: 0.000060  loss: 38.3946 (39.6849)  loss_n_40: 0.6992 (0.7198)  loss_n_60: 0.6471 (0.7128)  loss_n_80: 0.6690 (0.7913)  loss_n_100: 0.7277 (0.8629)  triple_100: 9.6688 (10.6183)  triple_80: 10.1585 (10.7614)  triple_60: 8.8289 (9.1822)  triple_40: 6.1787 (6.0363)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1620/1724]  eta: 0:10:13  lr: 0.000060  loss: 36.4263 (39.6583)  loss_n_40: 0.6992 (0.7195)  loss_n_60: 0.6374 (0.7121)  loss_n_80: 0.6627 (0.7904)  loss_n_100: 0.7227 (0.8619)  triple_100: 9.4330 (10.6098)  triple_80: 9.7028 (10.7534)  triple_60: 8.6229 (9.1763)  triple_40: 5.9160 (6.0349)  time: 5.8938  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1630/1724]  eta: 0:09:14  lr: 0.000060  loss: 35.4713 (39.6456)  loss_n_40: 0.6456 (0.7192)  loss_n_60: 0.6146 (0.7116)  loss_n_80: 0.6429 (0.7897)  loss_n_100: 0.7227 (0.8610)  triple_100: 9.3312 (10.6058)  triple_80: 9.5368 (10.7494)  triple_60: 8.2775 (9.1743)  triple_40: 5.8554 (6.0346)  time: 5.8911  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1640/1724]  eta: 0:08:15  lr: 0.000060  loss: 35.4713 (39.6192)  loss_n_40: 0.6160 (0.7186)  loss_n_60: 0.6025 (0.7107)  loss_n_80: 0.6311 (0.7887)  loss_n_100: 0.6916 (0.8600)  triple_100: 9.3526 (10.5984)  triple_80: 9.4647 (10.7417)  triple_60: 8.1507 (9.1687)  triple_40: 5.7538 (6.0324)  time: 5.8923  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1650/1724]  eta: 0:07:16  lr: 0.000060  loss: 34.9940 (39.5976)  loss_n_40: 0.5492 (0.7177)  loss_n_60: 0.6025 (0.7102)  loss_n_80: 0.6311 (0.7881)  loss_n_100: 0.6916 (0.8593)  triple_100: 9.3526 (10.5933)  triple_80: 9.5438 (10.7365)  triple_60: 8.3724 (9.1645)  triple_40: 5.4014 (6.0281)  time: 5.8945  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1660/1724]  eta: 0:06:17  lr: 0.000060  loss: 35.2356 (39.5649)  loss_n_40: 0.5988 (0.7177)  loss_n_60: 0.6111 (0.7097)  loss_n_80: 0.6741 (0.7874)  loss_n_100: 0.7250 (0.8585)  triple_100: 9.4557 (10.5829)  triple_80: 9.7565 (10.7269)  triple_60: 8.3785 (9.1573)  triple_40: 5.2334 (6.0245)  time: 5.8942  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1670/1724]  eta: 0:05:18  lr: 0.000060  loss: 35.2356 (39.5437)  loss_n_40: 0.6104 (0.7171)  loss_n_60: 0.5814 (0.7091)  loss_n_80: 0.6550 (0.7867)  loss_n_100: 0.7210 (0.8577)  triple_100: 9.5416 (10.5775)  triple_80: 9.7493 (10.7215)  triple_60: 8.2737 (9.1529)  triple_40: 5.4444 (6.0212)  time: 5.8950  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1680/1724]  eta: 0:04:19  lr: 0.000060  loss: 35.5595 (39.5183)  loss_n_40: 0.5531 (0.7163)  loss_n_60: 0.5577 (0.7082)  loss_n_80: 0.6202 (0.7857)  loss_n_100: 0.6774 (0.8565)  triple_100: 9.5369 (10.5702)  triple_80: 9.6803 (10.7142)  triple_60: 8.2737 (9.1474)  triple_40: 5.6086 (6.0199)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1690/1724]  eta: 0:03:20  lr: 0.000060  loss: 35.5044 (39.4971)  loss_n_40: 0.5596 (0.7161)  loss_n_60: 0.5659 (0.7076)  loss_n_80: 0.6284 (0.7850)  loss_n_100: 0.6830 (0.8558)  triple_100: 9.4466 (10.5637)  triple_80: 9.5837 (10.7078)  triple_60: 8.1405 (9.1423)  triple_40: 5.7047 (6.0187)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1700/1724]  eta: 0:02:21  lr: 0.000060  loss: 36.7723 (39.4808)  loss_n_40: 0.5926 (0.7157)  loss_n_60: 0.6010 (0.7070)  loss_n_80: 0.6651 (0.7843)  loss_n_100: 0.7088 (0.8550)  triple_100: 9.7103 (10.5587)  triple_80: 9.9198 (10.7034)  triple_60: 8.7190 (9.1392)  triple_40: 5.7029 (6.0175)  time: 5.8955  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1710/1724]  eta: 0:01:22  lr: 0.000060  loss: 35.1538 (39.4513)  loss_n_40: 0.5744 (0.7147)  loss_n_60: 0.5840 (0.7060)  loss_n_80: 0.6512 (0.7832)  loss_n_100: 0.6965 (0.8538)  triple_100: 9.1149 (10.5508)  triple_80: 9.3290 (10.6952)  triple_60: 8.3696 (9.1332)  triple_40: 5.4925 (6.0144)  time: 5.8953  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1720/1724]  eta: 0:00:23  lr: 0.000060  loss: 33.4480 (39.4253)  loss_n_40: 0.5332 (0.7146)  loss_n_60: 0.5516 (0.7055)  loss_n_80: 0.6143 (0.7824)  loss_n_100: 0.6603 (0.8529)  triple_100: 9.0284 (10.5433)  triple_80: 9.1334 (10.6877)  triple_60: 8.0240 (9.1274)  triple_40: 5.4536 (6.0115)  time: 5.8961  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4]  [1723/1724]  eta: 0:00:05  lr: 0.000060  loss: 34.4336 (39.4197)  loss_n_40: 0.5224 (0.7142)  loss_n_60: 0.5405 (0.7052)  loss_n_80: 0.6141 (0.7821)  loss_n_100: 0.6723 (0.8526)  triple_100: 9.1149 (10.5422)  triple_80: 9.2837 (10.6864)  triple_60: 8.0956 (9.1264)  triple_40: 5.4536 (6.0107)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:4] Total time: 2:49:26 (5.8970 s / it)\n",
      "Averaged stats: lr: 0.000060  loss: 34.4336 (39.4197)  loss_n_40: 0.5224 (0.7142)  loss_n_60: 0.5405 (0.7052)  loss_n_80: 0.6141 (0.7821)  loss_n_100: 0.6723 (0.8526)  triple_100: 9.1149 (10.5422)  triple_80: 9.2837 (10.6864)  triple_60: 8.0956 (9.1264)  triple_40: 5.4536 (6.0107)\n",
      "Valid: [epoch:4]  [  0/845]  eta: 0:21:07  loss: 30.7925 (30.7925)  loss_n_40: 0.8661 (0.8661)  loss_n_60: 0.7410 (0.7410)  loss_n_80: 0.8782 (0.8782)  loss_n_100: 0.8679 (0.8679)  triple_100: 7.7807 (7.7807)  triple_80: 8.2777 (8.2777)  triple_60: 6.8275 (6.8275)  triple_40: 4.5534 (4.5534)  time: 1.4996  data: 0.5181  max mem: 40153\n",
      "Valid: [epoch:4]  [ 10/845]  eta: 0:14:14  loss: 33.6477 (34.7260)  loss_n_40: 0.5359 (0.5748)  loss_n_60: 0.5403 (0.5718)  loss_n_80: 0.6375 (0.6465)  loss_n_100: 0.7314 (0.7057)  triple_100: 9.2544 (9.4582)  triple_80: 9.0668 (9.4820)  triple_60: 7.7131 (8.0786)  triple_40: 5.1692 (5.2083)  time: 1.0229  data: 0.0472  max mem: 40153\n",
      "Valid: [epoch:4]  [ 20/845]  eta: 0:13:45  loss: 36.2408 (35.5995)  loss_n_40: 0.5058 (0.5447)  loss_n_60: 0.5403 (0.5672)  loss_n_80: 0.6466 (0.6461)  loss_n_100: 0.7292 (0.7121)  triple_100: 9.8302 (9.7739)  triple_80: 9.8624 (9.7696)  triple_60: 8.4765 (8.3101)  triple_40: 5.3199 (5.2758)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [ 30/845]  eta: 0:13:28  loss: 34.4432 (34.5582)  loss_n_40: 0.5058 (0.5656)  loss_n_60: 0.5080 (0.5565)  loss_n_80: 0.6466 (0.6202)  loss_n_100: 0.6817 (0.6790)  triple_100: 9.2115 (9.3198)  triple_80: 9.3980 (9.3706)  triple_60: 8.3568 (8.1072)  triple_40: 5.4625 (5.3393)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [ 40/845]  eta: 0:13:15  loss: 33.6257 (34.5599)  loss_n_40: 0.5594 (0.5603)  loss_n_60: 0.4926 (0.5486)  loss_n_80: 0.5385 (0.6104)  loss_n_100: 0.5975 (0.6710)  triple_100: 8.4995 (9.3306)  triple_80: 8.8070 (9.3683)  triple_60: 7.9862 (8.1163)  triple_40: 5.4625 (5.3545)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [ 50/845]  eta: 0:13:03  loss: 35.1799 (34.6898)  loss_n_40: 0.4932 (0.5632)  loss_n_60: 0.5220 (0.5495)  loss_n_80: 0.6024 (0.6138)  loss_n_100: 0.6884 (0.6760)  triple_100: 9.6487 (9.3922)  triple_80: 9.6378 (9.4227)  triple_60: 8.3387 (8.1385)  triple_40: 5.3240 (5.3339)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [ 60/845]  eta: 0:12:52  loss: 33.4640 (34.2032)  loss_n_40: 0.4763 (0.5680)  loss_n_60: 0.5080 (0.5466)  loss_n_80: 0.5672 (0.6059)  loss_n_100: 0.6111 (0.6652)  triple_100: 9.0433 (9.1908)  triple_80: 9.0715 (9.2368)  triple_60: 7.9570 (8.0340)  triple_40: 5.2829 (5.3558)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [ 70/845]  eta: 0:12:41  loss: 33.8440 (34.5036)  loss_n_40: 0.5405 (0.5783)  loss_n_60: 0.5417 (0.5541)  loss_n_80: 0.5930 (0.6165)  loss_n_100: 0.6231 (0.6800)  triple_100: 9.1683 (9.3142)  triple_80: 9.0103 (9.3349)  triple_60: 7.9158 (8.0742)  triple_40: 5.3848 (5.3514)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [ 80/845]  eta: 0:12:31  loss: 36.3679 (34.6162)  loss_n_40: 0.5185 (0.5834)  loss_n_60: 0.5517 (0.5577)  loss_n_80: 0.6493 (0.6202)  loss_n_100: 0.7464 (0.6845)  triple_100: 10.2932 (9.3598)  triple_80: 10.2216 (9.3812)  triple_60: 8.4086 (8.0862)  triple_40: 5.3413 (5.3432)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [ 90/845]  eta: 0:12:21  loss: 35.4882 (34.4536)  loss_n_40: 0.4879 (0.5885)  loss_n_60: 0.5510 (0.5595)  loss_n_80: 0.6297 (0.6217)  loss_n_100: 0.7176 (0.6868)  triple_100: 9.5930 (9.2991)  triple_80: 9.5270 (9.3214)  triple_60: 8.2624 (8.0436)  triple_40: 5.2753 (5.3332)  time: 0.9762  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:4]  [100/845]  eta: 0:12:10  loss: 32.4761 (34.6619)  loss_n_40: 0.5264 (0.5902)  loss_n_60: 0.5239 (0.5656)  loss_n_80: 0.6194 (0.6276)  loss_n_100: 0.7029 (0.6933)  triple_100: 8.8195 (9.3381)  triple_80: 8.8614 (9.3661)  triple_60: 7.8606 (8.1017)  triple_40: 5.4740 (5.3793)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [110/845]  eta: 0:12:00  loss: 35.1255 (34.7067)  loss_n_40: 0.5473 (0.5884)  loss_n_60: 0.5724 (0.5661)  loss_n_80: 0.6408 (0.6295)  loss_n_100: 0.7359 (0.6971)  triple_100: 9.5021 (9.3584)  triple_80: 9.4691 (9.3823)  triple_60: 8.0973 (8.1004)  triple_40: 5.5803 (5.3844)  time: 0.9770  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:4]  [120/845]  eta: 0:11:50  loss: 34.0126 (34.6347)  loss_n_40: 0.4985 (0.5777)  loss_n_60: 0.5040 (0.5595)  loss_n_80: 0.5863 (0.6233)  loss_n_100: 0.6715 (0.6905)  triple_100: 9.3272 (9.3491)  triple_80: 9.2544 (9.3733)  triple_60: 7.9800 (8.0886)  triple_40: 5.3155 (5.3726)  time: 0.9773  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:4]  [130/845]  eta: 0:11:40  loss: 33.4284 (34.6696)  loss_n_40: 0.4706 (0.5761)  loss_n_60: 0.5007 (0.5584)  loss_n_80: 0.5569 (0.6219)  loss_n_100: 0.6208 (0.6882)  triple_100: 8.9812 (9.3488)  triple_80: 9.1250 (9.3819)  triple_60: 8.0493 (8.1048)  triple_40: 5.2736 (5.3895)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [140/845]  eta: 0:11:30  loss: 33.9644 (34.6709)  loss_n_40: 0.5214 (0.5848)  loss_n_60: 0.5619 (0.5630)  loss_n_80: 0.6333 (0.6269)  loss_n_100: 0.7125 (0.6933)  triple_100: 9.1798 (9.3538)  triple_80: 9.0427 (9.3826)  triple_60: 8.1943 (8.0869)  triple_40: 5.3470 (5.3796)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [150/845]  eta: 0:11:20  loss: 35.3391 (34.7665)  loss_n_40: 0.5233 (0.5813)  loss_n_60: 0.5353 (0.5635)  loss_n_80: 0.6406 (0.6288)  loss_n_100: 0.7100 (0.6955)  triple_100: 9.6079 (9.3920)  triple_80: 9.5330 (9.4215)  triple_60: 8.3051 (8.1083)  triple_40: 5.1865 (5.3757)  time: 0.9769  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:4]  [160/845]  eta: 0:11:11  loss: 34.7681 (34.7031)  loss_n_40: 0.4668 (0.5758)  loss_n_60: 0.5198 (0.5604)  loss_n_80: 0.6094 (0.6258)  loss_n_100: 0.6874 (0.6910)  triple_100: 9.3795 (9.3685)  triple_80: 9.4209 (9.4097)  triple_60: 8.1946 (8.1025)  triple_40: 5.3399 (5.3694)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [170/845]  eta: 0:11:01  loss: 33.3354 (34.6657)  loss_n_40: 0.4860 (0.5722)  loss_n_60: 0.4911 (0.5574)  loss_n_80: 0.5280 (0.6228)  loss_n_100: 0.5737 (0.6877)  triple_100: 9.0262 (9.3477)  triple_80: 9.0647 (9.4003)  triple_60: 7.6701 (8.0998)  triple_40: 5.4371 (5.3779)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [180/845]  eta: 0:10:51  loss: 33.3132 (34.6524)  loss_n_40: 0.4923 (0.5806)  loss_n_60: 0.5118 (0.5616)  loss_n_80: 0.6009 (0.6262)  loss_n_100: 0.6890 (0.6900)  triple_100: 9.0333 (9.3409)  triple_80: 9.0647 (9.3958)  triple_60: 7.6170 (8.0874)  triple_40: 5.3463 (5.3698)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [190/845]  eta: 0:10:41  loss: 34.4444 (34.5450)  loss_n_40: 0.5336 (0.5928)  loss_n_60: 0.5267 (0.5663)  loss_n_80: 0.6062 (0.6276)  loss_n_100: 0.6916 (0.6903)  triple_100: 8.9584 (9.2870)  triple_80: 9.0268 (9.3494)  triple_60: 7.9749 (8.0554)  triple_40: 5.2679 (5.3761)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [200/845]  eta: 0:10:31  loss: 34.6534 (34.6206)  loss_n_40: 0.5410 (0.5910)  loss_n_60: 0.5560 (0.5664)  loss_n_80: 0.6295 (0.6288)  loss_n_100: 0.7059 (0.6919)  triple_100: 9.1960 (9.3201)  triple_80: 9.1404 (9.3798)  triple_60: 8.3913 (8.0732)  triple_40: 5.2402 (5.3694)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [210/845]  eta: 0:10:21  loss: 37.0782 (34.7227)  loss_n_40: 0.5277 (0.5952)  loss_n_60: 0.5964 (0.5709)  loss_n_80: 0.6747 (0.6332)  loss_n_100: 0.7520 (0.6965)  triple_100: 10.4810 (9.3526)  triple_80: 10.3537 (9.4114)  triple_60: 8.6355 (8.0943)  triple_40: 5.2402 (5.3687)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [220/845]  eta: 0:10:11  loss: 34.6435 (34.6967)  loss_n_40: 0.5074 (0.5907)  loss_n_60: 0.5247 (0.5680)  loss_n_80: 0.6190 (0.6306)  loss_n_100: 0.7069 (0.6936)  triple_100: 9.5853 (9.3458)  triple_80: 9.4484 (9.4071)  triple_60: 8.2304 (8.0924)  triple_40: 5.3084 (5.3684)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [230/845]  eta: 0:10:01  loss: 33.7228 (34.7293)  loss_n_40: 0.5003 (0.5893)  loss_n_60: 0.5151 (0.5683)  loss_n_80: 0.5799 (0.6312)  loss_n_100: 0.6242 (0.6945)  triple_100: 8.9856 (9.3562)  triple_80: 9.1411 (9.4173)  triple_60: 8.0902 (8.1022)  triple_40: 5.2674 (5.3703)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [240/845]  eta: 0:09:52  loss: 33.2356 (34.6786)  loss_n_40: 0.5058 (0.5887)  loss_n_60: 0.5142 (0.5664)  loss_n_80: 0.5678 (0.6284)  loss_n_100: 0.6242 (0.6911)  triple_100: 8.9567 (9.3306)  triple_80: 8.9429 (9.3953)  triple_60: 8.0602 (8.0964)  triple_40: 5.4060 (5.3818)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [250/845]  eta: 0:09:42  loss: 33.1989 (34.6832)  loss_n_40: 0.4999 (0.5852)  loss_n_60: 0.4811 (0.5649)  loss_n_80: 0.5620 (0.6271)  loss_n_100: 0.6062 (0.6896)  triple_100: 8.8133 (9.3315)  triple_80: 8.9402 (9.3988)  triple_60: 7.8870 (8.1027)  triple_40: 5.4003 (5.3834)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [260/845]  eta: 0:09:32  loss: 34.4878 (34.7044)  loss_n_40: 0.5093 (0.5823)  loss_n_60: 0.5254 (0.5634)  loss_n_80: 0.5936 (0.6264)  loss_n_100: 0.6634 (0.6891)  triple_100: 9.1722 (9.3505)  triple_80: 9.4251 (9.4125)  triple_60: 8.2288 (8.1066)  triple_40: 5.1815 (5.3736)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [270/845]  eta: 0:09:22  loss: 35.6236 (34.7705)  loss_n_40: 0.4979 (0.5787)  loss_n_60: 0.5272 (0.5629)  loss_n_80: 0.6082 (0.6263)  loss_n_100: 0.6939 (0.6898)  triple_100: 10.0832 (9.3767)  triple_80: 9.8448 (9.4357)  triple_60: 8.3753 (8.1237)  triple_40: 5.2335 (5.3768)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [280/845]  eta: 0:09:12  loss: 36.7848 (34.8651)  loss_n_40: 0.4979 (0.5786)  loss_n_60: 0.5582 (0.5651)  loss_n_80: 0.6452 (0.6293)  loss_n_100: 0.7266 (0.6927)  triple_100: 10.2895 (9.4080)  triple_80: 10.2598 (9.4666)  triple_60: 8.5624 (8.1491)  triple_40: 5.4184 (5.3756)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [290/845]  eta: 0:09:02  loss: 35.9861 (34.8912)  loss_n_40: 0.4843 (0.5755)  loss_n_60: 0.5167 (0.5635)  loss_n_80: 0.6107 (0.6282)  loss_n_100: 0.6954 (0.6920)  triple_100: 9.9330 (9.4222)  triple_80: 9.9614 (9.4786)  triple_60: 8.4282 (8.1529)  triple_40: 5.3231 (5.3782)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [300/845]  eta: 0:08:53  loss: 34.5720 (34.8589)  loss_n_40: 0.4843 (0.5768)  loss_n_60: 0.5023 (0.5635)  loss_n_80: 0.5550 (0.6273)  loss_n_100: 0.6144 (0.6907)  triple_100: 9.2264 (9.4028)  triple_80: 9.2698 (9.4625)  triple_60: 7.9591 (8.1477)  triple_40: 5.4859 (5.3877)  time: 0.9773  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [310/845]  eta: 0:08:43  loss: 34.1663 (34.8646)  loss_n_40: 0.4708 (0.5735)  loss_n_60: 0.5239 (0.5618)  loss_n_80: 0.5760 (0.6259)  loss_n_100: 0.6486 (0.6894)  triple_100: 9.1905 (9.4083)  triple_80: 9.2698 (9.4661)  triple_60: 8.1514 (8.1509)  triple_40: 5.6116 (5.3887)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [320/845]  eta: 0:08:33  loss: 33.8976 (34.7893)  loss_n_40: 0.4837 (0.5781)  loss_n_60: 0.4994 (0.5628)  loss_n_80: 0.5892 (0.6265)  loss_n_100: 0.6515 (0.6895)  triple_100: 9.0943 (9.3795)  triple_80: 9.1775 (9.4397)  triple_60: 8.0375 (8.1256)  triple_40: 5.3417 (5.3875)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [330/845]  eta: 0:08:23  loss: 34.3109 (34.7915)  loss_n_40: 0.5111 (0.5794)  loss_n_60: 0.5617 (0.5641)  loss_n_80: 0.6428 (0.6281)  loss_n_100: 0.7059 (0.6911)  triple_100: 9.2703 (9.3819)  triple_80: 9.3301 (9.4425)  triple_60: 8.0905 (8.1231)  triple_40: 5.2529 (5.3814)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [340/845]  eta: 0:08:13  loss: 34.9096 (34.7380)  loss_n_40: 0.4994 (0.5784)  loss_n_60: 0.5388 (0.5629)  loss_n_80: 0.6428 (0.6268)  loss_n_100: 0.7131 (0.6896)  triple_100: 9.5455 (9.3651)  triple_80: 9.5356 (9.4249)  triple_60: 8.2495 (8.1109)  triple_40: 5.2529 (5.3793)  time: 0.9763  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:4]  [350/845]  eta: 0:08:04  loss: 34.7923 (34.7969)  loss_n_40: 0.4922 (0.5760)  loss_n_60: 0.5288 (0.5625)  loss_n_80: 0.5966 (0.6270)  loss_n_100: 0.6809 (0.6901)  triple_100: 9.5304 (9.3884)  triple_80: 9.5086 (9.4461)  triple_60: 8.1196 (8.1266)  triple_40: 5.3635 (5.3803)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [360/845]  eta: 0:07:54  loss: 34.5936 (34.7906)  loss_n_40: 0.4944 (0.5741)  loss_n_60: 0.5313 (0.5612)  loss_n_80: 0.5966 (0.6257)  loss_n_100: 0.6596 (0.6888)  triple_100: 9.2049 (9.3847)  triple_80: 9.5086 (9.4441)  triple_60: 8.1196 (8.1268)  triple_40: 5.4743 (5.3850)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [370/845]  eta: 0:07:44  loss: 34.0418 (34.8054)  loss_n_40: 0.5006 (0.5745)  loss_n_60: 0.5390 (0.5613)  loss_n_80: 0.6200 (0.6260)  loss_n_100: 0.6698 (0.6889)  triple_100: 9.0398 (9.3866)  triple_80: 9.2971 (9.4488)  triple_60: 8.1163 (8.1321)  triple_40: 5.4907 (5.3872)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [380/845]  eta: 0:07:34  loss: 34.5010 (34.7755)  loss_n_40: 0.4795 (0.5727)  loss_n_60: 0.5256 (0.5593)  loss_n_80: 0.6125 (0.6239)  loss_n_100: 0.6698 (0.6866)  triple_100: 9.0562 (9.3782)  triple_80: 9.2958 (9.4409)  triple_60: 8.0459 (8.1263)  triple_40: 5.4761 (5.3876)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [390/845]  eta: 0:07:24  loss: 33.7118 (34.7267)  loss_n_40: 0.5046 (0.5744)  loss_n_60: 0.4929 (0.5591)  loss_n_80: 0.5196 (0.6230)  loss_n_100: 0.6028 (0.6853)  triple_100: 8.9987 (9.3620)  triple_80: 9.0254 (9.4239)  triple_60: 7.9442 (8.1138)  triple_40: 5.3068 (5.3850)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [400/845]  eta: 0:07:15  loss: 33.6915 (34.7339)  loss_n_40: 0.5046 (0.5737)  loss_n_60: 0.4991 (0.5586)  loss_n_80: 0.5635 (0.6227)  loss_n_100: 0.6249 (0.6851)  triple_100: 9.2570 (9.3686)  triple_80: 9.0364 (9.4280)  triple_60: 8.0428 (8.1151)  triple_40: 5.2884 (5.3821)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [410/845]  eta: 0:07:05  loss: 33.4254 (34.7160)  loss_n_40: 0.4875 (0.5712)  loss_n_60: 0.4949 (0.5570)  loss_n_80: 0.5386 (0.6207)  loss_n_100: 0.6079 (0.6830)  triple_100: 9.0249 (9.3649)  triple_80: 9.0364 (9.4225)  triple_60: 7.9897 (8.1153)  triple_40: 5.2884 (5.3813)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [420/845]  eta: 0:06:55  loss: 34.1428 (34.7424)  loss_n_40: 0.4955 (0.5701)  loss_n_60: 0.5352 (0.5574)  loss_n_80: 0.6219 (0.6216)  loss_n_100: 0.6917 (0.6840)  triple_100: 9.0873 (9.3757)  triple_80: 9.4277 (9.4337)  triple_60: 7.8628 (8.1207)  triple_40: 5.4351 (5.3792)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [430/845]  eta: 0:06:45  loss: 35.5748 (34.7544)  loss_n_40: 0.4957 (0.5701)  loss_n_60: 0.5586 (0.5576)  loss_n_80: 0.6675 (0.6222)  loss_n_100: 0.7470 (0.6848)  triple_100: 9.9712 (9.3804)  triple_80: 9.9328 (9.4388)  triple_60: 8.2499 (8.1229)  triple_40: 5.3219 (5.3777)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [440/845]  eta: 0:06:35  loss: 35.9839 (34.7733)  loss_n_40: 0.4806 (0.5682)  loss_n_60: 0.5541 (0.5572)  loss_n_80: 0.6625 (0.6219)  loss_n_100: 0.7465 (0.6848)  triple_100: 9.9464 (9.3907)  triple_80: 10.1082 (9.4473)  triple_60: 8.4459 (8.1280)  triple_40: 5.2267 (5.3751)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [450/845]  eta: 0:06:26  loss: 33.1593 (34.6943)  loss_n_40: 0.5246 (0.5744)  loss_n_60: 0.5758 (0.5591)  loss_n_80: 0.6406 (0.6228)  loss_n_100: 0.7170 (0.6854)  triple_100: 9.0021 (9.3589)  triple_80: 9.1459 (9.4181)  triple_60: 8.0570 (8.1001)  triple_40: 5.2777 (5.3755)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [460/845]  eta: 0:06:16  loss: 31.8381 (34.6333)  loss_n_40: 0.5166 (0.5748)  loss_n_60: 0.5118 (0.5579)  loss_n_80: 0.5925 (0.6213)  loss_n_100: 0.6420 (0.6835)  triple_100: 8.3350 (9.3384)  triple_80: 8.5100 (9.3995)  triple_60: 7.6225 (8.0845)  triple_40: 5.3496 (5.3734)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [470/845]  eta: 0:06:06  loss: 33.3180 (34.6283)  loss_n_40: 0.4994 (0.5760)  loss_n_60: 0.5126 (0.5581)  loss_n_80: 0.5925 (0.6214)  loss_n_100: 0.6420 (0.6836)  triple_100: 8.8497 (9.3337)  triple_80: 8.8309 (9.3952)  triple_60: 7.7861 (8.0828)  triple_40: 5.4349 (5.3776)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [480/845]  eta: 0:05:56  loss: 33.6351 (34.6202)  loss_n_40: 0.4850 (0.5742)  loss_n_60: 0.5192 (0.5567)  loss_n_80: 0.5461 (0.6200)  loss_n_100: 0.5955 (0.6822)  triple_100: 8.9389 (9.3341)  triple_80: 8.8550 (9.3936)  triple_60: 7.7732 (8.0812)  triple_40: 5.5612 (5.3782)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [490/845]  eta: 0:05:47  loss: 33.7490 (34.6274)  loss_n_40: 0.4850 (0.5734)  loss_n_60: 0.5027 (0.5570)  loss_n_80: 0.5798 (0.6205)  loss_n_100: 0.6707 (0.6831)  triple_100: 9.2014 (9.3394)  triple_80: 8.9716 (9.3967)  triple_60: 7.8893 (8.0827)  triple_40: 5.2477 (5.3746)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [500/845]  eta: 0:05:37  loss: 36.1337 (34.6694)  loss_n_40: 0.4928 (0.5728)  loss_n_60: 0.5345 (0.5574)  loss_n_80: 0.6278 (0.6212)  loss_n_100: 0.7118 (0.6840)  triple_100: 9.9613 (9.3539)  triple_80: 10.0476 (9.4106)  triple_60: 8.4742 (8.0927)  triple_40: 5.3778 (5.3769)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [510/845]  eta: 0:05:27  loss: 34.5885 (34.6441)  loss_n_40: 0.4928 (0.5735)  loss_n_60: 0.5014 (0.5571)  loss_n_80: 0.5968 (0.6209)  loss_n_100: 0.6580 (0.6835)  triple_100: 9.8317 (9.3451)  triple_80: 9.7314 (9.4033)  triple_60: 8.2291 (8.0853)  triple_40: 5.3353 (5.3754)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [520/845]  eta: 0:05:17  loss: 33.2541 (34.6022)  loss_n_40: 0.5361 (0.5776)  loss_n_60: 0.5261 (0.5586)  loss_n_80: 0.6343 (0.6222)  loss_n_100: 0.7160 (0.6845)  triple_100: 8.8500 (9.3272)  triple_80: 8.9690 (9.3881)  triple_60: 7.7340 (8.0696)  triple_40: 5.2319 (5.3745)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [530/845]  eta: 0:05:07  loss: 33.2541 (34.5736)  loss_n_40: 0.5361 (0.5806)  loss_n_60: 0.5802 (0.5600)  loss_n_80: 0.6683 (0.6232)  loss_n_100: 0.7470 (0.6849)  triple_100: 9.0168 (9.3122)  triple_80: 9.0435 (9.3766)  triple_60: 7.7003 (8.0606)  triple_40: 5.3119 (5.3755)  time: 0.9763  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:4]  [540/845]  eta: 0:04:58  loss: 32.8526 (34.5413)  loss_n_40: 0.5141 (0.5813)  loss_n_60: 0.5001 (0.5599)  loss_n_80: 0.5651 (0.6222)  loss_n_100: 0.6416 (0.6838)  triple_100: 8.8831 (9.2968)  triple_80: 8.9034 (9.3621)  triple_60: 7.7461 (8.0546)  triple_40: 5.5419 (5.3806)  time: 0.9768  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:4]  [550/845]  eta: 0:04:48  loss: 33.6206 (34.5344)  loss_n_40: 0.5381 (0.5812)  loss_n_60: 0.4880 (0.5599)  loss_n_80: 0.5626 (0.6228)  loss_n_100: 0.6271 (0.6844)  triple_100: 8.9471 (9.2966)  triple_80: 9.0292 (9.3622)  triple_60: 7.9585 (8.0497)  triple_40: 5.4972 (5.3775)  time: 0.9766  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:4]  [560/845]  eta: 0:04:38  loss: 35.1560 (34.5562)  loss_n_40: 0.5041 (0.5796)  loss_n_60: 0.5229 (0.5591)  loss_n_80: 0.6167 (0.6222)  loss_n_100: 0.6835 (0.6838)  triple_100: 9.4804 (9.3023)  triple_80: 9.6040 (9.3701)  triple_60: 8.0467 (8.0571)  triple_40: 5.3330 (5.3821)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [570/845]  eta: 0:04:28  loss: 34.6280 (34.5591)  loss_n_40: 0.5055 (0.5805)  loss_n_60: 0.4821 (0.5591)  loss_n_80: 0.5878 (0.6220)  loss_n_100: 0.6748 (0.6833)  triple_100: 9.4804 (9.3011)  triple_80: 9.6040 (9.3703)  triple_60: 8.0665 (8.0580)  triple_40: 5.5954 (5.3848)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [580/845]  eta: 0:04:19  loss: 35.0666 (34.5776)  loss_n_40: 0.5043 (0.5793)  loss_n_60: 0.5265 (0.5588)  loss_n_80: 0.6139 (0.6217)  loss_n_100: 0.6810 (0.6834)  triple_100: 9.8971 (9.3081)  triple_80: 9.9003 (9.3762)  triple_60: 8.2935 (8.0631)  triple_40: 5.4637 (5.3870)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [590/845]  eta: 0:04:09  loss: 36.3790 (34.5867)  loss_n_40: 0.4821 (0.5777)  loss_n_60: 0.5399 (0.5582)  loss_n_80: 0.6131 (0.6211)  loss_n_100: 0.6857 (0.6831)  triple_100: 10.0333 (9.3124)  triple_80: 9.3724 (9.3783)  triple_60: 8.4514 (8.0669)  triple_40: 5.4532 (5.3890)  time: 0.9766  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:4]  [600/845]  eta: 0:03:59  loss: 33.6745 (34.6008)  loss_n_40: 0.4821 (0.5761)  loss_n_60: 0.4903 (0.5574)  loss_n_80: 0.5419 (0.6206)  loss_n_100: 0.6386 (0.6828)  triple_100: 9.0184 (9.3202)  triple_80: 9.0779 (9.3848)  triple_60: 7.9956 (8.0705)  triple_40: 5.4056 (5.3884)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [610/845]  eta: 0:03:49  loss: 33.9961 (34.5833)  loss_n_40: 0.5170 (0.5819)  loss_n_60: 0.5635 (0.5602)  loss_n_80: 0.6497 (0.6231)  loss_n_100: 0.7193 (0.6848)  triple_100: 9.1227 (9.3083)  triple_80: 9.5122 (9.3767)  triple_60: 7.9956 (8.0627)  triple_40: 5.0987 (5.3856)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [620/845]  eta: 0:03:39  loss: 33.9961 (34.5722)  loss_n_40: 0.5430 (0.5807)  loss_n_60: 0.5574 (0.5589)  loss_n_80: 0.6169 (0.6216)  loss_n_100: 0.6615 (0.6831)  triple_100: 9.0636 (9.3028)  triple_80: 9.1754 (9.3719)  triple_60: 7.9912 (8.0627)  triple_40: 5.5411 (5.3904)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [630/845]  eta: 0:03:30  loss: 33.2622 (34.5542)  loss_n_40: 0.5430 (0.5826)  loss_n_60: 0.5064 (0.5597)  loss_n_80: 0.5573 (0.6221)  loss_n_100: 0.6078 (0.6834)  triple_100: 8.9584 (9.2926)  triple_80: 9.0312 (9.3642)  triple_60: 7.9344 (8.0580)  triple_40: 5.6217 (5.3916)  time: 0.9765  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:4]  [640/845]  eta: 0:03:20  loss: 33.1212 (34.5477)  loss_n_40: 0.5420 (0.5835)  loss_n_60: 0.5704 (0.5599)  loss_n_80: 0.6271 (0.6219)  loss_n_100: 0.6954 (0.6831)  triple_100: 8.6950 (9.2874)  triple_80: 8.8402 (9.3604)  triple_60: 8.0068 (8.0567)  triple_40: 5.6241 (5.3948)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [650/845]  eta: 0:03:10  loss: 34.5133 (34.5558)  loss_n_40: 0.4794 (0.5817)  loss_n_60: 0.5629 (0.5592)  loss_n_80: 0.6052 (0.6215)  loss_n_100: 0.6701 (0.6828)  triple_100: 9.5422 (9.2939)  triple_80: 9.2850 (9.3654)  triple_60: 8.1866 (8.0592)  triple_40: 5.3078 (5.3920)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [660/845]  eta: 0:03:00  loss: 33.3544 (34.5401)  loss_n_40: 0.4744 (0.5804)  loss_n_60: 0.4876 (0.5582)  loss_n_80: 0.5303 (0.6201)  loss_n_100: 0.5713 (0.6814)  triple_100: 8.9831 (9.2880)  triple_80: 8.9891 (9.3602)  triple_60: 7.9205 (8.0589)  triple_40: 5.3251 (5.3929)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [670/845]  eta: 0:02:51  loss: 33.0924 (34.5371)  loss_n_40: 0.5287 (0.5800)  loss_n_60: 0.5064 (0.5579)  loss_n_80: 0.5327 (0.6199)  loss_n_100: 0.5886 (0.6813)  triple_100: 8.7568 (9.2888)  triple_80: 8.7625 (9.3593)  triple_60: 7.8470 (8.0580)  triple_40: 5.3251 (5.3919)  time: 0.9767  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:4]  [680/845]  eta: 0:02:41  loss: 33.1005 (34.5285)  loss_n_40: 0.5127 (0.5787)  loss_n_60: 0.5074 (0.5569)  loss_n_80: 0.5779 (0.6189)  loss_n_100: 0.6408 (0.6805)  triple_100: 8.9115 (9.2893)  triple_80: 8.7625 (9.3573)  triple_60: 7.8563 (8.0559)  triple_40: 5.1419 (5.3911)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [690/845]  eta: 0:02:31  loss: 33.8321 (34.5329)  loss_n_40: 0.5072 (0.5783)  loss_n_60: 0.5070 (0.5569)  loss_n_80: 0.5779 (0.6189)  loss_n_100: 0.6388 (0.6805)  triple_100: 8.9857 (9.2902)  triple_80: 9.0715 (9.3580)  triple_60: 7.8563 (8.0571)  triple_40: 5.4230 (5.3930)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [700/845]  eta: 0:02:21  loss: 33.8534 (34.5098)  loss_n_40: 0.5079 (0.5774)  loss_n_60: 0.5070 (0.5560)  loss_n_80: 0.5781 (0.6181)  loss_n_100: 0.6388 (0.6797)  triple_100: 8.9299 (9.2831)  triple_80: 9.0303 (9.3512)  triple_60: 7.8819 (8.0512)  triple_40: 5.3530 (5.3930)  time: 0.9760  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:4]  [710/845]  eta: 0:02:11  loss: 34.3650 (34.5207)  loss_n_40: 0.5097 (0.5787)  loss_n_60: 0.5416 (0.5567)  loss_n_80: 0.6275 (0.6191)  loss_n_100: 0.6918 (0.6807)  triple_100: 9.1871 (9.2863)  triple_80: 9.3878 (9.3553)  triple_60: 7.9726 (8.0512)  triple_40: 5.2213 (5.3927)  time: 0.9758  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:4]  [720/845]  eta: 0:02:02  loss: 35.8236 (34.5389)  loss_n_40: 0.5584 (0.5783)  loss_n_60: 0.5464 (0.5567)  loss_n_80: 0.6340 (0.6193)  loss_n_100: 0.7232 (0.6811)  triple_100: 9.7389 (9.2949)  triple_80: 9.7958 (9.3622)  triple_60: 8.3887 (8.0549)  triple_40: 5.2315 (5.3916)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [730/845]  eta: 0:01:52  loss: 35.2621 (34.5221)  loss_n_40: 0.5299 (0.5776)  loss_n_60: 0.5431 (0.5560)  loss_n_80: 0.6263 (0.6188)  loss_n_100: 0.7004 (0.6805)  triple_100: 9.6576 (9.2902)  triple_80: 9.5391 (9.3577)  triple_60: 8.2715 (8.0504)  triple_40: 5.2315 (5.3909)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [740/845]  eta: 0:01:42  loss: 33.7412 (34.5112)  loss_n_40: 0.4632 (0.5760)  loss_n_60: 0.4843 (0.5552)  loss_n_80: 0.5621 (0.6182)  loss_n_100: 0.6001 (0.6798)  triple_100: 9.0691 (9.2886)  triple_80: 9.2121 (9.3564)  triple_60: 7.8588 (8.0493)  triple_40: 5.2044 (5.3876)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [750/845]  eta: 0:01:32  loss: 33.9710 (34.5353)  loss_n_40: 0.4926 (0.5757)  loss_n_60: 0.4957 (0.5554)  loss_n_80: 0.5621 (0.6185)  loss_n_100: 0.6340 (0.6803)  triple_100: 9.3660 (9.2979)  triple_80: 9.3050 (9.3642)  triple_60: 7.9650 (8.0550)  triple_40: 5.2044 (5.3882)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [760/845]  eta: 0:01:23  loss: 35.6506 (34.5575)  loss_n_40: 0.5095 (0.5745)  loss_n_60: 0.5265 (0.5552)  loss_n_80: 0.6186 (0.6186)  loss_n_100: 0.7023 (0.6805)  triple_100: 10.0466 (9.3073)  triple_80: 9.9803 (9.3731)  triple_60: 8.4426 (8.0612)  triple_40: 5.2124 (5.3872)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [770/845]  eta: 0:01:13  loss: 35.6506 (34.5710)  loss_n_40: 0.4816 (0.5734)  loss_n_60: 0.5103 (0.5548)  loss_n_80: 0.5946 (0.6183)  loss_n_100: 0.6619 (0.6804)  triple_100: 9.8866 (9.3129)  triple_80: 9.8124 (9.3779)  triple_60: 8.3789 (8.0651)  triple_40: 5.4179 (5.3881)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [780/845]  eta: 0:01:03  loss: 34.7545 (34.5667)  loss_n_40: 0.4822 (0.5723)  loss_n_60: 0.4969 (0.5542)  loss_n_80: 0.5671 (0.6179)  loss_n_100: 0.6269 (0.6798)  triple_100: 9.4840 (9.3121)  triple_80: 9.5540 (9.3781)  triple_60: 8.2219 (8.0646)  triple_40: 5.4179 (5.3876)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [790/845]  eta: 0:00:53  loss: 33.1242 (34.5492)  loss_n_40: 0.4949 (0.5714)  loss_n_60: 0.4876 (0.5534)  loss_n_80: 0.5296 (0.6172)  loss_n_100: 0.5939 (0.6790)  triple_100: 8.8474 (9.3066)  triple_80: 8.9409 (9.3724)  triple_60: 7.7944 (8.0611)  triple_40: 5.3524 (5.3882)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [800/845]  eta: 0:00:43  loss: 32.8381 (34.5209)  loss_n_40: 0.4731 (0.5704)  loss_n_60: 0.4756 (0.5521)  loss_n_80: 0.5260 (0.6157)  loss_n_100: 0.5718 (0.6774)  triple_100: 8.6449 (9.2969)  triple_80: 8.7770 (9.3632)  triple_60: 7.7555 (8.0558)  triple_40: 5.4208 (5.3896)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [810/845]  eta: 0:00:34  loss: 32.2694 (34.5293)  loss_n_40: 0.4856 (0.5715)  loss_n_60: 0.4847 (0.5528)  loss_n_80: 0.5143 (0.6163)  loss_n_100: 0.5656 (0.6779)  triple_100: 8.6321 (9.2989)  triple_80: 8.6261 (9.3652)  triple_60: 7.6914 (8.0565)  triple_40: 5.3617 (5.3902)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [820/845]  eta: 0:00:24  loss: 32.7498 (34.5178)  loss_n_40: 0.4763 (0.5703)  loss_n_60: 0.5241 (0.5520)  loss_n_80: 0.5910 (0.6159)  loss_n_100: 0.6501 (0.6772)  triple_100: 8.7010 (9.2932)  triple_80: 8.8122 (9.3630)  triple_60: 7.7135 (8.0551)  triple_40: 5.2768 (5.3911)  time: 0.9765  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:4]  [830/845]  eta: 0:00:14  loss: 33.7948 (34.5152)  loss_n_40: 0.4763 (0.5713)  loss_n_60: 0.5072 (0.5526)  loss_n_80: 0.5780 (0.6161)  loss_n_100: 0.6501 (0.6774)  triple_100: 8.7575 (9.2906)  triple_80: 9.1050 (9.3611)  triple_60: 7.9082 (8.0543)  triple_40: 5.4720 (5.3918)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4]  [840/845]  eta: 0:00:04  loss: 35.3662 (34.5220)  loss_n_40: 0.4994 (0.5703)  loss_n_60: 0.5260 (0.5521)  loss_n_80: 0.6069 (0.6158)  loss_n_100: 0.6895 (0.6771)  triple_100: 9.4919 (9.2949)  triple_80: 9.4968 (9.3648)  triple_60: 8.1719 (8.0562)  triple_40: 5.3461 (5.3908)  time: 0.9767  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:4]  [844/845]  eta: 0:00:00  loss: 35.3662 (34.5132)  loss_n_40: 0.4914 (0.5708)  loss_n_60: 0.5287 (0.5535)  loss_n_80: 0.6233 (0.6166)  loss_n_100: 0.7117 (0.6775)  triple_100: 9.4919 (9.2884)  triple_80: 9.4711 (9.3610)  triple_60: 8.1719 (8.0563)  triple_40: 5.2482 (5.3891)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:4] Total time: 0:13:45 (0.9773 s / it)\n",
      "Averaged stats: loss: 35.3662 (34.5132)  loss_n_40: 0.4914 (0.5708)  loss_n_60: 0.5287 (0.5535)  loss_n_80: 0.6233 (0.6166)  loss_n_100: 0.7117 (0.6775)  triple_100: 9.4919 (9.2884)  triple_80: 9.4711 (9.3610)  triple_60: 8.1719 (8.0563)  triple_40: 5.2482 (5.3891)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle_2/low2high/epoch_4_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.677%\n",
      "Min loss_n_100: 0.677\n",
      "Best Epoch: 4.000\n",
      "Train: [epoch:5]  [   0/1724]  eta: 3:03:22  lr: 0.000080  loss: 37.0431 (37.0431)  loss_n_40: 0.5609 (0.5609)  loss_n_60: 0.6072 (0.6072)  loss_n_80: 0.6608 (0.6608)  loss_n_100: 0.7166 (0.7166)  triple_100: 9.9144 (9.9144)  triple_80: 9.8929 (9.8929)  triple_60: 8.8636 (8.8636)  triple_40: 5.8267 (5.8267)  time: 6.3820  data: 0.6459  max mem: 40153\n",
      "Train: [epoch:5]  [  10/1724]  eta: 2:49:43  lr: 0.000080  loss: 35.0534 (34.7049)  loss_n_40: 0.4960 (0.5503)  loss_n_60: 0.5081 (0.5423)  loss_n_80: 0.5878 (0.6077)  loss_n_100: 0.7065 (0.6907)  triple_100: 9.2626 (9.2362)  triple_80: 9.3381 (9.3509)  triple_60: 8.1562 (8.1129)  triple_40: 5.5179 (5.6139)  time: 5.9413  data: 0.0589  max mem: 40153\n",
      "Train: [epoch:5]  [  20/1724]  eta: 2:48:07  lr: 0.000080  loss: 36.7949 (38.5216)  loss_n_40: 0.7361 (0.7085)  loss_n_60: 0.7438 (0.7146)  loss_n_80: 0.7621 (0.7838)  loss_n_100: 0.8103 (0.8650)  triple_100: 10.1247 (10.1393)  triple_80: 10.1282 (10.3429)  triple_60: 8.5599 (9.0137)  triple_40: 5.6249 (5.9538)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [  30/1724]  eta: 2:46:56  lr: 0.000080  loss: 41.0216 (38.6653)  loss_n_40: 0.7877 (0.7372)  loss_n_60: 0.8194 (0.7409)  loss_n_80: 0.9236 (0.8101)  loss_n_100: 0.9979 (0.8872)  triple_100: 10.6090 (10.1264)  triple_80: 10.6844 (10.3737)  triple_60: 9.6619 (9.0137)  triple_40: 5.9838 (5.9762)  time: 5.8970  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [  40/1724]  eta: 2:45:50  lr: 0.000080  loss: 39.1334 (39.1467)  loss_n_40: 0.6928 (0.7216)  loss_n_60: 0.7391 (0.7340)  loss_n_80: 0.8265 (0.8099)  loss_n_100: 0.9192 (0.8871)  triple_100: 10.4859 (10.3600)  triple_80: 10.6578 (10.5791)  triple_60: 9.0449 (9.1197)  triple_40: 5.8031 (5.9354)  time: 5.8967  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:5]  [  50/1724]  eta: 2:44:47  lr: 0.000080  loss: 38.0113 (38.7089)  loss_n_40: 0.5910 (0.6989)  loss_n_60: 0.6294 (0.7122)  loss_n_80: 0.7095 (0.7879)  loss_n_100: 0.7961 (0.8656)  triple_100: 10.3342 (10.2840)  triple_80: 10.5036 (10.4693)  triple_60: 8.8725 (9.0226)  triple_40: 5.6971 (5.8683)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [  60/1724]  eta: 2:43:44  lr: 0.000080  loss: 36.3336 (38.4168)  loss_n_40: 0.5548 (0.6744)  loss_n_60: 0.6176 (0.6949)  loss_n_80: 0.7052 (0.7738)  loss_n_100: 0.7894 (0.8543)  triple_100: 10.0130 (10.2604)  triple_80: 10.0491 (10.4215)  triple_60: 8.4793 (8.9378)  triple_40: 5.4942 (5.7996)  time: 5.8957  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [  70/1724]  eta: 2:42:43  lr: 0.000080  loss: 36.1920 (38.1571)  loss_n_40: 0.5678 (0.6658)  loss_n_60: 0.6196 (0.6891)  loss_n_80: 0.7071 (0.7679)  loss_n_100: 0.7947 (0.8466)  triple_100: 9.6816 (10.1934)  triple_80: 9.7583 (10.3593)  triple_60: 8.4045 (8.8846)  triple_40: 5.4003 (5.7504)  time: 5.8949  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [  80/1724]  eta: 2:41:42  lr: 0.000080  loss: 36.5208 (38.1285)  loss_n_40: 0.5684 (0.6590)  loss_n_60: 0.6594 (0.6852)  loss_n_80: 0.7694 (0.7667)  loss_n_100: 0.8361 (0.8447)  triple_100: 9.6816 (10.2132)  triple_80: 9.8493 (10.3773)  triple_60: 8.5077 (8.8720)  triple_40: 5.2073 (5.7104)  time: 5.8950  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [  90/1724]  eta: 2:40:43  lr: 0.000080  loss: 37.8714 (38.0856)  loss_n_40: 0.6158 (0.6597)  loss_n_60: 0.6542 (0.6821)  loss_n_80: 0.7304 (0.7618)  loss_n_100: 0.7981 (0.8379)  triple_100: 10.3640 (10.1978)  triple_80: 10.4230 (10.3646)  triple_60: 8.8462 (8.8715)  triple_40: 5.5294 (5.7101)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 100/1724]  eta: 2:39:42  lr: 0.000080  loss: 36.7629 (37.8265)  loss_n_40: 0.6119 (0.6542)  loss_n_60: 0.6132 (0.6719)  loss_n_80: 0.6745 (0.7507)  loss_n_100: 0.7427 (0.8254)  triple_100: 9.8582 (10.1326)  triple_80: 10.0305 (10.2944)  triple_60: 8.5846 (8.8094)  triple_40: 5.5294 (5.6879)  time: 5.8957  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 110/1724]  eta: 2:38:43  lr: 0.000080  loss: 34.0082 (37.5855)  loss_n_40: 0.6027 (0.6521)  loss_n_60: 0.5845 (0.6651)  loss_n_80: 0.6525 (0.7418)  loss_n_100: 0.7091 (0.8140)  triple_100: 9.2218 (10.0557)  triple_80: 9.3097 (10.2204)  triple_60: 8.1088 (8.7590)  triple_40: 5.4486 (5.6776)  time: 5.8948  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 120/1724]  eta: 2:37:43  lr: 0.000080  loss: 34.0082 (37.3186)  loss_n_40: 0.4775 (0.6420)  loss_n_60: 0.5351 (0.6541)  loss_n_80: 0.6013 (0.7297)  loss_n_100: 0.6691 (0.8005)  triple_100: 9.1521 (9.9830)  triple_80: 9.2243 (10.1448)  triple_60: 8.0547 (8.7009)  triple_40: 5.5142 (5.6634)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 130/1724]  eta: 2:36:44  lr: 0.000080  loss: 35.0481 (37.1653)  loss_n_40: 0.4544 (0.6294)  loss_n_60: 0.5060 (0.6435)  loss_n_80: 0.5697 (0.7188)  loss_n_100: 0.6225 (0.7891)  triple_100: 9.4395 (9.9540)  triple_80: 9.4879 (10.1075)  triple_60: 8.2366 (8.6726)  triple_40: 5.5320 (5.6505)  time: 5.8973  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 140/1724]  eta: 2:35:44  lr: 0.000080  loss: 35.5538 (37.1204)  loss_n_40: 0.4544 (0.6211)  loss_n_60: 0.4986 (0.6383)  loss_n_80: 0.5841 (0.7135)  loss_n_100: 0.6498 (0.7833)  triple_100: 9.5745 (9.9499)  triple_80: 9.7498 (10.1029)  triple_60: 8.3904 (8.6741)  triple_40: 5.3539 (5.6373)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 150/1724]  eta: 2:34:45  lr: 0.000080  loss: 37.1593 (37.0905)  loss_n_40: 0.4663 (0.6172)  loss_n_60: 0.5433 (0.6343)  loss_n_80: 0.6078 (0.7094)  loss_n_100: 0.6636 (0.7791)  triple_100: 9.9279 (9.9477)  triple_80: 10.1030 (10.0971)  triple_60: 8.7061 (8.6686)  triple_40: 5.3539 (5.6371)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 160/1724]  eta: 2:33:46  lr: 0.000080  loss: 34.4113 (36.9429)  loss_n_40: 0.4663 (0.6141)  loss_n_60: 0.5433 (0.6293)  loss_n_80: 0.6024 (0.7035)  loss_n_100: 0.6597 (0.7721)  triple_100: 9.4953 (9.9041)  triple_80: 9.5532 (10.0544)  triple_60: 8.1474 (8.6386)  triple_40: 5.4613 (5.6269)  time: 5.8957  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 170/1724]  eta: 2:32:46  lr: 0.000080  loss: 34.5879 (36.8598)  loss_n_40: 0.4800 (0.6098)  loss_n_60: 0.5288 (0.6252)  loss_n_80: 0.5917 (0.6991)  loss_n_100: 0.6475 (0.7672)  triple_100: 9.4510 (9.8840)  triple_80: 9.5293 (10.0340)  triple_60: 8.1376 (8.6223)  triple_40: 5.3728 (5.6183)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 180/1724]  eta: 2:31:47  lr: 0.000080  loss: 34.5727 (36.7201)  loss_n_40: 0.4577 (0.6016)  loss_n_60: 0.4985 (0.6179)  loss_n_80: 0.5606 (0.6915)  loss_n_100: 0.6288 (0.7592)  triple_100: 9.2395 (9.8515)  triple_80: 9.4460 (9.9982)  triple_60: 8.1376 (8.5928)  triple_40: 5.4251 (5.6074)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 190/1724]  eta: 2:30:48  lr: 0.000080  loss: 34.0159 (36.6242)  loss_n_40: 0.4463 (0.5960)  loss_n_60: 0.4919 (0.6134)  loss_n_80: 0.5606 (0.6862)  loss_n_100: 0.6288 (0.7539)  triple_100: 9.1803 (9.8307)  triple_80: 9.2196 (9.9744)  triple_60: 8.1333 (8.5772)  triple_40: 5.4251 (5.5925)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 200/1724]  eta: 2:29:49  lr: 0.000080  loss: 34.2535 (36.5109)  loss_n_40: 0.4551 (0.5917)  loss_n_60: 0.5191 (0.6100)  loss_n_80: 0.5943 (0.6826)  loss_n_100: 0.6589 (0.7498)  triple_100: 9.2188 (9.8010)  triple_80: 9.2944 (9.9464)  triple_60: 8.1111 (8.5547)  triple_40: 5.2594 (5.5746)  time: 5.8956  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [ 210/1724]  eta: 2:28:50  lr: 0.000080  loss: 33.9156 (36.4030)  loss_n_40: 0.4807 (0.5896)  loss_n_60: 0.5424 (0.6084)  loss_n_80: 0.6111 (0.6805)  loss_n_100: 0.6729 (0.7473)  triple_100: 9.2188 (9.7646)  triple_80: 9.3871 (9.9154)  triple_60: 8.0088 (8.5346)  triple_40: 5.1941 (5.5625)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 220/1724]  eta: 2:27:50  lr: 0.000080  loss: 33.9156 (36.3219)  loss_n_40: 0.5219 (0.5875)  loss_n_60: 0.5589 (0.6067)  loss_n_80: 0.6293 (0.6780)  loss_n_100: 0.6879 (0.7441)  triple_100: 9.3001 (9.7341)  triple_80: 9.4870 (9.8901)  triple_60: 8.0088 (8.5240)  triple_40: 5.2208 (5.5574)  time: 5.8957  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 230/1724]  eta: 2:26:51  lr: 0.000080  loss: 34.1504 (36.2053)  loss_n_40: 0.5076 (0.5862)  loss_n_60: 0.5731 (0.6051)  loss_n_80: 0.6369 (0.6759)  loss_n_100: 0.6879 (0.7415)  triple_100: 9.1597 (9.7003)  triple_80: 9.3076 (9.8596)  triple_60: 8.0480 (8.4983)  triple_40: 5.1524 (5.5385)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 240/1724]  eta: 2:25:52  lr: 0.000080  loss: 33.2556 (36.0782)  loss_n_40: 0.5071 (0.5829)  loss_n_60: 0.5292 (0.6014)  loss_n_80: 0.5808 (0.6721)  loss_n_100: 0.6358 (0.7370)  triple_100: 8.8149 (9.6643)  triple_80: 9.1009 (9.8276)  triple_60: 7.9625 (8.4691)  triple_40: 5.0757 (5.5240)  time: 5.8963  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 250/1724]  eta: 2:24:53  lr: 0.000080  loss: 33.4426 (36.0236)  loss_n_40: 0.4435 (0.5789)  loss_n_60: 0.5140 (0.5982)  loss_n_80: 0.5808 (0.6688)  loss_n_100: 0.6358 (0.7334)  triple_100: 8.9648 (9.6505)  triple_80: 9.2307 (9.8147)  triple_60: 7.9625 (8.4602)  triple_40: 5.1767 (5.5189)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 260/1724]  eta: 2:23:54  lr: 0.000080  loss: 35.1911 (36.0701)  loss_n_40: 0.5162 (0.5778)  loss_n_60: 0.5434 (0.5980)  loss_n_80: 0.6139 (0.6690)  loss_n_100: 0.6708 (0.7341)  triple_100: 9.4306 (9.6678)  triple_80: 9.6781 (9.8315)  triple_60: 8.3161 (8.4714)  triple_40: 5.3535 (5.5207)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 270/1724]  eta: 2:22:55  lr: 0.000080  loss: 33.9774 (36.0142)  loss_n_40: 0.5221 (0.5765)  loss_n_60: 0.5736 (0.5976)  loss_n_80: 0.6551 (0.6687)  loss_n_100: 0.7180 (0.7338)  triple_100: 9.0556 (9.6534)  triple_80: 9.2463 (9.8184)  triple_60: 8.0445 (8.4592)  triple_40: 5.3535 (5.5066)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 280/1724]  eta: 2:21:56  lr: 0.000080  loss: 33.6089 (35.9203)  loss_n_40: 0.4907 (0.5745)  loss_n_60: 0.5505 (0.5960)  loss_n_80: 0.6229 (0.6669)  loss_n_100: 0.6829 (0.7314)  triple_100: 8.9705 (9.6232)  triple_80: 9.1404 (9.7922)  triple_60: 8.0054 (8.4412)  triple_40: 5.0578 (5.4949)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 290/1724]  eta: 2:20:57  lr: 0.000080  loss: 34.7011 (35.9172)  loss_n_40: 0.5018 (0.5747)  loss_n_60: 0.5732 (0.5964)  loss_n_80: 0.6364 (0.6671)  loss_n_100: 0.6806 (0.7315)  triple_100: 8.9963 (9.6231)  triple_80: 9.3922 (9.7931)  triple_60: 8.2878 (8.4409)  triple_40: 5.2191 (5.4904)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 300/1724]  eta: 2:19:58  lr: 0.000080  loss: 36.7657 (35.9600)  loss_n_40: 0.5511 (0.5781)  loss_n_60: 0.6161 (0.5985)  loss_n_80: 0.6879 (0.6692)  loss_n_100: 0.7504 (0.7336)  triple_100: 9.8599 (9.6344)  triple_80: 9.9446 (9.8042)  triple_60: 8.5861 (8.4470)  triple_40: 5.4124 (5.4950)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 310/1724]  eta: 2:18:59  lr: 0.000080  loss: 38.1303 (36.1176)  loss_n_40: 0.7236 (0.5946)  loss_n_60: 0.7793 (0.6208)  loss_n_80: 0.8588 (0.6927)  loss_n_100: 0.9340 (0.7577)  triple_100: 10.2479 (9.6537)  triple_80: 10.4699 (9.8336)  triple_60: 8.7829 (8.4699)  triple_40: 5.5380 (5.4945)  time: 5.8979  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 320/1724]  eta: 2:18:00  lr: 0.000080  loss: 46.5396 (36.7522)  loss_n_40: 1.3154 (0.6191)  loss_n_60: 1.4259 (0.6456)  loss_n_80: 1.5000 (0.7177)  loss_n_100: 1.5396 (0.7830)  triple_100: 11.8621 (9.7944)  triple_80: 11.5723 (9.9868)  triple_60: 10.4919 (8.6131)  triple_40: 6.2629 (5.5923)  time: 5.8981  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 330/1724]  eta: 2:17:01  lr: 0.000080  loss: 55.2858 (37.3015)  loss_n_40: 1.2374 (0.6360)  loss_n_60: 1.2806 (0.6644)  loss_n_80: 1.4429 (0.7366)  loss_n_100: 1.5119 (0.8015)  triple_100: 14.0944 (9.9288)  triple_80: 14.8127 (10.1338)  triple_60: 12.8130 (8.7444)  triple_40: 7.8088 (5.6561)  time: 5.8986  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 340/1724]  eta: 2:16:02  lr: 0.000080  loss: 52.7450 (37.7266)  loss_n_40: 1.0907 (0.6483)  loss_n_60: 1.0753 (0.6755)  loss_n_80: 1.1671 (0.7474)  loss_n_100: 1.2319 (0.8115)  triple_100: 13.6633 (10.0265)  triple_80: 14.0719 (10.2422)  triple_60: 12.5409 (8.8497)  triple_40: 7.7873 (5.7256)  time: 5.8985  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 350/1724]  eta: 2:15:03  lr: 0.000080  loss: 46.3083 (37.9328)  loss_n_40: 0.8995 (0.6530)  loss_n_60: 0.8952 (0.6796)  loss_n_80: 0.9814 (0.7517)  loss_n_100: 1.0424 (0.8157)  triple_100: 12.5488 (10.0802)  triple_80: 12.8000 (10.2983)  triple_60: 11.0729 (8.8979)  triple_40: 7.4470 (5.7564)  time: 5.8982  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 360/1724]  eta: 2:14:04  lr: 0.000080  loss: 41.7538 (38.0081)  loss_n_40: 0.7594 (0.6564)  loss_n_60: 0.7668 (0.6814)  loss_n_80: 0.8601 (0.7538)  loss_n_100: 0.9374 (0.8178)  triple_100: 11.1764 (10.1005)  triple_80: 11.3995 (10.3200)  triple_60: 9.8048 (8.9119)  triple_40: 6.3683 (5.7662)  time: 5.8997  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 370/1724]  eta: 2:13:05  lr: 0.000080  loss: 40.4360 (38.0764)  loss_n_40: 0.7712 (0.6611)  loss_n_60: 0.7414 (0.6847)  loss_n_80: 0.8419 (0.7572)  loss_n_100: 0.8948 (0.8210)  triple_100: 10.6826 (10.1176)  triple_80: 10.8926 (10.3394)  triple_60: 9.3230 (8.9261)  triple_40: 5.9459 (5.7692)  time: 5.9002  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 380/1724]  eta: 2:12:07  lr: 0.000080  loss: 40.2565 (38.1126)  loss_n_40: 0.7628 (0.6634)  loss_n_60: 0.7473 (0.6859)  loss_n_80: 0.8418 (0.7581)  loss_n_100: 0.8948 (0.8219)  triple_100: 10.5818 (10.1262)  triple_80: 10.8500 (10.3492)  triple_60: 9.3201 (8.9360)  triple_40: 5.7610 (5.7720)  time: 5.9002  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 390/1724]  eta: 2:11:08  lr: 0.000080  loss: 39.1208 (38.1764)  loss_n_40: 0.6284 (0.6630)  loss_n_60: 0.6708 (0.6857)  loss_n_80: 0.7239 (0.7580)  loss_n_100: 0.7900 (0.8220)  triple_100: 10.5818 (10.1478)  triple_80: 10.6229 (10.3686)  triple_60: 9.1710 (8.9517)  triple_40: 5.8754 (5.7796)  time: 5.9000  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 400/1724]  eta: 2:10:09  lr: 0.000080  loss: 38.8050 (38.1406)  loss_n_40: 0.6284 (0.6633)  loss_n_60: 0.6479 (0.6846)  loss_n_80: 0.7173 (0.7568)  loss_n_100: 0.7692 (0.8208)  triple_100: 10.5320 (10.1391)  triple_80: 10.5942 (10.3588)  triple_60: 9.1622 (8.9424)  triple_40: 5.7544 (5.7748)  time: 5.8996  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 410/1724]  eta: 2:09:10  lr: 0.000080  loss: 37.6026 (38.1287)  loss_n_40: 0.6603 (0.6650)  loss_n_60: 0.6688 (0.6858)  loss_n_80: 0.7485 (0.7581)  loss_n_100: 0.7968 (0.8220)  triple_100: 9.9727 (10.1359)  triple_80: 10.2090 (10.3587)  triple_60: 8.7447 (8.9368)  triple_40: 5.4193 (5.7666)  time: 5.8990  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 420/1724]  eta: 2:08:11  lr: 0.000080  loss: 37.2612 (38.1118)  loss_n_40: 0.6836 (0.6671)  loss_n_60: 0.7076 (0.6857)  loss_n_80: 0.7596 (0.7576)  loss_n_100: 0.7968 (0.8212)  triple_100: 9.6011 (10.1239)  triple_80: 10.0167 (10.3499)  triple_60: 8.7531 (8.9322)  triple_40: 5.5913 (5.7742)  time: 5.8983  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 430/1724]  eta: 2:07:12  lr: 0.000080  loss: 37.5371 (38.1230)  loss_n_40: 0.7496 (0.6685)  loss_n_60: 0.7043 (0.6863)  loss_n_80: 0.7596 (0.7585)  loss_n_100: 0.8089 (0.8218)  triple_100: 9.6011 (10.1239)  triple_80: 10.0293 (10.3520)  triple_60: 8.7531 (8.9342)  triple_40: 6.0210 (5.7778)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 440/1724]  eta: 2:06:13  lr: 0.000080  loss: 38.9795 (38.1368)  loss_n_40: 0.6668 (0.6704)  loss_n_60: 0.6810 (0.6867)  loss_n_80: 0.7414 (0.7584)  loss_n_100: 0.8089 (0.8215)  triple_100: 10.2616 (10.1250)  triple_80: 10.5296 (10.3540)  triple_60: 9.1139 (8.9371)  triple_40: 5.9520 (5.7837)  time: 5.8973  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [ 450/1724]  eta: 2:05:14  lr: 0.000080  loss: 39.1739 (38.1520)  loss_n_40: 0.6668 (0.6721)  loss_n_60: 0.6571 (0.6869)  loss_n_80: 0.7391 (0.7589)  loss_n_100: 0.7972 (0.8221)  triple_100: 10.3508 (10.1299)  triple_80: 10.5296 (10.3574)  triple_60: 9.1139 (8.9360)  triple_40: 5.9743 (5.7889)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 460/1724]  eta: 2:04:15  lr: 0.000080  loss: 38.2286 (38.1683)  loss_n_40: 0.6467 (0.6714)  loss_n_60: 0.6479 (0.6857)  loss_n_80: 0.7128 (0.7574)  loss_n_100: 0.7814 (0.8205)  triple_100: 10.2159 (10.1315)  triple_80: 10.2797 (10.3597)  triple_60: 8.9406 (8.9409)  triple_40: 6.0904 (5.8013)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 470/1724]  eta: 2:03:16  lr: 0.000080  loss: 37.6035 (38.1460)  loss_n_40: 0.6220 (0.6716)  loss_n_60: 0.6023 (0.6846)  loss_n_80: 0.6577 (0.7560)  loss_n_100: 0.7110 (0.8189)  triple_100: 9.8988 (10.1245)  triple_80: 10.1550 (10.3523)  triple_60: 8.9406 (8.9357)  triple_40: 5.9601 (5.8024)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 480/1724]  eta: 2:02:17  lr: 0.000080  loss: 37.7059 (38.1517)  loss_n_40: 0.5852 (0.6713)  loss_n_60: 0.6023 (0.6838)  loss_n_80: 0.6577 (0.7550)  loss_n_100: 0.7225 (0.8178)  triple_100: 10.0559 (10.1264)  triple_80: 10.2383 (10.3535)  triple_60: 8.8620 (8.9383)  triple_40: 5.9132 (5.8057)  time: 5.8949  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 490/1724]  eta: 2:01:18  lr: 0.000080  loss: 38.0974 (38.1645)  loss_n_40: 0.6165 (0.6721)  loss_n_60: 0.6201 (0.6837)  loss_n_80: 0.6941 (0.7549)  loss_n_100: 0.7704 (0.8179)  triple_100: 10.1762 (10.1338)  triple_80: 10.3143 (10.3589)  triple_60: 8.7745 (8.9391)  triple_40: 5.8702 (5.8042)  time: 5.8955  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 500/1724]  eta: 2:00:18  lr: 0.000080  loss: 37.4671 (38.1322)  loss_n_40: 0.6088 (0.6709)  loss_n_60: 0.6201 (0.6821)  loss_n_80: 0.6892 (0.7531)  loss_n_100: 0.7687 (0.8163)  triple_100: 10.1207 (10.1266)  triple_80: 10.2844 (10.3501)  triple_60: 8.7267 (8.9316)  triple_40: 5.7463 (5.8014)  time: 5.8958  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 510/1724]  eta: 1:59:19  lr: 0.000080  loss: 35.5588 (38.0907)  loss_n_40: 0.5679 (0.6693)  loss_n_60: 0.5683 (0.6798)  loss_n_80: 0.6312 (0.7505)  loss_n_100: 0.6920 (0.8135)  triple_100: 9.5745 (10.1142)  triple_80: 9.6976 (10.3369)  triple_60: 8.5152 (8.9242)  triple_40: 5.7165 (5.8021)  time: 5.8949  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 520/1724]  eta: 1:58:20  lr: 0.000080  loss: 35.0999 (38.0390)  loss_n_40: 0.5412 (0.6671)  loss_n_60: 0.5523 (0.6773)  loss_n_80: 0.6086 (0.7479)  loss_n_100: 0.6644 (0.8108)  triple_100: 9.5609 (10.1027)  triple_80: 9.6109 (10.3234)  triple_60: 8.1809 (8.9127)  triple_40: 5.5724 (5.7971)  time: 5.8944  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 530/1724]  eta: 1:57:21  lr: 0.000080  loss: 35.0999 (37.9764)  loss_n_40: 0.5305 (0.6656)  loss_n_60: 0.5423 (0.6750)  loss_n_80: 0.6038 (0.7453)  loss_n_100: 0.6638 (0.8079)  triple_100: 9.4452 (10.0854)  triple_80: 9.4825 (10.3048)  triple_60: 8.0991 (8.8996)  triple_40: 5.5641 (5.7929)  time: 5.8942  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 540/1724]  eta: 1:56:22  lr: 0.000080  loss: 36.6102 (37.9538)  loss_n_40: 0.5440 (0.6637)  loss_n_60: 0.5801 (0.6734)  loss_n_80: 0.6599 (0.7437)  loss_n_100: 0.7067 (0.8063)  triple_100: 9.9385 (10.0825)  triple_80: 10.1003 (10.3011)  triple_60: 8.6698 (8.8967)  triple_40: 5.5641 (5.7864)  time: 5.8945  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 550/1724]  eta: 1:55:23  lr: 0.000080  loss: 35.5472 (37.9033)  loss_n_40: 0.5143 (0.6607)  loss_n_60: 0.5436 (0.6706)  loss_n_80: 0.6359 (0.7407)  loss_n_100: 0.6963 (0.8031)  triple_100: 9.6078 (10.0704)  triple_80: 9.7339 (10.2875)  triple_60: 8.5274 (8.8869)  triple_40: 5.5813 (5.7834)  time: 5.8944  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 560/1724]  eta: 1:54:24  lr: 0.000080  loss: 35.5472 (37.8803)  loss_n_40: 0.4965 (0.6581)  loss_n_60: 0.5203 (0.6685)  loss_n_80: 0.5950 (0.7386)  loss_n_100: 0.6590 (0.8011)  triple_100: 9.6078 (10.0677)  triple_80: 9.8125 (10.2830)  triple_60: 8.4707 (8.8832)  triple_40: 5.5813 (5.7801)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 570/1724]  eta: 1:53:25  lr: 0.000080  loss: 35.3203 (37.8153)  loss_n_40: 0.5119 (0.6563)  loss_n_60: 0.5211 (0.6660)  loss_n_80: 0.6010 (0.7359)  loss_n_100: 0.6665 (0.7982)  triple_100: 9.6034 (10.0511)  triple_80: 9.7229 (10.2645)  triple_60: 8.4405 (8.8690)  triple_40: 5.3220 (5.7744)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 580/1724]  eta: 1:52:26  lr: 0.000080  loss: 33.6194 (37.7559)  loss_n_40: 0.5230 (0.6539)  loss_n_60: 0.5211 (0.6635)  loss_n_80: 0.5921 (0.7332)  loss_n_100: 0.6398 (0.7954)  triple_100: 8.7800 (10.0373)  triple_80: 9.0785 (10.2485)  triple_60: 7.9832 (8.8554)  triple_40: 5.3220 (5.7688)  time: 5.8952  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 590/1724]  eta: 1:51:27  lr: 0.000080  loss: 34.3793 (37.7037)  loss_n_40: 0.4830 (0.6516)  loss_n_60: 0.5254 (0.6612)  loss_n_80: 0.5756 (0.7306)  loss_n_100: 0.6070 (0.7927)  triple_100: 9.2383 (10.0250)  triple_80: 9.2956 (10.2346)  triple_60: 8.1025 (8.8450)  triple_40: 5.3801 (5.7630)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 600/1724]  eta: 1:50:28  lr: 0.000080  loss: 34.7588 (37.6571)  loss_n_40: 0.4675 (0.6487)  loss_n_60: 0.5026 (0.6585)  loss_n_80: 0.5687 (0.7278)  loss_n_100: 0.6212 (0.7897)  triple_100: 9.4098 (10.0134)  triple_80: 9.4600 (10.2215)  triple_60: 8.2129 (8.8357)  triple_40: 5.5003 (5.7619)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 610/1724]  eta: 1:49:29  lr: 0.000080  loss: 35.1762 (37.6128)  loss_n_40: 0.4691 (0.6473)  loss_n_60: 0.5097 (0.6565)  loss_n_80: 0.5760 (0.7257)  loss_n_100: 0.6316 (0.7874)  triple_100: 9.3431 (10.0014)  triple_80: 9.4842 (10.2091)  triple_60: 8.3137 (8.8258)  triple_40: 5.6175 (5.7595)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 620/1724]  eta: 1:48:30  lr: 0.000080  loss: 35.4456 (37.5806)  loss_n_40: 0.4893 (0.6446)  loss_n_60: 0.5213 (0.6543)  loss_n_80: 0.5765 (0.7234)  loss_n_100: 0.6368 (0.7851)  triple_100: 9.2028 (9.9945)  triple_80: 9.4842 (10.2010)  triple_60: 8.4301 (8.8199)  triple_40: 5.6486 (5.7577)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 630/1724]  eta: 1:47:31  lr: 0.000080  loss: 34.6025 (37.5466)  loss_n_40: 0.4831 (0.6423)  loss_n_60: 0.5059 (0.6523)  loss_n_80: 0.5675 (0.7211)  loss_n_100: 0.6153 (0.7828)  triple_100: 9.2889 (9.9855)  triple_80: 9.3767 (10.1917)  triple_60: 8.2872 (8.8142)  triple_40: 5.6486 (5.7566)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 640/1724]  eta: 1:46:32  lr: 0.000080  loss: 35.0664 (37.5285)  loss_n_40: 0.4944 (0.6416)  loss_n_60: 0.5270 (0.6514)  loss_n_80: 0.5937 (0.7201)  loss_n_100: 0.6329 (0.7816)  triple_100: 9.5025 (9.9810)  triple_80: 9.6587 (10.1869)  triple_60: 8.4055 (8.8110)  triple_40: 5.5995 (5.7549)  time: 5.8952  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 650/1724]  eta: 1:45:33  lr: 0.000080  loss: 33.2985 (37.4614)  loss_n_40: 0.4789 (0.6402)  loss_n_60: 0.5229 (0.6497)  loss_n_80: 0.6031 (0.7184)  loss_n_100: 0.6588 (0.7798)  triple_100: 9.0703 (9.9651)  triple_80: 9.1662 (10.1701)  triple_60: 7.8954 (8.7950)  triple_40: 5.1859 (5.7430)  time: 5.8947  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 660/1724]  eta: 1:44:34  lr: 0.000080  loss: 32.5234 (37.3897)  loss_n_40: 0.4673 (0.6386)  loss_n_60: 0.5460 (0.6484)  loss_n_80: 0.6241 (0.7174)  loss_n_100: 0.7009 (0.7790)  triple_100: 8.8112 (9.9489)  triple_80: 8.9161 (10.1529)  triple_60: 7.7421 (8.7776)  triple_40: 4.7655 (5.7271)  time: 5.8947  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:5]  [ 670/1724]  eta: 1:43:35  lr: 0.000080  loss: 32.7007 (37.3459)  loss_n_40: 0.5106 (0.6372)  loss_n_60: 0.5488 (0.6473)  loss_n_80: 0.6419 (0.7164)  loss_n_100: 0.7134 (0.7780)  triple_100: 8.8817 (9.9389)  triple_80: 9.0586 (10.1424)  triple_60: 7.7750 (8.7678)  triple_40: 4.7655 (5.7180)  time: 5.8951  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 680/1724]  eta: 1:42:36  lr: 0.000080  loss: 32.7007 (37.2793)  loss_n_40: 0.5307 (0.6362)  loss_n_60: 0.5410 (0.6458)  loss_n_80: 0.6015 (0.7147)  loss_n_100: 0.6708 (0.7762)  triple_100: 8.8021 (9.9217)  triple_80: 9.0586 (10.1247)  triple_60: 7.7750 (8.7519)  triple_40: 5.0418 (5.7082)  time: 5.8962  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [ 690/1724]  eta: 1:41:37  lr: 0.000080  loss: 30.9275 (37.1754)  loss_n_40: 0.5307 (0.6357)  loss_n_60: 0.5221 (0.6439)  loss_n_80: 0.5643 (0.7124)  loss_n_100: 0.6056 (0.7737)  triple_100: 8.2050 (9.8911)  triple_80: 8.2568 (10.0939)  triple_60: 7.1856 (8.7273)  triple_40: 5.0075 (5.6975)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 700/1724]  eta: 1:40:38  lr: 0.000080  loss: 30.6155 (37.1037)  loss_n_40: 0.5341 (0.6352)  loss_n_60: 0.5201 (0.6426)  loss_n_80: 0.5647 (0.7108)  loss_n_100: 0.6084 (0.7719)  triple_100: 8.2050 (9.8719)  triple_80: 8.2891 (10.0742)  triple_60: 7.1849 (8.7097)  triple_40: 4.9574 (5.6873)  time: 5.8943  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 710/1724]  eta: 1:39:39  lr: 0.000080  loss: 34.7959 (37.0901)  loss_n_40: 0.5175 (0.6337)  loss_n_60: 0.5391 (0.6414)  loss_n_80: 0.6069 (0.7095)  loss_n_100: 0.6563 (0.7707)  triple_100: 9.1746 (9.8691)  triple_80: 9.3396 (10.0710)  triple_60: 8.1995 (8.7084)  triple_40: 5.1452 (5.6863)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 720/1724]  eta: 1:38:40  lr: 0.000080  loss: 34.8093 (37.0510)  loss_n_40: 0.4760 (0.6316)  loss_n_60: 0.5158 (0.6396)  loss_n_80: 0.5796 (0.7075)  loss_n_100: 0.6479 (0.7685)  triple_100: 9.3017 (9.8594)  triple_80: 9.4432 (10.0605)  triple_60: 8.3604 (8.7022)  triple_40: 5.4199 (5.6818)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 730/1724]  eta: 1:37:41  lr: 0.000080  loss: 33.0297 (37.0015)  loss_n_40: 0.4510 (0.6295)  loss_n_60: 0.4837 (0.6376)  loss_n_80: 0.5278 (0.7052)  loss_n_100: 0.5794 (0.7661)  triple_100: 8.7060 (9.8452)  triple_80: 8.9393 (10.0466)  triple_60: 7.9294 (8.6924)  triple_40: 5.4199 (5.6790)  time: 5.8980  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 740/1724]  eta: 1:36:42  lr: 0.000080  loss: 32.7495 (36.9483)  loss_n_40: 0.4337 (0.6275)  loss_n_60: 0.4781 (0.6354)  loss_n_80: 0.5278 (0.7030)  loss_n_100: 0.5794 (0.7637)  triple_100: 8.7037 (9.8319)  triple_80: 8.8162 (10.0327)  triple_60: 7.7941 (8.6808)  triple_40: 5.2852 (5.6732)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 750/1724]  eta: 1:35:43  lr: 0.000080  loss: 33.7686 (36.9220)  loss_n_40: 0.4596 (0.6266)  loss_n_60: 0.4947 (0.6342)  loss_n_80: 0.5612 (0.7017)  loss_n_100: 0.6103 (0.7624)  triple_100: 9.1601 (9.8261)  triple_80: 9.2755 (10.0260)  triple_60: 7.9933 (8.6754)  triple_40: 5.2253 (5.6697)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 760/1724]  eta: 1:34:44  lr: 0.000080  loss: 33.9669 (36.8790)  loss_n_40: 0.4439 (0.6239)  loss_n_60: 0.4947 (0.6321)  loss_n_80: 0.5596 (0.6995)  loss_n_100: 0.6129 (0.7601)  triple_100: 9.2530 (9.8168)  triple_80: 9.3133 (10.0152)  triple_60: 8.0550 (8.6675)  triple_40: 5.2408 (5.6640)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 770/1724]  eta: 1:33:45  lr: 0.000080  loss: 33.4488 (36.8166)  loss_n_40: 0.4443 (0.6222)  loss_n_60: 0.4802 (0.6301)  loss_n_80: 0.5449 (0.6974)  loss_n_100: 0.6036 (0.7579)  triple_100: 9.0148 (9.8009)  triple_80: 9.0884 (9.9982)  triple_60: 7.9136 (8.6535)  triple_40: 5.1089 (5.6564)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 780/1724]  eta: 1:32:46  lr: 0.000080  loss: 31.1325 (36.7452)  loss_n_40: 0.4519 (0.6201)  loss_n_60: 0.4633 (0.6281)  loss_n_80: 0.5307 (0.6952)  loss_n_100: 0.5799 (0.7557)  triple_100: 8.2987 (9.7825)  triple_80: 8.3965 (9.9788)  triple_60: 7.4278 (8.6374)  triple_40: 5.0400 (5.6474)  time: 5.8950  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 790/1724]  eta: 1:31:47  lr: 0.000080  loss: 31.1537 (36.6972)  loss_n_40: 0.4365 (0.6184)  loss_n_60: 0.4799 (0.6268)  loss_n_80: 0.5334 (0.6937)  loss_n_100: 0.5929 (0.7542)  triple_100: 8.4241 (9.7699)  triple_80: 8.4826 (9.9660)  triple_60: 7.4264 (8.6279)  triple_40: 4.9127 (5.6403)  time: 5.8948  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 800/1724]  eta: 1:30:48  lr: 0.000080  loss: 31.6654 (36.6303)  loss_n_40: 0.4618 (0.6170)  loss_n_60: 0.4858 (0.6253)  loss_n_80: 0.5510 (0.6921)  loss_n_100: 0.6114 (0.7526)  triple_100: 8.4699 (9.7520)  triple_80: 8.6656 (9.9478)  triple_60: 7.4748 (8.6123)  triple_40: 4.8616 (5.6312)  time: 5.8953  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 810/1724]  eta: 1:29:49  lr: 0.000080  loss: 32.0860 (36.5682)  loss_n_40: 0.4454 (0.6152)  loss_n_60: 0.4721 (0.6237)  loss_n_80: 0.5454 (0.6904)  loss_n_100: 0.5938 (0.7507)  triple_100: 8.4388 (9.7354)  triple_80: 8.6532 (9.9312)  triple_60: 7.5817 (8.5995)  triple_40: 4.7800 (5.6221)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 820/1724]  eta: 1:28:50  lr: 0.000080  loss: 32.1288 (36.5187)  loss_n_40: 0.4237 (0.6131)  loss_n_60: 0.4721 (0.6220)  loss_n_80: 0.5346 (0.6886)  loss_n_100: 0.5867 (0.7489)  triple_100: 8.5756 (9.7232)  triple_80: 8.8219 (9.9184)  triple_60: 7.6345 (8.5895)  triple_40: 4.7726 (5.6149)  time: 5.8944  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 830/1724]  eta: 1:27:51  lr: 0.000080  loss: 32.1504 (36.4820)  loss_n_40: 0.4517 (0.6122)  loss_n_60: 0.4832 (0.6209)  loss_n_80: 0.5443 (0.6875)  loss_n_100: 0.5981 (0.7479)  triple_100: 8.5726 (9.7146)  triple_80: 8.7580 (9.9085)  triple_60: 7.6506 (8.5811)  triple_40: 4.8965 (5.6092)  time: 5.8940  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 840/1724]  eta: 1:26:52  lr: 0.000080  loss: 32.9258 (36.4578)  loss_n_40: 0.5206 (0.6113)  loss_n_60: 0.5333 (0.6200)  loss_n_80: 0.5791 (0.6866)  loss_n_100: 0.6580 (0.7470)  triple_100: 8.8626 (9.7088)  triple_80: 8.9337 (9.9021)  triple_60: 7.8189 (8.5767)  triple_40: 5.0255 (5.6054)  time: 5.8948  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 850/1724]  eta: 1:25:53  lr: 0.000080  loss: 34.6402 (36.4350)  loss_n_40: 0.4880 (0.6102)  loss_n_60: 0.5333 (0.6188)  loss_n_80: 0.5769 (0.6851)  loss_n_100: 0.6545 (0.7454)  triple_100: 9.1386 (9.7016)  triple_80: 9.5055 (9.8953)  triple_60: 8.2377 (8.5727)  triple_40: 5.3717 (5.6058)  time: 5.8952  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 860/1724]  eta: 1:24:54  lr: 0.000080  loss: 35.2873 (36.4397)  loss_n_40: 0.4863 (0.6091)  loss_n_60: 0.5312 (0.6184)  loss_n_80: 0.5769 (0.6849)  loss_n_100: 0.6448 (0.7452)  triple_100: 9.6897 (9.7056)  triple_80: 9.6753 (9.8985)  triple_60: 8.3383 (8.5750)  triple_40: 5.4421 (5.6031)  time: 5.8946  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 870/1724]  eta: 1:23:55  lr: 0.000080  loss: 35.7048 (36.4189)  loss_n_40: 0.4859 (0.6078)  loss_n_60: 0.5170 (0.6173)  loss_n_80: 0.6002 (0.6838)  loss_n_100: 0.6757 (0.7442)  triple_100: 9.7177 (9.7026)  triple_80: 9.7306 (9.8944)  triple_60: 8.3729 (8.5698)  triple_40: 5.2596 (5.5991)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 880/1724]  eta: 1:22:56  lr: 0.000080  loss: 34.3316 (36.3927)  loss_n_40: 0.4866 (0.6066)  loss_n_60: 0.5074 (0.6162)  loss_n_80: 0.5910 (0.6827)  loss_n_100: 0.6429 (0.7430)  triple_100: 9.2708 (9.6966)  triple_80: 9.4506 (9.8878)  triple_60: 8.0276 (8.5645)  triple_40: 5.1872 (5.5953)  time: 5.8973  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 890/1724]  eta: 1:21:57  lr: 0.000080  loss: 33.0974 (36.3557)  loss_n_40: 0.4897 (0.6053)  loss_n_60: 0.4901 (0.6147)  loss_n_80: 0.5610 (0.6810)  loss_n_100: 0.6146 (0.7413)  triple_100: 8.8873 (9.6866)  triple_80: 9.0008 (9.8775)  triple_60: 7.8576 (8.5564)  triple_40: 5.2569 (5.5928)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 900/1724]  eta: 1:20:59  lr: 0.000080  loss: 32.5009 (36.2997)  loss_n_40: 0.4539 (0.6041)  loss_n_60: 0.4615 (0.6131)  loss_n_80: 0.5184 (0.6793)  loss_n_100: 0.5924 (0.7395)  triple_100: 8.7560 (9.6730)  triple_80: 8.8919 (9.8626)  triple_60: 7.7122 (8.5431)  triple_40: 5.0436 (5.5848)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 910/1724]  eta: 1:20:00  lr: 0.000080  loss: 33.1123 (36.2839)  loss_n_40: 0.4421 (0.6026)  loss_n_60: 0.4644 (0.6120)  loss_n_80: 0.5316 (0.6781)  loss_n_100: 0.6055 (0.7384)  triple_100: 9.1559 (9.6708)  triple_80: 9.1378 (9.8589)  triple_60: 7.7677 (8.5401)  triple_40: 5.1524 (5.5830)  time: 5.8974  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 920/1724]  eta: 1:19:01  lr: 0.000080  loss: 33.9957 (36.2504)  loss_n_40: 0.4588 (0.6013)  loss_n_60: 0.5029 (0.6108)  loss_n_80: 0.5664 (0.6768)  loss_n_100: 0.6263 (0.7371)  triple_100: 9.3857 (9.6632)  triple_80: 9.3667 (9.8500)  triple_60: 8.1545 (8.5334)  triple_40: 5.1524 (5.5779)  time: 5.8984  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [ 930/1724]  eta: 1:18:02  lr: 0.000080  loss: 33.6419 (36.2162)  loss_n_40: 0.4485 (0.5996)  loss_n_60: 0.4895 (0.6093)  loss_n_80: 0.5519 (0.6753)  loss_n_100: 0.6192 (0.7355)  triple_100: 9.3196 (9.6547)  triple_80: 9.2580 (9.8412)  triple_60: 7.9912 (8.5263)  triple_40: 5.0478 (5.5743)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 940/1724]  eta: 1:17:03  lr: 0.000080  loss: 33.5343 (36.1950)  loss_n_40: 0.4437 (0.5980)  loss_n_60: 0.4561 (0.6079)  loss_n_80: 0.5168 (0.6738)  loss_n_100: 0.5741 (0.7339)  triple_100: 9.0018 (9.6493)  triple_80: 9.0544 (9.8351)  triple_60: 7.9289 (8.5230)  triple_40: 5.1662 (5.5740)  time: 5.8974  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 950/1724]  eta: 1:16:04  lr: 0.000080  loss: 33.5200 (36.1656)  loss_n_40: 0.4132 (0.5963)  loss_n_60: 0.4525 (0.6065)  loss_n_80: 0.5161 (0.6722)  loss_n_100: 0.5661 (0.7323)  triple_100: 8.9442 (9.6417)  triple_80: 9.0053 (9.8270)  triple_60: 7.9587 (8.5173)  triple_40: 5.5009 (5.5722)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 960/1724]  eta: 1:15:05  lr: 0.000080  loss: 33.0528 (36.1418)  loss_n_40: 0.4405 (0.5953)  loss_n_60: 0.4749 (0.6054)  loss_n_80: 0.5313 (0.6710)  loss_n_100: 0.5796 (0.7309)  triple_100: 8.8273 (9.6346)  triple_80: 8.9568 (9.8202)  triple_60: 7.9587 (8.5130)  triple_40: 5.4660 (5.5715)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 970/1724]  eta: 1:14:06  lr: 0.000080  loss: 32.8211 (36.1073)  loss_n_40: 0.4696 (0.5941)  loss_n_60: 0.4784 (0.6043)  loss_n_80: 0.5313 (0.6697)  loss_n_100: 0.5862 (0.7295)  triple_100: 8.6811 (9.6246)  triple_80: 8.8355 (9.8103)  triple_60: 7.8520 (8.5070)  triple_40: 5.3513 (5.5678)  time: 5.8980  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 980/1724]  eta: 1:13:07  lr: 0.000080  loss: 32.7065 (36.0850)  loss_n_40: 0.4429 (0.5927)  loss_n_60: 0.5070 (0.6033)  loss_n_80: 0.5472 (0.6687)  loss_n_100: 0.6018 (0.7285)  triple_100: 8.6350 (9.6188)  triple_80: 8.7819 (9.8046)  triple_60: 7.8490 (8.5037)  triple_40: 5.1086 (5.5647)  time: 5.8988  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [ 990/1724]  eta: 1:12:08  lr: 0.000080  loss: 32.8023 (36.0544)  loss_n_40: 0.4463 (0.5915)  loss_n_60: 0.4826 (0.6021)  loss_n_80: 0.5593 (0.6674)  loss_n_100: 0.6196 (0.7271)  triple_100: 8.6350 (9.6107)  triple_80: 8.8370 (9.7964)  triple_60: 7.8892 (8.4971)  triple_40: 5.1834 (5.5621)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1000/1724]  eta: 1:11:09  lr: 0.000080  loss: 33.3570 (36.0356)  loss_n_40: 0.4425 (0.5903)  loss_n_60: 0.4780 (0.6011)  loss_n_80: 0.5425 (0.6663)  loss_n_100: 0.5917 (0.7260)  triple_100: 9.0427 (9.6074)  triple_80: 9.1134 (9.7922)  triple_60: 7.9631 (8.4936)  triple_40: 5.2468 (5.5587)  time: 5.9001  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1010/1724]  eta: 1:10:10  lr: 0.000080  loss: 33.3570 (36.0008)  loss_n_40: 0.4314 (0.5888)  loss_n_60: 0.4667 (0.5997)  loss_n_80: 0.5362 (0.6649)  loss_n_100: 0.5873 (0.7245)  triple_100: 8.9972 (9.5990)  triple_80: 9.0844 (9.7827)  triple_60: 8.0461 (8.4862)  triple_40: 5.2160 (5.5549)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1020/1724]  eta: 1:09:11  lr: 0.000080  loss: 33.3468 (35.9906)  loss_n_40: 0.4334 (0.5881)  loss_n_60: 0.4906 (0.5989)  loss_n_80: 0.5401 (0.6640)  loss_n_100: 0.6074 (0.7237)  triple_100: 8.8715 (9.5963)  triple_80: 9.0718 (9.7796)  triple_60: 8.0013 (8.4848)  triple_40: 5.3241 (5.5553)  time: 5.8985  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1030/1724]  eta: 1:08:12  lr: 0.000080  loss: 34.5720 (35.9780)  loss_n_40: 0.4832 (0.5871)  loss_n_60: 0.5271 (0.5983)  loss_n_80: 0.5689 (0.6634)  loss_n_100: 0.6227 (0.7230)  triple_100: 9.2272 (9.5937)  triple_80: 9.3217 (9.7769)  triple_60: 8.1280 (8.4824)  triple_40: 5.4011 (5.5532)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1040/1724]  eta: 1:07:13  lr: 0.000080  loss: 34.3247 (35.9561)  loss_n_40: 0.4766 (0.5862)  loss_n_60: 0.5041 (0.5974)  loss_n_80: 0.5715 (0.6624)  loss_n_100: 0.6296 (0.7220)  triple_100: 9.1709 (9.5878)  triple_80: 9.4215 (9.7708)  triple_60: 8.1280 (8.4776)  triple_40: 5.3604 (5.5519)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1050/1724]  eta: 1:06:14  lr: 0.000080  loss: 33.0369 (35.9344)  loss_n_40: 0.4766 (0.5853)  loss_n_60: 0.4960 (0.5965)  loss_n_80: 0.5639 (0.6616)  loss_n_100: 0.6155 (0.7211)  triple_100: 8.9420 (9.5833)  triple_80: 9.1660 (9.7659)  triple_60: 7.8134 (8.4725)  triple_40: 5.2079 (5.5482)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1060/1724]  eta: 1:05:15  lr: 0.000080  loss: 32.7560 (35.8943)  loss_n_40: 0.4251 (0.5837)  loss_n_60: 0.4685 (0.5951)  loss_n_80: 0.5441 (0.6601)  loss_n_100: 0.5879 (0.7196)  triple_100: 8.8583 (9.5740)  triple_80: 8.9807 (9.7558)  triple_60: 7.7180 (8.4638)  triple_40: 5.0182 (5.5422)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1070/1724]  eta: 1:04:16  lr: 0.000080  loss: 31.7214 (35.8558)  loss_n_40: 0.4113 (0.5825)  loss_n_60: 0.4447 (0.5939)  loss_n_80: 0.5044 (0.6589)  loss_n_100: 0.5590 (0.7184)  triple_100: 8.6685 (9.5654)  triple_80: 8.7051 (9.7460)  triple_60: 7.5152 (8.4545)  triple_40: 4.8897 (5.5361)  time: 5.8975  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1080/1724]  eta: 1:03:17  lr: 0.000080  loss: 32.2528 (35.8393)  loss_n_40: 0.4419 (0.5814)  loss_n_60: 0.4549 (0.5930)  loss_n_80: 0.5279 (0.6581)  loss_n_100: 0.5916 (0.7176)  triple_100: 8.8357 (9.5617)  triple_80: 8.8736 (9.7419)  triple_60: 7.6238 (8.4515)  triple_40: 4.9701 (5.5340)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1090/1724]  eta: 1:02:18  lr: 0.000080  loss: 32.8260 (35.8207)  loss_n_40: 0.4356 (0.5802)  loss_n_60: 0.4695 (0.5922)  loss_n_80: 0.5407 (0.6573)  loss_n_100: 0.5948 (0.7167)  triple_100: 8.9374 (9.5576)  triple_80: 8.9978 (9.7377)  triple_60: 7.8373 (8.4477)  triple_40: 5.1033 (5.5314)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1100/1724]  eta: 1:01:19  lr: 0.000080  loss: 33.2148 (35.8015)  loss_n_40: 0.4536 (0.5796)  loss_n_60: 0.4760 (0.5914)  loss_n_80: 0.5522 (0.6564)  loss_n_100: 0.6087 (0.7158)  triple_100: 8.9374 (9.5531)  triple_80: 8.9978 (9.7326)  triple_60: 7.9733 (8.4443)  triple_40: 5.1127 (5.5283)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1110/1724]  eta: 1:00:20  lr: 0.000080  loss: 33.2148 (35.7782)  loss_n_40: 0.4450 (0.5783)  loss_n_60: 0.4760 (0.5903)  loss_n_80: 0.5208 (0.6551)  loss_n_100: 0.5757 (0.7144)  triple_100: 8.7733 (9.5463)  triple_80: 8.9360 (9.7256)  triple_60: 7.9733 (8.4407)  triple_40: 5.2859 (5.5274)  time: 5.8952  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1120/1724]  eta: 0:59:21  lr: 0.000080  loss: 32.7949 (35.7561)  loss_n_40: 0.4454 (0.5776)  loss_n_60: 0.4857 (0.5895)  loss_n_80: 0.5216 (0.6543)  loss_n_100: 0.5628 (0.7134)  triple_100: 8.7719 (9.5402)  triple_80: 8.9057 (9.7191)  triple_60: 7.8499 (8.4365)  triple_40: 5.2929 (5.5255)  time: 5.8947  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1130/1724]  eta: 0:58:22  lr: 0.000080  loss: 33.2148 (35.7349)  loss_n_40: 0.4515 (0.5764)  loss_n_60: 0.4727 (0.5884)  loss_n_80: 0.5217 (0.6530)  loss_n_100: 0.5742 (0.7120)  triple_100: 8.8485 (9.5341)  triple_80: 9.0150 (9.7123)  triple_60: 7.8499 (8.4326)  triple_40: 5.4966 (5.5261)  time: 5.8957  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1140/1724]  eta: 0:57:23  lr: 0.000080  loss: 33.4606 (35.7155)  loss_n_40: 0.4198 (0.5755)  loss_n_60: 0.4573 (0.5875)  loss_n_80: 0.5177 (0.6520)  loss_n_100: 0.5645 (0.7110)  triple_100: 8.8475 (9.5290)  triple_80: 9.0807 (9.7070)  triple_60: 7.9133 (8.4289)  triple_40: 5.4939 (5.5247)  time: 5.8963  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1150/1724]  eta: 0:56:24  lr: 0.000080  loss: 33.5089 (35.6959)  loss_n_40: 0.4644 (0.5749)  loss_n_60: 0.4739 (0.5867)  loss_n_80: 0.5326 (0.6510)  loss_n_100: 0.5863 (0.7099)  triple_100: 8.8475 (9.5229)  triple_80: 9.0868 (9.7007)  triple_60: 8.0614 (8.4253)  triple_40: 5.3729 (5.5244)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1160/1724]  eta: 0:55:25  lr: 0.000080  loss: 32.9402 (35.6817)  loss_n_40: 0.4818 (0.5743)  loss_n_60: 0.5127 (0.5864)  loss_n_80: 0.5511 (0.6506)  loss_n_100: 0.5936 (0.7093)  triple_100: 8.7308 (9.5188)  triple_80: 8.9129 (9.6973)  triple_60: 8.0232 (8.4238)  triple_40: 5.2319 (5.5211)  time: 5.8954  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [1170/1724]  eta: 0:54:26  lr: 0.000080  loss: 32.6232 (35.6581)  loss_n_40: 0.4594 (0.5732)  loss_n_60: 0.5011 (0.5854)  loss_n_80: 0.5511 (0.6495)  loss_n_100: 0.5936 (0.7081)  triple_100: 8.7308 (9.5121)  triple_80: 8.9129 (9.6904)  triple_60: 7.9260 (8.4201)  triple_40: 5.2319 (5.5193)  time: 5.8957  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1180/1724]  eta: 0:53:27  lr: 0.000080  loss: 32.6296 (35.6421)  loss_n_40: 0.4456 (0.5724)  loss_n_60: 0.4683 (0.5847)  loss_n_80: 0.5242 (0.6486)  loss_n_100: 0.5537 (0.7071)  triple_100: 8.6963 (9.5073)  triple_80: 8.8960 (9.6854)  triple_60: 7.9113 (8.4177)  triple_40: 5.3218 (5.5189)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1190/1724]  eta: 0:52:28  lr: 0.000080  loss: 32.5838 (35.6214)  loss_n_40: 0.4311 (0.5712)  loss_n_60: 0.4823 (0.5837)  loss_n_80: 0.5322 (0.6475)  loss_n_100: 0.5912 (0.7060)  triple_100: 8.8135 (9.5026)  triple_80: 8.8842 (9.6799)  triple_60: 7.8626 (8.4142)  triple_40: 5.2027 (5.5162)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1200/1724]  eta: 0:51:29  lr: 0.000080  loss: 32.5675 (35.5955)  loss_n_40: 0.4193 (0.5700)  loss_n_60: 0.4608 (0.5826)  loss_n_80: 0.5065 (0.6464)  loss_n_100: 0.5572 (0.7048)  triple_100: 8.8135 (9.4960)  triple_80: 8.8842 (9.6729)  triple_60: 7.8626 (8.4084)  triple_40: 5.1373 (5.5144)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1210/1724]  eta: 0:50:31  lr: 0.000080  loss: 32.6980 (35.5816)  loss_n_40: 0.4495 (0.5695)  loss_n_60: 0.4537 (0.5819)  loss_n_80: 0.5107 (0.6455)  loss_n_100: 0.5502 (0.7039)  triple_100: 8.8013 (9.4913)  triple_80: 8.8446 (9.6686)  triple_60: 7.7601 (8.4058)  triple_40: 5.3767 (5.5151)  time: 5.8979  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1220/1724]  eta: 0:49:32  lr: 0.000080  loss: 32.6139 (35.5637)  loss_n_40: 0.4919 (0.5690)  loss_n_60: 0.4637 (0.5812)  loss_n_80: 0.5107 (0.6446)  loss_n_100: 0.5566 (0.7028)  triple_100: 8.6190 (9.4847)  triple_80: 8.8125 (9.6623)  triple_60: 7.7601 (8.4026)  triple_40: 5.5874 (5.5166)  time: 5.8993  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:5]  [1230/1724]  eta: 0:48:33  lr: 0.000080  loss: 32.6139 (35.5524)  loss_n_40: 0.4567 (0.5682)  loss_n_60: 0.4608 (0.5804)  loss_n_80: 0.5079 (0.6437)  loss_n_100: 0.5638 (0.7019)  triple_100: 8.7394 (9.4807)  triple_80: 9.0093 (9.6585)  triple_60: 7.8762 (8.4012)  triple_40: 5.6541 (5.5179)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1240/1724]  eta: 0:47:34  lr: 0.000080  loss: 32.8890 (35.5337)  loss_n_40: 0.4234 (0.5670)  loss_n_60: 0.4734 (0.5796)  loss_n_80: 0.5396 (0.6429)  loss_n_100: 0.5875 (0.7011)  triple_100: 8.9823 (9.4764)  triple_80: 9.0366 (9.6537)  triple_60: 7.9043 (8.3974)  triple_40: 5.3851 (5.5155)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1250/1724]  eta: 0:46:35  lr: 0.000080  loss: 33.2087 (35.5213)  loss_n_40: 0.4234 (0.5664)  loss_n_60: 0.4734 (0.5791)  loss_n_80: 0.5411 (0.6424)  loss_n_100: 0.6014 (0.7006)  triple_100: 9.1109 (9.4747)  triple_80: 9.1681 (9.6512)  triple_60: 7.8937 (8.3946)  triple_40: 5.0666 (5.5122)  time: 5.8981  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1260/1724]  eta: 0:45:36  lr: 0.000080  loss: 33.3204 (35.4987)  loss_n_40: 0.4451 (0.5656)  loss_n_60: 0.4736 (0.5782)  loss_n_80: 0.5389 (0.6415)  loss_n_100: 0.5976 (0.6997)  triple_100: 9.1567 (9.4689)  triple_80: 9.1695 (9.6447)  triple_60: 7.8937 (8.3894)  triple_40: 5.1572 (5.5107)  time: 5.8978  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1270/1724]  eta: 0:44:37  lr: 0.000080  loss: 33.7337 (35.4808)  loss_n_40: 0.4451 (0.5651)  loss_n_60: 0.4746 (0.5775)  loss_n_80: 0.5497 (0.6408)  loss_n_100: 0.5939 (0.6989)  triple_100: 9.0208 (9.4644)  triple_80: 9.2362 (9.6400)  triple_60: 7.9598 (8.3853)  triple_40: 5.1877 (5.5088)  time: 5.8987  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1280/1724]  eta: 0:43:38  lr: 0.000080  loss: 33.9009 (35.4693)  loss_n_40: 0.4590 (0.5645)  loss_n_60: 0.4886 (0.5769)  loss_n_80: 0.5497 (0.6401)  loss_n_100: 0.6075 (0.6982)  triple_100: 9.2469 (9.4614)  triple_80: 9.2655 (9.6367)  triple_60: 8.0852 (8.3830)  triple_40: 5.3215 (5.5084)  time: 5.8987  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1290/1724]  eta: 0:42:39  lr: 0.000080  loss: 33.9009 (35.4705)  loss_n_40: 0.4674 (0.5640)  loss_n_60: 0.5130 (0.5766)  loss_n_80: 0.5621 (0.6398)  loss_n_100: 0.6090 (0.6979)  triple_100: 9.3253 (9.4625)  triple_80: 9.3496 (9.6375)  triple_60: 8.1887 (8.3840)  triple_40: 5.3215 (5.5082)  time: 5.8980  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1300/1724]  eta: 0:41:40  lr: 0.000080  loss: 33.5862 (35.4488)  loss_n_40: 0.4499 (0.5632)  loss_n_60: 0.5027 (0.5759)  loss_n_80: 0.5621 (0.6390)  loss_n_100: 0.6163 (0.6971)  triple_100: 9.1880 (9.4570)  triple_80: 9.1982 (9.6316)  triple_60: 8.0414 (8.3795)  triple_40: 5.2552 (5.5056)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1310/1724]  eta: 0:40:41  lr: 0.000080  loss: 31.9530 (35.4214)  loss_n_40: 0.4525 (0.5627)  loss_n_60: 0.4896 (0.5751)  loss_n_80: 0.5344 (0.6381)  loss_n_100: 0.5947 (0.6961)  triple_100: 8.3903 (9.4496)  triple_80: 8.6337 (9.6237)  triple_60: 7.5535 (8.3731)  triple_40: 5.1647 (5.5029)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1320/1724]  eta: 0:39:42  lr: 0.000080  loss: 31.9530 (35.4024)  loss_n_40: 0.4272 (0.5617)  loss_n_60: 0.4607 (0.5743)  loss_n_80: 0.5242 (0.6372)  loss_n_100: 0.5605 (0.6951)  triple_100: 8.5724 (9.4444)  triple_80: 8.6337 (9.6184)  triple_60: 7.5535 (8.3697)  triple_40: 5.2013 (5.5017)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1330/1724]  eta: 0:38:43  lr: 0.000080  loss: 32.6142 (35.3838)  loss_n_40: 0.4290 (0.5611)  loss_n_60: 0.4515 (0.5735)  loss_n_80: 0.4916 (0.6364)  loss_n_100: 0.5516 (0.6942)  triple_100: 8.6727 (9.4391)  triple_80: 8.7671 (9.6130)  triple_60: 7.8200 (8.3658)  triple_40: 5.3863 (5.5008)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1340/1724]  eta: 0:37:44  lr: 0.000080  loss: 32.6142 (35.3623)  loss_n_40: 0.4644 (0.5607)  loss_n_60: 0.4675 (0.5728)  loss_n_80: 0.5007 (0.6355)  loss_n_100: 0.5516 (0.6932)  triple_100: 8.6727 (9.4326)  triple_80: 8.7671 (9.6064)  triple_60: 7.7760 (8.3614)  triple_40: 5.4254 (5.4998)  time: 5.8974  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1350/1724]  eta: 0:36:45  lr: 0.000080  loss: 31.4080 (35.3295)  loss_n_40: 0.4193 (0.5596)  loss_n_60: 0.4355 (0.5717)  loss_n_80: 0.4948 (0.6342)  loss_n_100: 0.5147 (0.6918)  triple_100: 8.1384 (9.4228)  triple_80: 8.4239 (9.5967)  triple_60: 7.6025 (8.3545)  triple_40: 5.3833 (5.4982)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1360/1724]  eta: 0:35:46  lr: 0.000080  loss: 32.5577 (35.3199)  loss_n_40: 0.4407 (0.5594)  loss_n_60: 0.4697 (0.5715)  loss_n_80: 0.5130 (0.6338)  loss_n_100: 0.5427 (0.6912)  triple_100: 8.5599 (9.4186)  triple_80: 8.7171 (9.5934)  triple_60: 7.6577 (8.3537)  triple_40: 5.4163 (5.4984)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1370/1724]  eta: 0:34:47  lr: 0.000080  loss: 32.5838 (35.3008)  loss_n_40: 0.4747 (0.5586)  loss_n_60: 0.4729 (0.5706)  loss_n_80: 0.5234 (0.6328)  loss_n_100: 0.5509 (0.6902)  triple_100: 8.5849 (9.4134)  triple_80: 8.8447 (9.5876)  triple_60: 7.9273 (8.3495)  triple_40: 5.2870 (5.4982)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1380/1724]  eta: 0:33:48  lr: 0.000080  loss: 32.6494 (35.2872)  loss_n_40: 0.4322 (0.5580)  loss_n_60: 0.4352 (0.5699)  loss_n_80: 0.4936 (0.6320)  loss_n_100: 0.5530 (0.6894)  triple_100: 8.8223 (9.4098)  triple_80: 8.8452 (9.5838)  triple_60: 7.7039 (8.3467)  triple_40: 5.2976 (5.4976)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1390/1724]  eta: 0:32:49  lr: 0.000080  loss: 33.5946 (35.2739)  loss_n_40: 0.4317 (0.5573)  loss_n_60: 0.4662 (0.5692)  loss_n_80: 0.5355 (0.6313)  loss_n_100: 0.5803 (0.6887)  triple_100: 9.0369 (9.4072)  triple_80: 9.0831 (9.5803)  triple_60: 7.9641 (8.3441)  triple_40: 5.3010 (5.4958)  time: 5.8979  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1400/1724]  eta: 0:31:50  lr: 0.000080  loss: 34.2210 (35.2680)  loss_n_40: 0.4317 (0.5566)  loss_n_60: 0.4709 (0.5686)  loss_n_80: 0.5355 (0.6307)  loss_n_100: 0.5803 (0.6880)  triple_100: 9.0983 (9.4060)  triple_80: 9.2355 (9.5786)  triple_60: 8.1674 (8.3432)  triple_40: 5.3457 (5.4962)  time: 5.8979  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [1410/1724]  eta: 0:30:51  lr: 0.000080  loss: 34.2505 (35.2544)  loss_n_40: 0.4338 (0.5560)  loss_n_60: 0.4700 (0.5680)  loss_n_80: 0.5241 (0.6299)  loss_n_100: 0.5795 (0.6871)  triple_100: 9.1215 (9.4007)  triple_80: 9.2743 (9.5737)  triple_60: 8.1674 (8.3408)  triple_40: 5.6781 (5.4982)  time: 5.8970  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1420/1724]  eta: 0:29:52  lr: 0.000080  loss: 33.2519 (35.2353)  loss_n_40: 0.4297 (0.5553)  loss_n_60: 0.4596 (0.5672)  loss_n_80: 0.4910 (0.6290)  loss_n_100: 0.5358 (0.6861)  triple_100: 8.7872 (9.3952)  triple_80: 8.9386 (9.5681)  triple_60: 8.0013 (8.3369)  triple_40: 5.6630 (5.4976)  time: 5.8974  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1430/1724]  eta: 0:28:53  lr: 0.000080  loss: 32.5712 (35.2201)  loss_n_40: 0.4157 (0.5544)  loss_n_60: 0.4422 (0.5664)  loss_n_80: 0.4908 (0.6282)  loss_n_100: 0.5386 (0.6853)  triple_100: 8.6708 (9.3910)  triple_80: 8.8067 (9.5636)  triple_60: 7.7915 (8.3339)  triple_40: 5.3515 (5.4972)  time: 5.8974  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1440/1724]  eta: 0:27:54  lr: 0.000080  loss: 32.4541 (35.2006)  loss_n_40: 0.4193 (0.5537)  loss_n_60: 0.4502 (0.5658)  loss_n_80: 0.4977 (0.6275)  loss_n_100: 0.5587 (0.6846)  triple_100: 8.6635 (9.3858)  triple_80: 8.8067 (9.5581)  triple_60: 7.7637 (8.3299)  triple_40: 5.3357 (5.4954)  time: 5.8970  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1450/1724]  eta: 0:26:55  lr: 0.000080  loss: 31.9697 (35.1702)  loss_n_40: 0.4337 (0.5531)  loss_n_60: 0.4547 (0.5650)  loss_n_80: 0.4977 (0.6266)  loss_n_100: 0.5418 (0.6836)  triple_100: 8.3155 (9.3775)  triple_80: 8.5279 (9.5494)  triple_60: 7.6692 (8.3231)  triple_40: 5.0919 (5.4919)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1460/1724]  eta: 0:25:56  lr: 0.000080  loss: 31.0302 (35.1496)  loss_n_40: 0.4856 (0.5529)  loss_n_60: 0.4955 (0.5650)  loss_n_80: 0.5511 (0.6268)  loss_n_100: 0.6093 (0.6840)  triple_100: 8.1623 (9.3710)  triple_80: 8.3888 (9.5436)  triple_60: 7.3728 (8.3178)  triple_40: 4.8640 (5.4885)  time: 5.8970  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1470/1724]  eta: 0:24:57  lr: 0.000080  loss: 31.7765 (35.1358)  loss_n_40: 0.5293 (0.5533)  loss_n_60: 0.5692 (0.5655)  loss_n_80: 0.6428 (0.6271)  loss_n_100: 0.7064 (0.6843)  triple_100: 8.2400 (9.3652)  triple_80: 8.6315 (9.5393)  triple_60: 7.5586 (8.3153)  triple_40: 4.9414 (5.4858)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1480/1724]  eta: 0:23:58  lr: 0.000080  loss: 31.6882 (35.1136)  loss_n_40: 0.5250 (0.5530)  loss_n_60: 0.5761 (0.5653)  loss_n_80: 0.6368 (0.6270)  loss_n_100: 0.7064 (0.6842)  triple_100: 8.4839 (9.3594)  triple_80: 8.6951 (9.5339)  triple_60: 7.6247 (8.3103)  triple_40: 4.8344 (5.4805)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1490/1724]  eta: 0:22:59  lr: 0.000080  loss: 31.0625 (35.0848)  loss_n_40: 0.5205 (0.5531)  loss_n_60: 0.5141 (0.5650)  loss_n_80: 0.5544 (0.6266)  loss_n_100: 0.5997 (0.6837)  triple_100: 8.1963 (9.3503)  triple_80: 8.3561 (9.5255)  triple_60: 7.2818 (8.3035)  triple_40: 4.7529 (5.4770)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1500/1724]  eta: 0:22:00  lr: 0.000080  loss: 30.3989 (35.0597)  loss_n_40: 0.4970 (0.5526)  loss_n_60: 0.4882 (0.5645)  loss_n_80: 0.5467 (0.6260)  loss_n_100: 0.5791 (0.6831)  triple_100: 8.0528 (9.3426)  triple_80: 8.2199 (9.5182)  triple_60: 7.3885 (8.2983)  triple_40: 4.9983 (5.4744)  time: 5.8983  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1510/1724]  eta: 0:21:01  lr: 0.000080  loss: 30.6157 (35.0357)  loss_n_40: 0.4330 (0.5518)  loss_n_60: 0.4651 (0.5638)  loss_n_80: 0.5217 (0.6253)  loss_n_100: 0.5672 (0.6823)  triple_100: 8.3350 (9.3363)  triple_80: 8.3747 (9.5117)  triple_60: 7.4608 (8.2932)  triple_40: 5.0328 (5.4713)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1520/1724]  eta: 0:20:03  lr: 0.000080  loss: 31.7798 (35.0120)  loss_n_40: 0.4389 (0.5514)  loss_n_60: 0.4651 (0.5632)  loss_n_80: 0.5203 (0.6246)  loss_n_100: 0.5720 (0.6817)  triple_100: 8.3773 (9.3301)  triple_80: 8.4864 (9.5053)  triple_60: 7.4608 (8.2879)  triple_40: 4.9792 (5.4678)  time: 5.8996  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1530/1724]  eta: 0:19:04  lr: 0.000080  loss: 30.9481 (34.9828)  loss_n_40: 0.4637 (0.5509)  loss_n_60: 0.4829 (0.5627)  loss_n_80: 0.4970 (0.6239)  loss_n_100: 0.5681 (0.6809)  triple_100: 8.1870 (9.3214)  triple_80: 8.4483 (9.4968)  triple_60: 7.3627 (8.2818)  triple_40: 4.9352 (5.4643)  time: 5.8980  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1540/1724]  eta: 0:18:05  lr: 0.000080  loss: 30.6827 (34.9588)  loss_n_40: 0.4690 (0.5504)  loss_n_60: 0.4788 (0.5622)  loss_n_80: 0.5282 (0.6234)  loss_n_100: 0.5773 (0.6803)  triple_100: 8.0129 (9.3146)  triple_80: 8.3437 (9.4900)  triple_60: 7.2085 (8.2764)  triple_40: 4.9799 (5.4614)  time: 5.8976  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1550/1724]  eta: 0:17:06  lr: 0.000080  loss: 31.1461 (34.9402)  loss_n_40: 0.4867 (0.5507)  loss_n_60: 0.4935 (0.5621)  loss_n_80: 0.5391 (0.6233)  loss_n_100: 0.6061 (0.6802)  triple_100: 8.2270 (9.3088)  triple_80: 8.4032 (9.4849)  triple_60: 7.5202 (8.2711)  triple_40: 5.0039 (5.4591)  time: 5.8988  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1560/1724]  eta: 0:16:07  lr: 0.000080  loss: 33.2733 (34.9321)  loss_n_40: 0.6534 (0.5517)  loss_n_60: 0.5944 (0.5627)  loss_n_80: 0.6507 (0.6239)  loss_n_100: 0.6985 (0.6806)  triple_100: 8.6758 (9.3046)  triple_80: 8.8119 (9.4815)  triple_60: 7.5474 (8.2676)  triple_40: 5.2961 (5.4596)  time: 5.8987  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1570/1724]  eta: 0:15:08  lr: 0.000080  loss: 33.3435 (34.9260)  loss_n_40: 0.6466 (0.5523)  loss_n_60: 0.6034 (0.5629)  loss_n_80: 0.6507 (0.6240)  loss_n_100: 0.6925 (0.6807)  triple_100: 8.6758 (9.3013)  triple_80: 8.9177 (9.4793)  triple_60: 7.7792 (8.2656)  triple_40: 5.4731 (5.4597)  time: 5.8991  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1580/1724]  eta: 0:14:09  lr: 0.000080  loss: 32.7496 (34.9146)  loss_n_40: 0.6189 (0.5526)  loss_n_60: 0.5577 (0.5630)  loss_n_80: 0.6400 (0.6241)  loss_n_100: 0.6762 (0.6807)  triple_100: 8.7020 (9.2978)  triple_80: 8.9546 (9.4761)  triple_60: 7.7794 (8.2618)  triple_40: 5.3287 (5.4585)  time: 5.8982  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1590/1724]  eta: 0:13:10  lr: 0.000080  loss: 32.6311 (34.9004)  loss_n_40: 0.5791 (0.5529)  loss_n_60: 0.5504 (0.5628)  loss_n_80: 0.6087 (0.6239)  loss_n_100: 0.6547 (0.6805)  triple_100: 8.5005 (9.2930)  triple_80: 8.7914 (9.4719)  triple_60: 7.6085 (8.2579)  triple_40: 5.2327 (5.4574)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1600/1724]  eta: 0:12:11  lr: 0.000080  loss: 32.4101 (34.8962)  loss_n_40: 0.5758 (0.5530)  loss_n_60: 0.5407 (0.5628)  loss_n_80: 0.5919 (0.6239)  loss_n_100: 0.6446 (0.6804)  triple_100: 8.5005 (9.2917)  triple_80: 8.7207 (9.4708)  triple_60: 7.6085 (8.2564)  triple_40: 5.2282 (5.4572)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1610/1724]  eta: 0:11:12  lr: 0.000080  loss: 32.5873 (34.8807)  loss_n_40: 0.5207 (0.5530)  loss_n_60: 0.5167 (0.5625)  loss_n_80: 0.5843 (0.6236)  loss_n_100: 0.6439 (0.6801)  triple_100: 8.7827 (9.2876)  triple_80: 8.8599 (9.4668)  triple_60: 7.6171 (8.2522)  triple_40: 5.1387 (5.4549)  time: 5.8958  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1620/1724]  eta: 0:10:13  lr: 0.000080  loss: 31.9904 (34.8610)  loss_n_40: 0.4740 (0.5527)  loss_n_60: 0.5066 (0.5621)  loss_n_80: 0.5553 (0.6230)  loss_n_100: 0.6182 (0.6795)  triple_100: 8.5125 (9.2827)  triple_80: 8.6510 (9.4616)  triple_60: 7.4879 (8.2474)  triple_40: 4.9732 (5.4520)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1630/1724]  eta: 0:09:14  lr: 0.000080  loss: 31.3133 (34.8457)  loss_n_40: 0.4523 (0.5524)  loss_n_60: 0.4911 (0.5617)  loss_n_80: 0.5219 (0.6225)  loss_n_100: 0.5681 (0.6790)  triple_100: 8.2521 (9.2782)  triple_80: 8.5054 (9.4571)  triple_60: 7.5270 (8.2442)  triple_40: 5.1403 (5.4506)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1640/1724]  eta: 0:08:15  lr: 0.000080  loss: 31.4957 (34.8218)  loss_n_40: 0.4476 (0.5521)  loss_n_60: 0.4511 (0.5611)  loss_n_80: 0.5152 (0.6219)  loss_n_100: 0.5686 (0.6783)  triple_100: 8.3096 (9.2723)  triple_80: 8.5080 (9.4507)  triple_60: 7.5023 (8.2385)  triple_40: 4.9996 (5.4470)  time: 5.8956  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:5]  [1650/1724]  eta: 0:07:16  lr: 0.000080  loss: 31.9238 (34.8154)  loss_n_40: 0.5081 (0.5518)  loss_n_60: 0.4652 (0.5610)  loss_n_80: 0.5248 (0.6217)  loss_n_100: 0.5969 (0.6781)  triple_100: 8.6353 (9.2710)  triple_80: 8.7098 (9.4492)  triple_60: 7.4273 (8.2372)  triple_40: 4.8974 (5.4454)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1660/1724]  eta: 0:06:17  lr: 0.000080  loss: 32.0890 (34.8024)  loss_n_40: 0.5291 (0.5515)  loss_n_60: 0.4873 (0.5605)  loss_n_80: 0.5526 (0.6213)  loss_n_100: 0.6143 (0.6777)  triple_100: 8.8119 (9.2682)  triple_80: 8.9089 (9.4461)  triple_60: 7.6106 (8.2339)  triple_40: 4.9137 (5.4431)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1670/1724]  eta: 0:05:18  lr: 0.000080  loss: 31.8482 (34.7872)  loss_n_40: 0.4637 (0.5510)  loss_n_60: 0.4609 (0.5599)  loss_n_80: 0.5110 (0.6207)  loss_n_100: 0.5645 (0.6770)  triple_100: 8.4899 (9.2646)  triple_80: 8.6263 (9.4422)  triple_60: 7.6302 (8.2306)  triple_40: 4.9883 (5.4412)  time: 5.8953  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:5]  [1680/1724]  eta: 0:04:19  lr: 0.000080  loss: 31.8482 (34.7728)  loss_n_40: 0.4330 (0.5506)  loss_n_60: 0.4654 (0.5594)  loss_n_80: 0.5071 (0.6202)  loss_n_100: 0.5511 (0.6765)  triple_100: 8.4899 (9.2613)  triple_80: 8.6263 (9.4385)  triple_60: 7.7002 (8.2274)  triple_40: 5.0917 (5.4389)  time: 5.8966  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:5]  [1690/1724]  eta: 0:03:20  lr: 0.000080  loss: 32.7002 (34.7632)  loss_n_40: 0.4366 (0.5499)  loss_n_60: 0.4715 (0.5589)  loss_n_80: 0.5262 (0.6196)  loss_n_100: 0.5827 (0.6760)  triple_100: 8.8695 (9.2596)  triple_80: 9.0046 (9.4364)  triple_60: 7.7088 (8.2253)  triple_40: 5.0577 (5.4375)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1700/1724]  eta: 0:02:21  lr: 0.000080  loss: 32.7002 (34.7554)  loss_n_40: 0.4521 (0.5496)  loss_n_60: 0.4673 (0.5585)  loss_n_80: 0.5262 (0.6192)  loss_n_100: 0.5827 (0.6754)  triple_100: 8.8260 (9.2567)  triple_80: 8.8653 (9.4337)  triple_60: 7.7204 (8.2241)  triple_40: 5.3640 (5.4382)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1710/1724]  eta: 0:01:22  lr: 0.000080  loss: 32.4085 (34.7349)  loss_n_40: 0.4642 (0.5492)  loss_n_60: 0.4673 (0.5579)  loss_n_80: 0.5187 (0.6185)  loss_n_100: 0.5768 (0.6747)  triple_100: 8.5536 (9.2512)  triple_80: 8.8311 (9.4278)  triple_60: 7.7326 (8.2192)  triple_40: 5.2899 (5.4363)  time: 5.8962  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1720/1724]  eta: 0:00:23  lr: 0.000080  loss: 32.3301 (34.7262)  loss_n_40: 0.4538 (0.5487)  loss_n_60: 0.4534 (0.5575)  loss_n_80: 0.5156 (0.6180)  loss_n_100: 0.5763 (0.6743)  triple_100: 8.5536 (9.2491)  triple_80: 8.8311 (9.4254)  triple_60: 7.5628 (8.2178)  triple_40: 5.1043 (5.4353)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5]  [1723/1724]  eta: 0:00:05  lr: 0.000080  loss: 32.1896 (34.7185)  loss_n_40: 0.4234 (0.5485)  loss_n_60: 0.4337 (0.5573)  loss_n_80: 0.5015 (0.6178)  loss_n_100: 0.5657 (0.6740)  triple_100: 8.5737 (9.2473)  triple_80: 8.8311 (9.4236)  triple_60: 7.5628 (8.2161)  triple_40: 5.0594 (5.4339)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:5] Total time: 2:49:26 (5.8973 s / it)\n",
      "Averaged stats: lr: 0.000080  loss: 32.1896 (34.7185)  loss_n_40: 0.4234 (0.5485)  loss_n_60: 0.4337 (0.5573)  loss_n_80: 0.5015 (0.6178)  loss_n_100: 0.5657 (0.6740)  triple_100: 8.5737 (9.2473)  triple_80: 8.8311 (9.4236)  triple_60: 7.5628 (8.2161)  triple_40: 5.0594 (5.4339)\n",
      "Valid: [epoch:5]  [  0/845]  eta: 0:21:19  loss: 35.8229 (35.8229)  loss_n_40: 0.3864 (0.3864)  loss_n_60: 0.4837 (0.4837)  loss_n_80: 0.5880 (0.5880)  loss_n_100: 0.6822 (0.6822)  triple_100: 10.3447 (10.3447)  triple_80: 10.1628 (10.1628)  triple_60: 8.3635 (8.3635)  triple_40: 4.8115 (4.8115)  time: 1.5139  data: 0.5339  max mem: 40153\n",
      "Valid: [epoch:5]  [ 10/845]  eta: 0:14:15  loss: 30.4166 (31.1520)  loss_n_40: 0.3602 (0.3684)  loss_n_60: 0.4237 (0.4211)  loss_n_80: 0.5102 (0.4919)  loss_n_100: 0.5914 (0.5574)  triple_100: 8.4246 (8.6220)  triple_80: 8.3599 (8.6342)  triple_60: 7.3417 (7.3704)  triple_40: 4.6245 (4.6865)  time: 1.0243  data: 0.0487  max mem: 40153\n",
      "Valid: [epoch:5]  [ 20/845]  eta: 0:13:45  loss: 30.3758 (31.5610)  loss_n_40: 0.3602 (0.3929)  loss_n_60: 0.4182 (0.4469)  loss_n_80: 0.4845 (0.5169)  loss_n_100: 0.5264 (0.5795)  triple_100: 8.1898 (8.6654)  triple_80: 8.2882 (8.7358)  triple_60: 7.3417 (7.4761)  triple_40: 4.6548 (4.7475)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [ 30/845]  eta: 0:13:29  loss: 30.0225 (31.2807)  loss_n_40: 0.3593 (0.3876)  loss_n_60: 0.4126 (0.4343)  loss_n_80: 0.4845 (0.5023)  loss_n_100: 0.5264 (0.5634)  triple_100: 8.1131 (8.5784)  triple_80: 8.2882 (8.6396)  triple_60: 7.2817 (7.4140)  triple_40: 4.6783 (4.7612)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [ 40/845]  eta: 0:13:15  loss: 29.3503 (31.0380)  loss_n_40: 0.3593 (0.3852)  loss_n_60: 0.3747 (0.4275)  loss_n_80: 0.4268 (0.4939)  loss_n_100: 0.4934 (0.5539)  triple_100: 8.1131 (8.4962)  triple_80: 8.2083 (8.5609)  triple_60: 7.1378 (7.3572)  triple_40: 4.7536 (4.7631)  time: 0.9760  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [ 50/845]  eta: 0:13:04  loss: 29.1483 (30.5235)  loss_n_40: 0.3810 (0.3991)  loss_n_60: 0.3777 (0.4266)  loss_n_80: 0.4309 (0.4888)  loss_n_100: 0.4916 (0.5453)  triple_100: 8.0048 (8.3167)  triple_80: 7.9934 (8.3938)  triple_60: 6.8569 (7.2422)  triple_40: 4.6610 (4.7110)  time: 0.9759  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [ 60/845]  eta: 0:12:52  loss: 29.6247 (30.5544)  loss_n_40: 0.3887 (0.4008)  loss_n_60: 0.3973 (0.4290)  loss_n_80: 0.4535 (0.4906)  loss_n_100: 0.4986 (0.5483)  triple_100: 8.0344 (8.3182)  triple_80: 8.1788 (8.3922)  triple_60: 7.1161 (7.2472)  triple_40: 4.6047 (4.7281)  time: 0.9758  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [ 70/845]  eta: 0:12:42  loss: 29.9174 (30.6881)  loss_n_40: 0.3887 (0.4084)  loss_n_60: 0.4324 (0.4384)  loss_n_80: 0.4744 (0.4975)  loss_n_100: 0.5272 (0.5552)  triple_100: 8.0564 (8.3458)  triple_80: 8.2005 (8.4196)  triple_60: 7.1595 (7.2852)  triple_40: 4.7475 (4.7379)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [ 80/845]  eta: 0:12:31  loss: 30.5154 (30.7476)  loss_n_40: 0.3982 (0.4137)  loss_n_60: 0.4341 (0.4399)  loss_n_80: 0.4816 (0.4990)  loss_n_100: 0.5437 (0.5563)  triple_100: 8.2669 (8.3597)  triple_80: 8.3260 (8.4360)  triple_60: 7.2436 (7.2998)  triple_40: 4.7774 (4.7431)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [ 90/845]  eta: 0:12:21  loss: 30.5154 (30.6822)  loss_n_40: 0.3606 (0.4073)  loss_n_60: 0.4142 (0.4355)  loss_n_80: 0.4680 (0.4968)  loss_n_100: 0.5437 (0.5559)  triple_100: 8.4916 (8.3700)  triple_80: 8.3887 (8.4350)  triple_60: 7.0065 (7.2738)  triple_40: 4.5459 (4.7079)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [100/845]  eta: 0:12:11  loss: 30.4645 (30.7679)  loss_n_40: 0.3598 (0.4037)  loss_n_60: 0.4168 (0.4355)  loss_n_80: 0.5096 (0.4980)  loss_n_100: 0.5796 (0.5582)  triple_100: 8.5317 (8.4120)  triple_80: 8.5252 (8.4726)  triple_60: 7.0878 (7.2980)  triple_40: 4.3924 (4.6899)  time: 0.9772  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [110/845]  eta: 0:12:01  loss: 31.9851 (30.8362)  loss_n_40: 0.3804 (0.4018)  loss_n_60: 0.4521 (0.4359)  loss_n_80: 0.5533 (0.4995)  loss_n_100: 0.5947 (0.5602)  triple_100: 9.1201 (8.4459)  triple_80: 9.0267 (8.4983)  triple_60: 7.4907 (7.3106)  triple_40: 4.5327 (4.6839)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [120/845]  eta: 0:11:50  loss: 31.0631 (30.8146)  loss_n_40: 0.3702 (0.3992)  loss_n_60: 0.4109 (0.4349)  loss_n_80: 0.5026 (0.4989)  loss_n_100: 0.5908 (0.5603)  triple_100: 8.7169 (8.4511)  triple_80: 8.7003 (8.4973)  triple_60: 7.2139 (7.3063)  triple_40: 4.5879 (4.6666)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [130/845]  eta: 0:11:40  loss: 29.5733 (30.7487)  loss_n_40: 0.3462 (0.3976)  loss_n_60: 0.3911 (0.4337)  loss_n_80: 0.4609 (0.4977)  loss_n_100: 0.5217 (0.5593)  triple_100: 8.2597 (8.4373)  triple_80: 8.3054 (8.4794)  triple_60: 7.0643 (7.2895)  triple_40: 4.4053 (4.6542)  time: 0.9763  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:5]  [140/845]  eta: 0:11:30  loss: 29.7408 (30.7731)  loss_n_40: 0.3908 (0.3987)  loss_n_60: 0.4146 (0.4346)  loss_n_80: 0.4609 (0.4982)  loss_n_100: 0.5217 (0.5604)  triple_100: 7.8641 (8.4479)  triple_80: 8.0370 (8.4854)  triple_60: 7.1933 (7.2948)  triple_40: 4.4769 (4.6529)  time: 0.9765  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [150/845]  eta: 0:11:20  loss: 31.0733 (30.7220)  loss_n_40: 0.4045 (0.4000)  loss_n_60: 0.4282 (0.4336)  loss_n_80: 0.5073 (0.4979)  loss_n_100: 0.5588 (0.5597)  triple_100: 8.6041 (8.4319)  triple_80: 8.6862 (8.4696)  triple_60: 7.1933 (7.2720)  triple_40: 4.6146 (4.6574)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [160/845]  eta: 0:11:11  loss: 30.3466 (30.7470)  loss_n_40: 0.3695 (0.3983)  loss_n_60: 0.4059 (0.4327)  loss_n_80: 0.4837 (0.4975)  loss_n_100: 0.5435 (0.5596)  triple_100: 8.2824 (8.4450)  triple_80: 8.3791 (8.4804)  triple_60: 7.0332 (7.2771)  triple_40: 4.6146 (4.6565)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [170/845]  eta: 0:11:01  loss: 29.7647 (30.6703)  loss_n_40: 0.3591 (0.4003)  loss_n_60: 0.3978 (0.4318)  loss_n_80: 0.4709 (0.4960)  loss_n_100: 0.5242 (0.5578)  triple_100: 8.0763 (8.4169)  triple_80: 8.1907 (8.4545)  triple_60: 7.0563 (7.2585)  triple_40: 4.5621 (4.6547)  time: 0.9769  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [180/845]  eta: 0:10:51  loss: 29.7647 (30.6879)  loss_n_40: 0.3793 (0.3992)  loss_n_60: 0.4137 (0.4323)  loss_n_80: 0.4745 (0.4970)  loss_n_100: 0.5076 (0.5585)  triple_100: 8.0763 (8.4224)  triple_80: 8.2496 (8.4643)  triple_60: 7.0563 (7.2659)  triple_40: 4.5741 (4.6483)  time: 0.9770  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [190/845]  eta: 0:10:41  loss: 30.1814 (30.7155)  loss_n_40: 0.3931 (0.3997)  loss_n_60: 0.4198 (0.4330)  loss_n_80: 0.4745 (0.4982)  loss_n_100: 0.5189 (0.5597)  triple_100: 8.1854 (8.4328)  triple_80: 8.3578 (8.4736)  triple_60: 7.3593 (7.2694)  triple_40: 4.6451 (4.6491)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [200/845]  eta: 0:10:31  loss: 32.2717 (30.8004)  loss_n_40: 0.3935 (0.3989)  loss_n_60: 0.4423 (0.4341)  loss_n_80: 0.5170 (0.5002)  loss_n_100: 0.5892 (0.5628)  triple_100: 9.0210 (8.4674)  triple_80: 9.0218 (8.5039)  triple_60: 7.4265 (7.2849)  triple_40: 4.6442 (4.6483)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [210/845]  eta: 0:10:21  loss: 31.3508 (30.8081)  loss_n_40: 0.3886 (0.3980)  loss_n_60: 0.4399 (0.4334)  loss_n_80: 0.5086 (0.5001)  loss_n_100: 0.5825 (0.5629)  triple_100: 8.6199 (8.4739)  triple_80: 8.6850 (8.5107)  triple_60: 7.4265 (7.2862)  triple_40: 4.5632 (4.6429)  time: 0.9769  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [220/845]  eta: 0:10:11  loss: 31.3435 (30.9125)  loss_n_40: 0.3886 (0.3983)  loss_n_60: 0.4399 (0.4352)  loss_n_80: 0.5086 (0.5023)  loss_n_100: 0.5825 (0.5654)  triple_100: 8.6199 (8.5055)  triple_80: 8.6850 (8.5421)  triple_60: 7.6444 (7.3116)  triple_40: 4.5318 (4.6520)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [230/845]  eta: 0:10:01  loss: 29.8159 (30.8129)  loss_n_40: 0.3872 (0.3994)  loss_n_60: 0.4237 (0.4342)  loss_n_80: 0.5131 (0.5013)  loss_n_100: 0.5708 (0.5640)  triple_100: 8.4259 (8.4766)  triple_80: 8.3099 (8.5145)  triple_60: 6.9212 (7.2822)  triple_40: 4.4555 (4.6406)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [240/845]  eta: 0:09:52  loss: 29.4938 (30.7234)  loss_n_40: 0.3741 (0.4015)  loss_n_60: 0.4022 (0.4326)  loss_n_80: 0.4504 (0.4995)  loss_n_100: 0.5211 (0.5617)  triple_100: 7.9721 (8.4464)  triple_80: 8.0809 (8.4870)  triple_60: 6.8721 (7.2571)  triple_40: 4.4555 (4.6375)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [250/845]  eta: 0:09:42  loss: 28.0677 (30.6006)  loss_n_40: 0.3539 (0.4019)  loss_n_60: 0.3595 (0.4304)  loss_n_80: 0.4175 (0.4967)  loss_n_100: 0.4738 (0.5580)  triple_100: 7.5771 (8.4024)  triple_80: 7.7233 (8.4459)  triple_60: 6.6951 (7.2293)  triple_40: 4.6149 (4.6360)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [260/845]  eta: 0:09:32  loss: 28.9990 (30.5990)  loss_n_40: 0.3427 (0.4027)  loss_n_60: 0.3740 (0.4304)  loss_n_80: 0.4366 (0.4974)  loss_n_100: 0.4912 (0.5588)  triple_100: 7.7376 (8.4042)  triple_80: 7.8111 (8.4473)  triple_60: 6.8891 (7.2232)  triple_40: 4.6143 (4.6351)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [270/845]  eta: 0:09:22  loss: 29.4777 (30.5837)  loss_n_40: 0.3617 (0.4010)  loss_n_60: 0.3989 (0.4296)  loss_n_80: 0.4614 (0.4962)  loss_n_100: 0.5134 (0.5570)  triple_100: 8.1079 (8.3977)  triple_80: 8.1668 (8.4410)  triple_60: 7.0348 (7.2259)  triple_40: 4.6869 (4.6354)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [280/845]  eta: 0:09:12  loss: 30.0284 (30.5823)  loss_n_40: 0.3633 (0.4006)  loss_n_60: 0.3801 (0.4292)  loss_n_80: 0.4516 (0.4958)  loss_n_100: 0.5200 (0.5567)  triple_100: 8.1079 (8.3968)  triple_80: 8.2290 (8.4400)  triple_60: 7.1221 (7.2252)  triple_40: 4.7359 (4.6379)  time: 0.9765  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [290/845]  eta: 0:09:02  loss: 30.6351 (30.6148)  loss_n_40: 0.3858 (0.3999)  loss_n_60: 0.4367 (0.4295)  loss_n_80: 0.5147 (0.4966)  loss_n_100: 0.5814 (0.5576)  triple_100: 8.5374 (8.4104)  triple_80: 8.5475 (8.4529)  triple_60: 7.2458 (7.2316)  triple_40: 4.6974 (4.6365)  time: 0.9766  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [300/845]  eta: 0:08:53  loss: 30.6351 (30.6316)  loss_n_40: 0.3858 (0.4001)  loss_n_60: 0.4420 (0.4305)  loss_n_80: 0.5270 (0.4977)  loss_n_100: 0.5823 (0.5586)  triple_100: 8.5606 (8.4177)  triple_80: 8.5475 (8.4587)  triple_60: 7.4158 (7.2377)  triple_40: 4.5213 (4.6305)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [310/845]  eta: 0:08:43  loss: 29.3001 (30.6012)  loss_n_40: 0.3834 (0.3993)  loss_n_60: 0.3986 (0.4298)  loss_n_80: 0.4685 (0.4968)  loss_n_100: 0.5287 (0.5576)  triple_100: 8.1546 (8.4084)  triple_80: 8.0322 (8.4504)  triple_60: 7.0433 (7.2320)  triple_40: 4.5142 (4.6269)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [320/845]  eta: 0:08:33  loss: 30.3728 (30.6660)  loss_n_40: 0.3582 (0.3985)  loss_n_60: 0.4135 (0.4302)  loss_n_80: 0.4779 (0.4977)  loss_n_100: 0.5455 (0.5590)  triple_100: 8.5456 (8.4342)  triple_80: 8.5555 (8.4730)  triple_60: 7.3067 (7.2476)  triple_40: 4.6224 (4.6259)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [330/845]  eta: 0:08:23  loss: 30.9223 (30.6915)  loss_n_40: 0.3647 (0.3985)  loss_n_60: 0.4144 (0.4306)  loss_n_80: 0.4876 (0.4983)  loss_n_100: 0.5612 (0.5597)  triple_100: 8.8317 (8.4423)  triple_80: 8.7271 (8.4816)  triple_60: 7.3067 (7.2533)  triple_40: 4.6337 (4.6273)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [340/845]  eta: 0:08:13  loss: 30.6112 (30.7560)  loss_n_40: 0.3833 (0.3985)  loss_n_60: 0.4147 (0.4313)  loss_n_80: 0.4876 (0.4995)  loss_n_100: 0.5612 (0.5611)  triple_100: 8.4005 (8.4645)  triple_80: 8.4399 (8.5030)  triple_60: 7.3026 (7.2680)  triple_40: 4.7038 (4.6301)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [350/845]  eta: 0:08:04  loss: 30.6112 (30.7717)  loss_n_40: 0.3678 (0.3979)  loss_n_60: 0.4101 (0.4314)  loss_n_80: 0.4830 (0.4994)  loss_n_100: 0.5543 (0.5612)  triple_100: 8.3175 (8.4692)  triple_80: 8.4023 (8.5076)  triple_60: 7.4013 (7.2740)  triple_40: 4.6944 (4.6310)  time: 0.9768  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [360/845]  eta: 0:07:54  loss: 30.8177 (30.7921)  loss_n_40: 0.3678 (0.4000)  loss_n_60: 0.4097 (0.4318)  loss_n_80: 0.4624 (0.5001)  loss_n_100: 0.5543 (0.5618)  triple_100: 8.2831 (8.4734)  triple_80: 8.3807 (8.5122)  triple_60: 7.3706 (7.2753)  triple_40: 4.6944 (4.6376)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [370/845]  eta: 0:07:44  loss: 31.4750 (30.7992)  loss_n_40: 0.4328 (0.4038)  loss_n_60: 0.4705 (0.4328)  loss_n_80: 0.5651 (0.5012)  loss_n_100: 0.6229 (0.5630)  triple_100: 8.4899 (8.4760)  triple_80: 8.6191 (8.5145)  triple_60: 7.3706 (7.2716)  triple_40: 4.6863 (4.6364)  time: 0.9766  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [380/845]  eta: 0:07:34  loss: 32.9720 (30.8374)  loss_n_40: 0.3985 (0.4030)  loss_n_60: 0.4623 (0.4330)  loss_n_80: 0.5509 (0.5018)  loss_n_100: 0.6299 (0.5639)  triple_100: 9.1606 (8.4921)  triple_80: 9.3422 (8.5283)  triple_60: 7.6086 (7.2805)  triple_40: 4.5838 (4.6347)  time: 0.9768  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:5]  [390/845]  eta: 0:07:24  loss: 32.4275 (30.8542)  loss_n_40: 0.3822 (0.4037)  loss_n_60: 0.4555 (0.4342)  loss_n_80: 0.5450 (0.5031)  loss_n_100: 0.6252 (0.5655)  triple_100: 9.1606 (8.4984)  triple_80: 9.0788 (8.5342)  triple_60: 7.4579 (7.2828)  triple_40: 4.5838 (4.6324)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [400/845]  eta: 0:07:15  loss: 29.4576 (30.8577)  loss_n_40: 0.3628 (0.4028)  loss_n_60: 0.4083 (0.4340)  loss_n_80: 0.4809 (0.5028)  loss_n_100: 0.5387 (0.5653)  triple_100: 8.3419 (8.5002)  triple_80: 8.1883 (8.5355)  triple_60: 7.0920 (7.2859)  triple_40: 4.6037 (4.6312)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [410/845]  eta: 0:07:05  loss: 31.6259 (30.9035)  loss_n_40: 0.3707 (0.4038)  loss_n_60: 0.4166 (0.4353)  loss_n_80: 0.4809 (0.5043)  loss_n_100: 0.5551 (0.5671)  triple_100: 8.7230 (8.5163)  triple_80: 8.6807 (8.5499)  triple_60: 7.4909 (7.2955)  triple_40: 4.5165 (4.6312)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [420/845]  eta: 0:06:55  loss: 31.1222 (30.8912)  loss_n_40: 0.3715 (0.4028)  loss_n_60: 0.4166 (0.4345)  loss_n_80: 0.4808 (0.5035)  loss_n_100: 0.5551 (0.5662)  triple_100: 8.6652 (8.5124)  triple_80: 8.6382 (8.5456)  triple_60: 7.2153 (7.2938)  triple_40: 4.5243 (4.6323)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [430/845]  eta: 0:06:45  loss: 30.1939 (30.9175)  loss_n_40: 0.3633 (0.4032)  loss_n_60: 0.3883 (0.4351)  loss_n_80: 0.4597 (0.5042)  loss_n_100: 0.5289 (0.5669)  triple_100: 8.2324 (8.5180)  triple_80: 8.2321 (8.5527)  triple_60: 7.1099 (7.2995)  triple_40: 4.6527 (4.6379)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [440/845]  eta: 0:06:35  loss: 31.0328 (30.9258)  loss_n_40: 0.3854 (0.4034)  loss_n_60: 0.4147 (0.4352)  loss_n_80: 0.4986 (0.5043)  loss_n_100: 0.5665 (0.5670)  triple_100: 8.6018 (8.5199)  triple_80: 8.5488 (8.5540)  triple_60: 7.2244 (7.3011)  triple_40: 4.7414 (4.6408)  time: 0.9763  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [450/845]  eta: 0:06:26  loss: 30.7611 (30.9411)  loss_n_40: 0.3911 (0.4028)  loss_n_60: 0.4272 (0.4352)  loss_n_80: 0.5002 (0.5045)  loss_n_100: 0.5691 (0.5673)  triple_100: 8.3643 (8.5267)  triple_80: 8.5102 (8.5601)  triple_60: 7.3600 (7.3050)  triple_40: 4.6838 (4.6394)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [460/845]  eta: 0:06:16  loss: 29.6513 (30.9114)  loss_n_40: 0.3752 (0.4028)  loss_n_60: 0.4254 (0.4346)  loss_n_80: 0.4620 (0.5037)  loss_n_100: 0.5064 (0.5662)  triple_100: 7.9615 (8.5151)  triple_80: 8.2520 (8.5501)  triple_60: 7.2515 (7.2991)  triple_40: 4.6582 (4.6399)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [470/845]  eta: 0:06:06  loss: 29.6513 (30.9109)  loss_n_40: 0.3720 (0.4026)  loss_n_60: 0.4254 (0.4346)  loss_n_80: 0.4491 (0.5036)  loss_n_100: 0.5134 (0.5663)  triple_100: 7.9372 (8.5155)  triple_80: 8.0231 (8.5495)  triple_60: 7.1038 (7.2973)  triple_40: 4.6837 (4.6415)  time: 0.9763  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [480/845]  eta: 0:05:56  loss: 30.8066 (30.9261)  loss_n_40: 0.3733 (0.4023)  loss_n_60: 0.4280 (0.4347)  loss_n_80: 0.4853 (0.5038)  loss_n_100: 0.5595 (0.5666)  triple_100: 8.3844 (8.5203)  triple_80: 8.4022 (8.5549)  triple_60: 7.2814 (7.3003)  triple_40: 4.6597 (4.6432)  time: 0.9767  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [490/845]  eta: 0:05:47  loss: 30.9851 (30.9496)  loss_n_40: 0.3733 (0.4026)  loss_n_60: 0.4111 (0.4353)  loss_n_80: 0.4952 (0.5045)  loss_n_100: 0.5670 (0.5675)  triple_100: 8.6827 (8.5287)  triple_80: 8.6412 (8.5624)  triple_60: 7.3325 (7.3064)  triple_40: 4.5917 (4.6421)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [500/845]  eta: 0:05:37  loss: 31.6232 (30.9701)  loss_n_40: 0.3782 (0.4024)  loss_n_60: 0.4500 (0.4354)  loss_n_80: 0.5245 (0.5050)  loss_n_100: 0.5863 (0.5683)  triple_100: 8.7616 (8.5381)  triple_80: 8.8673 (8.5705)  triple_60: 7.6151 (7.3083)  triple_40: 4.6104 (4.6421)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [510/845]  eta: 0:05:27  loss: 31.8238 (30.9797)  loss_n_40: 0.3880 (0.4020)  loss_n_60: 0.4464 (0.4355)  loss_n_80: 0.5090 (0.5051)  loss_n_100: 0.5863 (0.5683)  triple_100: 8.5166 (8.5396)  triple_80: 8.7052 (8.5728)  triple_60: 7.5745 (7.3127)  triple_40: 4.6835 (4.6437)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [520/845]  eta: 0:05:17  loss: 30.2745 (30.9745)  loss_n_40: 0.3997 (0.4034)  loss_n_60: 0.4350 (0.4361)  loss_n_80: 0.4760 (0.5053)  loss_n_100: 0.5289 (0.5682)  triple_100: 8.0737 (8.5350)  triple_80: 8.2316 (8.5696)  triple_60: 7.3006 (7.3125)  triple_40: 4.7581 (4.6444)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [530/845]  eta: 0:05:07  loss: 30.0126 (30.9438)  loss_n_40: 0.4046 (0.4040)  loss_n_60: 0.4213 (0.4357)  loss_n_80: 0.4661 (0.5047)  loss_n_100: 0.5159 (0.5675)  triple_100: 8.0528 (8.5251)  triple_80: 8.1920 (8.5602)  triple_60: 7.0782 (7.3048)  triple_40: 4.5421 (4.6418)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [540/845]  eta: 0:04:58  loss: 30.1211 (30.9343)  loss_n_40: 0.4144 (0.4054)  loss_n_60: 0.4308 (0.4362)  loss_n_80: 0.4657 (0.5050)  loss_n_100: 0.5034 (0.5676)  triple_100: 8.2557 (8.5187)  triple_80: 8.2333 (8.5562)  triple_60: 7.2033 (7.3028)  triple_40: 4.4868 (4.6423)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [550/845]  eta: 0:04:48  loss: 29.8364 (30.9244)  loss_n_40: 0.4014 (0.4052)  loss_n_60: 0.4393 (0.4363)  loss_n_80: 0.5296 (0.5052)  loss_n_100: 0.6092 (0.5678)  triple_100: 8.5502 (8.5171)  triple_80: 8.4273 (8.5543)  triple_60: 7.3034 (7.3000)  triple_40: 4.4453 (4.6384)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [560/845]  eta: 0:04:38  loss: 29.8364 (30.9048)  loss_n_40: 0.3684 (0.4046)  loss_n_60: 0.4006 (0.4357)  loss_n_80: 0.4782 (0.5043)  loss_n_100: 0.5513 (0.5668)  triple_100: 8.4984 (8.5094)  triple_80: 8.4154 (8.5473)  triple_60: 6.9173 (7.2969)  triple_40: 4.5280 (4.6397)  time: 0.9762  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [570/845]  eta: 0:04:28  loss: 29.9221 (30.9061)  loss_n_40: 0.3751 (0.4053)  loss_n_60: 0.4111 (0.4361)  loss_n_80: 0.4782 (0.5047)  loss_n_100: 0.5503 (0.5673)  triple_100: 8.3457 (8.5109)  triple_80: 8.2731 (8.5469)  triple_60: 7.0123 (7.2966)  triple_40: 4.5584 (4.6383)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [580/845]  eta: 0:04:18  loss: 30.9594 (30.9144)  loss_n_40: 0.3751 (0.4054)  loss_n_60: 0.4366 (0.4364)  loss_n_80: 0.5112 (0.5048)  loss_n_100: 0.5606 (0.5674)  triple_100: 8.6111 (8.5127)  triple_80: 8.4803 (8.5485)  triple_60: 7.2710 (7.2992)  triple_40: 4.5203 (4.6401)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [590/845]  eta: 0:04:09  loss: 30.0688 (30.9010)  loss_n_40: 0.3761 (0.4068)  loss_n_60: 0.4353 (0.4367)  loss_n_80: 0.4827 (0.5049)  loss_n_100: 0.5430 (0.5676)  triple_100: 8.2196 (8.5079)  triple_80: 8.2008 (8.5435)  triple_60: 7.2289 (7.2945)  triple_40: 4.6284 (4.6391)  time: 0.9761  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [600/845]  eta: 0:03:59  loss: 29.4373 (30.8681)  loss_n_40: 0.3957 (0.4078)  loss_n_60: 0.4105 (0.4364)  loss_n_80: 0.4482 (0.5043)  loss_n_100: 0.5278 (0.5668)  triple_100: 8.1143 (8.4957)  triple_80: 8.0994 (8.5322)  triple_60: 7.1528 (7.2861)  triple_40: 4.6984 (4.6388)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [610/845]  eta: 0:03:49  loss: 28.9725 (30.8404)  loss_n_40: 0.3938 (0.4083)  loss_n_60: 0.4105 (0.4364)  loss_n_80: 0.4380 (0.5038)  loss_n_100: 0.4592 (0.5660)  triple_100: 7.8322 (8.4833)  triple_80: 7.8861 (8.5211)  triple_60: 6.9755 (7.2803)  triple_40: 4.6994 (4.6412)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [620/845]  eta: 0:03:39  loss: 29.5670 (30.8514)  loss_n_40: 0.3897 (0.4082)  loss_n_60: 0.4148 (0.4367)  loss_n_80: 0.4735 (0.5042)  loss_n_100: 0.5195 (0.5664)  triple_100: 7.8371 (8.4865)  triple_80: 7.9617 (8.5248)  triple_60: 7.2230 (7.2834)  triple_40: 4.6096 (4.6411)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [630/845]  eta: 0:03:30  loss: 30.9965 (30.8583)  loss_n_40: 0.3861 (0.4086)  loss_n_60: 0.4277 (0.4369)  loss_n_80: 0.4942 (0.5044)  loss_n_100: 0.5595 (0.5664)  triple_100: 8.3326 (8.4876)  triple_80: 8.6686 (8.5273)  triple_60: 7.3178 (7.2856)  triple_40: 4.6096 (4.6416)  time: 0.9757  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:5]  [640/845]  eta: 0:03:20  loss: 29.7907 (30.8630)  loss_n_40: 0.3962 (0.4085)  loss_n_60: 0.4263 (0.4371)  loss_n_80: 0.4877 (0.5046)  loss_n_100: 0.5653 (0.5668)  triple_100: 8.1921 (8.4906)  triple_80: 8.2742 (8.5294)  triple_60: 7.2019 (7.2868)  triple_40: 4.6210 (4.6392)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [650/845]  eta: 0:03:10  loss: 29.4765 (30.8375)  loss_n_40: 0.3604 (0.4084)  loss_n_60: 0.3942 (0.4365)  loss_n_80: 0.4535 (0.5039)  loss_n_100: 0.5429 (0.5659)  triple_100: 8.1522 (8.4805)  triple_80: 8.1999 (8.5200)  triple_60: 6.8754 (7.2802)  triple_40: 4.6484 (4.6422)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [660/845]  eta: 0:03:00  loss: 30.2433 (30.8649)  loss_n_40: 0.3705 (0.4079)  loss_n_60: 0.3971 (0.4369)  loss_n_80: 0.4723 (0.5045)  loss_n_100: 0.5549 (0.5667)  triple_100: 8.3494 (8.4910)  triple_80: 8.3366 (8.5298)  triple_60: 7.1860 (7.2870)  triple_40: 4.7039 (4.6411)  time: 0.9759  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [670/845]  eta: 0:02:50  loss: 32.2117 (30.8600)  loss_n_40: 0.3809 (0.4084)  loss_n_60: 0.4415 (0.4368)  loss_n_80: 0.5313 (0.5043)  loss_n_100: 0.5783 (0.5664)  triple_100: 8.7220 (8.4885)  triple_80: 9.1109 (8.5270)  triple_60: 7.4367 (7.2863)  triple_40: 4.7039 (4.6422)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [680/845]  eta: 0:02:41  loss: 29.6099 (30.8692)  loss_n_40: 0.3990 (0.4092)  loss_n_60: 0.3848 (0.4374)  loss_n_80: 0.4533 (0.5046)  loss_n_100: 0.5198 (0.5665)  triple_100: 7.9672 (8.4885)  triple_80: 8.0819 (8.5277)  triple_60: 7.1715 (7.2900)  triple_40: 4.8072 (4.6453)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [690/845]  eta: 0:02:31  loss: 30.6427 (30.8803)  loss_n_40: 0.4075 (0.4091)  loss_n_60: 0.4174 (0.4376)  loss_n_80: 0.4752 (0.5048)  loss_n_100: 0.5198 (0.5667)  triple_100: 8.1190 (8.4909)  triple_80: 8.1443 (8.5312)  triple_60: 7.4043 (7.2935)  triple_40: 4.7076 (4.6465)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [700/845]  eta: 0:02:21  loss: 31.2873 (30.8981)  loss_n_40: 0.3811 (0.4091)  loss_n_60: 0.4522 (0.4378)  loss_n_80: 0.4815 (0.5051)  loss_n_100: 0.5408 (0.5672)  triple_100: 8.4682 (8.4974)  triple_80: 8.6053 (8.5372)  triple_60: 7.4043 (7.2971)  triple_40: 4.6890 (4.6473)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [710/845]  eta: 0:02:11  loss: 31.2546 (30.9019)  loss_n_40: 0.3815 (0.4097)  loss_n_60: 0.4379 (0.4380)  loss_n_80: 0.5026 (0.5054)  loss_n_100: 0.5848 (0.5674)  triple_100: 8.6250 (8.4973)  triple_80: 8.4981 (8.5380)  triple_60: 7.2017 (7.2979)  triple_40: 4.6890 (4.6482)  time: 0.9764  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [720/845]  eta: 0:02:02  loss: 32.0906 (30.9109)  loss_n_40: 0.3938 (0.4098)  loss_n_60: 0.4579 (0.4382)  loss_n_80: 0.5200 (0.5055)  loss_n_100: 0.5848 (0.5676)  triple_100: 9.0154 (8.5010)  triple_80: 8.9910 (8.5409)  triple_60: 7.2986 (7.2999)  triple_40: 4.6984 (4.6481)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [730/845]  eta: 0:01:52  loss: 28.8199 (30.8513)  loss_n_40: 0.3938 (0.4104)  loss_n_60: 0.4129 (0.4374)  loss_n_80: 0.4614 (0.5045)  loss_n_100: 0.5188 (0.5663)  triple_100: 7.7309 (8.4806)  triple_80: 7.7694 (8.5220)  triple_60: 6.8454 (7.2853)  triple_40: 4.5776 (4.6448)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [740/845]  eta: 0:01:42  loss: 28.9099 (30.8625)  loss_n_40: 0.3997 (0.4104)  loss_n_60: 0.3988 (0.4375)  loss_n_80: 0.4560 (0.5046)  loss_n_100: 0.5061 (0.5663)  triple_100: 7.8329 (8.4842)  triple_80: 7.9005 (8.5250)  triple_60: 6.8491 (7.2877)  triple_40: 4.5906 (4.6468)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [750/845]  eta: 0:01:32  loss: 31.8072 (30.8674)  loss_n_40: 0.4079 (0.4105)  loss_n_60: 0.4483 (0.4378)  loss_n_80: 0.5284 (0.5047)  loss_n_100: 0.5950 (0.5665)  triple_100: 8.6927 (8.4854)  triple_80: 8.7691 (8.5254)  triple_60: 7.5135 (7.2885)  triple_40: 4.7378 (4.6486)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [760/845]  eta: 0:01:23  loss: 31.0648 (30.8579)  loss_n_40: 0.3932 (0.4102)  loss_n_60: 0.4388 (0.4375)  loss_n_80: 0.5125 (0.5043)  loss_n_100: 0.5550 (0.5661)  triple_100: 8.5732 (8.4822)  triple_80: 8.2199 (8.5224)  triple_60: 7.1078 (7.2876)  triple_40: 4.7251 (4.6477)  time: 0.9763  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [770/845]  eta: 0:01:13  loss: 28.4411 (30.8450)  loss_n_40: 0.3471 (0.4106)  loss_n_60: 0.3961 (0.4375)  loss_n_80: 0.4648 (0.5041)  loss_n_100: 0.4966 (0.5658)  triple_100: 7.6138 (8.4769)  triple_80: 7.8324 (8.5183)  triple_60: 6.9052 (7.2856)  triple_40: 4.5154 (4.6462)  time: 0.9763  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:5]  [780/845]  eta: 0:01:03  loss: 29.6752 (30.8300)  loss_n_40: 0.3609 (0.4107)  loss_n_60: 0.3883 (0.4371)  loss_n_80: 0.4567 (0.5036)  loss_n_100: 0.5012 (0.5652)  triple_100: 8.0884 (8.4703)  triple_80: 8.1137 (8.5126)  triple_60: 7.1136 (7.2819)  triple_40: 4.6865 (4.6485)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [790/845]  eta: 0:00:53  loss: 30.2668 (30.8354)  loss_n_40: 0.3609 (0.4104)  loss_n_60: 0.3883 (0.4370)  loss_n_80: 0.4567 (0.5036)  loss_n_100: 0.5245 (0.5653)  triple_100: 8.3144 (8.4730)  triple_80: 8.3012 (8.5149)  triple_60: 7.1136 (7.2826)  triple_40: 4.7041 (4.6487)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [800/845]  eta: 0:00:43  loss: 30.1728 (30.8420)  loss_n_40: 0.3690 (0.4099)  loss_n_60: 0.3890 (0.4367)  loss_n_80: 0.4628 (0.5034)  loss_n_100: 0.5245 (0.5651)  triple_100: 8.2779 (8.4752)  triple_80: 8.2974 (8.5169)  triple_60: 7.1447 (7.2843)  triple_40: 4.7041 (4.6504)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [810/845]  eta: 0:00:34  loss: 30.0578 (30.8440)  loss_n_40: 0.3852 (0.4105)  loss_n_60: 0.4211 (0.4370)  loss_n_80: 0.4651 (0.5035)  loss_n_100: 0.5089 (0.5652)  triple_100: 8.0843 (8.4753)  triple_80: 8.2424 (8.5168)  triple_60: 7.1564 (7.2848)  triple_40: 4.6457 (4.6509)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [820/845]  eta: 0:00:24  loss: 30.8927 (30.8429)  loss_n_40: 0.3924 (0.4103)  loss_n_60: 0.4478 (0.4369)  loss_n_80: 0.5062 (0.5035)  loss_n_100: 0.5821 (0.5652)  triple_100: 8.4612 (8.4755)  triple_80: 8.6333 (8.5167)  triple_60: 7.4044 (7.2848)  triple_40: 4.6156 (4.6499)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [830/845]  eta: 0:00:14  loss: 30.4497 (30.8457)  loss_n_40: 0.3782 (0.4103)  loss_n_60: 0.4330 (0.4371)  loss_n_80: 0.5062 (0.5036)  loss_n_100: 0.5821 (0.5653)  triple_100: 8.0632 (8.4764)  triple_80: 8.3851 (8.5176)  triple_60: 7.3906 (7.2856)  triple_40: 4.6225 (4.6498)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [840/845]  eta: 0:00:04  loss: 30.6092 (30.8487)  loss_n_40: 0.3959 (0.4119)  loss_n_60: 0.4606 (0.4378)  loss_n_80: 0.5366 (0.5041)  loss_n_100: 0.5788 (0.5657)  triple_100: 8.4682 (8.4753)  triple_80: 8.4590 (8.5169)  triple_60: 7.3301 (7.2854)  triple_40: 4.6765 (4.6517)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5]  [844/845]  eta: 0:00:00  loss: 30.6092 (30.8495)  loss_n_40: 0.3853 (0.4121)  loss_n_60: 0.4496 (0.4380)  loss_n_80: 0.5336 (0.5042)  loss_n_100: 0.5788 (0.5658)  triple_100: 8.4682 (8.4754)  triple_80: 8.4590 (8.5167)  triple_60: 7.3301 (7.2856)  triple_40: 4.6917 (4.6517)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:5] Total time: 0:13:45 (0.9771 s / it)\n",
      "Averaged stats: loss: 30.6092 (30.8495)  loss_n_40: 0.3853 (0.4121)  loss_n_60: 0.4496 (0.4380)  loss_n_80: 0.5336 (0.5042)  loss_n_100: 0.5788 (0.5658)  triple_100: 8.4682 (8.4754)  triple_80: 8.4590 (8.5167)  triple_60: 7.3301 (7.2856)  triple_40: 4.6917 (4.6517)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle_2/low2high/epoch_5_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.566%\n",
      "Min loss_n_100: 0.566\n",
      "Best Epoch: 5.000\n",
      "Train: [epoch:6]  [   0/1724]  eta: 3:03:47  lr: 0.000100  loss: 28.2257 (28.2257)  loss_n_40: 0.3948 (0.3948)  loss_n_60: 0.3603 (0.3603)  loss_n_80: 0.4481 (0.4481)  loss_n_100: 0.4723 (0.4723)  triple_100: 7.4496 (7.4496)  triple_80: 7.6789 (7.6789)  triple_60: 6.4463 (6.4463)  triple_40: 4.9754 (4.9754)  time: 6.3964  data: 0.6539  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [  10/1724]  eta: 2:49:39  lr: 0.000100  loss: 31.2358 (32.2504)  loss_n_40: 0.4004 (0.4515)  loss_n_60: 0.4460 (0.4743)  loss_n_80: 0.5160 (0.5372)  loss_n_100: 0.5682 (0.5877)  triple_100: 8.3722 (8.6066)  triple_80: 8.5686 (8.7989)  triple_60: 7.4514 (7.6554)  triple_40: 4.9470 (5.1389)  time: 5.9390  data: 0.0596  max mem: 40153\n",
      "Train: [epoch:6]  [  20/1724]  eta: 2:48:06  lr: 0.000100  loss: 31.9739 (32.7191)  loss_n_40: 0.4141 (0.4602)  loss_n_60: 0.4622 (0.4851)  loss_n_80: 0.5424 (0.5514)  loss_n_100: 0.6127 (0.6099)  triple_100: 8.5921 (8.8306)  triple_80: 8.7922 (8.9661)  triple_60: 7.5799 (7.7298)  triple_40: 4.9311 (5.0858)  time: 5.8953  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [  30/1724]  eta: 2:46:54  lr: 0.000100  loss: 31.9739 (32.3478)  loss_n_40: 0.4141 (0.4491)  loss_n_60: 0.4490 (0.4728)  loss_n_80: 0.5279 (0.5385)  loss_n_100: 0.5984 (0.5965)  triple_100: 8.7211 (8.7612)  triple_80: 8.7922 (8.8791)  triple_60: 7.5799 (7.6559)  triple_40: 4.8826 (4.9947)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [  40/1724]  eta: 2:45:48  lr: 0.000100  loss: 32.2099 (32.6400)  loss_n_40: 0.4168 (0.4477)  loss_n_60: 0.4376 (0.4748)  loss_n_80: 0.5136 (0.5407)  loss_n_100: 0.5718 (0.6009)  triple_100: 8.8447 (8.8507)  triple_80: 8.9284 (8.9498)  triple_60: 7.6685 (7.7224)  triple_40: 4.9360 (5.0530)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [  50/1724]  eta: 2:44:45  lr: 0.000100  loss: 33.2231 (32.7463)  loss_n_40: 0.4525 (0.4562)  loss_n_60: 0.4785 (0.4810)  loss_n_80: 0.5464 (0.5456)  loss_n_100: 0.6130 (0.6048)  triple_100: 8.8860 (8.8538)  triple_80: 9.0884 (8.9689)  triple_60: 8.0224 (7.7576)  triple_40: 5.1261 (5.0784)  time: 5.8961  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [  60/1724]  eta: 2:43:44  lr: 0.000100  loss: 33.1946 (32.6684)  loss_n_40: 0.4384 (0.4534)  loss_n_60: 0.4646 (0.4750)  loss_n_80: 0.5292 (0.5391)  loss_n_100: 0.5822 (0.5976)  triple_100: 8.7223 (8.8314)  triple_80: 8.8694 (8.9412)  triple_60: 8.0122 (7.7376)  triple_40: 5.1114 (5.0930)  time: 5.8973  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [  70/1724]  eta: 2:42:44  lr: 0.000100  loss: 31.9225 (32.5272)  loss_n_40: 0.3902 (0.4466)  loss_n_60: 0.4256 (0.4677)  loss_n_80: 0.4971 (0.5312)  loss_n_100: 0.5540 (0.5888)  triple_100: 8.6726 (8.7904)  triple_80: 8.7386 (8.8983)  triple_60: 7.5289 (7.7074)  triple_40: 5.0571 (5.0969)  time: 5.8983  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:6]  [  80/1724]  eta: 2:41:44  lr: 0.000100  loss: 31.9225 (32.6819)  loss_n_40: 0.4064 (0.4464)  loss_n_60: 0.4327 (0.4688)  loss_n_80: 0.5008 (0.5329)  loss_n_100: 0.5603 (0.5908)  triple_100: 8.6726 (8.8391)  triple_80: 8.7495 (8.9426)  triple_60: 7.5792 (7.7436)  triple_40: 5.1230 (5.1177)  time: 5.8980  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [  90/1724]  eta: 2:40:43  lr: 0.000100  loss: 32.4825 (32.8108)  loss_n_40: 0.4602 (0.4553)  loss_n_60: 0.4680 (0.4749)  loss_n_80: 0.5399 (0.5384)  loss_n_100: 0.5746 (0.5956)  triple_100: 8.8712 (8.8649)  triple_80: 8.9124 (8.9767)  triple_60: 7.8127 (7.7772)  triple_40: 5.0673 (5.1278)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 100/1724]  eta: 2:39:44  lr: 0.000100  loss: 32.7726 (32.7907)  loss_n_40: 0.4583 (0.4538)  loss_n_60: 0.4706 (0.4727)  loss_n_80: 0.5304 (0.5359)  loss_n_100: 0.5871 (0.5931)  triple_100: 8.7890 (8.8589)  triple_80: 8.9168 (8.9663)  triple_60: 7.8127 (7.7711)  triple_40: 5.0116 (5.1388)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 110/1724]  eta: 2:38:44  lr: 0.000100  loss: 32.4773 (32.7832)  loss_n_40: 0.4385 (0.4534)  loss_n_60: 0.4548 (0.4725)  loss_n_80: 0.5238 (0.5359)  loss_n_100: 0.5871 (0.5935)  triple_100: 8.7880 (8.8636)  triple_80: 8.9168 (8.9662)  triple_60: 7.6329 (7.7649)  triple_40: 5.0154 (5.1334)  time: 5.8981  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 120/1724]  eta: 2:37:45  lr: 0.000100  loss: 32.4773 (32.8094)  loss_n_40: 0.4344 (0.4528)  loss_n_60: 0.4610 (0.4737)  loss_n_80: 0.5251 (0.5365)  loss_n_100: 0.5838 (0.5936)  triple_100: 8.9018 (8.8619)  triple_80: 8.9394 (8.9703)  triple_60: 7.6329 (7.7785)  triple_40: 5.0275 (5.1421)  time: 5.8991  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 130/1724]  eta: 2:36:46  lr: 0.000100  loss: 32.0165 (32.7571)  loss_n_40: 0.4296 (0.4522)  loss_n_60: 0.4703 (0.4734)  loss_n_80: 0.5272 (0.5357)  loss_n_100: 0.5804 (0.5924)  triple_100: 8.5816 (8.8505)  triple_80: 8.7378 (8.9570)  triple_60: 7.6680 (7.7726)  triple_40: 5.0483 (5.1233)  time: 5.8994  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 140/1724]  eta: 2:35:46  lr: 0.000100  loss: 31.8944 (32.7219)  loss_n_40: 0.4161 (0.4506)  loss_n_60: 0.4291 (0.4714)  loss_n_80: 0.4936 (0.5336)  loss_n_100: 0.5561 (0.5900)  triple_100: 8.6168 (8.8382)  triple_80: 8.6712 (8.9464)  triple_60: 7.4992 (7.7651)  triple_40: 5.0014 (5.1265)  time: 5.8991  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 150/1724]  eta: 2:34:47  lr: 0.000100  loss: 31.8944 (32.6441)  loss_n_40: 0.4331 (0.4507)  loss_n_60: 0.4215 (0.4695)  loss_n_80: 0.4844 (0.5309)  loss_n_100: 0.5369 (0.5868)  triple_100: 8.6168 (8.8089)  triple_80: 8.6548 (8.9171)  triple_60: 7.4752 (7.7490)  triple_40: 5.0206 (5.1312)  time: 5.8993  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 160/1724]  eta: 2:33:48  lr: 0.000100  loss: 32.3363 (32.5703)  loss_n_40: 0.4041 (0.4480)  loss_n_60: 0.4413 (0.4669)  loss_n_80: 0.4844 (0.5278)  loss_n_100: 0.5418 (0.5834)  triple_100: 8.5597 (8.7855)  triple_80: 8.7718 (8.8939)  triple_60: 7.6885 (7.7350)  triple_40: 5.0206 (5.1298)  time: 5.8998  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 170/1724]  eta: 2:32:49  lr: 0.000100  loss: 31.2331 (32.4980)  loss_n_40: 0.4067 (0.4482)  loss_n_60: 0.4442 (0.4664)  loss_n_80: 0.4902 (0.5266)  loss_n_100: 0.5453 (0.5820)  triple_100: 8.2975 (8.7633)  triple_80: 8.4336 (8.8711)  triple_60: 7.4193 (7.7206)  triple_40: 4.9865 (5.1198)  time: 5.8980  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 180/1724]  eta: 2:31:49  lr: 0.000100  loss: 31.5674 (32.5017)  loss_n_40: 0.4622 (0.4491)  loss_n_60: 0.4515 (0.4674)  loss_n_80: 0.5099 (0.5278)  loss_n_100: 0.5726 (0.5832)  triple_100: 8.6086 (8.7685)  triple_80: 8.6826 (8.8757)  triple_60: 7.4835 (7.7224)  triple_40: 4.8328 (5.1076)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 190/1724]  eta: 2:30:50  lr: 0.000100  loss: 31.5330 (32.4223)  loss_n_40: 0.4622 (0.4492)  loss_n_60: 0.4455 (0.4662)  loss_n_80: 0.5099 (0.5265)  loss_n_100: 0.5733 (0.5819)  triple_100: 8.5537 (8.7476)  triple_80: 8.6918 (8.8542)  triple_60: 7.4835 (7.6999)  triple_40: 4.8092 (5.0969)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 200/1724]  eta: 2:29:51  lr: 0.000100  loss: 31.5330 (32.3982)  loss_n_40: 0.4221 (0.4475)  loss_n_60: 0.4406 (0.4651)  loss_n_80: 0.5048 (0.5252)  loss_n_100: 0.5588 (0.5806)  triple_100: 8.5375 (8.7420)  triple_80: 8.6386 (8.8473)  triple_60: 7.5115 (7.6961)  triple_40: 4.9240 (5.0945)  time: 5.8958  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 210/1724]  eta: 2:28:51  lr: 0.000100  loss: 32.2263 (32.3600)  loss_n_40: 0.4075 (0.4459)  loss_n_60: 0.4411 (0.4640)  loss_n_80: 0.4920 (0.5239)  loss_n_100: 0.5588 (0.5792)  triple_100: 8.7129 (8.7305)  triple_80: 8.8633 (8.8355)  triple_60: 7.5886 (7.6890)  triple_40: 4.9240 (5.0920)  time: 5.8958  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 220/1724]  eta: 2:27:52  lr: 0.000100  loss: 31.4079 (32.3019)  loss_n_40: 0.3799 (0.4431)  loss_n_60: 0.4173 (0.4618)  loss_n_80: 0.4768 (0.5214)  loss_n_100: 0.5185 (0.5765)  triple_100: 8.4856 (8.7131)  triple_80: 8.6734 (8.8189)  triple_60: 7.5350 (7.6789)  triple_40: 4.9232 (5.0882)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 230/1724]  eta: 2:26:53  lr: 0.000100  loss: 30.5358 (32.2570)  loss_n_40: 0.3614 (0.4402)  loss_n_60: 0.3926 (0.4595)  loss_n_80: 0.4457 (0.5189)  loss_n_100: 0.4945 (0.5737)  triple_100: 8.1496 (8.6981)  triple_80: 8.3319 (8.8053)  triple_60: 7.4143 (7.6708)  triple_40: 5.0212 (5.0905)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 240/1724]  eta: 2:25:54  lr: 0.000100  loss: 30.1796 (32.2436)  loss_n_40: 0.3614 (0.4399)  loss_n_60: 0.3926 (0.4590)  loss_n_80: 0.4446 (0.5183)  loss_n_100: 0.4943 (0.5730)  triple_100: 8.1465 (8.6908)  triple_80: 8.2047 (8.7994)  triple_60: 7.3098 (7.6688)  triple_40: 4.9629 (5.0943)  time: 5.8966  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [ 250/1724]  eta: 2:24:55  lr: 0.000100  loss: 30.0902 (32.1841)  loss_n_40: 0.3732 (0.4390)  loss_n_60: 0.4081 (0.4582)  loss_n_80: 0.4677 (0.5173)  loss_n_100: 0.5349 (0.5720)  triple_100: 8.1465 (8.6743)  triple_80: 8.2047 (8.7823)  triple_60: 7.3098 (7.6566)  triple_40: 4.8601 (5.0844)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 260/1724]  eta: 2:23:56  lr: 0.000100  loss: 30.0902 (32.1095)  loss_n_40: 0.3652 (0.4380)  loss_n_60: 0.4081 (0.4571)  loss_n_80: 0.4677 (0.5158)  loss_n_100: 0.5130 (0.5702)  triple_100: 8.0546 (8.6505)  triple_80: 8.1947 (8.7608)  triple_60: 7.2937 (7.6415)  triple_40: 4.7829 (5.0756)  time: 5.8963  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 270/1724]  eta: 2:22:57  lr: 0.000100  loss: 30.2953 (32.0724)  loss_n_40: 0.3824 (0.4362)  loss_n_60: 0.4220 (0.4561)  loss_n_80: 0.4694 (0.5147)  loss_n_100: 0.5130 (0.5688)  triple_100: 7.9774 (8.6393)  triple_80: 8.2286 (8.7508)  triple_60: 7.2917 (7.6349)  triple_40: 4.8954 (5.0716)  time: 5.8974  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 280/1724]  eta: 2:21:58  lr: 0.000100  loss: 30.6620 (32.0308)  loss_n_40: 0.3824 (0.4338)  loss_n_60: 0.4184 (0.4541)  loss_n_80: 0.4694 (0.5124)  loss_n_100: 0.5130 (0.5664)  triple_100: 8.0015 (8.6267)  triple_80: 8.2355 (8.7374)  triple_60: 7.4502 (7.6270)  triple_40: 5.0189 (5.0729)  time: 5.8986  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 290/1724]  eta: 2:20:59  lr: 0.000100  loss: 31.7756 (32.0265)  loss_n_40: 0.3844 (0.4348)  loss_n_60: 0.4211 (0.4544)  loss_n_80: 0.4884 (0.5125)  loss_n_100: 0.5444 (0.5661)  triple_100: 8.6598 (8.6220)  triple_80: 8.7162 (8.7352)  triple_60: 7.6144 (7.6270)  triple_40: 5.0351 (5.0744)  time: 5.8983  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 300/1724]  eta: 2:20:00  lr: 0.000100  loss: 31.4763 (32.0169)  loss_n_40: 0.4034 (0.4346)  loss_n_60: 0.4431 (0.4542)  loss_n_80: 0.5000 (0.5119)  loss_n_100: 0.5459 (0.5652)  triple_100: 8.3605 (8.6147)  triple_80: 8.5434 (8.7298)  triple_60: 7.6593 (7.6287)  triple_40: 4.9994 (5.0779)  time: 5.8981  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 310/1724]  eta: 2:19:01  lr: 0.000100  loss: 31.1379 (32.0380)  loss_n_40: 0.4186 (0.4349)  loss_n_60: 0.4431 (0.4549)  loss_n_80: 0.4960 (0.5124)  loss_n_100: 0.5403 (0.5658)  triple_100: 8.3564 (8.6201)  triple_80: 8.4608 (8.7350)  triple_60: 7.5005 (7.6356)  triple_40: 4.9994 (5.0793)  time: 5.8991  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 320/1724]  eta: 2:18:02  lr: 0.000100  loss: 32.7175 (32.0832)  loss_n_40: 0.4127 (0.4354)  loss_n_60: 0.4621 (0.4560)  loss_n_80: 0.5385 (0.5134)  loss_n_100: 0.5946 (0.5668)  triple_100: 8.8395 (8.6318)  triple_80: 8.9665 (8.7476)  triple_60: 7.8292 (7.6485)  triple_40: 5.0946 (5.0837)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 330/1724]  eta: 2:17:03  lr: 0.000100  loss: 32.8305 (32.1080)  loss_n_40: 0.4092 (0.4346)  loss_n_60: 0.4621 (0.4560)  loss_n_80: 0.5325 (0.5139)  loss_n_100: 0.5946 (0.5677)  triple_100: 8.9925 (8.6456)  triple_80: 9.0352 (8.7581)  triple_60: 7.8292 (7.6530)  triple_40: 5.0448 (5.0790)  time: 5.8988  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 340/1724]  eta: 2:16:04  lr: 0.000100  loss: 31.4773 (32.0788)  loss_n_40: 0.3923 (0.4338)  loss_n_60: 0.4284 (0.4551)  loss_n_80: 0.4907 (0.5128)  loss_n_100: 0.5562 (0.5665)  triple_100: 8.5018 (8.6379)  triple_80: 8.5592 (8.7501)  triple_60: 7.4191 (7.6475)  triple_40: 4.8665 (5.0751)  time: 5.8981  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 350/1724]  eta: 2:15:04  lr: 0.000100  loss: 31.4773 (32.0741)  loss_n_40: 0.3653 (0.4318)  loss_n_60: 0.4057 (0.4539)  loss_n_80: 0.4524 (0.5118)  loss_n_100: 0.5080 (0.5657)  triple_100: 8.4214 (8.6410)  triple_80: 8.5592 (8.7511)  triple_60: 7.6335 (7.6470)  triple_40: 4.9427 (5.0718)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 360/1724]  eta: 2:14:05  lr: 0.000100  loss: 31.7679 (32.0530)  loss_n_40: 0.3847 (0.4321)  loss_n_60: 0.4170 (0.4536)  loss_n_80: 0.4866 (0.5112)  loss_n_100: 0.5396 (0.5647)  triple_100: 8.4974 (8.6322)  triple_80: 8.6684 (8.7434)  triple_60: 7.6207 (7.6435)  triple_40: 4.9538 (5.0724)  time: 5.8958  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 370/1724]  eta: 2:13:06  lr: 0.000100  loss: 30.7884 (32.0328)  loss_n_40: 0.3927 (0.4311)  loss_n_60: 0.4126 (0.4530)  loss_n_80: 0.4636 (0.5106)  loss_n_100: 0.5115 (0.5641)  triple_100: 8.3698 (8.6272)  triple_80: 8.4109 (8.7384)  triple_60: 7.4373 (7.6402)  triple_40: 4.9617 (5.0683)  time: 5.8950  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 380/1724]  eta: 2:12:07  lr: 0.000100  loss: 30.5123 (31.9994)  loss_n_40: 0.3760 (0.4313)  loss_n_60: 0.4092 (0.4527)  loss_n_80: 0.4551 (0.5097)  loss_n_100: 0.5094 (0.5630)  triple_100: 8.2542 (8.6144)  triple_80: 8.3116 (8.7262)  triple_60: 7.3351 (7.6335)  triple_40: 4.8679 (5.0687)  time: 5.8946  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 390/1724]  eta: 2:11:08  lr: 0.000100  loss: 31.5681 (31.9833)  loss_n_40: 0.3760 (0.4297)  loss_n_60: 0.4105 (0.4516)  loss_n_80: 0.4721 (0.5087)  loss_n_100: 0.5289 (0.5622)  triple_100: 8.3687 (8.6124)  triple_80: 8.4954 (8.7227)  triple_60: 7.4946 (7.6293)  triple_40: 4.9090 (5.0666)  time: 5.8949  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 400/1724]  eta: 2:10:09  lr: 0.000100  loss: 31.2812 (31.9714)  loss_n_40: 0.3732 (0.4286)  loss_n_60: 0.4104 (0.4506)  loss_n_80: 0.4688 (0.5078)  loss_n_100: 0.5289 (0.5614)  triple_100: 8.4972 (8.6112)  triple_80: 8.5654 (8.7198)  triple_60: 7.4983 (7.6251)  triple_40: 4.9711 (5.0671)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 410/1724]  eta: 2:09:10  lr: 0.000100  loss: 30.8879 (31.9377)  loss_n_40: 0.3782 (0.4278)  loss_n_60: 0.3972 (0.4494)  loss_n_80: 0.4601 (0.5064)  loss_n_100: 0.5110 (0.5598)  triple_100: 8.2874 (8.6009)  triple_80: 8.4541 (8.7085)  triple_60: 7.3873 (7.6171)  triple_40: 5.0817 (5.0678)  time: 5.8945  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 420/1724]  eta: 2:08:11  lr: 0.000100  loss: 30.6730 (31.9416)  loss_n_40: 0.3971 (0.4277)  loss_n_60: 0.4006 (0.4495)  loss_n_80: 0.4450 (0.5063)  loss_n_100: 0.4985 (0.5595)  triple_100: 8.2163 (8.6007)  triple_80: 8.3590 (8.7089)  triple_60: 7.4104 (7.6208)  triple_40: 5.0832 (5.0681)  time: 5.8934  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 430/1724]  eta: 2:07:12  lr: 0.000100  loss: 30.8374 (31.9140)  loss_n_40: 0.4034 (0.4271)  loss_n_60: 0.4313 (0.4490)  loss_n_80: 0.4768 (0.5057)  loss_n_100: 0.5166 (0.5588)  triple_100: 8.2669 (8.5926)  triple_80: 8.3513 (8.7014)  triple_60: 7.4674 (7.6158)  triple_40: 4.8344 (5.0637)  time: 5.8952  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 440/1724]  eta: 2:06:13  lr: 0.000100  loss: 30.8374 (31.9029)  loss_n_40: 0.3953 (0.4270)  loss_n_60: 0.4314 (0.4486)  loss_n_80: 0.4768 (0.5052)  loss_n_100: 0.5168 (0.5581)  triple_100: 8.1940 (8.5876)  triple_80: 8.3206 (8.6968)  triple_60: 7.4517 (7.6142)  triple_40: 4.8180 (5.0653)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 450/1724]  eta: 2:05:13  lr: 0.000100  loss: 31.4063 (31.8825)  loss_n_40: 0.3973 (0.4271)  loss_n_60: 0.4314 (0.4482)  loss_n_80: 0.4772 (0.5047)  loss_n_100: 0.5213 (0.5576)  triple_100: 8.5228 (8.5817)  triple_80: 8.5258 (8.6903)  triple_60: 7.4474 (7.6087)  triple_40: 4.9869 (5.0643)  time: 5.8953  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 460/1724]  eta: 2:04:15  lr: 0.000100  loss: 32.2262 (31.8766)  loss_n_40: 0.3973 (0.4259)  loss_n_60: 0.4117 (0.4472)  loss_n_80: 0.4804 (0.5038)  loss_n_100: 0.5375 (0.5568)  triple_100: 8.7962 (8.5823)  triple_80: 8.8514 (8.6887)  triple_60: 7.6329 (7.6072)  triple_40: 4.9670 (5.0647)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 470/1724]  eta: 2:03:15  lr: 0.000100  loss: 30.2451 (31.8333)  loss_n_40: 0.3449 (0.4242)  loss_n_60: 0.3876 (0.4456)  loss_n_80: 0.4459 (0.5022)  loss_n_100: 0.5044 (0.5551)  triple_100: 8.2771 (8.5711)  triple_80: 8.2466 (8.6763)  triple_60: 7.1886 (7.5976)  triple_40: 4.9311 (5.0613)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 480/1724]  eta: 2:02:17  lr: 0.000100  loss: 30.3362 (31.8311)  loss_n_40: 0.3424 (0.4232)  loss_n_60: 0.3880 (0.4453)  loss_n_80: 0.4526 (0.5021)  loss_n_100: 0.5113 (0.5552)  triple_100: 8.3002 (8.5745)  triple_80: 8.3273 (8.6778)  triple_60: 7.2985 (7.5961)  triple_40: 4.8490 (5.0569)  time: 5.8977  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [ 490/1724]  eta: 2:01:18  lr: 0.000100  loss: 31.2939 (31.8526)  loss_n_40: 0.3964 (0.4237)  loss_n_60: 0.4352 (0.4457)  loss_n_80: 0.4973 (0.5026)  loss_n_100: 0.5623 (0.5558)  triple_100: 8.5088 (8.5804)  triple_80: 8.5295 (8.6833)  triple_60: 7.4545 (7.6004)  triple_40: 4.9325 (5.0605)  time: 5.9008  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 500/1724]  eta: 2:00:19  lr: 0.000100  loss: 31.2196 (31.8186)  loss_n_40: 0.3843 (0.4225)  loss_n_60: 0.4326 (0.4446)  loss_n_80: 0.4824 (0.5015)  loss_n_100: 0.5339 (0.5548)  triple_100: 8.5088 (8.5737)  triple_80: 8.5690 (8.6746)  triple_60: 7.3728 (7.5921)  triple_40: 4.9325 (5.0547)  time: 5.9012  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 510/1724]  eta: 1:59:20  lr: 0.000100  loss: 29.7553 (31.7857)  loss_n_40: 0.3497 (0.4218)  loss_n_60: 0.3949 (0.4440)  loss_n_80: 0.4653 (0.5007)  loss_n_100: 0.5154 (0.5539)  triple_100: 7.9575 (8.5632)  triple_80: 8.1148 (8.6643)  triple_60: 7.2378 (7.5862)  triple_40: 4.7980 (5.0516)  time: 5.8992  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 520/1724]  eta: 1:58:21  lr: 0.000100  loss: 29.7426 (31.7508)  loss_n_40: 0.3912 (0.4217)  loss_n_60: 0.3914 (0.4438)  loss_n_80: 0.4664 (0.5006)  loss_n_100: 0.5154 (0.5539)  triple_100: 8.0581 (8.5555)  triple_80: 8.1269 (8.6567)  triple_60: 7.0631 (7.5777)  triple_40: 4.5915 (5.0410)  time: 5.8985  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 530/1724]  eta: 1:57:22  lr: 0.000100  loss: 29.7735 (31.7257)  loss_n_40: 0.3866 (0.4215)  loss_n_60: 0.4082 (0.4437)  loss_n_80: 0.4713 (0.5005)  loss_n_100: 0.5342 (0.5537)  triple_100: 8.2423 (8.5478)  triple_80: 8.2641 (8.6498)  triple_60: 7.1156 (7.5732)  triple_40: 4.5213 (5.0355)  time: 5.8981  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 540/1724]  eta: 1:56:23  lr: 0.000100  loss: 30.4013 (31.7045)  loss_n_40: 0.3866 (0.4208)  loss_n_60: 0.4309 (0.4439)  loss_n_80: 0.5078 (0.5008)  loss_n_100: 0.5568 (0.5543)  triple_100: 8.2605 (8.5448)  triple_80: 8.3654 (8.6465)  triple_60: 7.2107 (7.5679)  triple_40: 4.5379 (5.0254)  time: 5.8978  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 550/1724]  eta: 1:55:24  lr: 0.000100  loss: 30.8560 (31.6972)  loss_n_40: 0.3721 (0.4202)  loss_n_60: 0.4459 (0.4440)  loss_n_80: 0.5158 (0.5012)  loss_n_100: 0.5761 (0.5548)  triple_100: 8.4265 (8.5442)  triple_80: 8.4559 (8.6461)  triple_60: 7.3610 (7.5664)  triple_40: 4.5944 (5.0203)  time: 5.8975  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 560/1724]  eta: 1:54:25  lr: 0.000100  loss: 30.7868 (31.6833)  loss_n_40: 0.3878 (0.4204)  loss_n_60: 0.4386 (0.4444)  loss_n_80: 0.5158 (0.5018)  loss_n_100: 0.5720 (0.5556)  triple_100: 8.3255 (8.5413)  triple_80: 8.5491 (8.6436)  triple_60: 7.2540 (7.5627)  triple_40: 4.6715 (5.0135)  time: 5.8970  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 570/1724]  eta: 1:53:26  lr: 0.000100  loss: 30.5103 (31.6771)  loss_n_40: 0.4025 (0.4198)  loss_n_60: 0.4263 (0.4447)  loss_n_80: 0.5451 (0.5027)  loss_n_100: 0.5900 (0.5566)  triple_100: 8.4449 (8.5444)  triple_80: 8.5491 (8.6458)  triple_60: 7.2177 (7.5594)  triple_40: 4.5640 (5.0036)  time: 5.8979  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 580/1724]  eta: 1:52:27  lr: 0.000100  loss: 30.8408 (31.7022)  loss_n_40: 0.3843 (0.4200)  loss_n_60: 0.4624 (0.4457)  loss_n_80: 0.5478 (0.5041)  loss_n_100: 0.6181 (0.5585)  triple_100: 8.4854 (8.5549)  triple_80: 8.5821 (8.6551)  triple_60: 7.2753 (7.5640)  triple_40: 4.5099 (4.9999)  time: 5.8974  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 590/1724]  eta: 1:51:28  lr: 0.000100  loss: 30.9202 (31.7126)  loss_n_40: 0.4094 (0.4202)  loss_n_60: 0.4750 (0.4465)  loss_n_80: 0.5678 (0.5052)  loss_n_100: 0.6450 (0.5599)  triple_100: 8.6510 (8.5608)  triple_80: 8.6179 (8.6608)  triple_60: 7.3962 (7.5645)  triple_40: 4.4962 (4.9947)  time: 5.8958  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 600/1724]  eta: 1:50:29  lr: 0.000100  loss: 30.9202 (31.6997)  loss_n_40: 0.4273 (0.4202)  loss_n_60: 0.4607 (0.4466)  loss_n_80: 0.5391 (0.5054)  loss_n_100: 0.5982 (0.5602)  triple_100: 8.6510 (8.5577)  triple_80: 8.6179 (8.6581)  triple_60: 7.2599 (7.5612)  triple_40: 4.6991 (4.9902)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 610/1724]  eta: 1:49:30  lr: 0.000100  loss: 31.2337 (31.7054)  loss_n_40: 0.4169 (0.4204)  loss_n_60: 0.4572 (0.4471)  loss_n_80: 0.5149 (0.5057)  loss_n_100: 0.5739 (0.5605)  triple_100: 8.4071 (8.5580)  triple_80: 8.4195 (8.6593)  triple_60: 7.4276 (7.5635)  triple_40: 4.7703 (4.9910)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 620/1724]  eta: 1:48:31  lr: 0.000100  loss: 31.2638 (31.7388)  loss_n_40: 0.4047 (0.4204)  loss_n_60: 0.4635 (0.4475)  loss_n_80: 0.5214 (0.5065)  loss_n_100: 0.5739 (0.5615)  triple_100: 8.4936 (8.5685)  triple_80: 8.6029 (8.6694)  triple_60: 7.5167 (7.5705)  triple_40: 4.9455 (4.9945)  time: 5.8978  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 630/1724]  eta: 1:47:32  lr: 0.000100  loss: 31.1225 (31.7127)  loss_n_40: 0.3685 (0.4199)  loss_n_60: 0.4237 (0.4469)  loss_n_80: 0.4851 (0.5059)  loss_n_100: 0.5374 (0.5608)  triple_100: 8.6017 (8.5616)  triple_80: 8.5860 (8.6624)  triple_60: 7.3859 (7.5637)  triple_40: 4.8753 (4.9915)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 640/1724]  eta: 1:46:33  lr: 0.000100  loss: 31.0934 (31.7239)  loss_n_40: 0.3555 (0.4196)  loss_n_60: 0.4016 (0.4467)  loss_n_80: 0.4694 (0.5059)  loss_n_100: 0.5255 (0.5609)  triple_100: 8.4684 (8.5654)  triple_80: 8.5313 (8.6664)  triple_60: 7.3859 (7.5664)  triple_40: 4.8753 (4.9926)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 650/1724]  eta: 1:45:34  lr: 0.000100  loss: 31.2375 (31.7269)  loss_n_40: 0.3506 (0.4187)  loss_n_60: 0.4017 (0.4464)  loss_n_80: 0.4694 (0.5056)  loss_n_100: 0.5253 (0.5607)  triple_100: 8.5975 (8.5673)  triple_80: 8.6433 (8.6678)  triple_60: 7.4496 (7.5682)  triple_40: 4.8298 (4.9922)  time: 5.8977  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 660/1724]  eta: 1:44:35  lr: 0.000100  loss: 31.2375 (31.7372)  loss_n_40: 0.3830 (0.4185)  loss_n_60: 0.4392 (0.4466)  loss_n_80: 0.5007 (0.5057)  loss_n_100: 0.5484 (0.5607)  triple_100: 8.5906 (8.5704)  triple_80: 8.6433 (8.6705)  triple_60: 7.5087 (7.5723)  triple_40: 4.9105 (4.9924)  time: 5.8982  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 670/1724]  eta: 1:43:36  lr: 0.000100  loss: 31.6202 (31.7437)  loss_n_40: 0.3938 (0.4183)  loss_n_60: 0.4446 (0.4468)  loss_n_80: 0.5180 (0.5060)  loss_n_100: 0.5659 (0.5610)  triple_100: 8.6007 (8.5731)  triple_80: 8.7589 (8.6731)  triple_60: 7.5983 (7.5748)  triple_40: 4.9483 (4.9906)  time: 5.8990  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 680/1724]  eta: 1:42:37  lr: 0.000100  loss: 31.1670 (31.7376)  loss_n_40: 0.3789 (0.4179)  loss_n_60: 0.4386 (0.4467)  loss_n_80: 0.4954 (0.5058)  loss_n_100: 0.5458 (0.5608)  triple_100: 8.4138 (8.5707)  triple_80: 8.5716 (8.6718)  triple_60: 7.5718 (7.5749)  triple_40: 4.7854 (4.9890)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 690/1724]  eta: 1:41:38  lr: 0.000100  loss: 31.7516 (31.7594)  loss_n_40: 0.4033 (0.4184)  loss_n_60: 0.4513 (0.4472)  loss_n_80: 0.4955 (0.5062)  loss_n_100: 0.5407 (0.5610)  triple_100: 8.3955 (8.5751)  triple_80: 8.5716 (8.6763)  triple_60: 7.6497 (7.5813)  triple_40: 4.9574 (4.9939)  time: 5.8972  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 700/1724]  eta: 1:40:39  lr: 0.000100  loss: 32.9711 (31.7711)  loss_n_40: 0.4400 (0.4184)  loss_n_60: 0.4719 (0.4473)  loss_n_80: 0.5309 (0.5062)  loss_n_100: 0.5871 (0.5610)  triple_100: 8.8918 (8.5779)  triple_80: 9.0453 (8.6792)  triple_60: 7.8897 (7.5848)  triple_40: 5.1646 (4.9962)  time: 5.8975  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 710/1724]  eta: 1:39:40  lr: 0.000100  loss: 32.3805 (31.7565)  loss_n_40: 0.3871 (0.4179)  loss_n_60: 0.4262 (0.4468)  loss_n_80: 0.4991 (0.5056)  loss_n_100: 0.5462 (0.5605)  triple_100: 8.6655 (8.5738)  triple_80: 8.8568 (8.6744)  triple_60: 7.6155 (7.5810)  triple_40: 5.0592 (4.9966)  time: 5.8978  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 720/1724]  eta: 1:38:41  lr: 0.000100  loss: 31.4737 (31.7558)  loss_n_40: 0.3807 (0.4179)  loss_n_60: 0.4368 (0.4467)  loss_n_80: 0.4991 (0.5056)  loss_n_100: 0.5755 (0.5604)  triple_100: 8.2617 (8.5736)  triple_80: 8.5606 (8.6744)  triple_60: 7.5643 (7.5806)  triple_40: 4.9482 (4.9967)  time: 5.8988  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [ 730/1724]  eta: 1:37:42  lr: 0.000100  loss: 32.2386 (31.7558)  loss_n_40: 0.3875 (0.4174)  loss_n_60: 0.4458 (0.4464)  loss_n_80: 0.5233 (0.5055)  loss_n_100: 0.5857 (0.5603)  triple_100: 8.6919 (8.5756)  triple_80: 8.7051 (8.6752)  triple_60: 7.6852 (7.5800)  triple_40: 4.9312 (4.9953)  time: 5.9001  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 740/1724]  eta: 1:36:43  lr: 0.000100  loss: 31.4146 (31.7711)  loss_n_40: 0.4033 (0.4177)  loss_n_60: 0.4458 (0.4468)  loss_n_80: 0.5211 (0.5059)  loss_n_100: 0.5804 (0.5608)  triple_100: 8.5290 (8.5804)  triple_80: 8.5701 (8.6800)  triple_60: 7.6292 (7.5837)  triple_40: 4.8695 (4.9959)  time: 5.9001  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 750/1724]  eta: 1:35:44  lr: 0.000100  loss: 30.8475 (31.7414)  loss_n_40: 0.3596 (0.4168)  loss_n_60: 0.3992 (0.4458)  loss_n_80: 0.4595 (0.5048)  loss_n_100: 0.5162 (0.5597)  triple_100: 8.3546 (8.5729)  triple_80: 8.4367 (8.6713)  triple_60: 7.3497 (7.5767)  triple_40: 4.8086 (4.9933)  time: 5.9009  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 760/1724]  eta: 1:34:45  lr: 0.000100  loss: 30.7001 (31.7483)  loss_n_40: 0.3515 (0.4166)  loss_n_60: 0.3886 (0.4458)  loss_n_80: 0.4505 (0.5048)  loss_n_100: 0.5104 (0.5596)  triple_100: 8.3975 (8.5749)  triple_80: 8.4367 (8.6728)  triple_60: 7.3610 (7.5793)  triple_40: 4.8086 (4.9945)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 770/1724]  eta: 1:33:46  lr: 0.000100  loss: 32.1580 (31.7593)  loss_n_40: 0.3845 (0.4164)  loss_n_60: 0.4279 (0.4459)  loss_n_80: 0.4929 (0.5049)  loss_n_100: 0.5522 (0.5599)  triple_100: 8.6905 (8.5789)  triple_80: 8.6883 (8.6765)  triple_60: 7.6142 (7.5823)  triple_40: 4.9451 (4.9945)  time: 5.9011  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 780/1724]  eta: 1:32:47  lr: 0.000100  loss: 32.5870 (31.7633)  loss_n_40: 0.3828 (0.4159)  loss_n_60: 0.4641 (0.4459)  loss_n_80: 0.5265 (0.5049)  loss_n_100: 0.5798 (0.5599)  triple_100: 8.8675 (8.5811)  triple_80: 9.0069 (8.6782)  triple_60: 7.7580 (7.5834)  triple_40: 4.9765 (4.9939)  time: 5.8999  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 790/1724]  eta: 1:31:48  lr: 0.000100  loss: 32.4411 (31.7743)  loss_n_40: 0.3858 (0.4162)  loss_n_60: 0.4611 (0.4463)  loss_n_80: 0.4979 (0.5053)  loss_n_100: 0.5557 (0.5603)  triple_100: 8.8290 (8.5830)  triple_80: 8.8869 (8.6808)  triple_60: 7.7814 (7.5870)  triple_40: 5.0192 (4.9954)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 800/1724]  eta: 1:30:49  lr: 0.000100  loss: 31.7709 (31.7924)  loss_n_40: 0.4165 (0.4171)  loss_n_60: 0.4626 (0.4471)  loss_n_80: 0.4979 (0.5060)  loss_n_100: 0.5543 (0.5609)  triple_100: 8.5509 (8.5857)  triple_80: 8.6912 (8.6846)  triple_60: 7.6753 (7.5918)  triple_40: 5.0192 (4.9992)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 810/1724]  eta: 1:29:50  lr: 0.000100  loss: 31.3057 (31.7884)  loss_n_40: 0.4227 (0.4173)  loss_n_60: 0.4626 (0.4472)  loss_n_80: 0.5100 (0.5060)  loss_n_100: 0.5543 (0.5608)  triple_100: 8.4531 (8.5823)  triple_80: 8.4829 (8.6826)  triple_60: 7.5716 (7.5919)  triple_40: 4.9852 (5.0003)  time: 5.9009  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 820/1724]  eta: 1:28:51  lr: 0.000100  loss: 31.3057 (31.7893)  loss_n_40: 0.4041 (0.4170)  loss_n_60: 0.4460 (0.4471)  loss_n_80: 0.5010 (0.5060)  loss_n_100: 0.5418 (0.5607)  triple_100: 8.4142 (8.5829)  triple_80: 8.4829 (8.6831)  triple_60: 7.5927 (7.5926)  triple_40: 4.9185 (4.9999)  time: 5.9009  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 830/1724]  eta: 1:27:53  lr: 0.000100  loss: 31.7634 (31.7935)  loss_n_40: 0.3900 (0.4173)  loss_n_60: 0.4252 (0.4472)  loss_n_80: 0.4820 (0.5060)  loss_n_100: 0.5341 (0.5607)  triple_100: 8.4142 (8.5824)  triple_80: 8.7085 (8.6833)  triple_60: 7.5927 (7.5937)  triple_40: 4.9659 (5.0030)  time: 5.9005  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 840/1724]  eta: 1:26:54  lr: 0.000100  loss: 30.2221 (31.7756)  loss_n_40: 0.4031 (0.4171)  loss_n_60: 0.4293 (0.4471)  loss_n_80: 0.4765 (0.5058)  loss_n_100: 0.5402 (0.5605)  triple_100: 8.0139 (8.5755)  triple_80: 8.3009 (8.6773)  triple_60: 7.2453 (7.5898)  triple_40: 4.9659 (5.0024)  time: 5.8996  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 850/1724]  eta: 1:25:55  lr: 0.000100  loss: 30.1847 (31.7679)  loss_n_40: 0.3803 (0.4171)  loss_n_60: 0.4293 (0.4473)  loss_n_80: 0.4902 (0.5061)  loss_n_100: 0.5427 (0.5608)  triple_100: 8.0742 (8.5734)  triple_80: 8.2604 (8.6756)  triple_60: 7.2120 (7.5876)  triple_40: 4.8044 (5.0000)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 860/1724]  eta: 1:24:56  lr: 0.000100  loss: 29.8691 (31.7469)  loss_n_40: 0.3659 (0.4167)  loss_n_60: 0.4024 (0.4470)  loss_n_80: 0.4625 (0.5057)  loss_n_100: 0.5265 (0.5605)  triple_100: 8.0680 (8.5670)  triple_80: 8.1680 (8.6692)  triple_60: 7.0963 (7.5824)  triple_40: 4.8021 (4.9986)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 870/1724]  eta: 1:23:57  lr: 0.000100  loss: 29.5396 (31.7238)  loss_n_40: 0.3667 (0.4170)  loss_n_60: 0.3998 (0.4469)  loss_n_80: 0.4592 (0.5054)  loss_n_100: 0.5171 (0.5602)  triple_100: 7.8433 (8.5589)  triple_80: 7.9209 (8.6608)  triple_60: 7.0332 (7.5770)  triple_40: 4.9427 (4.9977)  time: 5.9021  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 880/1724]  eta: 1:22:58  lr: 0.000100  loss: 30.6988 (31.7191)  loss_n_40: 0.4212 (0.4173)  loss_n_60: 0.4580 (0.4474)  loss_n_80: 0.4993 (0.5058)  loss_n_100: 0.5633 (0.5607)  triple_100: 8.2323 (8.5581)  triple_80: 8.2975 (8.6598)  triple_60: 7.3812 (7.5758)  triple_40: 4.8778 (4.9943)  time: 5.9026  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 890/1724]  eta: 1:21:59  lr: 0.000100  loss: 29.5835 (31.6849)  loss_n_40: 0.4086 (0.4172)  loss_n_60: 0.4302 (0.4470)  loss_n_80: 0.4826 (0.5053)  loss_n_100: 0.5532 (0.5602)  triple_100: 7.9418 (8.5465)  triple_80: 8.0321 (8.6494)  triple_60: 7.0739 (7.5677)  triple_40: 4.7368 (4.9917)  time: 5.9037  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 900/1724]  eta: 1:21:00  lr: 0.000100  loss: 29.1237 (31.6691)  loss_n_40: 0.3876 (0.4170)  loss_n_60: 0.4097 (0.4468)  loss_n_80: 0.4613 (0.5051)  loss_n_100: 0.5292 (0.5601)  triple_100: 7.8032 (8.5423)  triple_80: 7.8533 (8.6445)  triple_60: 6.9426 (7.5631)  triple_40: 4.6981 (4.9901)  time: 5.9034  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 910/1724]  eta: 1:20:01  lr: 0.000100  loss: 30.3448 (31.6621)  loss_n_40: 0.3742 (0.4167)  loss_n_60: 0.4219 (0.4467)  loss_n_80: 0.4847 (0.5051)  loss_n_100: 0.5451 (0.5601)  triple_100: 8.1251 (8.5407)  triple_80: 8.3303 (8.6429)  triple_60: 7.1888 (7.5613)  triple_40: 4.7124 (4.9886)  time: 5.9020  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 920/1724]  eta: 1:19:02  lr: 0.000100  loss: 28.9506 (31.6268)  loss_n_40: 0.3688 (0.4163)  loss_n_60: 0.4050 (0.4462)  loss_n_80: 0.4740 (0.5045)  loss_n_100: 0.5056 (0.5593)  triple_100: 7.7577 (8.5297)  triple_80: 7.8548 (8.6327)  triple_60: 6.9331 (7.5533)  triple_40: 4.7124 (4.9848)  time: 5.9019  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 930/1724]  eta: 1:18:03  lr: 0.000100  loss: 28.7205 (31.6267)  loss_n_40: 0.3705 (0.4163)  loss_n_60: 0.4082 (0.4461)  loss_n_80: 0.4434 (0.5043)  loss_n_100: 0.4879 (0.5591)  triple_100: 7.7010 (8.5294)  triple_80: 7.8060 (8.6325)  triple_60: 6.9564 (7.5540)  triple_40: 4.7335 (4.9850)  time: 5.9018  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 940/1724]  eta: 1:17:04  lr: 0.000100  loss: 30.7713 (31.6171)  loss_n_40: 0.3855 (0.4160)  loss_n_60: 0.4418 (0.4459)  loss_n_80: 0.4878 (0.5040)  loss_n_100: 0.5311 (0.5587)  triple_100: 8.2438 (8.5260)  triple_80: 8.3945 (8.6297)  triple_60: 7.4272 (7.5525)  triple_40: 4.8385 (4.9843)  time: 5.9019  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 950/1724]  eta: 1:16:05  lr: 0.000100  loss: 31.0752 (31.6156)  loss_n_40: 0.3898 (0.4159)  loss_n_60: 0.4418 (0.4459)  loss_n_80: 0.5058 (0.5041)  loss_n_100: 0.5564 (0.5590)  triple_100: 8.3922 (8.5278)  triple_80: 8.5230 (8.6306)  triple_60: 7.4850 (7.5518)  triple_40: 4.7607 (4.9805)  time: 5.9018  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 960/1724]  eta: 1:15:06  lr: 0.000100  loss: 31.2122 (31.6094)  loss_n_40: 0.3847 (0.4158)  loss_n_60: 0.4385 (0.4458)  loss_n_80: 0.5061 (0.5041)  loss_n_100: 0.5626 (0.5589)  triple_100: 8.6369 (8.5272)  triple_80: 8.6845 (8.6298)  triple_60: 7.4774 (7.5503)  triple_40: 4.6046 (4.9774)  time: 5.9003  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [ 970/1724]  eta: 1:14:07  lr: 0.000100  loss: 30.9330 (31.5937)  loss_n_40: 0.3770 (0.4156)  loss_n_60: 0.4146 (0.4455)  loss_n_80: 0.4882 (0.5037)  loss_n_100: 0.5395 (0.5584)  triple_100: 8.3434 (8.5224)  triple_80: 8.4820 (8.6253)  triple_60: 7.3839 (7.5470)  triple_40: 4.7602 (4.9760)  time: 5.8996  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 980/1724]  eta: 1:13:08  lr: 0.000100  loss: 30.5137 (31.5842)  loss_n_40: 0.3518 (0.4152)  loss_n_60: 0.3947 (0.4452)  loss_n_80: 0.4524 (0.5033)  loss_n_100: 0.5058 (0.5579)  triple_100: 8.1539 (8.5197)  triple_80: 8.2152 (8.6223)  triple_60: 7.1985 (7.5453)  triple_40: 4.7994 (4.9754)  time: 5.9003  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [ 990/1724]  eta: 1:12:09  lr: 0.000100  loss: 29.6620 (31.5675)  loss_n_40: 0.3574 (0.4148)  loss_n_60: 0.3932 (0.4447)  loss_n_80: 0.4388 (0.5027)  loss_n_100: 0.4794 (0.5573)  triple_100: 7.8743 (8.5145)  triple_80: 8.0540 (8.6173)  triple_60: 7.1282 (7.5414)  triple_40: 4.8759 (4.9749)  time: 5.8998  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1000/1724]  eta: 1:11:10  lr: 0.000100  loss: 29.9123 (31.5743)  loss_n_40: 0.3954 (0.4150)  loss_n_60: 0.4293 (0.4449)  loss_n_80: 0.4988 (0.5029)  loss_n_100: 0.5438 (0.5574)  triple_100: 8.1131 (8.5154)  triple_80: 8.2470 (8.6191)  triple_60: 7.2778 (7.5438)  triple_40: 4.9655 (4.9759)  time: 5.8986  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1010/1724]  eta: 1:10:11  lr: 0.000100  loss: 30.1143 (31.5566)  loss_n_40: 0.3905 (0.4145)  loss_n_60: 0.4098 (0.4444)  loss_n_80: 0.4773 (0.5023)  loss_n_100: 0.5413 (0.5569)  triple_100: 8.1469 (8.5103)  triple_80: 8.2643 (8.6143)  triple_60: 7.2961 (7.5398)  triple_40: 4.8978 (4.9742)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1020/1724]  eta: 1:09:12  lr: 0.000100  loss: 29.3563 (31.5396)  loss_n_40: 0.3635 (0.4145)  loss_n_60: 0.3866 (0.4441)  loss_n_80: 0.4276 (0.5019)  loss_n_100: 0.4852 (0.5564)  triple_100: 7.9225 (8.5039)  triple_80: 7.9393 (8.6084)  triple_60: 6.9976 (7.5358)  triple_40: 4.8262 (4.9746)  time: 5.8982  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:6]  [1030/1724]  eta: 1:08:13  lr: 0.000100  loss: 29.6043 (31.5246)  loss_n_40: 0.3783 (0.4145)  loss_n_60: 0.4018 (0.4439)  loss_n_80: 0.4381 (0.5016)  loss_n_100: 0.4877 (0.5560)  triple_100: 7.8401 (8.4986)  triple_80: 7.9393 (8.6036)  triple_60: 7.0664 (7.5322)  triple_40: 4.9031 (4.9741)  time: 5.8970  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1040/1724]  eta: 1:07:14  lr: 0.000100  loss: 29.8130 (31.5170)  loss_n_40: 0.3783 (0.4141)  loss_n_60: 0.4182 (0.4437)  loss_n_80: 0.4636 (0.5014)  loss_n_100: 0.5282 (0.5558)  triple_100: 8.1171 (8.4968)  triple_80: 8.2186 (8.6017)  triple_60: 7.1117 (7.5309)  triple_40: 4.7992 (4.9726)  time: 5.8959  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1050/1724]  eta: 1:06:15  lr: 0.000100  loss: 30.7412 (31.5133)  loss_n_40: 0.3794 (0.4140)  loss_n_60: 0.4218 (0.4437)  loss_n_80: 0.4721 (0.5015)  loss_n_100: 0.5210 (0.5558)  triple_100: 8.1773 (8.4954)  triple_80: 8.3024 (8.6008)  triple_60: 7.2166 (7.5297)  triple_40: 4.8107 (4.9723)  time: 5.8961  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1060/1724]  eta: 1:05:16  lr: 0.000100  loss: 30.0710 (31.4963)  loss_n_40: 0.4143 (0.4146)  loss_n_60: 0.4301 (0.4438)  loss_n_80: 0.4796 (0.5015)  loss_n_100: 0.5155 (0.5557)  triple_100: 7.9006 (8.4883)  triple_80: 8.0874 (8.5950)  triple_60: 7.0962 (7.5253)  triple_40: 4.9148 (4.9721)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1070/1724]  eta: 1:04:17  lr: 0.000100  loss: 30.0233 (31.4883)  loss_n_40: 0.4144 (0.4147)  loss_n_60: 0.4414 (0.4440)  loss_n_80: 0.4889 (0.5017)  loss_n_100: 0.5257 (0.5559)  triple_100: 7.9006 (8.4863)  triple_80: 8.0576 (8.5928)  triple_60: 7.0962 (7.5229)  triple_40: 4.8036 (4.9700)  time: 5.8963  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1080/1724]  eta: 1:03:18  lr: 0.000100  loss: 30.1149 (31.4793)  loss_n_40: 0.4063 (0.4149)  loss_n_60: 0.4570 (0.4441)  loss_n_80: 0.5011 (0.5017)  loss_n_100: 0.5544 (0.5560)  triple_100: 8.0581 (8.4826)  triple_80: 8.1906 (8.5891)  triple_60: 7.2901 (7.5208)  triple_40: 4.8612 (4.9701)  time: 5.8985  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1090/1724]  eta: 1:02:19  lr: 0.000100  loss: 29.1335 (31.4606)  loss_n_40: 0.4300 (0.4159)  loss_n_60: 0.4775 (0.4445)  loss_n_80: 0.4786 (0.5018)  loss_n_100: 0.5468 (0.5558)  triple_100: 7.5598 (8.4736)  triple_80: 7.6834 (8.5814)  triple_60: 6.9834 (7.5165)  triple_40: 5.0230 (4.9710)  time: 5.8975  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1100/1724]  eta: 1:01:20  lr: 0.000100  loss: 29.1283 (31.4496)  loss_n_40: 0.4178 (0.4161)  loss_n_60: 0.4439 (0.4445)  loss_n_80: 0.4736 (0.5018)  loss_n_100: 0.5336 (0.5558)  triple_100: 7.5598 (8.4692)  triple_80: 7.8036 (8.5777)  triple_60: 6.9429 (7.5135)  triple_40: 4.9041 (4.9709)  time: 5.8969  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1110/1724]  eta: 1:00:21  lr: 0.000100  loss: 29.5659 (31.4447)  loss_n_40: 0.3987 (0.4160)  loss_n_60: 0.4399 (0.4447)  loss_n_80: 0.4968 (0.5021)  loss_n_100: 0.5487 (0.5561)  triple_100: 7.8605 (8.4682)  triple_80: 8.1027 (8.5768)  triple_60: 7.0215 (7.5121)  triple_40: 4.6923 (4.9688)  time: 5.8978  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1120/1724]  eta: 0:59:22  lr: 0.000100  loss: 29.5335 (31.4232)  loss_n_40: 0.3987 (0.4161)  loss_n_60: 0.4379 (0.4447)  loss_n_80: 0.4905 (0.5019)  loss_n_100: 0.5404 (0.5559)  triple_100: 7.8419 (8.4616)  triple_80: 8.0978 (8.5706)  triple_60: 7.0215 (7.5070)  triple_40: 4.5375 (4.9653)  time: 5.8985  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1130/1724]  eta: 0:58:23  lr: 0.000100  loss: 29.4783 (31.4143)  loss_n_40: 0.3976 (0.4161)  loss_n_60: 0.4260 (0.4448)  loss_n_80: 0.4859 (0.5021)  loss_n_100: 0.5489 (0.5561)  triple_100: 7.8419 (8.4585)  triple_80: 8.0978 (8.5686)  triple_60: 6.9232 (7.5047)  triple_40: 4.4756 (4.9633)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1140/1724]  eta: 0:57:24  lr: 0.000100  loss: 31.4145 (31.4109)  loss_n_40: 0.4396 (0.4164)  loss_n_60: 0.4717 (0.4451)  loss_n_80: 0.5086 (0.5023)  loss_n_100: 0.5703 (0.5563)  triple_100: 8.3996 (8.4567)  triple_80: 8.5397 (8.5673)  triple_60: 7.4679 (7.5038)  triple_40: 4.8284 (4.9630)  time: 5.8996  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1150/1724]  eta: 0:56:25  lr: 0.000100  loss: 30.0782 (31.3934)  loss_n_40: 0.3943 (0.4164)  loss_n_60: 0.4314 (0.4449)  loss_n_80: 0.4855 (0.5021)  loss_n_100: 0.5411 (0.5559)  triple_100: 8.1886 (8.4512)  triple_80: 8.2307 (8.5621)  triple_60: 7.1321 (7.4994)  triple_40: 4.8623 (4.9614)  time: 5.9001  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1160/1724]  eta: 0:55:26  lr: 0.000100  loss: 30.0782 (31.3928)  loss_n_40: 0.4039 (0.4167)  loss_n_60: 0.4314 (0.4451)  loss_n_80: 0.4788 (0.5022)  loss_n_100: 0.5292 (0.5560)  triple_100: 7.9882 (8.4495)  triple_80: 8.2307 (8.5613)  triple_60: 7.1321 (7.4994)  triple_40: 4.9808 (4.9627)  time: 5.8990  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1170/1724]  eta: 0:54:27  lr: 0.000100  loss: 31.3686 (31.3856)  loss_n_40: 0.3979 (0.4165)  loss_n_60: 0.4333 (0.4450)  loss_n_80: 0.5041 (0.5020)  loss_n_100: 0.5568 (0.5558)  triple_100: 8.3414 (8.4474)  triple_80: 8.5415 (8.5591)  triple_60: 7.4613 (7.4980)  triple_40: 4.8801 (4.9617)  time: 5.8990  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1180/1724]  eta: 0:53:28  lr: 0.000100  loss: 30.4136 (31.3841)  loss_n_40: 0.3919 (0.4167)  loss_n_60: 0.4295 (0.4451)  loss_n_80: 0.4702 (0.5020)  loss_n_100: 0.5068 (0.5557)  triple_100: 8.0747 (8.4454)  triple_80: 8.3380 (8.5581)  triple_60: 7.3403 (7.4984)  triple_40: 4.8211 (4.9628)  time: 5.8992  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1190/1724]  eta: 0:52:29  lr: 0.000100  loss: 30.4136 (31.3845)  loss_n_40: 0.3739 (0.4168)  loss_n_60: 0.4356 (0.4451)  loss_n_80: 0.4636 (0.5019)  loss_n_100: 0.5068 (0.5556)  triple_100: 8.1445 (8.4450)  triple_80: 8.3380 (8.5580)  triple_60: 7.3403 (7.4988)  triple_40: 4.9585 (4.9634)  time: 5.8988  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1200/1724]  eta: 0:51:30  lr: 0.000100  loss: 30.4759 (31.3911)  loss_n_40: 0.3636 (0.4165)  loss_n_60: 0.4274 (0.4450)  loss_n_80: 0.4636 (0.5019)  loss_n_100: 0.5163 (0.5556)  triple_100: 8.2720 (8.4475)  triple_80: 8.3628 (8.5601)  triple_60: 7.3540 (7.5004)  triple_40: 4.9315 (4.9641)  time: 5.8988  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [1210/1724]  eta: 0:50:31  lr: 0.000100  loss: 30.2743 (31.3847)  loss_n_40: 0.3800 (0.4162)  loss_n_60: 0.4174 (0.4447)  loss_n_80: 0.4761 (0.5016)  loss_n_100: 0.5390 (0.5552)  triple_100: 8.2954 (8.4459)  triple_80: 8.3128 (8.5582)  triple_60: 7.2938 (7.4992)  triple_40: 4.9198 (4.9636)  time: 5.8989  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1220/1724]  eta: 0:49:32  lr: 0.000100  loss: 30.8102 (31.3883)  loss_n_40: 0.3943 (0.4165)  loss_n_60: 0.4174 (0.4448)  loss_n_80: 0.4761 (0.5017)  loss_n_100: 0.5390 (0.5552)  triple_100: 8.3298 (8.4465)  triple_80: 8.3837 (8.5592)  triple_60: 7.4131 (7.5001)  triple_40: 4.8735 (4.9642)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1230/1724]  eta: 0:48:33  lr: 0.000100  loss: 31.8473 (31.3993)  loss_n_40: 0.4181 (0.4166)  loss_n_60: 0.4477 (0.4451)  loss_n_80: 0.4884 (0.5020)  loss_n_100: 0.5418 (0.5554)  triple_100: 8.7106 (8.4489)  triple_80: 8.7725 (8.5620)  triple_60: 7.5762 (7.5036)  triple_40: 4.9015 (4.9656)  time: 5.8944  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1240/1724]  eta: 0:47:34  lr: 0.000100  loss: 31.1065 (31.4005)  loss_n_40: 0.4064 (0.4166)  loss_n_60: 0.4386 (0.4451)  loss_n_80: 0.4828 (0.5019)  loss_n_100: 0.5291 (0.5553)  triple_100: 8.3660 (8.4488)  triple_80: 8.5448 (8.5620)  triple_60: 7.5399 (7.5041)  triple_40: 5.1102 (4.9666)  time: 5.8952  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1250/1724]  eta: 0:46:35  lr: 0.000100  loss: 31.1602 (31.4103)  loss_n_40: 0.3876 (0.4165)  loss_n_60: 0.4304 (0.4451)  loss_n_80: 0.4667 (0.5018)  loss_n_100: 0.5064 (0.5552)  triple_100: 8.3323 (8.4504)  triple_80: 8.4826 (8.5639)  triple_60: 7.5489 (7.5074)  triple_40: 5.1517 (4.9701)  time: 5.8973  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1260/1724]  eta: 0:45:36  lr: 0.000100  loss: 31.5814 (31.4118)  loss_n_40: 0.3573 (0.4160)  loss_n_60: 0.3990 (0.4448)  loss_n_80: 0.4623 (0.5017)  loss_n_100: 0.5236 (0.5552)  triple_100: 8.5710 (8.4532)  triple_80: 8.7065 (8.5658)  triple_60: 7.6031 (7.5071)  triple_40: 4.8622 (4.9680)  time: 5.8985  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1270/1724]  eta: 0:44:37  lr: 0.000100  loss: 31.6848 (31.4108)  loss_n_40: 0.3549 (0.4159)  loss_n_60: 0.4224 (0.4448)  loss_n_80: 0.4871 (0.5017)  loss_n_100: 0.5376 (0.5552)  triple_100: 8.6910 (8.4534)  triple_80: 8.7600 (8.5660)  triple_60: 7.6038 (7.5073)  triple_40: 4.6540 (4.9666)  time: 5.8966  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1280/1724]  eta: 0:43:38  lr: 0.000100  loss: 31.4004 (31.4218)  loss_n_40: 0.3835 (0.4158)  loss_n_60: 0.4541 (0.4451)  loss_n_80: 0.5148 (0.5021)  loss_n_100: 0.5812 (0.5556)  triple_100: 8.6335 (8.4580)  triple_80: 8.6983 (8.5700)  triple_60: 7.6715 (7.5098)  triple_40: 4.6747 (4.9654)  time: 5.8944  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1290/1724]  eta: 0:42:39  lr: 0.000100  loss: 30.6605 (31.4124)  loss_n_40: 0.3643 (0.4153)  loss_n_60: 0.4319 (0.4447)  loss_n_80: 0.5036 (0.5017)  loss_n_100: 0.5717 (0.5552)  triple_100: 8.4919 (8.4568)  triple_80: 8.4975 (8.5681)  triple_60: 7.3479 (7.5077)  triple_40: 4.6168 (4.9630)  time: 5.8952  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1300/1724]  eta: 0:41:40  lr: 0.000100  loss: 30.7317 (31.4134)  loss_n_40: 0.3445 (0.4150)  loss_n_60: 0.3894 (0.4445)  loss_n_80: 0.4568 (0.5015)  loss_n_100: 0.5185 (0.5550)  triple_100: 8.4611 (8.4576)  triple_80: 8.5047 (8.5688)  triple_60: 7.2826 (7.5083)  triple_40: 4.7017 (4.9628)  time: 5.8967  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1310/1724]  eta: 0:40:41  lr: 0.000100  loss: 31.1466 (31.4079)  loss_n_40: 0.3445 (0.4145)  loss_n_60: 0.3894 (0.4440)  loss_n_80: 0.4347 (0.5010)  loss_n_100: 0.4899 (0.5544)  triple_100: 8.3367 (8.4559)  triple_80: 8.5062 (8.5670)  triple_60: 7.4638 (7.5074)  triple_40: 4.9899 (4.9638)  time: 5.8964  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1320/1724]  eta: 0:39:42  lr: 0.000100  loss: 30.0646 (31.4010)  loss_n_40: 0.3468 (0.4142)  loss_n_60: 0.3853 (0.4437)  loss_n_80: 0.4326 (0.5006)  loss_n_100: 0.4769 (0.5540)  triple_100: 8.0828 (8.4539)  triple_80: 8.1412 (8.5648)  triple_60: 7.2889 (7.5061)  triple_40: 4.9685 (4.9637)  time: 5.8955  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1330/1724]  eta: 0:38:43  lr: 0.000100  loss: 30.8779 (31.3970)  loss_n_40: 0.3468 (0.4138)  loss_n_60: 0.3923 (0.4433)  loss_n_80: 0.4558 (0.5002)  loss_n_100: 0.4988 (0.5536)  triple_100: 8.3306 (8.4529)  triple_80: 8.4411 (8.5636)  triple_60: 7.4478 (7.5056)  triple_40: 4.9217 (4.9640)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1340/1724]  eta: 0:37:44  lr: 0.000100  loss: 31.0067 (31.3918)  loss_n_40: 0.3346 (0.4135)  loss_n_60: 0.3923 (0.4431)  loss_n_80: 0.4601 (0.5000)  loss_n_100: 0.5151 (0.5533)  triple_100: 8.5171 (8.4520)  triple_80: 8.5476 (8.5621)  triple_60: 7.4499 (7.5047)  triple_40: 4.9270 (4.9631)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1350/1724]  eta: 0:36:45  lr: 0.000100  loss: 29.5332 (31.3784)  loss_n_40: 0.3487 (0.4131)  loss_n_60: 0.3907 (0.4426)  loss_n_80: 0.4414 (0.4994)  loss_n_100: 0.4838 (0.5527)  triple_100: 7.9223 (8.4484)  triple_80: 8.0230 (8.5584)  triple_60: 7.1042 (7.5018)  triple_40: 4.7894 (4.9620)  time: 5.8963  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1360/1724]  eta: 0:35:46  lr: 0.000100  loss: 29.4265 (31.3713)  loss_n_40: 0.3527 (0.4126)  loss_n_60: 0.3836 (0.4423)  loss_n_80: 0.4312 (0.4991)  loss_n_100: 0.4838 (0.5524)  triple_100: 7.8409 (8.4468)  triple_80: 7.9780 (8.5566)  triple_60: 7.1042 (7.5004)  triple_40: 4.7714 (4.9611)  time: 5.8973  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1370/1724]  eta: 0:34:48  lr: 0.000100  loss: 29.9480 (31.3615)  loss_n_40: 0.3522 (0.4122)  loss_n_60: 0.3713 (0.4418)  loss_n_80: 0.4312 (0.4986)  loss_n_100: 0.4882 (0.5519)  triple_100: 8.1299 (8.4448)  triple_80: 8.2070 (8.5540)  triple_60: 7.1332 (7.4981)  triple_40: 4.7965 (4.9603)  time: 5.8968  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1380/1724]  eta: 0:33:49  lr: 0.000100  loss: 30.3612 (31.3640)  loss_n_40: 0.3452 (0.4120)  loss_n_60: 0.3907 (0.4417)  loss_n_80: 0.4572 (0.4986)  loss_n_100: 0.5158 (0.5519)  triple_100: 8.3103 (8.4462)  triple_80: 8.3878 (8.5550)  triple_60: 7.2500 (7.4981)  triple_40: 4.7956 (4.9604)  time: 5.8955  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1390/1724]  eta: 0:32:50  lr: 0.000100  loss: 30.9373 (31.3609)  loss_n_40: 0.3494 (0.4117)  loss_n_60: 0.4067 (0.4415)  loss_n_80: 0.4735 (0.4985)  loss_n_100: 0.5404 (0.5518)  triple_100: 8.4802 (8.4469)  triple_80: 8.5976 (8.5552)  triple_60: 7.2889 (7.4973)  triple_40: 4.7069 (4.9581)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1400/1724]  eta: 0:31:51  lr: 0.000100  loss: 30.8253 (31.3668)  loss_n_40: 0.3458 (0.4113)  loss_n_60: 0.4194 (0.4414)  loss_n_80: 0.4696 (0.4983)  loss_n_100: 0.5136 (0.5517)  triple_100: 8.5039 (8.4492)  triple_80: 8.4655 (8.5571)  triple_60: 7.2942 (7.4991)  triple_40: 4.7512 (4.9587)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1410/1724]  eta: 0:30:52  lr: 0.000100  loss: 31.1404 (31.3781)  loss_n_40: 0.3506 (0.4111)  loss_n_60: 0.3989 (0.4414)  loss_n_80: 0.4513 (0.4983)  loss_n_100: 0.5155 (0.5517)  triple_100: 8.5408 (8.4525)  triple_80: 8.4933 (8.5601)  triple_60: 7.5426 (7.5022)  triple_40: 4.9898 (4.9608)  time: 5.8997  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1420/1724]  eta: 0:29:53  lr: 0.000100  loss: 31.1404 (31.3700)  loss_n_40: 0.3408 (0.4106)  loss_n_60: 0.3950 (0.4410)  loss_n_80: 0.4511 (0.4980)  loss_n_100: 0.5177 (0.5514)  triple_100: 8.5408 (8.4506)  triple_80: 8.4933 (8.5581)  triple_60: 7.4201 (7.5005)  triple_40: 4.9034 (4.9599)  time: 5.9014  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1430/1724]  eta: 0:28:54  lr: 0.000100  loss: 30.3966 (31.3667)  loss_n_40: 0.3271 (0.4100)  loss_n_60: 0.3762 (0.4406)  loss_n_80: 0.4245 (0.4976)  loss_n_100: 0.4749 (0.5509)  triple_100: 8.2475 (8.4506)  triple_80: 8.3011 (8.5578)  triple_60: 7.3403 (7.5005)  triple_40: 4.7429 (4.9588)  time: 5.9009  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1440/1724]  eta: 0:27:55  lr: 0.000100  loss: 31.1244 (31.3667)  loss_n_40: 0.3339 (0.4098)  loss_n_60: 0.3878 (0.4403)  loss_n_80: 0.4483 (0.4973)  loss_n_100: 0.4943 (0.5506)  triple_100: 8.3419 (8.4505)  triple_80: 8.4797 (8.5575)  triple_60: 7.4394 (7.5009)  triple_40: 4.8992 (4.9598)  time: 5.8991  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [1450/1724]  eta: 0:26:56  lr: 0.000100  loss: 31.1244 (31.3632)  loss_n_40: 0.3515 (0.4094)  loss_n_60: 0.3902 (0.4400)  loss_n_80: 0.4483 (0.4970)  loss_n_100: 0.4943 (0.5503)  triple_100: 8.3419 (8.4494)  triple_80: 8.4797 (8.5567)  triple_60: 7.4394 (7.5007)  triple_40: 4.9222 (4.9598)  time: 5.8993  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1460/1724]  eta: 0:25:57  lr: 0.000100  loss: 30.5090 (31.3617)  loss_n_40: 0.3486 (0.4092)  loss_n_60: 0.3884 (0.4399)  loss_n_80: 0.4403 (0.4969)  loss_n_100: 0.4939 (0.5501)  triple_100: 8.2739 (8.4489)  triple_80: 8.3327 (8.5563)  triple_60: 7.3554 (7.5006)  triple_40: 4.8673 (4.9597)  time: 5.8991  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1470/1724]  eta: 0:24:58  lr: 0.000100  loss: 30.8477 (31.3710)  loss_n_40: 0.3871 (0.4093)  loss_n_60: 0.4159 (0.4400)  loss_n_80: 0.4690 (0.4970)  loss_n_100: 0.5107 (0.5502)  triple_100: 8.2859 (8.4508)  triple_80: 8.4321 (8.5586)  triple_60: 7.5341 (7.5032)  triple_40: 4.9365 (4.9619)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1480/1724]  eta: 0:23:59  lr: 0.000100  loss: 31.2219 (31.3798)  loss_n_40: 0.3986 (0.4093)  loss_n_60: 0.4457 (0.4402)  loss_n_80: 0.5183 (0.4973)  loss_n_100: 0.5840 (0.5506)  triple_100: 8.6895 (8.4539)  triple_80: 8.7075 (8.5615)  triple_60: 7.5341 (7.5045)  triple_40: 4.9989 (4.9626)  time: 5.8984  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1490/1724]  eta: 0:23:00  lr: 0.000100  loss: 31.0725 (31.3797)  loss_n_40: 0.3923 (0.4091)  loss_n_60: 0.4533 (0.4401)  loss_n_80: 0.5336 (0.4973)  loss_n_100: 0.5816 (0.5507)  triple_100: 8.6528 (8.4554)  triple_80: 8.5933 (8.5623)  triple_60: 7.3723 (7.5040)  triple_40: 4.7737 (4.9608)  time: 5.8992  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1500/1724]  eta: 0:22:01  lr: 0.000100  loss: 31.4129 (31.3890)  loss_n_40: 0.3618 (0.4089)  loss_n_60: 0.4442 (0.4402)  loss_n_80: 0.5039 (0.4975)  loss_n_100: 0.5750 (0.5510)  triple_100: 8.7022 (8.4597)  triple_80: 8.7949 (8.5660)  triple_60: 7.4190 (7.5059)  triple_40: 4.6903 (4.9596)  time: 5.9000  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1510/1724]  eta: 0:21:02  lr: 0.000100  loss: 31.6638 (31.3886)  loss_n_40: 0.3614 (0.4086)  loss_n_60: 0.4099 (0.4400)  loss_n_80: 0.4928 (0.4973)  loss_n_100: 0.5615 (0.5508)  triple_100: 8.7022 (8.4596)  triple_80: 8.7949 (8.5660)  triple_60: 7.4631 (7.5063)  triple_40: 4.8299 (4.9598)  time: 5.8993  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1520/1724]  eta: 0:20:03  lr: 0.000100  loss: 30.5694 (31.3885)  loss_n_40: 0.3672 (0.4085)  loss_n_60: 0.4069 (0.4399)  loss_n_80: 0.4418 (0.4971)  loss_n_100: 0.5032 (0.5506)  triple_100: 8.3042 (8.4594)  triple_80: 8.3147 (8.5654)  triple_60: 7.3702 (7.5065)  triple_40: 4.9859 (4.9610)  time: 5.8987  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1530/1724]  eta: 0:19:04  lr: 0.000100  loss: 29.9403 (31.3777)  loss_n_40: 0.3518 (0.4083)  loss_n_60: 0.3935 (0.4396)  loss_n_80: 0.4470 (0.4968)  loss_n_100: 0.5061 (0.5502)  triple_100: 8.2210 (8.4566)  triple_80: 8.2942 (8.5624)  triple_60: 7.1537 (7.5042)  triple_40: 4.9659 (4.9596)  time: 5.8995  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1540/1724]  eta: 0:18:05  lr: 0.000100  loss: 30.2827 (31.3812)  loss_n_40: 0.3676 (0.4081)  loss_n_60: 0.3926 (0.4396)  loss_n_80: 0.4470 (0.4967)  loss_n_100: 0.4843 (0.5501)  triple_100: 8.2624 (8.4577)  triple_80: 8.2942 (8.5633)  triple_60: 7.3116 (7.5054)  triple_40: 4.7865 (4.9604)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1550/1724]  eta: 0:17:06  lr: 0.000100  loss: 30.8016 (31.3753)  loss_n_40: 0.3703 (0.4080)  loss_n_60: 0.4078 (0.4393)  loss_n_80: 0.4400 (0.4964)  loss_n_100: 0.4834 (0.5497)  triple_100: 8.2624 (8.4551)  triple_80: 8.3196 (8.5612)  triple_60: 7.3375 (7.5043)  triple_40: 5.0158 (4.9611)  time: 5.9024  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1560/1724]  eta: 0:16:07  lr: 0.000100  loss: 29.2315 (31.3655)  loss_n_40: 0.3326 (0.4075)  loss_n_60: 0.3676 (0.4388)  loss_n_80: 0.4130 (0.4959)  loss_n_100: 0.4514 (0.5492)  triple_100: 7.9099 (8.4527)  triple_80: 8.0253 (8.5585)  triple_60: 7.0517 (7.5020)  triple_40: 4.9261 (4.9608)  time: 5.9023  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1570/1724]  eta: 0:15:08  lr: 0.000100  loss: 30.3828 (31.3675)  loss_n_40: 0.3277 (0.4071)  loss_n_60: 0.3716 (0.4386)  loss_n_80: 0.4342 (0.4958)  loss_n_100: 0.4885 (0.5491)  triple_100: 8.2113 (8.4541)  triple_80: 8.3203 (8.5593)  triple_60: 7.2583 (7.5024)  triple_40: 4.8850 (4.9611)  time: 5.9018  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1580/1724]  eta: 0:14:09  lr: 0.000100  loss: 30.9093 (31.3758)  loss_n_40: 0.3504 (0.4070)  loss_n_60: 0.3963 (0.4386)  loss_n_80: 0.4666 (0.4957)  loss_n_100: 0.5250 (0.5490)  triple_100: 8.4314 (8.4564)  triple_80: 8.4532 (8.5617)  triple_60: 7.4975 (7.5047)  triple_40: 5.0803 (4.9627)  time: 5.9015  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1590/1724]  eta: 0:13:10  lr: 0.000100  loss: 30.8128 (31.3663)  loss_n_40: 0.3549 (0.4070)  loss_n_60: 0.4117 (0.4385)  loss_n_80: 0.4665 (0.4954)  loss_n_100: 0.5115 (0.5487)  triple_100: 8.2126 (8.4532)  triple_80: 8.3572 (8.5583)  triple_60: 7.4975 (7.5032)  triple_40: 4.9981 (4.9621)  time: 5.9005  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1600/1724]  eta: 0:12:11  lr: 0.000100  loss: 30.9905 (31.3677)  loss_n_40: 0.3748 (0.4068)  loss_n_60: 0.4117 (0.4384)  loss_n_80: 0.4546 (0.4953)  loss_n_100: 0.5033 (0.5486)  triple_100: 8.3416 (8.4536)  triple_80: 8.5262 (8.5587)  triple_60: 7.5254 (7.5039)  triple_40: 4.9560 (4.9623)  time: 5.9004  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1610/1724]  eta: 0:11:12  lr: 0.000100  loss: 30.8390 (31.3586)  loss_n_40: 0.3748 (0.4064)  loss_n_60: 0.3920 (0.4380)  loss_n_80: 0.4397 (0.4949)  loss_n_100: 0.4862 (0.5481)  triple_100: 8.1924 (8.4505)  triple_80: 8.3955 (8.5558)  triple_60: 7.4485 (7.5020)  triple_40: 5.0232 (4.9629)  time: 5.9010  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1620/1724]  eta: 0:10:13  lr: 0.000100  loss: 30.0533 (31.3579)  loss_n_40: 0.3542 (0.4062)  loss_n_60: 0.3822 (0.4378)  loss_n_80: 0.4293 (0.4948)  loss_n_100: 0.4803 (0.5480)  triple_100: 8.1199 (8.4509)  triple_80: 8.1611 (8.5558)  triple_60: 7.2938 (7.5017)  triple_40: 4.9067 (4.9626)  time: 5.8994  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1630/1724]  eta: 0:09:14  lr: 0.000100  loss: 30.5170 (31.3550)  loss_n_40: 0.3619 (0.4059)  loss_n_60: 0.4122 (0.4376)  loss_n_80: 0.4705 (0.4945)  loss_n_100: 0.5250 (0.5478)  triple_100: 8.2934 (8.4508)  triple_80: 8.3635 (8.5553)  triple_60: 7.3611 (7.5008)  triple_40: 4.8403 (4.9622)  time: 5.8996  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1640/1724]  eta: 0:08:15  lr: 0.000100  loss: 30.9729 (31.3546)  loss_n_40: 0.3439 (0.4056)  loss_n_60: 0.3961 (0.4373)  loss_n_80: 0.4528 (0.4943)  loss_n_100: 0.5089 (0.5476)  triple_100: 8.4934 (8.4513)  triple_80: 8.5276 (8.5553)  triple_60: 7.3875 (7.5008)  triple_40: 4.9548 (4.9624)  time: 5.9003  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1650/1724]  eta: 0:07:16  lr: 0.000100  loss: 31.7747 (31.3610)  loss_n_40: 0.3444 (0.4055)  loss_n_60: 0.4001 (0.4374)  loss_n_80: 0.4677 (0.4944)  loss_n_100: 0.5157 (0.5476)  triple_100: 8.6551 (8.4528)  triple_80: 8.7512 (8.5576)  triple_60: 7.6185 (7.5025)  triple_40: 5.0562 (4.9632)  time: 5.9004  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1660/1724]  eta: 0:06:17  lr: 0.000100  loss: 30.8336 (31.3550)  loss_n_40: 0.3444 (0.4052)  loss_n_60: 0.3922 (0.4370)  loss_n_80: 0.4489 (0.4940)  loss_n_100: 0.4745 (0.5471)  triple_100: 8.3351 (8.4509)  triple_80: 8.3717 (8.5556)  triple_60: 7.4606 (7.5016)  triple_40: 5.0562 (4.9636)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1670/1724]  eta: 0:05:18  lr: 0.000100  loss: 30.3158 (31.3476)  loss_n_40: 0.3232 (0.4048)  loss_n_60: 0.3651 (0.4366)  loss_n_80: 0.4146 (0.4935)  loss_n_100: 0.4657 (0.5467)  triple_100: 8.1487 (8.4485)  triple_80: 8.1993 (8.5531)  triple_60: 7.2876 (7.5001)  triple_40: 4.9757 (4.9642)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1680/1724]  eta: 0:04:19  lr: 0.000100  loss: 30.5340 (31.3506)  loss_n_40: 0.3439 (0.4046)  loss_n_60: 0.3734 (0.4365)  loss_n_80: 0.4367 (0.4935)  loss_n_100: 0.4992 (0.5467)  triple_100: 8.3067 (8.4503)  triple_80: 8.4209 (8.5545)  triple_60: 7.3977 (7.5005)  triple_40: 4.9680 (4.9639)  time: 5.9009  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:6]  [1690/1724]  eta: 0:03:20  lr: 0.000100  loss: 30.8869 (31.3479)  loss_n_40: 0.3386 (0.4042)  loss_n_60: 0.3712 (0.4362)  loss_n_80: 0.4390 (0.4932)  loss_n_100: 0.5036 (0.5464)  triple_100: 8.4819 (8.4501)  triple_80: 8.5063 (8.5540)  triple_60: 7.4161 (7.4998)  triple_40: 4.7590 (4.9639)  time: 5.9007  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1700/1724]  eta: 0:02:21  lr: 0.000100  loss: 30.0116 (31.3394)  loss_n_40: 0.3153 (0.4036)  loss_n_60: 0.3630 (0.4358)  loss_n_80: 0.4233 (0.4928)  loss_n_100: 0.4775 (0.5459)  triple_100: 8.1399 (8.4482)  triple_80: 8.1756 (8.5518)  triple_60: 7.1607 (7.4979)  triple_40: 4.8194 (4.9634)  time: 5.9012  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1710/1724]  eta: 0:01:22  lr: 0.000100  loss: 30.0116 (31.3375)  loss_n_40: 0.3153 (0.4033)  loss_n_60: 0.3594 (0.4355)  loss_n_80: 0.4147 (0.4925)  loss_n_100: 0.4698 (0.5457)  triple_100: 8.1399 (8.4479)  triple_80: 8.1756 (8.5514)  triple_60: 7.2046 (7.4979)  triple_40: 4.8606 (4.9633)  time: 5.9020  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1720/1724]  eta: 0:00:23  lr: 0.000100  loss: 31.4351 (31.3355)  loss_n_40: 0.3402 (0.4030)  loss_n_60: 0.4074 (0.4353)  loss_n_80: 0.4521 (0.4923)  loss_n_100: 0.5175 (0.5455)  triple_100: 8.4917 (8.4478)  triple_80: 8.5782 (8.5510)  triple_60: 7.4749 (7.4975)  triple_40: 4.8996 (4.9630)  time: 5.9021  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6]  [1723/1724]  eta: 0:00:05  lr: 0.000100  loss: 31.7332 (31.3369)  loss_n_40: 0.3458 (0.4030)  loss_n_60: 0.4079 (0.4353)  loss_n_80: 0.4769 (0.4923)  loss_n_100: 0.5292 (0.5455)  triple_100: 8.6727 (8.4482)  triple_80: 8.6551 (8.5514)  triple_60: 7.5620 (7.4979)  triple_40: 4.9317 (4.9634)  time: 5.9014  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:6] Total time: 2:49:29 (5.8989 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 31.7332 (31.3369)  loss_n_40: 0.3458 (0.4030)  loss_n_60: 0.4079 (0.4353)  loss_n_80: 0.4769 (0.4923)  loss_n_100: 0.5292 (0.5455)  triple_100: 8.6727 (8.4482)  triple_80: 8.6551 (8.5514)  triple_60: 7.5620 (7.4979)  triple_40: 4.9317 (4.9634)\n",
      "Valid: [epoch:6]  [  0/845]  eta: 0:22:25  loss: 38.5066 (38.5066)  loss_n_40: 0.6250 (0.6250)  loss_n_60: 0.5986 (0.5986)  loss_n_80: 0.7258 (0.7258)  loss_n_100: 0.7563 (0.7563)  triple_100: 10.4992 (10.4992)  triple_80: 10.6492 (10.6492)  triple_60: 8.4024 (8.4024)  triple_40: 6.2501 (6.2501)  time: 1.5927  data: 0.6115  max mem: 40153\n",
      "Valid: [epoch:6]  [ 10/845]  eta: 0:14:21  loss: 32.2559 (33.4640)  loss_n_40: 0.3834 (0.4077)  loss_n_60: 0.4505 (0.4615)  loss_n_80: 0.5193 (0.5285)  loss_n_100: 0.5587 (0.5812)  triple_100: 8.4458 (9.1289)  triple_80: 8.6559 (9.1684)  triple_60: 7.7590 (8.0057)  triple_40: 4.8961 (5.1821)  time: 1.0320  data: 0.0557  max mem: 40153\n",
      "Valid: [epoch:6]  [ 20/845]  eta: 0:13:49  loss: 29.9934 (32.9025)  loss_n_40: 0.3563 (0.3858)  loss_n_60: 0.4415 (0.4415)  loss_n_80: 0.4512 (0.5073)  loss_n_100: 0.5148 (0.5613)  triple_100: 8.1879 (8.9897)  triple_80: 8.1946 (9.0311)  triple_60: 7.2668 (7.8825)  triple_40: 4.9786 (5.1033)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [ 30/845]  eta: 0:13:31  loss: 32.2492 (32.4215)  loss_n_40: 0.3315 (0.3772)  loss_n_60: 0.4090 (0.4287)  loss_n_80: 0.4957 (0.4977)  loss_n_100: 0.5746 (0.5569)  triple_100: 9.0887 (8.9331)  triple_80: 9.0563 (8.9332)  triple_60: 7.5787 (7.7268)  triple_40: 4.9605 (4.9682)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [ 40/845]  eta: 0:13:18  loss: 31.6802 (32.1098)  loss_n_40: 0.3287 (0.3652)  loss_n_60: 0.3962 (0.4183)  loss_n_80: 0.4624 (0.4872)  loss_n_100: 0.5303 (0.5468)  triple_100: 8.8349 (8.8465)  triple_80: 8.8393 (8.8554)  triple_60: 7.5191 (7.6570)  triple_40: 4.8152 (4.9334)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [ 50/845]  eta: 0:13:05  loss: 30.4072 (31.6923)  loss_n_40: 0.3220 (0.3657)  loss_n_60: 0.3866 (0.4148)  loss_n_80: 0.4579 (0.4813)  loss_n_100: 0.5216 (0.5404)  triple_100: 8.4969 (8.7008)  triple_80: 8.4693 (8.7245)  triple_60: 7.3405 (7.5632)  triple_40: 4.7060 (4.9016)  time: 0.9770  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [ 60/845]  eta: 0:12:54  loss: 30.3933 (31.9143)  loss_n_40: 0.3655 (0.3695)  loss_n_60: 0.4368 (0.4194)  loss_n_80: 0.4981 (0.4864)  loss_n_100: 0.5293 (0.5455)  triple_100: 8.0664 (8.7601)  triple_80: 8.1102 (8.7878)  triple_60: 7.4467 (7.6155)  triple_40: 4.7872 (4.9302)  time: 0.9767  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [ 70/845]  eta: 0:12:43  loss: 32.0822 (31.8225)  loss_n_40: 0.3617 (0.3659)  loss_n_60: 0.4288 (0.4161)  loss_n_80: 0.5058 (0.4828)  loss_n_100: 0.5293 (0.5407)  triple_100: 8.6236 (8.7250)  triple_80: 8.7265 (8.7583)  triple_60: 7.5673 (7.5956)  triple_40: 4.9268 (4.9380)  time: 0.9768  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [ 80/845]  eta: 0:12:32  loss: 30.4125 (31.8466)  loss_n_40: 0.3353 (0.3682)  loss_n_60: 0.3776 (0.4184)  loss_n_80: 0.4436 (0.4849)  loss_n_100: 0.5086 (0.5423)  triple_100: 8.4770 (8.7232)  triple_80: 8.4390 (8.7617)  triple_60: 7.3226 (7.6016)  triple_40: 4.8743 (4.9462)  time: 0.9770  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [ 90/845]  eta: 0:12:22  loss: 31.3031 (31.8737)  loss_n_40: 0.3565 (0.3701)  loss_n_60: 0.4267 (0.4200)  loss_n_80: 0.4849 (0.4865)  loss_n_100: 0.5316 (0.5436)  triple_100: 8.5413 (8.7219)  triple_80: 8.4965 (8.7706)  triple_60: 7.6757 (7.6126)  triple_40: 4.8858 (4.9485)  time: 0.9768  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [100/845]  eta: 0:12:12  loss: 31.2015 (31.9011)  loss_n_40: 0.3502 (0.3719)  loss_n_60: 0.3957 (0.4215)  loss_n_80: 0.4638 (0.4878)  loss_n_100: 0.5236 (0.5441)  triple_100: 8.3768 (8.7195)  triple_80: 8.4965 (8.7758)  triple_60: 7.4051 (7.6151)  triple_40: 5.0804 (4.9655)  time: 0.9765  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [110/845]  eta: 0:12:01  loss: 30.0233 (31.7806)  loss_n_40: 0.3458 (0.3699)  loss_n_60: 0.3819 (0.4187)  loss_n_80: 0.4483 (0.4841)  loss_n_100: 0.4895 (0.5396)  triple_100: 8.2234 (8.6715)  triple_80: 8.2527 (8.7338)  triple_60: 7.2185 (7.5883)  triple_40: 5.0804 (4.9747)  time: 0.9762  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [120/845]  eta: 0:11:51  loss: 30.7933 (31.7793)  loss_n_40: 0.3471 (0.3685)  loss_n_60: 0.3941 (0.4175)  loss_n_80: 0.4505 (0.4826)  loss_n_100: 0.5098 (0.5386)  triple_100: 8.2880 (8.6754)  triple_80: 8.3025 (8.7319)  triple_60: 7.3515 (7.5895)  triple_40: 5.0029 (4.9754)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [130/845]  eta: 0:11:41  loss: 30.5827 (31.6291)  loss_n_40: 0.3231 (0.3651)  loss_n_60: 0.3677 (0.4137)  loss_n_80: 0.4258 (0.4776)  loss_n_100: 0.4732 (0.5328)  triple_100: 8.2544 (8.6227)  triple_80: 8.3025 (8.6822)  triple_60: 7.2047 (7.5584)  triple_40: 5.0029 (4.9767)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [140/845]  eta: 0:11:31  loss: 29.3042 (31.6554)  loss_n_40: 0.3116 (0.3635)  loss_n_60: 0.3613 (0.4131)  loss_n_80: 0.4037 (0.4772)  loss_n_100: 0.4378 (0.5327)  triple_100: 7.8450 (8.6345)  triple_80: 7.9455 (8.6919)  triple_60: 7.1674 (7.5642)  triple_40: 5.0194 (4.9783)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [150/845]  eta: 0:11:21  loss: 32.3905 (31.6306)  loss_n_40: 0.3256 (0.3654)  loss_n_60: 0.3930 (0.4132)  loss_n_80: 0.4715 (0.4776)  loss_n_100: 0.5414 (0.5336)  triple_100: 8.9999 (8.6352)  triple_80: 8.9222 (8.6853)  triple_60: 7.6471 (7.5493)  triple_40: 4.9785 (4.9709)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [160/845]  eta: 0:11:11  loss: 31.9919 (31.6510)  loss_n_40: 0.3322 (0.3637)  loss_n_60: 0.3887 (0.4120)  loss_n_80: 0.4632 (0.4767)  loss_n_100: 0.5257 (0.5333)  triple_100: 8.9999 (8.6508)  triple_80: 8.9155 (8.6938)  triple_60: 7.6471 (7.5527)  triple_40: 4.9785 (4.9681)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [170/845]  eta: 0:11:01  loss: 30.4436 (31.5681)  loss_n_40: 0.3227 (0.3629)  loss_n_60: 0.3656 (0.4099)  loss_n_80: 0.4321 (0.4743)  loss_n_100: 0.4887 (0.5306)  triple_100: 8.3451 (8.6253)  triple_80: 8.4749 (8.6692)  triple_60: 7.2819 (7.5349)  triple_40: 4.9036 (4.9609)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [180/845]  eta: 0:10:51  loss: 31.2103 (31.7071)  loss_n_40: 0.3459 (0.3654)  loss_n_60: 0.3973 (0.4133)  loss_n_80: 0.4560 (0.4787)  loss_n_100: 0.5147 (0.5355)  triple_100: 8.5731 (8.6693)  triple_80: 8.5883 (8.7116)  triple_60: 7.5022 (7.5615)  triple_40: 4.9307 (4.9719)  time: 0.9757  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:6]  [190/845]  eta: 0:10:41  loss: 31.2103 (31.6809)  loss_n_40: 0.3442 (0.3637)  loss_n_60: 0.3973 (0.4118)  loss_n_80: 0.4560 (0.4766)  loss_n_100: 0.5147 (0.5331)  triple_100: 8.4808 (8.6570)  triple_80: 8.5155 (8.7005)  triple_60: 7.5010 (7.5597)  triple_40: 5.0681 (4.9786)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [200/845]  eta: 0:10:31  loss: 30.4564 (31.6972)  loss_n_40: 0.3321 (0.3648)  loss_n_60: 0.3923 (0.4134)  loss_n_80: 0.4365 (0.4780)  loss_n_100: 0.4810 (0.5344)  triple_100: 8.2650 (8.6616)  triple_80: 8.3827 (8.7063)  triple_60: 7.4605 (7.5666)  triple_40: 4.9116 (4.9721)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [210/845]  eta: 0:10:21  loss: 31.0387 (31.6414)  loss_n_40: 0.3313 (0.3628)  loss_n_60: 0.3806 (0.4114)  loss_n_80: 0.4539 (0.4758)  loss_n_100: 0.5000 (0.5320)  triple_100: 8.2650 (8.6450)  triple_80: 8.4694 (8.6896)  triple_60: 7.4327 (7.5548)  triple_40: 4.8388 (4.9699)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [220/845]  eta: 0:10:11  loss: 31.4255 (31.7282)  loss_n_40: 0.3313 (0.3629)  loss_n_60: 0.3968 (0.4123)  loss_n_80: 0.4598 (0.4775)  loss_n_100: 0.5279 (0.5344)  triple_100: 8.7912 (8.6776)  triple_80: 8.7540 (8.7180)  triple_60: 7.5422 (7.5722)  triple_40: 4.8658 (4.9734)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [230/845]  eta: 0:10:02  loss: 31.6521 (31.6801)  loss_n_40: 0.3522 (0.3633)  loss_n_60: 0.4038 (0.4124)  loss_n_80: 0.4900 (0.4772)  loss_n_100: 0.5744 (0.5341)  triple_100: 8.8628 (8.6635)  triple_80: 8.8821 (8.7040)  triple_60: 7.6026 (7.5622)  triple_40: 4.8658 (4.9634)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [240/845]  eta: 0:09:52  loss: 30.2894 (31.5888)  loss_n_40: 0.3373 (0.3628)  loss_n_60: 0.3821 (0.4108)  loss_n_80: 0.4178 (0.4750)  loss_n_100: 0.4629 (0.5316)  triple_100: 8.1322 (8.6335)  triple_80: 8.1735 (8.6762)  triple_60: 7.2529 (7.5409)  triple_40: 4.8230 (4.9581)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [250/845]  eta: 0:09:42  loss: 31.1803 (31.5851)  loss_n_40: 0.3404 (0.3631)  loss_n_60: 0.3808 (0.4111)  loss_n_80: 0.4510 (0.4751)  loss_n_100: 0.4993 (0.5319)  triple_100: 8.1322 (8.6320)  triple_80: 8.1735 (8.6731)  triple_60: 7.3666 (7.5403)  triple_40: 4.9026 (4.9586)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [260/845]  eta: 0:09:32  loss: 32.1259 (31.6192)  loss_n_40: 0.3404 (0.3627)  loss_n_60: 0.4044 (0.4111)  loss_n_80: 0.4870 (0.4758)  loss_n_100: 0.5655 (0.5329)  triple_100: 8.7071 (8.6470)  triple_80: 8.7683 (8.6876)  triple_60: 7.6679 (7.5464)  triple_40: 4.9357 (4.9557)  time: 0.9761  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [270/845]  eta: 0:09:22  loss: 32.1259 (31.6855)  loss_n_40: 0.3480 (0.3643)  loss_n_60: 0.4044 (0.4125)  loss_n_80: 0.4839 (0.4776)  loss_n_100: 0.5531 (0.5346)  triple_100: 8.7071 (8.6635)  triple_80: 8.8737 (8.7056)  triple_60: 7.6038 (7.5568)  triple_40: 4.9357 (4.9707)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [280/845]  eta: 0:09:12  loss: 31.4793 (31.6613)  loss_n_40: 0.3632 (0.3642)  loss_n_60: 0.3920 (0.4122)  loss_n_80: 0.4383 (0.4768)  loss_n_100: 0.4924 (0.5336)  triple_100: 8.5851 (8.6543)  triple_80: 8.5981 (8.6981)  triple_60: 7.6038 (7.5554)  triple_40: 4.9888 (4.9667)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [290/845]  eta: 0:09:02  loss: 29.4425 (31.5895)  loss_n_40: 0.3134 (0.3625)  loss_n_60: 0.3701 (0.4102)  loss_n_80: 0.4015 (0.4744)  loss_n_100: 0.4545 (0.5310)  triple_100: 8.0541 (8.6324)  triple_80: 8.0928 (8.6757)  triple_60: 7.1045 (7.5397)  triple_40: 4.8999 (4.9636)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [300/845]  eta: 0:08:53  loss: 29.4425 (31.6060)  loss_n_40: 0.3134 (0.3635)  loss_n_60: 0.3719 (0.4108)  loss_n_80: 0.4176 (0.4754)  loss_n_100: 0.4667 (0.5321)  triple_100: 8.0541 (8.6379)  triple_80: 8.0928 (8.6804)  triple_60: 7.1045 (7.5406)  triple_40: 4.8999 (4.9651)  time: 0.9759  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [310/845]  eta: 0:08:43  loss: 30.0371 (31.5164)  loss_n_40: 0.3558 (0.3629)  loss_n_60: 0.3961 (0.4098)  loss_n_80: 0.4622 (0.4736)  loss_n_100: 0.5122 (0.5297)  triple_100: 7.9858 (8.6071)  triple_80: 8.2621 (8.6530)  triple_60: 7.2086 (7.5245)  triple_40: 4.8745 (4.9559)  time: 0.9757  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [320/845]  eta: 0:08:33  loss: 30.1822 (31.5619)  loss_n_40: 0.3404 (0.3635)  loss_n_60: 0.3961 (0.4108)  loss_n_80: 0.4622 (0.4748)  loss_n_100: 0.5214 (0.5310)  triple_100: 8.4644 (8.6211)  triple_80: 8.4448 (8.6666)  triple_60: 7.3335 (7.5353)  triple_40: 4.8211 (4.9588)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [330/845]  eta: 0:08:23  loss: 31.1898 (31.5477)  loss_n_40: 0.3479 (0.3638)  loss_n_60: 0.4162 (0.4111)  loss_n_80: 0.4816 (0.4750)  loss_n_100: 0.5358 (0.5312)  triple_100: 8.4602 (8.6148)  triple_80: 8.6520 (8.6615)  triple_60: 7.4440 (7.5314)  triple_40: 4.7841 (4.9588)  time: 0.9757  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [340/845]  eta: 0:08:13  loss: 31.0671 (31.5576)  loss_n_40: 0.3482 (0.3644)  loss_n_60: 0.4101 (0.4115)  loss_n_80: 0.4746 (0.4754)  loss_n_100: 0.5312 (0.5314)  triple_100: 8.3833 (8.6154)  triple_80: 8.4272 (8.6625)  triple_60: 7.2673 (7.5326)  triple_40: 4.8013 (4.9643)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [350/845]  eta: 0:08:04  loss: 32.1955 (31.5872)  loss_n_40: 0.3485 (0.3651)  loss_n_60: 0.3978 (0.4122)  loss_n_80: 0.4819 (0.4767)  loss_n_100: 0.5496 (0.5331)  triple_100: 8.9546 (8.6289)  triple_80: 8.9892 (8.6750)  triple_60: 7.5867 (7.5355)  triple_40: 4.9931 (4.9606)  time: 0.9759  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [360/845]  eta: 0:07:54  loss: 32.4942 (31.6136)  loss_n_40: 0.3505 (0.3648)  loss_n_60: 0.4114 (0.4124)  loss_n_80: 0.4972 (0.4770)  loss_n_100: 0.5716 (0.5335)  triple_100: 9.0819 (8.6384)  triple_80: 9.0212 (8.6843)  triple_60: 7.6056 (7.5418)  triple_40: 4.9739 (4.9614)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [370/845]  eta: 0:07:44  loss: 31.2263 (31.5656)  loss_n_40: 0.3486 (0.3653)  loss_n_60: 0.3787 (0.4115)  loss_n_80: 0.4390 (0.4757)  loss_n_100: 0.4977 (0.5320)  triple_100: 8.5967 (8.6236)  triple_80: 8.5727 (8.6691)  triple_60: 7.4964 (7.5317)  triple_40: 4.9587 (4.9568)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [380/845]  eta: 0:07:34  loss: 30.7143 (31.5682)  loss_n_40: 0.3400 (0.3643)  loss_n_60: 0.3787 (0.4111)  loss_n_80: 0.4390 (0.4755)  loss_n_100: 0.4928 (0.5319)  triple_100: 8.3277 (8.6260)  triple_80: 8.4803 (8.6713)  triple_60: 7.4247 (7.5323)  triple_40: 4.8740 (4.9558)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [390/845]  eta: 0:07:24  loss: 30.2885 (31.5283)  loss_n_40: 0.3097 (0.3629)  loss_n_60: 0.3601 (0.4098)  loss_n_80: 0.4333 (0.4740)  loss_n_100: 0.4844 (0.5304)  triple_100: 8.0814 (8.6169)  triple_80: 8.3320 (8.6607)  triple_60: 7.3001 (7.5240)  triple_40: 4.7672 (4.9497)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [400/845]  eta: 0:07:15  loss: 29.1444 (31.4941)  loss_n_40: 0.3092 (0.3623)  loss_n_60: 0.3395 (0.4088)  loss_n_80: 0.3874 (0.4728)  loss_n_100: 0.4305 (0.5291)  triple_100: 7.8580 (8.6055)  triple_80: 7.9373 (8.6493)  triple_60: 7.0257 (7.5162)  triple_40: 4.8255 (4.9501)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [410/845]  eta: 0:07:05  loss: 29.6563 (31.4607)  loss_n_40: 0.3336 (0.3628)  loss_n_60: 0.3573 (0.4083)  loss_n_80: 0.4155 (0.4720)  loss_n_100: 0.4617 (0.5281)  triple_100: 7.9379 (8.5952)  triple_80: 8.0199 (8.6393)  triple_60: 7.1121 (7.5088)  triple_40: 4.9096 (4.9461)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [420/845]  eta: 0:06:55  loss: 30.6714 (31.4705)  loss_n_40: 0.3352 (0.3625)  loss_n_60: 0.3967 (0.4081)  loss_n_80: 0.4553 (0.4719)  loss_n_100: 0.4979 (0.5280)  triple_100: 8.3776 (8.5996)  triple_80: 8.3820 (8.6422)  triple_60: 7.3222 (7.5115)  triple_40: 4.9258 (4.9468)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [430/845]  eta: 0:06:45  loss: 32.2588 (31.4920)  loss_n_40: 0.3661 (0.3635)  loss_n_60: 0.4084 (0.4087)  loss_n_80: 0.4967 (0.4726)  loss_n_100: 0.5721 (0.5288)  triple_100: 8.7872 (8.6060)  triple_80: 8.8037 (8.6484)  triple_60: 7.7239 (7.5139)  triple_40: 5.0103 (4.9501)  time: 0.9759  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:6]  [440/845]  eta: 0:06:35  loss: 34.0172 (31.5053)  loss_n_40: 0.4023 (0.3662)  loss_n_60: 0.4597 (0.4103)  loss_n_80: 0.5148 (0.4741)  loss_n_100: 0.5778 (0.5300)  triple_100: 9.0887 (8.6056)  triple_80: 9.2376 (8.6503)  triple_60: 7.8306 (7.5149)  triple_40: 4.9527 (4.9538)  time: 0.9755  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [450/845]  eta: 0:06:26  loss: 30.7778 (31.4943)  loss_n_40: 0.3818 (0.3655)  loss_n_60: 0.4362 (0.4098)  loss_n_80: 0.4861 (0.4736)  loss_n_100: 0.5497 (0.5294)  triple_100: 8.7828 (8.6036)  triple_80: 8.6379 (8.6480)  triple_60: 7.2936 (7.5131)  triple_40: 4.7584 (4.9514)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [460/845]  eta: 0:06:16  loss: 30.4972 (31.4586)  loss_n_40: 0.3346 (0.3662)  loss_n_60: 0.3736 (0.4098)  loss_n_80: 0.4300 (0.4731)  loss_n_100: 0.4835 (0.5287)  triple_100: 8.3589 (8.5932)  triple_80: 8.3510 (8.6377)  triple_60: 7.2897 (7.5057)  triple_40: 4.7584 (4.9442)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [470/845]  eta: 0:06:06  loss: 30.7455 (31.4474)  loss_n_40: 0.3242 (0.3656)  loss_n_60: 0.3755 (0.4094)  loss_n_80: 0.4422 (0.4727)  loss_n_100: 0.4872 (0.5283)  triple_100: 8.3589 (8.5912)  triple_80: 8.3510 (8.6343)  triple_60: 7.4240 (7.5035)  triple_40: 4.7947 (4.9425)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [480/845]  eta: 0:05:56  loss: 31.8228 (31.4731)  loss_n_40: 0.3420 (0.3661)  loss_n_60: 0.4161 (0.4103)  loss_n_80: 0.4966 (0.4736)  loss_n_100: 0.5604 (0.5294)  triple_100: 8.6632 (8.5989)  triple_80: 8.6419 (8.6414)  triple_60: 7.4955 (7.5108)  triple_40: 4.8033 (4.9427)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [490/845]  eta: 0:05:46  loss: 32.6053 (31.5025)  loss_n_40: 0.3639 (0.3658)  loss_n_60: 0.4286 (0.4106)  loss_n_80: 0.5088 (0.4740)  loss_n_100: 0.5812 (0.5301)  triple_100: 8.9544 (8.6101)  triple_80: 8.9418 (8.6514)  triple_60: 7.7093 (7.5177)  triple_40: 4.8626 (4.9429)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [500/845]  eta: 0:05:37  loss: 31.4906 (31.4988)  loss_n_40: 0.3379 (0.3650)  loss_n_60: 0.3853 (0.4101)  loss_n_80: 0.4488 (0.4735)  loss_n_100: 0.5102 (0.5295)  triple_100: 8.6958 (8.6098)  triple_80: 8.6457 (8.6506)  triple_60: 7.5795 (7.5179)  triple_40: 4.9741 (4.9423)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [510/845]  eta: 0:05:27  loss: 31.1457 (31.5050)  loss_n_40: 0.3374 (0.3646)  loss_n_60: 0.3679 (0.4099)  loss_n_80: 0.4194 (0.4734)  loss_n_100: 0.4693 (0.5295)  triple_100: 8.4682 (8.6125)  triple_80: 8.4618 (8.6523)  triple_60: 7.4678 (7.5195)  triple_40: 4.9741 (4.9434)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [520/845]  eta: 0:05:17  loss: 31.8161 (31.5118)  loss_n_40: 0.3574 (0.3655)  loss_n_60: 0.4019 (0.4104)  loss_n_80: 0.4576 (0.4737)  loss_n_100: 0.5184 (0.5298)  triple_100: 8.8883 (8.6130)  triple_80: 8.7981 (8.6532)  triple_60: 7.5597 (7.5211)  triple_40: 4.8653 (4.9452)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [530/845]  eta: 0:05:07  loss: 32.1817 (31.5418)  loss_n_40: 0.3572 (0.3653)  loss_n_60: 0.4184 (0.4107)  loss_n_80: 0.4992 (0.4745)  loss_n_100: 0.5826 (0.5308)  triple_100: 9.1652 (8.6259)  triple_80: 9.0274 (8.6643)  triple_60: 7.6841 (7.5260)  triple_40: 4.8230 (4.9443)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [540/845]  eta: 0:04:58  loss: 32.1293 (31.5527)  loss_n_40: 0.3423 (0.3649)  loss_n_60: 0.4184 (0.4108)  loss_n_80: 0.4893 (0.4748)  loss_n_100: 0.5466 (0.5313)  triple_100: 8.9866 (8.6323)  triple_80: 8.9400 (8.6696)  triple_60: 7.5447 (7.5280)  triple_40: 4.8197 (4.9410)  time: 0.9763  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [550/845]  eta: 0:04:48  loss: 34.0829 (31.5936)  loss_n_40: 0.3514 (0.3653)  loss_n_60: 0.4320 (0.4114)  loss_n_80: 0.5212 (0.4758)  loss_n_100: 0.5923 (0.5325)  triple_100: 9.3804 (8.6461)  triple_80: 9.4532 (8.6825)  triple_60: 7.8900 (7.5365)  triple_40: 4.9635 (4.9437)  time: 0.9764  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [560/845]  eta: 0:04:38  loss: 32.0138 (31.5830)  loss_n_40: 0.3533 (0.3647)  loss_n_60: 0.4320 (0.4109)  loss_n_80: 0.5052 (0.4751)  loss_n_100: 0.5704 (0.5318)  triple_100: 8.8293 (8.6429)  triple_80: 8.7534 (8.6787)  triple_60: 7.6285 (7.5343)  triple_40: 4.9635 (4.9446)  time: 0.9761  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [570/845]  eta: 0:04:28  loss: 30.9635 (31.5736)  loss_n_40: 0.3357 (0.3644)  loss_n_60: 0.3890 (0.4106)  loss_n_80: 0.4468 (0.4746)  loss_n_100: 0.5104 (0.5312)  triple_100: 8.4962 (8.6395)  triple_80: 8.4807 (8.6748)  triple_60: 7.4179 (7.5335)  triple_40: 4.8963 (4.9451)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [580/845]  eta: 0:04:18  loss: 31.7935 (31.5935)  loss_n_40: 0.3361 (0.3649)  loss_n_60: 0.3995 (0.4113)  loss_n_80: 0.4684 (0.4754)  loss_n_100: 0.5313 (0.5321)  triple_100: 8.6732 (8.6445)  triple_80: 8.7196 (8.6787)  triple_60: 7.6069 (7.5373)  triple_40: 5.0186 (4.9493)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [590/845]  eta: 0:04:09  loss: 32.6852 (31.6181)  loss_n_40: 0.3361 (0.3651)  loss_n_60: 0.4029 (0.4116)  loss_n_80: 0.4987 (0.4760)  loss_n_100: 0.5781 (0.5329)  triple_100: 8.8428 (8.6536)  triple_80: 8.7918 (8.6868)  triple_60: 7.7077 (7.5418)  triple_40: 5.0345 (4.9503)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [600/845]  eta: 0:03:59  loss: 31.5647 (31.6166)  loss_n_40: 0.3267 (0.3647)  loss_n_60: 0.3821 (0.4116)  loss_n_80: 0.4585 (0.4759)  loss_n_100: 0.5277 (0.5327)  triple_100: 8.8316 (8.6524)  triple_80: 8.7448 (8.6866)  triple_60: 7.5185 (7.5423)  triple_40: 4.9225 (4.9504)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [610/845]  eta: 0:03:49  loss: 30.3101 (31.5947)  loss_n_40: 0.3260 (0.3645)  loss_n_60: 0.3821 (0.4113)  loss_n_80: 0.4454 (0.4755)  loss_n_100: 0.5048 (0.5323)  triple_100: 8.1585 (8.6448)  triple_80: 8.1782 (8.6798)  triple_60: 7.3408 (7.5374)  triple_40: 4.9225 (4.9491)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [620/845]  eta: 0:03:39  loss: 30.3430 (31.5890)  loss_n_40: 0.3482 (0.3652)  loss_n_60: 0.3993 (0.4114)  loss_n_80: 0.4707 (0.4755)  loss_n_100: 0.5457 (0.5322)  triple_100: 8.1585 (8.6415)  triple_80: 8.1782 (8.6774)  triple_60: 7.2399 (7.5357)  triple_40: 4.8591 (4.9500)  time: 0.9757  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [630/845]  eta: 0:03:30  loss: 31.0813 (31.5684)  loss_n_40: 0.3459 (0.3649)  loss_n_60: 0.3926 (0.4112)  loss_n_80: 0.4406 (0.4751)  loss_n_100: 0.4866 (0.5317)  triple_100: 8.3216 (8.6349)  triple_80: 8.5065 (8.6714)  triple_60: 7.3881 (7.5316)  triple_40: 4.8431 (4.9476)  time: 0.9757  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [640/845]  eta: 0:03:20  loss: 30.4069 (31.5583)  loss_n_40: 0.3459 (0.3648)  loss_n_60: 0.3900 (0.4111)  loss_n_80: 0.4406 (0.4750)  loss_n_100: 0.4866 (0.5317)  triple_100: 8.3216 (8.6328)  triple_80: 8.4454 (8.6687)  triple_60: 7.3536 (7.5286)  triple_40: 4.8144 (4.9456)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [650/845]  eta: 0:03:10  loss: 30.9049 (31.5627)  loss_n_40: 0.3504 (0.3645)  loss_n_60: 0.4062 (0.4109)  loss_n_80: 0.4528 (0.4747)  loss_n_100: 0.5027 (0.5314)  triple_100: 8.3240 (8.6331)  triple_80: 8.4553 (8.6692)  triple_60: 7.4544 (7.5305)  triple_40: 5.0443 (4.9485)  time: 0.9757  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [660/845]  eta: 0:03:00  loss: 30.9049 (31.5663)  loss_n_40: 0.3471 (0.3646)  loss_n_60: 0.4071 (0.4111)  loss_n_80: 0.4767 (0.4750)  loss_n_100: 0.5064 (0.5317)  triple_100: 8.0939 (8.6336)  triple_80: 8.4553 (8.6704)  triple_60: 7.6321 (7.5314)  triple_40: 5.0443 (4.9485)  time: 0.9758  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [670/845]  eta: 0:02:50  loss: 30.7449 (31.5459)  loss_n_40: 0.3471 (0.3646)  loss_n_60: 0.4061 (0.4109)  loss_n_80: 0.4874 (0.4749)  loss_n_100: 0.5685 (0.5317)  triple_100: 8.6648 (8.6290)  triple_80: 8.6824 (8.6647)  triple_60: 7.5143 (7.5253)  triple_40: 4.7346 (4.9447)  time: 0.9754  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [680/845]  eta: 0:02:41  loss: 31.1953 (31.5535)  loss_n_40: 0.3482 (0.3649)  loss_n_60: 0.4061 (0.4112)  loss_n_80: 0.4874 (0.4752)  loss_n_100: 0.5685 (0.5320)  triple_100: 8.6648 (8.6304)  triple_80: 8.6824 (8.6654)  triple_60: 7.4843 (7.5266)  triple_40: 4.9651 (4.9478)  time: 0.9752  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:6]  [690/845]  eta: 0:02:31  loss: 31.1953 (31.5351)  loss_n_40: 0.3530 (0.3650)  loss_n_60: 0.4135 (0.4110)  loss_n_80: 0.4667 (0.4749)  loss_n_100: 0.5241 (0.5317)  triple_100: 8.5591 (8.6253)  triple_80: 8.5764 (8.6593)  triple_60: 7.4843 (7.5220)  triple_40: 5.0594 (4.9459)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [700/845]  eta: 0:02:21  loss: 30.8371 (31.5177)  loss_n_40: 0.3478 (0.3648)  loss_n_60: 0.3843 (0.4108)  loss_n_80: 0.4410 (0.4746)  loss_n_100: 0.5118 (0.5316)  triple_100: 8.6125 (8.6212)  triple_80: 8.6172 (8.6552)  triple_60: 7.3443 (7.5173)  triple_40: 4.8131 (4.9423)  time: 0.9760  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [710/845]  eta: 0:02:11  loss: 30.8371 (31.5134)  loss_n_40: 0.3245 (0.3647)  loss_n_60: 0.3843 (0.4107)  loss_n_80: 0.4410 (0.4745)  loss_n_100: 0.5118 (0.5313)  triple_100: 8.4434 (8.6181)  triple_80: 8.6172 (8.6528)  triple_60: 7.3847 (7.5174)  triple_40: 4.8131 (4.9439)  time: 0.9758  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [720/845]  eta: 0:02:02  loss: 31.8086 (31.5250)  loss_n_40: 0.3515 (0.3647)  loss_n_60: 0.4141 (0.4109)  loss_n_80: 0.4899 (0.4747)  loss_n_100: 0.5292 (0.5317)  triple_100: 8.4434 (8.6229)  triple_80: 8.6981 (8.6570)  triple_60: 7.5746 (7.5201)  triple_40: 4.8863 (4.9430)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [730/845]  eta: 0:01:52  loss: 31.6789 (31.5358)  loss_n_40: 0.3558 (0.3646)  loss_n_60: 0.4155 (0.4110)  loss_n_80: 0.4656 (0.4749)  loss_n_100: 0.5276 (0.5319)  triple_100: 8.6758 (8.6271)  triple_80: 8.6657 (8.6603)  triple_60: 7.5889 (7.5225)  triple_40: 4.8863 (4.9435)  time: 0.9756  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [740/845]  eta: 0:01:42  loss: 32.3562 (31.5778)  loss_n_40: 0.3545 (0.3651)  loss_n_60: 0.4156 (0.4116)  loss_n_80: 0.4799 (0.4756)  loss_n_100: 0.5411 (0.5327)  triple_100: 8.9802 (8.6396)  triple_80: 8.9433 (8.6724)  triple_60: 7.7447 (7.5319)  triple_40: 5.0201 (4.9489)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [750/845]  eta: 0:01:32  loss: 32.5596 (31.5718)  loss_n_40: 0.3524 (0.3647)  loss_n_60: 0.4191 (0.4111)  loss_n_80: 0.4799 (0.4751)  loss_n_100: 0.5411 (0.5321)  triple_100: 9.0113 (8.6373)  triple_80: 8.9433 (8.6702)  triple_60: 7.8156 (7.5307)  triple_40: 5.0720 (4.9506)  time: 0.9755  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [760/845]  eta: 0:01:23  loss: 29.2875 (31.5375)  loss_n_40: 0.3145 (0.3654)  loss_n_60: 0.3581 (0.4107)  loss_n_80: 0.3950 (0.4743)  loss_n_100: 0.4378 (0.5312)  triple_100: 7.8205 (8.6252)  triple_80: 7.9296 (8.6584)  triple_60: 7.1329 (7.5225)  triple_40: 5.0720 (4.9497)  time: 0.9751  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [770/845]  eta: 0:01:13  loss: 29.7228 (31.5214)  loss_n_40: 0.3274 (0.3649)  loss_n_60: 0.3625 (0.4102)  loss_n_80: 0.3950 (0.4737)  loss_n_100: 0.4443 (0.5305)  triple_100: 7.9514 (8.6200)  triple_80: 7.9545 (8.6534)  triple_60: 7.2433 (7.5199)  triple_40: 4.9273 (4.9488)  time: 0.9753  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [780/845]  eta: 0:01:03  loss: 30.8029 (31.5097)  loss_n_40: 0.3309 (0.3647)  loss_n_60: 0.3936 (0.4100)  loss_n_80: 0.4335 (0.4734)  loss_n_100: 0.4764 (0.5302)  triple_100: 8.2124 (8.6168)  triple_80: 8.5572 (8.6500)  triple_60: 7.5004 (7.5177)  triple_40: 4.8463 (4.9470)  time: 0.9752  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [790/845]  eta: 0:00:53  loss: 30.1627 (31.4951)  loss_n_40: 0.3384 (0.3643)  loss_n_60: 0.3897 (0.4096)  loss_n_80: 0.4077 (0.4729)  loss_n_100: 0.4553 (0.5295)  triple_100: 7.9051 (8.6119)  triple_80: 8.0296 (8.6450)  triple_60: 7.4163 (7.5156)  triple_40: 4.7229 (4.9463)  time: 0.9750  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [800/845]  eta: 0:00:43  loss: 29.1889 (31.4615)  loss_n_40: 0.3313 (0.3641)  loss_n_60: 0.3586 (0.4090)  loss_n_80: 0.3926 (0.4721)  loss_n_100: 0.4263 (0.5288)  triple_100: 7.8953 (8.6021)  triple_80: 7.9314 (8.6352)  triple_60: 7.0986 (7.5071)  triple_40: 4.7229 (4.9431)  time: 0.9750  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [810/845]  eta: 0:00:34  loss: 30.9508 (31.4799)  loss_n_40: 0.3335 (0.3641)  loss_n_60: 0.3933 (0.4091)  loss_n_80: 0.4695 (0.4724)  loss_n_100: 0.5387 (0.5291)  triple_100: 8.5514 (8.6082)  triple_80: 8.5581 (8.6406)  triple_60: 7.2695 (7.5112)  triple_40: 4.9251 (4.9452)  time: 0.9750  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [820/845]  eta: 0:00:24  loss: 31.1444 (31.4550)  loss_n_40: 0.3224 (0.3635)  loss_n_60: 0.3708 (0.4085)  loss_n_80: 0.4420 (0.4716)  loss_n_100: 0.5012 (0.5283)  triple_100: 8.4413 (8.6008)  triple_80: 8.5100 (8.6335)  triple_60: 7.4204 (7.5061)  triple_40: 4.9645 (4.9428)  time: 0.9751  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [830/845]  eta: 0:00:14  loss: 30.2553 (31.4613)  loss_n_40: 0.3446 (0.3638)  loss_n_60: 0.3728 (0.4088)  loss_n_80: 0.4420 (0.4720)  loss_n_100: 0.4936 (0.5286)  triple_100: 8.2668 (8.6028)  triple_80: 8.3912 (8.6360)  triple_60: 7.2247 (7.5074)  triple_40: 4.7407 (4.9418)  time: 0.9748  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:6]  [840/845]  eta: 0:00:04  loss: 30.5161 (31.4637)  loss_n_40: 0.3476 (0.3636)  loss_n_60: 0.3904 (0.4087)  loss_n_80: 0.4514 (0.4719)  loss_n_100: 0.5037 (0.5285)  triple_100: 8.2668 (8.6028)  triple_80: 8.4450 (8.6361)  triple_60: 7.4997 (7.5088)  triple_40: 5.0039 (4.9432)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6]  [844/845]  eta: 0:00:00  loss: 30.4569 (31.4594)  loss_n_40: 0.3476 (0.3636)  loss_n_60: 0.3911 (0.4087)  loss_n_80: 0.4514 (0.4718)  loss_n_100: 0.5037 (0.5284)  triple_100: 8.2398 (8.6022)  triple_80: 8.2723 (8.6348)  triple_60: 7.3584 (7.5074)  triple_40: 4.8172 (4.9425)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:6] Total time: 0:13:45 (0.9767 s / it)\n",
      "Averaged stats: loss: 30.4569 (31.4594)  loss_n_40: 0.3476 (0.3636)  loss_n_60: 0.3911 (0.4087)  loss_n_80: 0.4514 (0.4718)  loss_n_100: 0.5037 (0.5284)  triple_100: 8.2398 (8.6022)  triple_80: 8.2723 (8.6348)  triple_60: 7.3584 (7.5074)  triple_40: 4.8172 (4.9425)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle_2/low2high/epoch_6_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.528%\n",
      "Min loss_n_100: 0.528\n",
      "Best Epoch: 6.000\n",
      "Train: [epoch:7]  [   0/1724]  eta: 3:00:50  lr: 0.000120  loss: 28.0550 (28.0550)  loss_n_40: 0.5030 (0.5030)  loss_n_60: 0.4305 (0.4305)  loss_n_80: 0.4548 (0.4548)  loss_n_100: 0.4836 (0.4836)  triple_100: 7.0296 (7.0296)  triple_80: 7.3512 (7.3512)  triple_60: 6.6704 (6.6704)  triple_40: 5.1318 (5.1318)  time: 6.2936  data: 0.5745  max mem: 40153\n",
      "Train: [epoch:7]  [  10/1724]  eta: 2:49:18  lr: 0.000120  loss: 30.2405 (31.1480)  loss_n_40: 0.3601 (0.3903)  loss_n_60: 0.4154 (0.4175)  loss_n_80: 0.4563 (0.4702)  loss_n_100: 0.4987 (0.5171)  triple_100: 8.0166 (8.2945)  triple_80: 8.1930 (8.4472)  triple_60: 7.2827 (7.4583)  triple_40: 5.1318 (5.1529)  time: 5.9269  data: 0.0523  max mem: 40153\n",
      "Train: [epoch:7]  [  20/1724]  eta: 2:47:48  lr: 0.000120  loss: 29.3526 (30.1121)  loss_n_40: 0.3441 (0.3754)  loss_n_60: 0.3772 (0.3929)  loss_n_80: 0.4212 (0.4410)  loss_n_100: 0.4644 (0.4857)  triple_100: 7.9057 (8.0189)  triple_80: 8.0103 (8.1498)  triple_60: 7.1343 (7.2200)  triple_40: 5.0047 (5.0284)  time: 5.8895  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [  30/1724]  eta: 2:46:40  lr: 0.000120  loss: 28.9540 (29.9811)  loss_n_40: 0.3328 (0.3615)  loss_n_60: 0.3671 (0.3894)  loss_n_80: 0.4095 (0.4413)  loss_n_100: 0.4551 (0.4888)  triple_100: 7.6798 (8.0013)  triple_80: 7.7758 (8.1324)  triple_60: 6.9829 (7.1957)  triple_40: 4.8306 (4.9706)  time: 5.8907  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [  40/1724]  eta: 2:45:36  lr: 0.000120  loss: 30.0912 (30.2095)  loss_n_40: 0.3433 (0.3672)  loss_n_60: 0.4001 (0.4003)  loss_n_80: 0.4463 (0.4538)  loss_n_100: 0.5007 (0.5028)  triple_100: 8.2268 (8.0668)  triple_80: 8.2532 (8.1992)  triple_60: 7.2330 (7.2525)  triple_40: 4.8084 (4.9670)  time: 5.8923  data: 0.0003  max mem: 40153\n",
      "Train: [epoch:7]  [  50/1724]  eta: 2:44:34  lr: 0.000120  loss: 30.6333 (30.1605)  loss_n_40: 0.3707 (0.3710)  loss_n_60: 0.4153 (0.4058)  loss_n_80: 0.4952 (0.4612)  loss_n_100: 0.5556 (0.5110)  triple_100: 8.2740 (8.0638)  triple_80: 8.4021 (8.1993)  triple_60: 7.2669 (7.2293)  triple_40: 4.8081 (4.9191)  time: 5.8916  data: 0.0003  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [  60/1724]  eta: 2:43:34  lr: 0.000120  loss: 29.5867 (30.0043)  loss_n_40: 0.3707 (0.3760)  loss_n_60: 0.4148 (0.4075)  loss_n_80: 0.4703 (0.4622)  loss_n_100: 0.5363 (0.5131)  triple_100: 7.9069 (8.0163)  triple_80: 7.8791 (8.1471)  triple_60: 7.0351 (7.1843)  triple_40: 4.7219 (4.8977)  time: 5.8921  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [  70/1724]  eta: 2:42:33  lr: 0.000120  loss: 29.2514 (29.8893)  loss_n_40: 0.3847 (0.3780)  loss_n_60: 0.3980 (0.4106)  loss_n_80: 0.4644 (0.4667)  loss_n_100: 0.5280 (0.5208)  triple_100: 7.8456 (8.0115)  triple_80: 7.8109 (8.1218)  triple_60: 6.9128 (7.1427)  triple_40: 4.5231 (4.8372)  time: 5.8928  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [  80/1724]  eta: 2:41:33  lr: 0.000120  loss: 28.7376 (29.8081)  loss_n_40: 0.3807 (0.3799)  loss_n_60: 0.4162 (0.4136)  loss_n_80: 0.4797 (0.4706)  loss_n_100: 0.5383 (0.5253)  triple_100: 7.8235 (7.9939)  triple_80: 7.7889 (8.1029)  triple_60: 6.8580 (7.1217)  triple_40: 4.4286 (4.8003)  time: 5.8916  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [  90/1724]  eta: 2:40:33  lr: 0.000120  loss: 30.3126 (29.9325)  loss_n_40: 0.3807 (0.3827)  loss_n_60: 0.4360 (0.4168)  loss_n_80: 0.4958 (0.4747)  loss_n_100: 0.5598 (0.5298)  triple_100: 8.0838 (8.0256)  triple_80: 8.1770 (8.1328)  triple_60: 7.1495 (7.1450)  triple_40: 4.7788 (4.8251)  time: 5.8916  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 100/1724]  eta: 2:39:34  lr: 0.000120  loss: 29.9900 (29.7809)  loss_n_40: 0.3700 (0.3818)  loss_n_60: 0.4386 (0.4156)  loss_n_80: 0.4909 (0.4732)  loss_n_100: 0.5474 (0.5282)  triple_100: 8.1384 (7.9839)  triple_80: 8.1770 (8.0887)  triple_60: 7.0965 (7.1104)  triple_40: 4.6968 (4.7991)  time: 5.8913  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 110/1724]  eta: 2:38:34  lr: 0.000120  loss: 30.2803 (30.0100)  loss_n_40: 0.3570 (0.3822)  loss_n_60: 0.4357 (0.4200)  loss_n_80: 0.5095 (0.4796)  loss_n_100: 0.5779 (0.5364)  triple_100: 8.3251 (8.0695)  triple_80: 8.3381 (8.1638)  triple_60: 7.1988 (7.1579)  triple_40: 4.5602 (4.8004)  time: 5.8914  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 120/1724]  eta: 2:37:35  lr: 0.000120  loss: 30.9366 (29.9842)  loss_n_40: 0.3622 (0.3839)  loss_n_60: 0.4469 (0.4225)  loss_n_80: 0.5226 (0.4825)  loss_n_100: 0.5952 (0.5395)  triple_100: 8.5989 (8.0714)  triple_80: 8.5081 (8.1619)  triple_60: 7.3499 (7.1513)  triple_40: 4.5917 (4.7712)  time: 5.8911  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 130/1724]  eta: 2:36:35  lr: 0.000120  loss: 30.5443 (30.0778)  loss_n_40: 0.3622 (0.3837)  loss_n_60: 0.4470 (0.4229)  loss_n_80: 0.5148 (0.4831)  loss_n_100: 0.5414 (0.5399)  triple_100: 8.0647 (8.1009)  triple_80: 8.2722 (8.1917)  triple_60: 7.3499 (7.1748)  triple_40: 4.6221 (4.7809)  time: 5.8907  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 140/1724]  eta: 2:35:36  lr: 0.000120  loss: 31.6989 (30.2660)  loss_n_40: 0.3828 (0.3878)  loss_n_60: 0.4542 (0.4280)  loss_n_80: 0.5179 (0.4874)  loss_n_100: 0.5600 (0.5446)  triple_100: 8.4162 (8.1442)  triple_80: 8.5436 (8.2378)  triple_60: 7.5701 (7.2242)  triple_40: 5.0533 (4.8120)  time: 5.8916  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 150/1724]  eta: 2:34:37  lr: 0.000120  loss: 33.7610 (30.7361)  loss_n_40: 0.4521 (0.3997)  loss_n_60: 0.5080 (0.4398)  loss_n_80: 0.5891 (0.5003)  loss_n_100: 0.6627 (0.5578)  triple_100: 9.1239 (8.2600)  triple_80: 9.2176 (8.3660)  triple_60: 8.0925 (7.3321)  triple_40: 5.4130 (4.8803)  time: 5.8914  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 160/1724]  eta: 2:33:38  lr: 0.000120  loss: 35.1150 (30.9500)  loss_n_40: 0.4521 (0.4024)  loss_n_60: 0.5080 (0.4444)  loss_n_80: 0.5915 (0.5057)  loss_n_100: 0.6647 (0.5641)  triple_100: 9.6737 (8.3345)  triple_80: 9.7192 (8.4360)  triple_60: 8.1291 (7.3785)  triple_40: 5.4582 (4.8843)  time: 5.8920  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 170/1724]  eta: 2:32:38  lr: 0.000120  loss: 34.4215 (31.1517)  loss_n_40: 0.4317 (0.4117)  loss_n_60: 0.5065 (0.4541)  loss_n_80: 0.5889 (0.5159)  loss_n_100: 0.6647 (0.5752)  triple_100: 9.6035 (8.3989)  triple_80: 9.5545 (8.4968)  triple_60: 7.7980 (7.4141)  triple_40: 4.6945 (4.8849)  time: 5.8909  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 180/1724]  eta: 2:31:39  lr: 0.000120  loss: 35.6946 (31.4691)  loss_n_40: 0.4998 (0.4188)  loss_n_60: 0.6052 (0.4626)  loss_n_80: 0.6859 (0.5251)  loss_n_100: 0.7330 (0.5848)  triple_100: 9.6128 (8.4807)  triple_80: 9.7650 (8.5860)  triple_60: 8.4788 (7.4901)  triple_40: 5.1665 (4.9210)  time: 5.8889  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 190/1724]  eta: 2:30:39  lr: 0.000120  loss: 35.2381 (31.6149)  loss_n_40: 0.4870 (0.4213)  loss_n_60: 0.5417 (0.4664)  loss_n_80: 0.6364 (0.5296)  loss_n_100: 0.7149 (0.5896)  triple_100: 9.2407 (8.5261)  triple_80: 9.5476 (8.6336)  triple_60: 8.2024 (7.5222)  triple_40: 4.9988 (4.9262)  time: 5.8878  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 200/1724]  eta: 2:29:40  lr: 0.000120  loss: 32.3203 (31.6763)  loss_n_40: 0.4220 (0.4220)  loss_n_60: 0.4991 (0.4683)  loss_n_80: 0.5696 (0.5317)  loss_n_100: 0.6361 (0.5917)  triple_100: 8.9352 (8.5485)  triple_80: 9.0521 (8.6567)  triple_60: 7.6505 (7.5376)  triple_40: 4.7747 (4.9199)  time: 5.8877  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 210/1724]  eta: 2:28:41  lr: 0.000120  loss: 31.8040 (31.7088)  loss_n_40: 0.4229 (0.4238)  loss_n_60: 0.4721 (0.4692)  loss_n_80: 0.5526 (0.5322)  loss_n_100: 0.6237 (0.5918)  triple_100: 8.6165 (8.5509)  triple_80: 8.8503 (8.6628)  triple_60: 7.6505 (7.5460)  triple_40: 4.7916 (4.9320)  time: 5.8881  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 220/1724]  eta: 2:27:42  lr: 0.000120  loss: 31.8040 (31.7281)  loss_n_40: 0.4250 (0.4251)  loss_n_60: 0.4721 (0.4702)  loss_n_80: 0.5235 (0.5328)  loss_n_100: 0.5780 (0.5920)  triple_100: 8.4499 (8.5488)  triple_80: 8.7797 (8.6669)  triple_60: 7.6874 (7.5560)  triple_40: 4.9919 (4.9363)  time: 5.8890  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 230/1724]  eta: 2:26:43  lr: 0.000120  loss: 31.7669 (31.7373)  loss_n_40: 0.4250 (0.4248)  loss_n_60: 0.4606 (0.4700)  loss_n_80: 0.5272 (0.5327)  loss_n_100: 0.5780 (0.5919)  triple_100: 8.4372 (8.5561)  triple_80: 8.7723 (8.6739)  triple_60: 7.6517 (7.5589)  triple_40: 4.7364 (4.9291)  time: 5.8889  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 240/1724]  eta: 2:25:44  lr: 0.000120  loss: 31.0195 (31.7070)  loss_n_40: 0.3667 (0.4230)  loss_n_60: 0.4284 (0.4675)  loss_n_80: 0.4814 (0.5300)  loss_n_100: 0.5403 (0.5883)  triple_100: 8.3815 (8.5435)  triple_80: 8.5896 (8.6647)  triple_60: 7.4959 (7.5548)  triple_40: 4.9504 (4.9352)  time: 5.8891  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 250/1724]  eta: 2:24:44  lr: 0.000120  loss: 31.2869 (31.7194)  loss_n_40: 0.3547 (0.4204)  loss_n_60: 0.4055 (0.4656)  loss_n_80: 0.4802 (0.5284)  loss_n_100: 0.5310 (0.5867)  triple_100: 8.6360 (8.5529)  triple_80: 8.6840 (8.6711)  triple_60: 7.4648 (7.5584)  triple_40: 4.9655 (4.9359)  time: 5.8902  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 260/1724]  eta: 2:23:45  lr: 0.000120  loss: 31.0070 (31.6915)  loss_n_40: 0.3485 (0.4197)  loss_n_60: 0.4040 (0.4646)  loss_n_80: 0.4673 (0.5269)  loss_n_100: 0.5310 (0.5848)  triple_100: 8.4456 (8.5434)  triple_80: 8.4652 (8.6640)  triple_60: 7.4131 (7.5559)  triple_40: 4.8258 (4.9322)  time: 5.8905  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 270/1724]  eta: 2:22:46  lr: 0.000120  loss: 30.1086 (31.6532)  loss_n_40: 0.3791 (0.4192)  loss_n_60: 0.4237 (0.4637)  loss_n_80: 0.4712 (0.5256)  loss_n_100: 0.5367 (0.5831)  triple_100: 8.0825 (8.5314)  triple_80: 8.2652 (8.6531)  triple_60: 7.2950 (7.5509)  triple_40: 4.7156 (4.9261)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 280/1724]  eta: 2:21:47  lr: 0.000120  loss: 30.5362 (31.6593)  loss_n_40: 0.3931 (0.4185)  loss_n_60: 0.4456 (0.4628)  loss_n_80: 0.4957 (0.5240)  loss_n_100: 0.5452 (0.5808)  triple_100: 8.1616 (8.5274)  triple_80: 8.2597 (8.6507)  triple_60: 7.4453 (7.5580)  triple_40: 4.9306 (4.9372)  time: 5.8895  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 290/1724]  eta: 2:20:49  lr: 0.000120  loss: 32.4723 (31.7185)  loss_n_40: 0.3811 (0.4177)  loss_n_60: 0.4142 (0.4623)  loss_n_80: 0.4884 (0.5235)  loss_n_100: 0.5347 (0.5802)  triple_100: 8.7437 (8.5438)  triple_80: 8.8741 (8.6678)  triple_60: 7.7812 (7.5743)  triple_40: 5.1474 (4.9489)  time: 5.8920  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [ 300/1724]  eta: 2:19:50  lr: 0.000120  loss: 32.5016 (31.7785)  loss_n_40: 0.3849 (0.4178)  loss_n_60: 0.4473 (0.4623)  loss_n_80: 0.5178 (0.5237)  loss_n_100: 0.5658 (0.5803)  triple_100: 8.8907 (8.5618)  triple_80: 8.9072 (8.6844)  triple_60: 7.8100 (7.5886)  triple_40: 5.1037 (4.9595)  time: 5.8917  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 310/1724]  eta: 2:18:51  lr: 0.000120  loss: 32.3947 (31.7934)  loss_n_40: 0.3849 (0.4169)  loss_n_60: 0.4510 (0.4619)  loss_n_80: 0.5178 (0.5234)  loss_n_100: 0.5539 (0.5801)  triple_100: 8.8193 (8.5704)  triple_80: 8.9072 (8.6900)  triple_60: 7.7131 (7.5927)  triple_40: 4.9813 (4.9580)  time: 5.8909  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 320/1724]  eta: 2:17:52  lr: 0.000120  loss: 31.9458 (31.8258)  loss_n_40: 0.3699 (0.4155)  loss_n_60: 0.4313 (0.4612)  loss_n_80: 0.4940 (0.5228)  loss_n_100: 0.5539 (0.5798)  triple_100: 8.7429 (8.5830)  triple_80: 8.7071 (8.6992)  triple_60: 7.6233 (7.6012)  triple_40: 4.8635 (4.9632)  time: 5.8930  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 330/1724]  eta: 2:16:53  lr: 0.000120  loss: 30.0914 (31.7662)  loss_n_40: 0.3385 (0.4131)  loss_n_60: 0.3997 (0.4585)  loss_n_80: 0.4650 (0.5196)  loss_n_100: 0.5089 (0.5762)  triple_100: 8.1783 (8.5651)  triple_80: 8.2677 (8.6808)  triple_60: 7.3400 (7.5899)  triple_40: 4.9238 (4.9629)  time: 5.8932  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 340/1724]  eta: 2:15:54  lr: 0.000120  loss: 29.7736 (31.7432)  loss_n_40: 0.3414 (0.4118)  loss_n_60: 0.3797 (0.4568)  loss_n_80: 0.4238 (0.5176)  loss_n_100: 0.4718 (0.5738)  triple_100: 8.0576 (8.5560)  triple_80: 8.0748 (8.6723)  triple_60: 7.1565 (7.5866)  triple_40: 4.9404 (4.9683)  time: 5.8919  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 350/1724]  eta: 2:14:55  lr: 0.000120  loss: 31.1126 (31.7315)  loss_n_40: 0.3587 (0.4103)  loss_n_60: 0.3912 (0.4553)  loss_n_80: 0.4534 (0.5160)  loss_n_100: 0.5016 (0.5722)  triple_100: 8.3613 (8.5556)  triple_80: 8.5754 (8.6702)  triple_60: 7.4094 (7.5843)  triple_40: 4.9404 (4.9676)  time: 5.8924  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 360/1724]  eta: 2:13:56  lr: 0.000120  loss: 31.4109 (31.7376)  loss_n_40: 0.3615 (0.4100)  loss_n_60: 0.4007 (0.4548)  loss_n_80: 0.4661 (0.5154)  loss_n_100: 0.5320 (0.5716)  triple_100: 8.6664 (8.5594)  triple_80: 8.6633 (8.6720)  triple_60: 7.4927 (7.5866)  triple_40: 4.8827 (4.9679)  time: 5.8926  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 370/1724]  eta: 2:12:57  lr: 0.000120  loss: 32.0615 (31.7490)  loss_n_40: 0.4012 (0.4107)  loss_n_60: 0.4347 (0.4550)  loss_n_80: 0.4790 (0.5150)  loss_n_100: 0.5365 (0.5707)  triple_100: 8.4945 (8.5560)  triple_80: 8.6131 (8.6720)  triple_60: 7.7353 (7.5930)  triple_40: 4.9610 (4.9767)  time: 5.8918  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 380/1724]  eta: 2:11:58  lr: 0.000120  loss: 30.8299 (31.7217)  loss_n_40: 0.4012 (0.4100)  loss_n_60: 0.4179 (0.4537)  loss_n_80: 0.4542 (0.5134)  loss_n_100: 0.4968 (0.5688)  triple_100: 8.3546 (8.5467)  triple_80: 8.3362 (8.6635)  triple_60: 7.4262 (7.5886)  triple_40: 5.0343 (4.9770)  time: 5.8914  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 390/1724]  eta: 2:10:59  lr: 0.000120  loss: 30.3195 (31.7086)  loss_n_40: 0.3756 (0.4095)  loss_n_60: 0.4054 (0.4529)  loss_n_80: 0.4406 (0.5122)  loss_n_100: 0.4732 (0.5673)  triple_100: 8.0227 (8.5402)  triple_80: 8.2057 (8.6588)  triple_60: 7.3599 (7.5887)  triple_40: 5.0182 (4.9792)  time: 5.8922  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 400/1724]  eta: 2:10:00  lr: 0.000120  loss: 30.4903 (31.6903)  loss_n_40: 0.3756 (0.4084)  loss_n_60: 0.4097 (0.4516)  loss_n_80: 0.4474 (0.5107)  loss_n_100: 0.4770 (0.5656)  triple_100: 8.0660 (8.5343)  triple_80: 8.2103 (8.6527)  triple_60: 7.4532 (7.5856)  triple_40: 5.0457 (4.9814)  time: 5.8925  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 410/1724]  eta: 2:09:02  lr: 0.000120  loss: 30.4903 (31.6689)  loss_n_40: 0.3587 (0.4070)  loss_n_60: 0.3828 (0.4502)  loss_n_80: 0.4212 (0.5093)  loss_n_100: 0.4656 (0.5640)  triple_100: 8.1449 (8.5300)  triple_80: 8.3796 (8.6476)  triple_60: 7.4246 (7.5813)  triple_40: 4.9820 (4.9794)  time: 5.8918  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 420/1724]  eta: 2:08:03  lr: 0.000120  loss: 31.6842 (31.7068)  loss_n_40: 0.3616 (0.4068)  loss_n_60: 0.4105 (0.4505)  loss_n_80: 0.4864 (0.5096)  loss_n_100: 0.5426 (0.5644)  triple_100: 8.7057 (8.5441)  triple_80: 8.7495 (8.6603)  triple_60: 7.5913 (7.5913)  triple_40: 4.8819 (4.9798)  time: 5.8916  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 430/1724]  eta: 2:07:04  lr: 0.000120  loss: 31.0206 (31.6657)  loss_n_40: 0.3565 (0.4051)  loss_n_60: 0.4073 (0.4486)  loss_n_80: 0.4666 (0.5076)  loss_n_100: 0.5272 (0.5623)  triple_100: 8.6076 (8.5348)  triple_80: 8.7270 (8.6496)  triple_60: 7.5308 (7.5828)  triple_40: 4.8339 (4.9748)  time: 5.8912  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 440/1724]  eta: 2:06:05  lr: 0.000120  loss: 29.9404 (31.6225)  loss_n_40: 0.3268 (0.4031)  loss_n_60: 0.3574 (0.4466)  loss_n_80: 0.4031 (0.5052)  loss_n_100: 0.4544 (0.5596)  triple_100: 8.0949 (8.5220)  triple_80: 8.1275 (8.6359)  triple_60: 7.2414 (7.5749)  triple_40: 4.8835 (4.9752)  time: 5.8920  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 450/1724]  eta: 2:05:06  lr: 0.000120  loss: 29.9404 (31.5991)  loss_n_40: 0.3214 (0.4020)  loss_n_60: 0.3607 (0.4453)  loss_n_80: 0.4129 (0.5038)  loss_n_100: 0.4653 (0.5580)  triple_100: 8.0949 (8.5157)  triple_80: 8.1275 (8.6291)  triple_60: 7.3142 (7.5714)  triple_40: 4.8823 (4.9737)  time: 5.8916  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 460/1724]  eta: 2:04:07  lr: 0.000120  loss: 30.1974 (31.5795)  loss_n_40: 0.3293 (0.4007)  loss_n_60: 0.3892 (0.4442)  loss_n_80: 0.4455 (0.5025)  loss_n_100: 0.4875 (0.5565)  triple_100: 8.1474 (8.5105)  triple_80: 8.2314 (8.6239)  triple_60: 7.3786 (7.5690)  triple_40: 4.8365 (4.9721)  time: 5.8904  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 470/1724]  eta: 2:03:08  lr: 0.000120  loss: 29.8524 (31.5463)  loss_n_40: 0.3191 (0.3990)  loss_n_60: 0.3598 (0.4426)  loss_n_80: 0.4137 (0.5008)  loss_n_100: 0.4662 (0.5545)  triple_100: 8.0217 (8.5011)  triple_80: 8.1614 (8.6147)  triple_60: 7.3132 (7.5628)  triple_40: 4.8258 (4.9707)  time: 5.8917  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 480/1724]  eta: 2:02:09  lr: 0.000120  loss: 30.8025 (31.5533)  loss_n_40: 0.3236 (0.3984)  loss_n_60: 0.3859 (0.4421)  loss_n_80: 0.4411 (0.5003)  loss_n_100: 0.4849 (0.5540)  triple_100: 8.3263 (8.5042)  triple_80: 8.4620 (8.6175)  triple_60: 7.3223 (7.5656)  triple_40: 4.8258 (4.9711)  time: 5.8924  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 490/1724]  eta: 2:01:10  lr: 0.000120  loss: 31.2024 (31.5471)  loss_n_40: 0.3454 (0.3976)  loss_n_60: 0.3981 (0.4413)  loss_n_80: 0.4574 (0.4994)  loss_n_100: 0.5105 (0.5530)  triple_100: 8.5202 (8.5027)  triple_80: 8.5623 (8.6154)  triple_60: 7.5673 (7.5656)  triple_40: 5.0001 (4.9721)  time: 5.8915  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 500/1724]  eta: 2:00:11  lr: 0.000120  loss: 31.9221 (31.5512)  loss_n_40: 0.3626 (0.3975)  loss_n_60: 0.4187 (0.4412)  loss_n_80: 0.4836 (0.4991)  loss_n_100: 0.5473 (0.5526)  triple_100: 8.6433 (8.5017)  triple_80: 8.6824 (8.6153)  triple_60: 7.7402 (7.5677)  triple_40: 5.0466 (4.9761)  time: 5.8909  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 510/1724]  eta: 1:59:12  lr: 0.000120  loss: 31.8776 (31.5740)  loss_n_40: 0.3626 (0.3972)  loss_n_60: 0.4187 (0.4409)  loss_n_80: 0.4836 (0.4987)  loss_n_100: 0.5473 (0.5522)  triple_100: 8.6221 (8.5068)  triple_80: 8.6824 (8.6208)  triple_60: 7.6600 (7.5745)  triple_40: 5.0818 (4.9828)  time: 5.8911  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:7]  [ 520/1724]  eta: 1:58:13  lr: 0.000120  loss: 31.8776 (31.5831)  loss_n_40: 0.3843 (0.3976)  loss_n_60: 0.4228 (0.4410)  loss_n_80: 0.4787 (0.4988)  loss_n_100: 0.5399 (0.5523)  triple_100: 8.5833 (8.5098)  triple_80: 8.7938 (8.6233)  triple_60: 7.6600 (7.5761)  triple_40: 5.0818 (4.9842)  time: 5.8916  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 530/1724]  eta: 1:57:14  lr: 0.000120  loss: 30.5252 (31.5679)  loss_n_40: 0.3636 (0.3964)  loss_n_60: 0.4063 (0.4399)  loss_n_80: 0.4783 (0.4979)  loss_n_100: 0.5359 (0.5515)  triple_100: 8.4184 (8.5080)  triple_80: 8.3637 (8.6200)  triple_60: 7.2253 (7.5719)  triple_40: 4.8544 (4.9824)  time: 5.8916  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [ 540/1724]  eta: 1:56:15  lr: 0.000120  loss: 30.5252 (31.5516)  loss_n_40: 0.3507 (0.3961)  loss_n_60: 0.4063 (0.4394)  loss_n_80: 0.4728 (0.4974)  loss_n_100: 0.5131 (0.5510)  triple_100: 8.1911 (8.5029)  triple_80: 8.3078 (8.6157)  triple_60: 7.2200 (7.5667)  triple_40: 4.8458 (4.9823)  time: 5.8912  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 550/1724]  eta: 1:55:16  lr: 0.000120  loss: 30.7972 (31.5373)  loss_n_40: 0.3574 (0.3953)  loss_n_60: 0.4137 (0.4385)  loss_n_80: 0.4728 (0.4964)  loss_n_100: 0.5125 (0.5498)  triple_100: 8.1911 (8.4978)  triple_80: 8.3078 (8.6109)  triple_60: 7.2929 (7.5642)  triple_40: 4.9112 (4.9844)  time: 5.8909  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 560/1724]  eta: 1:54:18  lr: 0.000120  loss: 31.5479 (31.5426)  loss_n_40: 0.3508 (0.3947)  loss_n_60: 0.4137 (0.4382)  loss_n_80: 0.4790 (0.4961)  loss_n_100: 0.5367 (0.5496)  triple_100: 8.6606 (8.5026)  triple_80: 8.7695 (8.6141)  triple_60: 7.5780 (7.5660)  triple_40: 4.8755 (4.9814)  time: 5.8913  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 570/1724]  eta: 1:53:19  lr: 0.000120  loss: 30.4988 (31.5210)  loss_n_40: 0.3430 (0.3939)  loss_n_60: 0.4060 (0.4376)  loss_n_80: 0.4807 (0.4956)  loss_n_100: 0.5285 (0.5490)  triple_100: 8.4494 (8.4997)  triple_80: 8.5246 (8.6102)  triple_60: 7.3108 (7.5610)  triple_40: 4.5743 (4.9740)  time: 5.8914  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 580/1724]  eta: 1:52:20  lr: 0.000120  loss: 30.1995 (31.5168)  loss_n_40: 0.3359 (0.3932)  loss_n_60: 0.3892 (0.4368)  loss_n_80: 0.4660 (0.4950)  loss_n_100: 0.5280 (0.5484)  triple_100: 8.1344 (8.5003)  triple_80: 8.3088 (8.6099)  triple_60: 7.3000 (7.5594)  triple_40: 4.6504 (4.9736)  time: 5.8920  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 590/1724]  eta: 1:51:21  lr: 0.000120  loss: 31.9728 (31.5328)  loss_n_40: 0.3643 (0.3930)  loss_n_60: 0.4152 (0.4367)  loss_n_80: 0.4760 (0.4948)  loss_n_100: 0.5283 (0.5482)  triple_100: 8.7482 (8.5036)  triple_80: 8.8259 (8.6136)  triple_60: 7.6108 (7.5640)  triple_40: 4.9765 (4.9790)  time: 5.8927  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 600/1724]  eta: 1:50:22  lr: 0.000120  loss: 31.3110 (31.5160)  loss_n_40: 0.3562 (0.3921)  loss_n_60: 0.3878 (0.4357)  loss_n_80: 0.4575 (0.4937)  loss_n_100: 0.5143 (0.5470)  triple_100: 8.5786 (8.4984)  triple_80: 8.6732 (8.6080)  triple_60: 7.5078 (7.5605)  triple_40: 5.2406 (4.9807)  time: 5.8920  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 610/1724]  eta: 1:49:23  lr: 0.000120  loss: 30.5558 (31.5113)  loss_n_40: 0.3538 (0.3917)  loss_n_60: 0.3808 (0.4352)  loss_n_80: 0.4421 (0.4931)  loss_n_100: 0.4724 (0.5462)  triple_100: 8.1616 (8.4951)  triple_80: 8.2319 (8.6055)  triple_60: 7.3837 (7.5602)  triple_40: 4.9181 (4.9844)  time: 5.8933  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 620/1724]  eta: 1:48:24  lr: 0.000120  loss: 30.2147 (31.4889)  loss_n_40: 0.3570 (0.3912)  loss_n_60: 0.3890 (0.4345)  loss_n_80: 0.4421 (0.4922)  loss_n_100: 0.4769 (0.5451)  triple_100: 8.0185 (8.4874)  triple_80: 8.1567 (8.5983)  triple_60: 7.3085 (7.5555)  triple_40: 4.9769 (4.9846)  time: 5.8941  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 630/1724]  eta: 1:47:25  lr: 0.000120  loss: 30.2147 (31.4767)  loss_n_40: 0.3359 (0.3904)  loss_n_60: 0.3709 (0.4338)  loss_n_80: 0.4223 (0.4914)  loss_n_100: 0.4722 (0.5444)  triple_100: 8.1325 (8.4852)  triple_80: 8.1685 (8.5953)  triple_60: 7.3178 (7.5535)  triple_40: 4.8859 (4.9828)  time: 5.8920  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 640/1724]  eta: 1:46:26  lr: 0.000120  loss: 30.3994 (31.4793)  loss_n_40: 0.3408 (0.3901)  loss_n_60: 0.3744 (0.4336)  loss_n_80: 0.4223 (0.4913)  loss_n_100: 0.4771 (0.5442)  triple_100: 8.4202 (8.4864)  triple_80: 8.3985 (8.5960)  triple_60: 7.3216 (7.5546)  triple_40: 4.8620 (4.9829)  time: 5.8917  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 650/1724]  eta: 1:45:27  lr: 0.000120  loss: 30.5442 (31.4783)  loss_n_40: 0.3493 (0.3896)  loss_n_60: 0.3905 (0.4332)  loss_n_80: 0.4559 (0.4908)  loss_n_100: 0.4970 (0.5437)  triple_100: 8.2077 (8.4868)  triple_80: 8.2495 (8.5953)  triple_60: 7.3417 (7.5547)  triple_40: 4.9429 (4.9842)  time: 5.8928  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 660/1724]  eta: 1:44:29  lr: 0.000120  loss: 30.7859 (31.4808)  loss_n_40: 0.3395 (0.3891)  loss_n_60: 0.3899 (0.4328)  loss_n_80: 0.4396 (0.4905)  loss_n_100: 0.4978 (0.5435)  triple_100: 8.4248 (8.4888)  triple_80: 8.4479 (8.5967)  triple_60: 7.4779 (7.5545)  triple_40: 5.0213 (4.9849)  time: 5.8934  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 670/1724]  eta: 1:43:30  lr: 0.000120  loss: 31.5388 (31.4904)  loss_n_40: 0.3366 (0.3887)  loss_n_60: 0.4100 (0.4328)  loss_n_80: 0.4840 (0.4908)  loss_n_100: 0.5477 (0.5440)  triple_100: 8.5409 (8.4949)  triple_80: 8.6468 (8.6014)  triple_60: 7.5817 (7.5552)  triple_40: 4.8329 (4.9827)  time: 5.8928  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 680/1724]  eta: 1:42:31  lr: 0.000120  loss: 31.5388 (31.5167)  loss_n_40: 0.3746 (0.3889)  loss_n_60: 0.4393 (0.4333)  loss_n_80: 0.5334 (0.4917)  loss_n_100: 0.6016 (0.5451)  triple_100: 8.8670 (8.5060)  triple_80: 8.8644 (8.6117)  triple_60: 7.4841 (7.5601)  triple_40: 4.6095 (4.9799)  time: 5.8917  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 690/1724]  eta: 1:41:32  lr: 0.000120  loss: 33.5130 (31.5524)  loss_n_40: 0.3899 (0.3890)  loss_n_60: 0.4623 (0.4338)  loss_n_80: 0.5429 (0.4926)  loss_n_100: 0.6267 (0.5463)  triple_100: 9.3694 (8.5199)  triple_80: 9.3528 (8.6240)  triple_60: 7.7785 (7.5666)  triple_40: 4.8416 (4.9802)  time: 5.8902  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 700/1724]  eta: 1:40:33  lr: 0.000120  loss: 31.9715 (31.5582)  loss_n_40: 0.3731 (0.3888)  loss_n_60: 0.4398 (0.4336)  loss_n_80: 0.5101 (0.4925)  loss_n_100: 0.5788 (0.5462)  triple_100: 8.8585 (8.5227)  triple_80: 8.8534 (8.6264)  triple_60: 7.5580 (7.5675)  triple_40: 4.9195 (4.9806)  time: 5.8901  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 710/1724]  eta: 1:39:34  lr: 0.000120  loss: 31.2048 (31.5719)  loss_n_40: 0.3587 (0.3889)  loss_n_60: 0.4193 (0.4338)  loss_n_80: 0.4679 (0.4927)  loss_n_100: 0.5186 (0.5462)  triple_100: 8.2738 (8.5261)  triple_80: 8.4279 (8.6297)  triple_60: 7.4913 (7.5713)  triple_40: 5.0347 (4.9833)  time: 5.8910  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 720/1724]  eta: 1:38:35  lr: 0.000120  loss: 31.2513 (31.5583)  loss_n_40: 0.3587 (0.3885)  loss_n_60: 0.4044 (0.4333)  loss_n_80: 0.4596 (0.4921)  loss_n_100: 0.4986 (0.5457)  triple_100: 8.3518 (8.5228)  triple_80: 8.3403 (8.6259)  triple_60: 7.3932 (7.5685)  triple_40: 4.9606 (4.9816)  time: 5.8906  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 730/1724]  eta: 1:37:36  lr: 0.000120  loss: 30.7507 (31.5468)  loss_n_40: 0.3376 (0.3881)  loss_n_60: 0.3941 (0.4327)  loss_n_80: 0.4563 (0.4914)  loss_n_100: 0.4986 (0.5448)  triple_100: 8.3518 (8.5182)  triple_80: 8.4506 (8.6218)  triple_60: 7.3546 (7.5662)  triple_40: 4.9606 (4.9836)  time: 5.8915  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 740/1724]  eta: 1:36:37  lr: 0.000120  loss: 30.7178 (31.5374)  loss_n_40: 0.3363 (0.3873)  loss_n_60: 0.3766 (0.4320)  loss_n_80: 0.4232 (0.4907)  loss_n_100: 0.4632 (0.5442)  triple_100: 8.0670 (8.5164)  triple_80: 8.2185 (8.6193)  triple_60: 7.3220 (7.5637)  triple_40: 4.9929 (4.9838)  time: 5.8920  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 750/1724]  eta: 1:35:38  lr: 0.000120  loss: 29.8230 (31.5353)  loss_n_40: 0.3510 (0.3877)  loss_n_60: 0.3889 (0.4321)  loss_n_80: 0.4396 (0.4907)  loss_n_100: 0.4753 (0.5440)  triple_100: 8.0705 (8.5155)  triple_80: 8.2185 (8.6187)  triple_60: 7.1626 (7.5631)  triple_40: 4.9358 (4.9836)  time: 5.8919  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 760/1724]  eta: 1:34:39  lr: 0.000120  loss: 30.7613 (31.5435)  loss_n_40: 0.3598 (0.3875)  loss_n_60: 0.4140 (0.4321)  loss_n_80: 0.4892 (0.4910)  loss_n_100: 0.5532 (0.5445)  triple_100: 8.4312 (8.5207)  triple_80: 8.4743 (8.6226)  triple_60: 7.2441 (7.5637)  triple_40: 4.7605 (4.9815)  time: 5.8936  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 770/1724]  eta: 1:33:40  lr: 0.000120  loss: 31.4628 (31.5464)  loss_n_40: 0.3403 (0.3871)  loss_n_60: 0.4140 (0.4318)  loss_n_80: 0.4957 (0.4908)  loss_n_100: 0.5557 (0.5444)  triple_100: 8.7205 (8.5235)  triple_80: 8.7179 (8.6245)  triple_60: 7.4642 (7.5640)  triple_40: 4.7738 (4.9802)  time: 5.8940  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [ 780/1724]  eta: 1:32:41  lr: 0.000120  loss: 31.4628 (31.5351)  loss_n_40: 0.3484 (0.3870)  loss_n_60: 0.4089 (0.4316)  loss_n_80: 0.4700 (0.4907)  loss_n_100: 0.5098 (0.5440)  triple_100: 8.5632 (8.5199)  triple_80: 8.6958 (8.6227)  triple_60: 7.4387 (7.5615)  triple_40: 4.7738 (4.9778)  time: 5.8931  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 790/1724]  eta: 1:31:43  lr: 0.000120  loss: 31.1018 (31.5392)  loss_n_40: 0.3484 (0.3867)  loss_n_60: 0.4065 (0.4313)  loss_n_80: 0.4608 (0.4903)  loss_n_100: 0.5076 (0.5435)  triple_100: 8.3480 (8.5199)  triple_80: 8.5105 (8.6234)  triple_60: 7.4387 (7.5632)  triple_40: 4.9273 (4.9808)  time: 5.8933  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 800/1724]  eta: 1:30:44  lr: 0.000120  loss: 30.8097 (31.5223)  loss_n_40: 0.3203 (0.3858)  loss_n_60: 0.3733 (0.4305)  loss_n_80: 0.4230 (0.4894)  loss_n_100: 0.4714 (0.5426)  triple_100: 8.3540 (8.5156)  triple_80: 8.3961 (8.6187)  triple_60: 7.4016 (7.5596)  triple_40: 4.9636 (4.9799)  time: 5.8944  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 810/1724]  eta: 1:29:45  lr: 0.000120  loss: 30.6599 (31.5125)  loss_n_40: 0.3196 (0.3851)  loss_n_60: 0.3734 (0.4299)  loss_n_80: 0.4363 (0.4890)  loss_n_100: 0.4906 (0.5423)  triple_100: 8.3368 (8.5149)  triple_80: 8.3614 (8.6172)  triple_60: 7.3665 (7.5571)  triple_40: 4.8139 (4.9769)  time: 5.8956  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 820/1724]  eta: 1:28:46  lr: 0.000120  loss: 29.8352 (31.4854)  loss_n_40: 0.3074 (0.3842)  loss_n_60: 0.3458 (0.4290)  loss_n_80: 0.4002 (0.4878)  loss_n_100: 0.4469 (0.5409)  triple_100: 8.1002 (8.5062)  triple_80: 8.2077 (8.6087)  triple_60: 7.1097 (7.5519)  triple_40: 4.8135 (4.9767)  time: 5.8935  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 830/1724]  eta: 1:27:47  lr: 0.000120  loss: 29.1089 (31.4705)  loss_n_40: 0.3068 (0.3838)  loss_n_60: 0.3383 (0.4284)  loss_n_80: 0.3783 (0.4871)  loss_n_100: 0.4269 (0.5400)  triple_100: 7.7904 (8.5010)  triple_80: 7.9124 (8.6035)  triple_60: 7.0959 (7.5496)  triple_40: 5.0517 (4.9771)  time: 5.8919  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 840/1724]  eta: 1:26:48  lr: 0.000120  loss: 29.8420 (31.4656)  loss_n_40: 0.3068 (0.3832)  loss_n_60: 0.3525 (0.4279)  loss_n_80: 0.4130 (0.4866)  loss_n_100: 0.4550 (0.5395)  triple_100: 7.9466 (8.5000)  triple_80: 8.0035 (8.6022)  triple_60: 7.2330 (7.5488)  triple_40: 4.8564 (4.9773)  time: 5.8927  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 850/1724]  eta: 1:25:49  lr: 0.000120  loss: 29.4192 (31.4388)  loss_n_40: 0.3047 (0.3825)  loss_n_60: 0.3467 (0.4271)  loss_n_80: 0.3963 (0.4857)  loss_n_100: 0.4500 (0.5386)  triple_100: 7.9803 (8.4935)  triple_80: 7.9952 (8.5949)  triple_60: 7.0970 (7.5424)  triple_40: 4.8137 (4.9742)  time: 5.8936  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 860/1724]  eta: 1:24:50  lr: 0.000120  loss: 29.1539 (31.4181)  loss_n_40: 0.3081 (0.3818)  loss_n_60: 0.3365 (0.4262)  loss_n_80: 0.3865 (0.4847)  loss_n_100: 0.4318 (0.5376)  triple_100: 7.8406 (8.4870)  triple_80: 7.8957 (8.5880)  triple_60: 6.9802 (7.5378)  triple_40: 4.8802 (4.9750)  time: 5.8926  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 870/1724]  eta: 1:23:51  lr: 0.000120  loss: 29.4632 (31.4142)  loss_n_40: 0.3168 (0.3815)  loss_n_60: 0.3383 (0.4258)  loss_n_80: 0.3865 (0.4841)  loss_n_100: 0.4318 (0.5368)  triple_100: 7.8883 (8.4841)  triple_80: 7.9341 (8.5856)  triple_60: 7.1205 (7.5379)  triple_40: 4.9280 (4.9785)  time: 5.8916  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 880/1724]  eta: 1:22:52  lr: 0.000120  loss: 31.0464 (31.4116)  loss_n_40: 0.3462 (0.3811)  loss_n_60: 0.3854 (0.4254)  loss_n_80: 0.4345 (0.4838)  loss_n_100: 0.4871 (0.5366)  triple_100: 8.2143 (8.4847)  triple_80: 8.2832 (8.5855)  triple_60: 7.5582 (7.5368)  triple_40: 4.8909 (4.9777)  time: 5.8927  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 890/1724]  eta: 1:21:53  lr: 0.000120  loss: 31.0360 (31.4098)  loss_n_40: 0.3376 (0.3806)  loss_n_60: 0.3953 (0.4251)  loss_n_80: 0.4625 (0.4835)  loss_n_100: 0.5095 (0.5363)  triple_100: 8.3819 (8.4853)  triple_80: 8.4573 (8.5854)  triple_60: 7.3614 (7.5363)  triple_40: 4.8362 (4.9772)  time: 5.8928  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 900/1724]  eta: 1:20:55  lr: 0.000120  loss: 31.0521 (31.4126)  loss_n_40: 0.3416 (0.3804)  loss_n_60: 0.3953 (0.4250)  loss_n_80: 0.4749 (0.4834)  loss_n_100: 0.5385 (0.5363)  triple_100: 8.6101 (8.4868)  triple_80: 8.6131 (8.5863)  triple_60: 7.4144 (7.5370)  triple_40: 5.0063 (4.9774)  time: 5.8920  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 910/1724]  eta: 1:19:56  lr: 0.000120  loss: 31.1559 (31.4065)  loss_n_40: 0.3536 (0.3803)  loss_n_60: 0.4246 (0.4248)  loss_n_80: 0.4833 (0.4832)  loss_n_100: 0.5312 (0.5360)  triple_100: 8.4306 (8.4846)  triple_80: 8.4963 (8.5844)  triple_60: 7.6565 (7.5362)  triple_40: 5.0063 (4.9769)  time: 5.8911  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 920/1724]  eta: 1:18:57  lr: 0.000120  loss: 30.9668 (31.3983)  loss_n_40: 0.3505 (0.3803)  loss_n_60: 0.3980 (0.4246)  loss_n_80: 0.4518 (0.4829)  loss_n_100: 0.4996 (0.5356)  triple_100: 8.2528 (8.4813)  triple_80: 8.3840 (8.5812)  triple_60: 7.3319 (7.5342)  triple_40: 4.9109 (4.9783)  time: 5.8908  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 930/1724]  eta: 1:17:58  lr: 0.000120  loss: 29.8417 (31.3756)  loss_n_40: 0.3412 (0.3797)  loss_n_60: 0.3790 (0.4239)  loss_n_80: 0.4341 (0.4822)  loss_n_100: 0.4810 (0.5350)  triple_100: 8.0918 (8.4756)  triple_80: 8.1400 (8.5750)  triple_60: 7.1530 (7.5286)  triple_40: 4.8259 (4.9757)  time: 5.8903  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 940/1724]  eta: 1:16:59  lr: 0.000120  loss: 29.5706 (31.3590)  loss_n_40: 0.3157 (0.3791)  loss_n_60: 0.3539 (0.4233)  loss_n_80: 0.4153 (0.4817)  loss_n_100: 0.4730 (0.5345)  triple_100: 8.0799 (8.4723)  triple_80: 8.1400 (8.5712)  triple_60: 7.0822 (7.5246)  triple_40: 4.7238 (4.9723)  time: 5.8897  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 950/1724]  eta: 1:16:00  lr: 0.000120  loss: 30.0669 (31.3511)  loss_n_40: 0.3272 (0.3788)  loss_n_60: 0.3658 (0.4229)  loss_n_80: 0.4223 (0.4812)  loss_n_100: 0.4755 (0.5339)  triple_100: 8.1185 (8.4697)  triple_80: 8.2707 (8.5686)  triple_60: 7.2042 (7.5233)  triple_40: 4.7541 (4.9727)  time: 5.8895  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 960/1724]  eta: 1:15:01  lr: 0.000120  loss: 30.5627 (31.3535)  loss_n_40: 0.3345 (0.3786)  loss_n_60: 0.3837 (0.4227)  loss_n_80: 0.4280 (0.4810)  loss_n_100: 0.4680 (0.5336)  triple_100: 8.3137 (8.4700)  triple_80: 8.3437 (8.5693)  triple_60: 7.3957 (7.5243)  triple_40: 4.8742 (4.9740)  time: 5.8888  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 970/1724]  eta: 1:14:02  lr: 0.000120  loss: 30.5627 (31.3368)  loss_n_40: 0.3277 (0.3781)  loss_n_60: 0.3831 (0.4222)  loss_n_80: 0.4378 (0.4804)  loss_n_100: 0.4731 (0.5330)  triple_100: 8.2982 (8.4659)  triple_80: 8.3437 (8.5648)  triple_60: 7.3481 (7.5205)  triple_40: 4.8378 (4.9719)  time: 5.8898  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 980/1724]  eta: 1:13:03  lr: 0.000120  loss: 30.3321 (31.3279)  loss_n_40: 0.3267 (0.3776)  loss_n_60: 0.3669 (0.4218)  loss_n_80: 0.4220 (0.4800)  loss_n_100: 0.4859 (0.5327)  triple_100: 8.2982 (8.4643)  triple_80: 8.3930 (8.5628)  triple_60: 7.2744 (7.5188)  triple_40: 4.7653 (4.9699)  time: 5.8912  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [ 990/1724]  eta: 1:12:04  lr: 0.000120  loss: 30.6044 (31.3185)  loss_n_40: 0.3426 (0.3774)  loss_n_60: 0.3700 (0.4216)  loss_n_80: 0.4436 (0.4798)  loss_n_100: 0.4863 (0.5324)  triple_100: 8.3487 (8.4620)  triple_80: 8.3655 (8.5603)  triple_60: 7.2744 (7.5165)  triple_40: 4.7604 (4.9686)  time: 5.8907  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1000/1724]  eta: 1:11:05  lr: 0.000120  loss: 30.7642 (31.3170)  loss_n_40: 0.3594 (0.3772)  loss_n_60: 0.3792 (0.4215)  loss_n_80: 0.4436 (0.4797)  loss_n_100: 0.4923 (0.5323)  triple_100: 8.3487 (8.4624)  triple_80: 8.3624 (8.5602)  triple_60: 7.3984 (7.5158)  triple_40: 4.8304 (4.9679)  time: 5.8910  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1010/1724]  eta: 1:10:06  lr: 0.000120  loss: 30.3210 (31.3116)  loss_n_40: 0.3196 (0.3769)  loss_n_60: 0.3718 (0.4211)  loss_n_80: 0.4331 (0.4794)  loss_n_100: 0.4923 (0.5320)  triple_100: 8.3746 (8.4613)  triple_80: 8.3624 (8.5586)  triple_60: 7.2818 (7.5142)  triple_40: 4.9098 (4.9681)  time: 5.8905  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [1020/1724]  eta: 1:09:07  lr: 0.000120  loss: 31.1348 (31.3296)  loss_n_40: 0.3502 (0.3770)  loss_n_60: 0.4058 (0.4214)  loss_n_80: 0.4689 (0.4798)  loss_n_100: 0.5186 (0.5325)  triple_100: 8.4545 (8.4665)  triple_80: 8.4740 (8.5637)  triple_60: 7.4552 (7.5185)  triple_40: 4.9827 (4.9702)  time: 5.8916  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1030/1724]  eta: 1:08:08  lr: 0.000120  loss: 33.0369 (31.3333)  loss_n_40: 0.3804 (0.3771)  loss_n_60: 0.4329 (0.4216)  loss_n_80: 0.5247 (0.4801)  loss_n_100: 0.6038 (0.5328)  triple_100: 8.9524 (8.4681)  triple_80: 9.0384 (8.5652)  triple_60: 7.8654 (7.5195)  triple_40: 4.9040 (4.9690)  time: 5.8935  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1040/1724]  eta: 1:07:10  lr: 0.000120  loss: 30.7220 (31.3373)  loss_n_40: 0.3621 (0.3771)  loss_n_60: 0.4164 (0.4217)  loss_n_80: 0.4727 (0.4801)  loss_n_100: 0.5249 (0.5328)  triple_100: 8.2603 (8.4692)  triple_80: 8.3333 (8.5666)  triple_60: 7.5904 (7.5209)  triple_40: 4.8225 (4.9690)  time: 5.8915  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1050/1724]  eta: 1:06:11  lr: 0.000120  loss: 30.6988 (31.3340)  loss_n_40: 0.3552 (0.3767)  loss_n_60: 0.4019 (0.4214)  loss_n_80: 0.4537 (0.4798)  loss_n_100: 0.4917 (0.5325)  triple_100: 8.2603 (8.4690)  triple_80: 8.3228 (8.5658)  triple_60: 7.4272 (7.5202)  triple_40: 4.8816 (4.9686)  time: 5.8902  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1060/1724]  eta: 1:05:12  lr: 0.000120  loss: 30.8948 (31.3393)  loss_n_40: 0.3516 (0.3767)  loss_n_60: 0.3935 (0.4214)  loss_n_80: 0.4507 (0.4799)  loss_n_100: 0.4957 (0.5326)  triple_100: 8.3408 (8.4712)  triple_80: 8.4123 (8.5676)  triple_60: 7.4272 (7.5212)  triple_40: 4.9410 (4.9688)  time: 5.8905  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1070/1724]  eta: 1:04:13  lr: 0.000120  loss: 31.2256 (31.3466)  loss_n_40: 0.3555 (0.3764)  loss_n_60: 0.3971 (0.4212)  loss_n_80: 0.4600 (0.4798)  loss_n_100: 0.5144 (0.5326)  triple_100: 8.6742 (8.4739)  triple_80: 8.6748 (8.5699)  triple_60: 7.5538 (7.5231)  triple_40: 5.0053 (4.9697)  time: 5.8901  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1080/1724]  eta: 1:03:14  lr: 0.000120  loss: 30.3496 (31.3318)  loss_n_40: 0.3313 (0.3761)  loss_n_60: 0.3759 (0.4208)  loss_n_80: 0.4292 (0.4792)  loss_n_100: 0.4747 (0.5319)  triple_100: 8.1978 (8.4688)  triple_80: 8.3415 (8.5653)  triple_60: 7.4391 (7.5204)  triple_40: 4.9701 (4.9693)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1090/1724]  eta: 1:02:15  lr: 0.000120  loss: 30.3297 (31.3388)  loss_n_40: 0.3817 (0.3761)  loss_n_60: 0.3928 (0.4208)  loss_n_80: 0.4292 (0.4793)  loss_n_100: 0.4747 (0.5318)  triple_100: 8.0799 (8.4698)  triple_80: 8.2008 (8.5673)  triple_60: 7.4443 (7.5224)  triple_40: 4.9934 (4.9712)  time: 5.8866  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1100/1724]  eta: 1:01:16  lr: 0.000120  loss: 30.8409 (31.3322)  loss_n_40: 0.3607 (0.3758)  loss_n_60: 0.4104 (0.4204)  loss_n_80: 0.4628 (0.4788)  loss_n_100: 0.5135 (0.5313)  triple_100: 8.3020 (8.4674)  triple_80: 8.5017 (8.5650)  triple_60: 7.4443 (7.5215)  triple_40: 4.9934 (4.9721)  time: 5.8875  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1110/1724]  eta: 1:00:17  lr: 0.000120  loss: 29.8728 (31.3312)  loss_n_40: 0.3277 (0.3756)  loss_n_60: 0.3568 (0.4202)  loss_n_80: 0.4200 (0.4786)  loss_n_100: 0.4793 (0.5311)  triple_100: 7.8821 (8.4667)  triple_80: 8.1386 (8.5645)  triple_60: 7.1573 (7.5215)  triple_40: 4.9431 (4.9732)  time: 5.8890  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1120/1724]  eta: 0:59:18  lr: 0.000120  loss: 30.5522 (31.3372)  loss_n_40: 0.3304 (0.3753)  loss_n_60: 0.3970 (0.4201)  loss_n_80: 0.4497 (0.4786)  loss_n_100: 0.4977 (0.5311)  triple_100: 8.3566 (8.4696)  triple_80: 8.3514 (8.5670)  triple_60: 7.3118 (7.5228)  triple_40: 4.8673 (4.9726)  time: 5.8890  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:7]  [1130/1724]  eta: 0:58:19  lr: 0.000120  loss: 30.8278 (31.3317)  loss_n_40: 0.3329 (0.3749)  loss_n_60: 0.3970 (0.4199)  loss_n_80: 0.4633 (0.4783)  loss_n_100: 0.5068 (0.5307)  triple_100: 8.4446 (8.4676)  triple_80: 8.4701 (8.5653)  triple_60: 7.3720 (7.5217)  triple_40: 4.8673 (4.9732)  time: 5.8885  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:7]  [1140/1724]  eta: 0:57:20  lr: 0.000120  loss: 29.3979 (31.3176)  loss_n_40: 0.3188 (0.3745)  loss_n_60: 0.3527 (0.4193)  loss_n_80: 0.3948 (0.4777)  loss_n_100: 0.4397 (0.5301)  triple_100: 7.9296 (8.4635)  triple_80: 7.9981 (8.5613)  triple_60: 7.1563 (7.5188)  triple_40: 4.8390 (4.9723)  time: 5.8888  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1150/1724]  eta: 0:56:21  lr: 0.000120  loss: 29.3942 (31.3080)  loss_n_40: 0.3032 (0.3739)  loss_n_60: 0.3504 (0.4188)  loss_n_80: 0.3927 (0.4772)  loss_n_100: 0.4335 (0.5296)  triple_100: 7.9296 (8.4616)  triple_80: 7.9981 (8.5588)  triple_60: 7.1429 (7.5166)  triple_40: 4.8195 (4.9716)  time: 5.8912  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1160/1724]  eta: 0:55:22  lr: 0.000120  loss: 28.9769 (31.2825)  loss_n_40: 0.2971 (0.3733)  loss_n_60: 0.3408 (0.4181)  loss_n_80: 0.3834 (0.4763)  loss_n_100: 0.4239 (0.5286)  triple_100: 7.7021 (8.4541)  triple_80: 7.9683 (8.5510)  triple_60: 7.0273 (7.5113)  triple_40: 4.7601 (4.9698)  time: 5.8907  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1170/1724]  eta: 0:54:23  lr: 0.000120  loss: 29.1850 (31.2667)  loss_n_40: 0.2971 (0.3726)  loss_n_60: 0.3385 (0.4175)  loss_n_80: 0.3834 (0.4756)  loss_n_100: 0.4239 (0.5279)  triple_100: 7.8715 (8.4503)  triple_80: 7.9683 (8.5463)  triple_60: 7.0273 (7.5079)  triple_40: 4.7868 (4.9686)  time: 5.8888  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1180/1724]  eta: 0:53:25  lr: 0.000120  loss: 29.7265 (31.2631)  loss_n_40: 0.3171 (0.3725)  loss_n_60: 0.3717 (0.4173)  loss_n_80: 0.4212 (0.4754)  loss_n_100: 0.4744 (0.5277)  triple_100: 8.1217 (8.4490)  triple_80: 8.1229 (8.5452)  triple_60: 7.2440 (7.5079)  triple_40: 4.7538 (4.9682)  time: 5.8889  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1190/1724]  eta: 0:52:26  lr: 0.000120  loss: 29.8936 (31.2577)  loss_n_40: 0.3205 (0.3722)  loss_n_60: 0.3745 (0.4171)  loss_n_80: 0.4314 (0.4750)  loss_n_100: 0.4744 (0.5272)  triple_100: 8.1217 (8.4472)  triple_80: 8.1229 (8.5432)  triple_60: 7.2879 (7.5074)  triple_40: 4.7815 (4.9683)  time: 5.8893  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1200/1724]  eta: 0:51:27  lr: 0.000120  loss: 29.0686 (31.2399)  loss_n_40: 0.3094 (0.3716)  loss_n_60: 0.3572 (0.4165)  loss_n_80: 0.4058 (0.4744)  loss_n_100: 0.4498 (0.5265)  triple_100: 7.7170 (8.4420)  triple_80: 7.8631 (8.5380)  triple_60: 7.1140 (7.5037)  triple_40: 4.8521 (4.9672)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1210/1724]  eta: 0:50:28  lr: 0.000120  loss: 28.7145 (31.2274)  loss_n_40: 0.3076 (0.3712)  loss_n_60: 0.3472 (0.4160)  loss_n_80: 0.4058 (0.4739)  loss_n_100: 0.4498 (0.5259)  triple_100: 7.7170 (8.4384)  triple_80: 7.7847 (8.5343)  triple_60: 6.9692 (7.5014)  triple_40: 4.7721 (4.9663)  time: 5.8893  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1220/1724]  eta: 0:49:29  lr: 0.000120  loss: 28.8405 (31.2150)  loss_n_40: 0.3062 (0.3708)  loss_n_60: 0.3448 (0.4156)  loss_n_80: 0.3949 (0.4734)  loss_n_100: 0.4412 (0.5254)  triple_100: 7.8978 (8.4344)  triple_80: 7.9272 (8.5303)  triple_60: 6.9844 (7.4987)  triple_40: 4.8365 (4.9665)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1230/1724]  eta: 0:48:30  lr: 0.000120  loss: 29.0744 (31.2003)  loss_n_40: 0.3062 (0.3703)  loss_n_60: 0.3388 (0.4151)  loss_n_80: 0.3882 (0.4727)  loss_n_100: 0.4412 (0.5248)  triple_100: 7.8936 (8.4305)  triple_80: 7.8687 (8.5259)  triple_60: 6.9876 (7.4952)  triple_40: 4.8742 (4.9658)  time: 5.8891  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1240/1724]  eta: 0:47:31  lr: 0.000120  loss: 29.3638 (31.1915)  loss_n_40: 0.3110 (0.3701)  loss_n_60: 0.3553 (0.4147)  loss_n_80: 0.4100 (0.4724)  loss_n_100: 0.4532 (0.5243)  triple_100: 7.9617 (8.4276)  triple_80: 8.0730 (8.5231)  triple_60: 7.0997 (7.4933)  triple_40: 4.8144 (4.9659)  time: 5.8891  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1250/1724]  eta: 0:46:32  lr: 0.000120  loss: 29.3362 (31.1716)  loss_n_40: 0.3121 (0.3696)  loss_n_60: 0.3553 (0.4142)  loss_n_80: 0.4100 (0.4717)  loss_n_100: 0.4509 (0.5235)  triple_100: 7.8049 (8.4214)  triple_80: 7.9804 (8.5173)  triple_60: 7.1077 (7.4891)  triple_40: 4.8228 (4.9648)  time: 5.8894  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [1260/1724]  eta: 0:45:33  lr: 0.000120  loss: 29.3338 (31.1732)  loss_n_40: 0.3255 (0.3696)  loss_n_60: 0.3515 (0.4141)  loss_n_80: 0.4033 (0.4716)  loss_n_100: 0.4481 (0.5234)  triple_100: 7.7014 (8.4211)  triple_80: 7.9382 (8.5174)  triple_60: 6.9997 (7.4899)  triple_40: 4.8870 (4.9660)  time: 5.8895  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1270/1724]  eta: 0:44:34  lr: 0.000120  loss: 29.7295 (31.1671)  loss_n_40: 0.3358 (0.3695)  loss_n_60: 0.3648 (0.4139)  loss_n_80: 0.4111 (0.4714)  loss_n_100: 0.4559 (0.5231)  triple_100: 7.9749 (8.4191)  triple_80: 8.0741 (8.5153)  triple_60: 7.1183 (7.4885)  triple_40: 5.0607 (4.9662)  time: 5.8894  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1280/1724]  eta: 0:43:35  lr: 0.000120  loss: 29.4273 (31.1585)  loss_n_40: 0.3183 (0.3692)  loss_n_60: 0.3550 (0.4135)  loss_n_80: 0.4028 (0.4710)  loss_n_100: 0.4504 (0.5227)  triple_100: 7.8682 (8.4168)  triple_80: 7.9326 (8.5129)  triple_60: 7.1018 (7.4868)  triple_40: 4.7666 (4.9655)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1290/1724]  eta: 0:42:36  lr: 0.000120  loss: 29.7597 (31.1554)  loss_n_40: 0.3056 (0.3689)  loss_n_60: 0.3550 (0.4132)  loss_n_80: 0.4076 (0.4707)  loss_n_100: 0.4504 (0.5224)  triple_100: 8.0617 (8.4163)  triple_80: 8.0590 (8.5121)  triple_60: 7.1385 (7.4862)  triple_40: 4.9004 (4.9656)  time: 5.8894  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1300/1724]  eta: 0:41:37  lr: 0.000120  loss: 29.2548 (31.1358)  loss_n_40: 0.2964 (0.3682)  loss_n_60: 0.3401 (0.4126)  loss_n_80: 0.3991 (0.4700)  loss_n_100: 0.4441 (0.5217)  triple_100: 7.8502 (8.4114)  triple_80: 7.9849 (8.5071)  triple_60: 7.0195 (7.4815)  triple_40: 4.7428 (4.9632)  time: 5.8884  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1310/1724]  eta: 0:40:39  lr: 0.000120  loss: 29.1966 (31.1289)  loss_n_40: 0.2933 (0.3678)  loss_n_60: 0.3351 (0.4121)  loss_n_80: 0.3933 (0.4696)  loss_n_100: 0.4422 (0.5212)  triple_100: 7.9135 (8.4094)  triple_80: 7.9849 (8.5050)  triple_60: 7.0195 (7.4801)  triple_40: 4.7586 (4.9637)  time: 5.8876  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1320/1724]  eta: 0:39:40  lr: 0.000120  loss: 29.4061 (31.1183)  loss_n_40: 0.2933 (0.3675)  loss_n_60: 0.3429 (0.4118)  loss_n_80: 0.4031 (0.4692)  loss_n_100: 0.4582 (0.5208)  triple_100: 7.9779 (8.4067)  triple_80: 8.0944 (8.5022)  triple_60: 7.0755 (7.4779)  triple_40: 4.7783 (4.9621)  time: 5.8883  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1330/1724]  eta: 0:38:41  lr: 0.000120  loss: 29.7806 (31.1097)  loss_n_40: 0.3155 (0.3673)  loss_n_60: 0.3528 (0.4115)  loss_n_80: 0.4166 (0.4689)  loss_n_100: 0.4655 (0.5204)  triple_100: 8.0417 (8.4046)  triple_80: 8.0761 (8.4996)  triple_60: 7.1472 (7.4760)  triple_40: 4.7155 (4.9614)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1340/1724]  eta: 0:37:42  lr: 0.000120  loss: 29.7698 (31.1001)  loss_n_40: 0.3152 (0.3669)  loss_n_60: 0.3614 (0.4112)  loss_n_80: 0.4166 (0.4686)  loss_n_100: 0.4655 (0.5201)  triple_100: 8.0417 (8.4023)  triple_80: 8.0761 (8.4972)  triple_60: 7.1472 (7.4739)  triple_40: 4.7155 (4.9599)  time: 5.8890  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1350/1724]  eta: 0:36:43  lr: 0.000120  loss: 29.9797 (31.0942)  loss_n_40: 0.3137 (0.3666)  loss_n_60: 0.3717 (0.4111)  loss_n_80: 0.4381 (0.4685)  loss_n_100: 0.4974 (0.5201)  triple_100: 8.2756 (8.4017)  triple_80: 8.2685 (8.4964)  triple_60: 7.1557 (7.4722)  triple_40: 4.6314 (4.9576)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1360/1724]  eta: 0:35:44  lr: 0.000120  loss: 31.7465 (31.1112)  loss_n_40: 0.3592 (0.3670)  loss_n_60: 0.4293 (0.4116)  loss_n_80: 0.4906 (0.4691)  loss_n_100: 0.5448 (0.5207)  triple_100: 8.4394 (8.4058)  triple_80: 8.6350 (8.5011)  triple_60: 7.5693 (7.4763)  triple_40: 4.9699 (4.9595)  time: 5.8878  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1370/1724]  eta: 0:34:45  lr: 0.000120  loss: 34.1192 (31.1352)  loss_n_40: 0.4392 (0.3676)  loss_n_60: 0.4755 (0.4121)  loss_n_80: 0.5340 (0.4695)  loss_n_100: 0.5800 (0.5211)  triple_100: 8.8537 (8.4104)  triple_80: 9.1697 (8.5067)  triple_60: 8.2343 (7.4826)  triple_40: 5.5237 (4.9652)  time: 5.8881  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1380/1724]  eta: 0:33:46  lr: 0.000120  loss: 34.0682 (31.1517)  loss_n_40: 0.4097 (0.3678)  loss_n_60: 0.4691 (0.4123)  loss_n_80: 0.5340 (0.4698)  loss_n_100: 0.5806 (0.5215)  triple_100: 9.0846 (8.4148)  triple_80: 9.1788 (8.5114)  triple_60: 8.2197 (7.4868)  triple_40: 5.5211 (4.9673)  time: 5.8881  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1390/1724]  eta: 0:32:47  lr: 0.000120  loss: 31.7714 (31.1445)  loss_n_40: 0.3652 (0.3678)  loss_n_60: 0.4011 (0.4122)  loss_n_80: 0.4554 (0.4696)  loss_n_100: 0.4916 (0.5212)  triple_100: 8.3545 (8.4120)  triple_80: 8.5104 (8.5089)  triple_60: 7.6641 (7.4853)  triple_40: 5.0173 (4.9674)  time: 5.8878  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1400/1724]  eta: 0:31:48  lr: 0.000120  loss: 30.3559 (31.1556)  loss_n_40: 0.3358 (0.3679)  loss_n_60: 0.3834 (0.4124)  loss_n_80: 0.4382 (0.4698)  loss_n_100: 0.4890 (0.5214)  triple_100: 8.1488 (8.4151)  triple_80: 8.3117 (8.5125)  triple_60: 7.3415 (7.4882)  triple_40: 4.8929 (4.9682)  time: 5.8879  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1410/1724]  eta: 0:30:49  lr: 0.000120  loss: 31.2037 (31.1551)  loss_n_40: 0.3685 (0.3679)  loss_n_60: 0.4056 (0.4123)  loss_n_80: 0.4593 (0.4696)  loss_n_100: 0.5036 (0.5211)  triple_100: 8.4448 (8.4136)  triple_80: 8.6405 (8.5117)  triple_60: 7.4549 (7.4890)  triple_40: 5.0075 (4.9698)  time: 5.8901  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1420/1724]  eta: 0:29:50  lr: 0.000120  loss: 30.7683 (31.1515)  loss_n_40: 0.3772 (0.3680)  loss_n_60: 0.4088 (0.4122)  loss_n_80: 0.4482 (0.4694)  loss_n_100: 0.4853 (0.5207)  triple_100: 8.0075 (8.4105)  triple_80: 8.2813 (8.5093)  triple_60: 7.4622 (7.4891)  triple_40: 5.2351 (4.9723)  time: 5.8908  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1430/1724]  eta: 0:28:51  lr: 0.000120  loss: 30.8031 (31.1559)  loss_n_40: 0.3833 (0.3683)  loss_n_60: 0.4143 (0.4123)  loss_n_80: 0.4506 (0.4694)  loss_n_100: 0.4925 (0.5207)  triple_100: 8.0075 (8.4104)  triple_80: 8.2813 (8.5096)  triple_60: 7.5154 (7.4906)  triple_40: 5.2907 (4.9745)  time: 5.8899  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1440/1724]  eta: 0:27:53  lr: 0.000120  loss: 31.0276 (31.1503)  loss_n_40: 0.3812 (0.3683)  loss_n_60: 0.4145 (0.4122)  loss_n_80: 0.4652 (0.4692)  loss_n_100: 0.5092 (0.5205)  triple_100: 8.4726 (8.4078)  triple_80: 8.5597 (8.5075)  triple_60: 7.4895 (7.4897)  triple_40: 5.1007 (4.9753)  time: 5.8898  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1450/1724]  eta: 0:26:54  lr: 0.000120  loss: 29.5375 (31.1388)  loss_n_40: 0.3479 (0.3681)  loss_n_60: 0.3737 (0.4120)  loss_n_80: 0.4174 (0.4689)  loss_n_100: 0.4506 (0.5200)  triple_100: 7.7069 (8.4033)  triple_80: 7.8571 (8.5037)  triple_60: 7.1566 (7.4878)  triple_40: 4.9905 (4.9750)  time: 5.8889  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1460/1724]  eta: 0:25:55  lr: 0.000120  loss: 28.1226 (31.1195)  loss_n_40: 0.3206 (0.3677)  loss_n_60: 0.3585 (0.4116)  loss_n_80: 0.4093 (0.4684)  loss_n_100: 0.4611 (0.5196)  triple_100: 7.4814 (8.3977)  triple_80: 7.6767 (8.4983)  triple_60: 6.8470 (7.4834)  triple_40: 4.7591 (4.9726)  time: 5.8881  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1470/1724]  eta: 0:24:56  lr: 0.000120  loss: 29.3798 (31.1252)  loss_n_40: 0.3206 (0.3677)  loss_n_60: 0.3732 (0.4117)  loss_n_80: 0.4314 (0.4685)  loss_n_100: 0.4740 (0.5197)  triple_100: 7.8995 (8.3996)  triple_80: 7.9457 (8.5002)  triple_60: 7.1162 (7.4851)  triple_40: 4.7591 (4.9727)  time: 5.8882  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1480/1724]  eta: 0:23:57  lr: 0.000120  loss: 31.8978 (31.1341)  loss_n_40: 0.3459 (0.3677)  loss_n_60: 0.4053 (0.4118)  loss_n_80: 0.4687 (0.4687)  loss_n_100: 0.5280 (0.5199)  triple_100: 8.7077 (8.4028)  triple_80: 8.7926 (8.5031)  triple_60: 7.7337 (7.4870)  triple_40: 4.9807 (4.9730)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1490/1724]  eta: 0:22:58  lr: 0.000120  loss: 30.8273 (31.1379)  loss_n_40: 0.3436 (0.3677)  loss_n_60: 0.4023 (0.4119)  loss_n_80: 0.4530 (0.4688)  loss_n_100: 0.5073 (0.5199)  triple_100: 8.3123 (8.4033)  triple_80: 8.4329 (8.5039)  triple_60: 7.4098 (7.4881)  triple_40: 5.0257 (4.9744)  time: 5.8889  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7]  [1500/1724]  eta: 0:21:59  lr: 0.000120  loss: 30.5871 (31.1358)  loss_n_40: 0.3579 (0.3676)  loss_n_60: 0.3890 (0.4118)  loss_n_80: 0.4436 (0.4686)  loss_n_100: 0.4923 (0.5198)  triple_100: 8.1647 (8.4026)  triple_80: 8.3373 (8.5031)  triple_60: 7.4098 (7.4876)  triple_40: 5.0946 (4.9746)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1510/1724]  eta: 0:21:00  lr: 0.000120  loss: 31.3182 (31.1395)  loss_n_40: 0.3598 (0.3676)  loss_n_60: 0.4016 (0.4117)  loss_n_80: 0.4528 (0.4686)  loss_n_100: 0.5012 (0.5198)  triple_100: 8.4101 (8.4037)  triple_80: 8.4543 (8.5040)  triple_60: 7.6176 (7.4887)  triple_40: 5.0438 (4.9753)  time: 5.8902  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1520/1724]  eta: 0:20:01  lr: 0.000120  loss: 31.0546 (31.1263)  loss_n_40: 0.3460 (0.3674)  loss_n_60: 0.3910 (0.4114)  loss_n_80: 0.4408 (0.4682)  loss_n_100: 0.4880 (0.5193)  triple_100: 8.2862 (8.3998)  triple_80: 8.3701 (8.5000)  triple_60: 7.4157 (7.4859)  triple_40: 4.9872 (4.9744)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1530/1724]  eta: 0:19:02  lr: 0.000120  loss: 28.6214 (31.1157)  loss_n_40: 0.3328 (0.3672)  loss_n_60: 0.3650 (0.4111)  loss_n_80: 0.4051 (0.4678)  loss_n_100: 0.4372 (0.5188)  triple_100: 7.6057 (8.3957)  triple_80: 7.7342 (8.4964)  triple_60: 7.0037 (7.4840)  triple_40: 4.9347 (4.9748)  time: 5.8910  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1540/1724]  eta: 0:18:03  lr: 0.000120  loss: 28.8343 (31.1058)  loss_n_40: 0.3445 (0.3671)  loss_n_60: 0.3439 (0.4108)  loss_n_80: 0.3869 (0.4674)  loss_n_100: 0.4286 (0.5183)  triple_100: 7.5824 (8.3917)  triple_80: 7.8564 (8.4929)  triple_60: 7.0545 (7.4820)  triple_40: 5.0326 (4.9755)  time: 5.8901  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1550/1724]  eta: 0:17:05  lr: 0.000120  loss: 28.9725 (31.0906)  loss_n_40: 0.3229 (0.3668)  loss_n_60: 0.3405 (0.4104)  loss_n_80: 0.3781 (0.4669)  loss_n_100: 0.4231 (0.5178)  triple_100: 7.8379 (8.3872)  triple_80: 7.8702 (8.4883)  triple_60: 7.0068 (7.4784)  triple_40: 4.9406 (4.9750)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1560/1724]  eta: 0:16:06  lr: 0.000120  loss: 28.9725 (31.0811)  loss_n_40: 0.3229 (0.3666)  loss_n_60: 0.3515 (0.4101)  loss_n_80: 0.4045 (0.4666)  loss_n_100: 0.4500 (0.5174)  triple_100: 7.8489 (8.3848)  triple_80: 7.9326 (8.4857)  triple_60: 6.9972 (7.4763)  triple_40: 4.7994 (4.9737)  time: 5.8904  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1570/1724]  eta: 0:15:07  lr: 0.000120  loss: 29.6910 (31.0731)  loss_n_40: 0.3293 (0.3663)  loss_n_60: 0.3579 (0.4098)  loss_n_80: 0.4045 (0.4662)  loss_n_100: 0.4609 (0.5170)  triple_100: 7.8794 (8.3822)  triple_80: 8.0125 (8.4830)  triple_60: 7.2031 (7.4746)  triple_40: 4.8299 (4.9739)  time: 5.8904  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1580/1724]  eta: 0:14:08  lr: 0.000120  loss: 29.6910 (31.0616)  loss_n_40: 0.3268 (0.3662)  loss_n_60: 0.3604 (0.4094)  loss_n_80: 0.3998 (0.4658)  loss_n_100: 0.4417 (0.5165)  triple_100: 7.8528 (8.3783)  triple_80: 7.9673 (8.4791)  triple_60: 7.1651 (7.4722)  triple_40: 4.8886 (4.9740)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1590/1724]  eta: 0:13:09  lr: 0.000120  loss: 28.9287 (31.0508)  loss_n_40: 0.3061 (0.3659)  loss_n_60: 0.3573 (0.4091)  loss_n_80: 0.3943 (0.4653)  loss_n_100: 0.4346 (0.5161)  triple_100: 7.7977 (8.3753)  triple_80: 7.8707 (8.4759)  triple_60: 7.0034 (7.4698)  triple_40: 4.8455 (4.9735)  time: 5.8903  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1600/1724]  eta: 0:12:10  lr: 0.000120  loss: 29.0468 (31.0454)  loss_n_40: 0.3261 (0.3658)  loss_n_60: 0.3582 (0.4089)  loss_n_80: 0.4077 (0.4651)  loss_n_100: 0.4417 (0.5158)  triple_100: 7.8509 (8.3737)  triple_80: 7.9544 (8.4741)  triple_60: 7.0230 (7.4684)  triple_40: 4.8541 (4.9735)  time: 5.8912  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1610/1724]  eta: 0:11:11  lr: 0.000120  loss: 29.8485 (31.0366)  loss_n_40: 0.3457 (0.3657)  loss_n_60: 0.3807 (0.4086)  loss_n_80: 0.4125 (0.4648)  loss_n_100: 0.4476 (0.5155)  triple_100: 7.8198 (8.3703)  triple_80: 7.9993 (8.4709)  triple_60: 7.1950 (7.4667)  triple_40: 5.0442 (4.9740)  time: 5.8913  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1620/1724]  eta: 0:10:12  lr: 0.000120  loss: 29.1550 (31.0269)  loss_n_40: 0.3545 (0.3658)  loss_n_60: 0.3807 (0.4086)  loss_n_80: 0.4301 (0.4647)  loss_n_100: 0.4797 (0.5154)  triple_100: 7.9445 (8.3681)  triple_80: 7.9996 (8.4686)  triple_60: 7.0804 (7.4640)  triple_40: 4.7684 (4.9717)  time: 5.8906  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1630/1724]  eta: 0:09:13  lr: 0.000120  loss: 29.6839 (31.0262)  loss_n_40: 0.3694 (0.3659)  loss_n_60: 0.3958 (0.4086)  loss_n_80: 0.4556 (0.4648)  loss_n_100: 0.5023 (0.5154)  triple_100: 8.0443 (8.3674)  triple_80: 8.1136 (8.4683)  triple_60: 7.1398 (7.4639)  triple_40: 4.8054 (4.9719)  time: 5.8903  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1640/1724]  eta: 0:08:14  lr: 0.000120  loss: 30.8993 (31.0265)  loss_n_40: 0.3809 (0.3662)  loss_n_60: 0.4089 (0.4088)  loss_n_80: 0.4498 (0.4648)  loss_n_100: 0.4947 (0.5154)  triple_100: 8.2866 (8.3654)  triple_80: 8.4916 (8.4675)  triple_60: 7.3232 (7.4645)  triple_40: 5.1193 (4.9740)  time: 5.8891  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1650/1724]  eta: 0:07:15  lr: 0.000120  loss: 29.7123 (31.0260)  loss_n_40: 0.4038 (0.3665)  loss_n_60: 0.4326 (0.4090)  loss_n_80: 0.4554 (0.4649)  loss_n_100: 0.4892 (0.5155)  triple_100: 7.8974 (8.3632)  triple_80: 8.0524 (8.4669)  triple_60: 7.2664 (7.4652)  triple_40: 5.2437 (4.9747)  time: 5.8877  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1660/1724]  eta: 0:06:17  lr: 0.000120  loss: 30.0352 (31.0193)  loss_n_40: 0.4049 (0.3668)  loss_n_60: 0.4326 (0.4092)  loss_n_80: 0.4881 (0.4650)  loss_n_100: 0.5150 (0.5155)  triple_100: 7.9029 (8.3605)  triple_80: 8.1478 (8.4651)  triple_60: 7.2362 (7.4639)  triple_40: 4.8266 (4.9734)  time: 5.8880  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1670/1724]  eta: 0:05:18  lr: 0.000120  loss: 29.9655 (31.0188)  loss_n_40: 0.3949 (0.3670)  loss_n_60: 0.4301 (0.4093)  loss_n_80: 0.4816 (0.4652)  loss_n_100: 0.5298 (0.5156)  triple_100: 7.8466 (8.3594)  triple_80: 7.9900 (8.4647)  triple_60: 7.2473 (7.4640)  triple_40: 4.7468 (4.9736)  time: 5.8877  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1680/1724]  eta: 0:04:19  lr: 0.000120  loss: 30.1932 (31.0155)  loss_n_40: 0.3740 (0.3671)  loss_n_60: 0.4301 (0.4094)  loss_n_80: 0.4795 (0.4653)  loss_n_100: 0.5298 (0.5158)  triple_100: 8.0229 (8.3587)  triple_80: 8.2909 (8.4643)  triple_60: 7.3336 (7.4628)  triple_40: 4.7468 (4.9721)  time: 5.8869  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1690/1724]  eta: 0:03:20  lr: 0.000120  loss: 30.5432 (31.0094)  loss_n_40: 0.3357 (0.3669)  loss_n_60: 0.3915 (0.4093)  loss_n_80: 0.4648 (0.4652)  loss_n_100: 0.5218 (0.5157)  triple_100: 8.1434 (8.3573)  triple_80: 8.4967 (8.4631)  triple_60: 7.3576 (7.4614)  triple_40: 4.6736 (4.9705)  time: 5.8866  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1700/1724]  eta: 0:02:21  lr: 0.000120  loss: 28.8614 (30.9959)  loss_n_40: 0.3071 (0.3665)  loss_n_60: 0.3508 (0.4089)  loss_n_80: 0.4048 (0.4648)  loss_n_100: 0.4697 (0.5154)  triple_100: 7.9476 (8.3538)  triple_80: 7.9933 (8.4594)  triple_60: 6.9619 (7.4584)  triple_40: 4.6911 (4.9687)  time: 5.8858  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1710/1724]  eta: 0:01:22  lr: 0.000120  loss: 28.3982 (30.9822)  loss_n_40: 0.3048 (0.3662)  loss_n_60: 0.3419 (0.4086)  loss_n_80: 0.3979 (0.4645)  loss_n_100: 0.4519 (0.5150)  triple_100: 7.6488 (8.3501)  triple_80: 7.6862 (8.4557)  triple_60: 6.9299 (7.4553)  triple_40: 4.6581 (4.9669)  time: 5.8861  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1720/1724]  eta: 0:00:23  lr: 0.000120  loss: 29.2134 (30.9746)  loss_n_40: 0.3189 (0.3660)  loss_n_60: 0.3428 (0.4083)  loss_n_80: 0.4029 (0.4641)  loss_n_100: 0.4537 (0.5146)  triple_100: 7.9166 (8.3477)  triple_80: 7.9652 (8.4533)  triple_60: 7.1132 (7.4539)  triple_40: 4.8031 (4.9667)  time: 5.8868  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:7]  [1723/1724]  eta: 0:00:05  lr: 0.000120  loss: 28.8278 (30.9694)  loss_n_40: 0.3189 (0.3659)  loss_n_60: 0.3421 (0.4082)  loss_n_80: 0.3925 (0.4640)  loss_n_100: 0.4394 (0.5145)  triple_100: 7.8186 (8.3460)  triple_80: 7.9453 (8.4516)  triple_60: 7.0647 (7.4528)  triple_40: 4.8031 (4.9663)  time: 5.8866  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:7] Total time: 2:49:15 (5.8909 s / it)\n",
      "Averaged stats: lr: 0.000120  loss: 28.8278 (30.9694)  loss_n_40: 0.3189 (0.3659)  loss_n_60: 0.3421 (0.4082)  loss_n_80: 0.3925 (0.4640)  loss_n_100: 0.4394 (0.5145)  triple_100: 7.8186 (8.3460)  triple_80: 7.9453 (8.4516)  triple_60: 7.0647 (7.4528)  triple_40: 4.8031 (4.9663)\n",
      "Valid: [epoch:7]  [  0/845]  eta: 0:24:06  loss: 32.6627 (32.6627)  loss_n_40: 0.5733 (0.5733)  loss_n_60: 0.4974 (0.4974)  loss_n_80: 0.5279 (0.5279)  loss_n_100: 0.5519 (0.5519)  triple_100: 8.2022 (8.2022)  triple_80: 8.4031 (8.4031)  triple_60: 7.6941 (7.6941)  triple_40: 6.2127 (6.2127)  time: 1.7121  data: 0.7356  max mem: 40153\n",
      "Valid: [epoch:7]  [ 10/845]  eta: 0:14:28  loss: 28.4997 (29.3353)  loss_n_40: 0.2894 (0.3268)  loss_n_60: 0.3377 (0.3571)  loss_n_80: 0.3895 (0.4119)  loss_n_100: 0.4361 (0.4600)  triple_100: 7.8792 (7.9119)  triple_80: 7.8833 (7.9917)  triple_60: 6.8842 (7.0313)  triple_40: 4.6771 (4.8444)  time: 1.0402  data: 0.0670  max mem: 40153\n",
      "Valid: [epoch:7]  [ 20/845]  eta: 0:13:51  loss: 28.4353 (29.0293)  loss_n_40: 0.3046 (0.3540)  loss_n_60: 0.3373 (0.3685)  loss_n_80: 0.3832 (0.4139)  loss_n_100: 0.4221 (0.4508)  triple_100: 7.4183 (7.6862)  triple_80: 7.5736 (7.8543)  triple_60: 6.7969 (7.0004)  triple_40: 4.7053 (4.9013)  time: 0.9729  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [ 30/845]  eta: 0:13:32  loss: 29.1032 (29.4988)  loss_n_40: 0.3156 (0.3543)  loss_n_60: 0.3415 (0.3775)  loss_n_80: 0.3822 (0.4260)  loss_n_100: 0.4221 (0.4664)  triple_100: 7.6350 (7.8610)  triple_80: 7.7056 (8.0045)  triple_60: 7.1576 (7.1058)  triple_40: 4.8433 (4.9033)  time: 0.9734  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [ 40/845]  eta: 0:13:18  loss: 29.5172 (29.5610)  loss_n_40: 0.3200 (0.3519)  loss_n_60: 0.3632 (0.3784)  loss_n_80: 0.3998 (0.4268)  loss_n_100: 0.4579 (0.4680)  triple_100: 7.6422 (7.8809)  triple_80: 7.7182 (8.0208)  triple_60: 7.1543 (7.1231)  triple_40: 4.8749 (4.9112)  time: 0.9737  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [ 50/845]  eta: 0:13:05  loss: 28.4767 (29.6416)  loss_n_40: 0.3310 (0.3510)  loss_n_60: 0.3671 (0.3800)  loss_n_80: 0.4234 (0.4285)  loss_n_100: 0.4640 (0.4702)  triple_100: 7.5664 (7.9042)  triple_80: 7.6464 (8.0419)  triple_60: 6.9642 (7.1533)  triple_40: 4.8622 (4.9125)  time: 0.9739  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [ 60/845]  eta: 0:12:53  loss: 29.7939 (29.5945)  loss_n_40: 0.3237 (0.3588)  loss_n_60: 0.3940 (0.3847)  loss_n_80: 0.4511 (0.4332)  loss_n_100: 0.4649 (0.4717)  triple_100: 7.9731 (7.8760)  triple_80: 8.0944 (8.0314)  triple_60: 7.1003 (7.1388)  triple_40: 4.8010 (4.9000)  time: 0.9741  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [ 70/845]  eta: 0:12:42  loss: 28.7144 (29.5890)  loss_n_40: 0.3217 (0.3528)  loss_n_60: 0.3797 (0.3807)  loss_n_80: 0.4111 (0.4300)  loss_n_100: 0.4528 (0.4705)  triple_100: 7.7514 (7.8885)  triple_80: 7.7263 (8.0308)  triple_60: 6.7805 (7.1333)  triple_40: 4.7589 (4.9022)  time: 0.9741  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [ 80/845]  eta: 0:12:31  loss: 29.1875 (29.5759)  loss_n_40: 0.3054 (0.3474)  loss_n_60: 0.3545 (0.3779)  loss_n_80: 0.4011 (0.4282)  loss_n_100: 0.4528 (0.4694)  triple_100: 7.8791 (7.8928)  triple_80: 7.9605 (8.0366)  triple_60: 6.9811 (7.1319)  triple_40: 4.8340 (4.8918)  time: 0.9744  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [ 90/845]  eta: 0:12:21  loss: 29.1875 (29.5244)  loss_n_40: 0.3106 (0.3447)  loss_n_60: 0.3588 (0.3757)  loss_n_80: 0.4057 (0.4257)  loss_n_100: 0.4325 (0.4673)  triple_100: 7.6698 (7.8793)  triple_80: 7.7137 (8.0200)  triple_60: 6.9811 (7.1165)  triple_40: 4.8340 (4.8953)  time: 0.9744  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [100/845]  eta: 0:12:10  loss: 28.9436 (29.4577)  loss_n_40: 0.3149 (0.3418)  loss_n_60: 0.3588 (0.3735)  loss_n_80: 0.4057 (0.4244)  loss_n_100: 0.4311 (0.4661)  triple_100: 7.5568 (7.8639)  triple_80: 7.7137 (8.0081)  triple_60: 6.8679 (7.0984)  triple_40: 4.7936 (4.8815)  time: 0.9743  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [110/845]  eta: 0:12:00  loss: 28.4384 (29.3619)  loss_n_40: 0.2807 (0.3391)  loss_n_60: 0.3241 (0.3712)  loss_n_80: 0.3846 (0.4213)  loss_n_100: 0.4175 (0.4629)  triple_100: 7.5657 (7.8315)  triple_80: 7.7632 (7.9796)  triple_60: 6.9847 (7.0806)  triple_40: 4.7862 (4.8757)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [120/845]  eta: 0:11:50  loss: 28.4384 (29.3336)  loss_n_40: 0.2898 (0.3414)  loss_n_60: 0.3329 (0.3726)  loss_n_80: 0.3889 (0.4227)  loss_n_100: 0.4299 (0.4634)  triple_100: 7.6147 (7.8172)  triple_80: 7.7632 (7.9732)  triple_60: 6.9847 (7.0713)  triple_40: 4.7306 (4.8717)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [130/845]  eta: 0:11:40  loss: 29.7984 (29.4248)  loss_n_40: 0.3327 (0.3423)  loss_n_60: 0.3702 (0.3741)  loss_n_80: 0.4285 (0.4249)  loss_n_100: 0.4938 (0.4670)  triple_100: 7.9571 (7.8557)  triple_80: 8.0539 (8.0022)  triple_60: 7.1432 (7.0888)  triple_40: 4.6978 (4.8698)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [140/845]  eta: 0:11:30  loss: 30.1624 (29.4093)  loss_n_40: 0.3132 (0.3432)  loss_n_60: 0.3703 (0.3743)  loss_n_80: 0.4479 (0.4247)  loss_n_100: 0.5117 (0.4669)  triple_100: 7.8021 (7.8501)  triple_80: 8.3835 (7.9906)  triple_60: 7.1432 (7.0864)  triple_40: 4.6978 (4.8731)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [150/845]  eta: 0:11:20  loss: 28.6527 (29.3588)  loss_n_40: 0.3089 (0.3410)  loss_n_60: 0.3310 (0.3725)  loss_n_80: 0.3684 (0.4234)  loss_n_100: 0.4173 (0.4661)  triple_100: 7.5656 (7.8409)  triple_80: 7.6180 (7.9792)  triple_60: 6.7933 (7.0711)  triple_40: 4.7165 (4.8647)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [160/845]  eta: 0:11:10  loss: 28.8656 (29.4228)  loss_n_40: 0.3112 (0.3394)  loss_n_60: 0.3464 (0.3726)  loss_n_80: 0.4076 (0.4240)  loss_n_100: 0.4634 (0.4676)  triple_100: 7.6880 (7.8701)  triple_80: 7.8558 (8.0025)  triple_60: 7.0142 (7.0865)  triple_40: 4.7165 (4.8602)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [170/845]  eta: 0:11:00  loss: 28.8656 (29.4778)  loss_n_40: 0.3188 (0.3421)  loss_n_60: 0.3464 (0.3743)  loss_n_80: 0.4076 (0.4259)  loss_n_100: 0.4634 (0.4696)  triple_100: 7.7616 (7.8806)  triple_80: 7.8558 (8.0170)  triple_60: 7.0142 (7.0952)  triple_40: 4.8404 (4.8731)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [180/845]  eta: 0:10:50  loss: 28.7537 (29.5293)  loss_n_40: 0.3267 (0.3411)  loss_n_60: 0.3611 (0.3748)  loss_n_80: 0.4034 (0.4268)  loss_n_100: 0.4463 (0.4711)  triple_100: 7.7616 (7.9036)  triple_80: 7.7924 (8.0376)  triple_60: 6.9980 (7.1082)  triple_40: 4.8398 (4.8661)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [190/845]  eta: 0:10:40  loss: 29.2390 (29.5192)  loss_n_40: 0.3126 (0.3390)  loss_n_60: 0.3611 (0.3734)  loss_n_80: 0.4142 (0.4260)  loss_n_100: 0.4768 (0.4708)  triple_100: 8.0648 (7.9136)  triple_80: 8.0824 (8.0416)  triple_60: 7.0380 (7.1053)  triple_40: 4.6345 (4.8493)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [200/845]  eta: 0:10:30  loss: 29.2390 (29.4965)  loss_n_40: 0.2994 (0.3405)  loss_n_60: 0.3524 (0.3740)  loss_n_80: 0.4131 (0.4265)  loss_n_100: 0.4690 (0.4715)  triple_100: 8.0584 (7.9086)  triple_80: 8.0824 (8.0353)  triple_60: 7.0275 (7.0994)  triple_40: 4.6345 (4.8409)  time: 0.9741  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [210/845]  eta: 0:10:20  loss: 28.6340 (29.4328)  loss_n_40: 0.3113 (0.3428)  loss_n_60: 0.3678 (0.3741)  loss_n_80: 0.4022 (0.4252)  loss_n_100: 0.4354 (0.4693)  triple_100: 7.6240 (7.8821)  triple_80: 7.7094 (8.0098)  triple_60: 7.0140 (7.0874)  triple_40: 4.7086 (4.8420)  time: 0.9741  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [220/845]  eta: 0:10:10  loss: 28.9179 (29.4940)  loss_n_40: 0.2960 (0.3414)  loss_n_60: 0.3564 (0.3743)  loss_n_80: 0.3837 (0.4262)  loss_n_100: 0.4187 (0.4705)  triple_100: 7.7424 (7.9080)  triple_80: 7.8701 (8.0344)  triple_60: 7.0642 (7.1024)  triple_40: 4.6787 (4.8367)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [230/845]  eta: 0:10:01  loss: 28.9179 (29.5171)  loss_n_40: 0.2956 (0.3421)  loss_n_60: 0.3388 (0.3755)  loss_n_80: 0.4011 (0.4269)  loss_n_100: 0.4490 (0.4712)  triple_100: 7.8021 (7.9124)  triple_80: 7.9354 (8.0386)  triple_60: 7.0644 (7.1123)  triple_40: 4.6787 (4.8379)  time: 0.9747  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:7]  [240/845]  eta: 0:09:51  loss: 28.4914 (29.4654)  loss_n_40: 0.3012 (0.3407)  loss_n_60: 0.3384 (0.3739)  loss_n_80: 0.3805 (0.4249)  loss_n_100: 0.4178 (0.4691)  triple_100: 7.5111 (7.8969)  triple_80: 7.6481 (8.0227)  triple_60: 6.9587 (7.1009)  triple_40: 4.7984 (4.8364)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [250/845]  eta: 0:09:41  loss: 28.9040 (29.5286)  loss_n_40: 0.3129 (0.3398)  loss_n_60: 0.3446 (0.3741)  loss_n_80: 0.4006 (0.4256)  loss_n_100: 0.4467 (0.4702)  triple_100: 7.7016 (7.9195)  triple_80: 7.8521 (8.0437)  triple_60: 6.8545 (7.1155)  triple_40: 4.8053 (4.8403)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [260/845]  eta: 0:09:31  loss: 29.7423 (29.5268)  loss_n_40: 0.3129 (0.3383)  loss_n_60: 0.3546 (0.3732)  loss_n_80: 0.4117 (0.4248)  loss_n_100: 0.4473 (0.4696)  triple_100: 7.8612 (7.9233)  triple_80: 8.2049 (8.0453)  triple_60: 7.3885 (7.1157)  triple_40: 4.7088 (4.8366)  time: 0.9748  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [270/845]  eta: 0:09:21  loss: 29.2204 (29.5230)  loss_n_40: 0.3096 (0.3392)  loss_n_60: 0.3611 (0.3739)  loss_n_80: 0.3918 (0.4253)  loss_n_100: 0.4294 (0.4698)  triple_100: 7.6582 (7.9167)  triple_80: 7.8380 (8.0435)  triple_60: 7.2341 (7.1169)  triple_40: 4.6463 (4.8377)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [280/845]  eta: 0:09:11  loss: 29.1033 (29.5561)  loss_n_40: 0.3098 (0.3394)  loss_n_60: 0.3611 (0.3741)  loss_n_80: 0.4083 (0.4255)  loss_n_100: 0.4448 (0.4701)  triple_100: 7.7632 (7.9275)  triple_80: 7.8380 (8.0517)  triple_60: 7.1145 (7.1256)  triple_40: 4.9457 (4.8423)  time: 0.9744  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [290/845]  eta: 0:09:02  loss: 29.1033 (29.5586)  loss_n_40: 0.3098 (0.3397)  loss_n_60: 0.3570 (0.3742)  loss_n_80: 0.3955 (0.4253)  loss_n_100: 0.4380 (0.4696)  triple_100: 7.9122 (7.9245)  triple_80: 7.9074 (8.0502)  triple_60: 7.1145 (7.1282)  triple_40: 4.7869 (4.8468)  time: 0.9747  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [300/845]  eta: 0:08:52  loss: 28.2015 (29.5153)  loss_n_40: 0.3157 (0.3401)  loss_n_60: 0.3330 (0.3737)  loss_n_80: 0.3733 (0.4245)  loss_n_100: 0.4072 (0.4684)  triple_100: 7.4461 (7.9071)  triple_80: 7.8255 (8.0351)  triple_60: 6.9351 (7.1185)  triple_40: 4.7212 (4.8480)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [310/845]  eta: 0:08:42  loss: 29.5004 (29.5490)  loss_n_40: 0.3161 (0.3411)  loss_n_60: 0.3493 (0.3750)  loss_n_80: 0.4149 (0.4261)  loss_n_100: 0.4788 (0.4708)  triple_100: 7.6830 (7.9194)  triple_80: 7.9040 (8.0451)  triple_60: 6.9351 (7.1243)  triple_40: 4.7521 (4.8471)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [320/845]  eta: 0:08:32  loss: 29.9156 (29.5562)  loss_n_40: 0.3008 (0.3396)  loss_n_60: 0.3715 (0.3746)  loss_n_80: 0.4465 (0.4262)  loss_n_100: 0.5084 (0.4714)  triple_100: 8.3695 (7.9299)  triple_80: 8.2725 (8.0531)  triple_60: 7.1964 (7.1248)  triple_40: 4.6792 (4.8367)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [330/845]  eta: 0:08:22  loss: 29.3399 (29.5786)  loss_n_40: 0.3046 (0.3418)  loss_n_60: 0.3552 (0.3759)  loss_n_80: 0.4387 (0.4272)  loss_n_100: 0.5011 (0.4721)  triple_100: 7.8795 (7.9274)  triple_80: 8.2580 (8.0541)  triple_60: 6.9112 (7.1311)  triple_40: 4.7637 (4.8489)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [340/845]  eta: 0:08:13  loss: 29.2595 (29.5412)  loss_n_40: 0.3089 (0.3427)  loss_n_60: 0.3366 (0.3758)  loss_n_80: 0.3811 (0.4269)  loss_n_100: 0.4274 (0.4716)  triple_100: 7.5634 (7.9123)  triple_80: 7.9232 (8.0409)  triple_60: 7.0365 (7.1210)  triple_40: 5.0555 (4.8500)  time: 0.9738  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [350/845]  eta: 0:08:03  loss: 28.6264 (29.5258)  loss_n_40: 0.3089 (0.3424)  loss_n_60: 0.3544 (0.3755)  loss_n_80: 0.4161 (0.4265)  loss_n_100: 0.4552 (0.4712)  triple_100: 7.4499 (7.9064)  triple_80: 7.8446 (8.0369)  triple_60: 6.9947 (7.1178)  triple_40: 4.8343 (4.8492)  time: 0.9742  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [360/845]  eta: 0:07:53  loss: 28.6264 (29.5497)  loss_n_40: 0.2930 (0.3417)  loss_n_60: 0.3589 (0.3754)  loss_n_80: 0.4170 (0.4266)  loss_n_100: 0.4591 (0.4713)  triple_100: 7.5692 (7.9159)  triple_80: 7.8446 (8.0454)  triple_60: 6.9947 (7.1238)  triple_40: 4.7060 (4.8495)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [370/845]  eta: 0:07:43  loss: 28.0622 (29.5222)  loss_n_40: 0.2858 (0.3405)  loss_n_60: 0.3227 (0.3743)  loss_n_80: 0.3669 (0.4253)  loss_n_100: 0.4187 (0.4699)  triple_100: 7.5692 (7.9071)  triple_80: 7.6085 (8.0380)  triple_60: 6.8315 (7.1188)  triple_40: 4.7817 (4.8483)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [380/845]  eta: 0:07:33  loss: 28.7394 (29.5459)  loss_n_40: 0.3119 (0.3397)  loss_n_60: 0.3516 (0.3741)  loss_n_80: 0.4047 (0.4253)  loss_n_100: 0.4616 (0.4702)  triple_100: 7.7535 (7.9177)  triple_80: 7.8007 (8.0450)  triple_60: 6.9773 (7.1235)  triple_40: 4.8087 (4.8504)  time: 0.9749  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [390/845]  eta: 0:07:24  loss: 28.7193 (29.5207)  loss_n_40: 0.3008 (0.3389)  loss_n_60: 0.3392 (0.3732)  loss_n_80: 0.3875 (0.4241)  loss_n_100: 0.4425 (0.4689)  triple_100: 7.7535 (7.9096)  triple_80: 7.8007 (8.0361)  triple_60: 6.9773 (7.1185)  triple_40: 4.7981 (4.8514)  time: 0.9749  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [400/845]  eta: 0:07:14  loss: 27.7477 (29.4965)  loss_n_40: 0.2894 (0.3384)  loss_n_60: 0.3185 (0.3725)  loss_n_80: 0.3545 (0.4233)  loss_n_100: 0.3964 (0.4680)  triple_100: 7.3630 (7.9025)  triple_80: 7.4396 (8.0283)  triple_60: 6.7744 (7.1133)  triple_40: 4.7964 (4.8501)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [410/845]  eta: 0:07:04  loss: 27.8566 (29.4842)  loss_n_40: 0.3194 (0.3397)  loss_n_60: 0.3486 (0.3730)  loss_n_80: 0.3739 (0.4237)  loss_n_100: 0.4070 (0.4685)  triple_100: 7.3630 (7.8975)  triple_80: 7.4925 (8.0236)  triple_60: 6.7998 (7.1095)  triple_40: 4.8515 (4.8486)  time: 0.9749  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [420/845]  eta: 0:06:54  loss: 27.3644 (29.4410)  loss_n_40: 0.3027 (0.3391)  loss_n_60: 0.3164 (0.3721)  loss_n_80: 0.3706 (0.4226)  loss_n_100: 0.4119 (0.4670)  triple_100: 7.3084 (7.8825)  triple_80: 7.4116 (8.0099)  triple_60: 6.5968 (7.1001)  triple_40: 4.7982 (4.8476)  time: 0.9748  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [430/845]  eta: 0:06:45  loss: 28.5732 (29.4728)  loss_n_40: 0.3007 (0.3399)  loss_n_60: 0.3293 (0.3731)  loss_n_80: 0.3706 (0.4235)  loss_n_100: 0.4207 (0.4677)  triple_100: 7.3555 (7.8887)  triple_80: 7.5026 (8.0182)  triple_60: 6.9875 (7.1086)  triple_40: 4.7982 (4.8533)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [440/845]  eta: 0:06:35  loss: 29.5137 (29.4668)  loss_n_40: 0.3111 (0.3403)  loss_n_60: 0.3584 (0.3733)  loss_n_80: 0.4135 (0.4236)  loss_n_100: 0.4624 (0.4678)  triple_100: 7.8934 (7.8863)  triple_80: 8.0517 (8.0170)  triple_60: 7.1224 (7.1069)  triple_40: 4.7250 (4.8517)  time: 0.9742  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [450/845]  eta: 0:06:25  loss: 28.8975 (29.4592)  loss_n_40: 0.2976 (0.3394)  loss_n_60: 0.3426 (0.3727)  loss_n_80: 0.3890 (0.4229)  loss_n_100: 0.4250 (0.4669)  triple_100: 7.7952 (7.8829)  triple_80: 7.9867 (8.0147)  triple_60: 6.9970 (7.1080)  triple_40: 4.7427 (4.8517)  time: 0.9746  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [460/845]  eta: 0:06:15  loss: 28.7223 (29.4620)  loss_n_40: 0.2999 (0.3391)  loss_n_60: 0.3276 (0.3726)  loss_n_80: 0.3740 (0.4226)  loss_n_100: 0.4205 (0.4664)  triple_100: 7.5815 (7.8798)  triple_80: 7.8669 (8.0144)  triple_60: 6.9626 (7.1111)  triple_40: 4.9028 (4.8560)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [470/845]  eta: 0:06:05  loss: 29.3293 (29.4463)  loss_n_40: 0.3318 (0.3384)  loss_n_60: 0.3442 (0.3720)  loss_n_80: 0.3971 (0.4218)  loss_n_100: 0.4430 (0.4655)  triple_100: 7.5993 (7.8748)  triple_80: 7.9928 (8.0092)  triple_60: 7.0011 (7.1085)  triple_40: 4.9028 (4.8561)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [480/845]  eta: 0:05:56  loss: 29.3293 (29.4722)  loss_n_40: 0.3012 (0.3399)  loss_n_60: 0.3442 (0.3730)  loss_n_80: 0.3971 (0.4228)  loss_n_100: 0.4430 (0.4664)  triple_100: 7.8896 (7.8790)  triple_80: 7.9966 (8.0144)  triple_60: 7.0011 (7.1137)  triple_40: 4.9703 (4.8630)  time: 0.9743  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:7]  [490/845]  eta: 0:05:46  loss: 30.0431 (29.4625)  loss_n_40: 0.3329 (0.3397)  loss_n_60: 0.3567 (0.3727)  loss_n_80: 0.4144 (0.4223)  loss_n_100: 0.4780 (0.4659)  triple_100: 7.9867 (7.8756)  triple_80: 8.3218 (8.0111)  triple_60: 7.1558 (7.1120)  triple_40: 4.9703 (4.8631)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [500/845]  eta: 0:05:36  loss: 30.0431 (29.4569)  loss_n_40: 0.3300 (0.3398)  loss_n_60: 0.3575 (0.3726)  loss_n_80: 0.4093 (0.4222)  loss_n_100: 0.4599 (0.4659)  triple_100: 7.9867 (7.8755)  triple_80: 8.1719 (8.0102)  triple_60: 7.1777 (7.1101)  triple_40: 4.8455 (4.8606)  time: 0.9748  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [510/845]  eta: 0:05:26  loss: 29.7767 (29.4519)  loss_n_40: 0.3300 (0.3411)  loss_n_60: 0.3708 (0.3734)  loss_n_80: 0.4254 (0.4228)  loss_n_100: 0.4612 (0.4661)  triple_100: 8.0997 (7.8717)  triple_80: 8.1719 (8.0087)  triple_60: 7.1887 (7.1097)  triple_40: 4.7793 (4.8585)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [520/845]  eta: 0:05:17  loss: 28.3682 (29.4177)  loss_n_40: 0.3229 (0.3404)  loss_n_60: 0.3587 (0.3726)  loss_n_80: 0.4098 (0.4221)  loss_n_100: 0.4374 (0.4654)  triple_100: 7.6149 (7.8630)  triple_80: 7.8779 (8.0003)  triple_60: 6.8847 (7.1009)  triple_40: 4.6359 (4.8531)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [530/845]  eta: 0:05:07  loss: 28.2652 (29.4178)  loss_n_40: 0.3038 (0.3403)  loss_n_60: 0.3328 (0.3725)  loss_n_80: 0.3843 (0.4220)  loss_n_100: 0.4263 (0.4654)  triple_100: 7.3200 (7.8640)  triple_80: 7.5039 (8.0003)  triple_60: 6.8847 (7.1012)  triple_40: 4.6552 (4.8522)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [540/845]  eta: 0:04:57  loss: 30.8082 (29.4539)  loss_n_40: 0.3381 (0.3405)  loss_n_60: 0.3806 (0.3731)  loss_n_80: 0.4499 (0.4228)  loss_n_100: 0.5088 (0.4664)  triple_100: 8.3102 (7.8760)  triple_80: 8.4147 (8.0119)  triple_60: 7.3602 (7.1097)  triple_40: 4.7841 (4.8536)  time: 0.9749  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [550/845]  eta: 0:04:47  loss: 31.0308 (29.4788)  loss_n_40: 0.3397 (0.3409)  loss_n_60: 0.3995 (0.3736)  loss_n_80: 0.4552 (0.4236)  loss_n_100: 0.5256 (0.4675)  triple_100: 8.5911 (7.8854)  triple_80: 8.7219 (8.0196)  triple_60: 7.4975 (7.1142)  triple_40: 4.8497 (4.8539)  time: 0.9752  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [560/845]  eta: 0:04:38  loss: 29.7567 (29.4718)  loss_n_40: 0.3319 (0.3404)  loss_n_60: 0.3782 (0.3733)  loss_n_80: 0.4433 (0.4232)  loss_n_100: 0.4867 (0.4670)  triple_100: 7.9244 (7.8818)  triple_80: 8.1072 (8.0180)  triple_60: 7.2018 (7.1138)  triple_40: 4.8497 (4.8543)  time: 0.9748  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [570/845]  eta: 0:04:28  loss: 29.3580 (29.4902)  loss_n_40: 0.2940 (0.3406)  loss_n_60: 0.3350 (0.3736)  loss_n_80: 0.3827 (0.4235)  loss_n_100: 0.4297 (0.4673)  triple_100: 7.9244 (7.8869)  triple_80: 7.9951 (8.0227)  triple_60: 7.1243 (7.1185)  triple_40: 4.7042 (4.8570)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [580/845]  eta: 0:04:18  loss: 28.6878 (29.4633)  loss_n_40: 0.2795 (0.3397)  loss_n_60: 0.3276 (0.3729)  loss_n_80: 0.3826 (0.4227)  loss_n_100: 0.4224 (0.4665)  triple_100: 7.6261 (7.8792)  triple_80: 7.8285 (8.0157)  triple_60: 6.8874 (7.1125)  triple_40: 4.6649 (4.8542)  time: 0.9748  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [590/845]  eta: 0:04:08  loss: 28.6234 (29.4773)  loss_n_40: 0.2836 (0.3395)  loss_n_60: 0.3376 (0.3731)  loss_n_80: 0.3803 (0.4231)  loss_n_100: 0.4224 (0.4669)  triple_100: 7.6123 (7.8835)  triple_80: 7.8266 (8.0204)  triple_60: 6.9112 (7.1167)  triple_40: 4.7567 (4.8541)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [600/845]  eta: 0:03:59  loss: 29.3545 (29.4674)  loss_n_40: 0.2998 (0.3393)  loss_n_60: 0.3543 (0.3730)  loss_n_80: 0.4130 (0.4229)  loss_n_100: 0.4410 (0.4669)  triple_100: 7.7055 (7.8831)  triple_80: 8.1286 (8.0188)  triple_60: 7.0835 (7.1138)  triple_40: 4.7567 (4.8496)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [610/845]  eta: 0:03:49  loss: 29.1727 (29.4545)  loss_n_40: 0.2998 (0.3396)  loss_n_60: 0.3548 (0.3728)  loss_n_80: 0.4104 (0.4227)  loss_n_100: 0.4541 (0.4665)  triple_100: 7.7055 (7.8780)  triple_80: 8.0418 (8.0147)  triple_60: 7.0382 (7.1106)  triple_40: 4.6452 (4.8497)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [620/845]  eta: 0:03:39  loss: 28.8828 (29.4597)  loss_n_40: 0.2997 (0.3394)  loss_n_60: 0.3530 (0.3728)  loss_n_80: 0.4077 (0.4227)  loss_n_100: 0.4260 (0.4666)  triple_100: 7.5224 (7.8779)  triple_80: 7.8723 (8.0159)  triple_60: 7.0524 (7.1119)  triple_40: 4.8136 (4.8524)  time: 0.9744  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [630/845]  eta: 0:03:29  loss: 30.0376 (29.4614)  loss_n_40: 0.3126 (0.3392)  loss_n_60: 0.3512 (0.3725)  loss_n_80: 0.4046 (0.4224)  loss_n_100: 0.4495 (0.4664)  triple_100: 7.5749 (7.8790)  triple_80: 7.8738 (8.0159)  triple_60: 7.1277 (7.1118)  triple_40: 4.8281 (4.8541)  time: 0.9741  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [640/845]  eta: 0:03:19  loss: 29.9262 (29.4526)  loss_n_40: 0.3188 (0.3391)  loss_n_60: 0.3589 (0.3724)  loss_n_80: 0.4222 (0.4222)  loss_n_100: 0.4728 (0.4662)  triple_100: 8.0896 (7.8766)  triple_80: 8.1128 (8.0126)  triple_60: 7.1099 (7.1099)  triple_40: 4.8887 (4.8535)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [650/845]  eta: 0:03:10  loss: 29.2126 (29.4526)  loss_n_40: 0.2996 (0.3388)  loss_n_60: 0.3575 (0.3722)  loss_n_80: 0.4222 (0.4220)  loss_n_100: 0.4728 (0.4660)  triple_100: 7.7451 (7.8772)  triple_80: 7.9397 (8.0127)  triple_60: 6.9775 (7.1103)  triple_40: 4.7884 (4.8535)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [660/845]  eta: 0:03:00  loss: 30.1139 (29.4584)  loss_n_40: 0.2941 (0.3383)  loss_n_60: 0.3575 (0.3720)  loss_n_80: 0.4259 (0.4219)  loss_n_100: 0.4806 (0.4659)  triple_100: 8.1217 (7.8811)  triple_80: 8.2535 (8.0150)  triple_60: 7.1728 (7.1109)  triple_40: 4.7884 (4.8533)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [670/845]  eta: 0:02:50  loss: 28.0438 (29.4399)  loss_n_40: 0.3009 (0.3376)  loss_n_60: 0.3354 (0.3713)  loss_n_80: 0.3759 (0.4212)  loss_n_100: 0.4276 (0.4652)  triple_100: 7.4444 (7.8763)  triple_80: 7.5626 (8.0097)  triple_60: 6.9045 (7.1070)  triple_40: 4.7796 (4.8516)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [680/845]  eta: 0:02:40  loss: 29.1962 (29.4476)  loss_n_40: 0.3033 (0.3373)  loss_n_60: 0.3384 (0.3711)  loss_n_80: 0.3833 (0.4211)  loss_n_100: 0.4341 (0.4652)  triple_100: 7.6621 (7.8796)  triple_80: 7.8488 (8.0124)  triple_60: 7.0460 (7.1086)  triple_40: 4.7796 (4.8524)  time: 0.9751  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [690/845]  eta: 0:02:31  loss: 29.5571 (29.4412)  loss_n_40: 0.3059 (0.3368)  loss_n_60: 0.3463 (0.3707)  loss_n_80: 0.3973 (0.4207)  loss_n_100: 0.4417 (0.4649)  triple_100: 7.6850 (7.8783)  triple_80: 8.0242 (8.0107)  triple_60: 7.0636 (7.1069)  triple_40: 4.8207 (4.8522)  time: 0.9751  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [700/845]  eta: 0:02:21  loss: 28.9963 (29.4463)  loss_n_40: 0.3100 (0.3368)  loss_n_60: 0.3425 (0.3707)  loss_n_80: 0.3910 (0.4206)  loss_n_100: 0.4339 (0.4648)  triple_100: 7.5564 (7.8792)  triple_80: 7.7517 (8.0115)  triple_60: 7.0396 (7.1080)  triple_40: 4.8925 (4.8547)  time: 0.9744  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [710/845]  eta: 0:02:11  loss: 29.5601 (29.4653)  loss_n_40: 0.3266 (0.3367)  loss_n_60: 0.3527 (0.3709)  loss_n_80: 0.4115 (0.4210)  loss_n_100: 0.4555 (0.4653)  triple_100: 7.5564 (7.8857)  triple_80: 7.7814 (8.0175)  triple_60: 7.1503 (7.1122)  triple_40: 4.8925 (4.8561)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [720/845]  eta: 0:02:01  loss: 28.0142 (29.4279)  loss_n_40: 0.3143 (0.3364)  loss_n_60: 0.3399 (0.3703)  loss_n_80: 0.3949 (0.4203)  loss_n_100: 0.4480 (0.4643)  triple_100: 7.5165 (7.8737)  triple_80: 7.6341 (8.0069)  triple_60: 6.7629 (7.1038)  triple_40: 4.6947 (4.8522)  time: 0.9741  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [730/845]  eta: 0:01:52  loss: 27.3493 (29.4210)  loss_n_40: 0.2836 (0.3360)  loss_n_60: 0.3288 (0.3700)  loss_n_80: 0.3742 (0.4201)  loss_n_100: 0.4257 (0.4642)  triple_100: 7.3016 (7.8719)  triple_80: 7.4244 (8.0056)  triple_60: 6.7275 (7.1020)  triple_40: 4.5827 (4.8511)  time: 0.9743  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:7]  [740/845]  eta: 0:01:42  loss: 28.7778 (29.4279)  loss_n_40: 0.2939 (0.3356)  loss_n_60: 0.3465 (0.3699)  loss_n_80: 0.3835 (0.4200)  loss_n_100: 0.4292 (0.4641)  triple_100: 7.7532 (7.8746)  triple_80: 7.9317 (8.0082)  triple_60: 7.1109 (7.1041)  triple_40: 4.7536 (4.8515)  time: 0.9745  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [750/845]  eta: 0:01:32  loss: 29.4836 (29.4217)  loss_n_40: 0.3158 (0.3355)  loss_n_60: 0.3499 (0.3697)  loss_n_80: 0.3927 (0.4198)  loss_n_100: 0.4182 (0.4640)  triple_100: 7.8936 (7.8734)  triple_80: 7.9317 (8.0066)  triple_60: 7.1135 (7.1020)  triple_40: 4.7537 (4.8506)  time: 0.9747  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [760/845]  eta: 0:01:22  loss: 27.8292 (29.4210)  loss_n_40: 0.3241 (0.3353)  loss_n_60: 0.3609 (0.3696)  loss_n_80: 0.3927 (0.4198)  loss_n_100: 0.4468 (0.4641)  triple_100: 7.4167 (7.8750)  triple_80: 7.7324 (8.0075)  triple_60: 6.7398 (7.1013)  triple_40: 4.7510 (4.8484)  time: 0.9748  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:7]  [770/845]  eta: 0:01:13  loss: 28.3124 (29.4118)  loss_n_40: 0.3055 (0.3351)  loss_n_60: 0.3609 (0.3693)  loss_n_80: 0.4150 (0.4194)  loss_n_100: 0.4483 (0.4636)  triple_100: 7.5965 (7.8722)  triple_80: 7.7161 (8.0047)  triple_60: 6.8971 (7.0997)  triple_40: 4.8001 (4.8478)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [780/845]  eta: 0:01:03  loss: 29.1714 (29.4198)  loss_n_40: 0.3055 (0.3347)  loss_n_60: 0.3452 (0.3690)  loss_n_80: 0.3763 (0.4192)  loss_n_100: 0.4162 (0.4635)  triple_100: 7.6227 (7.8754)  triple_80: 7.8301 (8.0068)  triple_60: 7.0358 (7.1016)  triple_40: 4.8785 (4.8495)  time: 0.9749  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [790/845]  eta: 0:00:53  loss: 29.4076 (29.4190)  loss_n_40: 0.3158 (0.3347)  loss_n_60: 0.3452 (0.3690)  loss_n_80: 0.3849 (0.4192)  loss_n_100: 0.4227 (0.4633)  triple_100: 7.7999 (7.8743)  triple_80: 7.9612 (8.0062)  triple_60: 7.1693 (7.1018)  triple_40: 4.9219 (4.8506)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [800/845]  eta: 0:00:43  loss: 29.5841 (29.4250)  loss_n_40: 0.3375 (0.3348)  loss_n_60: 0.3646 (0.3691)  loss_n_80: 0.4297 (0.4194)  loss_n_100: 0.4684 (0.4637)  triple_100: 7.9043 (7.8778)  triple_80: 7.9793 (8.0080)  triple_60: 7.1693 (7.1028)  triple_40: 4.8493 (4.8495)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [810/845]  eta: 0:00:34  loss: 29.1226 (29.4305)  loss_n_40: 0.3381 (0.3348)  loss_n_60: 0.3531 (0.3692)  loss_n_80: 0.4234 (0.4196)  loss_n_100: 0.4620 (0.4639)  triple_100: 7.8964 (7.8797)  triple_80: 7.9793 (8.0100)  triple_60: 6.9785 (7.1029)  triple_40: 4.8206 (4.8504)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [820/845]  eta: 0:00:24  loss: 29.1226 (29.4358)  loss_n_40: 0.3213 (0.3352)  loss_n_60: 0.3944 (0.3696)  loss_n_80: 0.4263 (0.4200)  loss_n_100: 0.4722 (0.4645)  triple_100: 7.5360 (7.8814)  triple_80: 7.7379 (8.0116)  triple_60: 7.1090 (7.1033)  triple_40: 4.7891 (4.8503)  time: 0.9748  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [830/845]  eta: 0:00:14  loss: 29.5186 (29.4307)  loss_n_40: 0.3213 (0.3355)  loss_n_60: 0.3774 (0.3696)  loss_n_80: 0.4191 (0.4200)  loss_n_100: 0.4824 (0.4645)  triple_100: 8.0876 (7.8790)  triple_80: 7.6584 (8.0081)  triple_60: 7.1787 (7.1018)  triple_40: 4.7891 (4.8521)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [840/845]  eta: 0:00:04  loss: 28.6456 (29.4372)  loss_n_40: 0.3094 (0.3354)  loss_n_60: 0.3477 (0.3697)  loss_n_80: 0.4009 (0.4202)  loss_n_100: 0.4364 (0.4648)  triple_100: 7.8748 (7.8818)  triple_80: 7.7132 (8.0105)  triple_60: 6.8855 (7.1026)  triple_40: 4.8646 (4.8522)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7]  [844/845]  eta: 0:00:00  loss: 29.0019 (29.4397)  loss_n_40: 0.3094 (0.3353)  loss_n_60: 0.3763 (0.3698)  loss_n_80: 0.4011 (0.4203)  loss_n_100: 0.4420 (0.4649)  triple_100: 7.8748 (7.8827)  triple_80: 7.9254 (8.0112)  triple_60: 7.1215 (7.1034)  triple_40: 4.8372 (4.8521)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:7] Total time: 0:13:44 (0.9755 s / it)\n",
      "Averaged stats: loss: 29.0019 (29.4397)  loss_n_40: 0.3094 (0.3353)  loss_n_60: 0.3763 (0.3698)  loss_n_80: 0.4011 (0.4203)  loss_n_100: 0.4420 (0.4649)  triple_100: 7.8748 (7.8827)  triple_80: 7.9254 (8.0112)  triple_60: 7.1215 (7.1034)  triple_40: 4.8372 (4.8521)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle_2/low2high/epoch_7_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.465%\n",
      "Min loss_n_100: 0.465\n",
      "Best Epoch: 7.000\n",
      "Train: [epoch:8]  [   0/1724]  eta: 3:00:36  lr: 0.000140  loss: 27.5846 (27.5846)  loss_n_40: 0.3144 (0.3144)  loss_n_60: 0.3347 (0.3347)  loss_n_80: 0.3907 (0.3907)  loss_n_100: 0.4600 (0.4600)  triple_100: 7.4818 (7.4818)  triple_80: 7.4599 (7.4599)  triple_60: 6.5873 (6.5873)  triple_40: 4.5560 (4.5560)  time: 6.2858  data: 0.5607  max mem: 40153\n",
      "Train: [epoch:8]  [  10/1724]  eta: 2:49:13  lr: 0.000140  loss: 28.5432 (29.0684)  loss_n_40: 0.3215 (0.3359)  loss_n_60: 0.3508 (0.3669)  loss_n_80: 0.3907 (0.4083)  loss_n_100: 0.4456 (0.4510)  triple_100: 7.4818 (7.6864)  triple_80: 7.5802 (7.8328)  triple_60: 7.0268 (7.0723)  triple_40: 4.9984 (4.9148)  time: 5.9238  data: 0.0511  max mem: 40153\n",
      "Train: [epoch:8]  [  20/1724]  eta: 2:47:44  lr: 0.000140  loss: 28.7134 (29.3317)  loss_n_40: 0.3215 (0.3364)  loss_n_60: 0.3544 (0.3739)  loss_n_80: 0.3960 (0.4203)  loss_n_100: 0.4456 (0.4639)  triple_100: 7.7517 (7.8113)  triple_80: 7.8468 (7.9620)  triple_60: 6.9338 (7.1233)  triple_40: 4.9471 (4.8405)  time: 5.8873  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [  30/1724]  eta: 2:46:34  lr: 0.000140  loss: 28.7679 (29.4167)  loss_n_40: 0.3240 (0.3403)  loss_n_60: 0.3687 (0.3818)  loss_n_80: 0.4247 (0.4318)  loss_n_100: 0.4805 (0.4779)  triple_100: 7.8847 (7.8768)  triple_80: 7.9769 (8.0212)  triple_60: 6.9338 (7.1168)  triple_40: 4.6398 (4.7701)  time: 5.8870  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [  40/1724]  eta: 2:45:31  lr: 0.000140  loss: 29.4694 (29.4942)  loss_n_40: 0.3267 (0.3364)  loss_n_60: 0.3860 (0.3823)  loss_n_80: 0.4465 (0.4355)  loss_n_100: 0.5014 (0.4845)  triple_100: 8.0003 (7.9508)  triple_80: 8.1045 (8.0754)  triple_60: 7.1059 (7.1259)  triple_40: 4.5176 (4.7033)  time: 5.8887  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [  50/1724]  eta: 2:44:30  lr: 0.000140  loss: 28.9242 (29.3028)  loss_n_40: 0.3022 (0.3301)  loss_n_60: 0.3709 (0.3778)  loss_n_80: 0.4329 (0.4322)  loss_n_100: 0.4935 (0.4819)  triple_100: 7.9773 (7.9227)  triple_80: 8.0152 (8.0394)  triple_60: 6.8737 (7.0690)  triple_40: 4.4430 (4.6497)  time: 5.8908  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [  60/1724]  eta: 2:43:29  lr: 0.000140  loss: 28.4837 (29.3358)  loss_n_40: 0.3175 (0.3316)  loss_n_60: 0.3643 (0.3808)  loss_n_80: 0.4270 (0.4386)  loss_n_100: 0.4782 (0.4902)  triple_100: 7.8589 (7.9569)  triple_80: 7.9486 (8.0675)  triple_60: 6.8073 (7.0504)  triple_40: 4.4184 (4.6198)  time: 5.8901  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [  70/1724]  eta: 2:42:29  lr: 0.000140  loss: 29.6303 (29.5577)  loss_n_40: 0.3478 (0.3381)  loss_n_60: 0.3995 (0.3872)  loss_n_80: 0.4719 (0.4455)  loss_n_100: 0.5401 (0.4982)  triple_100: 8.2116 (8.0132)  triple_80: 8.2317 (8.1283)  triple_60: 7.0444 (7.1042)  triple_40: 4.5353 (4.6430)  time: 5.8887  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [  80/1724]  eta: 2:41:29  lr: 0.000140  loss: 29.9048 (29.6151)  loss_n_40: 0.3541 (0.3415)  loss_n_60: 0.4013 (0.3894)  loss_n_80: 0.4719 (0.4480)  loss_n_100: 0.5208 (0.5000)  triple_100: 8.2116 (8.0235)  triple_80: 8.3144 (8.1516)  triple_60: 7.2390 (7.1168)  triple_40: 4.6595 (4.6442)  time: 5.8894  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [  90/1724]  eta: 2:40:29  lr: 0.000140  loss: 29.9048 (29.8123)  loss_n_40: 0.3484 (0.3457)  loss_n_60: 0.4009 (0.3919)  loss_n_80: 0.4594 (0.4494)  loss_n_100: 0.4976 (0.5005)  triple_100: 7.8725 (8.0574)  triple_80: 8.2492 (8.1882)  triple_60: 7.2458 (7.1709)  triple_40: 4.8735 (4.7084)  time: 5.8907  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 100/1724]  eta: 2:39:30  lr: 0.000140  loss: 29.2416 (29.8346)  loss_n_40: 0.3290 (0.3448)  loss_n_60: 0.3561 (0.3906)  loss_n_80: 0.4123 (0.4477)  loss_n_100: 0.4484 (0.4977)  triple_100: 7.8378 (8.0492)  triple_80: 8.0289 (8.1900)  triple_60: 7.1344 (7.1824)  triple_40: 4.9720 (4.7321)  time: 5.8902  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [ 110/1724]  eta: 2:38:30  lr: 0.000140  loss: 29.0851 (29.8738)  loss_n_40: 0.3532 (0.3468)  loss_n_60: 0.3785 (0.3917)  loss_n_80: 0.4133 (0.4475)  loss_n_100: 0.4484 (0.4974)  triple_100: 7.7777 (8.0475)  triple_80: 7.9251 (8.1889)  triple_60: 7.1039 (7.2009)  triple_40: 4.9224 (4.7530)  time: 5.8894  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 120/1724]  eta: 2:37:31  lr: 0.000140  loss: 28.9160 (29.7641)  loss_n_40: 0.3268 (0.3436)  loss_n_60: 0.3739 (0.3885)  loss_n_80: 0.4093 (0.4438)  loss_n_100: 0.4599 (0.4933)  triple_100: 7.6473 (8.0167)  triple_80: 7.8433 (8.1556)  triple_60: 7.0290 (7.1786)  triple_40: 4.7087 (4.7439)  time: 5.8875  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 130/1724]  eta: 2:36:31  lr: 0.000140  loss: 28.1893 (29.6892)  loss_n_40: 0.3018 (0.3441)  loss_n_60: 0.3519 (0.3874)  loss_n_80: 0.4095 (0.4428)  loss_n_100: 0.4538 (0.4916)  triple_100: 7.6473 (7.9917)  triple_80: 7.8107 (8.1333)  triple_60: 6.7704 (7.1572)  triple_40: 4.6161 (4.7412)  time: 5.8859  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 140/1724]  eta: 2:35:31  lr: 0.000140  loss: 29.2445 (29.7310)  loss_n_40: 0.3252 (0.3439)  loss_n_60: 0.3603 (0.3875)  loss_n_80: 0.4184 (0.4439)  loss_n_100: 0.4759 (0.4934)  triple_100: 7.9289 (8.0105)  triple_80: 7.8688 (8.1468)  triple_60: 6.9635 (7.1599)  triple_40: 4.6535 (4.7451)  time: 5.8859  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 150/1724]  eta: 2:34:32  lr: 0.000140  loss: 29.9544 (29.7850)  loss_n_40: 0.3461 (0.3455)  loss_n_60: 0.3812 (0.3890)  loss_n_80: 0.4495 (0.4459)  loss_n_100: 0.5072 (0.4958)  triple_100: 8.0900 (8.0250)  triple_80: 8.2535 (8.1634)  triple_60: 7.1411 (7.1716)  triple_40: 4.7592 (4.7488)  time: 5.8855  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 160/1724]  eta: 2:33:32  lr: 0.000140  loss: 29.9544 (29.8318)  loss_n_40: 0.3560 (0.3463)  loss_n_60: 0.4050 (0.3902)  loss_n_80: 0.4495 (0.4472)  loss_n_100: 0.5072 (0.4971)  triple_100: 8.0165 (8.0395)  triple_80: 8.2446 (8.1778)  triple_60: 7.1917 (7.1843)  triple_40: 4.6986 (4.7494)  time: 5.8863  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 170/1724]  eta: 2:32:33  lr: 0.000140  loss: 30.2680 (29.8753)  loss_n_40: 0.3444 (0.3480)  loss_n_60: 0.4109 (0.3922)  loss_n_80: 0.4721 (0.4489)  loss_n_100: 0.5162 (0.4989)  triple_100: 8.0165 (8.0495)  triple_80: 8.2263 (8.1886)  triple_60: 7.2370 (7.1970)  triple_40: 4.6073 (4.7524)  time: 5.8877  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 180/1724]  eta: 2:31:34  lr: 0.000140  loss: 30.2680 (29.9526)  loss_n_40: 0.3358 (0.3477)  loss_n_60: 0.3964 (0.3934)  loss_n_80: 0.4818 (0.4512)  loss_n_100: 0.5470 (0.5021)  triple_100: 8.2643 (8.0838)  triple_80: 8.2548 (8.2171)  triple_60: 7.2370 (7.2104)  triple_40: 4.6026 (4.7469)  time: 5.8889  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 190/1724]  eta: 2:30:35  lr: 0.000140  loss: 30.7166 (30.0106)  loss_n_40: 0.3358 (0.3485)  loss_n_60: 0.3949 (0.3944)  loss_n_80: 0.4651 (0.4520)  loss_n_100: 0.5154 (0.5030)  triple_100: 8.2643 (8.0956)  triple_80: 8.2548 (8.2296)  triple_60: 7.2546 (7.2263)  triple_40: 4.6995 (4.7612)  time: 5.8895  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 200/1724]  eta: 2:29:36  lr: 0.000140  loss: 30.5595 (30.0577)  loss_n_40: 0.3405 (0.3493)  loss_n_60: 0.3988 (0.3951)  loss_n_80: 0.4438 (0.4531)  loss_n_100: 0.4889 (0.5042)  triple_100: 8.0484 (8.1100)  triple_80: 8.2193 (8.2435)  triple_60: 7.2440 (7.2350)  triple_40: 4.9102 (4.7675)  time: 5.8888  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 210/1724]  eta: 2:28:37  lr: 0.000140  loss: 29.7804 (30.0098)  loss_n_40: 0.3201 (0.3476)  loss_n_60: 0.3745 (0.3936)  loss_n_80: 0.4438 (0.4515)  loss_n_100: 0.4811 (0.5026)  triple_100: 7.9991 (8.0981)  triple_80: 8.2193 (8.2316)  triple_60: 7.1730 (7.2240)  triple_40: 4.6814 (4.7609)  time: 5.8880  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 220/1724]  eta: 2:27:38  lr: 0.000140  loss: 28.4802 (29.9543)  loss_n_40: 0.3149 (0.3465)  loss_n_60: 0.3633 (0.3927)  loss_n_80: 0.4081 (0.4506)  loss_n_100: 0.4597 (0.5017)  triple_100: 7.6686 (8.0820)  triple_80: 7.8139 (8.2156)  triple_60: 6.9232 (7.2118)  triple_40: 4.5652 (4.7534)  time: 5.8881  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 230/1724]  eta: 2:26:39  lr: 0.000140  loss: 28.1285 (29.8597)  loss_n_40: 0.3195 (0.3450)  loss_n_60: 0.3508 (0.3908)  loss_n_80: 0.3946 (0.4482)  loss_n_100: 0.4443 (0.4991)  triple_100: 7.5917 (8.0549)  triple_80: 7.6467 (8.1884)  triple_60: 6.7930 (7.1917)  triple_40: 4.5598 (4.7416)  time: 5.8880  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 240/1724]  eta: 2:25:40  lr: 0.000140  loss: 28.5233 (29.8500)  loss_n_40: 0.3242 (0.3448)  loss_n_60: 0.3648 (0.3909)  loss_n_80: 0.4167 (0.4480)  loss_n_100: 0.4554 (0.4988)  triple_100: 7.7277 (8.0525)  triple_80: 7.8312 (8.1855)  triple_60: 6.9774 (7.1926)  triple_40: 4.5511 (4.7369)  time: 5.8880  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 250/1724]  eta: 2:24:41  lr: 0.000140  loss: 28.8726 (29.7728)  loss_n_40: 0.2897 (0.3425)  loss_n_60: 0.3491 (0.3884)  loss_n_80: 0.4083 (0.4452)  loss_n_100: 0.4567 (0.4957)  triple_100: 7.8198 (8.0328)  triple_80: 7.8640 (8.1639)  triple_60: 6.8973 (7.1749)  triple_40: 4.5770 (4.7294)  time: 5.8893  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 260/1724]  eta: 2:23:42  lr: 0.000140  loss: 27.7280 (29.7167)  loss_n_40: 0.2841 (0.3407)  loss_n_60: 0.3209 (0.3862)  loss_n_80: 0.3667 (0.4425)  loss_n_100: 0.4039 (0.4927)  triple_100: 7.4486 (8.0164)  triple_80: 7.6127 (8.1466)  triple_60: 6.7664 (7.1639)  triple_40: 4.6324 (4.7277)  time: 5.8893  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 270/1724]  eta: 2:22:43  lr: 0.000140  loss: 28.1241 (29.6636)  loss_n_40: 0.3079 (0.3406)  loss_n_60: 0.3468 (0.3850)  loss_n_80: 0.3829 (0.4407)  loss_n_100: 0.4254 (0.4905)  triple_100: 7.4486 (7.9957)  triple_80: 7.6058 (8.1272)  triple_60: 6.8348 (7.1531)  triple_40: 4.7567 (4.7306)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 280/1724]  eta: 2:21:44  lr: 0.000140  loss: 28.3048 (29.6219)  loss_n_40: 0.3100 (0.3393)  loss_n_60: 0.3487 (0.3836)  loss_n_80: 0.4011 (0.4391)  loss_n_100: 0.4425 (0.4886)  triple_100: 7.5470 (7.9838)  triple_80: 7.6737 (8.1147)  triple_60: 6.8675 (7.1439)  triple_40: 4.7481 (4.7289)  time: 5.8890  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 290/1724]  eta: 2:20:46  lr: 0.000140  loss: 28.0386 (29.5822)  loss_n_40: 0.2824 (0.3378)  loss_n_60: 0.3392 (0.3821)  loss_n_80: 0.3893 (0.4374)  loss_n_100: 0.4327 (0.4868)  triple_100: 7.6643 (7.9738)  triple_80: 7.7556 (8.1032)  triple_60: 6.7620 (7.1360)  triple_40: 4.6758 (4.7251)  time: 5.8911  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 300/1724]  eta: 2:19:47  lr: 0.000140  loss: 28.1972 (29.5800)  loss_n_40: 0.3162 (0.3377)  loss_n_60: 0.3448 (0.3819)  loss_n_80: 0.3900 (0.4371)  loss_n_100: 0.4327 (0.4864)  triple_100: 7.7236 (7.9729)  triple_80: 7.7888 (8.1013)  triple_60: 6.8201 (7.1350)  triple_40: 4.6758 (4.7275)  time: 5.8915  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 310/1724]  eta: 2:18:48  lr: 0.000140  loss: 29.8545 (29.6072)  loss_n_40: 0.3463 (0.3382)  loss_n_60: 0.3735 (0.3819)  loss_n_80: 0.4176 (0.4368)  loss_n_100: 0.4575 (0.4860)  triple_100: 7.9298 (7.9765)  triple_80: 8.0138 (8.1049)  triple_60: 7.2608 (7.1427)  triple_40: 4.9846 (4.7401)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 320/1724]  eta: 2:17:49  lr: 0.000140  loss: 30.2254 (29.6220)  loss_n_40: 0.3404 (0.3381)  loss_n_60: 0.3725 (0.3817)  loss_n_80: 0.4195 (0.4364)  loss_n_100: 0.4792 (0.4856)  triple_100: 8.0027 (7.9804)  triple_80: 8.1367 (8.1074)  triple_60: 7.3070 (7.1464)  triple_40: 5.0190 (4.7459)  time: 5.8897  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 330/1724]  eta: 2:16:50  lr: 0.000140  loss: 29.1517 (29.6152)  loss_n_40: 0.3039 (0.3373)  loss_n_60: 0.3529 (0.3810)  loss_n_80: 0.4103 (0.4357)  loss_n_100: 0.4674 (0.4848)  triple_100: 7.9233 (7.9790)  triple_80: 7.9885 (8.1055)  triple_60: 7.0874 (7.1452)  triple_40: 4.7891 (4.7467)  time: 5.8905  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 340/1724]  eta: 2:15:51  lr: 0.000140  loss: 30.2815 (29.6580)  loss_n_40: 0.3072 (0.3384)  loss_n_60: 0.3655 (0.3818)  loss_n_80: 0.4303 (0.4365)  loss_n_100: 0.4733 (0.4854)  triple_100: 7.9941 (7.9893)  triple_80: 8.1368 (8.1158)  triple_60: 7.2795 (7.1549)  triple_40: 4.7553 (4.7560)  time: 5.8894  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [ 350/1724]  eta: 2:14:52  lr: 0.000140  loss: 30.5351 (29.6802)  loss_n_40: 0.3517 (0.3389)  loss_n_60: 0.3909 (0.3820)  loss_n_80: 0.4428 (0.4369)  loss_n_100: 0.5038 (0.4858)  triple_100: 8.4556 (7.9970)  triple_80: 8.4017 (8.1224)  triple_60: 7.3169 (7.1591)  triple_40: 4.6899 (4.7581)  time: 5.8899  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 360/1724]  eta: 2:13:53  lr: 0.000140  loss: 29.9260 (29.6853)  loss_n_40: 0.3212 (0.3386)  loss_n_60: 0.3670 (0.3816)  loss_n_80: 0.4302 (0.4364)  loss_n_100: 0.4759 (0.4852)  triple_100: 8.1093 (7.9974)  triple_80: 8.1888 (8.1233)  triple_60: 7.2524 (7.1613)  triple_40: 4.7506 (4.7615)  time: 5.8912  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 370/1724]  eta: 2:12:55  lr: 0.000140  loss: 29.9260 (29.6801)  loss_n_40: 0.3213 (0.3384)  loss_n_60: 0.3636 (0.3814)  loss_n_80: 0.4232 (0.4359)  loss_n_100: 0.4621 (0.4846)  triple_100: 8.0030 (7.9933)  triple_80: 8.1308 (8.1198)  triple_60: 7.1375 (7.1615)  triple_40: 4.7569 (4.7651)  time: 5.8904  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 380/1724]  eta: 2:11:56  lr: 0.000140  loss: 29.5991 (29.6828)  loss_n_40: 0.3331 (0.3380)  loss_n_60: 0.3660 (0.3813)  loss_n_80: 0.4028 (0.4358)  loss_n_100: 0.4583 (0.4846)  triple_100: 8.0901 (7.9975)  triple_80: 8.1308 (8.1225)  triple_60: 7.0515 (7.1625)  triple_40: 4.6267 (4.7605)  time: 5.8919  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 390/1724]  eta: 2:10:57  lr: 0.000140  loss: 29.1808 (29.6566)  loss_n_40: 0.3280 (0.3373)  loss_n_60: 0.3640 (0.3805)  loss_n_80: 0.4039 (0.4349)  loss_n_100: 0.4510 (0.4836)  triple_100: 7.8400 (7.9916)  triple_80: 7.9479 (8.1156)  triple_60: 7.0274 (7.1570)  triple_40: 4.5187 (4.7563)  time: 5.8909  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 400/1724]  eta: 2:09:58  lr: 0.000140  loss: 28.3132 (29.6256)  loss_n_40: 0.2728 (0.3359)  loss_n_60: 0.3403 (0.3792)  loss_n_80: 0.4021 (0.4338)  loss_n_100: 0.4410 (0.4827)  triple_100: 7.7239 (7.9893)  triple_80: 7.7669 (8.1087)  triple_60: 6.7491 (7.1483)  triple_40: 4.4182 (4.7477)  time: 5.8891  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 410/1724]  eta: 2:08:59  lr: 0.000140  loss: 28.7489 (29.6206)  loss_n_40: 0.2806 (0.3357)  loss_n_60: 0.3374 (0.3789)  loss_n_80: 0.4021 (0.4336)  loss_n_100: 0.4564 (0.4827)  triple_100: 7.9834 (7.9898)  triple_80: 7.7669 (8.1065)  triple_60: 6.7227 (7.1449)  triple_40: 4.5257 (4.7486)  time: 5.8887  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 420/1724]  eta: 2:08:00  lr: 0.000140  loss: 28.7489 (29.6047)  loss_n_40: 0.3036 (0.3354)  loss_n_60: 0.3473 (0.3783)  loss_n_80: 0.4078 (0.4328)  loss_n_100: 0.4564 (0.4816)  triple_100: 7.8130 (7.9830)  triple_80: 7.9515 (8.1004)  triple_60: 6.8503 (7.1419)  triple_40: 4.7713 (4.7513)  time: 5.8877  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 430/1724]  eta: 2:07:01  lr: 0.000140  loss: 29.4839 (29.6604)  loss_n_40: 0.3364 (0.3361)  loss_n_60: 0.3750 (0.3791)  loss_n_80: 0.4300 (0.4337)  loss_n_100: 0.4741 (0.4825)  triple_100: 8.0066 (7.9972)  triple_80: 8.0797 (8.1146)  triple_60: 7.1062 (7.1556)  triple_40: 4.8401 (4.7617)  time: 5.8883  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 440/1724]  eta: 2:06:02  lr: 0.000140  loss: 30.8894 (29.6724)  loss_n_40: 0.3482 (0.3364)  loss_n_60: 0.3822 (0.3792)  loss_n_80: 0.4402 (0.4336)  loss_n_100: 0.4913 (0.4823)  triple_100: 8.3432 (7.9997)  triple_80: 8.5075 (8.1173)  triple_60: 7.4234 (7.1595)  triple_40: 4.9927 (4.7647)  time: 5.8885  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 450/1724]  eta: 2:05:03  lr: 0.000140  loss: 30.1145 (29.6909)  loss_n_40: 0.3322 (0.3364)  loss_n_60: 0.3682 (0.3791)  loss_n_80: 0.4345 (0.4333)  loss_n_100: 0.4660 (0.4819)  triple_100: 8.1104 (8.0026)  triple_80: 8.2451 (8.1205)  triple_60: 7.2793 (7.1654)  triple_40: 4.9778 (4.7716)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 460/1724]  eta: 2:04:04  lr: 0.000140  loss: 28.7856 (29.6598)  loss_n_40: 0.2967 (0.3353)  loss_n_60: 0.3254 (0.3779)  loss_n_80: 0.3750 (0.4320)  loss_n_100: 0.4237 (0.4804)  triple_100: 7.7230 (7.9941)  triple_80: 7.8364 (8.1110)  triple_60: 7.0164 (7.1592)  triple_40: 4.8002 (4.7700)  time: 5.8883  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 470/1724]  eta: 2:03:05  lr: 0.000140  loss: 28.7009 (29.6575)  loss_n_40: 0.2967 (0.3352)  loss_n_60: 0.3464 (0.3779)  loss_n_80: 0.3911 (0.4318)  loss_n_100: 0.4247 (0.4801)  triple_100: 7.7230 (7.9935)  triple_80: 7.7761 (8.1099)  triple_60: 7.0164 (7.1592)  triple_40: 4.7239 (4.7700)  time: 5.8882  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 480/1724]  eta: 2:02:06  lr: 0.000140  loss: 29.4301 (29.6654)  loss_n_40: 0.3173 (0.3353)  loss_n_60: 0.3653 (0.3779)  loss_n_80: 0.4205 (0.4317)  loss_n_100: 0.4695 (0.4800)  triple_100: 7.9390 (7.9944)  triple_80: 8.0606 (8.1106)  triple_60: 7.1413 (7.1614)  triple_40: 4.7942 (4.7740)  time: 5.8891  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 490/1724]  eta: 2:01:07  lr: 0.000140  loss: 29.4301 (29.6803)  loss_n_40: 0.3350 (0.3357)  loss_n_60: 0.3722 (0.3782)  loss_n_80: 0.4218 (0.4321)  loss_n_100: 0.4726 (0.4804)  triple_100: 7.9390 (7.9987)  triple_80: 8.0606 (8.1151)  triple_60: 7.1380 (7.1652)  triple_40: 4.8054 (4.7750)  time: 5.8893  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 500/1724]  eta: 2:00:09  lr: 0.000140  loss: 29.0030 (29.6632)  loss_n_40: 0.3181 (0.3351)  loss_n_60: 0.3694 (0.3776)  loss_n_80: 0.4153 (0.4313)  loss_n_100: 0.4365 (0.4796)  triple_100: 7.8123 (7.9935)  triple_80: 7.9130 (8.1097)  triple_60: 7.0378 (7.1617)  triple_40: 4.7341 (4.7748)  time: 5.8899  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 510/1724]  eta: 1:59:10  lr: 0.000140  loss: 28.6524 (29.6383)  loss_n_40: 0.2944 (0.3353)  loss_n_60: 0.3392 (0.3773)  loss_n_80: 0.3889 (0.4306)  loss_n_100: 0.4365 (0.4788)  triple_100: 7.8236 (7.9837)  triple_80: 7.8724 (8.1003)  triple_60: 7.0165 (7.1569)  triple_40: 4.7341 (4.7753)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 520/1724]  eta: 1:58:11  lr: 0.000140  loss: 29.0335 (29.6494)  loss_n_40: 0.3279 (0.3355)  loss_n_60: 0.3696 (0.3776)  loss_n_80: 0.4118 (0.4310)  loss_n_100: 0.4722 (0.4792)  triple_100: 7.8521 (7.9876)  triple_80: 8.0519 (8.1037)  triple_60: 7.0217 (7.1594)  triple_40: 4.6836 (4.7755)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 530/1724]  eta: 1:57:12  lr: 0.000140  loss: 30.3193 (29.6643)  loss_n_40: 0.3306 (0.3356)  loss_n_60: 0.3835 (0.3778)  loss_n_80: 0.4436 (0.4312)  loss_n_100: 0.4857 (0.4796)  triple_100: 8.0480 (7.9931)  triple_80: 8.2123 (8.1086)  triple_60: 7.3593 (7.1627)  triple_40: 4.6818 (4.7757)  time: 5.8882  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 540/1724]  eta: 1:56:13  lr: 0.000140  loss: 31.0101 (29.7255)  loss_n_40: 0.3311 (0.3366)  loss_n_60: 0.3935 (0.3789)  loss_n_80: 0.4602 (0.4326)  loss_n_100: 0.5139 (0.4810)  triple_100: 8.6381 (8.0100)  triple_80: 8.5945 (8.1253)  triple_60: 7.3593 (7.1771)  triple_40: 4.9149 (4.7840)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 550/1724]  eta: 1:55:14  lr: 0.000140  loss: 30.6934 (29.7473)  loss_n_40: 0.3311 (0.3365)  loss_n_60: 0.3929 (0.3793)  loss_n_80: 0.4582 (0.4332)  loss_n_100: 0.5139 (0.4820)  triple_100: 8.5863 (8.0204)  triple_80: 8.5284 (8.1341)  triple_60: 7.3353 (7.1808)  triple_40: 4.7772 (4.7811)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 560/1724]  eta: 1:54:15  lr: 0.000140  loss: 30.6429 (29.7501)  loss_n_40: 0.3120 (0.3363)  loss_n_60: 0.3704 (0.3791)  loss_n_80: 0.4377 (0.4331)  loss_n_100: 0.4911 (0.4818)  triple_100: 8.4172 (8.0226)  triple_80: 8.4849 (8.1351)  triple_60: 7.3177 (7.1808)  triple_40: 4.6197 (4.7813)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 570/1724]  eta: 1:53:16  lr: 0.000140  loss: 29.2753 (29.7481)  loss_n_40: 0.3122 (0.3362)  loss_n_60: 0.3561 (0.3789)  loss_n_80: 0.4131 (0.4330)  loss_n_100: 0.4663 (0.4819)  triple_100: 7.8848 (8.0245)  triple_80: 7.9340 (8.1356)  triple_60: 6.9968 (7.1786)  triple_40: 4.7178 (4.7795)  time: 5.8886  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 580/1724]  eta: 1:52:17  lr: 0.000140  loss: 29.4286 (29.7819)  loss_n_40: 0.3217 (0.3371)  loss_n_60: 0.3720 (0.3796)  loss_n_80: 0.4331 (0.4339)  loss_n_100: 0.4893 (0.4829)  triple_100: 7.9740 (8.0346)  triple_80: 7.9782 (8.1449)  triple_60: 7.0157 (7.1845)  triple_40: 4.7859 (4.7846)  time: 5.8885  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [ 590/1724]  eta: 1:51:18  lr: 0.000140  loss: 30.2200 (29.7951)  loss_n_40: 0.3355 (0.3372)  loss_n_60: 0.3986 (0.3800)  loss_n_80: 0.4792 (0.4346)  loss_n_100: 0.5332 (0.4838)  triple_100: 8.4636 (8.0414)  triple_80: 8.5353 (8.1505)  triple_60: 7.1613 (7.1859)  triple_40: 4.6786 (4.7817)  time: 5.8889  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 600/1724]  eta: 1:50:19  lr: 0.000140  loss: 30.0118 (29.7879)  loss_n_40: 0.3117 (0.3371)  loss_n_60: 0.3785 (0.3797)  loss_n_80: 0.4484 (0.4343)  loss_n_100: 0.4845 (0.4836)  triple_100: 8.1134 (8.0407)  triple_80: 8.3276 (8.1488)  triple_60: 7.1231 (7.1839)  triple_40: 4.6728 (4.7798)  time: 5.8897  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 610/1724]  eta: 1:49:21  lr: 0.000140  loss: 28.9340 (29.7749)  loss_n_40: 0.3202 (0.3370)  loss_n_60: 0.3492 (0.3793)  loss_n_80: 0.3857 (0.4339)  loss_n_100: 0.4378 (0.4830)  triple_100: 7.8224 (8.0359)  triple_80: 7.8199 (8.1440)  triple_60: 7.0394 (7.1806)  triple_40: 4.7352 (4.7813)  time: 5.8912  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 620/1724]  eta: 1:48:22  lr: 0.000140  loss: 29.3491 (29.7880)  loss_n_40: 0.3362 (0.3368)  loss_n_60: 0.3566 (0.3794)  loss_n_80: 0.4237 (0.4340)  loss_n_100: 0.4654 (0.4831)  triple_100: 7.8891 (8.0407)  triple_80: 8.0423 (8.1482)  triple_60: 7.0918 (7.1840)  triple_40: 4.8475 (4.7815)  time: 5.8917  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 630/1724]  eta: 1:47:23  lr: 0.000140  loss: 29.6277 (29.8046)  loss_n_40: 0.3274 (0.3367)  loss_n_60: 0.3589 (0.3795)  loss_n_80: 0.4283 (0.4342)  loss_n_100: 0.4654 (0.4833)  triple_100: 8.1257 (8.0458)  triple_80: 8.1802 (8.1528)  triple_60: 7.1819 (7.1882)  triple_40: 4.8960 (4.7842)  time: 5.8919  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 640/1724]  eta: 1:46:24  lr: 0.000140  loss: 29.6277 (29.8034)  loss_n_40: 0.3071 (0.3365)  loss_n_60: 0.3587 (0.3792)  loss_n_80: 0.4098 (0.4339)  loss_n_100: 0.4576 (0.4830)  triple_100: 8.1099 (8.0452)  triple_80: 8.1829 (8.1518)  triple_60: 7.1819 (7.1873)  triple_40: 4.8250 (4.7865)  time: 5.8921  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 650/1724]  eta: 1:45:25  lr: 0.000140  loss: 30.4135 (29.8375)  loss_n_40: 0.3721 (0.3377)  loss_n_60: 0.4030 (0.3802)  loss_n_80: 0.4503 (0.4347)  loss_n_100: 0.4902 (0.4836)  triple_100: 8.1852 (8.0517)  triple_80: 8.2941 (8.1593)  triple_60: 7.4242 (7.1963)  triple_40: 5.0115 (4.7941)  time: 5.8907  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 660/1724]  eta: 1:44:26  lr: 0.000140  loss: 30.8647 (29.8545)  loss_n_40: 0.3758 (0.3380)  loss_n_60: 0.4214 (0.3806)  loss_n_80: 0.4694 (0.4351)  loss_n_100: 0.5142 (0.4840)  triple_100: 8.4689 (8.0564)  triple_80: 8.5301 (8.1639)  triple_60: 7.4979 (7.1999)  triple_40: 5.0115 (4.7964)  time: 5.8894  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 670/1724]  eta: 1:43:27  lr: 0.000140  loss: 29.8873 (29.8640)  loss_n_40: 0.3452 (0.3381)  loss_n_60: 0.3940 (0.3806)  loss_n_80: 0.4382 (0.4352)  loss_n_100: 0.4869 (0.4842)  triple_100: 8.1456 (8.0600)  triple_80: 8.2145 (8.1670)  triple_60: 7.2346 (7.2019)  triple_40: 4.8226 (4.7971)  time: 5.8903  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 680/1724]  eta: 1:42:28  lr: 0.000140  loss: 29.8101 (29.8642)  loss_n_40: 0.3191 (0.3379)  loss_n_60: 0.3644 (0.3806)  loss_n_80: 0.4272 (0.4352)  loss_n_100: 0.4863 (0.4842)  triple_100: 8.1456 (8.0612)  triple_80: 8.2145 (8.1674)  triple_60: 7.0033 (7.2019)  triple_40: 4.6372 (4.7957)  time: 5.8899  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 690/1724]  eta: 1:41:29  lr: 0.000140  loss: 29.8916 (29.8708)  loss_n_40: 0.3168 (0.3379)  loss_n_60: 0.3608 (0.3805)  loss_n_80: 0.4194 (0.4350)  loss_n_100: 0.4687 (0.4840)  triple_100: 8.2017 (8.0624)  triple_80: 8.1897 (8.1685)  triple_60: 7.1212 (7.2040)  triple_40: 4.8617 (4.7985)  time: 5.8883  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 700/1724]  eta: 1:40:31  lr: 0.000140  loss: 30.2799 (29.8800)  loss_n_40: 0.3179 (0.3380)  loss_n_60: 0.3695 (0.3805)  loss_n_80: 0.4284 (0.4351)  loss_n_100: 0.4687 (0.4842)  triple_100: 8.2122 (8.0660)  triple_80: 8.1897 (8.1714)  triple_60: 7.2719 (7.2056)  triple_40: 4.8862 (4.7993)  time: 5.8878  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 710/1724]  eta: 1:39:32  lr: 0.000140  loss: 29.5265 (29.8672)  loss_n_40: 0.3076 (0.3377)  loss_n_60: 0.3626 (0.3801)  loss_n_80: 0.4216 (0.4344)  loss_n_100: 0.4658 (0.4834)  triple_100: 8.0220 (8.0615)  triple_80: 8.0457 (8.1667)  triple_60: 7.1331 (7.2036)  triple_40: 4.8079 (4.7998)  time: 5.8877  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 720/1724]  eta: 1:38:33  lr: 0.000140  loss: 28.8000 (29.8650)  loss_n_40: 0.3062 (0.3374)  loss_n_60: 0.3527 (0.3798)  loss_n_80: 0.4053 (0.4340)  loss_n_100: 0.4536 (0.4830)  triple_100: 7.7665 (8.0612)  triple_80: 7.8080 (8.1654)  triple_60: 6.9414 (7.2035)  triple_40: 4.7906 (4.8006)  time: 5.9138  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 730/1724]  eta: 1:37:34  lr: 0.000140  loss: 29.2136 (29.8645)  loss_n_40: 0.3172 (0.3374)  loss_n_60: 0.3606 (0.3796)  loss_n_80: 0.4081 (0.4338)  loss_n_100: 0.4506 (0.4828)  triple_100: 7.8373 (8.0608)  triple_80: 7.8317 (8.1647)  triple_60: 7.0934 (7.2033)  triple_40: 4.8440 (4.8022)  time: 5.9131  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 740/1724]  eta: 1:36:35  lr: 0.000140  loss: 29.7261 (29.8615)  loss_n_40: 0.3168 (0.3371)  loss_n_60: 0.3606 (0.3793)  loss_n_80: 0.4221 (0.4336)  loss_n_100: 0.4800 (0.4826)  triple_100: 8.0053 (8.0609)  triple_80: 8.1533 (8.1641)  triple_60: 7.0934 (7.2018)  triple_40: 4.7178 (4.8020)  time: 5.8864  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 750/1724]  eta: 1:35:37  lr: 0.000140  loss: 29.7535 (29.8574)  loss_n_40: 0.2954 (0.3366)  loss_n_60: 0.3607 (0.3790)  loss_n_80: 0.4292 (0.4333)  loss_n_100: 0.4896 (0.4824)  triple_100: 8.2743 (8.0619)  triple_80: 8.2372 (8.1640)  triple_60: 7.1577 (7.2009)  triple_40: 4.6019 (4.7994)  time: 5.8878  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 760/1724]  eta: 1:34:38  lr: 0.000140  loss: 30.1005 (29.8520)  loss_n_40: 0.2954 (0.3362)  loss_n_60: 0.3621 (0.3787)  loss_n_80: 0.4331 (0.4331)  loss_n_100: 0.4913 (0.4821)  triple_100: 8.3611 (8.0613)  triple_80: 8.3295 (8.1630)  triple_60: 7.1854 (7.1995)  triple_40: 4.5853 (4.7980)  time: 5.8884  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 770/1724]  eta: 1:33:39  lr: 0.000140  loss: 29.4419 (29.8639)  loss_n_40: 0.3183 (0.3361)  loss_n_60: 0.3540 (0.3787)  loss_n_80: 0.4345 (0.4332)  loss_n_100: 0.4763 (0.4822)  triple_100: 8.0334 (8.0650)  triple_80: 8.2382 (8.1666)  triple_60: 7.1274 (7.2023)  triple_40: 4.6777 (4.7998)  time: 5.8873  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 780/1724]  eta: 1:32:40  lr: 0.000140  loss: 29.3303 (29.8586)  loss_n_40: 0.3187 (0.3359)  loss_n_60: 0.3540 (0.3785)  loss_n_80: 0.4078 (0.4329)  loss_n_100: 0.4513 (0.4819)  triple_100: 7.9884 (8.0635)  triple_80: 8.0405 (8.1650)  triple_60: 7.0626 (7.2010)  triple_40: 4.7578 (4.7999)  time: 5.8885  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 790/1724]  eta: 1:31:41  lr: 0.000140  loss: 29.3054 (29.8495)  loss_n_40: 0.3117 (0.3355)  loss_n_60: 0.3469 (0.3781)  loss_n_80: 0.4005 (0.4326)  loss_n_100: 0.4511 (0.4817)  triple_100: 7.9258 (8.0628)  triple_80: 8.0181 (8.1631)  triple_60: 7.0270 (7.1983)  triple_40: 4.7373 (4.7975)  time: 5.8901  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 800/1724]  eta: 1:30:42  lr: 0.000140  loss: 28.6473 (29.8329)  loss_n_40: 0.3034 (0.3350)  loss_n_60: 0.3374 (0.3776)  loss_n_80: 0.3839 (0.4321)  loss_n_100: 0.4319 (0.4811)  triple_100: 7.7945 (8.0581)  triple_80: 7.8408 (8.1583)  triple_60: 7.0159 (7.1950)  triple_40: 4.5798 (4.7957)  time: 5.8899  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 810/1724]  eta: 1:29:43  lr: 0.000140  loss: 28.9828 (29.8373)  loss_n_40: 0.3143 (0.3352)  loss_n_60: 0.3433 (0.3777)  loss_n_80: 0.3973 (0.4322)  loss_n_100: 0.4522 (0.4812)  triple_100: 7.7323 (8.0591)  triple_80: 7.7951 (8.1595)  triple_60: 7.0174 (7.1957)  triple_40: 4.6841 (4.7965)  time: 5.8899  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 820/1724]  eta: 1:28:44  lr: 0.000140  loss: 28.9828 (29.8214)  loss_n_40: 0.3132 (0.3349)  loss_n_60: 0.3511 (0.3772)  loss_n_80: 0.4063 (0.4316)  loss_n_100: 0.4514 (0.4805)  triple_100: 7.7478 (8.0544)  triple_80: 7.9005 (8.1547)  triple_60: 6.9633 (7.1919)  triple_40: 4.8169 (4.7962)  time: 5.8899  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [ 830/1724]  eta: 1:27:45  lr: 0.000140  loss: 28.9494 (29.8277)  loss_n_40: 0.3076 (0.3351)  loss_n_60: 0.3511 (0.3774)  loss_n_80: 0.4063 (0.4318)  loss_n_100: 0.4514 (0.4808)  triple_100: 7.8230 (8.0564)  triple_80: 7.9013 (8.1566)  triple_60: 6.9633 (7.1925)  triple_40: 4.8169 (4.7970)  time: 5.8900  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 840/1724]  eta: 1:26:46  lr: 0.000140  loss: 30.4421 (29.8392)  loss_n_40: 0.3262 (0.3352)  loss_n_60: 0.3694 (0.3774)  loss_n_80: 0.4375 (0.4319)  loss_n_100: 0.4987 (0.4809)  triple_100: 8.1700 (8.0596)  triple_80: 8.3069 (8.1595)  triple_60: 7.3371 (7.1955)  triple_40: 4.8023 (4.7993)  time: 5.8891  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 850/1724]  eta: 1:25:47  lr: 0.000140  loss: 29.9323 (29.8463)  loss_n_40: 0.3110 (0.3352)  loss_n_60: 0.3694 (0.3776)  loss_n_80: 0.4297 (0.4321)  loss_n_100: 0.4790 (0.4812)  triple_100: 8.1700 (8.0622)  triple_80: 8.2697 (8.1615)  triple_60: 7.2138 (7.1968)  triple_40: 4.8197 (4.7997)  time: 5.8883  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 860/1724]  eta: 1:24:49  lr: 0.000140  loss: 30.6994 (29.8740)  loss_n_40: 0.3674 (0.3359)  loss_n_60: 0.3976 (0.3783)  loss_n_80: 0.4481 (0.4329)  loss_n_100: 0.5043 (0.4820)  triple_100: 8.3845 (8.0700)  triple_80: 8.3404 (8.1693)  triple_60: 7.3584 (7.2033)  triple_40: 4.8342 (4.8022)  time: 5.8886  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 870/1724]  eta: 1:23:50  lr: 0.000140  loss: 31.6040 (29.8931)  loss_n_40: 0.3679 (0.3363)  loss_n_60: 0.3976 (0.3786)  loss_n_80: 0.4481 (0.4331)  loss_n_100: 0.5043 (0.4821)  triple_100: 8.4783 (8.0739)  triple_80: 8.5937 (8.1735)  triple_60: 7.5654 (7.2083)  triple_40: 5.0932 (4.8073)  time: 5.8888  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 880/1724]  eta: 1:22:51  lr: 0.000140  loss: 31.1778 (29.9004)  loss_n_40: 0.3619 (0.3367)  loss_n_60: 0.3929 (0.3788)  loss_n_80: 0.4395 (0.4332)  loss_n_100: 0.4777 (0.4822)  triple_100: 8.3978 (8.0743)  triple_80: 8.5142 (8.1745)  triple_60: 7.5320 (7.2105)  triple_40: 5.1264 (4.8102)  time: 5.8908  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 890/1724]  eta: 1:21:52  lr: 0.000140  loss: 30.1043 (29.9058)  loss_n_40: 0.3515 (0.3368)  loss_n_60: 0.3832 (0.3787)  loss_n_80: 0.4220 (0.4330)  loss_n_100: 0.4669 (0.4818)  triple_100: 7.9766 (8.0738)  triple_80: 8.0997 (8.1743)  triple_60: 7.3688 (7.2130)  triple_40: 5.1264 (4.8144)  time: 5.8918  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 900/1724]  eta: 1:20:53  lr: 0.000140  loss: 29.9453 (29.9099)  loss_n_40: 0.3491 (0.3369)  loss_n_60: 0.3618 (0.3787)  loss_n_80: 0.4141 (0.4330)  loss_n_100: 0.4525 (0.4817)  triple_100: 7.9461 (8.0740)  triple_80: 8.0311 (8.1749)  triple_60: 7.2436 (7.2135)  triple_40: 5.0835 (4.8172)  time: 5.8914  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 910/1724]  eta: 1:19:54  lr: 0.000140  loss: 29.6605 (29.9074)  loss_n_40: 0.3233 (0.3369)  loss_n_60: 0.3597 (0.3785)  loss_n_80: 0.3986 (0.4327)  loss_n_100: 0.4427 (0.4814)  triple_100: 7.8830 (8.0725)  triple_80: 7.9753 (8.1734)  triple_60: 7.1302 (7.2130)  triple_40: 5.0293 (4.8190)  time: 5.8917  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 920/1724]  eta: 1:18:55  lr: 0.000140  loss: 29.3324 (29.8985)  loss_n_40: 0.3144 (0.3368)  loss_n_60: 0.3382 (0.3782)  loss_n_80: 0.3801 (0.4322)  loss_n_100: 0.4231 (0.4808)  triple_100: 7.7935 (8.0687)  triple_80: 7.8560 (8.1699)  triple_60: 7.1190 (7.2112)  triple_40: 4.9277 (4.8207)  time: 5.8919  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 930/1724]  eta: 1:17:56  lr: 0.000140  loss: 29.6068 (29.9050)  loss_n_40: 0.3144 (0.3369)  loss_n_60: 0.3436 (0.3782)  loss_n_80: 0.3913 (0.4322)  loss_n_100: 0.4297 (0.4807)  triple_100: 7.8702 (8.0698)  triple_80: 7.9885 (8.1712)  triple_60: 7.1433 (7.2127)  triple_40: 5.0404 (4.8234)  time: 5.8929  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 940/1724]  eta: 1:16:57  lr: 0.000140  loss: 29.7581 (29.9020)  loss_n_40: 0.3194 (0.3370)  loss_n_60: 0.3499 (0.3780)  loss_n_80: 0.3940 (0.4320)  loss_n_100: 0.4377 (0.4803)  triple_100: 7.8702 (8.0667)  triple_80: 8.0079 (8.1691)  triple_60: 7.2392 (7.2125)  triple_40: 5.0825 (4.8263)  time: 5.8943  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 950/1724]  eta: 1:15:59  lr: 0.000140  loss: 29.6367 (29.9060)  loss_n_40: 0.3160 (0.3370)  loss_n_60: 0.3531 (0.3779)  loss_n_80: 0.3998 (0.4318)  loss_n_100: 0.4483 (0.4801)  triple_100: 7.8532 (8.0667)  triple_80: 8.0079 (8.1694)  triple_60: 7.1987 (7.2135)  triple_40: 5.0725 (4.8295)  time: 5.8935  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 960/1724]  eta: 1:15:00  lr: 0.000140  loss: 29.8969 (29.9131)  loss_n_40: 0.3304 (0.3373)  loss_n_60: 0.3554 (0.3781)  loss_n_80: 0.4075 (0.4319)  loss_n_100: 0.4483 (0.4801)  triple_100: 7.8881 (8.0660)  triple_80: 8.1297 (8.1701)  triple_60: 7.1987 (7.2158)  triple_40: 5.0846 (4.8339)  time: 5.8927  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [ 970/1724]  eta: 1:14:01  lr: 0.000140  loss: 30.0752 (29.9188)  loss_n_40: 0.3188 (0.3371)  loss_n_60: 0.3572 (0.3780)  loss_n_80: 0.4020 (0.4318)  loss_n_100: 0.4421 (0.4799)  triple_100: 7.9068 (8.0671)  triple_80: 8.1885 (8.1715)  triple_60: 7.3139 (7.2178)  triple_40: 5.0846 (4.8356)  time: 5.8924  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 980/1724]  eta: 1:13:02  lr: 0.000140  loss: 29.8165 (29.9231)  loss_n_40: 0.3099 (0.3371)  loss_n_60: 0.3466 (0.3779)  loss_n_80: 0.4005 (0.4316)  loss_n_100: 0.4414 (0.4796)  triple_100: 7.9045 (8.0665)  triple_80: 8.0254 (8.1717)  triple_60: 7.2914 (7.2196)  triple_40: 5.0084 (4.8391)  time: 5.8915  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [ 990/1724]  eta: 1:12:03  lr: 0.000140  loss: 29.9310 (29.9402)  loss_n_40: 0.3293 (0.3373)  loss_n_60: 0.3689 (0.3782)  loss_n_80: 0.4030 (0.4318)  loss_n_100: 0.4562 (0.4798)  triple_100: 7.9688 (8.0703)  triple_80: 8.0254 (8.1757)  triple_60: 7.3666 (7.2246)  triple_40: 5.0785 (4.8426)  time: 5.8913  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1000/1724]  eta: 1:11:04  lr: 0.000140  loss: 30.8813 (29.9507)  loss_n_40: 0.3391 (0.3375)  loss_n_60: 0.3751 (0.3783)  loss_n_80: 0.4221 (0.4319)  loss_n_100: 0.4588 (0.4798)  triple_100: 8.3092 (8.0727)  triple_80: 8.3503 (8.1783)  triple_60: 7.4517 (7.2268)  triple_40: 5.0992 (4.8454)  time: 5.8907  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1010/1724]  eta: 1:10:05  lr: 0.000140  loss: 29.9359 (29.9444)  loss_n_40: 0.3206 (0.3373)  loss_n_60: 0.3652 (0.3781)  loss_n_80: 0.4221 (0.4317)  loss_n_100: 0.4570 (0.4795)  triple_100: 8.0078 (8.0704)  triple_80: 8.1288 (8.1763)  triple_60: 7.2504 (7.2256)  triple_40: 4.9900 (4.8456)  time: 5.8905  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1020/1724]  eta: 1:09:06  lr: 0.000140  loss: 29.5829 (29.9487)  loss_n_40: 0.3133 (0.3372)  loss_n_60: 0.3473 (0.3780)  loss_n_80: 0.4045 (0.4315)  loss_n_100: 0.4550 (0.4793)  triple_100: 7.9689 (8.0710)  triple_80: 8.1288 (8.1772)  triple_60: 7.1883 (7.2270)  triple_40: 4.9267 (4.8475)  time: 5.8909  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1030/1724]  eta: 1:08:07  lr: 0.000140  loss: 28.7592 (29.9356)  loss_n_40: 0.2889 (0.3369)  loss_n_60: 0.3426 (0.3776)  loss_n_80: 0.3762 (0.4310)  loss_n_100: 0.4182 (0.4787)  triple_100: 7.5932 (8.0670)  triple_80: 7.7720 (8.1730)  triple_60: 6.9648 (7.2244)  triple_40: 4.8543 (4.8469)  time: 5.8905  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1040/1724]  eta: 1:07:08  lr: 0.000140  loss: 28.6757 (29.9353)  loss_n_40: 0.2829 (0.3368)  loss_n_60: 0.3360 (0.3773)  loss_n_80: 0.3762 (0.4307)  loss_n_100: 0.4144 (0.4784)  triple_100: 7.6082 (8.0659)  triple_80: 7.7639 (8.1721)  triple_60: 6.9648 (7.2249)  triple_40: 4.8424 (4.8492)  time: 5.8894  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1050/1724]  eta: 1:06:10  lr: 0.000140  loss: 28.9973 (29.9291)  loss_n_40: 0.2799 (0.3365)  loss_n_60: 0.3155 (0.3770)  loss_n_80: 0.3571 (0.4302)  loss_n_100: 0.3958 (0.4778)  triple_100: 7.6400 (8.0630)  triple_80: 7.7899 (8.1696)  triple_60: 7.0598 (7.2244)  triple_40: 4.8820 (4.8506)  time: 5.8887  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1060/1724]  eta: 1:05:11  lr: 0.000140  loss: 28.4128 (29.9173)  loss_n_40: 0.2811 (0.3363)  loss_n_60: 0.3294 (0.3766)  loss_n_80: 0.3553 (0.4298)  loss_n_100: 0.3990 (0.4772)  triple_100: 7.6004 (8.0585)  triple_80: 7.7641 (8.1654)  triple_60: 6.9185 (7.2220)  triple_40: 4.8820 (4.8515)  time: 5.8881  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [1070/1724]  eta: 1:04:12  lr: 0.000140  loss: 28.7346 (29.9105)  loss_n_40: 0.2939 (0.3361)  loss_n_60: 0.3421 (0.3764)  loss_n_80: 0.3896 (0.4295)  loss_n_100: 0.4347 (0.4769)  triple_100: 7.7125 (8.0567)  triple_80: 7.8303 (8.1635)  triple_60: 6.9755 (7.2206)  triple_40: 4.7307 (4.8508)  time: 5.8870  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1080/1724]  eta: 1:03:13  lr: 0.000140  loss: 30.2627 (29.9187)  loss_n_40: 0.3211 (0.3362)  loss_n_60: 0.3663 (0.3765)  loss_n_80: 0.4205 (0.4296)  loss_n_100: 0.4648 (0.4771)  triple_100: 8.2163 (8.0594)  triple_80: 8.2201 (8.1659)  triple_60: 7.2964 (7.2223)  triple_40: 4.7830 (4.8517)  time: 5.8876  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1090/1724]  eta: 1:02:14  lr: 0.000140  loss: 30.3970 (29.9189)  loss_n_40: 0.3211 (0.3360)  loss_n_60: 0.3690 (0.3763)  loss_n_80: 0.4271 (0.4294)  loss_n_100: 0.4781 (0.4769)  triple_100: 8.2434 (8.0593)  triple_80: 8.3080 (8.1655)  triple_60: 7.3046 (7.2226)  triple_40: 4.9853 (4.8528)  time: 5.8875  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1100/1724]  eta: 1:01:15  lr: 0.000140  loss: 29.5475 (29.9141)  loss_n_40: 0.3198 (0.3358)  loss_n_60: 0.3567 (0.3762)  loss_n_80: 0.4090 (0.4293)  loss_n_100: 0.4629 (0.4767)  triple_100: 7.9713 (8.0581)  triple_80: 8.0367 (8.1642)  triple_60: 7.0933 (7.2215)  triple_40: 4.9281 (4.8523)  time: 5.8866  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1110/1724]  eta: 1:00:16  lr: 0.000140  loss: 29.5618 (29.9225)  loss_n_40: 0.3224 (0.3358)  loss_n_60: 0.3625 (0.3762)  loss_n_80: 0.4151 (0.4293)  loss_n_100: 0.4637 (0.4768)  triple_100: 8.0544 (8.0607)  triple_80: 8.1189 (8.1666)  triple_60: 7.2710 (7.2233)  triple_40: 4.8475 (4.8538)  time: 5.8878  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1120/1724]  eta: 0:59:17  lr: 0.000140  loss: 30.7328 (29.9290)  loss_n_40: 0.3344 (0.3359)  loss_n_60: 0.3813 (0.3762)  loss_n_80: 0.4391 (0.4294)  loss_n_100: 0.4788 (0.4769)  triple_100: 8.2026 (8.0626)  triple_80: 8.4057 (8.1680)  triple_60: 7.3076 (7.2248)  triple_40: 4.9695 (4.8553)  time: 5.8886  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1130/1724]  eta: 0:58:18  lr: 0.000140  loss: 29.9541 (29.9222)  loss_n_40: 0.3183 (0.3358)  loss_n_60: 0.3572 (0.3760)  loss_n_80: 0.4169 (0.4291)  loss_n_100: 0.4550 (0.4766)  triple_100: 8.0605 (8.0604)  triple_80: 8.0641 (8.1657)  triple_60: 7.1576 (7.2231)  triple_40: 4.8366 (4.8557)  time: 5.8892  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1140/1724]  eta: 0:57:19  lr: 0.000140  loss: 29.1166 (29.9229)  loss_n_40: 0.3152 (0.3358)  loss_n_60: 0.3445 (0.3760)  loss_n_80: 0.3970 (0.4291)  loss_n_100: 0.4355 (0.4765)  triple_100: 7.7429 (8.0602)  triple_80: 7.8809 (8.1655)  triple_60: 7.0515 (7.2232)  triple_40: 4.8275 (4.8564)  time: 5.8895  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1150/1724]  eta: 0:56:20  lr: 0.000140  loss: 29.1166 (29.9206)  loss_n_40: 0.3086 (0.3356)  loss_n_60: 0.3337 (0.3758)  loss_n_80: 0.3774 (0.4288)  loss_n_100: 0.4206 (0.4762)  triple_100: 7.7771 (8.0595)  triple_80: 7.8387 (8.1646)  triple_60: 7.0497 (7.2226)  triple_40: 4.9008 (4.8575)  time: 5.8888  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1160/1724]  eta: 0:55:22  lr: 0.000140  loss: 28.8455 (29.9144)  loss_n_40: 0.3142 (0.3355)  loss_n_60: 0.3333 (0.3755)  loss_n_80: 0.3826 (0.4286)  loss_n_100: 0.4236 (0.4760)  triple_100: 7.7771 (8.0570)  triple_80: 7.7933 (8.1619)  triple_60: 7.0118 (7.2208)  triple_40: 5.0116 (4.8592)  time: 5.8878  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1170/1724]  eta: 0:54:23  lr: 0.000140  loss: 29.5595 (29.9193)  loss_n_40: 0.3265 (0.3360)  loss_n_60: 0.3602 (0.3760)  loss_n_80: 0.4091 (0.4291)  loss_n_100: 0.4510 (0.4767)  triple_100: 7.9935 (8.0592)  triple_80: 8.0178 (8.1637)  triple_60: 6.9597 (7.2206)  triple_40: 4.9946 (4.8580)  time: 5.8881  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1180/1724]  eta: 0:53:24  lr: 0.000140  loss: 29.5595 (29.9126)  loss_n_40: 0.3463 (0.3364)  loss_n_60: 0.4022 (0.3763)  loss_n_80: 0.4652 (0.4295)  loss_n_100: 0.5081 (0.4770)  triple_100: 7.9935 (8.0573)  triple_80: 8.0624 (8.1616)  triple_60: 6.9446 (7.2178)  triple_40: 4.6170 (4.8566)  time: 5.8883  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1190/1724]  eta: 0:52:25  lr: 0.000140  loss: 29.7362 (29.9301)  loss_n_40: 0.3624 (0.3371)  loss_n_60: 0.4208 (0.3770)  loss_n_80: 0.4763 (0.4303)  loss_n_100: 0.5415 (0.4780)  triple_100: 8.3110 (8.0627)  triple_80: 8.3079 (8.1672)  triple_60: 7.1369 (7.2212)  triple_40: 4.7409 (4.8567)  time: 5.8889  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1200/1724]  eta: 0:51:26  lr: 0.000140  loss: 31.7819 (29.9402)  loss_n_40: 0.4018 (0.3378)  loss_n_60: 0.4558 (0.3775)  loss_n_80: 0.5082 (0.4308)  loss_n_100: 0.5621 (0.4785)  triple_100: 8.3153 (8.0650)  triple_80: 8.6160 (8.1698)  triple_60: 7.5881 (7.2232)  triple_40: 4.9197 (4.8575)  time: 5.8899  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1210/1724]  eta: 0:50:27  lr: 0.000140  loss: 30.5344 (29.9542)  loss_n_40: 0.3744 (0.3384)  loss_n_60: 0.4119 (0.3780)  loss_n_80: 0.4707 (0.4313)  loss_n_100: 0.5231 (0.4790)  triple_100: 8.1418 (8.0686)  triple_80: 8.3121 (8.1735)  triple_60: 7.3559 (7.2261)  triple_40: 4.9501 (4.8593)  time: 5.8904  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1220/1724]  eta: 0:49:28  lr: 0.000140  loss: 29.6641 (29.9542)  loss_n_40: 0.3659 (0.3385)  loss_n_60: 0.3915 (0.3780)  loss_n_80: 0.4396 (0.4313)  loss_n_100: 0.4832 (0.4790)  triple_100: 7.9184 (8.0682)  triple_80: 8.0630 (8.1731)  triple_60: 7.1488 (7.2264)  triple_40: 4.8462 (4.8596)  time: 5.8903  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1230/1724]  eta: 0:48:29  lr: 0.000140  loss: 29.3850 (29.9611)  loss_n_40: 0.3389 (0.3387)  loss_n_60: 0.3760 (0.3783)  loss_n_80: 0.4343 (0.4315)  loss_n_100: 0.4753 (0.4793)  triple_100: 7.9169 (8.0706)  triple_80: 8.0042 (8.1753)  triple_60: 7.1130 (7.2276)  triple_40: 4.7889 (4.8597)  time: 5.8895  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1240/1724]  eta: 0:47:30  lr: 0.000140  loss: 29.4278 (29.9553)  loss_n_40: 0.3402 (0.3388)  loss_n_60: 0.3701 (0.3781)  loss_n_80: 0.4189 (0.4313)  loss_n_100: 0.4619 (0.4790)  triple_100: 7.8696 (8.0678)  triple_80: 8.0114 (8.1729)  triple_60: 7.1150 (7.2267)  triple_40: 4.8545 (4.8608)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1250/1724]  eta: 0:46:31  lr: 0.000140  loss: 28.8081 (29.9518)  loss_n_40: 0.3276 (0.3387)  loss_n_60: 0.3564 (0.3780)  loss_n_80: 0.3881 (0.4311)  loss_n_100: 0.4176 (0.4787)  triple_100: 7.6585 (8.0665)  triple_80: 7.7773 (8.1717)  triple_60: 7.0876 (7.2265)  triple_40: 4.8848 (4.8605)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1260/1724]  eta: 0:45:32  lr: 0.000140  loss: 28.4362 (29.9464)  loss_n_40: 0.3018 (0.3384)  loss_n_60: 0.3208 (0.3777)  loss_n_80: 0.3636 (0.4307)  loss_n_100: 0.3966 (0.4783)  triple_100: 7.6425 (8.0645)  triple_80: 7.6992 (8.1694)  triple_60: 6.9803 (7.2254)  triple_40: 4.8777 (4.8620)  time: 5.8898  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1270/1724]  eta: 0:44:34  lr: 0.000140  loss: 29.3610 (29.9529)  loss_n_40: 0.3131 (0.3387)  loss_n_60: 0.3561 (0.3780)  loss_n_80: 0.3946 (0.4310)  loss_n_100: 0.4344 (0.4785)  triple_100: 7.7179 (8.0655)  triple_80: 7.8533 (8.1712)  triple_60: 7.1622 (7.2274)  triple_40: 4.9426 (4.8625)  time: 5.8894  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1280/1724]  eta: 0:43:35  lr: 0.000140  loss: 30.3335 (29.9527)  loss_n_40: 0.3582 (0.3388)  loss_n_60: 0.3886 (0.3780)  loss_n_80: 0.4548 (0.4311)  loss_n_100: 0.5038 (0.4786)  triple_100: 7.9825 (8.0656)  triple_80: 8.2030 (8.1714)  triple_60: 7.2470 (7.2266)  triple_40: 4.9398 (4.8626)  time: 5.8872  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1290/1724]  eta: 0:42:36  lr: 0.000140  loss: 30.6767 (29.9586)  loss_n_40: 0.3467 (0.3389)  loss_n_60: 0.3820 (0.3781)  loss_n_80: 0.4290 (0.4312)  loss_n_100: 0.4750 (0.4787)  triple_100: 8.2660 (8.0673)  triple_80: 8.4116 (8.1730)  triple_60: 7.2677 (7.2279)  triple_40: 5.0212 (4.8636)  time: 5.8862  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1300/1724]  eta: 0:41:37  lr: 0.000140  loss: 30.6767 (29.9613)  loss_n_40: 0.3213 (0.3388)  loss_n_60: 0.3700 (0.3781)  loss_n_80: 0.4288 (0.4312)  loss_n_100: 0.4750 (0.4786)  triple_100: 8.3671 (8.0683)  triple_80: 8.4790 (8.1738)  triple_60: 7.2876 (7.2287)  triple_40: 4.9836 (4.8637)  time: 5.8873  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [1310/1724]  eta: 0:40:38  lr: 0.000140  loss: 28.5583 (29.9566)  loss_n_40: 0.3114 (0.3386)  loss_n_60: 0.3425 (0.3780)  loss_n_80: 0.3997 (0.4312)  loss_n_100: 0.4633 (0.4787)  triple_100: 7.8136 (8.0684)  triple_80: 7.8628 (8.1737)  triple_60: 7.0046 (7.2277)  triple_40: 4.6387 (4.8603)  time: 5.8880  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1320/1724]  eta: 0:39:39  lr: 0.000140  loss: 28.5583 (29.9646)  loss_n_40: 0.3114 (0.3387)  loss_n_60: 0.3611 (0.3782)  loss_n_80: 0.3997 (0.4314)  loss_n_100: 0.4633 (0.4789)  triple_100: 7.8622 (8.0706)  triple_80: 7.8713 (8.1758)  triple_60: 7.0046 (7.2299)  triple_40: 4.6081 (4.8612)  time: 5.8875  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1330/1724]  eta: 0:38:40  lr: 0.000140  loss: 29.9020 (29.9751)  loss_n_40: 0.3296 (0.3389)  loss_n_60: 0.3788 (0.3784)  loss_n_80: 0.4292 (0.4316)  loss_n_100: 0.4699 (0.4791)  triple_100: 8.1306 (8.0734)  triple_80: 8.1564 (8.1788)  triple_60: 7.1761 (7.2324)  triple_40: 4.9225 (4.8625)  time: 5.8867  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1340/1724]  eta: 0:37:41  lr: 0.000140  loss: 29.6493 (29.9739)  loss_n_40: 0.3268 (0.3387)  loss_n_60: 0.3697 (0.3783)  loss_n_80: 0.4279 (0.4315)  loss_n_100: 0.4668 (0.4790)  triple_100: 7.9112 (8.0728)  triple_80: 8.0490 (8.1781)  triple_60: 7.1761 (7.2322)  triple_40: 4.8799 (4.8633)  time: 5.8863  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1350/1724]  eta: 0:36:42  lr: 0.000140  loss: 30.0956 (29.9832)  loss_n_40: 0.3315 (0.3389)  loss_n_60: 0.3696 (0.3785)  loss_n_80: 0.4279 (0.4318)  loss_n_100: 0.4698 (0.4793)  triple_100: 8.2563 (8.0765)  triple_80: 8.3153 (8.1814)  triple_60: 7.2045 (7.2339)  triple_40: 4.8376 (4.8629)  time: 5.8865  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1360/1724]  eta: 0:35:43  lr: 0.000140  loss: 31.1062 (29.9918)  loss_n_40: 0.3428 (0.3389)  loss_n_60: 0.4036 (0.3787)  loss_n_80: 0.4711 (0.4321)  loss_n_100: 0.5378 (0.4798)  triple_100: 8.6721 (8.0805)  triple_80: 8.6266 (8.1848)  triple_60: 7.5393 (7.2355)  triple_40: 4.7652 (4.8614)  time: 5.8876  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1370/1724]  eta: 0:34:44  lr: 0.000140  loss: 30.4477 (29.9924)  loss_n_40: 0.3257 (0.3388)  loss_n_60: 0.3999 (0.3787)  loss_n_80: 0.4619 (0.4322)  loss_n_100: 0.5362 (0.4800)  triple_100: 8.4031 (8.0826)  triple_80: 8.5223 (8.1861)  triple_60: 7.2637 (7.2352)  triple_40: 4.5025 (4.8589)  time: 5.8885  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1380/1724]  eta: 0:33:46  lr: 0.000140  loss: 30.2031 (29.9920)  loss_n_40: 0.3210 (0.3387)  loss_n_60: 0.3766 (0.3787)  loss_n_80: 0.4456 (0.4322)  loss_n_100: 0.5076 (0.4800)  triple_100: 8.3305 (8.0832)  triple_80: 8.3488 (8.1866)  triple_60: 7.1940 (7.2351)  triple_40: 4.5347 (4.8574)  time: 5.8877  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1390/1724]  eta: 0:32:47  lr: 0.000140  loss: 29.4862 (29.9911)  loss_n_40: 0.3217 (0.3389)  loss_n_60: 0.3659 (0.3788)  loss_n_80: 0.4298 (0.4323)  loss_n_100: 0.4856 (0.4800)  triple_100: 8.2583 (8.0829)  triple_80: 8.2390 (8.1863)  triple_60: 7.1287 (7.2348)  triple_40: 4.6642 (4.8572)  time: 5.8875  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1400/1724]  eta: 0:31:48  lr: 0.000140  loss: 30.0463 (29.9918)  loss_n_40: 0.3395 (0.3388)  loss_n_60: 0.3625 (0.3788)  loss_n_80: 0.4247 (0.4324)  loss_n_100: 0.4792 (0.4802)  triple_100: 8.0773 (8.0843)  triple_80: 8.2162 (8.1873)  triple_60: 7.1188 (7.2345)  triple_40: 4.6891 (4.8556)  time: 5.8876  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1410/1724]  eta: 0:30:49  lr: 0.000140  loss: 30.0463 (29.9876)  loss_n_40: 0.3395 (0.3388)  loss_n_60: 0.3913 (0.3789)  loss_n_80: 0.4515 (0.4325)  loss_n_100: 0.5037 (0.4803)  triple_100: 8.1910 (8.0838)  triple_80: 8.2499 (8.1863)  triple_60: 7.1965 (7.2335)  triple_40: 4.6310 (4.8536)  time: 5.8874  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1420/1724]  eta: 0:29:50  lr: 0.000140  loss: 29.8739 (29.9879)  loss_n_40: 0.3408 (0.3387)  loss_n_60: 0.3834 (0.3789)  loss_n_80: 0.4493 (0.4325)  loss_n_100: 0.4999 (0.4804)  triple_100: 8.1676 (8.0844)  triple_80: 8.2018 (8.1866)  triple_60: 7.1580 (7.2334)  triple_40: 4.6310 (4.8530)  time: 5.8875  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1430/1724]  eta: 0:28:51  lr: 0.000140  loss: 29.1933 (29.9800)  loss_n_40: 0.3222 (0.3387)  loss_n_60: 0.3697 (0.3788)  loss_n_80: 0.4235 (0.4324)  loss_n_100: 0.4705 (0.4802)  triple_100: 8.0661 (8.0820)  triple_80: 8.0984 (8.1844)  triple_60: 7.1580 (7.2320)  triple_40: 4.6484 (4.8516)  time: 5.8864  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1440/1724]  eta: 0:27:52  lr: 0.000140  loss: 28.9630 (29.9745)  loss_n_40: 0.3166 (0.3384)  loss_n_60: 0.3598 (0.3786)  loss_n_80: 0.4006 (0.4322)  loss_n_100: 0.4556 (0.4800)  triple_100: 7.8656 (8.0812)  triple_80: 7.9226 (8.1833)  triple_60: 7.0025 (7.2306)  triple_40: 4.6411 (4.8501)  time: 5.8867  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1450/1724]  eta: 0:26:53  lr: 0.000140  loss: 29.2595 (29.9760)  loss_n_40: 0.3224 (0.3385)  loss_n_60: 0.3598 (0.3787)  loss_n_80: 0.4121 (0.4322)  loss_n_100: 0.4556 (0.4801)  triple_100: 7.9535 (8.0812)  triple_80: 7.9866 (8.1836)  triple_60: 7.0955 (7.2312)  triple_40: 4.7529 (4.8505)  time: 5.8896  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1460/1724]  eta: 0:25:54  lr: 0.000140  loss: 28.7465 (29.9642)  loss_n_40: 0.2900 (0.3381)  loss_n_60: 0.3276 (0.3783)  loss_n_80: 0.3831 (0.4318)  loss_n_100: 0.4302 (0.4797)  triple_100: 7.8263 (8.0787)  triple_80: 7.8936 (8.1806)  triple_60: 6.9401 (7.2283)  triple_40: 4.7061 (4.8487)  time: 5.8897  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1470/1724]  eta: 0:24:55  lr: 0.000140  loss: 28.6838 (29.9573)  loss_n_40: 0.2708 (0.3377)  loss_n_60: 0.3240 (0.3780)  loss_n_80: 0.3831 (0.4316)  loss_n_100: 0.4339 (0.4794)  triple_100: 7.8967 (8.0782)  triple_80: 7.8936 (8.1792)  triple_60: 6.8766 (7.2265)  triple_40: 4.5921 (4.8468)  time: 5.8886  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1480/1724]  eta: 0:23:57  lr: 0.000140  loss: 28.7183 (29.9478)  loss_n_40: 0.2786 (0.3374)  loss_n_60: 0.3291 (0.3776)  loss_n_80: 0.3807 (0.4312)  loss_n_100: 0.4307 (0.4790)  triple_100: 7.8745 (8.0756)  triple_80: 7.8972 (8.1763)  triple_60: 6.9438 (7.2242)  triple_40: 4.6645 (4.8464)  time: 5.8888  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1490/1724]  eta: 0:22:58  lr: 0.000140  loss: 28.3674 (29.9384)  loss_n_40: 0.2856 (0.3372)  loss_n_60: 0.3297 (0.3774)  loss_n_80: 0.3807 (0.4310)  loss_n_100: 0.4227 (0.4788)  triple_100: 7.6579 (8.0731)  triple_80: 7.7401 (8.1738)  triple_60: 6.8586 (7.2221)  triple_40: 4.6729 (4.8451)  time: 5.8887  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1500/1724]  eta: 0:21:59  lr: 0.000140  loss: 28.4149 (29.9332)  loss_n_40: 0.3075 (0.3371)  loss_n_60: 0.3474 (0.3773)  loss_n_80: 0.3912 (0.4309)  loss_n_100: 0.4313 (0.4787)  triple_100: 7.6779 (8.0715)  triple_80: 7.7784 (8.1722)  triple_60: 6.8586 (7.2211)  triple_40: 4.6591 (4.8446)  time: 5.8892  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1510/1724]  eta: 0:21:00  lr: 0.000140  loss: 28.4025 (29.9251)  loss_n_40: 0.3056 (0.3369)  loss_n_60: 0.3429 (0.3772)  loss_n_80: 0.3921 (0.4307)  loss_n_100: 0.4272 (0.4784)  triple_100: 7.6779 (8.0690)  triple_80: 7.7784 (8.1699)  triple_60: 6.8705 (7.2194)  triple_40: 4.6591 (4.8437)  time: 5.8888  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1520/1724]  eta: 0:20:01  lr: 0.000140  loss: 28.4025 (29.9302)  loss_n_40: 0.3120 (0.3370)  loss_n_60: 0.3533 (0.3773)  loss_n_80: 0.4046 (0.4307)  loss_n_100: 0.4474 (0.4785)  triple_100: 7.7116 (8.0697)  triple_80: 7.8156 (8.1708)  triple_60: 6.9534 (7.2208)  triple_40: 4.7650 (4.8453)  time: 5.8880  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1530/1724]  eta: 0:19:02  lr: 0.000140  loss: 29.5361 (29.9269)  loss_n_40: 0.3049 (0.3368)  loss_n_60: 0.3512 (0.3771)  loss_n_80: 0.4100 (0.4306)  loss_n_100: 0.4622 (0.4783)  triple_100: 8.0218 (8.0694)  triple_80: 8.0474 (8.1701)  triple_60: 7.1351 (7.2198)  triple_40: 4.8913 (4.8448)  time: 5.8878  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1540/1724]  eta: 0:18:03  lr: 0.000140  loss: 29.5361 (29.9291)  loss_n_40: 0.3049 (0.3368)  loss_n_60: 0.3512 (0.3771)  loss_n_80: 0.4096 (0.4305)  loss_n_100: 0.4585 (0.4783)  triple_100: 8.0094 (8.0695)  triple_80: 8.1506 (8.1705)  triple_60: 7.1351 (7.2207)  triple_40: 4.8664 (4.8456)  time: 5.8878  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:8]  [1550/1724]  eta: 0:17:04  lr: 0.000140  loss: 29.9284 (29.9271)  loss_n_40: 0.3163 (0.3366)  loss_n_60: 0.3632 (0.3770)  loss_n_80: 0.4096 (0.4304)  loss_n_100: 0.4538 (0.4781)  triple_100: 8.0288 (8.0689)  triple_80: 8.1974 (8.1699)  triple_60: 7.2648 (7.2205)  triple_40: 4.8827 (4.8458)  time: 5.8887  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1560/1724]  eta: 0:16:05  lr: 0.000140  loss: 29.4161 (29.9207)  loss_n_40: 0.3163 (0.3364)  loss_n_60: 0.3638 (0.3769)  loss_n_80: 0.4080 (0.4303)  loss_n_100: 0.4538 (0.4781)  triple_100: 8.0162 (8.0676)  triple_80: 8.1048 (8.1684)  triple_60: 7.0515 (7.2189)  triple_40: 4.6915 (4.8441)  time: 5.8881  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1570/1724]  eta: 0:15:06  lr: 0.000140  loss: 28.6932 (29.9201)  loss_n_40: 0.3174 (0.3364)  loss_n_60: 0.3659 (0.3769)  loss_n_80: 0.4197 (0.4304)  loss_n_100: 0.4752 (0.4781)  triple_100: 7.9020 (8.0673)  triple_80: 7.9070 (8.1689)  triple_60: 6.9624 (7.2185)  triple_40: 4.6035 (4.8437)  time: 5.8870  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1580/1724]  eta: 0:14:08  lr: 0.000140  loss: 29.1916 (29.9194)  loss_n_40: 0.3371 (0.3364)  loss_n_60: 0.3803 (0.3769)  loss_n_80: 0.4218 (0.4304)  loss_n_100: 0.4618 (0.4781)  triple_100: 7.9020 (8.0671)  triple_80: 7.9470 (8.1685)  triple_60: 6.9624 (7.2183)  triple_40: 4.6606 (4.8438)  time: 5.8878  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1590/1724]  eta: 0:13:09  lr: 0.000140  loss: 28.9264 (29.9150)  loss_n_40: 0.3081 (0.3363)  loss_n_60: 0.3526 (0.3768)  loss_n_80: 0.4065 (0.4303)  loss_n_100: 0.4451 (0.4780)  triple_100: 7.7226 (8.0654)  triple_80: 7.8517 (8.1671)  triple_60: 7.0216 (7.2174)  triple_40: 4.7533 (4.8437)  time: 5.8892  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1600/1724]  eta: 0:12:10  lr: 0.000140  loss: 28.7063 (29.9107)  loss_n_40: 0.3040 (0.3364)  loss_n_60: 0.3508 (0.3769)  loss_n_80: 0.3947 (0.4304)  loss_n_100: 0.4427 (0.4780)  triple_100: 7.6304 (8.0639)  triple_80: 7.7982 (8.1658)  triple_60: 6.9854 (7.2163)  triple_40: 4.6485 (4.8430)  time: 5.8897  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1610/1724]  eta: 0:11:11  lr: 0.000140  loss: 27.9483 (29.9029)  loss_n_40: 0.3323 (0.3363)  loss_n_60: 0.3682 (0.3768)  loss_n_80: 0.4138 (0.4303)  loss_n_100: 0.4605 (0.4780)  triple_100: 7.6058 (8.0618)  triple_80: 7.6090 (8.1637)  triple_60: 6.8114 (7.2143)  triple_40: 4.5716 (4.8417)  time: 5.8883  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1620/1724]  eta: 0:10:12  lr: 0.000140  loss: 27.6814 (29.8934)  loss_n_40: 0.3183 (0.3361)  loss_n_60: 0.3487 (0.3767)  loss_n_80: 0.3972 (0.4302)  loss_n_100: 0.4500 (0.4778)  triple_100: 7.4524 (8.0593)  triple_80: 7.6090 (8.1614)  triple_60: 6.6600 (7.2121)  triple_40: 4.4873 (4.8398)  time: 5.8877  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1630/1724]  eta: 0:09:13  lr: 0.000140  loss: 27.6468 (29.8867)  loss_n_40: 0.3047 (0.3360)  loss_n_60: 0.3446 (0.3766)  loss_n_80: 0.3886 (0.4301)  loss_n_100: 0.4365 (0.4778)  triple_100: 7.4082 (8.0577)  triple_80: 7.5430 (8.1596)  triple_60: 6.6748 (7.2104)  triple_40: 4.5694 (4.8385)  time: 5.8886  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1640/1724]  eta: 0:08:14  lr: 0.000140  loss: 27.7011 (29.8731)  loss_n_40: 0.3004 (0.3362)  loss_n_60: 0.3351 (0.3767)  loss_n_80: 0.3898 (0.4302)  loss_n_100: 0.4372 (0.4779)  triple_100: 7.4082 (8.0539)  triple_80: 7.4815 (8.1556)  triple_60: 6.6433 (7.2067)  triple_40: 4.4907 (4.8360)  time: 5.8893  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1650/1724]  eta: 0:07:15  lr: 0.000140  loss: 28.6703 (29.8821)  loss_n_40: 0.3732 (0.3373)  loss_n_60: 0.4664 (0.3783)  loss_n_80: 0.5706 (0.4318)  loss_n_100: 0.5701 (0.4797)  triple_100: 7.8611 (8.0568)  triple_80: 7.7199 (8.1574)  triple_60: 6.8867 (7.2070)  triple_40: 4.3322 (4.8338)  time: 5.8903  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1660/1724]  eta: 0:06:16  lr: 0.000140  loss: 31.8058 (29.8940)  loss_n_40: 0.4634 (0.3381)  loss_n_60: 0.5456 (0.3792)  loss_n_80: 0.6413 (0.4330)  loss_n_100: 0.7237 (0.4810)  triple_100: 8.7841 (8.0613)  triple_80: 8.7293 (8.1611)  triple_60: 7.2737 (7.2081)  triple_40: 4.4175 (4.8322)  time: 5.8913  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1670/1724]  eta: 0:05:18  lr: 0.000140  loss: 30.4953 (29.8905)  loss_n_40: 0.4403 (0.3386)  loss_n_60: 0.4952 (0.3796)  loss_n_80: 0.5621 (0.4335)  loss_n_100: 0.6321 (0.4816)  triple_100: 8.3672 (8.0614)  triple_80: 8.3419 (8.1604)  triple_60: 6.9849 (7.2058)  triple_40: 4.4175 (4.8295)  time: 5.8905  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1680/1724]  eta: 0:04:19  lr: 0.000140  loss: 28.8727 (29.8880)  loss_n_40: 0.3520 (0.3387)  loss_n_60: 0.4158 (0.3799)  loss_n_80: 0.4884 (0.4338)  loss_n_100: 0.5514 (0.4820)  triple_100: 8.0345 (8.0623)  triple_80: 7.9523 (8.1602)  triple_60: 6.8295 (7.2043)  triple_40: 4.3424 (4.8268)  time: 5.8895  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1690/1724]  eta: 0:03:20  lr: 0.000140  loss: 28.9478 (29.9026)  loss_n_40: 0.3499 (0.3390)  loss_n_60: 0.4085 (0.3804)  loss_n_80: 0.4852 (0.4346)  loss_n_100: 0.5634 (0.4830)  triple_100: 8.2644 (8.0683)  triple_80: 8.1489 (8.1656)  triple_60: 6.9261 (7.2064)  triple_40: 4.4346 (4.8253)  time: 5.8904  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1700/1724]  eta: 0:02:21  lr: 0.000140  loss: 29.6640 (29.9001)  loss_n_40: 0.3499 (0.3390)  loss_n_60: 0.4070 (0.3805)  loss_n_80: 0.4802 (0.4348)  loss_n_100: 0.5525 (0.4833)  triple_100: 8.3525 (8.0695)  triple_80: 8.2616 (8.1658)  triple_60: 6.9404 (7.2049)  triple_40: 4.2981 (4.8223)  time: 5.8906  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:8]  [1710/1724]  eta: 0:01:22  lr: 0.000140  loss: 28.8816 (29.8963)  loss_n_40: 0.3311 (0.3393)  loss_n_60: 0.3956 (0.3807)  loss_n_80: 0.4664 (0.4350)  loss_n_100: 0.5347 (0.4835)  triple_100: 8.1731 (8.0693)  triple_80: 7.9875 (8.1654)  triple_60: 6.8257 (7.2034)  triple_40: 4.3035 (4.8197)  time: 5.8903  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1720/1724]  eta: 0:00:23  lr: 0.000140  loss: 29.1582 (29.8971)  loss_n_40: 0.2955 (0.3390)  loss_n_60: 0.3552 (0.3806)  loss_n_80: 0.4288 (0.4350)  loss_n_100: 0.4921 (0.4836)  triple_100: 8.1855 (8.0712)  triple_80: 8.1445 (8.1666)  triple_60: 6.8802 (7.2032)  triple_40: 4.3767 (4.8178)  time: 5.8883  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8]  [1723/1724]  eta: 0:00:05  lr: 0.000140  loss: 29.3716 (29.8992)  loss_n_40: 0.2955 (0.3391)  loss_n_60: 0.3601 (0.3808)  loss_n_80: 0.4407 (0.4355)  loss_n_100: 0.5145 (0.4843)  triple_100: 8.2716 (8.0730)  triple_80: 8.1945 (8.1679)  triple_60: 6.8802 (7.2024)  triple_40: 4.3767 (4.8162)  time: 5.8876  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:8] Total time: 2:49:14 (5.8898 s / it)\n",
      "Averaged stats: lr: 0.000140  loss: 29.3716 (29.8992)  loss_n_40: 0.2955 (0.3391)  loss_n_60: 0.3601 (0.3808)  loss_n_80: 0.4407 (0.4355)  loss_n_100: 0.5145 (0.4843)  triple_100: 8.2716 (8.0730)  triple_80: 8.1945 (8.1679)  triple_60: 6.8802 (7.2024)  triple_40: 4.3767 (4.8162)\n",
      "Valid: [epoch:8]  [  0/845]  eta: 0:21:13  loss: 37.1142 (37.1142)  loss_n_40: 0.3623 (0.3623)  loss_n_60: 0.5591 (0.5591)  loss_n_80: 0.7219 (0.7219)  loss_n_100: 0.8456 (0.8456)  triple_100: 11.3671 (11.3671)  triple_80: 11.1540 (11.1540)  triple_60: 8.5223 (8.5223)  triple_40: 3.5819 (3.5819)  time: 1.5069  data: 0.5310  max mem: 40153\n",
      "Valid: [epoch:8]  [ 10/845]  eta: 0:14:13  loss: 31.0988 (32.4757)  loss_n_40: 0.3173 (0.4048)  loss_n_60: 0.5107 (0.5102)  loss_n_80: 0.6822 (0.6460)  loss_n_100: 0.8128 (0.7532)  triple_100: 9.9038 (9.4939)  triple_80: 9.7109 (9.3693)  triple_60: 6.9566 (7.3235)  triple_40: 3.7544 (3.9747)  time: 1.0222  data: 0.0484  max mem: 40153\n",
      "Valid: [epoch:8]  [ 20/845]  eta: 0:13:44  loss: 32.0191 (33.7289)  loss_n_40: 0.3437 (0.3911)  loss_n_60: 0.5107 (0.5319)  loss_n_80: 0.6822 (0.6820)  loss_n_100: 0.7976 (0.7997)  triple_100: 9.9038 (10.0583)  triple_80: 9.7109 (9.8773)  triple_60: 7.2964 (7.6182)  triple_40: 3.5426 (3.7704)  time: 0.9741  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [ 30/845]  eta: 0:13:28  loss: 34.2144 (34.0529)  loss_n_40: 0.3705 (0.4065)  loss_n_60: 0.5538 (0.5474)  loss_n_80: 0.7043 (0.6966)  loss_n_100: 0.8041 (0.8153)  triple_100: 10.5340 (10.1346)  triple_80: 10.2271 (9.9579)  triple_60: 7.9053 (7.6815)  triple_40: 3.5295 (3.8131)  time: 0.9746  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:8]  [ 40/845]  eta: 0:13:14  loss: 31.9439 (34.0019)  loss_n_40: 0.3422 (0.4071)  loss_n_60: 0.5065 (0.5417)  loss_n_80: 0.6409 (0.6875)  loss_n_100: 0.7605 (0.7995)  triple_100: 9.4418 (10.0789)  triple_80: 9.3201 (9.9337)  triple_60: 7.3865 (7.6892)  triple_40: 3.6396 (3.8642)  time: 0.9748  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [ 50/845]  eta: 0:13:03  loss: 31.6047 (33.8147)  loss_n_40: 0.3315 (0.4049)  loss_n_60: 0.4617 (0.5341)  loss_n_80: 0.6022 (0.6747)  loss_n_100: 0.7005 (0.7837)  triple_100: 9.3693 (10.0036)  triple_80: 9.2316 (9.8633)  triple_60: 7.1527 (7.6678)  triple_40: 3.7386 (3.8827)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [ 60/845]  eta: 0:12:51  loss: 31.5815 (34.2445)  loss_n_40: 0.3253 (0.4186)  loss_n_60: 0.5120 (0.5519)  loss_n_80: 0.6968 (0.6969)  loss_n_100: 0.8417 (0.8071)  triple_100: 9.6962 (10.1263)  triple_80: 9.4365 (10.0002)  triple_60: 7.2599 (7.7357)  triple_40: 3.6485 (3.9078)  time: 0.9749  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [ 70/845]  eta: 0:12:41  loss: 31.5815 (34.0675)  loss_n_40: 0.3526 (0.4169)  loss_n_60: 0.5120 (0.5515)  loss_n_80: 0.6968 (0.6956)  loss_n_100: 0.8419 (0.8053)  triple_100: 9.6962 (10.0517)  triple_80: 9.4365 (9.9316)  triple_60: 7.2599 (7.7000)  triple_40: 3.6836 (3.9148)  time: 0.9750  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [ 80/845]  eta: 0:12:30  loss: 32.0401 (34.0160)  loss_n_40: 0.3583 (0.4234)  loss_n_60: 0.5254 (0.5542)  loss_n_80: 0.6743 (0.6946)  loss_n_100: 0.7953 (0.8022)  triple_100: 9.2199 (10.0168)  triple_80: 9.1875 (9.9006)  triple_60: 7.3974 (7.7019)  triple_40: 3.7273 (3.9222)  time: 0.9749  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [ 90/845]  eta: 0:12:20  loss: 30.8812 (33.9062)  loss_n_40: 0.3105 (0.4200)  loss_n_60: 0.4293 (0.5473)  loss_n_80: 0.5668 (0.6863)  loss_n_100: 0.6456 (0.7921)  triple_100: 9.0752 (9.9680)  triple_80: 9.0961 (9.8577)  triple_60: 7.1180 (7.6796)  triple_40: 3.7479 (3.9551)  time: 0.9748  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [100/845]  eta: 0:12:10  loss: 30.7976 (33.6960)  loss_n_40: 0.3392 (0.4322)  loss_n_60: 0.5004 (0.5522)  loss_n_80: 0.6066 (0.6865)  loss_n_100: 0.6785 (0.7898)  triple_100: 9.0587 (9.8537)  triple_80: 8.9682 (9.7658)  triple_60: 6.7873 (7.6330)  triple_40: 3.8024 (3.9827)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [110/845]  eta: 0:11:59  loss: 32.3047 (33.7947)  loss_n_40: 0.3425 (0.4296)  loss_n_60: 0.5004 (0.5495)  loss_n_80: 0.6238 (0.6835)  loss_n_100: 0.7163 (0.7869)  triple_100: 9.3572 (9.8876)  triple_80: 9.6263 (9.7935)  triple_60: 7.0463 (7.6600)  triple_40: 3.7188 (4.0041)  time: 0.9749  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [120/845]  eta: 0:11:49  loss: 34.2463 (34.0710)  loss_n_40: 0.3670 (0.4279)  loss_n_60: 0.5290 (0.5531)  loss_n_80: 0.6830 (0.6907)  loss_n_100: 0.7903 (0.7956)  triple_100: 10.2944 (10.0057)  triple_80: 10.1139 (9.9015)  triple_60: 7.7964 (7.7159)  triple_40: 3.6375 (3.9806)  time: 0.9751  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [130/845]  eta: 0:11:39  loss: 35.4407 (34.1426)  loss_n_40: 0.3808 (0.4232)  loss_n_60: 0.5636 (0.5520)  loss_n_80: 0.7328 (0.6914)  loss_n_100: 0.8565 (0.7975)  triple_100: 10.6698 (10.0539)  triple_80: 10.5121 (9.9411)  triple_60: 8.1878 (7.7306)  triple_40: 3.6028 (3.9528)  time: 0.9754  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [140/845]  eta: 0:11:29  loss: 31.4089 (33.9579)  loss_n_40: 0.3158 (0.4238)  loss_n_60: 0.4434 (0.5483)  loss_n_80: 0.5836 (0.6847)  loss_n_100: 0.7096 (0.7891)  triple_100: 9.4287 (9.9784)  triple_80: 9.1975 (9.8729)  triple_60: 7.1332 (7.6969)  triple_40: 3.6392 (3.9638)  time: 0.9753  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [150/845]  eta: 0:11:19  loss: 30.5268 (33.8752)  loss_n_40: 0.3017 (0.4198)  loss_n_60: 0.4232 (0.5440)  loss_n_80: 0.5711 (0.6799)  loss_n_100: 0.6630 (0.7849)  triple_100: 9.0352 (9.9588)  triple_80: 8.8944 (9.8468)  triple_60: 6.9846 (7.6807)  triple_40: 3.7231 (3.9603)  time: 0.9750  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [160/845]  eta: 0:11:10  loss: 30.7339 (33.8892)  loss_n_40: 0.4001 (0.4279)  loss_n_60: 0.5783 (0.5502)  loss_n_80: 0.7297 (0.6840)  loss_n_100: 0.7540 (0.7876)  triple_100: 9.0352 (9.9351)  triple_80: 8.8944 (9.8356)  triple_60: 7.6601 (7.6901)  triple_40: 3.7681 (3.9787)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [170/845]  eta: 0:11:00  loss: 33.6707 (33.8716)  loss_n_40: 0.4001 (0.4269)  loss_n_60: 0.5783 (0.5492)  loss_n_80: 0.7297 (0.6825)  loss_n_100: 0.7589 (0.7859)  triple_100: 9.5996 (9.9347)  triple_80: 9.4325 (9.8330)  triple_60: 7.8318 (7.6904)  triple_40: 3.7007 (3.9690)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [180/845]  eta: 0:10:50  loss: 31.7227 (33.8549)  loss_n_40: 0.3714 (0.4294)  loss_n_60: 0.5086 (0.5504)  loss_n_80: 0.6538 (0.6830)  loss_n_100: 0.7638 (0.7855)  triple_100: 9.4439 (9.9145)  triple_80: 9.3070 (9.8161)  triple_60: 7.2396 (7.6894)  triple_40: 3.6585 (3.9867)  time: 0.9748  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [190/845]  eta: 0:10:40  loss: 30.4652 (33.7775)  loss_n_40: 0.3703 (0.4281)  loss_n_60: 0.4490 (0.5480)  loss_n_80: 0.5843 (0.6807)  loss_n_100: 0.6927 (0.7827)  triple_100: 8.9398 (9.8927)  triple_80: 8.9691 (9.7958)  triple_60: 6.8189 (7.6698)  triple_40: 3.6876 (3.9798)  time: 0.9748  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [200/845]  eta: 0:10:30  loss: 30.4652 (33.6849)  loss_n_40: 0.3116 (0.4243)  loss_n_60: 0.4276 (0.5433)  loss_n_80: 0.5600 (0.6753)  loss_n_100: 0.6398 (0.7762)  triple_100: 9.0062 (9.8629)  triple_80: 8.8563 (9.7665)  triple_60: 7.0261 (7.6551)  triple_40: 3.8046 (3.9813)  time: 0.9749  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [210/845]  eta: 0:10:20  loss: 32.0215 (33.6925)  loss_n_40: 0.3434 (0.4237)  loss_n_60: 0.4863 (0.5436)  loss_n_80: 0.6238 (0.6762)  loss_n_100: 0.7052 (0.7775)  triple_100: 9.3908 (9.8752)  triple_80: 9.2919 (9.7765)  triple_60: 7.2443 (7.6534)  triple_40: 3.6875 (3.9664)  time: 0.9752  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [220/845]  eta: 0:10:10  loss: 32.3431 (33.6695)  loss_n_40: 0.3735 (0.4238)  loss_n_60: 0.5408 (0.5439)  loss_n_80: 0.7023 (0.6765)  loss_n_100: 0.7718 (0.7775)  triple_100: 9.5584 (9.8683)  triple_80: 9.3651 (9.7702)  triple_60: 7.4214 (7.6476)  triple_40: 3.6063 (3.9617)  time: 0.9751  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [230/845]  eta: 0:10:00  loss: 30.5982 (33.5456)  loss_n_40: 0.3401 (0.4199)  loss_n_60: 0.4428 (0.5401)  loss_n_80: 0.5823 (0.6730)  loss_n_100: 0.6766 (0.7740)  triple_100: 8.9783 (9.8354)  triple_80: 8.8525 (9.7342)  triple_60: 6.8529 (7.6171)  triple_40: 3.7336 (3.9520)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [240/845]  eta: 0:09:51  loss: 30.6598 (33.5823)  loss_n_40: 0.3174 (0.4188)  loss_n_60: 0.4289 (0.5397)  loss_n_80: 0.5644 (0.6732)  loss_n_100: 0.6580 (0.7741)  triple_100: 8.9522 (9.8529)  triple_80: 8.8048 (9.7502)  triple_60: 7.0322 (7.6250)  triple_40: 3.6770 (3.9484)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [250/845]  eta: 0:09:41  loss: 31.9595 (33.5547)  loss_n_40: 0.3284 (0.4156)  loss_n_60: 0.4545 (0.5373)  loss_n_80: 0.5945 (0.6712)  loss_n_100: 0.6899 (0.7724)  triple_100: 9.6263 (9.8533)  triple_80: 9.4229 (9.7485)  triple_60: 7.3247 (7.6180)  triple_40: 3.6574 (3.9385)  time: 0.9748  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [260/845]  eta: 0:09:31  loss: 33.7605 (33.6570)  loss_n_40: 0.3381 (0.4184)  loss_n_60: 0.4949 (0.5390)  loss_n_80: 0.6518 (0.6725)  loss_n_100: 0.7766 (0.7733)  triple_100: 9.6263 (9.8750)  triple_80: 9.8788 (9.7719)  triple_60: 7.4398 (7.6435)  triple_40: 3.7580 (3.9634)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [270/845]  eta: 0:09:21  loss: 35.2917 (33.6675)  loss_n_40: 0.3631 (0.4180)  loss_n_60: 0.5258 (0.5395)  loss_n_80: 0.6833 (0.6729)  loss_n_100: 0.8140 (0.7736)  triple_100: 10.3036 (9.8783)  triple_80: 10.1110 (9.7759)  triple_60: 7.8923 (7.6490)  triple_40: 3.7189 (3.9603)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [280/845]  eta: 0:09:11  loss: 33.0897 (33.6357)  loss_n_40: 0.3401 (0.4160)  loss_n_60: 0.4909 (0.5378)  loss_n_80: 0.6507 (0.6722)  loss_n_100: 0.7661 (0.7734)  triple_100: 9.7952 (9.8781)  triple_80: 9.5747 (9.7718)  triple_60: 7.5059 (7.6356)  triple_40: 3.5890 (3.9507)  time: 0.9747  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:8]  [290/845]  eta: 0:09:02  loss: 34.3275 (33.7531)  loss_n_40: 0.3703 (0.4220)  loss_n_60: 0.5106 (0.5425)  loss_n_80: 0.7106 (0.6762)  loss_n_100: 0.8589 (0.7772)  triple_100: 9.9700 (9.9003)  triple_80: 9.8762 (9.7992)  triple_60: 7.5618 (7.6637)  triple_40: 3.6778 (3.9719)  time: 0.9747  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [300/845]  eta: 0:08:52  loss: 32.5253 (33.6530)  loss_n_40: 0.3703 (0.4219)  loss_n_60: 0.5482 (0.5410)  loss_n_80: 0.7084 (0.6737)  loss_n_100: 0.7772 (0.7738)  triple_100: 9.1049 (9.8632)  triple_80: 8.9591 (9.7657)  triple_60: 7.5463 (7.6438)  triple_40: 3.7025 (3.9700)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [310/845]  eta: 0:08:42  loss: 34.5531 (33.7871)  loss_n_40: 0.4107 (0.4228)  loss_n_60: 0.6121 (0.5439)  loss_n_80: 0.7587 (0.6779)  loss_n_100: 0.8519 (0.7790)  triple_100: 10.1254 (9.9148)  triple_80: 10.0746 (9.8129)  triple_60: 7.7478 (7.6712)  triple_40: 3.5723 (3.9646)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [320/845]  eta: 0:08:32  loss: 35.8692 (33.7968)  loss_n_40: 0.4023 (0.4210)  loss_n_60: 0.6027 (0.5433)  loss_n_80: 0.7851 (0.6781)  loss_n_100: 0.8822 (0.7801)  triple_100: 10.9823 (9.9281)  triple_80: 10.6653 (9.8219)  triple_60: 8.0641 (7.6703)  triple_40: 3.6244 (3.9540)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [330/845]  eta: 0:08:22  loss: 35.0372 (33.8317)  loss_n_40: 0.3834 (0.4243)  loss_n_60: 0.5784 (0.5465)  loss_n_80: 0.7546 (0.6806)  loss_n_100: 0.8425 (0.7823)  triple_100: 10.6430 (9.9323)  triple_80: 10.3670 (9.8278)  triple_60: 7.9447 (7.6804)  triple_40: 3.6586 (3.9576)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [340/845]  eta: 0:08:13  loss: 34.1658 (33.8794)  loss_n_40: 0.4061 (0.4256)  loss_n_60: 0.6167 (0.5483)  loss_n_80: 0.7878 (0.6827)  loss_n_100: 0.8699 (0.7844)  triple_100: 10.5936 (9.9429)  triple_80: 10.3488 (9.8403)  triple_60: 7.9014 (7.6902)  triple_40: 3.6764 (3.9650)  time: 0.9741  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [350/845]  eta: 0:08:03  loss: 33.1969 (33.8780)  loss_n_40: 0.4327 (0.4266)  loss_n_60: 0.6211 (0.5483)  loss_n_80: 0.7878 (0.6825)  loss_n_100: 0.8640 (0.7839)  triple_100: 9.4733 (9.9359)  triple_80: 9.5079 (9.8354)  triple_60: 7.3811 (7.6917)  triple_40: 3.8310 (3.9737)  time: 0.9740  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [360/845]  eta: 0:07:53  loss: 33.2484 (33.9345)  loss_n_40: 0.4424 (0.4315)  loss_n_60: 0.6316 (0.5516)  loss_n_80: 0.7606 (0.6848)  loss_n_100: 0.8384 (0.7859)  triple_100: 9.5944 (9.9398)  triple_80: 9.5163 (9.8404)  triple_60: 7.9105 (7.7077)  triple_40: 3.8643 (3.9926)  time: 0.9740  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [370/845]  eta: 0:07:43  loss: 34.5387 (33.9249)  loss_n_40: 0.4240 (0.4325)  loss_n_60: 0.6227 (0.5521)  loss_n_80: 0.6992 (0.6852)  loss_n_100: 0.8206 (0.7856)  triple_100: 9.6430 (9.9321)  triple_80: 9.5995 (9.8375)  triple_60: 7.9105 (7.7061)  triple_40: 3.8108 (3.9937)  time: 0.9742  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [380/845]  eta: 0:07:33  loss: 30.8548 (33.9076)  loss_n_40: 0.3156 (0.4323)  loss_n_60: 0.4565 (0.5514)  loss_n_80: 0.5870 (0.6840)  loss_n_100: 0.6891 (0.7842)  triple_100: 9.1606 (9.9211)  triple_80: 8.9215 (9.8283)  triple_60: 7.0246 (7.7043)  triple_40: 3.8550 (4.0020)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [390/845]  eta: 0:07:24  loss: 30.7795 (33.8678)  loss_n_40: 0.3024 (0.4305)  loss_n_60: 0.4082 (0.5496)  loss_n_80: 0.5357 (0.6821)  loss_n_100: 0.6316 (0.7821)  triple_100: 9.0432 (9.9096)  triple_80: 8.9120 (9.8169)  triple_60: 6.8548 (7.6970)  triple_40: 3.8594 (4.0000)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [400/845]  eta: 0:07:14  loss: 31.1837 (33.8856)  loss_n_40: 0.3101 (0.4317)  loss_n_60: 0.4163 (0.5505)  loss_n_80: 0.5380 (0.6824)  loss_n_100: 0.6316 (0.7818)  triple_100: 9.2164 (9.9075)  triple_80: 9.1253 (9.8200)  triple_60: 7.2188 (7.7050)  triple_40: 3.7431 (4.0065)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [410/845]  eta: 0:07:04  loss: 36.8670 (33.9507)  loss_n_40: 0.3897 (0.4315)  loss_n_60: 0.5698 (0.5520)  loss_n_80: 0.7462 (0.6848)  loss_n_100: 0.8758 (0.7849)  triple_100: 9.7790 (9.9354)  triple_80: 9.8387 (9.8449)  triple_60: 8.2218 (7.7179)  triple_40: 3.7096 (3.9993)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [420/845]  eta: 0:06:54  loss: 33.6476 (33.9238)  loss_n_40: 0.3898 (0.4312)  loss_n_60: 0.5631 (0.5519)  loss_n_80: 0.7205 (0.6846)  loss_n_100: 0.8438 (0.7846)  triple_100: 9.6167 (9.9260)  triple_80: 9.5094 (9.8343)  triple_60: 7.8016 (7.7124)  triple_40: 3.6995 (3.9988)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [430/845]  eta: 0:06:44  loss: 31.2692 (33.9321)  loss_n_40: 0.3685 (0.4293)  loss_n_60: 0.4919 (0.5508)  loss_n_80: 0.5944 (0.6841)  loss_n_100: 0.7069 (0.7844)  triple_100: 9.2167 (9.9370)  triple_80: 9.1347 (9.8429)  triple_60: 7.2765 (7.7126)  triple_40: 3.7341 (3.9909)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [440/845]  eta: 0:06:35  loss: 31.2653 (33.9085)  loss_n_40: 0.3316 (0.4274)  loss_n_60: 0.4622 (0.5495)  loss_n_80: 0.6250 (0.6832)  loss_n_100: 0.7158 (0.7836)  triple_100: 9.2321 (9.9356)  triple_80: 9.1474 (9.8402)  triple_60: 7.1037 (7.7053)  triple_40: 3.6773 (3.9836)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [450/845]  eta: 0:06:25  loss: 31.7656 (33.9387)  loss_n_40: 0.3429 (0.4274)  loss_n_60: 0.4894 (0.5501)  loss_n_80: 0.6349 (0.6840)  loss_n_100: 0.7524 (0.7846)  triple_100: 9.5210 (9.9451)  triple_80: 9.4274 (9.8498)  triple_60: 7.2545 (7.7141)  triple_40: 3.6773 (3.9835)  time: 0.9746  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [460/845]  eta: 0:06:15  loss: 33.7527 (33.8943)  loss_n_40: 0.3497 (0.4278)  loss_n_60: 0.5066 (0.5501)  loss_n_80: 0.6632 (0.6833)  loss_n_100: 0.7657 (0.7832)  triple_100: 10.0573 (9.9245)  triple_80: 10.0310 (9.8333)  triple_60: 7.5059 (7.7058)  triple_40: 3.7683 (3.9862)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [470/845]  eta: 0:06:05  loss: 31.2545 (33.8896)  loss_n_40: 0.3497 (0.4302)  loss_n_60: 0.5066 (0.5523)  loss_n_80: 0.6767 (0.6849)  loss_n_100: 0.8033 (0.7847)  triple_100: 9.0900 (9.9133)  triple_80: 8.9671 (9.8257)  triple_60: 7.1016 (7.7062)  triple_40: 3.6974 (3.9923)  time: 0.9740  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [480/845]  eta: 0:05:56  loss: 31.7457 (33.8750)  loss_n_40: 0.3664 (0.4318)  loss_n_60: 0.5616 (0.5529)  loss_n_80: 0.6572 (0.6846)  loss_n_100: 0.7543 (0.7840)  triple_100: 9.1205 (9.9026)  triple_80: 9.0661 (9.8167)  triple_60: 7.1432 (7.7049)  triple_40: 3.6922 (3.9975)  time: 0.9741  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [490/845]  eta: 0:05:46  loss: 30.9271 (33.8733)  loss_n_40: 0.3301 (0.4318)  loss_n_60: 0.4659 (0.5530)  loss_n_80: 0.6124 (0.6848)  loss_n_100: 0.7088 (0.7843)  triple_100: 9.1205 (9.8998)  triple_80: 9.0661 (9.8135)  triple_60: 7.0872 (7.7056)  triple_40: 3.7361 (4.0005)  time: 0.9741  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [500/845]  eta: 0:05:36  loss: 32.2377 (33.8861)  loss_n_40: 0.3464 (0.4305)  loss_n_60: 0.4559 (0.5523)  loss_n_80: 0.5981 (0.6847)  loss_n_100: 0.6852 (0.7845)  triple_100: 9.4313 (9.9110)  triple_80: 9.4353 (9.8223)  triple_60: 7.2315 (7.7064)  triple_40: 3.7795 (3.9944)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [510/845]  eta: 0:05:26  loss: 31.5973 (33.8448)  loss_n_40: 0.3251 (0.4295)  loss_n_60: 0.4513 (0.5511)  loss_n_80: 0.5847 (0.6833)  loss_n_100: 0.6769 (0.7829)  triple_100: 9.2402 (9.8984)  triple_80: 9.2345 (9.8099)  triple_60: 7.2315 (7.6982)  triple_40: 3.6805 (3.9914)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [520/845]  eta: 0:05:17  loss: 31.2859 (33.8457)  loss_n_40: 0.3200 (0.4287)  loss_n_60: 0.4395 (0.5505)  loss_n_80: 0.5737 (0.6829)  loss_n_100: 0.6762 (0.7827)  triple_100: 9.2173 (9.9014)  triple_80: 9.1200 (9.8125)  triple_60: 7.1894 (7.6983)  triple_40: 3.6715 (3.9887)  time: 0.9748  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [530/845]  eta: 0:05:07  loss: 34.2056 (33.8984)  loss_n_40: 0.3921 (0.4301)  loss_n_60: 0.5772 (0.5526)  loss_n_80: 0.7062 (0.6853)  loss_n_100: 0.8744 (0.7855)  triple_100: 10.3580 (9.9191)  triple_80: 10.3024 (9.8301)  triple_60: 7.6256 (7.7094)  triple_40: 3.5473 (3.9863)  time: 0.9746  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:8]  [540/845]  eta: 0:04:57  loss: 32.7241 (33.8732)  loss_n_40: 0.3616 (0.4297)  loss_n_60: 0.5149 (0.5520)  loss_n_80: 0.6229 (0.6845)  loss_n_100: 0.7253 (0.7846)  triple_100: 9.8812 (9.9134)  triple_80: 9.7370 (9.8235)  triple_60: 7.4001 (7.7031)  triple_40: 3.5615 (3.9824)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [550/845]  eta: 0:04:47  loss: 31.1722 (33.8276)  loss_n_40: 0.3285 (0.4295)  loss_n_60: 0.4516 (0.5511)  loss_n_80: 0.5738 (0.6831)  loss_n_100: 0.6706 (0.7829)  triple_100: 9.2674 (9.8951)  triple_80: 9.0560 (9.8069)  triple_60: 7.0787 (7.6936)  triple_40: 3.7331 (3.9853)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [560/845]  eta: 0:04:38  loss: 31.9964 (33.8369)  loss_n_40: 0.3442 (0.4301)  loss_n_60: 0.4803 (0.5514)  loss_n_80: 0.5949 (0.6833)  loss_n_100: 0.6902 (0.7829)  triple_100: 9.2727 (9.8948)  triple_80: 9.2526 (9.8079)  triple_60: 7.2332 (7.6973)  triple_40: 3.7331 (3.9892)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [570/845]  eta: 0:04:28  loss: 34.5909 (33.8950)  loss_n_40: 0.3577 (0.4316)  loss_n_60: 0.5425 (0.5532)  loss_n_80: 0.7037 (0.6851)  loss_n_100: 0.8317 (0.7850)  triple_100: 10.1318 (9.9125)  triple_80: 10.0770 (9.8257)  triple_60: 8.0151 (7.7110)  triple_40: 3.6447 (3.9910)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [580/845]  eta: 0:04:18  loss: 37.7254 (33.9342)  loss_n_40: 0.3953 (0.4308)  loss_n_60: 0.5867 (0.5534)  loss_n_80: 0.7695 (0.6862)  loss_n_100: 0.9059 (0.7865)  triple_100: 11.2446 (9.9323)  triple_80: 10.9358 (9.8429)  triple_60: 8.5016 (7.7178)  triple_40: 3.5355 (3.9842)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [590/845]  eta: 0:04:08  loss: 37.7443 (33.9827)  loss_n_40: 0.4090 (0.4308)  loss_n_60: 0.6353 (0.5543)  loss_n_80: 0.8244 (0.6878)  loss_n_100: 0.9271 (0.7887)  triple_100: 11.2475 (9.9532)  triple_80: 11.1661 (9.8617)  triple_60: 8.4672 (7.7258)  triple_40: 3.5432 (3.9803)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [600/845]  eta: 0:03:58  loss: 35.2417 (33.9860)  loss_n_40: 0.3624 (0.4305)  loss_n_60: 0.5210 (0.5540)  loss_n_80: 0.6919 (0.6874)  loss_n_100: 0.7790 (0.7884)  triple_100: 10.2450 (9.9548)  triple_80: 10.0261 (9.8632)  triple_60: 7.9448 (7.7273)  triple_40: 3.5947 (3.9805)  time: 0.9747  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [610/845]  eta: 0:03:49  loss: 34.3632 (34.0170)  loss_n_40: 0.3822 (0.4308)  loss_n_60: 0.5568 (0.5550)  loss_n_80: 0.6919 (0.6888)  loss_n_100: 0.8017 (0.7900)  triple_100: 9.7890 (9.9670)  triple_80: 9.8361 (9.8739)  triple_60: 7.9119 (7.7340)  triple_40: 3.6147 (3.9775)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [620/845]  eta: 0:03:39  loss: 35.6164 (34.0616)  loss_n_40: 0.4126 (0.4325)  loss_n_60: 0.6118 (0.5561)  loss_n_80: 0.7400 (0.6897)  loss_n_100: 0.8760 (0.7907)  triple_100: 10.9888 (9.9741)  triple_80: 10.7698 (9.8817)  triple_60: 7.9766 (7.7461)  triple_40: 3.6981 (3.9905)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [630/845]  eta: 0:03:29  loss: 35.3660 (34.0566)  loss_n_40: 0.3769 (0.4322)  loss_n_60: 0.5715 (0.5560)  loss_n_80: 0.7027 (0.6896)  loss_n_100: 0.8299 (0.7908)  triple_100: 10.7875 (9.9742)  triple_80: 10.6688 (9.8814)  triple_60: 7.9766 (7.7445)  triple_40: 3.6981 (3.9879)  time: 0.9746  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [640/845]  eta: 0:03:19  loss: 34.9386 (34.0732)  loss_n_40: 0.3768 (0.4327)  loss_n_60: 0.5455 (0.5565)  loss_n_80: 0.7012 (0.6900)  loss_n_100: 0.8299 (0.7911)  triple_100: 10.3994 (9.9783)  triple_80: 10.0749 (9.8855)  triple_60: 7.8060 (7.7496)  triple_40: 3.5162 (3.9894)  time: 0.9748  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [650/845]  eta: 0:03:10  loss: 35.1194 (34.0839)  loss_n_40: 0.3662 (0.4319)  loss_n_60: 0.5402 (0.5565)  loss_n_80: 0.7012 (0.6905)  loss_n_100: 0.8329 (0.7919)  triple_100: 10.3994 (9.9858)  triple_80: 10.0749 (9.8918)  triple_60: 7.8475 (7.7501)  triple_40: 3.5603 (3.9855)  time: 0.9748  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [660/845]  eta: 0:03:00  loss: 36.0235 (34.1207)  loss_n_40: 0.3757 (0.4319)  loss_n_60: 0.5663 (0.5576)  loss_n_80: 0.7503 (0.6924)  loss_n_100: 0.8958 (0.7945)  triple_100: 10.8676 (10.0038)  triple_80: 10.5142 (9.9072)  triple_60: 7.8793 (7.7549)  triple_40: 3.5333 (3.9786)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [670/845]  eta: 0:02:50  loss: 36.4031 (34.1372)  loss_n_40: 0.3834 (0.4336)  loss_n_60: 0.5726 (0.5590)  loss_n_80: 0.7745 (0.6938)  loss_n_100: 0.8874 (0.7958)  triple_100: 10.8123 (10.0040)  triple_80: 10.4407 (9.9098)  triple_60: 8.3046 (7.7585)  triple_40: 3.4533 (3.9827)  time: 0.9741  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [680/845]  eta: 0:02:40  loss: 35.0365 (34.1674)  loss_n_40: 0.3700 (0.4343)  loss_n_60: 0.5449 (0.5595)  loss_n_80: 0.7309 (0.6944)  loss_n_100: 0.8662 (0.7966)  triple_100: 10.0403 (10.0120)  triple_80: 9.9017 (9.9166)  triple_60: 7.7581 (7.7644)  triple_40: 3.6831 (3.9896)  time: 0.9740  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [690/845]  eta: 0:02:31  loss: 33.2734 (34.1392)  loss_n_40: 0.3278 (0.4338)  loss_n_60: 0.4765 (0.5585)  loss_n_80: 0.6249 (0.6930)  loss_n_100: 0.7381 (0.7948)  triple_100: 9.2961 (10.0006)  triple_80: 9.2059 (9.9060)  triple_60: 7.4523 (7.7603)  triple_40: 3.7467 (3.9922)  time: 0.9745  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [700/845]  eta: 0:02:21  loss: 33.5491 (34.1528)  loss_n_40: 0.3548 (0.4329)  loss_n_60: 0.4878 (0.5581)  loss_n_80: 0.6323 (0.6929)  loss_n_100: 0.7381 (0.7948)  triple_100: 9.3566 (10.0092)  triple_80: 9.2037 (9.9128)  triple_60: 7.6778 (7.7630)  triple_40: 3.8347 (3.9891)  time: 0.9746  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [710/845]  eta: 0:02:11  loss: 32.0926 (34.1311)  loss_n_40: 0.3357 (0.4316)  loss_n_60: 0.4686 (0.5571)  loss_n_80: 0.6089 (0.6920)  loss_n_100: 0.7357 (0.7941)  triple_100: 9.6140 (10.0057)  triple_80: 9.5138 (9.9081)  triple_60: 7.3006 (7.7578)  triple_40: 3.7106 (3.9846)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [720/845]  eta: 0:02:01  loss: 32.4492 (34.1351)  loss_n_40: 0.3266 (0.4313)  loss_n_60: 0.4653 (0.5569)  loss_n_80: 0.6058 (0.6918)  loss_n_100: 0.7159 (0.7939)  triple_100: 9.6976 (10.0068)  triple_80: 9.5648 (9.9090)  triple_60: 7.3689 (7.7597)  triple_40: 3.6176 (3.9858)  time: 0.9740  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [730/845]  eta: 0:01:52  loss: 37.5519 (34.2028)  loss_n_40: 0.4206 (0.4341)  loss_n_60: 0.6259 (0.5593)  loss_n_80: 0.7754 (0.6937)  loss_n_100: 0.8576 (0.7956)  triple_100: 10.9073 (10.0208)  triple_80: 10.7528 (9.9252)  triple_60: 8.2905 (7.7766)  triple_40: 3.8108 (3.9975)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [740/845]  eta: 0:01:42  loss: 38.6138 (34.2136)  loss_n_40: 0.4098 (0.4334)  loss_n_60: 0.6176 (0.5591)  loss_n_80: 0.7754 (0.6939)  loss_n_100: 0.8437 (0.7959)  triple_100: 11.1412 (10.0281)  triple_80: 10.9939 (9.9315)  triple_60: 8.6567 (7.7780)  triple_40: 3.7443 (3.9937)  time: 0.9744  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [750/845]  eta: 0:01:32  loss: 32.9787 (34.1859)  loss_n_40: 0.3440 (0.4338)  loss_n_60: 0.4806 (0.5591)  loss_n_80: 0.6229 (0.6932)  loss_n_100: 0.7284 (0.7948)  triple_100: 9.3655 (10.0152)  triple_80: 9.3012 (9.9208)  triple_60: 7.4903 (7.7742)  triple_40: 3.6458 (3.9948)  time: 0.9742  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [760/845]  eta: 0:01:22  loss: 31.2488 (34.1770)  loss_n_40: 0.3433 (0.4332)  loss_n_60: 0.4702 (0.5588)  loss_n_80: 0.6201 (0.6927)  loss_n_100: 0.6896 (0.7942)  triple_100: 9.1440 (10.0138)  triple_80: 8.9780 (9.9192)  triple_60: 7.5070 (7.7738)  triple_40: 3.6473 (3.9912)  time: 0.9741  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [770/845]  eta: 0:01:13  loss: 32.9259 (34.1836)  loss_n_40: 0.3498 (0.4336)  loss_n_60: 0.5027 (0.5589)  loss_n_80: 0.6544 (0.6927)  loss_n_100: 0.7432 (0.7940)  triple_100: 9.8277 (10.0143)  triple_80: 9.7378 (9.9202)  triple_60: 7.6077 (7.7764)  triple_40: 3.6473 (3.9934)  time: 0.9745  data: 0.0002  max mem: 40153\n",
      "Valid: [epoch:8]  [780/845]  eta: 0:01:03  loss: 32.3157 (34.1727)  loss_n_40: 0.3365 (0.4331)  loss_n_60: 0.4778 (0.5586)  loss_n_80: 0.6247 (0.6924)  loss_n_100: 0.7421 (0.7939)  triple_100: 9.7949 (10.0138)  triple_80: 9.6226 (9.9184)  triple_60: 7.3154 (7.7733)  triple_40: 3.6178 (3.9891)  time: 0.9746  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [epoch:8]  [790/845]  eta: 0:00:53  loss: 31.6750 (34.1621)  loss_n_40: 0.3365 (0.4338)  loss_n_60: 0.4688 (0.5590)  loss_n_80: 0.6185 (0.6927)  loss_n_100: 0.7421 (0.7939)  triple_100: 9.3010 (10.0081)  triple_80: 9.1848 (9.9141)  triple_60: 7.3154 (7.7711)  triple_40: 3.6187 (3.9894)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [800/845]  eta: 0:00:43  loss: 34.0293 (34.1874)  loss_n_40: 0.3855 (0.4348)  loss_n_60: 0.5700 (0.5601)  loss_n_80: 0.7577 (0.6939)  loss_n_100: 0.8597 (0.7952)  triple_100: 9.7774 (10.0145)  triple_80: 9.5358 (9.9207)  triple_60: 7.9479 (7.7768)  triple_40: 3.6364 (3.9914)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [810/845]  eta: 0:00:34  loss: 31.8707 (34.1520)  loss_n_40: 0.3191 (0.4345)  loss_n_60: 0.4622 (0.5596)  loss_n_80: 0.6809 (0.6930)  loss_n_100: 0.7852 (0.7943)  triple_100: 9.5131 (10.0028)  triple_80: 9.2996 (9.9091)  triple_60: 7.3968 (7.7690)  triple_40: 3.6109 (3.9897)  time: 0.9748  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [820/845]  eta: 0:00:24  loss: 31.8707 (34.1385)  loss_n_40: 0.3185 (0.4354)  loss_n_60: 0.4622 (0.5601)  loss_n_80: 0.5856 (0.6934)  loss_n_100: 0.6847 (0.7944)  triple_100: 8.7551 (9.9941)  triple_80: 9.0718 (9.9025)  triple_60: 7.3354 (7.7654)  triple_40: 3.6757 (3.9932)  time: 0.9750  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [830/845]  eta: 0:00:14  loss: 32.1809 (34.1059)  loss_n_40: 0.3487 (0.4348)  loss_n_60: 0.4923 (0.5593)  loss_n_80: 0.6441 (0.6923)  loss_n_100: 0.7355 (0.7931)  triple_100: 8.1892 (9.9829)  triple_80: 8.3368 (9.8922)  triple_60: 7.2588 (7.7584)  triple_40: 3.7824 (3.9929)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [840/845]  eta: 0:00:04  loss: 30.6520 (34.1133)  loss_n_40: 0.3487 (0.4363)  loss_n_60: 0.4876 (0.5604)  loss_n_80: 0.6368 (0.6932)  loss_n_100: 0.6710 (0.7936)  triple_100: 8.3554 (9.9794)  triple_80: 8.4346 (9.8918)  triple_60: 7.0186 (7.7610)  triple_40: 3.7824 (3.9976)  time: 0.9743  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8]  [844/845]  eta: 0:00:00  loss: 30.6998 (34.1007)  loss_n_40: 0.3215 (0.4357)  loss_n_60: 0.4362 (0.5597)  loss_n_80: 0.5636 (0.6924)  loss_n_100: 0.6710 (0.7929)  triple_100: 8.4534 (9.9764)  triple_80: 9.0623 (9.8885)  triple_60: 7.2349 (7.7584)  triple_40: 3.7589 (3.9968)  time: 0.9746  data: 0.0001  max mem: 40153\n",
      "Valid: [epoch:8] Total time: 0:13:44 (0.9754 s / it)\n",
      "Averaged stats: loss: 30.6998 (34.1007)  loss_n_40: 0.3215 (0.4357)  loss_n_60: 0.4362 (0.5597)  loss_n_80: 0.5636 (0.6924)  loss_n_100: 0.6710 (0.7929)  triple_100: 8.4534 (9.9764)  triple_80: 9.0623 (9.8885)  triple_60: 7.2349 (7.7584)  triple_40: 3.7589 (3.9968)\n",
      "/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle_2/low2high/epoch_8_input_n_20.png\n",
      "loss_n_100 of the network on the 845 valid images: 0.793%\n",
      "Min loss_n_100: 0.465\n",
      "Best Epoch: 7.000\n",
      "Train: [epoch:9]  [   0/1724]  eta: 3:01:04  lr: 0.000160  loss: 32.6516 (32.6516)  loss_n_40: 0.4807 (0.4807)  loss_n_60: 0.5732 (0.5732)  loss_n_80: 0.6861 (0.6861)  loss_n_100: 0.7593 (0.7593)  triple_100: 9.3491 (9.3491)  triple_80: 9.4064 (9.4064)  triple_60: 7.5041 (7.5041)  triple_40: 3.8927 (3.8927)  time: 6.3018  data: 0.5822  max mem: 40153\n",
      "Train: [epoch:9]  [  10/1724]  eta: 2:49:16  lr: 0.000160  loss: 43.0525 (42.9486)  loss_n_40: 0.8594 (0.8035)  loss_n_60: 0.7836 (0.8273)  loss_n_80: 0.9582 (0.9897)  loss_n_100: 1.0215 (1.0847)  triple_100: 11.9734 (11.8263)  triple_80: 12.0509 (11.9251)  triple_60: 9.1555 (9.3292)  triple_40: 6.1109 (6.1628)  time: 5.9255  data: 0.0531  max mem: 40153\n",
      "Train: [epoch:9]  [  20/1724]  eta: 2:47:46  lr: 0.000160  loss: 46.4191 (45.3471)  loss_n_40: 0.8594 (0.8748)  loss_n_60: 0.8055 (0.8794)  loss_n_80: 0.9648 (1.0192)  loss_n_100: 1.0459 (1.1065)  triple_100: 12.3749 (12.2952)  triple_80: 12.6357 (12.4716)  triple_60: 10.0089 (9.9888)  triple_40: 6.9551 (6.7116)  time: 5.8877  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [  30/1724]  eta: 2:46:37  lr: 0.000160  loss: 46.5007 (45.0536)  loss_n_40: 0.6793 (0.8109)  loss_n_60: 0.8055 (0.8566)  loss_n_80: 0.9613 (1.0022)  loss_n_100: 1.0716 (1.1000)  triple_100: 12.6086 (12.3765)  triple_80: 12.7623 (12.5216)  triple_60: 10.5227 (10.0041)  triple_40: 6.1496 (6.3816)  time: 5.8882  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [  40/1724]  eta: 2:45:34  lr: 0.000160  loss: 42.0588 (43.8555)  loss_n_40: 0.5748 (0.7490)  loss_n_60: 0.7000 (0.8062)  loss_n_80: 0.8640 (0.9462)  loss_n_100: 0.9373 (1.0425)  triple_100: 11.8737 (12.1128)  triple_80: 11.9668 (12.2268)  triple_60: 9.5788 (9.8075)  triple_40: 5.6553 (6.1645)  time: 5.8911  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [  50/1724]  eta: 2:44:33  lr: 0.000160  loss: 39.4964 (42.7371)  loss_n_40: 0.4856 (0.6929)  loss_n_60: 0.5973 (0.7626)  loss_n_80: 0.7165 (0.8997)  loss_n_100: 0.8124 (0.9951)  triple_100: 11.0579 (11.8702)  triple_80: 11.0707 (11.9586)  triple_60: 9.0142 (9.6189)  triple_40: 5.1265 (5.9391)  time: 5.8938  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [  60/1724]  eta: 2:43:33  lr: 0.000160  loss: 36.5259 (41.6255)  loss_n_40: 0.4156 (0.6525)  loss_n_60: 0.5453 (0.7258)  loss_n_80: 0.6439 (0.8553)  loss_n_100: 0.7350 (0.9473)  triple_100: 10.2105 (11.5541)  triple_80: 10.3751 (11.6433)  triple_60: 8.5489 (9.4367)  triple_40: 4.8954 (5.8105)  time: 5.8940  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [  70/1724]  eta: 2:42:33  lr: 0.000160  loss: 35.3781 (40.6999)  loss_n_40: 0.4047 (0.6275)  loss_n_60: 0.5021 (0.7016)  loss_n_80: 0.6064 (0.8251)  loss_n_100: 0.7014 (0.9152)  triple_100: 9.6979 (11.2924)  triple_80: 9.8260 (11.3809)  triple_60: 8.3144 (9.2662)  triple_40: 4.9375 (5.6910)  time: 5.8940  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [  80/1724]  eta: 2:41:34  lr: 0.000160  loss: 35.4447 (40.0799)  loss_n_40: 0.4134 (0.6075)  loss_n_60: 0.5043 (0.6812)  loss_n_80: 0.6064 (0.8000)  loss_n_100: 0.6685 (0.8882)  triple_100: 9.6846 (11.1117)  triple_80: 9.8260 (11.1973)  triple_60: 8.3536 (9.1612)  triple_40: 5.0789 (5.6327)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [  90/1724]  eta: 2:40:35  lr: 0.000160  loss: 34.4111 (39.3655)  loss_n_40: 0.4282 (0.5921)  loss_n_60: 0.5196 (0.6665)  loss_n_80: 0.6126 (0.7791)  loss_n_100: 0.6685 (0.8641)  triple_100: 9.5510 (10.8961)  triple_80: 9.6794 (10.9869)  triple_60: 8.3877 (9.0382)  triple_40: 4.8141 (5.5425)  time: 5.8954  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 100/1724]  eta: 2:39:35  lr: 0.000160  loss: 33.3317 (38.7170)  loss_n_40: 0.3662 (0.5735)  loss_n_60: 0.4522 (0.6469)  loss_n_80: 0.5307 (0.7555)  loss_n_100: 0.6047 (0.8382)  triple_100: 9.1623 (10.7027)  triple_80: 9.2760 (10.7928)  triple_60: 7.9627 (8.9147)  triple_40: 4.7534 (5.4926)  time: 5.8935  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 110/1724]  eta: 2:38:36  lr: 0.000160  loss: 31.3334 (38.0279)  loss_n_40: 0.3185 (0.5492)  loss_n_60: 0.4072 (0.6229)  loss_n_80: 0.4876 (0.7282)  loss_n_100: 0.5464 (0.8089)  triple_100: 8.7370 (10.5133)  triple_80: 8.7251 (10.5962)  triple_60: 7.5020 (8.7814)  triple_40: 4.7616 (5.4277)  time: 5.8926  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 120/1724]  eta: 2:37:36  lr: 0.000160  loss: 31.3421 (37.6144)  loss_n_40: 0.3111 (0.5354)  loss_n_60: 0.4065 (0.6091)  loss_n_80: 0.4720 (0.7118)  loss_n_100: 0.5310 (0.7911)  triple_100: 8.6426 (10.3957)  triple_80: 8.6450 (10.4747)  triple_60: 7.5529 (8.7092)  triple_40: 4.7616 (5.3874)  time: 5.8932  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 130/1724]  eta: 2:36:37  lr: 0.000160  loss: 31.2209 (37.2123)  loss_n_40: 0.3111 (0.5213)  loss_n_60: 0.4065 (0.5949)  loss_n_80: 0.4825 (0.6956)  loss_n_100: 0.5400 (0.7737)  triple_100: 8.7126 (10.2851)  triple_80: 8.6880 (10.3604)  triple_60: 7.5167 (8.6346)  triple_40: 4.7469 (5.3467)  time: 5.8935  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 140/1724]  eta: 2:35:38  lr: 0.000160  loss: 31.0647 (36.7232)  loss_n_40: 0.2931 (0.5047)  loss_n_60: 0.3780 (0.5795)  loss_n_80: 0.4650 (0.6785)  loss_n_100: 0.5333 (0.7556)  triple_100: 8.6015 (10.1600)  triple_80: 8.6105 (10.2278)  triple_60: 7.4299 (8.5361)  triple_40: 4.4767 (5.2811)  time: 5.8923  data: 0.0002  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:9]  [ 150/1724]  eta: 2:34:39  lr: 0.000160  loss: 30.9260 (36.3546)  loss_n_40: 0.2795 (0.4924)  loss_n_60: 0.3760 (0.5674)  loss_n_80: 0.4572 (0.6650)  loss_n_100: 0.5218 (0.7415)  triple_100: 8.6015 (10.0651)  triple_80: 8.6105 (10.1255)  triple_60: 7.3396 (8.4621)  triple_40: 4.3964 (5.2356)  time: 5.8933  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 160/1724]  eta: 2:33:40  lr: 0.000160  loss: 28.7021 (35.9460)  loss_n_40: 0.2698 (0.4812)  loss_n_60: 0.3637 (0.5551)  loss_n_80: 0.4439 (0.6516)  loss_n_100: 0.5143 (0.7270)  triple_100: 8.2553 (9.9576)  triple_80: 8.1265 (10.0139)  triple_60: 6.8904 (8.3749)  triple_40: 4.4140 (5.1847)  time: 5.8939  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 170/1724]  eta: 2:32:40  lr: 0.000160  loss: 29.5814 (35.6398)  loss_n_40: 0.2726 (0.4712)  loss_n_60: 0.3635 (0.5456)  loss_n_80: 0.4432 (0.6410)  loss_n_100: 0.5011 (0.7158)  triple_100: 8.2875 (9.8735)  triple_80: 8.2505 (9.9279)  triple_60: 7.0083 (8.3123)  triple_40: 4.4806 (5.1525)  time: 5.8929  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 180/1724]  eta: 2:31:41  lr: 0.000160  loss: 30.3126 (35.3442)  loss_n_40: 0.3019 (0.4623)  loss_n_60: 0.3850 (0.5371)  loss_n_80: 0.4470 (0.6319)  loss_n_100: 0.5151 (0.7057)  triple_100: 8.4460 (9.7906)  triple_80: 8.3416 (9.8457)  triple_60: 7.2146 (8.2495)  triple_40: 4.6386 (5.1213)  time: 5.8922  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 190/1724]  eta: 2:30:42  lr: 0.000160  loss: 30.7915 (35.1804)  loss_n_40: 0.3155 (0.4567)  loss_n_60: 0.3965 (0.5323)  loss_n_80: 0.4725 (0.6268)  loss_n_100: 0.5321 (0.7006)  triple_100: 8.6001 (9.7530)  triple_80: 8.6609 (9.8046)  triple_60: 7.2964 (8.2163)  triple_40: 4.4645 (5.0901)  time: 5.8914  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 200/1724]  eta: 2:29:43  lr: 0.000160  loss: 29.9728 (34.8798)  loss_n_40: 0.2873 (0.4478)  loss_n_60: 0.3708 (0.5228)  loss_n_80: 0.4505 (0.6161)  loss_n_100: 0.5149 (0.6890)  triple_100: 8.3615 (9.6699)  triple_80: 8.3181 (9.7194)  triple_60: 7.2136 (8.1546)  triple_40: 4.4623 (5.0601)  time: 5.8920  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 210/1724]  eta: 2:28:44  lr: 0.000160  loss: 29.9728 (34.7180)  loss_n_40: 0.2873 (0.4464)  loss_n_60: 0.3646 (0.5233)  loss_n_80: 0.4505 (0.6135)  loss_n_100: 0.5048 (0.6852)  triple_100: 8.3486 (9.6076)  triple_80: 8.3181 (9.6642)  triple_60: 7.1015 (8.1284)  triple_40: 4.4885 (5.0494)  time: 5.8929  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 220/1724]  eta: 2:27:45  lr: 0.000160  loss: 30.0966 (34.5293)  loss_n_40: 0.3422 (0.4406)  loss_n_60: 0.3882 (0.5175)  loss_n_80: 0.4573 (0.6072)  loss_n_100: 0.5295 (0.6787)  triple_100: 8.3502 (9.5594)  triple_80: 8.3189 (9.6122)  triple_60: 7.1015 (8.0871)  triple_40: 4.5722 (5.0267)  time: 5.8934  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 230/1724]  eta: 2:26:46  lr: 0.000160  loss: 31.0502 (34.4667)  loss_n_40: 0.3215 (0.4372)  loss_n_60: 0.4053 (0.5153)  loss_n_80: 0.4836 (0.6053)  loss_n_100: 0.5356 (0.6773)  triple_100: 8.5376 (9.5519)  triple_80: 8.5467 (9.5996)  triple_60: 7.4277 (8.0737)  triple_40: 4.5039 (5.0065)  time: 5.8936  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 240/1724]  eta: 2:25:47  lr: 0.000160  loss: 31.3196 (34.3775)  loss_n_40: 0.3343 (0.4353)  loss_n_60: 0.4053 (0.5138)  loss_n_80: 0.4945 (0.6028)  loss_n_100: 0.5477 (0.6746)  triple_100: 8.7373 (9.5261)  triple_80: 8.8302 (9.5739)  triple_60: 7.4448 (8.0584)  triple_40: 4.4995 (4.9926)  time: 5.8927  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 250/1724]  eta: 2:24:48  lr: 0.000160  loss: 30.4713 (34.2189)  loss_n_40: 0.3335 (0.4311)  loss_n_60: 0.3963 (0.5106)  loss_n_80: 0.4563 (0.5977)  loss_n_100: 0.5175 (0.6686)  triple_100: 8.5747 (9.4765)  triple_80: 8.5956 (9.5269)  triple_60: 7.2771 (8.0303)  triple_40: 4.4754 (4.9772)  time: 5.8929  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 260/1724]  eta: 2:23:49  lr: 0.000160  loss: 30.1193 (34.0745)  loss_n_40: 0.3335 (0.4274)  loss_n_60: 0.4050 (0.5061)  loss_n_80: 0.4585 (0.5923)  loss_n_100: 0.4946 (0.6625)  triple_100: 8.1588 (9.4318)  triple_80: 8.4054 (9.4836)  triple_60: 7.2318 (8.0030)  triple_40: 4.4823 (4.9678)  time: 5.8935  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 270/1724]  eta: 2:22:50  lr: 0.000160  loss: 30.2388 (33.9494)  loss_n_40: 0.2867 (0.4225)  loss_n_60: 0.3885 (0.5010)  loss_n_80: 0.4357 (0.5864)  loss_n_100: 0.4952 (0.6562)  triple_100: 8.4462 (9.3969)  triple_80: 8.4815 (9.4474)  triple_60: 7.2493 (7.9806)  triple_40: 4.4085 (4.9583)  time: 5.8947  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 280/1724]  eta: 2:21:51  lr: 0.000160  loss: 28.8249 (33.7444)  loss_n_40: 0.2607 (0.4170)  loss_n_60: 0.3346 (0.4944)  loss_n_80: 0.3999 (0.5791)  loss_n_100: 0.4600 (0.6483)  triple_100: 8.0491 (9.3395)  triple_80: 7.9971 (9.3880)  triple_60: 6.9673 (7.9372)  triple_40: 4.4648 (4.9409)  time: 5.8963  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 290/1724]  eta: 2:20:52  lr: 0.000160  loss: 29.0273 (33.6724)  loss_n_40: 0.2658 (0.4138)  loss_n_60: 0.3346 (0.4910)  loss_n_80: 0.4026 (0.5752)  loss_n_100: 0.4627 (0.6440)  triple_100: 8.1099 (9.3170)  triple_80: 8.1464 (9.3648)  triple_60: 6.9833 (7.9242)  triple_40: 4.5188 (4.9425)  time: 5.8961  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 300/1724]  eta: 2:19:53  lr: 0.000160  loss: 29.1234 (33.4917)  loss_n_40: 0.3133 (0.4117)  loss_n_60: 0.3720 (0.4870)  loss_n_80: 0.4476 (0.5702)  loss_n_100: 0.4991 (0.6383)  triple_100: 8.0114 (9.2620)  triple_80: 8.0924 (9.3096)  triple_60: 6.9524 (7.8845)  triple_40: 4.5112 (4.9284)  time: 5.8946  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 310/1724]  eta: 2:18:54  lr: 0.000160  loss: 28.3986 (33.3816)  loss_n_40: 0.2903 (0.4086)  loss_n_60: 0.3472 (0.4834)  loss_n_80: 0.4205 (0.5661)  loss_n_100: 0.4961 (0.6339)  triple_100: 7.8742 (9.2319)  triple_80: 7.9345 (9.2781)  triple_60: 6.8670 (7.8613)  triple_40: 4.4578 (4.9183)  time: 5.8927  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 320/1724]  eta: 2:17:55  lr: 0.000160  loss: 28.4185 (33.2484)  loss_n_40: 0.2596 (0.4048)  loss_n_60: 0.3285 (0.4792)  loss_n_80: 0.4079 (0.5613)  loss_n_100: 0.4609 (0.6288)  triple_100: 8.0191 (9.1949)  triple_80: 7.9459 (9.2400)  triple_60: 6.8670 (7.8343)  triple_40: 4.4386 (4.9050)  time: 5.8934  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 330/1724]  eta: 2:16:56  lr: 0.000160  loss: 29.4926 (33.1727)  loss_n_40: 0.2723 (0.4018)  loss_n_60: 0.3462 (0.4763)  loss_n_80: 0.4309 (0.5581)  loss_n_100: 0.4866 (0.6255)  triple_100: 8.0874 (9.1754)  triple_80: 8.2128 (9.2191)  triple_60: 7.1083 (7.8194)  triple_40: 4.4352 (4.8970)  time: 5.8947  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 340/1724]  eta: 2:15:57  lr: 0.000160  loss: 30.2887 (33.1041)  loss_n_40: 0.2784 (0.3987)  loss_n_60: 0.3559 (0.4733)  loss_n_80: 0.4325 (0.5549)  loss_n_100: 0.5046 (0.6222)  triple_100: 8.4533 (9.1585)  triple_80: 8.4303 (9.2005)  triple_60: 7.1542 (7.8066)  triple_40: 4.4352 (4.8893)  time: 5.8950  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 350/1724]  eta: 2:14:59  lr: 0.000160  loss: 29.5718 (33.0154)  loss_n_40: 0.2784 (0.3962)  loss_n_60: 0.3559 (0.4702)  loss_n_80: 0.4003 (0.5510)  loss_n_100: 0.4597 (0.6179)  triple_100: 8.0758 (9.1283)  triple_80: 8.1019 (9.1713)  triple_60: 7.1064 (7.7911)  triple_40: 4.5996 (4.8893)  time: 5.8965  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 360/1724]  eta: 2:14:00  lr: 0.000160  loss: 29.3565 (32.9217)  loss_n_40: 0.2679 (0.3942)  loss_n_60: 0.3433 (0.4674)  loss_n_80: 0.4003 (0.5482)  loss_n_100: 0.4597 (0.6146)  triple_100: 7.9297 (9.1003)  triple_80: 8.0036 (9.1454)  triple_60: 6.9288 (7.7700)  triple_40: 4.5724 (4.8816)  time: 5.8974  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 370/1724]  eta: 2:13:01  lr: 0.000160  loss: 28.2308 (32.7881)  loss_n_40: 0.2551 (0.3905)  loss_n_60: 0.3206 (0.4633)  loss_n_80: 0.3730 (0.5435)  loss_n_100: 0.4367 (0.6094)  triple_100: 7.8697 (9.0623)  triple_80: 7.8172 (9.1065)  triple_60: 6.6991 (7.7424)  triple_40: 4.4031 (4.8701)  time: 5.8962  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 380/1724]  eta: 2:12:02  lr: 0.000160  loss: 27.4497 (32.6541)  loss_n_40: 0.2551 (0.3872)  loss_n_60: 0.3176 (0.4596)  loss_n_80: 0.3691 (0.5391)  loss_n_100: 0.4178 (0.6046)  triple_100: 7.5435 (9.0225)  triple_80: 7.5337 (9.0671)  triple_60: 6.6602 (7.7148)  triple_40: 4.4331 (4.8594)  time: 5.8962  data: 0.0001  max mem: 40153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [epoch:9]  [ 390/1724]  eta: 2:11:03  lr: 0.000160  loss: 28.0200 (32.5705)  loss_n_40: 0.2784 (0.3861)  loss_n_60: 0.3228 (0.4577)  loss_n_80: 0.3874 (0.5366)  loss_n_100: 0.4418 (0.6016)  triple_100: 7.7966 (8.9957)  triple_80: 7.7792 (9.0412)  triple_60: 6.7313 (7.6989)  triple_40: 4.4078 (4.8526)  time: 5.8962  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 400/1724]  eta: 2:10:04  lr: 0.000160  loss: 29.4257 (32.5427)  loss_n_40: 0.2954 (0.3859)  loss_n_60: 0.3560 (0.4569)  loss_n_80: 0.4178 (0.5352)  loss_n_100: 0.4829 (0.5996)  triple_100: 8.1504 (8.9795)  triple_80: 8.1336 (9.0288)  triple_60: 7.1104 (7.6970)  triple_40: 4.5563 (4.8597)  time: 5.8957  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 410/1724]  eta: 2:09:05  lr: 0.000160  loss: 29.8436 (32.4750)  loss_n_40: 0.3033 (0.3841)  loss_n_60: 0.3958 (0.4550)  loss_n_80: 0.4529 (0.5328)  loss_n_100: 0.4975 (0.5970)  triple_100: 8.0714 (8.9591)  triple_80: 8.2236 (9.0088)  triple_60: 7.1913 (7.6838)  triple_40: 4.7867 (4.8543)  time: 5.8963  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 420/1724]  eta: 2:08:06  lr: 0.000160  loss: 30.2142 (32.4361)  loss_n_40: 0.3033 (0.3827)  loss_n_60: 0.3958 (0.4539)  loss_n_80: 0.4529 (0.5318)  loss_n_100: 0.4975 (0.5960)  triple_100: 8.0714 (8.9517)  triple_80: 8.2432 (8.9996)  triple_60: 7.3688 (7.6746)  triple_40: 4.4688 (4.8458)  time: 5.8956  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 430/1724]  eta: 2:07:07  lr: 0.000160  loss: 31.4142 (32.3982)  loss_n_40: 0.3353 (0.3818)  loss_n_60: 0.4222 (0.4529)  loss_n_80: 0.4633 (0.5303)  loss_n_100: 0.5119 (0.5943)  triple_100: 8.5200 (8.9385)  triple_80: 8.6786 (8.9868)  triple_60: 7.4037 (7.6703)  triple_40: 4.4847 (4.8432)  time: 5.8960  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 440/1724]  eta: 2:06:08  lr: 0.000160  loss: 30.0148 (32.3523)  loss_n_40: 0.3343 (0.3806)  loss_n_60: 0.3819 (0.4515)  loss_n_80: 0.4461 (0.5289)  loss_n_100: 0.5079 (0.5928)  triple_100: 8.5039 (8.9249)  triple_80: 8.3969 (8.9748)  triple_60: 7.1562 (7.6592)  triple_40: 4.5143 (4.8396)  time: 5.8971  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 450/1724]  eta: 2:05:10  lr: 0.000160  loss: 29.6123 (32.2821)  loss_n_40: 0.2899 (0.3789)  loss_n_60: 0.3596 (0.4497)  loss_n_80: 0.4372 (0.5271)  loss_n_100: 0.4989 (0.5909)  triple_100: 8.0986 (8.9079)  triple_80: 8.2262 (8.9559)  triple_60: 7.0253 (7.6429)  triple_40: 4.4967 (4.8288)  time: 5.8962  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 460/1724]  eta: 2:04:11  lr: 0.000160  loss: 28.6293 (32.2226)  loss_n_40: 0.2697 (0.3769)  loss_n_60: 0.3492 (0.4477)  loss_n_80: 0.4117 (0.5249)  loss_n_100: 0.4786 (0.5886)  triple_100: 7.9956 (8.8924)  triple_80: 8.0101 (8.9398)  triple_60: 6.7971 (7.6307)  triple_40: 4.3212 (4.8216)  time: 5.8955  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 470/1724]  eta: 2:03:12  lr: 0.000160  loss: 28.6683 (32.1602)  loss_n_40: 0.2728 (0.3752)  loss_n_60: 0.3409 (0.4460)  loss_n_80: 0.4050 (0.5229)  loss_n_100: 0.4621 (0.5863)  triple_100: 7.9843 (8.8750)  triple_80: 7.9687 (8.9221)  triple_60: 6.9249 (7.6181)  triple_40: 4.3556 (4.8147)  time: 5.8947  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 480/1724]  eta: 2:02:13  lr: 0.000160  loss: 29.3643 (32.1223)  loss_n_40: 0.2876 (0.3740)  loss_n_60: 0.3690 (0.4447)  loss_n_80: 0.4334 (0.5214)  loss_n_100: 0.4856 (0.5847)  triple_100: 8.1647 (8.8652)  triple_80: 8.0904 (8.9115)  triple_60: 7.0847 (7.6114)  triple_40: 4.3854 (4.8095)  time: 5.8942  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 490/1724]  eta: 2:01:14  lr: 0.000160  loss: 29.3643 (32.0653)  loss_n_40: 0.3055 (0.3734)  loss_n_60: 0.3595 (0.4440)  loss_n_80: 0.4250 (0.5198)  loss_n_100: 0.4853 (0.5827)  triple_100: 8.0489 (8.8451)  triple_80: 8.0904 (8.8931)  triple_60: 7.1499 (7.6017)  triple_40: 4.3998 (4.8055)  time: 5.8944  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 500/1724]  eta: 2:00:15  lr: 0.000160  loss: 27.4596 (31.9857)  loss_n_40: 0.2470 (0.3714)  loss_n_60: 0.3157 (0.4417)  loss_n_80: 0.3707 (0.5170)  loss_n_100: 0.4164 (0.5795)  triple_100: 7.6101 (8.8210)  triple_80: 7.5915 (8.8696)  triple_60: 6.6667 (7.5861)  triple_40: 4.3998 (4.7994)  time: 5.8949  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 510/1724]  eta: 1:59:16  lr: 0.000160  loss: 28.2229 (31.9369)  loss_n_40: 0.2524 (0.3695)  loss_n_60: 0.3157 (0.4398)  loss_n_80: 0.3707 (0.5150)  loss_n_100: 0.4255 (0.5774)  triple_100: 7.8112 (8.8091)  triple_80: 7.8271 (8.8565)  triple_60: 6.8514 (7.5761)  triple_40: 4.3973 (4.7935)  time: 5.8948  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 520/1724]  eta: 1:58:17  lr: 0.000160  loss: 29.0680 (31.9056)  loss_n_40: 0.2862 (0.3682)  loss_n_60: 0.3547 (0.4383)  loss_n_80: 0.4231 (0.5133)  loss_n_100: 0.4695 (0.5755)  triple_100: 8.1260 (8.7996)  triple_80: 8.0849 (8.8467)  triple_60: 7.0590 (7.5706)  triple_40: 4.5107 (4.7934)  time: 5.8947  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 530/1724]  eta: 1:57:18  lr: 0.000160  loss: 29.6392 (31.8575)  loss_n_40: 0.2959 (0.3670)  loss_n_60: 0.3805 (0.4370)  loss_n_80: 0.4451 (0.5119)  loss_n_100: 0.4993 (0.5741)  triple_100: 8.1260 (8.7865)  triple_80: 8.1959 (8.8335)  triple_60: 7.1006 (7.5600)  triple_40: 4.5789 (4.7875)  time: 5.8947  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 540/1724]  eta: 1:56:19  lr: 0.000160  loss: 29.6392 (31.8180)  loss_n_40: 0.3224 (0.3664)  loss_n_60: 0.3895 (0.4363)  loss_n_80: 0.4475 (0.5107)  loss_n_100: 0.4993 (0.5728)  triple_100: 8.0431 (8.7752)  triple_80: 8.1959 (8.8215)  triple_60: 7.1050 (7.5531)  triple_40: 4.4450 (4.7819)  time: 5.8939  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 550/1724]  eta: 1:55:20  lr: 0.000160  loss: 29.0701 (31.7704)  loss_n_40: 0.3248 (0.3655)  loss_n_60: 0.3737 (0.4350)  loss_n_80: 0.4238 (0.5091)  loss_n_100: 0.4849 (0.5711)  triple_100: 8.0657 (8.7601)  triple_80: 8.0567 (8.8070)  triple_60: 7.0434 (7.5438)  triple_40: 4.4450 (4.7787)  time: 5.8936  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 560/1724]  eta: 1:54:21  lr: 0.000160  loss: 28.0595 (31.7089)  loss_n_40: 0.2910 (0.3641)  loss_n_60: 0.3473 (0.4334)  loss_n_80: 0.3945 (0.5071)  loss_n_100: 0.4369 (0.5689)  triple_100: 7.6234 (8.7412)  triple_80: 7.7293 (8.7886)  triple_60: 6.8259 (7.5315)  triple_40: 4.4229 (4.7741)  time: 5.8937  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 570/1724]  eta: 1:53:22  lr: 0.000160  loss: 28.1241 (31.6522)  loss_n_40: 0.2790 (0.3624)  loss_n_60: 0.3335 (0.4316)  loss_n_80: 0.3850 (0.5051)  loss_n_100: 0.4404 (0.5667)  triple_100: 7.7643 (8.7265)  triple_80: 7.7639 (8.7729)  triple_60: 6.7695 (7.5197)  triple_40: 4.4131 (4.7673)  time: 5.8926  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 580/1724]  eta: 1:52:23  lr: 0.000160  loss: 28.3240 (31.6127)  loss_n_40: 0.2525 (0.3609)  loss_n_60: 0.3143 (0.4302)  loss_n_80: 0.3809 (0.5036)  loss_n_100: 0.4370 (0.5652)  triple_100: 7.9989 (8.7172)  triple_80: 7.9417 (8.7622)  triple_60: 6.8017 (7.5112)  triple_40: 4.3786 (4.7621)  time: 5.8912  data: 0.0001  max mem: 40153\n",
      "Train: [epoch:9]  [ 590/1724]  eta: 1:51:24  lr: 0.000160  loss: 27.8336 (31.5566)  loss_n_40: 0.2558 (0.3595)  loss_n_60: 0.3264 (0.4287)  loss_n_80: 0.3919 (0.5018)  loss_n_100: 0.4532 (0.5633)  triple_100: 7.6700 (8.7010)  triple_80: 7.6591 (8.7454)  triple_60: 6.7522 (7.4991)  triple_40: 4.3693 (4.7577)  time: 5.8905  data: 0.0002  max mem: 40153\n",
      "Train: [epoch:9]  [ 600/1724]  eta: 1:50:25  lr: 0.000160  loss: 27.8336 (31.5216)  loss_n_40: 0.2917 (0.3589)  loss_n_60: 0.3382 (0.4278)  loss_n_80: 0.4022 (0.5008)  loss_n_100: 0.4586 (0.5620)  triple_100: 7.6700 (8.6898)  triple_80: 7.6591 (8.7351)  triple_60: 6.7772 (7.4922)  triple_40: 4.4820 (4.7549)  time: 5.8910  data: 0.0002  max mem: 40153\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 223, in <module>\n",
      "    main(args)\n",
      "  File \"train.py\", line 171, in main\n",
      "    train_stats = train_Sinogram(model, criterion, data_loader_train, optimizer, device, epoch, args.teacher_forcing)\n",
      "  File \"/workspace/sunggu/4.Dose_img2img/scripts study/engine.py\", line 86, in train_Sinogram\n",
      "    loss, loss_detail = criterion(n_40=pred_n_40, n_60=pred_n_60, n_80=pred_n_80, n_100=pred_n_100, gt_40=input_n_40, gt_60=input_n_60, gt_80=input_n_80, gt_100=input_n_100)\n",
      "  File \"/home/sunggu/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/workspace/sunggu/4.Dose_img2img/scripts study/losses.py\", line 188, in forward\n",
      "    + self.loss_Triple(anchor=n_80, positive=gt_80, negative=n_40)\n",
      "  File \"/home/sunggu/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/workspace/sunggu/4.Dose_img2img/scripts study/losses.py\", line 72, in forward\n",
      "    loss += self.weights[i]*self.criterion(anchor=a_vgg[i], positive=p_vgg[i].detach(), negative=n_vgg[i])        \n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--batch-size 4 \\\n",
    "--epochs 1000 \\\n",
    "--min-lr 5e-6 \\\n",
    "--lr 2e-4 \\\n",
    "--data-set 'Sinogram' \\\n",
    "--model-name 'Seqeunce_UNet_Hidden_bottle' \\\n",
    "--criterion 'Perceptual_L1_Triple_Loss' \\\n",
    "--criterion_mode '1:1' \\\n",
    "--output_dir '/workspace/sunggu/4.Dose_img2img/model/Seqeunce_UNet_Hidden_bottle_2' \\\n",
    "--save_dir '/workspace/sunggu/4.Dose_img2img/Predictions/Train/png/Seqeunce_UNet_Hidden_bottle_2/low2high/' \\\n",
    "--validate-every 5 \\\n",
    "--num_workers 8 \\\n",
    "--multi-gpu-mode 'DataParallel' \\\n",
    "--teacher_forcing \"False\"\n",
    "\n",
    "# --resume '/workspace/sunggu/4.Dose_img2img/model/[Sequence_All_Hidden_Unet]Dose Unet/epoch_6_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "def list_sort_nicely(l):   \n",
    "    def tryint(s):        \n",
    "        try:            \n",
    "            return int(s)        \n",
    "        except:            \n",
    "            return s\n",
    "        \n",
    "    def alphanum_key(s):\n",
    "        return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "    l.sort(key=alphanum_key)    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_20_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_NPY/Valid/*/20/*/*/*.npy'))\n",
    "n_40_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_NPY/Valid/*/40/*/*/*.npy'))\n",
    "n_60_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_NPY/Valid/*/60/*/*/*.npy'))\n",
    "n_80_imgs   = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_NPY/Valid/*/80/*/*/*.npy'))\n",
    "n_100_imgs  = list_sort_nicely(glob.glob('/workspace/sunggu/4.Dose_img2img/dataset/*Brain_3mm_NPY/Valid/*/X/*/*/*.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_windowing_V2(x):\n",
    "    x = np.clip(x, a_min=0.250, a_max=0.270)\n",
    "    x -= x.min()\n",
    "    x /= x.max()  \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n20  = np.load(n_20_imgs[20])\n",
    "n40  = np.load(n_40_imgs[20])\n",
    "n60  = np.load(n_60_imgs[20])\n",
    "n80  = np.load(n_80_imgs[20])\n",
    "n100 = np.load(n_100_imgs[20])\n",
    "\n",
    "\n",
    "# n20   = visual_windowing_V2(n20)\n",
    "# n40   = visual_windowing_V2(n40)\n",
    "# n60   = visual_windowing_V2(n60)\n",
    "# n80   = visual_windowing_V2(n80)\n",
    "# n100  = visual_windowing_V2(n100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(24,6))\n",
    "\n",
    "plt.subplot(141)\n",
    "# plt.imshow(np.abs(n100-n20), 'jet')\n",
    "a = np.abs(n100-n80)\n",
    "a[a==0] = 1e-100\n",
    "plt.imshow(np.log(a), 'gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(142)\n",
    "a = np.abs(n100-n60)\n",
    "a[a==0] = 1e-100\n",
    "plt.imshow(np.log(a), 'gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(143)\n",
    "a = np.abs(n100-n40)\n",
    "a[a==0] = 1e-100\n",
    "plt.imshow(np.log(a), 'gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(144)\n",
    "a = np.abs(n100-n20)\n",
    "a[a==0] = 1e-100\n",
    "plt.imshow(np.log(a), 'gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = torch.nn.TripletMarginLoss(margin=1.0, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n100 = torch.tensor(n100)\n",
    "n80 = torch.tensor(n80)\n",
    "n60 = torch.tensor(n60)\n",
    "n40 = torch.tensor(n40)\n",
    "n20 = torch.tensor(n20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_100_80  = triplet_loss(anchor=n100.flatten(1), positive=n80.flatten(1), negative=n60.flatten(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 \n",
    "triple_100_80  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40\n",
    "triple_100_80  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60\n",
    "triple_100_80  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag(torch.tensor(4), torch.tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = triplet_loss(anchor, positive, negative)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222.983px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
